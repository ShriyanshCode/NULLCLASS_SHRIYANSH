[
    {
        "paper_id": 1409.6444,
        "authors": "Ladislav Kristoufek",
        "title": "On the interplay between short and long term memory in the power-law\n  cross-correlations setting",
        "comments": "6 pages",
        "journal-ref": "Physica A: Statistical Mechanics and Its Applications 421, pp.\n  218-222 (2015)",
        "doi": "10.1016/j.physa.2014.11.040",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We focus on emergence of the power-law cross-correlations from processes with\nboth short and long term memory properties. In the case of correlated\nerror-terms, the power-law decay of the cross-correlation function comes\nautomatically with the characteristics of separate processes. Bivariate Hurst\nexponent is then equal to an average of separate Hurst exponents of the\nanalyzed processes. Strength of short term memory has no effect on these\nasymptotic properties. Implications of these findings for the power-law\ncross-correlations concept are further discussed.\n"
    },
    {
        "paper_id": 1409.6645,
        "authors": "Miha Troha and Raphael Hauser",
        "title": "Calculation of a power price equilibrium",
        "comments": "arXiv admin note: text overlap with arXiv:1408.2464",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a tractable quadratic programming formulation for\ncalculating the equilibrium term structure of electricity prices. We rely on a\ntheoretical model described in [21], but extend it so that it reflects actually\ntraded electricity contracts, transaction costs and liquidity considerations.\nOur numerical simulations examine the properties of the term structure and its\ndependence on various parameters of the model. The proposed quadratic\nprogramming formulation is applied to calculate the equilibrium term structure\nof electricity prices in the UK power grid consisting of a few hundred power\nplants. The impact of ramp up and ramp down constraints are also studied.\n"
    },
    {
        "paper_id": 1409.6646,
        "authors": "Guy Katriel",
        "title": "The Immediate Exchange model: an analytical investigation",
        "comments": null,
        "journal-ref": "The European Physical Journal B, 88:19 (2015)",
        "doi": "10.1140/epjb/e2014-50661-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Immediate Exchange model, recently introduced by Heinsalu and\nPatriarca [Eur. Phys. J. B 87: 170 (2014)], who showed by simulations that the\nwealth distribution in this model converges to a Gamma distribution with shape\nparameter $2$. Here we justify this conclusion analytically, in the\ninfinite-population limit. An infinite-population version of the model is\nderived, describing the evolution of the wealth distribution in terms of\niterations of a nonlinear operator on the space of probability densities. It is\nproved that the Gamma distributions with shape parameter $2$ are fixed points\nof this operator, and that, starting with an arbitrary wealth distribution, the\nprocess converges to one of these fixed points. We also discuss the mixed model\nintroduced in the same paper, in which exchanges are either bidirectional or\nunidirectional with fixed probability. We prove that, although, as found by\nHeinsalu and Patriarca, the equilibrium distribution can be closely fit by\nGamma distributions, the equilibrium distribution for this model is {\\it{not}}\na Gamma distribution.\n"
    },
    {
        "paper_id": 1409.6649,
        "authors": "Assaf Almog, Tiziano Squartini, Diego Garlaschelli",
        "title": "A GDP-driven model for the binary and weighted structure of the\n  International Trade Network",
        "comments": null,
        "journal-ref": "New J. Phys. 17, 013009 (2015)",
        "doi": "10.1088/1367-2630/17/1/013009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent events such as the global financial crisis have renewed the interest\nin the topic of economic networks. One of the main channels of shock\npropagation among countries is the International Trade Network (ITN). Two\nimportant models for the ITN structure, the classical gravity model of trade\n(more popular among economists) and the fitness model (more popular among\nnetworks scientists), are both limited to the characterization of only one\nrepresentation of the ITN. The gravity model satisfactorily predicts the volume\nof trade between connected countries, but cannot reproduce the observed missing\nlinks (i.e. the topology). On the other hand, the fitness model can\nsuccessfully replicate the topology of the ITN, but cannot predict the volumes.\nThis paper tries to make an important step forward in the unification of those\ntwo frameworks, by proposing a new GDP-driven model which can simultaneously\nreproduce the binary and the weighted properties of the ITN. Specifically, we\nadopt a maximum-entropy approach where both the degree and the strength of each\nnode is preserved. We then identify strong nonlinear relationships between the\nGDP and the parameters of the model. This ultimately results in a weighted\ngeneralization of the fitness model of trade, where the GDP plays the role of a\n`macroeconomic fitness' shaping the binary and the weighted structure of the\nITN simultaneously. Our model mathematically highlights an important asymmetry\nin the role of binary and weighted network properties, namely the fact that\nbinary properties can be inferred without the knowledge of weighted ones, while\nthe opposite is not true.\n"
    },
    {
        "paper_id": 1409.6773,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "On a Stopping Game in continuous time",
        "comments": "To appear in the Proceedings of the AMS. Final version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a zero-sum continuous time stopping game in which the pay-off is\nrevealed in the maximum of the two stopping times instead of the minimum, which\nis the case in Dynkin games.\n"
    },
    {
        "paper_id": 1409.6857,
        "authors": "Ladislav Kristoufek",
        "title": "Finite sample properties of power-law cross-correlations estimators",
        "comments": "19 pages, 18 tables",
        "journal-ref": "Physica A: Statistical Mechanics and Its Applications 419, pp.\n  513-525 (2015)",
        "doi": "10.1016/j.physa.2014.10.068",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We study finite sample properties of estimators of power-law\ncross-correlations -- detrended cross-correlation analysis (DCCA), height\ncross-correlation analysis (HXA) and detrending moving-average\ncross-correlation analysis (DMCA) -- with a special focus on short-term memory\nbias as well as power-law coherency. Presented broad Monte Carlo simulation\nstudy focuses on different time series lengths, specific methods' parameter\nsetting, and memory strength. We find that each method is best suited for\ndifferent time series dynamics so that there is no clear winner between the\nthree. The method selection should be then made based on observed dynamic\nproperties of the analyzed series.\n"
    },
    {
        "paper_id": 1409.694,
        "authors": "Patrick Beissner and Frank Riedel",
        "title": "Non-Implementability of Arrow-Debreu Equilibria by Continuous Trading\n  under Knightian Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under risk, Arrow-Debreu equilibria can be implemented as Radner equilibria\nby continuous trading of few long-lived securities. We show that this result\ngenerically fails if there is Knightian uncertainty in the volatility.\nImplementation is only possible if all discounted net trades of the equilibrium\nallocation are mean ambiguity-free.\n"
    },
    {
        "paper_id": 1409.7002,
        "authors": "Krzysztof Urbanowicz",
        "title": "Entropy and Optimization of Portfolios",
        "comments": "7 figures, 6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We briefly review the approach to optimization of portfolios according to the\ntheory of Markowitz and propose a further modification that can improve the\noutcome of the optimization process. The modification takes account of the\nentropic contribution from the time series used to compute the parameters in\nthe Markowitz method.\n"
    },
    {
        "paper_id": 1409.7028,
        "authors": "Tomasz R. Bielecki and Igor Cialenco and Marcin Pitera",
        "title": "A unified approach to time consistency of dynamic risk measures and\n  dynamic performance measures in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1287/moor.2017.0858",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a flexible framework allowing for a unified study of\ntime consistency of risk measures and performance measures (also known as\nacceptability indices). The proposed framework not only integrates existing\nforms of time consistency, but also provides a comprehensive toolbox for\nanalysis and synthesis of the concept of time consistency in decision making.\nIn particular, it allows for in depth comparative analysis of (most of) the\nexisting types of time consistency -- a feat that has not be possible before\nand which is done in the companion paper [BCP2016] to this one. In our approach\nthe time consistency is studied for a large class of maps that are postulated\nto satisfy only two properties -- monotonicity and locality. The time\nconsistency is defined in terms of an update rule. The form of the update rule\nintroduced here is novel, and is perfectly suited for developing the unifying\nframework that is worked out in this paper. As an illustration of the\napplicability of our approach, we show how to recover almost all concepts of\nweak time consistency by means of constructing appropriate update rules.\n"
    },
    {
        "paper_id": 1409.7269,
        "authors": "Jan Kallsen, Johannes Muhle-Karbe",
        "title": "High-Resilience Limits of Block-Shaped Order Books",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that wealth processes in the block-shaped order book model of\nObizhaeva/Wang converge to their counterparts in the reduced-form model\nproposed by Almgren/Chriss, as the resilience of the order book tends to\ninfinity. As an application of this limit theorem, we explain how to reduce\nportfolio choice in highly-resilient Obizhaeva/Wang models to the corresponding\nproblem in an Almgren/Chriss setup with small quadratic trading costs.\n"
    },
    {
        "paper_id": 1409.7512,
        "authors": "G. Augustins, L. Etienne, J-B. Ferdy, R. Ferrer, B. Godelle, E.\n  Pitard, F. Rousset",
        "title": "The evolution of wealth transmission in human populations: a stochastic\n  model",
        "comments": "5 pages, 3 figures",
        "journal-ref": "Journal of Physics: Conference series, 490, 012052 (2014)",
        "doi": "10.1088/1742-6596/490/1/012052",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reproductive success and survival are influenced by wealth in human\npopulations. Wealth is transmitted to offsprings and strategies of transmission\nvary over time and among populations, the main variation being how equally\nwealth is transmitted to children. Here we propose a model where we simulate\nboth the dynamics of wealth in a population and the evolution of a trait that\ndetermines how wealth is transmitted from parents to offspring, in a darwinian\ncontext.\n"
    },
    {
        "paper_id": 1409.772,
        "authors": "Y. Lemp\\'eri\\`ere, C. Deremble, T. T. Nguyen, P. Seager, M. Potters,\n  J. P. Bouchaud",
        "title": "Risk Premia: Asymmetric Tail Risks and Excess Returns",
        "comments": "25 pages, 9 Figures, 8 Tables. Revised version after first round of\n  referees",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present extensive evidence that ``risk premium'' is strongly correlated\nwith tail-risk skewness but very little with volatility. We introduce a new,\nintuitive definition of skewness and elicit an approximately linear relation\nbetween the Sharpe ratio of various risk premium strategies (Equity,\nFama-French, FX Carry, Short Vol, Bonds, Credit) and their negative skewness.\nWe find a clear exception to this rule: trend following has both positive\nskewness and positive excess returns. This is also true, albeit less markedly,\nof the Fama-French ``Value'' factor and of the ``Low Volatility'' strategy.\nThis suggests that some strategies are not risk premia but genuine market\nanomalies. Based on our results, we propose an objective criterion to assess\nthe quality of a risk-premium portfolio.\n"
    },
    {
        "paper_id": 1409.7802,
        "authors": "Baojun Bian, Harry Zheng",
        "title": "Turnpike Property and Convergence Rate for an Investment Model with\n  General Utility Functions",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we aim to address two questions faced by a long-term investor\nwith a power-type utility at high levels of wealth: one is whether the turnpike\nproperty still holds for a general utility that is not necessarily\ndifferentiable or strictly concave, the other is whether the error and the\nconvergence rate of the turnpike property can be estimated. We give positive\nanswers to both questions. To achieve these results, we first show that there\nis a classical solution to the HJB equation and give a representation of the\nsolution in terms of the dual function of the solution to the dual HJB\nequation. We demonstrate the usefulness of that representation with some\nnontrivial examples that would be difficult to solve with the trial and error\nmethod. We then combine the dual method and the partial differential equation\nmethod to give a direct proof to the turnpike property and to estimate the\nerror and the convergence rate of the optimal policy when the utility function\nis continuously differentiable and strictly concave. We finally relax the\nconditions of the utility function and provide some sufficient conditions that\nguarantee the turnpike property and the convergence rate in terms of both\nprimal and dual utility functions.\n"
    },
    {
        "paper_id": 1409.7933,
        "authors": "Lorenzo Mercuri, Edit Rroji",
        "title": "Parametric Risk Parity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Any optimization algorithm based on the risk parity approach requires the\nformulation of portfolio total risk in terms of marginal contributions. In this\npaper we use the independence of the underlying factors in the market to derive\nthe centered moments required in the risk decomposition process when the\nmodified versions of Value at Risk and Expected Shortfall are considered.\n  The choice of the Mixed Tempered Stable distribution seems adequate for\nfitting skewed and heavy tailed distributions. The ensuing detailed description\nof the optimization procedure is due to the existence of analytical higher\norder moments. Better results are achieved in terms of out of sample\nperformance and greater diversification.\n"
    },
    {
        "paper_id": 1409.796,
        "authors": "Erhan Bayraktar, Alexander Munk",
        "title": "An $\\alpha$-stable limit theorem under sublinear expectation",
        "comments": "Published at http://dx.doi.org/10.3150/15-BEJ737 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2016, Vol. 22, No. 4, 2548-2578",
        "doi": "10.3150/15-BEJ737",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For $\\alpha\\in (1,2)$, we present a generalized central limit theorem for\n$\\alpha$-stable random variables under sublinear expectation. The foundation of\nour proof is an interior regularity estimate for partial integro-differential\nequations (PIDEs). A classical generalized central limit theorem is recovered\nas a special case, provided a mild but natural additional condition holds. Our\napproach contrasts with previous arguments for the result in the linear setting\nwhich have typically relied upon tools that are non-existent in the sublinear\nframework, for example, characteristic functions.\n"
    },
    {
        "paper_id": 1409.8024,
        "authors": "Aleksejus Kononovicius, Vygintas Gontis",
        "title": "Herding interactions as an opportunity to prevent extreme events in\n  financial markets",
        "comments": "11 pages, 5 figures",
        "journal-ref": "The European Physical Journal B (2015) 88:189",
        "doi": "10.1140/epjb/e2015-60160-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A characteristic feature of complex systems in general is a tight coupling\nbetween their constituent parts. In complex socio-economic systems this kind of\nbehavior leads to self-organization, which may be both desirable (e.g. social\ncooperation) and undesirable (e.g. mass panic, financial \"bubbles\" or\n\"crashes\"). Abundance of the empirical data as well as general insights into\nthe trading behavior enables the creation of simple agent-based models\nreproducing sophisticated statistical features of the financial markets. In\nthis contribution we consider a possibility to prevent self-organized extreme\nevents in artificial financial market setup built upon a simple agent-based\nherding model. We show that introduction of agents with predefined\nfundamentalist trading behavior helps to significantly reduce the probability\nof the extreme price fluctuations events. We also test random trading control\nstrategy, which was previously found to be promising, and find that its impact\non the market is rather ambiguous. Though some of the results indicate that it\nmight actually stabilize financial fluctuations.\n"
    },
    {
        "paper_id": 1409.803,
        "authors": "Arnab Chatterjee",
        "title": "Socio-economic inequalities: a statistical physics perspective",
        "comments": "26 pages, 7 figs. review article. To be published in \"Econophysics\n  and Data Driven Modelling of Market Dynamics\", Eds. F Abergel, H. Aoyama,\n  B.K. Chakrabarti, A. Chakraborti, A. Ghosh, New Economic Windows, Springer\n  (2015)",
        "journal-ref": null,
        "doi": "10.1007/978-3-319-08473-2_12",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Socio-economic inequalities are manifested in different aspects of our social\nlife. We discuss various aspects, beginning with the evolutionary and\nhistorical origins, and discussing the major issues from the social and\neconomic point of view. The subject has attracted scholars from across various\ndisciplines, including physicists, who bring in a unique perspective to the\nfield. The major attempts to analyze the results, address the causes, and\nunderstand the origins using statistical tools and statistical physics concepts\nare discussed.\n"
    },
    {
        "paper_id": 1409.8037,
        "authors": "David Hobson and Yeqi Zhu",
        "title": "Multi-asset consumption-investment problems with infinite transaction\n  costs",
        "comments": "arXiv admin note: text overlap with arXiv:1409.3394",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The subject of this paper is an optimal consumption/optimal portfolio problem\nwith transaction costs and with multiple risky assets.\n  In our model the transaction costs take a special form in that transaction\ncosts on purchases of one of the risky assets (the endowed asset) are infinite,\nand transaction costs involving the other risky assets are zero. Effectively,\nthe endowed asset can only be sold. In general, multi-asset optional\nconsumption/optimal portfolio problems are very challenging, but the extra\nstructure we introduce allows us to make significant progress towards an\nanalytical solution.\n  For an agent with CRRA utility we completely characterise the different types\nof optimal behaviours. These include always selling the entire holdings of the\nendowed asset immediately, selling the endowed asset whenever the ratio of the\nvalue of the holdings of the endowed asset to other wealth gets above a\ncritical ratio, and selling the endowed asset only when other wealth is zero.\nThis characterisation is in terms of solutions of a boundary crossing problem\nfor a first order ODE. The technical contribution is to show that the problem\nof solving the HJB equation, which is a second order, non-linear PDE subject to\nsmooth fit at an unknown free boundary, can be reduced to this much simpler\nproblem involving an explicit first order ODE. This technical contribution is\nat the heart of our analytical and numerical results, and allows us to prove\nmonotonicity of the critical exercise threshold and the certainty equivalent\nvalue in the model parameters.\n"
    },
    {
        "paper_id": 1409.8119,
        "authors": "Darko Sarvan, Djordje Stratimirovic, Suzana Blesic, Vladimir Miljkovic",
        "title": "Scaling analysis of time series of daily prices from stock markets of\n  transitional economies in the Western Balkans",
        "comments": "2 tables, 4 figures, article",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2014-50655-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we have analyzed scaling properties of time series of stock\nmarket indices (SMIs) of developing economies of Western Balkans, and have\ncompared the results we have obtained with the results from more developed\neconomies. We have used three different techniques of data analysis to obtain\nand verify our findings: Detrended Fluctuation Analysis (DFA) method, Detrended\nMoving Average (DMA) method, and Wavelet Transformation (WT) analysis. We have\nfound scaling behavior in all SMI data sets that we have analyzed. The scaling\nof our SMI series changes from long-range correlated to slightly\nanti-correlated behavior with the change in growth or maturity of the economy\nthe stock market is embedded in. We also report the presence of effects of\npotential periodic-like influences on the SMI data that we have analyzed. One\nsuch influence is visible in all our SMI series, and appears at a period\n$T_{p}\\approx 90$ days. We propose that the existence of various periodic-like\ninfluences on SMI data may partially explain the observed difference in types\nof correlated behavior of corresponding scaling functions.\n"
    },
    {
        "paper_id": 1409.815,
        "authors": "Adam D. Bull",
        "title": "Near-optimal estimation of jump activity in semimartingales",
        "comments": "Published at http://dx.doi.org/10.1214/15-AOS1349 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 58-86",
        "doi": "10.1214/15-AOS1349",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In quantitative finance, we often model asset prices as semimartingales, with\ndrift, diffusion and jump components. The jump activity index measures the\nstrength of the jumps at high frequencies, and is of interest both in model\nselection and fitting, and in volatility estimation. In this paper, we give a\nnovel estimate of the jump activity, together with corresponding confidence\nintervals. Our estimate improves upon previous work, achieving near-optimal\nrates of convergence, and good finite-sample performance in Monte-Carlo\nexperiments.\n"
    },
    {
        "paper_id": 1409.8269,
        "authors": "H.R.N. van Erp, R.O. Linger, and P.H.A.J.M. van Gelder",
        "title": "Fact Sheet Research on Bayesian Decision Theory",
        "comments": "The first and third authors are affiliated with the Faculty of\n  Technology, Policy and Management, TU Delft. The second author is affiliated\n  with the Faculty of Clinical Epidemiology, RUG UMCG",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this fact sheet we give some preliminary research results on the Bayesian\nDecision Theory. This theory has been under construction for the past two\nyears. But what started as an intuitive enough idea, now seems to have the\nmakings of something more fundamental.\n"
    },
    {
        "paper_id": 1409.8321,
        "authors": "Jo\\~ao da Gama Batista, Jean-Philippe Bouchaud and Damien Challet",
        "title": "Sudden Trust Collapse in Networked Societies",
        "comments": "15 pages, 10 figures",
        "journal-ref": "Eur. Phys. J. B 88 (3) 55 (2015)",
        "doi": "10.1140/epjb/e2015-50645-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trust is a collective, self-fulfilling phenomenon that suggests analogies\nwith phase transitions. We introduce a stylized model for the build-up and\ncollapse of trust in networks, which generically displays a first order\ntransition. The basic assumption of our model is that whereas trust begets\ntrust, panic also begets panic, in the sense that a small decrease in trust may\nbe amplified and ultimately lead to a sudden and catastrophic drop of trust. We\nshow, using both numerical simulations and mean-field analytic arguments, that\nthere are extended regions of the parameter space where two equilibrium states\ncoexist: a well-connected network where confidence is high, and a poorly\nconnected network where confidence is low. In these coexistence regions,\nspontaneous jumps from the well-connected state to the poorly connected state\ncan occur, corresponding to a sudden collapse of trust that is not caused by\nany major external catastrophe. In large systems, spontaneous crises are\nreplaced by history dependence: whether the system is found in one state or in\nthe other essentially depends on initial conditions. Finally, we document a new\nphase, in which agents are connected yet distrustful.\n"
    },
    {
        "paper_id": 1409.8497,
        "authors": "Iacopo Mastromatteo",
        "title": "Apparent impact: the hidden cost of one-shot trades",
        "comments": "28 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of the execution of a moderate size order in an illiquid\nmarket within the framework of a solvable Markovian model. We suppose that in\norder to avoid impact costs, a trader decides to execute her order through a\nunique trade, waiting for enough liquidity to accumulate at the best quote. We\nfind that despite the absence of a proper price impact, such trader faces an\nexecution cost arising from a non-vanishing correlation among volume at the\nbest quotes and price changes. We characterize analytically the statistics of\nthe execution time and its cost by mapping the problem to the simpler one of\ncalculating a set of first-passage probabilities on a semi-infinite strip. We\nfinally argue that price impact cannot be completely avoided by conditioning\nthe execution of an order to a more favorable liquidity scenario.\n"
    },
    {
        "paper_id": 1409.8528,
        "authors": "S. Hokamp and G. Seibold",
        "title": "Tax Compliance and Public Goods Provision -- An Agent-based Econophysics\n  Approach",
        "comments": "28 pages, 3 figures, accepted for publication in the Central European\n  Journal of Economic Modelling and Econometrics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We calculate the dynamics of tax evasion within a multi-agent econophysics\nmodel which is adopted from the theory of magnetism and previously has been\nshown to capture the main characteristics from agent-based based models which\nbuild on the standard Allingham and Sandmo approach. In particular, we\nimplement a feedback of public goods provision on the decision-making of\nselfish agents which aim to pursue their self interest. Our results imply that\nsuch a feedback enhances the moral attitude of selfish agents thus reducing the\npercentage of tax evasion. Two parameters govern the behavior of selfish\nagents, (i) the rate of adaption to changes in public goods provision and (ii)\nthe threshold of perception of public goods provision. Furtheron we analyze the\ntax evasion dynamics for different agent co mpositions and under the feedback\nof public goods provision. We conclude that policymakers may enhance tax\ncompliance behavior via the threshold of perception by means of targeted public\nrelations.\n"
    },
    {
        "paper_id": 1409.8609,
        "authors": "Pawe{\\l} Fiedor, Artur Ho{\\l}da",
        "title": "Time Evolution of Non-linear Currency Networks",
        "comments": "16 pages, 4 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1304.7717 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are complex adaptive systems, and are commonly studied as\ncomplex networks. Most of such studies fall short in two respects: they do not\naccount for non-linearity of the studied relationships, and they create one\nnetwork for the whole studied time series, providing an average picture of a\nvery long, economically non-homogeneous, period. In this study we look at the\ncurrency markets by creating networks which can account for non-linearity in\nthe underlying relationships, and are based on short time horizons with the use\nof running window approach. Since information--theoretic measures are slow to\nconverge, we use Hirschfeld-Gebelein-Renyi Maximum Correlation Coefficient as a\nmeasure of the relationships between currencies. We use the Randomized\nDependence Coefficient (RDC) as an estimator of the above. It measures the\ndependence between random samples as the largest canonical correlation between\nk randomly chosen non-linear projections of their copula transformations. On\nthis basis we create full graphs, and further filter them into minimally\nspanning trees. We create such networks for each window moving along the\nstudied time series, and analyse the time evolution of various network\ncharacteristics, particularly the degree distributions, and their economic\nsignificance. We apply this procedure to a dataset describing logarithmic\nchanges in exchange rates in relation to silver for 27 world currencies for the\nyears between 2002 and 2013.\n"
    },
    {
        "paper_id": 1410.0104,
        "authors": "Nima Dehmamy, Sergey V. Buldyrev, Shlomo Havlin, H. Eugene Stanley,\n  Irena Vodenska",
        "title": "Classical mechanics of economic networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial networks are dynamic. To assess their systemic importance to the\nworld-wide economic network and avert losses we need models that take the time\nvariations of the links and nodes into account. Using the methodology of\nclassical mechanics and Laplacian determinism we develop a model that can\npredict the response of the financial network to a shock. We also propose a way\nof measuring the systemic importance of the banks, which we call BankRank.\nUsing European Bank Authority 2011 stress test exposure data, we apply our\nmodel to the bipartite network connecting the largest institutional debt\nholders of the troubled European countries (Greece, Italy, Portugal, Spain, and\nIreland). From simulating our model we can determine whether a network is in a\n\"stable\" state in which shocks do not cause major losses, or a \"unstable\" state\nin which devastating damages occur. Fitting the parameters of the model, which\nplay the role of physical coupling constants, to Eurozone crisis data shows\nthat before the Eurozone crisis the system was mostly in a \"stable\" regime, and\nthat during the crisis it transitioned into an \"unstable\" regime. The numerical\nsolutions produced by our model match closely the actual time-line of events of\nthe crisis. We also find that, while the largest holders are usually more\nimportant, in the unstable regime smaller holders also exhibit systemic\nimportance. Our model also proves useful for determining the vulnerability of\nbanks and assets to shocks. This suggests that our model may be a useful tool\nfor simulating the response dynamics of shared portfolio networks.\n"
    },
    {
        "paper_id": 1410.0112,
        "authors": "Jir\\^o Akahori, Nien-Lin Liu, Maria Elvira Mancino and Yukie Yasuda",
        "title": "The Fourier estimation method with positive semi-definite estimators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a slight modification of the Fourier estimation\nmethod of the spot volatility (matrix) process of a continuous It\\^o\nsemimartingale where the estimators are always non-negative definite. Since the\nestimators are factorized, computational cost will be saved a lot.\n"
    },
    {
        "paper_id": 1410.0125,
        "authors": "A.V. Leonidov, E.L. Rumyantsev",
        "title": "Systemic Interbank Network Risks in Russia",
        "comments": "Based on the talk given by A. Leonidov at the workshop \"Random Graphs\n  and Their Applications\", Moscow, October 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modelling of contagion in interbank networks is discussed. A model taking\ninto account bow-tie structure and dissasortativity of interbank networks is\ndeveloped. The model is shown to provide a good quantitative description of the\nRussian interbank market. Detailed arguments favoring the non-percolative\nnature of contagion-related risks in the Russian interbank market are given.\n"
    },
    {
        "paper_id": 1410.0249,
        "authors": "Emanuele Pugliese, Andrea Zaccaria, and Luciano Pietronero",
        "title": "On the convergence of the Fitness-Complexity Algorithm",
        "comments": "13 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1140/epjst/e2015-50118-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the convergence properties of an algorithm which has been\nrecently proposed to measure the competitiveness of countries and the quality\nof their exported products. These quantities are called respectively Fitness F\nand Complexity Q. The algorithm was originally based on the adjacency matrix M\nof the bipartite network connecting countries with the products they export,\nbut can be applied to any bipartite network. The structure of the adjacency\nmatrix turns to be essential to determine which countries and products converge\nto non zero values of F and Q. Also the speed of convergence to zero depends on\nthe matrix structure. A major role is played by the shape of the ordered matrix\nand, in particular, only those matrices whose diagonal does not cross the empty\npart are guaranteed to have non zero values as outputs when the algorithm\nreaches the fixed point. We prove this result analytically for simplified\nstructures of the matrix, and numerically for real cases. Finally, we propose\nsome practical indications to take into account our results when the algorithm\nis applied.\n"
    },
    {
        "paper_id": 1410.0384,
        "authors": "Scott Robertson, Konstantinos Spiliopoulos",
        "title": "Indifference pricing for Contingent Claims: Large Deviations Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study utility indifference prices and optimal purchasing quantities for a\nnon-traded contingent claim in an incomplete semi-martingale market with\nvanishing hedging errors. We make connections with the theory of large\ndeviations. We concentrate on sequences of semi-complete markets where in the\n$n^{th}$ market, the claim $B_n$ admits the decomposition $B_n = D_n+Y_n$.\nHere, $D_n$ is replicable by trading in the underlying assets $S_n$, but $Y_n$\nis independent of $S_n$. Under broad conditions, we may assume that $Y_n$\nvanishes in accordance with a large deviations principle as $n$ grows. In this\nsetting, for an exponential investor, we identify the limit of the average\nindifference price $p_n(q_n)$, for $q_n$ units of $B_n$, as $n\\rightarrow\n\\infty$. We show that if $|q_n|\\rightarrow\\infty$, the limiting price typically\ndiffers from the price obtained by assuming bounded positions\n$\\sup_n|q_n|<\\infty$, and the difference is explicitly identifiable using large\ndeviations theory. Furthermore, we show that optimal purchase quantities occur\nat the large deviations scaling, and hence large positions arise endogenously\nin this setting.\n"
    },
    {
        "paper_id": 1410.0448,
        "authors": "Tianyang Nie, Marek Rutkowski",
        "title": "Fair and profitable bilateral prices under funding costs and\n  collateralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bielecki and Rutkowski (2014) introduced and studied a generic nonlinear\nmarket model, which includes several risky assets, multiple funding accounts\nand margin accounts. In this paper, we examine the pricing and hedging of\ncontract both from the perspective of the hedger and the counterparty with\narbitrary initial endowments. We derive inequalities for unilateral prices and\nwe give the range for either fair bilateral prices or bilaterally profitable\nprices. We also study the monotonicity of a unilateral price with respect to\nthe initial endowment. Our study hinges on results for BSDE driven by\ncontinuous martingales obtained in Nie and Rutkowski (2014), but we also derive\nthe pricing PDEs for path-independent contingent claims of European style in a\nMarkovian framework.\n"
    },
    {
        "paper_id": 1410.0594,
        "authors": "Giovanni Mottola",
        "title": "Generalized Dynkin game of switching type representation for defaultable\n  claims in presence of contingent CSA",
        "comments": "This paper version has been replaced by the author due to some\n  misprints and typo in the old version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the solution's existence for a generalized Dynkin game of switching\ntype which is shown to be the natural representation for general defaultable\nOTC contract with contingent CSA. This is a theoretical counterparty risk\nmitigation mechanism that allows the counterparty of a general OTC contract to\nswitch from zero to full/perfect collateralization and switch back whenever she\nwants until contract maturity paying some switching costs and taking into\naccount the running costs that emerge over time. In this paper we allow for the\nstrategic interaction between the counterparties of the underlying contract,\nwhich makes the problem solution much more tough. We are motivated in this\nresearch by the importance to show the economic sense - in terms of optimal\ncontract design - of a contingent counterparty risk mitigation mechanism like\nour one. In particular, we show that the existence of the solution and the game\nNash equilibrium is connected with the solution of a system of non-linear\nreflected BSDE which remains an open problem. We then provide the basic ideas\nto numerically search the game equilibrium via an iterative optimal stopping\napproach and we show the existence of the solution for our problem under strong\ncondition, in the so called symmetric case.\n"
    },
    {
        "paper_id": 1410.0673,
        "authors": "Tianyang Nie, Marek Rutkowski",
        "title": "Fair bilateral prices in Bergman's model",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1410.0448",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bielecki and Rutkowski (2014) introduced and studied a generic nonlinear\nmarket model, which includes several risky assets, multiple funding accounts\nand margin accounts. In this paper, we examine the pricing and hedging of\ncontract both from the perspective of the hedger and the counterparty with\narbitrary initial endowments. We derive inequalities for unilateral prices and\nwe give the range for either fair bilateral prices or bilaterally profitable\nprices. We also study the monotonicity of a unilateral price with respect to\nthe initial endowment. Our study hinges on results for BSDE driven by\ncontinuous martingales obtained in Nie and Rutkowski (2014), but we also derive\nthe pricing PDEs for path-independent contingent claims of European style in a\nMarkovian framework.\n"
    },
    {
        "paper_id": 1410.0852,
        "authors": "Raphael Hauser, Sergey Shahverdyan and Paul Embrechts",
        "title": "A General Duality Relation with Applications in Quantitative Risk\n  Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A fundamental problem in risk management is the robust aggregation of\ndifferent sources of risk in a situation where little or no data are available\nto infer information about their dependencies. A popular approach to solving\nthis problem is to formulate an optimization problem under which one maximizes\na risk measure over all multivariate distributions that are consistent with the\navailable data. In several special cases of such models, there exist dual\nproblems that are easier to solve or approximate, yielding robust bounds on the\naggregated risk. In this chapter we formulate a general optimization problem,\nwhich can be seen as a doubly infinite linear programming problem, and we show\nthat the associated dual generalizes several well known special cases and\nextends to new risk management models we propose.\n"
    },
    {
        "paper_id": 1410.0915,
        "authors": "Kim Weston",
        "title": "Stability of Utility Maximization in Nonequivalent Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stability of the utility maximization problem with random endowment and\nindifference prices is studied for a sequence of financial markets in an\nincomplete Brownian setting. Our novelty lies in the nonequivalence of markets,\nin which the volatility of asset prices (as well as the drift) varies.\nDegeneracies arise from the presence of nonequivalence. In the positive real\nline utility framework, a counterexample is presented showing that the expected\nutility maximization problem can be unstable. A positive stability result is\nproven for utility functions on the entire real line.\n"
    },
    {
        "paper_id": 1410.0946,
        "authors": "Kasper Larsen, Oleksii Mostovyi, Gordan \\v{Z}itkovi\\'c",
        "title": "An expansion in the model space in the context of utility maximization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the framework of an incomplete financial market where the stock price\ndynamics are modeled by a continuous semimartingale (not necessarily Markovian)\nan explicit second-order expansion formula for the power investor's value\nfunction - seen as a function of the underlying market price of risk process -\nis provided. This allows us to provide first-order approximations of the\noptimal primal and dual controls. Two specific calibrated numerical examples\nillustrating the accuracy of the method are also given.\n"
    },
    {
        "paper_id": 1410.0991,
        "authors": "Wanyang Dai",
        "title": "Mean-variance hedging based on an incomplete market with external risk\n  factors of non-Gaussian OU processes",
        "comments": "36 pages",
        "journal-ref": "Mathematical Problems in Engineering, Volume 2015 (Regular Issue),\n  Article ID 625289, 20 pages, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we prove the global risk optimality of the hedging strategy of\ncontingent claim, which is explicitly (or called semi-explicitly) constructed\nfor an incomplete financial market with external risk factors of non-Gaussian\nOrnstein-Uhlenbeck (NGOU) processes. Analytical and numerical examples are both\npresented to illustrate the effectiveness of our optimal strategy. Our study\nestablishes the connection between our financial system and existing general\nsemimartingale based discussions by justifying required conditions. More\nprecisely, there are three steps involved. First, we firmly prove the\nno-arbitrage condition to be true for our financial market, which is used as an\nassumption in existing discussions. In doing so, we explicitly construct the\nsquare-integrable density process of the variance-optimal martingale measure\n(VOMM). Second, we derive a backward stochastic differential equation (BSDE)\nwith jumps for the mean-value process of a given contingent claim. The unique\nexistence of adapted strong solution to the BSDE is proved under suitable\nterminal conditions including both European call and put options as special\ncases. Third, by combining the solution of the BSDE and the VOMM, we reach the\njustification of the global risk optimality for our hedging strategy.\n"
    },
    {
        "paper_id": 1410.1101,
        "authors": "Rodrigo S. Targino, Gareth W. Peters, Pavel V. Shevchenko",
        "title": "Sequential Monte Carlo Samplers for capital allocation under\n  copula-dependent risk models",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics 61 (2015) 206-226",
        "doi": "10.1016/j.insmatheco.2015.01.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we assume a multivariate risk model has been developed for a\nportfolio and its capital derived as a homogeneous risk measure. The Euler (or\ngradient) principle, then, states that the capital to be allocated to each\ncomponent of the portfolio has to be calculated as an expectation conditional\nto a rare event, which can be challenging to evaluate in practice. We exploit\nthe copula-dependence within the portfolio risks to design a Sequential Monte\nCarlo Samplers based estimate to the marginal conditional expectations involved\nin the problem, showing its efficiency through a series of computational\nexamples.\n"
    },
    {
        "paper_id": 1410.1136,
        "authors": "Vladimir Dombrovskii, Tatyana Obyedko",
        "title": "Dynamic Investment Portfolio Optimization under Constraints in the\n  Financial Market with Regime Switching using Model Predictive Control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we consider the optimal portfolio selection problem under hard\nconstraints on trading volume amounts when the dynamics of the risky asset\nreturns are governed by a discrete-time approximation of the Markov-modulated\ngeometric Brownian motion. The states of Markov chain are interpreted as the\nstates of an economy. The problem is stated as a dynamic tracking problem of a\nreference portfolio with desired return. We propose to use the model predictive\ncontrol (MPC) methodology in order to obtain feedback trading strategies. Our\napproach is tested on a set of a real data from the radically different\nfinancial markets: the Russian Stock Exchange MICEX, the New York Stock\nExchange and the Foreign Exchange Market (FOREX).\n"
    },
    {
        "paper_id": 1410.122,
        "authors": "Cody Hyndman and Xinghua Zhou",
        "title": "Explicit solutions of quadratic FBSDEs arising from quadratic term\n  structure models",
        "comments": "23 pages, minor revisions (typos, restatement of main results to rely\n  on additional lemmas)",
        "journal-ref": null,
        "doi": "10.1080/07362994.2015.1009548",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide explicit solutions of certain forward-backward stochastic\ndifferential equations (FBSDEs) with quadratic growth. These particular FBSDEs\nare associated with quadratic term structure models of interest rates and\ncharacterize the zero-coupon bond price. The results of this paper are\nnaturally related to similar results on affine term structure models of Hyndman\n(Math. Financ. Econ. 2(2):107-128, 2009) due to the relationship between\nquadratic functionals of Gaussian processes and linear functionals of affine\nprocesses. Similar to the affine case a sufficient condition for the explicit\nsolutions to hold is the solvability in a fixed interval of Riccati-type\nordinary differential equations. However, in contrast to the affine case, these\nRiccati equations are easily associated with those occurring in\nlinear-quadratic control problems. We also consider quadratic models for a\nrisky asset price and characterize the futures price and forward price of the\nasset in terms of similar FBSDEs. An example is considered, using an approach\nbased on stochastic flows that is related to the FBSDE approach, to further\nemphasize the parallels between the affine and quadratic models. An appendix\ndiscusses solvability and explicit solutions of the Riccati equations.\n"
    },
    {
        "paper_id": 1410.1287,
        "authors": "K. Gad, J. L. Pedersen",
        "title": "Rationality parameter for exercising American put",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main result of this paper is a probabilistic proof of the penalty method\nfor approximating the price of an American put in the Black-Scholes market. The\nmethod gives a parametrized family of partial differential equations, and by\nvarying the parameter the corresponding solutions converge to the price of an\nAmerican put. For each PDE the parameter may be interpreted as a rationality\nparameter of the holder of the option. The method may be extended to other\nvaluation situations given as an optimal stopping problem with no explicit\nsolution. The method may also be used for valuations where actors do not behave\ncompletely rationally but instead have randomness affecting their choices. The\nrationality parameter is a measure for this randomness.\n"
    },
    {
        "paper_id": 1410.1426,
        "authors": "Jarno Talponen",
        "title": "On volatility smile and an investment strategy with out-of-the-money\n  calls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A motivating question in this paper is whether a sensible investment strategy\nmay systematically contain long positions in out-of-the-money European calls\nwith short expiry. Here we consider a very simple trading strategy for calls.\nThe main points of this note are the following. First, the presented trading\nstrategy appears very lucrative in the Black-Scholes-Merton (BSM) framework. In\nfact, it is such even to the extent that the BSM model turns out to be, in a\nsense, incompatible with the CAPM. Second, if one wishes to adapt these models\ntogether, then the adjustment of the consistent pricing rule (i.e. modifying\nstate price densities) inevitably leads to some form of volatility smile and\nthis is the main point of the paper. Moreover, these observations arise from\npurely structural considerations.\n"
    },
    {
        "paper_id": 1410.1481,
        "authors": "Olivier Gu\\'eant",
        "title": "Optimal execution of ASR contracts with fixed notional",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Be it for taking advantage of stock undervaluation or in order to distribute\npart of their profits to shareholders, firms may buy back their own shares. One\nof the way they proceed is by including Accelerated Share Repurchases (ASR) as\npart of their repurchase programs. In this article, we study the pricing and\noptimal execution strategy of an ASR contract with fixed notional. In such a\ncontract the firm pays a fixed notional $F$ to the bank and receives, in\nexchange, a number of shares corresponding to the ratio between $F$ and the\naverage stock price over the purchase period, the duration of this period being\ndecided upon by the bank. From a mathematical point of view, the problem is\nrelated to both optimal execution and exotic option pricing.\n"
    },
    {
        "paper_id": 1410.1611,
        "authors": "Zura Kakushadze",
        "title": "Path Integral and Asset Pricing",
        "comments": "30 pages; a few trivial typos corrected, references updated, no other\n  changes",
        "journal-ref": "Quantitative Finance 15(11) (2015) 1759-1771, Featured Article",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a pragmatic/pedagogical discussion of using Euclidean path integral\nin asset pricing. We then illustrate the path integral approach on short-rate\nmodels. By understanding the change of path integral measure in the\nVasicek/Hull-White model, we can apply the same techniques to \"less-tractable\"\nmodels such as the Black-Karasinski model. We give explicit formulas for\ncomputing the bond pricing function in such models in the analog of quantum\nmechanical \"semiclassical\" approximation. We also outline how to apply\nperturbative quantum mechanical techniques beyond the \"semiclassical\"\napproximation, which are facilitated by Feynman diagrams.\n"
    },
    {
        "paper_id": 1410.1664,
        "authors": "Kaj Nystr\\\"om and Mikko Parviainen",
        "title": "Tug-of-war, market manipulation and option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an option pricing model based on a tug-of-war game. This\ntwo-player zero-sum stochastic differential game is formulated in the context\nof a multi-dimensional financial market. The issuer and the holder try to\nmanipulate asset price processes in order to minimize and maximize the expected\ndiscounted reward. We prove that the game has a value and that the value\nfunction is the unique viscosity solution to a terminal value problem for a\nparabolic partial differential equation involving the non-linear and completely\ndegenerate infinity Laplace operator.\n"
    },
    {
        "paper_id": 1410.2034,
        "authors": "Damiano Brigo, Cyril Durand",
        "title": "An initial approach to Risk Management of Funding Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we sketch an initial tentative approach to funding costs\nanalysis and management for contracts with bilateral counterparty risk in a\nsimplified setting. We depart from the existing literature by analyzing the\nissue of funding costs and benefits under the assumption that the associated\nrisks cannot be hedged properly. We also model the treasury funding spread by\nmeans of a stochastic Weighted Cost of Funding Spread (WCFS) which helps\ndescribing more realistic financing policies of a financial institution. We\nelaborate on some limitations in replication-based Funding / Credit Valuation\nAdjustments we worked on ourselves in the past, namely CVA, DVA, FVA and\nrelated quantities as generally discussed in the industry. We advocate as a\ndifferent possibility, when replication is not possible, the analysis of the\nfunding profit and loss distribution and explain how long term funding spreads,\nwrong way risk and systemic risk are generally overlooked in most of the\ncurrent literature on risk measurement of funding costs. As a matter of initial\nillustration, we discuss in detail the funding management of interest rate\nswaps with bilateral counterparty risk in the simplified setup of our framework\nthrough numerical examples and via a few simplified assumptions.\n"
    },
    {
        "paper_id": 1410.2121,
        "authors": "Giulio Cimini, Tiziano Squartini, Nicol\\`o Musmeci, Michelangelo\n  Puliga, Andrea Gabrielli, Diego Garlaschelli, Stefano Battiston, Guido\n  Caldarelli",
        "title": "Reconstructing topological properties of complex networks using the\n  fitness model",
        "comments": null,
        "journal-ref": "Social Informatics (series: Lec. Notes Comp. Science 8852/2015),\n  pp. 323-333, Springer (edited by L. M. Aiello and D. McFarland) (2015)",
        "doi": "10.1007/978-3-319-15168-7_41",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major problem in the study of complex socioeconomic systems is represented\nby privacy issues$-$that can put severe limitations on the amount of accessible\ninformation, forcing to build models on the basis of incomplete knowledge. In\nthis paper we investigate a novel method to reconstruct global topological\nproperties of a complex network starting from limited information. This method\nuses the knowledge of an intrinsic property of the nodes (indicated as\nfitness), and the number of connections of only a limited subset of nodes, in\norder to generate an ensemble of exponential random graphs that are\nrepresentative of the real systems and that can be used to estimate its\ntopological properties. Here we focus in particular on reconstructing the most\nbasic properties that are commonly used to describe a network: density of\nlinks, assortativity, clustering. We test the method on both benchmark\nsynthetic networks and real economic and financial systems, finding a\nremarkable robustness with respect to the number of nodes used for calibration.\nThe method thus represents a valuable tool for gaining insights on\nprivacy-protected systems.\n"
    },
    {
        "paper_id": 1410.2282,
        "authors": "Hyungbin Park",
        "title": "Ross Recovery with Recurrent and Transient Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, Ross showed that it is possible to recover an objective measure\nfrom a risk-neutral measure. His model assumes that there is a finite-state\nMarkov process X that drives the economy in discrete time. Many authors\nextended his model to a continuous-time setting with a Markov diffusion process\nX with state space R. Unfortunately, the continuous-time model fails to recover\nan objective measure from a risk-neutral measure. We determine under which\ninformation recovery is possible in the continuous-time model. It was proven\nthat if X is recurrent under the objective measure, then recovery is possible.\nIn this article, when X is transient under the objective measure, we\ninvestigate what information is sufficient to recover.\n"
    },
    {
        "paper_id": 1410.2549,
        "authors": "Vanessa Hoffmann de Quadros, Juan Carlos Gonz\\'alez-Avella and Jos\\'e\n  Roberto Iglesias",
        "title": "Propagation of Systemic Risk in Interbank Networks",
        "comments": "20 pages and 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work explores the characteristics of financial contagion in networks\nwhose links distributions approaches a power law, using a model that defines\nbanks balance sheets from information of network connectivity. By varying the\nparameters for the creation of the network, several interbank networks are\nbuilt, in which the concentrations of debts and credits are obtained from links\ndistributions during the creation networks process. Three main types of\ninterbank network are analyzed for their resilience to contagion: i)\nconcentration of debts is greater than concentration of credits, ii)\nconcentration of credits is greater than concentration of debts and iii)\nconcentrations of debts and credits are similar. We also tested the effect of a\nvariation in connectivity in conjunction with variation in concentration of\nlinks. The results suggest that more connected networks with high concentration\nof credits (featuring nodes that are large creditors of the system) present\ngreater resilience to contagion when compared with the others networks\nanalyzed. Evaluating some topological indices of systemic risk suggested by the\nliterature we have verified the ability of these indices to explain the impact\non the system caused by the failure of a node. There is a clear positive\ncorrelation between the topological indices and the magnitude of losses in the\ncase of networks with high concentration of debts. This correlation is smaller\nfor more resilient networks.\n"
    },
    {
        "paper_id": 1410.255,
        "authors": "Jorgen Vitting Andersen, Ioannis Vrontos, Petros Dellaportas and Serge\n  Galam",
        "title": "Communication impacting financial markets",
        "comments": "8 pages, 3 figures, to appear in EPL",
        "journal-ref": null,
        "doi": "10.1209/0295-5075/108/28007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Behavioral finance has become an increasingly important subfield of finance.\nHowever the main parts of behavioral finance, prospect theory included,\nunderstand financial markets through individual investment behavior. Behavioral\nfinance thereby ignores any interaction between participants. We introduce a\nsocio-financial model that studies the impact of communication on the pricing\nin financial markets. Considering the simplest possible case where each market\nparticipant has either a positive (bullish) or negative (bearish) sentiment\nwith respect to the market, we model the evolution of the sentiment in the\npopulation due to communication in subgroups of different sizes. Nonlinear\nfeedback effects between the market performance and changes in sentiments are\ntaking into account by assuming that the market performance is dependent on\nchanges in sentiments (e.g. a large sudden positive change in bullishness would\nlead to more buying). The market performance in turn has an impact on the\nsentiment through the transition probabilities to change an opinion in a group\nof a given size. The idea is that if for example the market has observed a\nrecent downturn, it will be easier for even a bearish minority to convince a\nbullish majority to change opinion compared to the case where the meeting takes\nplace in a bullish upturn of the market. Within the framework of our proposed\nmodel, financial markets stylized facts such as volatility clustering and\nextreme events may be perceived as arising due to abrupt sentiment changes via\nongoing communication of the market participants. The model introduces a new\nvolatility measure which is apt of capturing volatility clustering and from\nmaximum likelihood analysis we are able to apply the model to real data and\ngive additional long term insight into where a market is heading.\n"
    },
    {
        "paper_id": 1410.257,
        "authors": "Zhang Li, Xiaojun Lin, Borja Peleato-Inarrea, and Ilya Pollak",
        "title": "Optimal Monitoring and Mitigation of Systemic Risk in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the problem of optimally allocating a cash injection into\na financial system in distress. Given a one-period borrower-lender network in\nwhich all debts are due at the same time and have the same seniority, we\naddress the problem of allocating a fixed amount of cash among the nodes to\nminimize the weighted sum of unpaid liabilities. Assuming all the loan amounts\nand asset values are fixed and that there are no bankruptcy costs, we show that\nthis problem is equivalent to a linear program. We develop a duality-based\ndistributed algorithm to solve it which is useful for applications where it is\ndesirable to avoid centralized data gathering and computation. We also consider\nthe problem of minimizing the expectation of the weighted sum of unpaid\nliabilities under the assumption that the net external asset holdings of all\ninstitutions are stochastic. We show that this problem is a two-stage\nstochastic linear program. To solve it, we develop two algorithms based on:\nBenders decomposition algorithm and projected stochastic gradient descent. We\nshow that if the defaulting nodes never pay anything, the deterministic optimal\ncash injection allocation problem is an NP-hard mixed-integer linear program.\nHowever, modern optimization software enables the computation of very accurate\nsolutions to this problem on a personal computer in a few seconds for network\nsizes comparable with the size of the US banking system. In addition, we\naddress the problem of allocating the cash injection amount so as to minimize\nthe number of nodes in default. For this problem, we develop two heuristic\nalgorithms: a reweighted l1 minimization algorithm and a greedy algorithm. We\nillustrate these two algorithms using three synthetic network structures for\nwhich the optimal solution can be calculated exactly. We also compare these two\nalgorithms on three types random networks which are more complex.\n"
    },
    {
        "paper_id": 1410.289,
        "authors": "Tariq Ahmad Mir, Marcel Ausloos, Roy Cerqueti",
        "title": "Benford's law predicted digit distribution of aggregated income taxes:\n  the surprising conformity of Italian cities and regions",
        "comments": "18 pages, 5 tables, 4 figures, 61 references, To appear in European\n  Physical Journal B",
        "journal-ref": "Eur. Phys. J. B (2014) 87: 261",
        "doi": "10.1140/epjb/e2014-50525-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The yearly aggregated tax income data of all, more than 8000, Italian\nmunicipalities are analyzed for a period of five years, from 2007 to 2011, to\nsearch for conformity or not with Benford's law, a counter-intuitive phenomenon\nobserved in large tabulated data where the occurrence of numbers having smaller\ninitial digits is more favored than those with larger digits. This is done in\nanticipation that large deviations from Benford's law will be found in view of\ntax evasion supposedly being widespread across Italy. Contrary to expectations,\nwe show that the overall tax income data for all these years is in excellent\nagreement with Benford's law. Furthermore, we also analyze the data of\nCalabria, Campania and Sicily, the three Italian regions known for strong\npresence of mafia, to see if there are any marked deviations from Benford's\nlaw. Again, we find that all yearly data sets for Calabria and Sicily agree\nwith Benford's law whereas only the 2007 and 2008 yearly data show departures\nfrom the law for Campania. These results are again surprising in view of\nunderground and illegal nature of economic activities of mafia which\nsignificantly contribute to tax evasion. Some hypothesis for the found\nconformity is presented.\n"
    },
    {
        "paper_id": 1410.2976,
        "authors": "Michael R. Tehranchi",
        "title": "Arbitrage theory without a num\\'eraire",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note develops an arbitrage theory for a discrete-time market model\nwithout the assumption of the existence of a num\\'eraire asset. Fundamental\ntheorems of asset pricing are stated and proven in this context. The\ndistinction between the notions of investment-consumption arbitrage and\npure-investment arbitrage provide a discrete-time analogue of the distinction\nbetween the notions of absolute arbitrage and relative arbitrage in the\ncontinuous-time theory. Applications to the modelling of bubbles is discussed.\n"
    },
    {
        "paper_id": 1410.3128,
        "authors": "Elvis Oltean, Fedor Kusmartsev",
        "title": "A study of Methods from Statistical Mechanics applied to income\n  distribution",
        "comments": "Fermi-Dirac distribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We applied Dirac distribution, Bose-Einstein distribution, and occasionally\nBoltzmann-Gibbs distribution in order to determine which is optimal for income\ndistribution on a large pool of countries. The best fit to the data was\nobserved in the case of Fermi-Dirac distribution, for which the coefficient of\ndetermination showed the best goodness of fit to the data. Using this\ndistribution for data (spun throughout more years), we obtained the underlying\ncritical parameters of annual income distribution such as chemical potential\nand temperature. The next step was to explore the evolution of income using\neconomic analogues to chemical potential and temperature. Using as background\nthe analogy made by Yakovenko between temperature from thermodynamic systems\nand nominal income from Economics, we found other analogies that would allow\nfurther analysis and explanation of income.\n"
    },
    {
        "paper_id": 1410.3394,
        "authors": "Jim Gatheral, Thibault Jaisson and Mathieu Rosenbaum",
        "title": "Volatility is rough",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating volatility from recent high frequency data, we revisit the\nquestion of the smoothness of the volatility process. Our main result is that\nlog-volatility behaves essentially as a fractional Brownian motion with Hurst\nexponent H of order 0.1, at any reasonable time scale. This leads us to adopt\nthe fractional stochastic volatility (FSV) model of Comte and Renault. We call\nour model Rough FSV (RFSV) to underline that, in contrast to FSV, H<1/2. We\ndemonstrate that our RFSV model is remarkably consistent with financial time\nseries data; one application is that it enables us to obtain improved forecasts\nof realized volatility. Furthermore, we find that although volatility is not\nlong memory in the RFSV model, classical statistical procedures aiming at\ndetecting volatility persistence tend to conclude the presence of long memory\nin data generated from it. This sheds light on why long memory of volatility\nhas been widely accepted as a stylized fact. Finally, we provide a quantitative\nmarket microstructure-based foundation for our findings, relating the roughness\nof volatility to high frequency trading and order splitting.\n"
    },
    {
        "paper_id": 1410.3793,
        "authors": "Camilo Hernandez and Mauricio Junca",
        "title": "Optimal dividend payment under time of ruin contraint: Exponential case",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2015.09.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the classical optimal dividends problem under the\nCram\\'er-Lundberg model with exponential claim sizes subject to a constraint on\nthe time of ruin. We introduce the dual problem and show that the complementary\nslackness conditions are satisfied, thus there is no duality gap. Therefore the\noptimal value function can be obtained as the point-wise infimum of auxiliary\nvalue functions indexed by Lagrange multipliers. We also present a series of\nnumerical examples.\n"
    },
    {
        "paper_id": 1410.3811,
        "authors": "Elvis Oltean, Fedor V. Kusmartsev",
        "title": "Applications of statistical physics distributions to several types of\n  income",
        "comments": "Fermi-Dirac distribution, polynomial distribution, income\n  distribution (other than disposable income)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores several types of income which have not been explored so\nfar by authors who tackled income and wealth distribution using Statistical\nPhysics. The main types of income we plan to analyze are income before\nredistribution (or gross income), income of retired people (or pensions), and\nincome of active people (mostly wages). The distributions used to analyze\nincome distributions are Fermi-Dirac distribution and polynomial distribution\n(as this is present in describing the behavior of dynamic systems in certain\naspects). The data we utilize for our analysis are from France and the UK. We\nfind that both distributions are robust in describing these varieties of\nincome. The main finding we consider to be the applicability of these\ndistributions to pensions, which are not regulated entirely by market\nmechanisms.\n"
    },
    {
        "paper_id": 1410.3851,
        "authors": "Elvis Oltean, Fedor Kusmartsev",
        "title": "An Econophysical dynamical approach of expenditure and income\n  distribution in the UK",
        "comments": "polynomial distribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the exploration regarding dynamical approach of macroeconomic\nvariables by tackling systematically expenditure using Statistical Physics\nmodels (for the first time to the best of our knowledge). Also, using\npolynomial distribution which characterizes the behavior of dynamical systems\nin certain situations, we extend also our analysis to mean income data from the\nUK that span for a time interval of 35 years. We find that most of the values\nfor coefficient of determination obtained from fitting the data from\nconsecutive years analysis to be above 80%. We used for our analysis first\ndegree polynomial, but higher degree polynomials and longer time intervals\nbetween the years considered can dramatically increase goodness of the fit. As\nthis methodology was applied successfully to income and wealth, we can conclude\nthat macroeconomic systems can be treated similarly to dynamic systems from\nPhysics. Subsequently, the analysis could be extended to other macroeconomic\nindicators.\n"
    },
    {
        "paper_id": 1410.386,
        "authors": "Elvis Oltean",
        "title": "An econophysical approach of polynomial distribution applied to income\n  and expenditure",
        "comments": "polynomial distribution applied to income and expenditure\n  distribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Polynomial distribution can be applied to dynamical systems in certain\nsituations. Macroeconomic systems characterized by economic variables such as\nincome and wealth can be modelled similarly using polynomials. We extend our\nprevious work to data regarding income from a more diversified pool of\ncountries, which contains developed countries with high income, developed\ncountries with middle income, developing and underdeveloped countries. Also,\nfor the first time we look at the applicability of polynomial distribution to\nexpenditure (consumption). Using cumulative distribution function, we found\nthat polynomials are applicable with a high degree of success to the\ndistribution of income to all countries considered without significant\ndifferences. Moreover, expenditure data can be fitted very well by this\npolynomial distribution. We considered a distribution to be robust if the\nvalues for coefficient of determination are higher than 90%. Using this\ncriterion, we decided the degree for the polynomials used in our analysis by\ntrying to minimize the number of coefficients, respectively first or second\ndegree. Lastly, we look at possible correlation between the values from\ncoefficient of determination and Gini coefficient for disposable income.\n"
    },
    {
        "paper_id": 1410.3865,
        "authors": "Elvis Oltean, Fedor Kusmartsev",
        "title": "A statistical physics analysis of expenditure in the UK",
        "comments": "polynomial distribution, Fermi-Dirac distribution, expenditure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most papers which explored so far macroeconomic variables took into account\nincome and wealth. Equally important as the previous macroeconomic variables is\nthe expenditure or consumption, which shows the amount of goods and services\nthat a person or a household purchased. Using statistical distributions from\nPhysics, such as Fermi-Dirac and polynomial distributions, we try to fit the\ndata regarding the expenditure distribution divided in deciles of population\naccording to their income (gross and disposable expenditure are taken into\naccount). Using coefficient of determination as theoretical tool in order to\nassess the degree of success for these distributions, we find that both\ndistributions are really robust in describing the expenditure distribution,\nregardless the data set or the methodology used to calculate the expenditure\nvalues for the deciles of income. This is the first paper to our knowledge\nwhich tackles expenditure, especially using a method to describe expenditure\nsuch as lower limit on expenditure. This is also relevant since it allows the\napproach of macroeconomic systems using more variables characterizing their\nactivity, can help in the investigation of living standards and inequality, and\npoints to more theoretical explorations which can be very useful for the\nEconomics and business practice.\n"
    },
    {
        "paper_id": 1410.4382,
        "authors": "Mark H.A. Davis",
        "title": "Verification of internal risk measure estimates",
        "comments": "29 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns sequential computation of risk measures for financial\ndata and asks how, given a risk measurement procedure, we can tell whether the\nanswers it produces are `correct'. We draw the distinction between `external'\nand `internal' risk measures and concentrate on the latter, where we observe\ndata in real time, make predictions and observe outcomes. It is argued that\nevaluation of such procedures is best addressed from the point of view of\nprobability forecasting or Dawid's theory of `prequential statistics' [Dawid,\nJRSS(A)1984]. We introduce a concept of `calibration' of a risk measure in a\ndynamic setting, following the precepts of Dawid's weak and strong prequential\nprinciples, and examine its application to quantile forecasting (VaR -- value\nat risk) and to mean estimation (applicable to CVaR -- expected shortfall). The\nrelationship between these ideas and `elicitability' [Gneiting, JASA 2011] is\nexamined. We show in particular that VaR has special properties not shared by\nany other risk measure. Turning to CVaR we argue that its main deficiency is\nthe unquantifiable tail dependence of estimators. In a final section we show\nthat a simple data-driven feedback algorithm can produce VaR estimates on\nfinancial data that easily pass both the consistency test and a further\nnewly-introduced statistical test for independence of a binary sequence.\n"
    },
    {
        "paper_id": 1410.4694,
        "authors": "Zhen Zhu, Michelangelo Puliga, Federica Cerina, Alessandro Chessa,\n  Massimo Riccaboni",
        "title": "Global Value Trees",
        "comments": "26 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1407.0225",
        "journal-ref": "PLoS ONE 10(5): e0126699 (2015)",
        "doi": "10.1371/journal.pone.0126699",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fragmentation of production across countries has become an important\nfeature of the globalization in recent decades and is often conceptualized by\nthe term, global value chains (GVCs). When empirically investigating the GVCs,\nprevious studies are mainly interested in knowing how global the GVCs are\nrather than how the GVCs look like. From a complex networks perspective, we use\nthe World Input-Output Database (WIOD) to study the global production system.\nWe find that the industry-level GVCs are indeed not chain-like but are better\ncharacterized by the tree topology. Hence, we compute the global value trees\n(GVTs) for all the industries available in the WIOD. Moreover, we compute an\nindustry importance measure based on the GVTs and compare it with other network\ncentrality measures. Finally, we discuss some future applications of the GVTs.\n"
    },
    {
        "paper_id": 1410.4807,
        "authors": "A. V. Lebedev, P. P. Zabreiko",
        "title": "Banach geometry of arbitrage free markets",
        "comments": "19 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article presents a description of geometry of Banach structures forming\nmathematical base of markets arbitrage absence type phenomena. In this\nconnection the role of reflexive subspaces (replacing classically considered\nfinite-dimensional subspaces) and plasterable cones is uncovered.\n"
    },
    {
        "paper_id": 1410.4847,
        "authors": "Yoshiharu Maeno, Kenji Nishiguchi, Satoshi Morinaga, Hirokazu\n  Matsushima",
        "title": "Impact of shadow banks on financial contagion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An asset network systemic risk (ANWSER) model is presented to investigate the\nimpact of how shadow banks are intermingled in a financial system on the\nseverity of financial contagion. Particularly, the focus of this study is the\nimpact of the following three representative topologies of an interbank loan\nnetwork between shadow banks and regulated banks. (1) Random mixing network:\nshadow banks and regulated banks are intermingled randomly. (2)\nAsset-correlated mixing network: banks having bigger assets are a regulated\nbank and other banks are shadow banks. (3) Layered mixing network: banks in a\nshadow bank layer are connected to banks in a regulated bank layer with some\ninterbank loans.\n"
    },
    {
        "paper_id": 1410.4866,
        "authors": "Elvis Oltean, Fedor Kusmartsev",
        "title": "A polynomial distribution applied to income and wealth distribution",
        "comments": "polynomial distribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Income and wealth distribution affect stability of a society to a large\nextent and high inequality affects it negatively. Moreover, in the case of\ndeveloped countries, recently has been proven that inequality is closely\nrelated to all negative phenomena affecting society. So far, Econophysics\npapers tried to analyse income and wealth distribution by employing\ndistributions such as Fermi-Dirac, Bose-Einstein, Maxwell-Boltzmann, lognormal\n(Gibrat), and exponential. Generally, distributions describe mostly income and\nless wealth distribution for low and middle income segment of population, which\naccounts about 90% of the population. Our approach is based on a totally new\ndistribution, not used so far in the literature regarding income and wealth\ndistribution. Using cumulative distribution method, we find that polynomial\nfunctions, regardless of their degree (first, second, or higher), can describe\nwith very high accuracy both income and wealth distribution. Moreover, we find\nthat polynomial functions describe income and wealth distribution for entire\npopulation including upper income segment for which traditionally Pareto\ndistribution is used.\n"
    },
    {
        "paper_id": 1410.4922,
        "authors": "Roy Cerqueti and Marcel Ausloos",
        "title": "Assessing the Inequalities of Wealth in Regions: the Italian Case",
        "comments": "to be published in Quality and Quantity; 23 pages; 1 figure; 23\n  tables; 19 references",
        "journal-ref": "Quality and Quantity, 49(6), 2307-2323 (2015)",
        "doi": "10.1007/s11135-014-0111-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses region wealth size distributions, through their member\ncities aggregated tax income. As an illustration, the official data of the\nItalian Ministry of Economics and Finance has been considered, for all Italian\nmunicipalities, over the period 2007-2011. Yearly data of the aggregated tax\nincome is transformed into a few indicators: the Gini, Theil, and\nHerfindahl-Hirschman indices. On one hand, the relative interest of each index\nis discussed. On the other hand, numerical results confirm that Italy is\ndivided into very different regional realities, a few which are specifically\noutlined. This shows the interest of transforming data in an adequate manner\nand of comparing such indices.\n"
    },
    {
        "paper_id": 1410.4962,
        "authors": "Sara Biagini, Bruno Bouchard, Constantinos Kardaras, Marcel Nutz",
        "title": "Robust Fundamental Theorem for Continuous Processes",
        "comments": "Forthcoming in 'Mathematical Finance'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous-time financial market with continuous price processes\nunder model uncertainty, modeled via a family $\\mathcal{P}$ of possible\nphysical measures. A robust notion ${\\rm NA}_{1}(\\mathcal{P})$ of no-arbitrage\nof the first kind is introduced; it postulates that a nonnegative, nonvanishing\nclaim cannot be superhedged for free by using simple trading strategies. Our\nfirst main result is a version of the fundamental theorem of asset pricing:\n${\\rm NA}_{1}(\\mathcal{P})$ holds if and only if every $P\\in\\mathcal{P}$ admits\na martingale measure which is equivalent up to a certain lifetime. The second\nmain result provides the existence of optimal superhedging strategies for\ngeneral contingent claims and a representation of the superhedging price in\nterms of martingale measures.\n"
    },
    {
        "paper_id": 1410.5068,
        "authors": "Andries Brandsma and d'Artis Kancs and Philippe Monfort and Alexandra\n  Rillaers",
        "title": "RHOMOLO: A Dynamic Spatial General Equilibrium Model for Assessing the\n  Impact of Cohesion Policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents the newly developed dynamic spatial general equilibrium\nmodel of European Commission, RHOMOLO. The model incorporates several elements\nfrom economic geography in a novel and theoretically consistent way. It\ndescribes the location choice of different types of agents and captures the\ninterplay between agglomeration and dispersion forces in determining the\nspatial equilibrium. The model is also dynamic as it allows for the\naccumulation of factors of production, human capital and technology. This makes\nRHOMOLO well suited for simulating policy scenario related to the EU cohesion\npolicy and for the analysis of its impact on the regions and the Member States\nof the union.\n"
    },
    {
        "paper_id": 1410.5328,
        "authors": "Carlos Abad and Garud Iyengar",
        "title": "Portfolio Selection with Multiple Spectral Risk Constraints",
        "comments": "20 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an iterative gradient-based algorithm to efficiently solve the\nportfolio selection problem with multiple spectral risk constraints. Since the\nconditional value at risk (CVaR) is a special case of the spectral risk\nmeasure, our algorithm solves portfolio selection problems with multiple CVaR\nconstraints. In each step, the algorithm solves very simple separable convex\nquadratic programs; hence, we show that the spectral risk constrained portfolio\nselection problem can be solved using the technology developed for solving\nmean-variance problems. The algorithm extends to the case where the objective\nis a weighted sum of the mean return and either a weighted combination or the\nmaximum of a set of spectral risk measures. We report numerical results that\nshow that our proposed algorithm is very efficient; it is at least one order of\nmagnitude faster than the state-of-the-art general purpose solver for all\npractical instances. One can leverage this efficiency to be robust against\nmodel risk by including constraints with respect to several different risk\nmodels.\n"
    },
    {
        "paper_id": 1410.5466,
        "authors": "Samuel Drapeau and Asgar Jamneshan",
        "title": "Conditional Preference Orders and their Numerical Representations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an axiomatic system modeling conditional preference orders which\nis based on conditional set theory. Conditional numerical representations are\nintroduced, and a conditional version of the theorems of Debreu on the\nexistence of numerical representations is proved. The conditionally continuous\nrepresentations follow from a conditional version of Debreu's Gap Lemma the\nproof of which relies on a conditional version of the axiom of choice, free of\nany measurable selection argument. We give a conditional version of the von\nNeumann and Morgenstern representation as well as automatic conditional\ncontinuity results, and illustrate them by examples.\n"
    },
    {
        "paper_id": 1410.5513,
        "authors": "Zura Kakushadze",
        "title": "4-Factor Model for Overnight Returns",
        "comments": "19 pages; a minor remark and references added; to appear in Wilmott\n  Magazine",
        "journal-ref": "Wilmott Magazine 2015(79) (2015) 56-62",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a 4-factor model for overnight returns and give explicit\ndefinitions of our 4 factors. Long horizon fundamental factors such as value\nand growth lack predictive power for overnight (or similar short horizon)\nreturns and are not included. All 4 factors are constructed based on intraday\nprice and volume data and are analogous to size (price), volatility, momentum\nand liquidity (volume). Historical regressions a la Fama and MacBeth (1973)\nsuggest that our 4 factors have sizable serial t-statistic and appear to be\nrelevant predictors for overnight returns. We check this by using our 4-factor\nmodel in an explicit intraday mean-reversion alpha.\n"
    },
    {
        "paper_id": 1410.5621,
        "authors": "Nicol\\'o Musmeci and Tomaso Aste and Tiziana Di Matteo",
        "title": "Risk diversification: a study of persistence with a filtered\n  correlation-network approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The evolution with time of the correlation structure of equity returns is\nstudied by means of a filtered network approach investigating persistences and\nrecurrences and their implications for risk diversification strategies. We\nbuild dynamically Planar Maximally Filtered Graphs from the correlation\nstructure over a rolling window and we study the persistence of the associated\nDirected Bubble Hierarchical Tree (DBHT) clustering structure. We observe that\nthe DBHT clustering structure is quite stable during the early 2000' becoming\ngradually less persistent before the unfolding of the 2007-2008 crisis. The\ncorrelation structure eventually recovers persistence in the aftermath of the\ncrisis settling up a new phase, distinct from the pre-cysts structure, where\nthe market structure is less related to industrial sector activity. Notably, we\nobserve that - presently - the correlation structure is loosing again\npersistence indicating the building-up of another, different, phase. Such\ndynamical changes in persistence and their occurrence at the unfolding of\nfinancial crises rises concerns about the effectiveness of correlation-based\nportfolio management tools for risk diversification.\n"
    },
    {
        "paper_id": 1410.5787,
        "authors": "Nassim Nicholas Taleb, Rupert Read, Raphael Douady, Joseph Norman, and\n  Yaneer Bar-Yam",
        "title": "The Precautionary Principle (with Application to the Genetic\n  Modification of Organisms)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a non-naive version of the Precautionary (PP) that allows us to\navoid paranoia and paralysis by confining precaution to specific domains and\nproblems. PP is intended to deal with uncertainty and risk in cases where the\nabsence of evidence and the incompleteness of scientific knowledge carries\nprofound implications and in the presence of risks of \"black swans\", unforeseen\nand unforeseable events of extreme consequence. We formalize PP, placing it\nwithin the statistical and probabilistic structure of ruin problems, in which a\nsystem is at risk of total failure, and in place of risk we use a formal\nfragility based approach. We make a central distinction between 1) thin and fat\ntails, 2) Local and systemic risks and place PP in the joint Fat Tails and\nsystemic cases. We discuss the implications for GMOs (compared to Nuclear\nenergy) and show that GMOs represent a public risk of global harm (while harm\nfrom nuclear energy is comparatively limited and better characterized). PP\nshould be used to prescribe severe limits on GMOs.\n"
    },
    {
        "paper_id": 1410.5955,
        "authors": "Hi Jun Choe, Jeong Ho Chu and So Jeong Shin",
        "title": "Recombining binomial tree for constant elasticity of variance process",
        "comments": "17 pages, 7 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theme in this paper is the recombining binomial tree to price American\nput option when the underlying stock follows constant elasticity of\nvariance(CEV) process. Recombining nodes of binomial tree are decided from\nfinite difference scheme to emulate CEV process and the tree has a linear\ncomplexity. Also it is derived from the differential equation the asymptotic\nenvelope of the boundary of tree. Conducting numerical experiments, we confirm\nthe convergence and accuracy of the pricing by our recombining binomial tree\nmethod. As a result, we can compute the price of American put option under CEV\nmodel, effectively.\n"
    },
    {
        "paper_id": 1410.5996,
        "authors": "Vladimir V'yugin",
        "title": "Log-Optimal Portfolio Selection Using the Blackwell Approachability\n  Theorem",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method for constructing the log-optimal portfolio using the\nwell-calibrated forecasts of market values. Dawid's notion of calibration and\nthe Blackwell approachability theorem are used for computing well-calibrated\nforecasts. We select a portfolio using this \"artificial\" probability\ndistribution of market values. Our portfolio performs asymptotically at least\nas well as any stationary portfolio that redistributes the investment at each\nround using a continuous function of side information. Unlike in classical\nmathematical finance theory, no stochastic assumptions are made about market\nvalues.\n"
    },
    {
        "paper_id": 1410.6005,
        "authors": "John Cotter and Enrique Salvador",
        "title": "The non-linear trade-off between return and risk: a regime-switching\n  multi-factor framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study develops a multi-factor framework where not only market risk is\nconsidered but also potential changes in the investment opportunity set.\nAlthough previous studies find no clear evidence about a positive and\nsignificant relation between return and risk, favourable evidence can be\nobtained if a non-linear relation is pursued. The positive and significant\nrisk-return trade-off is essentially observed during low volatility periods.\nHowever, this relationship is not obtained during periods of high volatility.\nAlso, different patterns for the risk premium dynamics in low and high\nvolatility periods are obtained both in prices of risk and market risk\ndynamics.\n"
    },
    {
        "paper_id": 1410.6084,
        "authors": "Philip A. Ernst, Michael B. Imerman, Larry Shepp, and Quan Zhou",
        "title": "Fiscal stimulus as an optimal control problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the Great Recession, Democrats in the United States argued that\ngovernment spending could be utilized to \"grease the wheels\" of the economy in\norder to create wealth and to increase employment; Republicans, on the other\nhand, contended that government spending is wasteful and discouraged\ninvestment, thereby increasing unemployment. Today, in 2020, we find ourselves\nin the midst of another crisis where government spending and fiscal stimulus is\nagain being considered as a solution. In the present paper, we address this\nquestion by formulating an optimal control problem generalizing the model of\nRadner & Shepp (1996). The model allows for the company to borrow continuously\nfrom the government. We prove that there exists an optimal strategy; rigorous\nverification proofs for its optimality are provided. We proceed to prove that\ngovernment loans increase the expected net value of a company. We also examine\nthe consequences of different profit-taking behaviors among firms who receive\nfiscal stimulus.\n"
    },
    {
        "paper_id": 1410.6144,
        "authors": "Dmitry Kramkov, Sergio Pulido",
        "title": "Stability and analytic expansions of local solutions of systems of\n  quadratic BSDEs with applications to a price impact model",
        "comments": "Final version, 28 pages",
        "journal-ref": "SIAM Journal on Financial Mathematics, 2016, Vol. 7, No. 1 : pp.\n  567-587",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain stability estimates and derive analytic expansions for local\nsolutions of multi-dimensional quadratic BSDEs. We apply these results to a\nfinancial model where the prices of risky assets are quoted by a representative\ndealer in such a way that it is optimal to meet an exogenous demand. We show\nthat the prices are stable under the demand process and derive their analytic\nexpansions for small risk aversion coefficients of the dealer.\n"
    },
    {
        "paper_id": 1410.615,
        "authors": "Jingwei Liu, Jiwen Luo, Xing Chen",
        "title": "Pricing of European Basket Call Option under Exponential\n  Ornstein-Uhlenbeck Process",
        "comments": "This paper has been withdrawn by the author Jingwei Liu due to a\n  crucial error in equation (11)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing of European basket call option with n-assets and a bond is discussed\nin this paper, where all prices of n-assets and the bond are driven by\nExponential Ornstein-Uhlenbeck processes. The close-form of European basket\noption pricing formula is derived. Utilizing with 1-order differential\napproximate numerical solution of stochastic differential equation (Milstein\nmethod), a simulation example of European basket option pricing with 3 assets\nis also given.\n"
    },
    {
        "paper_id": 1410.6321,
        "authors": "Beata Stehlikova",
        "title": "Perturbation analysis of a nonlinear equation arising in the\n  Schaefer-Schwartz model of interest rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We deal with the interest rate model proposed by Schaefer and Schwartz, which\nmodels the long rate and the spread, defined as the difference between the\nshort and the long rates. The approximate analytical formula for the bond\nprices suggested by the authors requires a computation of a certain constant,\ndefined via a nonlinear equation and an integral of a solution to a system of\nordinary differential equations. In this paper we use perturbation methods to\ncompute this constant. Coefficients of its expansion are given in a closed form\nand can be constructed to arbitrary order. However, our numerical results show\nthat a very good accuracy is achieved already after using a small number of\nterms.\n"
    },
    {
        "paper_id": 1410.6408,
        "authors": "Gianluca Cassese",
        "title": "Asset Pricing in an Imperfect World",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1406.0412",
        "journal-ref": null,
        "doi": "10.1007/s00199-016-0999-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a model with no given probability measure, we consider asset pricing in\nthe presence of frictions and other imperfections and characterize the property\nof coherent pricing, a notion related to (but much weaker than) the no\narbitrage property. We show that prices are coherent if and only if the set of\npricing measures is non empty, i.e. if pricing by expectation is possible. We\nthen obtain a decomposition of coherent prices highlighting the role of\nbubbles. Eventually we show that under very weak conditions the coherent\npricing of options allows for a very clear representation which allows, as in\nBreeden and Litzenberger, to extract the implied probability.\n"
    },
    {
        "paper_id": 1410.6646,
        "authors": "Serguei Saavedra, Luis J. Gilarranz, Rudolf P. Rohr, Michael Schnabel,\n  Brian Uzzi, Jordi Bascompte",
        "title": "Stock fluctuations are correlated and amplified across networks of\n  interlocking directorates",
        "comments": null,
        "journal-ref": "EPJ Data Science 3: 30 (2014)",
        "doi": "10.1140/epjds/s13688-014-0030-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traded corporations are required by law to have a majority of outside\ndirectors on their board. This requirement allows the existence of directors\nwho sit on the board of two or more corporations at the same time, generating\nwhat is commonly known as interlocking directorates. While research has shown\nthat networks of interlocking directorates facilitate the transmission of\ninformation between corporations, little is known about the extent to which\nsuch interlocking networks can explain the fluctuations of stock price returns.\nYet, this is a special concern since the risk of amplifying stock fluctuations\nis latent. To answer this question, here we analyze the board composition,\ntraders' perception, and stock performance of more than 1500 US traded\ncorporations from 2007-2011. First, we find that the fewer degrees of\nseparation between two corporations in the interlocking network, the stronger\nthe temporal correlation between their stock price returns. Second, we find\nthat the centrality of traded corporations in the interlocking network\ncorrelates with the frequency at which financial traders talk about such\ncorporations, and this frequency is in turn proportional to the corresponding\ntraded volume. Third, we show that the centrality of corporations was\nnegatively associated with their stock performance in 2008, the year of the big\nfinancial crash. These results suggest that the strategic decisions made by\ninterlocking directorates are strongly followed by stock analysts and have the\npotential to correlate and amplify the movement of stock prices during\nfinancial crashes. These results may have relevant implications for scholars,\ninvestors, and regulators.\n"
    },
    {
        "paper_id": 1410.6841,
        "authors": "Yuri A. Katz",
        "title": "qGaussian model of default",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the qGaussian generalization of the Merton framework, which takes\ninto account slow fluctuations of the volatility of the firms market value of\nfinancial assets. The minimal version of the model depends on the Tsallis\nentropic parameter q and the generalized distance to default. The empirical\nfoundation and implications of the model are illustrated by the study of 645\nNorth American industrial firms during the financial crisis, 2006 - 2012. All\ndefaulters in the sample have exceptionally large, corresponding to unusually\nfat-tailed unconditional distributions of log-asset-returns. Using Receiver\nOperating Characteristic curves, we demonstrate the high forecasting power of\nthe model in prediction of 1-year defaults. Our study suggests that the level\nof complexity of the realized time series, quantified by q, should be taken\ninto account to improve valuations of default risk.\n"
    },
    {
        "paper_id": 1410.6898,
        "authors": "Mauro Bernardi, Leopoldo Catania and Lea Petrella",
        "title": "Are news important to predict large losses?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the impact of news to predict extreme financial\nreturns using high frequency data. We consider several model specifications\ndiffering for the dynamic property of the underlying stochastic process as well\nas for the innovation process. Since news are essentially qualitative measures,\nthey are firstly transformed into quantitative measures which are subsequently\nintroduced as exogenous regressors into the conditional volatility dynamics.\nThree basic sentiment indexes are constructed starting from three list of words\ndefined by historical market news response and by a discriminant analysis.\nModels are evaluated in terms of their predictive accuracy to forecast\nout-of-sample Value-at-Risk of the STOXX Europe 600 sectors at different\nconfidence levels using several statistic tests and the Model Confidence Set\nprocedure of Hansen et al. (2011). Since the Hansen's procedure usually\ndelivers a set of models having the same VaR predictive ability, we propose a\nnew forecasting combination technique that dynamically weights the VaR\npredictions obtained by the models belonging to the optimal final set. Our\nresults confirms that the inclusion of exogenous information as well as the\nright specification of the returns' conditional distribution significantly\ndecrease the number of actual versus expected VaR violations towards one, as\nthis is especially true for higher confidence levels.\n"
    },
    {
        "paper_id": 1410.7206,
        "authors": "Antoine Jacquier and Patrick Roome",
        "title": "Large-Maturity Regimes of the Heston Forward Smile",
        "comments": "32 pages, 16 figures New Section 5 providing more (financial)\n  intuitions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a full characterisation of the large-maturity forward implied\nvolatility smile in the Heston model. Although the leading decay is provided by\na fairly classical large deviations behaviour, the algebraic expansion\nproviding the higher-order terms highly depends on the parameters, and\ndifferent powers of the maturity come into play. As a by-product of the\nanalysis we provide new implied volatility asymptotics, both in the forward\ncase and in the spot case, as well as extended SVI-type formulae. The proofs\nare based on extensions and refinements of sharp large deviations theory, in\nparticular in cases where standard convexity arguments fail.\n"
    },
    {
        "paper_id": 1410.7316,
        "authors": "Aleksandar Mijatovic, Martijn Pistorius, Johannes Stolte",
        "title": "Randomisation and recursion methods for mixed-exponential Levy models,\n  with financial applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new Monte Carlo variance reduction method to estimate the\nexpectation of two commonly encountered path-dependent functionals:\nfirst-passage times and occupation times of sets. The method is based on a\nrecursive approximation of the first-passage time probability and expected\noccupation time of sets of a Levy bridge process that relies in part on a\nrandomisation of the time parameter. We establish this recursion for general\nLevy processes and derive its explicit form for mixed-exponential\njump-diffusions, a dense subclass (in the sense of weak approximation) of Levy\nprocesses, which includes Brownian motion with drift, Kou's double-exponential\nmodel and hyper-exponential jump-diffusion models. We present a highly accurate\nnumerical realisation and derive error estimates. By way of illustration the\nmethod is applied to the valuation of range accruals and barrier options under\nexponential Levy models and Bates-type stochastic volatility models with\nexponential jumps. Compared with standard Monte Carlo methods, we find that the\nmethod is significantly more efficient.\n"
    },
    {
        "paper_id": 1410.7317,
        "authors": "Neil Shephard and Justin J. Yang",
        "title": "Continuous time analysis of fleeting discrete price moves",
        "comments": "39 pages, 12 figures, \\baselineskip=20pt, submitted to \"Journal of\n  the American Statistical Association\"",
        "journal-ref": null,
        "doi": "10.1080/01621459.2016.1192544",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel model of financial prices where: (i) prices are\ndiscrete; (ii) prices change in continuous time; (iii) a high proportion of\nprice changes are reversed in a fraction of a second. Our model is analytically\ntractable and directly formulated in terms of the calendar time and price\nimpact curve. The resulting c\\`{a}dl\\`{a}g price process is a piecewise\nconstant semimartingale with finite activity, finite variation and no Brownian\nmotion component. We use moment-based estimations to fit four high frequency\nfutures data sets and demonstrate the descriptive power of our proposed model.\nThis model is able to describe the observed dynamics of price changes over\nthree different orders of magnitude of time intervals.\n"
    },
    {
        "paper_id": 1410.7453,
        "authors": "Cody B. Hyndman and Menachem Wenger",
        "title": "GMWB Riders in a Binomial Framework - Pricing, Hedging, and\n  Diversification of Mortality Risk",
        "comments": "41 pages, 11 figures; This paper combines a previous version titled\n  \"Pricing and Hedging GMWB Riders in a Binomial Framework\" (arXiv:1410.7453v1)\n  and the working paper titled \"Diversification of mortality risk in GMWB rider\n  pricing and hedging\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a binomial model for a guaranteed minimum withdrawal benefit\n(GMWB) rider to a variable annuity (VA) under optimal policyholder behaviour.\nThe binomial model results in explicitly formulated perfect hedging strategies\nfunded using only periodic fee income. We consider the separate perspectives of\nthe insurer and policyholder and introduce a unifying relationship.\nDecompositions of the VA and GMWB contract into term-certain payments and\noptions representing the guarantee and early surrender features are extended to\nthe binomial framework. We incorporate an approximation algorithm for Asian\noptions that significantly improves efficiency of the binomial model while\nretaining accuracy. Several numerical examples are provided which illustrate\nboth the accuracy and the tractability of the binomial model. We extend the\nbinomial model to include policy holder mortality and death benefits. Pricing,\nhedging, and the decompositions of the contract are extended to incorporate\nmortality risk. We prove limiting results for the hedging strategies and\ndemonstrate mortality risk diversification. Numerical examples are provided\nwhich illustrate the effectiveness of hedging and the diversification of\nmortality risk under capacity constraints with finite pools.\n"
    },
    {
        "paper_id": 1410.7799,
        "authors": "Luca Onorante and Adrian E. Raftery",
        "title": "Dynamic Model Averaging in Large Model Spaces Using Dynamic Occam's\n  Window",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bayesian model averaging has become a widely used approach to accounting for\nuncertainty about the structural form of the model generating the data. When\ndata arrive sequentially and the generating model can change over time, Dynamic\nModel Averaging (DMA) extends model averaging to deal with this situation.\nOften in macroeconomics, however, many candidate explanatory variables are\navailable and the number of possible models becomes too large for DMA to be\napplied in its original form. We propose a new method for this situation which\nallows us to perform DMA without considering the whole model space, but using a\nsubset of models and dynamically optimizing the choice of models at each point\nin time. This yields a dynamic form of Occam's window. We evaluate the method\nin the context of the problem of nowcasting GDP in the Euro area. We find that\nits forecasting performance compares well that of other methods.\n  Keywords: Bayesian model averaging; Model uncertainty; Nowcasting; Occam's\nwindow.\n"
    },
    {
        "paper_id": 1410.7845,
        "authors": "Ying Zhang and Chuancun Yin",
        "title": "A new multivariate dependence measure based on comonotonicity",
        "comments": "18pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new multivariate dependence measure based on\ncomonotonicity by means of product moment which motivated by the recent papers\nof Koch and Schepper (ASTIN Bulletin 41 (2011) 191-213) and Dhaene et al.\n(Journal of Computational and Applied Mathematics 263 (2014) 78-87). Some\ndifferences and relations between the new dependence measure and other\nmultivariate measures are an- alyzed. We also give several characteristics of\nthis measure and estimations based on the definitions and its property are\npresented.\n"
    },
    {
        "paper_id": 1410.7961,
        "authors": "Hao-Che Chen",
        "title": "Visualisation of financial time series by linear principal component\n  analysis and nonlinear principal component analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this dissertation, the main goal is visualisation of financial time\nseries. We expect that visualisation of financial time series will be a useful\nauxiliary for technical analysis. Firstly, we review the technical analysis\nmethods and test our trading rules, which are built by the essential concepts\nof technical analysis. Next, we compare the quality of linear principal\ncomponent analysis and nonlinear principal component analysis in financial\nmarket visualisation. We compare different methods of data preprocessing for\nvisualisation purposes. Using visualisation, we demonstrate the difference\nbetween normal and crisis time period. Thus, the visualisation of financial\nmarket can be a tool to support technical analysis.\n"
    },
    {
        "paper_id": 1410.8042,
        "authors": "Vladimir Dombrovskii and Tatyana Obedko",
        "title": "Portfolio Optimization in the Financial Market with Correlated Returns\n  under Constraints, Transaction Costs and Different Rates for Borrowing and\n  Lending",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1410.1136",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we consider the optimal portfolio selection problem under hard\nconstraints on trading amounts, transaction costs and different rates for\nborrowing and lending when the risky asset returns are serially correlated. No\nassumptions about the correlation structure between different time points or\nabout the distribution of the asset returns are needed. The problem is stated\nas a dynamic tracking problem of a reference portfolio with desired return. Our\napproach is tested on a set of a real data from Russian Stock Exchange MICEX.\n"
    },
    {
        "paper_id": 1410.816,
        "authors": "Hyungbin Park",
        "title": "Pricing and Hedging Long-Term Options",
        "comments": "This paper has been withdrawn by the author due to several errors\n  like Proposition 3.2 and 3.3",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we investigate the behavior of long-term options. In many\ncases, option prices follow an exponential decay (or growth) rate for further\nmaturity dates. We determine under what conditions option prices are\ncharacterized by this property. To see this, we use the martingale extraction\nmethod through which a pricing operator is transformed into a semigroup\noperator, which is easier to address.\n  We also explore notions of hedging long-term options. Hedging is an attempt\nto reduce market risks, and we investigate the price sensitivities (Greeks)\nwith respect to such risks, which are typically repre- sented by variations in\nthe underlying process of an option. We combine the Malliavin calculus with the\nmartingale extraction method to analyze Greeks. We see that the ratios between\nGreeks and the option price are expressed in a simple form in the long term.\n"
    },
    {
        "paper_id": 1410.8224,
        "authors": "Masaaki Fukasawa",
        "title": "Efficient price dynamics in a limit order market: an utility\n  indifference approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct an utility-based dynamic asset pricing model for a limit order\nmarket. The price is nonlinear in volume and subject to market impact. We solve\nan optimal hedging problem under the market impact and derive the dynamics of\nthe efficient price, that is, the asset price when a representative liquidity\ndemander follows an optimal strategy. We show that a Pareto efficient\nallocation is achieved under a completeness condi- tion. We give an explicit\nrepresentation of the efficient price for several examples. In particular, we\nobserve that the volatility of the asset depends on the convexity of an initial\nendowment. Further, we observe that an asset price crash is invoked by an\nendowment shock. We establish a dynamic programming principle under an\nincomplete framework.\n"
    },
    {
        "paper_id": 1410.8409,
        "authors": "Denis S. Grebenkov and Jeremy Serror",
        "title": "Optimal Allocation of Trend Following Strategies",
        "comments": null,
        "journal-ref": "Physica A 433, 107-125 (2015)",
        "doi": "10.1016/j.physa.2015.03.078",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a portfolio allocation problem for trend following (TF)\nstrategies on multiple correlated assets. Under simplifying assumptions of a\nGaussian market and linear TF strategies, we derive analytical formulas for the\nmean and variance of the portfolio return. We construct then the optimal\nportfolio that maximizes risk-adjusted return by accounting for inter-asset\ncorrelations. The dynamic allocation problem for $n$ assets is shown to be\nequivalent to the classical static allocation problem for $n^2$ virtual assets\nthat include lead-lag corrections in positions of TF strategies. The respective\nroles of asset auto-correlations and inter-asset correlations are investigated\nin depth for the two-asset case and a sector model. In contrast to the\nprinciple of diversification suggesting to treat uncorrelated assets, we show\nthat inter-asset correlations allow one to estimate apparent trends more\nreliably and to adjust the TF positions more efficiently. If properly accounted\nfor, inter-asset correlations are not deteriorative but beneficial for\nportfolio management that can open new profit opportunities for trend\nfollowers.\n"
    },
    {
        "paper_id": 1410.8427,
        "authors": "Ivan Medovikov",
        "title": "When does the stock market listen to economic news? New evidence from\n  copulas and news wires",
        "comments": "37 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study association between macroeconomic news and stock market returns\nusing the statistical theory of copulas, and a new comprehensive measure of\nnews based on the indexing of news wires. We find the impact of economic news\non equity returns to be nonlinear and asymmetric. In particular, controlling\nfor economic conditions and surprises associated with releases of economic\ndata, we find that the market reacts strongly and negatively to the most\nunfavourable macroeconomic news, but appears to largely discount the good news.\nThis relationship persists throughout the different stages of the business\ncycle.\n"
    },
    {
        "paper_id": 1410.8432,
        "authors": "Zhijian Wang and Bin Xu",
        "title": "Cycling in stochastic general equilibrium",
        "comments": "16 pages, 8 figures, keywords: general equilibrium; time reversal\n  symmetry; entanglement of deviations; angular momentum; macroeconomic\n  engineering",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By generalizing the measurements on the game experiments of mixed strategy\nNash equilibrium, we study the dynamical pattern in a representative dynamic\nstochastic general equilibrium (DSGE). The DSGE model describes the\nentanglements of the three variables (output gap [$y$], inflation [$\\pi$] and\nnominal interest rate [$r$]) which can be presented in 3D phase space. We find\nthat, even though the trajectory of $\\pi\\!-\\!y\\!-\\!r$ in phase space appears\nhighly stochastic, it can be visualized and quantified. It exhibits as\nclockwise cycles, counterclockwise cycles and weak cycles, respectively, when\nprojected onto $\\pi\\!-\\!y$, $y\\!-\\!r$ and $r\\!-\\!\\pi$ phase planes. We find\nalso that empirical data of United State (1960-2013) significantly exhibit same\ncycles. The resemblance between the cycles in general equilibrium and the\ncycles in mixed strategy Nash equilibrium suggest that, there generally exists\ndynamical fine structures accompanying with equilibrium. The fine structure,\ndescribing the entanglement of the non-equilibrium (the constantly deviating\nfrom the equilibrium), displays as endless cycles.\n"
    },
    {
        "paper_id": 1410.8504,
        "authors": "Mauro Bernardi and Leopoldo Catania",
        "title": "The Model Confidence Set package for R",
        "comments": "20 pages, 2 tables, 15 code chunk",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the R package MCS which implements the Model Confidence\nSet (MCS) procedure recently developed by Hansen et al. (2011). The Hansen's\nprocedure consists on a sequence of tests which permits to construct a set of\n'superior' models, where the null hypothesis of Equal Predictive Ability (EPA)\nis not rejected at a certain confidence level. The EPA statistic tests is\ncalculated for an arbitrary loss function, meaning that we could test models on\nvarious aspects, for example punctual forecasts. The relevance of the package\nis shown using an example which aims at illustrating in details the use of the\nfunctions provided by the package. The example compares the ability of\ndifferent models belonging to the ARCH family to predict large financial\nlosses. We also discuss the implementation of the ARCH--type models and their\nmaximum likelihood estimation using the popular R package rugarch developed by\nGhalanos (2014).\n"
    },
    {
        "paper_id": 1410.8595,
        "authors": "Polynice Oyono Ngou and Cody Hyndman",
        "title": "A Fourier interpolation method for numerical solution of FBSDEs: Global\n  convergence, stability, and higher order discretizations",
        "comments": "28 pages, 8 figures; Previously titled 'Global convergence and\n  stability of a convolution method for numerical solution of BSDEs'\n  (1410.8595v1)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The convolution method for the numerical solution of forward-backward\nstochastic differential equations (FBSDEs), introduced in [21], uses a uniform\nspace grid. In this paper we utilize a tree-like spatial discretization that\napproximates the BSDE on the tree, so that no spatial interpolation procedure\nis necessary. In addition to suppressing extrapolation error, leading to a\nglobally convergent numerical solution for the FBSDE, we provide explicit\nconvergence rates. On this alternative grid the conditional expectations\ninvolved in the time discretization of the BSDE are computed using Fourier\nanalysis and the fast Fourier transform (FFT) algorithm. The method is then\nextended to higher-order time discretizations of FBSDEs. Numerical results\ndemonstrating convergence are presented using a commodity price model,\nincorporating seasonality, and forward prices.\n"
    },
    {
        "paper_id": 1410.8609,
        "authors": "Xiaolin Luo and Pavel Shevchenko",
        "title": "Fast Numerical Method for Pricing of Variable Annuities with Guaranteed\n  Minimum Withdrawal Benefit under Optimal Withdrawal Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A variable annuity contract with Guaranteed Minimum Withdrawal Benefit (GMWB)\npromises to return the entire initial investment through cash withdrawals\nduring the policy life plus the remaining account balance at maturity,\nregardless of the portfolio performance. Under the optimal withdrawal strategy\nof a policyholder, the pricing of variable annuities with GMWB becomes an\noptimal stochastic control problem. So far in the literature these contracts\nhave only been evaluated by solving partial differential equations (PDE) using\nthe finite difference method. The well-known Least-Squares or similar Monte\nCarlo methods cannot be applied to pricing these contracts because the paths of\nthe underlying wealth process are affected by optimal cash withdrawals (control\nvariables) and thus cannot be simulated forward in time. In this paper we\npresent a very efficient new algorithm for pricing these contracts in the case\nwhen transition density of the underlying asset between withdrawal dates or its\nmoments are known. This algorithm relies on computing the expected contract\nvalue through a high order Gauss-Hermite quadrature applied on a cubic spline\ninterpolation. Numerical results from the new algorithm for a series of GMWB\ncontract are then presented, in comparison with results using the finite\ndifference method solving corresponding PDE. The comparison demonstrates that\nthe new algorithm produces results in very close agreement with those of the\nfinite difference method, but at the same time it is significantly faster;\nvirtually instant results on a standard desktop PC.\n"
    },
    {
        "paper_id": 1410.8671,
        "authors": "Oliver Kley and Claudia Kluppelberg and Gesine Reinert",
        "title": "Risk in a large claims insurance market with bipartite graph structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the influence of sharing large exogeneous losses to the reinsurance\nmarket by a bipartite graph. Using Pareto-tailed claims and multivariate\nregular variation we obtain asymptotic results for the Value-at-Risk and the\nConditional Tail Expectation. We show that the dependence on the network\nstructure plays a fundamental role in their asymptotic behaviour. As is\nwell-known in a non-network setting, if the Pareto exponent is larger than 1,\nthen for the individual agent (reinsurance company) diversification is\nbeneficial, whereas when it is less than 1, concentration on a few objects is\nthe better strategy. An additional aspect of this paper is the amount of\nuninsured losses which have to be convered by society. In the situation of\nnetworks of agents, in our setting diversification is never detrimental\nconcerning the amount of uninsured losses. If the Pareto-tailed claims have\nfinite mean, diversification turns out to be never detrimental, both for\nsociety and for individual agents. In contrast, if the Pareto-tailed claims\nhave infinite mean, a conflicting situation may arise between the incentives of\nindividual agents and the interest of some regulator to keep risk for society\nsmall. We explain the influence of the network structure on diversification\neffects in different network scenarios.\n"
    },
    {
        "paper_id": 1411.0426,
        "authors": "Freddy Delbaen, Fabio Bellini, Valeria Bignozzi and Johanna F. Ziegel",
        "title": "Risk measures with the CxLS property",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present contribution we characterize law determined convex risk\nmeasures that have convex level sets at the level of distributions. By relaxing\nthe assumptions in Weber (2006), we show that these risk measures can be\nidentified with a class of generalized shortfall risk measures. As a direct\nconsequence, we are able to extend the results in Ziegel (2014) and Bellini and\nBignozzi (2014) on convex elicitable risk measures and confirm that expectiles\nare the only elicitable coherent risk measures. Further, we provide a simple\ncharacterization of robustness for convex risk measures in terms of a weak\nnotion of mixture continuity.\n"
    },
    {
        "paper_id": 1411.0496,
        "authors": "Ladislav Kristoufek",
        "title": "Detrended fluctuation analysis as a regression framework: Estimating\n  dependence at different scales",
        "comments": "10 pages, 6 figures",
        "journal-ref": "Physical Review E 91, 022802 (2015)",
        "doi": "10.1103/PhysRevE.91.022802",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We propose a framework combining detrended fluctuation analysis with standard\nregression methodology. The method is built on detrended variances and\ncovariances and it is designed to estimate regression parameters at different\nscales and under potential non-stationarity and power-law correlations. The\nformer feature allows for distinguishing between effects for a pair of\nvariables from different temporal perspectives. The latter ones make the method\na significant improvement over the standard least squares estimation.\nTheoretical claims are supported by Monte Carlo simulations. The method is then\napplied on selected examples from physics, finance, environmental science and\nepidemiology. For most of the studied cases, the relationship between variables\nof interest varies strongly across scales.\n"
    },
    {
        "paper_id": 1411.057,
        "authors": "Santanu Dey, Sandeep Juneja, Karthyek R. A. Murthy",
        "title": "Incorporating Views on Marginal Distributions in the Calibration of Risk\n  Models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1203.0643",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Entropy based ideas find wide-ranging applications in finance for calibrating\nmodels of portfolio risk as well as options pricing. The abstracted problem,\nextensively studied in the literature, corresponds to finding a probability\nmeasure that minimizes relative entropy with respect to a specified measure\nwhile satisfying constraints on moments of associated random variables. These\nmoments may correspond to views held by experts in the portfolio risk setting\nand to market prices of liquid options for options pricing models. However, it\nis reasonable that in the former settings, the experts may have views on tails\nof risks of some securities. Similarly, in options pricing, significant\nliterature focuses on arriving at the implied risk neutral density of benchmark\ninstruments through observed market prices. With the intent of calibrating\nmodels to these more general stipulations, we develop a unified entropy based\nmethodology to allow constraints on both moments as well as marginal\ndistributions of functions of underlying securities. This is applied to\nMarkowitz portfolio framework, where a view that a particular portfolio incurs\nheavy tailed losses is shown to lead to fatter and more reasonable tails for\nlosses of component securities. We also use this methodology to price\nnon-traded options using market information such as observed option prices and\nimplied risk neutral densities of benchmark instruments.\n"
    },
    {
        "paper_id": 1411.0849,
        "authors": "Nicole B\\\"auerle, Igor Gilitschenski, Uwe D. Hanebeck",
        "title": "Exact and Approximate Hidden Markov Chain Filters Based on Discrete\n  Observations",
        "comments": "Minor changes",
        "journal-ref": "Statistics and Risk Modeling 32 (3-4), 159-176 (2015)",
        "doi": "10.1515/strm-2015-0004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a Hidden Markov Model (HMM) where the integrated continuous-time\nMarkov chain can be observed at discrete time points perturbed by a Brownian\nmotion. The aim is to derive a filter for the underlying continuous-time Markov\nchain. The recursion formula for the discrete-time filter is easy to derive,\nhowever involves densities which are very hard to obtain. In this paper we\nderive exact formulas for the necessary densities in the case the state space\nof the HMM consists of two elements only. This is done by relating the\nunderlying integrated continuous-time Markov chain to the so-called asymmetric\ntelegraph process and by using recent results on this process. In case the\nstate space consists of more than two elements we present three different ways\nto approximate the densities for the filter. The first approach is based on the\ncontinuous filter problem. The second approach is to derive a PDE for the\ndensities and solve it numerically and the third approach is a crude discrete\ntime approximation of the Markov chain. All three approaches are compared in a\nnumerical study.\n"
    },
    {
        "paper_id": 1411.1103,
        "authors": "Mauricio Junca and Rafael Serrano",
        "title": "Utility maximization in pure-jump models driven by marked point\n  processes and nonlinear wealth dynamics",
        "comments": "4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore martingale and convex duality techniques to study optimal\ninvestment strategies that maximize expected risk-averse utility from\nconsumption and terminal wealth. We consider a market model with jumps driven\nby (multivariate) marked point processes and so-called non-linear wealth\ndynamics which allows to take account of relaxed assumptions such as\ndifferential borrowing and lending interest rates or short positions with cash\ncollateral and negative rebate rates. We give suffcient conditions for\nexistence of optimal policies for agents with logarithmic and CRRA power\nutility. We find closed-form solutions for the optimal value function in the\ncase of pure-jump models with jump-size distributions modulated by a two-state\nMarkov chain.\n"
    },
    {
        "paper_id": 1411.1152,
        "authors": "Ignacio Esponda and Demian Pouzo",
        "title": "Berk-Nash Equilibrium: A Framework for Modeling Agents with Misspecified\n  Models",
        "comments": "The statement of Theorem 3 in the previous version was corrected. We\n  thank Yuichi Yamamoto for pointing out this correction to us",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an equilibrium framework that relaxes the standard assumption that\npeople have a correctly-specified view of their environment. Each player is\ncharacterized by a (possibly misspecified) subjective model, which describes\nthe set of feasible beliefs over payoff-relevant consequences as a function of\nactions. We introduce the notion of a Berk-Nash equilibrium: Each player\nfollows a strategy that is optimal given her belief, and her belief is\nrestricted to be the best fit among the set of beliefs she considers possible.\nThe notion of best fit is formalized in terms of minimizing the\nKullback-Leibler divergence, which is endogenous and depends on the equilibrium\nstrategy profile. Standard solution concepts such as Nash equilibrium and\nself-confirming equilibrium constitute special cases where players have\ncorrectly-specified models. We provide a learning foundation for Berk-Nash\nequilibrium by extending and combining results from the statistics literature\non misspecified learning and the economics literature on learning in games.\n"
    },
    {
        "paper_id": 1411.1229,
        "authors": "Peter Bank, Yan Dolinsky, and Selim G\\\"okay",
        "title": "Super-replication with nonlinear transaction costs and volatility\n  uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study super-replication of contingent claims in an illiquid market with\nmodel uncertainty. Illiquidity is captured by nonlinear transaction costs in\ndiscrete time and model uncertainty arises as our only assumption on stock\nprice returns is that they are in a range specified by fixed volatility bounds.\nWe provide a dual characterization of super-replication prices as a supremum of\npenalized expectations for the contingent claim's payoff. We also describe the\nscaling limit of this dual representation when the number of trading periods\nincreases to infinity. Hence, this paper complements the results in [11] and\n[19] for the case of model uncertainty.\n"
    },
    {
        "paper_id": 1411.1348,
        "authors": "Raffaella Calabrese and Silvia Osmetti",
        "title": "Modelling cross-border systemic risk in the European banking sector: a\n  copula approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new methodology based on the Marshall-Olkin (MO) copula to model\ncross-border systemic risk. The proposed framework estimates the impact of the\nsystematic and idiosyncratic components on systemic risk. Initially, we propose\na maximum-likelihood method to estimate the parameter of the MO copula. In\norder to use the data on non-distressed banks for these estimates, we consider\ntimes to bank failures as censored samples. Hence, we propose an estimation\nprocedure for the MO copula on censored data. The empirical evidence from\nEuropean banks shows that the proposed censored model avoid possible\nunderestimation of the contagion risk.\n"
    },
    {
        "paper_id": 1411.1356,
        "authors": "Yoshiharu Maeno, Kenji Nishiguchi, Satoshi Morinaga, Hirokazu\n  Matsushima",
        "title": "Impact of credit default swaps on financial contagion",
        "comments": "presented at the IEEE Computational Intelligence for Financial\n  Engineering and Economics, London, March 2014",
        "journal-ref": null,
        "doi": "10.1109/CIFEr.2014.6924067",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It had been believed in the conventional practice that the risk of a bank\ngoing bankrupt is lessened in a straightforward manner by transferring the risk\nof loan defaults. But the failure of American International Group in 2008 posed\na more complex aspect of financial contagion. This study presents an extension\nof the asset network systemic risk model (ANWSER) to investigate whether credit\ndefault swaps mitigate or intensify the severity of financial contagion. A\nprotection buyer bank transfers the risk of every possible debtor bank default\nto protection seller banks. The empirical distribution of the number of bank\nbankruptcies is obtained with the extended model. Systemic capital buffer ratio\nis calculated from the distribution. The ratio quantifies the effective loss\nabsorbency capability of the entire financial system to force back financial\ncontagion. The key finding is that the leverage ratio is a good estimate of a\nsystemic capital buffer ratio as the backstop of a financial system. The risk\ntransfer from small and medium banks to big banks in an interbank network does\nnot mitigate the severity of financial contagion.\n"
    },
    {
        "paper_id": 1411.1368,
        "authors": "Cy Maor and Eilon Solan",
        "title": "Cooperation under Incomplete Information on the Discount Factors",
        "comments": "appears in International Journal of Game Theory, 2014",
        "journal-ref": "Int J Game Theory (2015) 44: 321",
        "doi": "10.1007/s00182-014-0431-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In repeated games, cooperation is possible in equilibrium only if players are\nsufficiently patient, and long-term gains from cooperation outweigh short-term\ngains from deviation. What happens if the players have incomplete information\nregarding each other's discount factors? In this paper we look at repeated\ngames in which each player has incomplete information regarding the other\nplayer's discount factor, and ask when full cooperation can arise in\nequilibrium. We provide necessary and sufficient conditions that allow full\ncooperation in equilibrium that is composed of grim trigger strategies, and\ncharacterize the states of the world in which full cooperation occurs. We then\nask whether these \"cooperation events\" are close to those in the complete\ninformation case, when the information on the other player's discount factor is\n\"almost\" complete.\n"
    },
    {
        "paper_id": 1411.156,
        "authors": "Maciej Jagielski, Rafa{\\l} Duczmal, Ryszard Kutner",
        "title": "Income Distribution in the European Union Versus in the United States",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2015.03.071",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the refined approach -- our extension of the Yakovenko et al.\nformalism -- is universal in the sense that it describes well both household\nincomes in the European Union and the individual incomes in the United States\nfor social classes of any income. This formalism allowed the study of the\nimpact of the recent world-wide financial crisis on the annual incomes of\ndifferent social classes. Hence, we indicate the existence of a possible\nprecursor of a market crisis. Besides, we find the most painful impact of the\ncrisis on incomes of all social classes.\n"
    },
    {
        "paper_id": 1411.1609,
        "authors": "Halim Zeghdoudi, Meriem Bouhadjar and Mohamed Riad Remita",
        "title": "On Stochastic Orders and its applications : Policy limits and\n  Deductibles",
        "comments": "This paper has been withdrawn by the author due to a crucial sign\n  errors in typing and mathematical formulas",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on stochastic orders and its applications : policy limits\nand deductibles. Further, many applications and some examples are given :\ncomparison of two families of copulas, individual and collective risk model,\nreinsurance contracts and dependent portfolios increase risk. More precisely,\nwe propose a new model for insurance risks while we give some properties. To\nthis end, we obtain the ordering of the optimal allocation of policy limits and\ndeductibles for this model.\n"
    },
    {
        "paper_id": 1411.1624,
        "authors": "Francesco Caravenna, Jacopo Corbetta",
        "title": "General smile asymptotics with bounded maturity",
        "comments": "35 pages, 2 figures. To appear on SIAM Journal on Financial\n  Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide explicit conditions on the distribution of risk-neutral\nlog-returns which yield sharp asymptotic estimates on the implied volatility\nsmile. We allow for a variety of asymptotic regimes, including both small\nmaturity (with arbitrary strike) and extreme strike (with arbitrary bounded\nmaturity), extending previous work of Benaim and Friz [Math. Finance 19 (2009),\n1-12]. We present applications to popular models, including Carr-Wu finite\nmoment logstable model, Merton's jump diffusion model and Heston's model.\n"
    },
    {
        "paper_id": 1411.1689,
        "authors": "Mateusz Denys, Tomasz Gubiec, Ryszard Kutner",
        "title": "Universality of Tsallis q-exponential of interoccurrence times within\n  the microscopic model of cunning agents",
        "comments": "3 pages, 2 figures, SMSEC 2014 conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We proposed the agent-based model of financial markets where agents (or\ntraders) are represented by three-state spins located on the plane lattice or\nsocial network. The spin variable represents only the individual opinion\n(advice) that each trader gives to his nearest neighbors. In the model the\nagents can be considered as cunning. For instance, although agent having\ncurrently a maximal value of the spin advises his nearest neighbors to buy some\nstocks he, perfidiously, will sell some stocks in the next Monte Carlo step or\nwill occupy a neutral position. In general, the trader has three possibilities:\nhe can buy some stocks if his opinion change within a single time step is\npositive, sell some stocks if this change is negative, or remain inactive if\nhis opinion is unchanged. The predictions of our model, found by simulations,\nwell agree with the empirical universal distribution of interoccurrence times\nbetween daily losses below negative thresholds following the Tsallis\nq-exponential.\n"
    },
    {
        "paper_id": 1411.1924,
        "authors": "Daniel Wilson-Nunn and Hector Zenil",
        "title": "On the Complexity and Behaviour of Cryptocurrencies Compared to Other\n  Markets",
        "comments": "16 pages, 11 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the behaviour of Bitcoin has interesting similarities to stock\nand precious metal markets, such as gold and silver. We report that whilst\nLitecoin, the second largest cryptocurrency, closely follows Bitcoin's\nbehaviour, it does not show all the reported properties of Bitcoin. Agreements\nbetween apparently disparate complexity measures have been found, and it is\nshown that statistical, information-theoretic, algorithmic and fractal measures\nhave different but interesting capabilities of clustering families of markets\nby type. The report is particularly interesting because of the range and novel\nuse of some measures of complexity to characterize price behaviour, because of\nthe IRS designation of Bitcoin as an investment property and not a currency,\nand the announcement of the Canadian government's own electronic currency\nMintChip.\n"
    },
    {
        "paper_id": 1411.1929,
        "authors": "W.P. Weijland",
        "title": "A General Equilibrium Theorem for the Economy of Giving",
        "comments": "14 pages, nine figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In [1] we presented a model for transactions when goods are given away in the\nexpectation of a later settlement. In settings where people keep track of their\nsocial accounts we were able to redefine concepts like account balance, yield\ncurve and the law of diminishing returns. In this paper we establish a general\nequilibrium theorem, conjectured in [1], by developing sufficient conditions\nfor any instance of the standard model (or Gift Economy Model) to have a unique\nequilibrium. The convergence to that equilibrium is exponential and for each\npair of entities P and Q the total sum of yields from all mutual transactions\nis equal to zero.\n  [1] W.P. Weijland, Mathematical Foundations for the Economy of Giving, ArXiv\nCategories: q-fin.GN, Report 1401.4664, 2014.\n"
    },
    {
        "paper_id": 1411.2138,
        "authors": "Stefano Bartolini and Francesco Sarracino",
        "title": "It's not the economy, stupid! How social capital and GDP relate to\n  happiness over time",
        "comments": "48 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What predicts the evolution over time of subjective well-being? We correlate\nthe trends of subjective well-being with the trends of social capital and/or\nGDP. We find that in the long and medium run social capital largely predicts\nthe trends of subjective wellbeing in our sample of countries. In the\nshort-term this relationship weakens. Indeed, in the short run, changes in\nsocial capital predict a much smaller portion of the changes in subjective\nwell-being than over longer periods. GDP follows a reverse path, thus\nconfirming the Easterlin paradox: in the short run GDP is more positively\ncorrelated to well-being than in the medium-term, while in the long run this\ncorrelation vanishes.\n"
    },
    {
        "paper_id": 1411.2153,
        "authors": "Simone Cirillo, Stefan Lloyd, Peter Nordin",
        "title": "Evolving intraday foreign exchange trading strategies utilizing multiple\n  instruments price series",
        "comments": "15 pages, 10 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a Genetic Programming architecture for the generation of foreign\nexchange trading strategies. The system's principal features are the evolution\nof free-form strategies which do not rely on any prior models and the\nutilization of price series from multiple instruments as input data. This\nlatter feature constitutes an innovation with respect to previous works\ndocumented in literature. In this article we utilize Open, High, Low, Close bar\ndata at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPY\ncurrency pairs. We will test the implementation analyzing the in-sample and\nout-of-sample performance of strategies for trading the USD.JPY obtained across\nmultiple algorithm runs. We will also evaluate the differences between\nstrategies selected according to two different criteria: one relies on the\nfitness obtained on the training set only, the second one makes use of an\nadditional validation dataset. Strategy activity and trade accuracy are\nremarkably stable between in and out of sample results. From a profitability\naspect, the two criteria both result in strategies successful on out-of-sample\ndata but exhibiting different characteristics. The overall best performing\nout-of-sample strategy achieves a yearly return of 19%.\n"
    },
    {
        "paper_id": 1411.2167,
        "authors": "Shidong Wang, Renaud Foucart, Cheng Wan",
        "title": "Comeback kids: an evolutionary approach of the long-run innovation\n  process",
        "comments": "26 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a theoretical framework to understand when firms may benefit from\nexploiting previously abandoned technologies and brands. We model for the long\nrun process of innovation, allowing for sustainable diversity and comebacks of\nold brands and technologies. We present two extensions to the logistic and\nLotka-Volterra equations, which describe the diffusion of an innovation. First,\nwe extend the short-term competition to a long-term process characterized by a\nsequence of innovations and substitutions. Second, by allowing the\nsubstitutions to be incomplete, we extend the one-dimensional process to a\ntree-form multidimensional one featuring diversification throughout the\nlong-term development.\n"
    },
    {
        "paper_id": 1411.2215,
        "authors": "Shingo Ichiki and Katsuhiro Nishinari",
        "title": "Simple Stochastic Order-Book Model of Swarm Behavior in Continuous\n  Double Auction",
        "comments": "15 pages, 10 figures, 4 tables",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.11.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we present a simple stochastic order-book model for investors'\nswarm behaviors seen in the continuous double auction mechanism, which is\nemployed by major global exchanges. Our study shows a characteristic called\n\"fat tail\" is seen in the data obtained from our model that incorporates the\ninvestors' swarm behaviors. Our model captures two swarm behaviors: one is\ninvestors' behavior to follow a trend in the historical price movement, and\nanother is investors' behavior to send orders that contradict a trend in the\nhistorical price movement. In order to capture the features of influence by the\nswarm behaviors, from price data derived from our simulations using these\nmodels, we analyzed the price movement range, that is, how much the price is\nmoved when it is continuously moved in a single direction. Depending on the\ntype of swarm behavior, we saw a difference in the cumulative frequency\ndistribution of this price movement range. In particular, for the model of\ninvestors who followed a trend in the historical price movement, we saw the\npower law in the tail of the cumulative frequency distribution of this price\nmovement range. In addition, we analyzed the shape of the tail of the\ncumulative frequency distribution. The result demonstrated that one of the\nreasons the trend following of price occurs is that orders temporarily swarm on\nthe order book in accordance with past price trends.\n"
    },
    {
        "paper_id": 1411.2395,
        "authors": "Giorgio Ferrari, Paavo Salminen",
        "title": "Irreversible Investment under L\\'evy Uncertainty: an Equation for the\n  Optimal Boundary",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a new equation for the optimal investment boundary of a general\nirreversible investment problem under exponential L\\'evy uncertainty. The\nproblem is set as an infinite time-horizon, two-dimensional degenerate singular\nstochastic control problem. In line with the results recently obtained in a\ndiffusive setting, we show that the optimal boundary is intimately linked to\nthe unique optional solution of an appropriate Bank-El Karoui representation\nproblem. Such a relation and the Wiener Hopf factorization allow us to derive\nan integral equation for the optimal investment boundary. In case the\nunderlying L\\'evy process hits any real point with positive probability we show\nthat the integral equation for the investment boundary is uniquely satisfied by\nthe unique solution of another equation which is easier to handle. As a\nremarkable by-product we prove the continuity of the optimal investment\nboundary. The paper is concluded with explicit results for profit functions of\n(i) Cobb-Douglas type and (ii) CES type. In the first case the function is\nseparable and in the second case non-separable.\n"
    },
    {
        "paper_id": 1411.2525,
        "authors": "Boguk Kim, Chulwoo Han, Frank Chongwoo Park",
        "title": "Optimising Credit Portfolio Using a Quadratic Nonlinear Projection\n  Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A novel optimisation framework through quadratic nonlinear projection is\nintroduced for credit portfolio when the portfolio risk is measured by\nConditional Value-at-Risk (CVaR). The whole optimisation procedure to search\ntoward the optimal portfolio state is conducted by a series of single-step\noptimisations under the local constraints described in the multi-dimensional\nconstraint parameter space as functions of the total amount of portfolio\nadjustment. Each single-step optimisation is approximated by the first-order\nvariation of the weight increments with respect to the total amount of\nportfolio adjustment and is solved in the form of locally exact formula\nformulated in the general Lagrange multiplier method. Our method can deal with\noptimisation for general nonlinear objective functions, such as the\nreturn-to-risk ratio maximisation or the diversification index, as well as the\nrisk minimisation or the return maximisation.\n"
    },
    {
        "paper_id": 1411.2628,
        "authors": "Liviu-Adrian Cotfas, Camelia Delcea, Nicolae Cotfas",
        "title": "Exact solution of a generalized version of the Black-Scholes equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a generalized version of the Black-Scholes equation depending on a\nparameter $a\\!\\in \\!(-\\infty,0)$. It satisfies the martingale condition and\ncoincides with the Black-Scholes equation in the limit case $a\\nearrow 0$. We\nshow that the generalized equation is exactly solvable in terms of Hermite\npolynomials and numerically compare its solution with the solution of the\nBlack-Scholes equation.\n"
    },
    {
        "paper_id": 1411.2675,
        "authors": "Jingnan Fan, Andrzej Ruszczynski",
        "title": "Process-Based Risk Measures and Risk-Averse Control of Discrete-Time\n  Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For controlled discrete-time stochastic processes we introduce a new class of\ndynamic risk measures, which we call process-based. Their main features are\nthat they measure risk of processes that are functions of the history of a base\nprocess. We introduce a new concept of conditional stochastic time consistency\nand we derive the structure of process-based risk measures enjoying this\nproperty. We show that they can be equivalently represented by a collection of\nstatic law-invariant risk measures on the space of functions of the state of\nthe base process. We apply this result to controlled Markov processes and we\nderive dynamic programming equations.\n"
    },
    {
        "paper_id": 1411.2835,
        "authors": "Jos\\'e Manuel Corcuera, Giulia Di Nunno, Gergely Farkas, and Bernt\n  {\\O}ksendal",
        "title": "A continuous auction model with insiders and random time of information\n  release",
        "comments": "Withdrawn by the authors due to an error in Theorem 17",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a unified framework we study equilibrium in the presence of an insider\nhaving information on the signal of the firm value, which is naturally\nconnected to the fundamental price of the firm related asset. The fundamental\nvalue itself is announced at a future random (stopping) time. We consider two\ncases. First when the release time of information is known to the insider and\nthen when it is unknown also to her. Allowing for very general dynamics, we\nstudy the structure of the insider's optimal strategies in equilibrium and we\ndiscuss market efficiency. In particular, we show that in the case the insider\nknows the information release time, the market is fully efficient. In the case\nthe insider does not know this random time, we see that there is an equilibrium\nwith no full efficiency, but where the sensitivity of prices is decreasing in\ntime according with the probability that the announcement time is greater than\nthe current time. In other words, the prices become more and more stable as the\nannouncement approaches.\n"
    },
    {
        "paper_id": 1411.295,
        "authors": "Peter B. Lerner",
        "title": "Algebraic Form of Malliavin Calculus: Creation-Annihilation Operators,\n  Conserved Currents and All That",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The extremely useful method of Malliavin calculus has not yet gained adequate\npopularity because of the complicated analytic apparatus of this method. The\nauthor attempts here to propose a simplified algebraic formalism similar to\nMalliavin calculus, but based on the notion of creation-annihilation operators\ninstead of Malliavin derivative to replace analytic theorems with algebraic\ncomputations. Three test problems: the valuation of portfolio with stochastic\npayoff function, the expression of the terminal payoff through stochastic\nintegral and the approximate equation for the high-frequency market measure are\ndiscussed in Appendices.\n"
    },
    {
        "paper_id": 1411.3075,
        "authors": "Likuan Qin and Vadim Linetsky",
        "title": "Positive Eigenfunctions of Markovian Pricing Operators:\n  Hansen-Scheinkman Factorization, Ross Recovery and Long-Term Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a spectral theory of Markovian asset pricing models where\nthe underlying economic uncertainty follows a continuous-time Markov process X\nwith a general state space (Borel right process (BRP)) and the stochastic\ndiscount factor (SDF) is a positive semimartingale multiplicative functional of\nX. A key result is the uniqueness theorem for a positive eigenfunction of the\npricing operator such that X is recurrent under a new probability measure\nassociated with this eigenfunction (recurrent eigenfunction). As economic\napplications, we prove uniqueness of the Hansen and Scheinkman (2009)\nfactorization of the Markovian SDF corresponding to the recurrent\neigenfunction, extend the Recovery Theorem of Ross (2015) from discrete time,\nfinite state irreducible Markov chains to recurrent BRPs, and obtain the long\nmaturity asymptotics of the pricing operator. When an asset pricing model is\nspecified by given risk-neutral probabilities together with a short rate\nfunction of the Markovian state, we give sufficient conditions for existence of\na recurrent eigenfunction and provide explicit examples in a number of\nimportant financial models, including affine and quadratic diffusion models and\nan affine model with jumps. These examples show that the recurrence assumption,\nin addition to fixing uniqueness, rules out unstable economic dynamics, such as\nthe short rate asymptotically going to infinity or to a zero lower bound trap\nwithout possibility of escaping.\n"
    },
    {
        "paper_id": 1411.3078,
        "authors": "Likuan Qin and Vadim Linetsky",
        "title": "Long Term Risk: A Martingale Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the long-term factorization of the stochastic discount\nfactor introduced and studied by Alvarez and Jermann (2005) in discretetime\nergodic environments and by Hansen and Scheinkman (2009) and Hansen (2012) in\nMarkovian environments to general semimartingale environments. The transitory\ncomponent discounts at the stochastic rate of return on the long bond and is\nfactorized into discounting at the long-term yield and a positive\nsemimartingale that extends the principal eigenfunction of Hansen and\nScheinkman (2009) to the semimartingale setting. The permanent component is a\nmartingale that accomplishes a change of probabilities to the long forward\nmeasure, the limit of T-forward measures. The change of probabilities from the\ndata generating to the long forward measure absorbs the long-term risk-return\ntrade-off and interprets the latter as the long-term risk-neutral measure.\n"
    },
    {
        "paper_id": 1411.3399,
        "authors": "Javier Morales, V\\'ictor Tercero, Fernando Camacho, Eduardo Cordero,\n  Luis L\\'opez, F-Javier Almaguer",
        "title": "Trend and Fractality Assessment of Mexico's Stock Exchange",
        "comments": "19 pages and 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The total value of domestic market capitalization of the Mexican Stock\nExchange was calculated at 520 billion of dollars by the end of November 2013.\nTo manage this system and make optimum capital investments, its dynamics needs\nto be predicted. However, randomness within the stock indexes makes forecasting\na difficult task. To address this issue, in this work, trends and fractality\nwere studied using GNU-R over the opening and closing prices indexes over the\npast 23 years. Returns, Kernel density estimation, autocorrelation function and\nR/S analysis and the Hurst exponent were used in this research. As a result, it\nwas found that the Kernel estimation density and the autocorrelation function\nshown the presence of long-range memory effects. In a first approximation, the\nreturns of closing prices seems to behave according to a Markovian random walk\nwith a length of step size given by an alpha-stable random process. For extreme\nvalues, returns decay asymptotically as a power law with a characteristic\nexponent approximately equal to 2.5.\n"
    },
    {
        "paper_id": 1411.3615,
        "authors": "Ricardo P\\'erez-Marco",
        "title": "Kelly criterion for variable pay-off",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine Kelly criterion for a game with variable pay-off. The Kelly\nfraction satisfies a fundamental integral equation and is smaller than the\nclassical Kelly fraction for the same game with the constant average pay-off.\n"
    },
    {
        "paper_id": 1411.3618,
        "authors": "Ben Hambly, Matthieu Mariapragassam, Christoph Reisinger",
        "title": "A Forward Equation for Barrier Options under the Brunick&Shreve\n  Markovian Projection",
        "comments": "20 pages, Quantitative Finance Volume 16, 2016 - Issue 6",
        "journal-ref": null,
        "doi": "10.1080/14697688.2015.1099718",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a forward equation for arbitrage-free barrier option prices, in\nterms of Markovian projections of the stochastic volatility process, in\ncontinuous semi-martingale models. This provides a Dupire-type formula for the\ncoefficient derived by Brunick and Shreve for their mimicking diffusion and can\nbe interpreted as the canonical extension of local volatility for barrier\noptions. Alternatively, a forward partial-integro differential equation (PIDE)\nis introduced which provides up-and-out call prices, under a Brunick-Shreve\nmodel, for the complete set of strikes, barriers and maturities in one solution\nstep. Similar to the vanilla forward PDE, the above-named forward PIDE can\nserve as a building block for an efficient calibration routine including\nbarrier option quotes. We provide a discretisation scheme for the PIDE as well\nas a numerical validation.\n"
    },
    {
        "paper_id": 1411.3947,
        "authors": "Antoine E. Zambelli",
        "title": "Incorporating Views on Market Dynamics in Options Hedging",
        "comments": "6 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the possibility of incorporating information or views of market\nmovements during the holding period of a portfolio, in the hedging of European\noptions with respect to the underlying. Given a fixed holding period interval,\nwe explore whether it is possible to adjust the number of shares needed to\neffectively hedge our position to account for views on market dynamics from\npresent until the end of our interval, to account for the time-dependence of\nthe options' sensitivity to the underlying. We derive an analytical expression\nfor the number of shares needed by adjusting the standard Black-Scholes-Merton\n$\\Delta$ quantity, in the case of an arbitrary process for implied volatility,\nand we present numerical results.\n"
    },
    {
        "paper_id": 1411.3977,
        "authors": "Chiara Sabelli, Michele Pioppi, Luca Sitzia and Giacomo Bormetti",
        "title": "Multi-curve HJM modelling for risk management",
        "comments": "63 pages, 18 figures, 15 tables; new calibration procedure to\n  estimate market risk-premia; new forecasting methodology; added references;\n  minor revisions to the text",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a HJM approach to the projection of multiple yield curves\ndeveloped to capture the volatility content of historical term structures for\nrisk management purposes. Since we observe the empirical data at daily\nfrequency and only for a finite number of time-to-maturity buckets, we propose\na modelling framework which is inherently discrete. In particular, we show how\nto approximate the HJM continuous time description of the multi-curve dynamics\nby a Vector Autoregressive process of order one. The resulting dynamics lends\nitself to a feasible estimation of the model volatility-correlation structure\nand market risk-premia. Then, resorting to the Principal Component Analysis we\nfurther simplify the dynamics reducing the number of covariance components.\nApplying the constant volatility version of our model on a sample of curves\nfrom the Euro area, we demonstrate its forecasting ability through an\nout-of-sample test.\n"
    },
    {
        "paper_id": 1411.4193,
        "authors": "Peter Spoida",
        "title": "Characterization of Market Models in the Presence of Traded Vanilla and\n  Barrier Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize the set of market models when there are a finite number of\ntraded Vanilla and Barrier options with maturity $T$ written on the asset $S$.\nFrom a probabilistic perspective, our result describes the set of joint\ndistributions for $(S_T, \\sup_{u \\leq T} S_u)$ when a finite number of marginal\nlaw constraints on both $S_T$ and $\\sup_{u \\leq T} S_u$ is imposed. An\nextension to the case of multiple maturities is obtained.\n  Our characterization requires a decomposition of the call price function and\nonce it is obtained, we can explicitly express certain joint probabilities in\nthis model. In order to obtain a fully specified joint distribution we discuss\ninterpolation methods.\n"
    },
    {
        "paper_id": 1411.4265,
        "authors": "Wolfgang Reitgruber",
        "title": "Methodological thoughts on expected loss estimates for IFRS 9\n  impairment: hidden reserves, cyclical loss predictions and LGD backtesting",
        "comments": "31 pages. Forthcoming in Credit Technology by Serasa Experian, No 92,\n  Sept 2015 (in English and Portuguese with local market additions by Carlos\n  Antonio Campos Nogueira)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After the release of the final accounting standards for impairment in July\n2014 by the IASB, banks will face the next significant methodological challenge\nafter Basel 2. In this paper, first methodological thoughts are presented, and\nways how to approach underlying questions are proposed. It starts with a\ndetailed discussion of the structural conservatism in the final standard. The\nexposure value iACV(c) (idealized Amortized Cost Value), as originally\nintroduced in the Exposure Draft 2009 (ED 2009), will be interpreted as\neconomic value under amortized cost accounting and provides the valuation\nbenchmark under IFRS 9. Consequently, iACV(c) can be used to quantify\nconservatism (ie potential hidden reserves) in the actual implementation of the\nfinal standard and to separate operational side-effects caused by the local\nimplementation from actual credit risk impacts. The second part continues with\na quantification of expected credit losses based on Impact of Risk(c) instead\nof traditional cost of risk measures. An objective framework is suggested which\nallows for improved testing of forward looking credit risk estimates during\ncredit cycles. This framework will prove useful to mitigate overly pro-cyclical\nprovisioning and to reduce earnings volatility. Finally, an LGD monitoring and\nbacktesting approach, applicable under regulatory requirements and accounting\nstandards as well, is proposed. On basis of the NPL Dashboard, part of the\nImpact of Risk(c) framework, specific key risk indicators are introduced that\nallow for a detailed assessment of collections performance versus LGD in in NPL\nportfolio (bucket 3).\n"
    },
    {
        "paper_id": 1411.4438,
        "authors": "Randall Martyr",
        "title": "Solving finite time horizon Dynkin games by optimal switching",
        "comments": "17 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1017/jpr.2016.57",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses recent results on continuous-time finite-horizon optimal\nswitching problems with negative switching costs to prove the existence of a\nsaddle point in an optimal stopping (Dynkin) game. Sufficient conditions for\nthe game's value to be continuous with respect to the time horizon are obtained\nusing recent results on norm estimates for doubly reflected backward stochastic\ndifferential equations. This theory is then demonstrated numerically for the\nspecial cases of cancellable call and put options in a Black-Scholes market.\n"
    },
    {
        "paper_id": 1411.4441,
        "authors": "Kerem Ugurlu",
        "title": "On the Coherent Risk Measure Representations in the Discrete Probability\n  Spaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a complete characterization of both comonotone and not comonotone\ncoherent risk measures in the discrete finite probability space, where each\noutcome is equally likely. To the best of our knowledge, this is the first work\nthat characterizes \\textit{and} distinguishes comonotone and not comonotone\ncoherent risk measures via a simplified AVaR representation in this probability\nspace, which is crucial in applications and simulations.\n"
    },
    {
        "paper_id": 1411.4606,
        "authors": "Jihun Han, Hyungbin Park",
        "title": "The Intrinsic Bounds on the Risk Premium of Markovian Pricing Kernels",
        "comments": "arXiv admin note: text overlap with arXiv:1410.2282",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The risk premium is one of main concepts in mathematical finance. It is a\nmeasure of the trade-offs investors make between return and risk and is defined\nby the excess return relative to the risk-free interest rate that is earned\nfrom an asset per one unit of risk. The purpose of this article is to determine\nupper and lower bounds on the risk premium of an asset based on the market\nprices of options. One of the key assumptions to achieve this goal is that the\nmarket is Markovian. Under this assumption, we can transform the problem of\nfinding the bounds into a second-order differential equation. We then obtain\nupper and lower bounds on the risk premium by analyzing the differential\nequation.\n"
    },
    {
        "paper_id": 1411.4633,
        "authors": "Angus O. Unegbu",
        "title": "Theories of Accounting: Evolution & Developments, Income-Determination\n  and Diversities in Use",
        "comments": "Keywords: Review of Accounting Theories, Financial Reporting,\n  Corporate Reports, Financial Statements, Developments in Accounting",
        "journal-ref": "Research Journal of Finance and Accounting, Vol. 5(19), 2014.\n  http://www.iiste.org/Journals/index.php/RJFA/article/download/16836/17176",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accounting frameworks follow stipulations of existing Accounting Theories.\nThis exploratory research sets out to trace the evolution of accounting\ntheories of Charge and Discharge Syndrome and the Corollary of Double Entry.\nFurthermore, it dives into the theories of Income Determination, garnishing it\nwith areas of diversities in the use of Accounting Information while review of\ntheories of recent growths and developments in Accounting are not left out. The\nmethod of research adopted is exploratory review of existing accounting\nliterature. It is observed that the emergence of these theories exist to\nminimize fraud, errors, misappropriations and pilfering of Corporate assets. It\nis recommended that implementation prescriptions of these theories by\nInternational Financial Reporting Standard Committee and Practicing Accountants\nshould be adhered to and simplified so as to avoid confusing and scandalous\nreporting of financial statements\n"
    },
    {
        "paper_id": 1411.4756,
        "authors": "Gabriell Mate and Zoltan Neda",
        "title": "Diversification versus specialization -- lessons from a noise driven\n  linear dynamical system",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Specialization and diversification are two major strategies that complex\nsystems might exploit. Given a fixed amount of resources, the question is\nwhether to invest this in elements that respond in a correlated manner to\nexternal perturbations, or to build a diversified system with groups of\nelements that respond in a not necessarily correlated manner. This general\ndilemma is investigated here using a high dimensional discrete dynamical system\nsubject to an external noise, analyzing the statistical properties of an order\nparameter that quantifies growth. Our analytical solution suggests that\ndiversification is a good strategy once the system has a fair amount of\nresources. For systems with small or extremely large supplies, we argue that\nspecialization might be a more successful strategy. We discuss the results also\nfrom the perspective of economic and biologic systems.\n"
    },
    {
        "paper_id": 1411.4851,
        "authors": "Frank Gehmlich and Thorsten Schmidt",
        "title": "Dynamic Defaultable Term Structure Modelling beyond the Intensity\n  Paradigm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The two main approaches in credit risk are the structural approach pioneered\nin Merton (1974) and the reduced-form framework proposed in Jarrow & Turnbull\n(1995) and in Artzner & Delbaen (1995). The goal of this article is to provide\na unified view on both approaches. This is achieved by studying reduced-form\napproaches under weak assumptions. In particular we do not assume the global\nexistence of a default intensity and allow default at fixed or predictable\ntimes with positive probability, such as coupon payment dates.\n  In this generalized framework we study dynamic term structures prone to\ndefault risk following the forward-rate approach proposed in\nHeath-Jarrow-Morton (1992). It turns out, that previously considered models\nlead to arbitrage possibilities when default may happen at a predictable time\nwith positive probability. A suitable generalization of the forward-rate\napproach contains an additional stochastic integral with atoms at predictable\ntimes and necessary and sufficient conditions for a suitable no-arbitrage\ncondition (NAFL) are given. In the view of efficient implementations we develop\na new class of affine models which do not satisfy the standard assumption of\nstochastic continuity.\n  The chosen approach is intimately related to the theory of enlargement of\nfiltrations, to which we provide a small example by means of filtering theory\nwhere the Azema supermartingale contains upward and downward jumps, both at\npredictable and totally inaccessible stopping times.\n"
    },
    {
        "paper_id": 1411.497,
        "authors": "David Walsh-Jones, Daniel Jones, Christoph Reisinger",
        "title": "Modelling of dependence in high-dimensional financial time series by\n  cluster-derived canonical vines",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend existing models in the financial literature by introducing a\ncluster-derived canonical vine (CDCV) copula model for capturing high\ndimensional dependence between financial time series. This model utilises a\nsimplified market-sector vine copula framework similar to those introduced by\nHeinen and Valdesogo (2008) and Brechmann and Czado (2013), which can be\napplied by conditioning asset time series on a market-sector hierarchy of\nindexes. While this has been shown by the aforementioned authors to control the\nexcessive parameterisation of vine copulas in high dimensions, their models\nhave relied on the provision of externally sourced market and sector indexes,\nlimiting their wider applicability due to the imposition of restrictions on the\nnumber and composition of such sectors. By implementing the CDCV model, we\ndemonstrate that such reliance on external indexes is redundant as we can\nachieve equivalent or improved performance by deriving a hierarchy of indexes\ndirectly from a clustering of the asset time series, thus abstracting the\nmodelling process from the underlying data.\n"
    },
    {
        "paper_id": 1411.5062,
        "authors": "Tim Leung and Xin Li",
        "title": "Optimal Mean Reversion Trading with Transaction Costs and Stop-Loss Exit",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance, Vol. 18,\n  No. 3, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the industry practice of pairs trading, we study the optimal\ntiming strategies for trading a mean-reverting price spread. An optimal double\nstopping problem is formulated to analyze the timing to start and subsequently\nliquidate the position subject to transaction costs. Modeling the price spread\nby an Ornstein-Uhlenbeck process, we apply a probabilistic methodology and\nrigorously derive the optimal price intervals for market entry and exit. As an\nextension, we incorporate a stop-loss constraint to limit the maximum loss. We\nshow that the entry region is characterized by a bounded price interval that\nlies strictly above the stop-loss level. As for the exit timing, a higher\nstop-loss level always implies a lower optimal take-profit level. Both\nanalytical and numerical results are provided to illustrate the dependence of\ntiming strategies on model parameters such as transaction cost and stop-loss\nlevel.\n"
    },
    {
        "paper_id": 1411.5159,
        "authors": "Hac\\`ene Djellout, Arnaud Guillin, Yacouba Samoura",
        "title": "Large deviations of the realized (co-)volatility vector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Realized statistics based on high frequency returns have become very popular\nin financial economics. In recent years, different non-parametric estimators of\nthe variation of a log-price process have appeared. These were developed by\nmany authors and were motivated by the existence of complete records of price\ndata. Among them are the realized quadratic (co-)variation which is perhaps the\nmost well known example, providing a consistent estimator of the integrated\n(co-)volatility when the logarithmic price process is continuous. Limit results\nsuch as the weak law of large numbers or the central limit theorem have been\nproved in different contexts. In this paper, we propose to study the large\ndeviation properties of realized (co-)volatility (i.e., when the number of high\nfrequency observations in a fixed time interval increases to infinity. More\nspecifically, we consider a bivariate model with synchronous observation\nschemes and correlated Brownian motions of the following form: $dX\\_{\\ell,t} =\n\\sigma\\_{\\ell,t}dB\\_{\\ell,t}+b\\_{\\ell}(t,\\omega)dt$ for $\\ell=1,2$, where\n$X\\_{\\ell}$ denotes the log-price, we are concerned with the large deviation\nestimation of the vector $V\\_t^n(X)=(Q\\_{1,t}^n(X), Q\\_{2,t}^n(X),\nC\\_{t}^n(X))$ where $Q\\_{\\ell,t}^n(X)$ and $C\\_{t}^n(X)$ represente the\nestimator of the quadratic variational processes\n$Q\\_{\\ell,t}=\\int\\_0^t\\sigma\\_{\\ell,s}^2ds$ and the integrated covariance\n$C\\_t=\\int\\_0^t\\sigma\\_{1,s}\\sigma\\_{2,s}\\rho\\_sds$ respectively, with\n$\\rho\\_t=cov(B\\_{1,t}, B\\_{2,t})$. Our main motivation is to improve upon the\nexisting limit theorems. Our large deviations results can be used to evaluate\nand approximate tail probabilities of realized (co-)volatility. As an\napplication we provide the large deviation for the standard dependence measures\nbetween the two assets returns such as the realized regression coefficients up\nto time $t$, or the realized correlation. Our study should contribute to the\nrecent trend of research on the (co-)variance estimation problems, which are\nquite often discussed in high-frequency financial data analysis.\n"
    },
    {
        "paper_id": 1411.5453,
        "authors": "Xiaolin Luo and Pavel V. Shevchenko",
        "title": "Valuation of Variable Annuities with Guaranteed Minimum Withdrawal and\n  Death Benefits via Stochastic Control Optimization",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1410.8609",
        "journal-ref": "Insurance: Mathematics and Economics 62 (2015) 5-15",
        "doi": "10.1016/j.insmatheco.2015.02.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a numerical valuation of variable annuities with\ncombined Guaranteed Minimum Withdrawal Benefit (GMWB) and Guaranteed Minimum\nDeath Benefit (GMDB) under optimal policyholder behaviour solved as an optimal\nstochastic control problem. This product simultaneously deals with financial\nrisk, mortality risk and human behaviour. We assume that market is complete in\nfinancial risk and mortality risk is completely diversified by selling enough\npolicies and thus the annuity price can be expressed as appropriate\nexpectation. The computing engine employed to solve the optimal stochastic\ncontrol problem is based on a robust and efficient Gauss-Hermite quadrature\nmethod with cubic spline. We present results for three different types of death\nbenefit and show that, under the optimal policyholder behaviour, adding the\npremium for the death benefit on top of the GMWB can be problematic for\ncontracts with long maturities if the continuous fee structure is kept, which\nis ordinarily assumed for a GMWB contract. In fact for some long maturities it\ncan be shown that the fee cannot be charged as any proportion of the account\nvalue -- there is no solution to match the initial premium with the fair\nannuity price. On the other hand, the extra fee due to adding the death benefit\ncan be charged upfront or in periodic instalment of fixed amount, and it is\ncheaper than buying a separate life insurance.\n"
    },
    {
        "paper_id": 1411.5625,
        "authors": "Erika Gomes-Gon\\c{c}alves (UC3M), Henryk Gzyl (IESA) and Silvia\n  Mayoral (UC3M)",
        "title": "Two maxentropic approaches to determine the probability density of\n  compound risk losses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we present an application of two maxentropic procedures to determine the\nprobability density distribution of compound sums of random variables, using\nonly a finite number of empirically determined fractional moments. The two\nmethods are the Standard method of Maximum Entropy (SME), and the method of\nMaximum Entropy in the Mean (MEM). We shall verify that the reconstructions\nobtained satisfy a variety of statistical quality criteria, and provide good\nestimations of VaR and TVaR, which are important measures for risk management\npurposes. We analyze the performance and robustness of these two procedures in\nseveral numerical examples, in which the frequency of losses is Poisson and the\nindividual losses are lognormal random variables. As side product of the work,\nwe obtain a rather accurate description of the density of the compound random\nvariable. This is an extension of a previous application based on the Standard\nMaximum Entropy approach (SME) where the analytic form of the Laplace transform\nwas available to a case in which only observed or simulated data is used. These\napproaches are also used to develop a procedure to determine the distribution\nof the individual losses through the knowledge of the total loss. Then, in the\ncase of having only historical total losses, it is possible to decompound or\ndisaggregate the random sums in its frequency/severity distributions, through a\nprobabilistic inverse problem.\n"
    },
    {
        "paper_id": 1411.608,
        "authors": "Tim Leung, Xin Li, Zheng Wang",
        "title": "Optimal Starting-Stopping and Switching of a CIR Process with Fixed\n  Costs",
        "comments": "To appear in Risk and Decision Analysis",
        "journal-ref": "Risk & Decision Analysis, vol. 5, no.2-3, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the problem of starting and stopping a Cox-Ingersoll-Ross\n(CIR) process with fixed costs. In addition, we also study a related optimal\nswitching problem that involves an infinite sequence of starts and stops. We\nestablish the conditions under which the starting-stopping and switching\nproblems admit the same optimal starting and/or stopping strategies. We\nrigorously prove that the optimal starting and stopping strategies are of\nthreshold type, and give the analytical expressions for the value functions in\nterms of confluent hypergeometric functions. Numerical examples are provided to\nillustrate the dependence of timing strategies on model parameters and\ntransaction costs.\n"
    },
    {
        "paper_id": 1411.625,
        "authors": "Gaurab Aryal and Federico Zincenko",
        "title": "Identification and Estimation of Multidimensional Screening",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the identification and estimation of a multidimensional screening\nmodel, where a monopolist sells a multi-attribute product to consumers with\nprivate information about their multidimensional preferences. Under optimal\nscreening, the seller designs product and payment rules that exclude \"low-type\"\nconsumers, bunches the \"medium types\" at \"medium-quality\" products, and\nperfectly screens the \"high types.\" We determine sufficient conditions to\nidentify the joint distribution of preferences and the marginal costs from data\non optimal individual choices and payments. Then, we propose estimators for\nthese objects, establish their asymptotic properties, and assess their\nsmall-sample performance using Monte Carlo experiments.\n"
    },
    {
        "paper_id": 1411.6256,
        "authors": "Jose Miguel Zapata",
        "title": "Randomized versions of Mazur lemma and Krein-Smulian theorem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend to the framework of locally $L^0$-convex modules some results from\nclassical convex analysis. Namely, randomized versions of Mazur lemma and\nKrein-Smulian theorem under mild stability properties are provided.\n"
    },
    {
        "paper_id": 1411.6657,
        "authors": "Farzad Pourbabaee, Minsuk Kwak and Traian A. Pirvu",
        "title": "Risk minimization and portfolio diversification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of minimizing capital at risk in the Black-Scholes\nsetting. The portfolio problem is studied given the possibility that a\ncorrelation constraint between the portfolio and a financial index is imposed.\nThe optimal portfolio is obtained in closed form. The effects of the\ncorrelation constraint are explored; it turns out that this portfolio\nconstraint leads to a more diversified portfolio.\n"
    },
    {
        "paper_id": 1411.6938,
        "authors": "Sigurd Assing and Yufan Zhao",
        "title": "On Trading American Put Options with Interactive Volatility",
        "comments": "improved version (minor corrections), 28 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a simple stochastic volatility model, whose novelty consists in\ntaking into account hitting times of the asset price, and study the optimal\nstopping problem corresponding to a put option whose time horizon (after the\nasset price hits a certain level) is exponentially distributed. We obtain\nexplicit optimal stopping rules in various cases one of which is interestingly\ncomplex because of an unexpected disconnected continuation region. Finally, we\ndiscuss in detail how these stopping rules could be used for trading an\nAmerican put when the trader expects a market drop in the near future.\n"
    },
    {
        "paper_id": 1411.7231,
        "authors": "Boualem Djehiche and Hamidou Tembine",
        "title": "Risk-Sensitive Mean-Field Type Control under Partial Observation",
        "comments": "arXiv admin note: text overlap with arXiv:1404.1441",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a stochastic maximum principle (SMP) for control problems of\npartially observed diffusions of mean-field type with risk-sensitive\nperformance functionals.\n"
    },
    {
        "paper_id": 1411.7494,
        "authors": "Ronald Hochreiter",
        "title": "An Evolutionary Optimization Approach to Risk Parity Portfolio Selection",
        "comments": null,
        "journal-ref": "Lecture Notes in Computer Science Volume 9028: 279-288. 2015",
        "doi": "10.1007/978-3-319-16549-3_23",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present an evolutionary optimization approach to solve the\nrisk parity portfolio selection problem. While there exist convex optimization\napproaches to solve this problem when long-only portfolios are considered, the\noptimization problem becomes non-trivial in the long-short case. To solve this\nproblem, we propose a genetic algorithm as well as a local search heuristic.\nThis algorithmic framework is able to compute solutions successfully. Numerical\nresults using real-world data substantiate the practicability of the approach\npresented in this paper.\n"
    },
    {
        "paper_id": 1411.7502,
        "authors": "Xuefeng Gao and S.J. Deng",
        "title": "Hydrodynamic limit of order book dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we establish a fluid limit for a two--sided Markov order book\nmodel. Our main result states that in a certain asymptotic regime, a pair of\nmeasure-valued processes representing the \"sell-side shape\" and \"buy-side\nshape\" of an order book converges to a pair of deterministic measure-valued\nprocesses in a certain sense. We also test our fluid approximation on data. The\nempirical results suggest that the approximation is reasonably good for\nliquidly--traded stocks in certain time periods.\n"
    },
    {
        "paper_id": 1411.7593,
        "authors": "Rafael Diaz, Laura Gomez",
        "title": "Indirect Influences in International Trade",
        "comments": null,
        "journal-ref": "Networks and Heterogenous Media 10 (2015) 149-165",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the problem of gauging the influence exerted by a given country on\nthe global trade market from the viewpoint of complex networks. In particular,\nwe apply the PWP method for computing indirect influences on the world trade\nnetwork.\n"
    },
    {
        "paper_id": 1411.7613,
        "authors": "Giulio Cimini, Tiziano Squartini, Diego Garlaschelli, Andrea Gabrielli",
        "title": "Systemic risk analysis in reconstructed economic and financial networks",
        "comments": null,
        "journal-ref": "Sci. Rep. 5 (15758) (2015)",
        "doi": "10.1038/srep15758",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address a fundamental problem that is systematically encountered when\nmodeling complex systems: the limitedness of the information available. In the\ncase of economic and financial networks, privacy issues severely limit the\ninformation that can be accessed and, as a consequence, the possibility of\ncorrectly estimating the resilience of these systems to events such as\nfinancial shocks, crises and cascade failures. Here we present an innovative\nmethod to reconstruct the structure of such partially-accessible systems, based\non the knowledge of intrinsic node-specific properties and of the number of\nconnections of only a limited subset of nodes. This information is used to\ncalibrate an inference procedure based on fundamental concepts derived from\nstatistical physics, which allows to generate ensembles of directed weighted\nnetworks intended to represent the real system, so that the real network\nproperties can be estimated with their average values within the ensemble. Here\nwe test the method both on synthetic and empirical networks, focusing on the\nproperties that are commonly used to measure systemic risk. Indeed, the method\nshows a remarkable robustness with respect to the limitedness of the\ninformation available, thus representing a valuable tool for gaining insights\non privacy-protected economic and financial systems.\n"
    },
    {
        "paper_id": 1411.7653,
        "authors": "Hamza Guennoun, Antoine Jacquier, Patrick Roome, Fangwei Shi",
        "title": "Asymptotic behaviour of the fractional Heston model",
        "comments": "21 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the fractional Heston model originally proposed by Comte, Coutin\nand Renault. Inspired by recent ground-breaking work on rough volatility, which\nshowed that models with volatility driven by fractional Brownian motion with\nshort memory allows for better calibration of the volatility surface and more\nrobust estimation of time series of historical volatility, we provide a\ncharacterisation of the short- and long-maturity asymptotics of the implied\nvolatility smile. Our analysis reveals that the short-memory property precisely\nprovides a jump-type behaviour of the smile for short maturities, thereby\nfixing the well-known standard inability of classical stochastic volatility\nmodels to fit the short-end of the volatility smile.\n"
    },
    {
        "paper_id": 1411.767,
        "authors": "Erwan Pierre, St\\'ephane Villeneuve, Xavier Warin",
        "title": "Liquidity Management with Decreasing-returns-to-scale and Secured Credit\n  Line",
        "comments": "48 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the dividend and investment policies of a cash\nconstrained firm that has access to costly external funding. We depart from the\nliterature by allowing the firm to issue collateralized debt to increase its\ninvestment in productive assets resulting in a performance sensitive interest\nrate on debt. We formulate this problem as a bi-dimensional singular control\nproblem and use both a viscosity solution approch and a verification technique\nto get qualitative properties of the value function. We further solve\nquasi-explicitly the control problem in two special cases.\n"
    },
    {
        "paper_id": 1411.7805,
        "authors": "Gregor Chliamovitch, Alexandre Dupuis, Bastien Chopard, Anton Golub",
        "title": "Improving predictability of time series using maximum entropy methods",
        "comments": "4 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1209/0295-5075/110/10003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss how maximum entropy methods may be applied to the reconstruction\nof Markov processes underlying empirical time series and compare this approach\nto usual frequency sampling. It is shown that, at least in low dimension, there\nexists a subset of the space of stochastic matrices for which the MaxEnt method\nis more efficient than sampling, in the sense that shorter historical samples\nhave to be considered to reach the same accuracy. Considering short samples is\nof particular interest when modelling smoothly non-stationary processes, for\nthen it provides, under some conditions, a powerful forecasting tool. The\nmethod is illustrated for a discretized empirical series of exchange rates.\n"
    },
    {
        "paper_id": 1411.788,
        "authors": "Roy Cerqueti and Marcel Ausloos",
        "title": "Evidence of Economic Regularities and Disparities of Italian Regions\n  From Aggregated Tax Income Size Data",
        "comments": "34 pages; 49 references; 29 figures; 9 Tables; to be published in\n  Physica A",
        "journal-ref": "Physica A 421 (2015) 187-207",
        "doi": "10.1016/j.physa.2014.11.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the size distribution, - in economic terms - of the\nItalian municipalities over the period 2007-2011. Yearly data are rather well\nfitted by a modified Lavalette law, while Zipf-Mandelbrot-Pareto law seems to\nfail in this doing. The analysis is performed either at a national as well as\nat a local (regional and provincial) level. Deviations are discussed as\noriginating in so called king and vice-roy effects. Results confirm that Italy\nis shared among very different regional realities. The case of Lazio is\npuzzling.\n"
    },
    {
        "paper_id": 1411.7991,
        "authors": "Alain Belanger, Ndoune Ndoune",
        "title": "Existence and Uniqueness of a Steady State for an OTC Market with\n  Several Assets",
        "comments": "11 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1308.2957",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and study a class of over-the-counter market models specified by\nsystems of Ordinary Differential Equations (ODE's), in the spirit of Duffie-\nG^arleanu-Pedersen [6]. The key innovation is allowing for multiple assets. We\nshow the existence and uniqueness of a steady state for these ODE's.\n"
    },
    {
        "paper_id": 1412.0042,
        "authors": "Jaroslav Borovi\\v{c}ka, Lars Peter Hansen, Jos\\'e A. Scheinkman",
        "title": "Misspecified Recovery",
        "comments": "51 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Asset prices contain information about the probability distribution of future\nstates and the stochastic discounting of those states as used by investors. To\nbetter understand the challenge in distinguishing investors' beliefs from\nrisk-adjusted discounting, we use Perron-Frobenius Theory to isolate a positive\nmartingale component of the stochastic discount factor process. This component\nrecovers a probability measure that absorbs long-term risk adjustments. When\nthe martingale is not degenerate, surmising that this recovered probability\ncaptures investors' beliefs distorts inference about risk-return tradeoffs.\nStochastic discount factors in many structural models of asset prices have\nempirically relevant martingale components.\n"
    },
    {
        "paper_id": 1412.0064,
        "authors": "Silvio Tarca and Marek Rutkowski",
        "title": "Assessing the Basel II Internal Ratings-Based Approach: Empirical\n  Evidence from Australia",
        "comments": "Addressed critiques of the Basel II IRB approach in the literature\n  and updated figures, as well as general editing to tighten the prose",
        "journal-ref": null,
        "doi": "10.1108/JFRC-05-2015-0024",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Basel II internal ratings-based (IRB) approach to capital adequacy for\ncredit risk implements an asymptotic single risk factor (ASRF) model.\nMeasurements from the ASRF model of the prevailing state of Australia's economy\nand the level of capitalisation of its banking sector find general agreement\nwith macroeconomic indicators, financial statistics and external credit\nratings. However, given the range of economic conditions, from mild contraction\nto moderate expansion, experienced in Australia since the implementation of\nBasel II, we cannot attest to the validity of the model specification of the\nIRB approach for its intended purpose of solvency assessment. With the\nimplementation of Basel II preceding the time when the effect of the financial\ncrisis of 2007-09 was most acutely felt, our empirical findings offer a\nfundamental assessment of the impact of the crisis on the Australian banking\nsector. Access to internal bank data collected by the prudential regulator\ndistinguishes our research from other empirical studies on the IRB approach and\nrecent crisis.\n"
    },
    {
        "paper_id": 1412.0127,
        "authors": "Marcel Ausloos",
        "title": "A biased view of a few possible components when reflecting on the\n  present decade financial and economic crisis",
        "comments": "13 pages; 70 refs.; a chapter prepared for \"Polymorphic Crisis\n  Readings on the Great Recession of the 21st century\" edited by Roy Cerqueti",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is the present economic and financial crisis similar to some previous one? It\nwould be so nice to prove that universality laws exist for predicting such rare\nevents under a minimum set of realistic hypotheses. First, I briefly recall\nwhether patterns, like business cycles, are indeed found, and can be modeled\nwithin a statistical physics, or econophysics, framework. I point to a\nsimulation model for describing such so called business cycles, under exo- and\nendo-genous conditions I discuss self-organized and provoked crashes and their\npredictions. I emphasize the role of an of- ten forgotten ingredient: the time\ndelay in the information flow. I wonder about the information content of\nfinancial data, its mis-interpretation and market manipulation.\n"
    },
    {
        "paper_id": 1412.0141,
        "authors": "Jonathan Donier and Julius Bonart and Iacopo Mastromatteo and\n  Jean-Philippe Bouchaud",
        "title": "A fully consistent, minimal model for non-linear market impact",
        "comments": "17 pages, 8 figures, two new Appendices and several clarifications\n  added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a minimal theory of non-linear price impact based on a linear\n(latent) order book approximation, inspired by diffusion-reaction models and\ngeneral arguments. Our framework allows one to compute the average price\ntrajectory in the presence of a meta-order, that consistently generalizes\npreviously proposed propagator models. We account for the universally observed\nsquare-root impact law, and predict non-trivial trajectories when trading is\ninterrupted or reversed. We prove that our framework is free of price\nmanipulation, and that prices can be made diffusive (albeit with a generic\nshort-term mean-reverting contribution). Our model suggests that prices can be\ndecomposed into a transient \"mechanical\" impact component and a permanent\n\"informational\" component.\n"
    },
    {
        "paper_id": 1412.0148,
        "authors": "Miha Troha and Raphael Hauser",
        "title": "The impact of startup costs and the grid operator on the power price\n  equilibrium",
        "comments": "Corrected paragraph style on page 16. arXiv admin note: substantial\n  text overlap with arXiv:1409.6645",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a quadratic programming model that can be used for\ncalculating the term structure of electricity prices while explicitly modeling\nstartup costs of power plants. In contrast to other approaches presented in the\nliterature, we incorporate the startup costs in a mathematically rigorous\nmanner without relying on ad hoc heuristics. Moreover, we propose a tractable\napproach for estimating the startup costs of power plants based on their\nhistorical production. Through numerical simulations applied to the entire UK\npower grid, we demonstrate that the inclusion of startup costs is necessary for\nthe modeling of electricity prices in realistic power systems. Numerical\nresults show that startup costs make electricity prices very spiky. In the\nsecond part of the paper, we extend the initial model by including the grid\noperator who is responsible for managing the grid. Numerical simulations\ndemonstrate that robust decision making of the grid operator can significantly\ndecrease the number and severity of spikes in the electricity price and improve\nthe reliability of the power grid.\n"
    },
    {
        "paper_id": 1412.0217,
        "authors": "Emmanuel Bacry, Adrian Iuga, Matthieu Lasnier, Charles-Albert Lehalle",
        "title": "Market impacts and the life cycle of investors orders",
        "comments": "30 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we use a database of around 400,000 metaorders issued by\ninvestors and electronically traded on European markets in 2010 in order to\nstudy market impact at different scales.\n  At the intraday scale we confirm a square root temporary impact in the daily\nparticipation, and we shed light on a duration factor in $1/T^{\\gamma}$ with\n$\\gamma \\simeq 0.25$. Including this factor in the fits reinforces the square\nroot shape of impact. We observe a power-law for the transient impact with an\nexponent between $0.5$ (for long metaorders) and $0.8$ (for shorter ones).\nMoreover we show that the market does not anticipate the size of the\nmeta-orders. The intraday decay seems to exhibit two regimes (though hard to\nidentify precisely): a \"slow\" regime right after the execution of the\nmeta-order followed by a faster one. At the daily time scale, we show price\nmoves after a metaorder can be split between realizations of expected returns\nthat have triggered the investing decision and an idiosynchratic impact that\nslowly decays to zero.\n  Moreover we propose a class of toy models based on Hawkes processes (the\nHawkes Impact Models, HIM) to illustrate our reasoning.\n  We show how the Impulsive-HIM model, despite its simplicity, embeds appealing\nfeatures like transience and decay of impact. The latter is parametrized by a\nparameter $C$ having a macroscopic interpretation: the ratio of contrarian\nreaction (i.e. impact decay) and of the \"herding\" reaction (i.e. impact\namplification).\n"
    },
    {
        "paper_id": 1412.0542,
        "authors": "Marco B. Caminati, Manfred Kerber, Colin Rowat",
        "title": "Budget Imbalance Criteria for Auctions: A Formalized Theorem",
        "comments": "6th Podlasie Conference on Mathematics 2014, 11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an original theorem in auction theory: it specifies general\nconditions under which the sum of the payments of all bidders is necessarily\nnot identically zero, and more generally not constant. Moreover, it explicitly\nsupplies a construction for a finite minimal set of possible bids on which such\na sum is not constant. In particular, this theorem applies to the important\ncase of a second-price Vickrey auction, where it reduces to a basic result of\nwhich a novel proof is given. To enhance the confidence in this new theorem, it\nhas been formalized in Isabelle/HOL: the main results and definitions of the\nformal proof are re- produced here in common mathematical language, and are\naccompanied by an informal discussion about the underlying ideas.\n"
    },
    {
        "paper_id": 1412.095,
        "authors": "Luca Amendola (ITP, University of Heidelberg)",
        "title": "Firm size distribution in Italy and employment protection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The number of Italian firms in function of the number of workers is well\napproximated by an inverse power law up to 15 workers but shows a clear\ndownward deflection beyond this point, both when using old pre-1999 data and\nwhen using recent (2014) data. This phenomenon could be associated with\nemployent protection legislation which applies to companies with more than 15\nworkers (the Statuto dei Lavoratori). The deflection disappears for agriculture\nfirms, for which the protection legislation applies already above 5 workers. In\nthis note it is estimated that a correction of this deflection could bring an\nincrease from 3.9 to 5.8% in new jobs in firms with a workforce between 5 to 25\nworkers.\n"
    },
    {
        "paper_id": 1412.1183,
        "authors": "Marek Rutkowski and Silvio Tarca",
        "title": "Regulatory Capital Modelling for Credit Risk",
        "comments": "Updated figures and references to theorems/laws, as well as general\n  editing to tighten the prose. arXiv admin note: text overlap with\n  arXiv:1412.0064",
        "journal-ref": null,
        "doi": "10.1142/S021902491550034X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Basel II internal ratings-based (IRB) approach to capital adequacy for\ncredit risk plays an important role in protecting the Australian banking sector\nagainst insolvency. We outline the mathematical foundations of regulatory\ncapital for credit risk, and extend the model specification of the IRB approach\nto a more general setting than the usual Gaussian case. It rests on the\nproposition that quantiles of the distribution of conditional expectation of\nportfolio percentage loss may be substituted for quantiles of the portfolio\nloss distribution. We present a more economical proof of this proposition under\nweaker assumptions. Then, constructing a portfolio that is representative of\ncredit exposures of the Australian banking sector, we measure the rate of\nconvergence, in terms of number of obligors, of empirical loss distributions to\nthe asymptotic (infinitely fine-grained) portfolio loss distribution. Moreover,\nwe evaluate the sensitivity of credit risk capital to dependence structure as\nmodelled by asset correlations and elliptical copulas. Access to internal bank\ndata collected by the prudential regulator distinguishes our research from\nother empirical studies on the IRB approach.\n"
    },
    {
        "paper_id": 1412.1293,
        "authors": "Ahmet Celikoglu and Ugur Tirnakli",
        "title": "Skewness and kurtosis analysis for non-Gaussian distributions",
        "comments": "15 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a recent paper [\\textit{M. Cristelli, A. Zaccaria and L. Pietronero, Phys.\nRev. E 85, 066108 (2012)}], Cristelli \\textit{et al.} analysed relation between\nskewness and kurtosis for complex dynamical systems and identified two\npower-law regimes of non-Gaussianity, one of which scales with an exponent of 2\nand the other is with $4/3$. Finally the authors concluded that the observed\nrelation is a universal fact in complex dynamical systems. Here, we test the\nproposed universal relation between skewness and kurtosis with large number of\nsynthetic data and show that in fact it is not universal and originates only\ndue to the small number of data points in the data sets considered. The\nproposed relation is tested using two different non-Gaussian distributions,\nnamely $q$-Gaussian and Levy distributions. We clearly show that this relation\ndisappears for sufficiently large data sets provided that the second moment of\nthe distribution is finite. We find that, contrary to the claims of Cristelli\n\\textit{et al.} regarding a power-law scaling regime, kurtosis saturates to a\nsingle value, which is of course different from the Gaussian case ($K=3$), as\nthe number of data is increased. On the other hand, if the second moment of the\ndistribution is infinite, then the kurtosis seems to never converge to a single\nvalue. The converged kurtosis value for the finite second moment distributions\nand the number of data points needed to reach this value depend on the\ndeviation of the original distribution from the Gaussian case. We also argue\nthat the use of kurtosis to compare distributions to decide which one deviates\nfrom the Gaussian more can lead to incorrect results even for finite second\nmoment distributions for small data sets, whereas it is totally misleading for\ninfinite second moment distributions where the difference depends on $N$ for\nall finite $N$.\n"
    },
    {
        "paper_id": 1412.1298,
        "authors": "Nicole B\\\"auerle and Viola Riess",
        "title": "Gas Storage valuation with regime switching",
        "comments": null,
        "journal-ref": "Energy Systems 7(3), 499-528, 2016",
        "doi": "10.1007/s12667-015-0178-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we treat a gas storage valuation problem as a Markov Decision\nProcess. As opposed to existing literature we model the gas price process as a\nregime-switching model. Such a model has shown to fit market data quite well in\nChen and Forsyth (2010). Before we apply a numerical algorithm to solve the\nproblem, we first identify the structure of the optimal injection and withdraw\npolicy. This part extends results in Secomandi (2010). Knowing the structure\nreduces the complexity of the involved recursion in the algorithms by one\nvariable. We explain the usage and implementation of two algorithms: A\nMultinomial-Tree Algorithm and a Least-Square Monte Carlo Algorithm. Both\nalgorithms are shown to work for the regime-switching extension. In a numerical\nstudy we compare these two algorithms.\n"
    },
    {
        "paper_id": 1412.1325,
        "authors": "Giovanni Mottola",
        "title": "Reflected Backward SDE approach to the price-hedge of defaultable claims\n  with contingent switching CSA",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we study the price-hedge issue for general defaultable contracts\ncharacterized by the presence of a contingent CSA of switching type. This is a\ncontingent risk mitigation mechanism that allow the counterparties of a\ndefaultable contract to switch from zero to full/perfect collateralization and\nswitch back whenever until maturity T paying some instantaneous switching costs\n, taking in account in the picture CVA, collateralization and the funding\nproblem. We have been lead to the study of this theoretical pricing/hedging\nproblem, by the economic significance of this type of mechanism which allows a\ngreater flexibility in managing all the defaultable contract risks with respect\nto the \"standard\" non contingent mitigation mechanisms (as full or partial\ncollateralization). In particular, our approach through hedging strategy\ndecomposition of the claim (proposition 2.2.5) and its price-hedge\nrepresentation through system of nonlinear reflected BSDE (theorem 3.2.4) are\nthe main contribution of the work.\n"
    },
    {
        "paper_id": 1412.1429,
        "authors": "Florian Stebegg",
        "title": "Model-Independent Pricing of Asian Options via Optimal Martingale\n  Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we discuss the problem of calculating optimal\nmodel-independent (robust) bounds for the price of Asian options with discrete\nand continuous averaging. We will give geometric characterisations of the\nmaximising and the minimising pricing model for certain types of Asian options\nin discrete and continuous time. In discrete time the problem is reduced to\nfinding the optimal martingale transport for the cost function $|x+y|$. In the\ncontinuous time case we consider the cases with one and two given marginals. We\ndescribe the maximising models in both of these cases as well as the minimising\nmodel in the one-marginal case and relate the two-marginals case to the\ndiscrete time problem with two marginals.\n"
    },
    {
        "paper_id": 1412.1469,
        "authors": "Giovanni Mottola",
        "title": "A stochastic switching control model arising in general OTC contracts\n  with contingent CSA in presence of CVA, collateral and funding",
        "comments": "34 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present work studies and analyzes general defaultable OTC contract in\npresence of a contingent CSA, which is a theoretical counterparty risk\nmitigation mechanism of switching type that allows the counterparty of a\ngeneral OTC contract to switch from zero to full/perfect collateralization and\nswitch back whenever she wants until contract maturity paying some switching\ncosts and taking into account the running costs that emerge over time. The\nmotivation and the underlying economic idea is to show that the current\nfull/partial collateralization mechanisms defined within contracts' CSA - and\nnow imposed by the banking supervision authorities - are \"suboptimal\" and less\neconomic than the contingent one that allows to optimally take in account all\nthe relevant driver namely the expected costs of counterparty default losses -\nrepresented by the (bilateral) CVA - and the expected collateral and funding\ncosts. In this perspective, we tackle the problem from the risk management and\noptimal design point of view solving - under some working assumptions - the\nderived stochastic switching control model via Snell envelope technique and\nimportant results of the theory of the backward stochastic differential\nequations with reflection (RBSDE). We have also studied the numerical solution\nproviding an algorithm procedure for the value function computation based on an\niterative optimal stopping approach.\n"
    },
    {
        "paper_id": 1412.1618,
        "authors": "Patryk Skowron and Mariusz Karpiarz and Agata Fronczak and Piotr\n  Fronczak",
        "title": "Spanning trees of the World Trade Web: real-world data and the gravity\n  model of trade",
        "comments": "7 pages, 7 figures, RevTeX two column style, presented at the 7th\n  Polish Symposium on Econo- and Sociophysics in Lublin (2014)",
        "journal-ref": null,
        "doi": "10.12693/APhysPolA.127.A-123",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the statistical features of the weighted\ninternational-trade network. By finding the maximum weight spanning trees for\nthis network we make the extraction of the truly relevant connections forming\nthe network's backbone. We discuss the role of large-sized countries (strongest\neconomies) in the tree. Finally, we compare the topological properties of this\nbackbone to the maximum weight spanning trees obtained from the gravity model\nof trade. We show that the model correctly reproduces the backbone of the\nreal-world economy.\n"
    },
    {
        "paper_id": 1412.1679,
        "authors": "Stefano Gurciullo",
        "title": "Stess-testing the system: Financial shock contagion in the realm of\n  uncertainty",
        "comments": "50 pages, Paper presented as part of the coursework for the PhD\n  Transfer Viva",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  This work proposes an augmented variant of DebtRank with uncertainty\nintervals as a method to investigate and assess systemic risk in financial\nnetworks, in a context of incomplete data. The algorithm is tested against a\ndefault contagion algorithm on three ensembles of networks with increasing\ndensity, estimated from real-world banking data related to the largest 227 EU15\nfinancial institutions indexed in a stock market. Results suggest that DebtRank\nis capable of capturing increasing rates of systemic risk in a more sensitive\nand continuous way, thereby acting as an early-warning signal. The paper\nproposes three policy instruments based on this approach: the monitoring of\nsystemic risk over time by applying the augmented DebtRank on time snapshots of\ninterbank networks, a stress-testing framework able to test the systemic\nimportance of financial institutions on different shock scenarios, and the\nevaluation of distribution of systemic losses in currency value.\n"
    },
    {
        "paper_id": 1412.1991,
        "authors": "Kamille Sofie T{\\aa}gholt Gad, Jeppe Juhl, Mogens Steffensen",
        "title": "Reserve-Dependent Surrender",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the modelling and valuation of surrender and other behavioural\noptions in life insurance and pension. We place ourselves in between the two\nextremes of completely arbitrary intervention and optimal intervention by the\npolicyholder. We present a method that is based on differential equations and\nthat can be used to approximate contract values when policyholders exhibit\noptimal behaviour. This presentation includes a specification of sufficient\nconditions for both consistency of the model and convergence of the contract\nvalues. When not going to the limit in the approximation we obtain a technique\nfor balancing off arbitrary and optimal behaviour in a simple, intuitive way.\nThis leads to our suggestions for intervention models where one single\nparameter reflects the extent of rationality among policyholders. In a series\nof numerical examples we illustrate the impact of the rationality parameter on\nthe contract values.\n"
    },
    {
        "paper_id": 1412.2053,
        "authors": "Erhan Bayraktar and Song Yao",
        "title": "Doubly Reflected BSDEs with Integrable Parameters and Related Dynkin\n  Games",
        "comments": "Final version. To appear in Stochastic Processes and Their\n  Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a doubly reflected backward stochastic differential equation (BSDE)\nwith integrable parameters and the related Dynkin game. When the lower obstacle\n$L$ and the upper obstacle $U$ of the equation are completely separated, we\nconstruct a unique solution of the doubly reflected BSDE by pasting local\nsolutions and show that the $Y-$component of the unique solution represents the\nvalue process of the corresponding Dynkin game under $g-$evaluation, a\nnonlinear expectation induced by BSDEs with the same generator $g$ as the\ndoubly reflected BSDE concerned. In particular, the first time when process $Y\n$ meets $L$ and the first time when process $Y $ meets $U$ form a saddle point\nof the Dynkin game.\n"
    },
    {
        "paper_id": 1412.2124,
        "authors": "Robert G\\k{e}barowski (1), Stanis{\\l}aw Dro\\.zd\\.z (1 and 2), Andrzej\n  Z. G\\'orski (2), Pawe{\\l} O\\'swi\\k{e}cimka (2) ((1) Wydzia{\\l} Fizyki,\n  Matematyki i Informatyki, Politechnika Krakowska im. Tadeusza Ko\\'sciuszki,\n  Krak\\'ow, Poland, (2) Instytut Fizyki J\\k{a}drowej PAN, Krak\\'ow, Poland)",
        "title": "Competition of Commodities for the Status of Money in an Agent-based\n  Model",
        "comments": "10 pages, 4 figures",
        "journal-ref": "Acta Physica Polonica A vol. 127, No. 3-A, pp A-51 - A-54 (2015)",
        "doi": "10.12693/APhysPolA.127.A-51",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this model study of the commodity market, we present some evidence of\ncompetition of commodities for the status of money in the regime of parameters,\nwhere emergence of money is possible. The competition reveals itself as a\nrivalry of a few (typically two) dominant commodities, which take the status of\nmoney in turn.\n"
    },
    {
        "paper_id": 1412.2152,
        "authors": "Elia Zarinelli, Michele Treccani, J. Doyne Farmer, Fabrizio Lillo",
        "title": "Beyond the square root: Evidence for logarithmic dependence of market\n  impact on size and participation rate",
        "comments": "38 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We make an extensive empirical study of the market impact of large orders\n(metaorders) executed in the U.S. equity market between 2007 and 2009. We show\nthat the square root market impact formula, which is widely used in the\nindustry and supported by previous published research, provides a good fit only\nacross about two orders of magnitude in order size. A logarithmic functional\nform fits the data better, providing a good fit across almost five orders of\nmagnitude. We introduce the concept of an \"impact surface\" to model the impact\nas a function of both the duration and the participation rate of the metaorder,\nfinding again a logarithmic dependence. We show that during the execution the\nprice trajectory deviates from the market impact, a clear indication of\nnon-VWAP executions. Surprisingly, we find that sometimes the price starts\nreverting well before the end of the execution. Finally we show that, although\non average the impact relaxes to approximately 2/3 of the peak impact, the\nprecise asymptotic value of the price depends on the participation rate and on\nthe duration of the metaorder. We present evidence that this might be due to a\nherding phenomenon among metaorders.\n"
    },
    {
        "paper_id": 1412.2262,
        "authors": "Erhan Bayraktar, David Promislow, Virginia Young",
        "title": "Purchasing Term Life Insurance to Reach a Bequest Goal while Consuming",
        "comments": "Final version. To appear in the SIAM Journal on Financial\n  Mathematics. Keywords: Term life insurance, bequest motive, consumption,\n  optimal investment, stochastic control",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the optimal strategies for purchasing term life insurance and\nfor investing in a risky financial market in order to maximize the probability\nof reaching a bequest goal while consuming from an investment account. We\nextend Bayraktar and Young (2015) by allowing the individual to purchase term\nlife insurance to reach her bequest goal. The premium rate for life insurance,\n$h$, serves as a parameter to connect two seemingly unrelated problems. As the\npremium rate approaches $0$, covering the bequest goal becomes costless, so the\nindividual simply wants to avoid ruin that might result from her consumption.\nThus, as $h$ approaches $0$, the problem in this paper becomes equivalent to\nminimizing the probability of lifetime ruin, which is solved in Young (2004).\nOn the other hand, as the premium rate becomes arbitrarily large, the\nindividual will not buy life insurance to reach her bequest goal. Thus, as $h$\napproaches infinity, the problem in this paper becomes equivalent to maximizing\nthe probability of reaching the bequest goal when life insurance is not\navailable in the market, which is solved in Bayraktar and Young (2015).\n"
    },
    {
        "paper_id": 1412.2399,
        "authors": "Werner Ebeling and Andrea Scharnhorst",
        "title": "Modellierungskonzepte der Synergetik und der Theorie der\n  Selbstorganisation",
        "comments": "In German, extended first version, final version Ebeling, W., &\n  Scharnhorst, A. (2015). Modellierungskonzepte der Synergetik und der Theorie\n  der Selbstorganisation. In N. Braun & N. J. Saam (Eds.), Handbuch\n  Modellbildung und Simulation in den Sozialwissenschaften (pp. 419--452).\n  Wiesbaden: Springer Fachmedien Wiesbaden. doi:10.1007/978-3-658-01164-2 (in\n  German)",
        "journal-ref": null,
        "doi": "10.1007/978-3-658-01164-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mnay models situated in the current research landscape of modelling and\nsimulating social processes have roots in physics. This is visible in the name\nof specialties as Econophysics or Sociophysics. This chapter describes the\nhistory of knowledge transfer from physics, in particular physics of\nself-organization and evolution, to the social sciences. We discuss why\nphysicists felt called to describe social processes. Across models and\nsimulations the question how to explain the emergence of something new is the\nmost intriguing one. We present one model approach to this problem and\nintroduce a game -- Evolino -- inviting a larger audience to get acquainted\nwith abstract evolution-theory approaches to describe the quest for new ideas.\n"
    },
    {
        "paper_id": 1412.2453,
        "authors": "Tianyang Nie, Marek Rutkowski",
        "title": "A BSDE approach to fair bilateral pricing under endogenous\n  collateralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our previous results are extended to the case of the margin account, which\nmay depend on the contract's value for the hedger and/or the counterparty. The\npresent work generalizes also the papers by Bergman (1995), Mercurio (2013) and\nPiterbarg (2010). Using the comparison theorems for BSDEs, we derive\ninequalities for the unilateral prices and we give the range for its fair\nbilateral prices. We also establish results yielding the link to the market\nmodel with a single interest rate. In the case where the collateral amount is\nnegotiated between the counterparties, so that it depends on their respective\nunilateral values, the backward stochastic viability property studied by\nBuckdahn et al. (2000) is used to derive the bounds on fair bilateral prices.\n"
    },
    {
        "paper_id": 1412.2746,
        "authors": "Andrey Nechaev",
        "title": "Taxation as an instrument of stimulation of innovation-active business\n  entities",
        "comments": "innovation; business entity; taxation; tax rate; return on assets",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The analysis of the theoretical material revealed the lack of consensus on\ndefini-tion of the tax stimulation of innovation-active business entities\nwithin the re-gional taxation. The definition tax stimulation of\ninnovation-active business en-tities is specified.\n"
    },
    {
        "paper_id": 1412.3126,
        "authors": "Omar Rojas and Carlos Trejo-Pech",
        "title": "Financial Time Series: Stylized Facts for the Mexican Stock Exchange\n  Index Compared to Developed Markets",
        "comments": "Chapter of book Nonlinear Time Series and Finance, Semei\n  Coronado-Ram\\'irez et al. (eds.), Universidad de Guadalajara, 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present some stylized facts exhibited by the time series of returns of the\nMexican Stock Exchange Index (IPC) and compare them to a sample of both\ndeveloped (USA, UK and Japan) and emerging markets (Brazil and India). The\nperiod of study is 1997-2011. The stylized facts are related mostly to the\nprobability distribution func- tion and the autocorrelation function (e.g. fat\ntails, non-normality, volatility cluster- ing, among others). We find that\npositive skewness for returns in Mexico and Brazil, but not in the rest,\nsuggest investment opportunities. Evidence of nonlinearity is also documented.\n"
    },
    {
        "paper_id": 1412.314,
        "authors": "Dirk Becherer and Plamen Turkedjiev",
        "title": "Multilevel approximation of backward stochastic differential equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a multilevel approach to compute approximate solutions to backward\ndifferential equations (BSDEs). The fully implementable algorithm of our\nmultilevel scheme constructs sequential martingale control variates along a\nsequence of refining time-grids to reduce statistical approximation errors in\nan adaptive and generic way. We provide an error analysis with explicit and\nnon-asymptotic error estimates for the multilevel scheme under general\nconditions on the forward process and the BSDE data. It is shown that the\nmultilevel approach can reduce the computational complexity to achieve\nprecision $\\epsilon$, ensured by error estimates, essentially by one order (in\n$\\epsilon^{-1}$) in comparison to established methods, which is substantial.\nComputational examples support the validity of the theoretical analysis,\ndemonstrating efficiency improvements in practice.\n"
    },
    {
        "paper_id": 1412.323,
        "authors": "Michel Denuit and Anna Kiriliouk and Johan Segers",
        "title": "Max-factor individual risk models with application to credit portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Individual risk models need to capture possible correlations as failing to do\nso typically results in an underestimation of extreme quantiles of the\naggregate loss. Such dependence modelling is particularly important for\nmanaging credit risk, for instance, where joint defaults are a major cause of\nconcern. Often, the dependence between the individual loss occurrence\nindicators is driven by a small number of unobservable factors. Conditional\nloss probabilities are then expressed as monotone functions of linear\ncombinations of these hidden factors. However, combining the factors in a\nlinear way allows for some compensation between them. Such diversification\neffects are not always desirable and this is why the present work proposes a\nnew model replacing linear combinations with maxima. These max-factor models\ngive more insight into which of the factors is dominant.\n"
    },
    {
        "paper_id": 1412.353,
        "authors": "Tongseok Lim",
        "title": "Optimal martingale transport between radially symmetric marginals in\n  general dimensions",
        "comments": "Some clarifications were made in the proofs",
        "journal-ref": "Stochastic Processes and their Applications, 2019",
        "doi": "10.1016/j.spa.2019.06.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the optimal structure of couplings for the \\emph{Martingale\ntransport problem} between radially symmetric initial and terminal laws $\\mu,\n\\nu$ on $\\R^d$ and show the uniqueness of optimizer. Here optimality means that\nsuch solutions will minimize the functional $\\E |X-Y|^p$ where $0<p \\leq 1$,\nand the dimension $d$ is arbitrary.\n"
    },
    {
        "paper_id": 1412.3623,
        "authors": "Q. Feng, C.W. Oosterlee",
        "title": "Monte Carlo Calculation of Exposure Profiles and Greeks for Bermudan and\n  Barrier Options under the Heston Hull-White Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Valuation of Credit Valuation Adjustment (CVA) has become an important field\nas its calculation is required in Basel III, issued in 2010, in the wake of the\ncredit crisis. Exposure, which is defined as the potential future loss of a\ndefault event without any recovery, is one of the key elementsfor pricing CVA.\nThis paper provides a backward dynamics framework for assessing exposure\nprofiles of European, Bermudan and barrier options under the Heston and Heston\nHull-White asset dynamics. We discuss the potential of an efficient and\nadaptive Monte Carlo approach, the Stochastic Grid Bundling Method}(SGBM),\nwhich employs the techniques of simulation, regression and bundling. Greeks of\nthe exposure profiles can be calculated in the same backward iteration with\nlittle extra effort. Assuming independence between default event and exposure\nprofiles, we give examples of calculating exposure, CVA and Greeks for Bermudan\nand barrier options.\n"
    },
    {
        "paper_id": 1412.3948,
        "authors": "Gabriele Ranco, Ilaria Bordino, Giacomo Bormetti, Guido Caldarelli,\n  Fabrizio Lillo, Michele Treccani",
        "title": "Coupling news sentiment with web browsing data improves prediction of\n  intra-day price dynamics",
        "comments": "24 pages, 4 figures, 5 tables. Figure 2 replaced. Few typos corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The new digital revolution of big data is deeply changing our capability of\nunderstanding society and forecasting the outcome of many social and economic\nsystems. Unfortunately, information can be very heterogeneous in the\nimportance, relevance, and surprise it conveys, affecting severely the\npredictive power of semantic and statistical methods. Here we show that the\naggregation of web users' behavior can be elicited to overcome this problem in\na hard to predict complex system, namely the financial market. Specifically,\nour in-sample analysis shows that the combined use of sentiment analysis of\nnews and browsing activity of users of Yahoo! Finance greatly helps forecasting\nintra-day and daily price changes of a set of 100 highly capitalized US stocks\ntraded in the period 2012-2013. Sentiment analysis or browsing activity when\ntaken alone have very small or no predictive power. Conversely, when\nconsidering a \"news signal\" where in a given time interval we compute the\naverage sentiment of the clicked news, weighted by the number of clicks, we\nshow that for nearly 50% of the companies such signal Granger-causes hourly\nprice returns. Our result indicates a \"wisdom-of-the-crowd\" effect that allows\nto exploit users' activity to identify and weigh properly the relevant and\nsurprising news, enhancing considerably the forecasting power of the news\nsentiment.\n"
    },
    {
        "paper_id": 1412.4045,
        "authors": "Denis Belomestny and Tigran Nagapetyan",
        "title": "Variance reduced multilevel path simulation: going beyond the complexity\n  $\\varepsilon^{-2}$",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper a novel modification of the multilevel Monte Carlo approach,\nallowing for further significant complexity reduction, is proposed. The idea of\nthe modification is to use the method of control variates to reduce variance at\nlevel zero. We show that, under a proper choice of control variates, one can\nreduce the complexity order of the modified MLMC algorithm down to\n$\\varepsilon^{-2+\\delta}$ for any $\\delta\\in [0,1)$ with $\\varepsilon$ being\nthe precision to be achieved. These theoretical results are illustrated by\nseveral numerical examples.\n"
    },
    {
        "paper_id": 1412.4208,
        "authors": "Michail Anthropelos and Constantinos Kardaras",
        "title": "Equilibrium in risk-sharing games",
        "comments": "48 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The large majority of risk-sharing transactions involve few agents, each of\nwhom can heavily influence the structure and the prices of securities. This\npaper proposes a game where agents' strategic sets consist of all possible\nsharing securities and pricing kernels that are consistent with Arrow-Debreu\nsharing rules. First, it is shown that agents' best response problems have\nunique solutions. The risk-sharing Nash equilibrium admits a finite-dimensional\ncharacterisation and it is proved to exist for arbitrary number of agents and\nbe unique in the two-agent game. In equilibrium, agents declare beliefs on\nfuture random outcomes different than their actual probability assessments, and\nthe risk-sharing securities are endogenously bounded, implying (among other\nthings) loss of efficiency. In addition, an analysis regarding extremely risk\ntolerant agents indicates that they profit more from the Nash risk-sharing\nequilibrium as compared to the Arrow-Debreu one.\n"
    },
    {
        "paper_id": 1412.4342,
        "authors": "Zura Kakushadze",
        "title": "Russian-Doll Risk Models",
        "comments": "25 pages; expanded version",
        "journal-ref": "Journal of Asset Management 16(3) (2015) 170-185",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a simple explicit algorithm for building multi-factor risk models. It\ndramatically reduces the number of or altogether eliminates the risk factors\nfor which the factor covariance matrix needs to be computed. This is achieved\nvia a nested \"Russian-doll\" embedding: the factor covariance matrix itself is\nmodeled via a factor model, whose factor covariance matrix in turn is modeled\nvia a factor model, and so on. We discuss in detail how to implement this\nalgorithm in the case of (binary) industry classification based risk factors\n(e.g., \"sector -> industry -> sub-industry\"), and also in the presence of\n(non-binary) style factors. Our algorithm is particularly useful when long\nhistorical lookbacks are unavailable or undesirable, e.g., in short-horizon\nquant trading.\n"
    },
    {
        "paper_id": 1412.4428,
        "authors": "Timothy Christensen",
        "title": "Nonparametric Stochastic Discount Factor Decomposition",
        "comments": null,
        "journal-ref": "Econometrica 85(5) (2017) 1501-1536",
        "doi": "10.3982/ECTA11600",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic discount factor (SDF) processes in dynamic economies admit a\npermanent-transitory decomposition in which the permanent component\ncharacterizes pricing over long investment horizons. This paper introduces an\nempirical framework to analyze the permanent-transitory decomposition of SDF\nprocesses. Specifically, we show how to estimate nonparametrically the solution\nto the Perron-Frobenius eigenfunction problem of Hansen and Scheinkman (2009).\nOur empirical framework allows researchers to (i) recover the time series of\nthe estimated permanent and transitory components and (ii) estimate the yield\nand the change of measure which characterize pricing over long investment\nhorizons. We also introduce nonparametric estimators of the continuation value\nfunction in a class of models with recursive preferences by reinterpreting the\nvalue function recursion as a nonlinear Perron-Frobenius problem. We establish\nconsistency and convergence rates of the eigenfunction estimators and\nasymptotic normality of the eigenvalue estimator and estimators of related\nfunctionals. As an application, we study an economy where the representative\nagent is endowed with recursive preferences, allowing for general (nonlinear)\nconsumption and earnings growth dynamics.\n"
    },
    {
        "paper_id": 1412.4503,
        "authors": "Jonathan Donier, Julius Bonart",
        "title": "A Million Metaorder Analysis of Market Impact on the Bitcoin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a thorough empirical analysis of market impact on the Bitcoin/USD\nexchange market using a complete dataset that allows us to reconstruct more\nthan one million metaorders. We empirically confirm the \"square-root law'' for\nmarket impact, which holds on four decades in spite of the quasi-absence of\nstatistical arbitrage and market marking strategies. We show that the\nsquare-root impact holds during the whole trajectory of a metaorder and not\nonly for the final execution price. We also attempt to decompose the order flow\ninto an \"informed'' and \"uninformed'' component, the latter leading to an\nalmost complete long-term decay of impact. This study sheds light on the\nhypotheses and predictions of several market impact models recently proposed in\nthe literature and promotes heterogeneous agent models as promising candidates\nto explain price impact on the Bitcoin market -- and, we believe, on other\nmarkets as well.\n"
    },
    {
        "paper_id": 1412.4695,
        "authors": "Ricardo P\\'erez-Marco",
        "title": "On Pareto theory of circulation of elites",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that Pareto theory of circulation of elites results from our wealth\nevolution model, Kelly criterion for optimal betting and Keynes' observation of\n\"animal spirits\" that drive the economy and cause that human financial\ndecisions are prone to excess risk-taking.\n"
    },
    {
        "paper_id": 1412.4698,
        "authors": "Julio Backhoff, Ulrich Horst",
        "title": "Conditional Analysis and a Principal-Agent problem",
        "comments": "27 pages. Forthcoming in Siam Journal on Financial Mathematics\n  (SIFIN)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze conditional optimization problems arising in discrete time\nPrincipal-Agent problems of delegated portfolio optimization with linear\ncontracts. Applying tools from Conditional Analysis we show that some results\nknown in the literature for very specific instances of the problem carry over\nto translation invariant and time-consistent utility functions in very general\nprobabilistic settings. However, we find that optimal contracts must in general\nmake use of derivatives for compensation.\n"
    },
    {
        "paper_id": 1412.4839,
        "authors": "Gianbiagio Curato, Jim Gatheral and Fabrizio Lillo",
        "title": "Optimal execution with nonlinear transient market impact",
        "comments": "41 pages, 16 figures, Market Microstructure: confronting many\n  viewpoints (2014)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of the optimal execution of a large trade in the\npresence of nonlinear transient impact. We propose an approach based on\nhomotopy analysis, whereby a well behaved initial strategy is continuously\ndeformed to lower the expected execution cost. We find that the optimal\nsolution is front loaded for concave impact and that its expected cost is\nsignificantly lower than that of conventional strategies. We then consider\nbrute force numerical optimization of the cost functional; we find that the\noptimal solution for a buy program typically features a few short intense\nbuying periods separated by long periods of weak selling. Indeed, in some cases\nwe find negative expected cost. We show that this undesirable characteristic of\nthe nonlinear transient impact model may be mitigated either by introducing a\nbid-ask spread cost or by imposing convexity of the instantaneous market impact\nfunction for large trading rates.\n"
    },
    {
        "paper_id": 1412.5072,
        "authors": "Oleh Danyliv, Bruce Bland, Daniel Nicholass",
        "title": "Convenient liquidity measure for Financial markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A liquidity measure based on consideration and price range is proposed.\nInitially defined for daily data, Liquidity Index (LIX) can also be estimated\nvia intraday data by using a time scaling mechanism. The link between LIX and\nthe liquidity measure based on weighted average bid-ask spread is established.\nUsing this liquidity measure, an elementary liquidity algebra is possible: from\nthe estimation of the execution cost, the liquidity of a basket of instruments\nis obtained. A formula for the liquidity of an ETF, from the liquidity of its\nconstituencies and the liquidity of ETF shares, is derived.\n"
    },
    {
        "paper_id": 1412.5332,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Efficient XVA Management: Pricing, Hedging, and Attribution using\n  Trade-Level Regression and Global Conditioning",
        "comments": "17 pages, 4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Banks must manage their trading books, not just value them. Pricing includes\nvaluation adjustments collectively known as XVA (at least credit, funding,\ncapital and tax), so management must also include XVA. In trading book\nmanagement we focus on pricing, hedging, and allocation of prices or hedging\ncosts to desks on an individual trade basis. We show how to combine three\ntechnical elements to radically simplify XVA management, both in terms of the\ncalculations, and the implementation of the calculations. The three technical\nelements are: trade-level regression; analytic computation of sensitivities;\nand global conditioning. All three are required to obtain the radical\nefficiency gains and implementation simplification. Moreover, many of the\ncalculations are inherently parallel and suitable for GPU implementation. The\nresulting methodology for XVA management is sufficiently general that we can\ncover pricing, first- and second-order sensitivities, and exact trade-level\nallocation of pricing and sensitivities within the same framework. Managing\nincremental changes to portfolios exactly is also radically simplified.\n"
    },
    {
        "paper_id": 1412.5351,
        "authors": "Galina Andreeva, Raffaella Calabrese and Silvia Angela Osmetti",
        "title": "A comparative analysis of the UK and Italian small businesses using\n  Generalised Extreme Value models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a cross-country comparison of significant predictors of\nsmall business failure between Italy and the UK. Financial measures of\nprofitability, leverage, coverage, liquidity, scale and non-financial\ninformation are explored, some commonalities and differences are highlighted.\nSeveral models are considered, starting with the logis- tic regression which is\na standard approach in credit risk modelling. Some important improvements are\ninvestigated. Generalised Extreme Value (GEV) regression is applied to correct\nfor the symmetric link function of the logistic regression. The assumption of\nnon-linearity is relaxed through application of BGEVA, non-parametric additive\nmodel based on the GEV link function. Two methods of handling missing values\nare compared: multiple imputation and Weights of Evidence (WoE) transformation.\nThe results suggest that the best predictive performance is obtained by BGEVA,\nthus implying the necessity of taking into account the relative volume of\ndefaults and non-linear patterns when modelling SME performance. WoE for the\nmajority of models considered show better prediction as compared to multiple\nimputation, suggesting that missing values could be informative and should not\nbe assumed to be missing at random.\n"
    },
    {
        "paper_id": 1412.5397,
        "authors": "Juehui Shi",
        "title": "Comprehensive Time-Series Regression Models Using GRETL -- U.S. GDP and\n  Government Consumption Expenditures & Gross Investment from 1980 to 2013",
        "comments": "82 Pages with Gretl codes included",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2540535",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman\nfilter, transfer-function and intervention models, unit root tests,\ncointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M,\nTaylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly\ntime series of GDP and Government Consumption Expenditures & Gross Investment\n(GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II)\nRegression Models; (III) Discussion. Additionally, I discovered a unique\ninteraction between GDP and GCEGI in both the short-run and the long-run and\nprovided policy makers with some suggestions. For example in the short run, GDP\nresponded positively and very significantly (0.00248) to GCEGI, while GCEGI\nreacted positively but not too significantly (0.08051) to GDP. In the long run,\ncurrent GDP responded negatively and permanently (0.09229) to a shock in past\nGCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a\nshock in past GDP. Therefore, policy makers should not adjust current GCEGI\nbased merely on the condition of current and past GDP. Although increasing\nGCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI\nmight not be good to the long-term health of GDP. Instead, a balanced,\nsustainable, and economically viable solution is recommended, so that the\nshort-term benefits to the current economy from increasing GCEGI often largely\nsecured by the long-term loan outweigh or at least equal to the negative effect\nto the future economy from the long-term debt incurred by the loan. Finally, I\nfound that non-normally distributed volatility models generally perform better\nthan normally distributed ones. More specifically, TARCH-GED performs the best\nin the group of non-normally distributed, while GARCH-M does the best in the\ngroup of normally distributed.\n"
    },
    {
        "paper_id": 1412.5452,
        "authors": "Jozsef Mezei and Peter Sarlin",
        "title": "Aggregation operators for the measurement of systemic risk",
        "comments": "Submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The policy objective of safeguarding financial stability has stimulated a\nwave of research on systemic risk analytics, yet it still faces challenges in\nmeasurability. This paper models systemic risk by tapping into expert knowledge\nof financial supervisors. We decompose systemic risk into a number of\ninterconnected segments, for which the level of vulnerability is measured. The\nsystem is modeled in the form of a Fuzzy Cognitive Map (FCM), in which nodes\nrepresent vulnerability in segments and links their interconnectedness. A main\nproblem tackled in this paper is the aggregation of values in different\ninterrelated nodes of the network to obtain an estimate systemic risk. To this\nend, the Choquet integral is employed for aggregating expert evaluations of\nmeasures, as it allows for the integration of interrelations among factors in\nthe aggregation process. The approach is illustrated through two applications\nin a European setting. First, we provide an estimation of systemic risk with a\nof pan-European set-up. Second, we estimate country-level risks, allowing for a\nmore granular decomposition. This sets a starting point for the use of the\nrich, oftentimes tacit, knowledge in policy organizations.\n"
    },
    {
        "paper_id": 1412.552,
        "authors": "Matthew Lorig",
        "title": "Indifference prices and implied volatilities",
        "comments": "345 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general local-stochastic volatility model and an investor with\nexponential utility. For a European-style contingent claim, whose payoff may\ndepend on either a traded or non-traded asset, we derive an explicit\napproximation for both the buyer's and seller's indifference price. For\nEuropean calls on a traded asset, we translate indifference prices into an\nexplicit approximation of the buyer's and seller's implied volatility surface.\nFor European claims on a non-traded asset, we establish rigorous error bounds\nfor the indifference price approximation. Finally, we implement our\nindifference price and implied volatility approximations in two examples.\n"
    },
    {
        "paper_id": 1412.5558,
        "authors": "Stanislaus Maier-Paape and Andreas Platen",
        "title": "Backtest of Trading Systems on Candle Charts",
        "comments": "12 pages, 19 figures; Keywords: backtest evaluation, historical\n  simulation, trading system, candle chart, imperfect data",
        "journal-ref": "IFTA Journal, pp. 10-17, 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we try to design the necessary calculation needed for\nbacktesting trading systems when only candle chart data are available. We lay\nparticular emphasis on situations which are not or not uniquely decidable and\ngive possible strategies to handle such situations.\n"
    },
    {
        "paper_id": 1412.6063,
        "authors": "Jamal Amani Rad and Kourosh Parand and Saeid Abbasbandy",
        "title": "Local weak form meshless techniques based on the radial point\n  interpolation (RPI) method and local boundary integral equation (LBIE) method\n  to evaluate European and American options",
        "comments": null,
        "journal-ref": "dx.doi.org/10.1016/j.cnsns.2014.07.015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the first time in mathematical finance field, we propose the local weak\nform meshless methods for option pricing; especially in this paper we select\nand analysis two schemes of them named local boundary integral equation method\n(LBIE) based on moving least squares approximation (MLS) and local radial point\ninterpolation (LRPI) based on Wu's compactly supported radial basis functions\n(WCS-RBFs). LBIE and LRPI schemes are the truly meshless methods, because, a\ntraditional non-overlapping, continuous mesh is not required, either for the\nconstruction of the shape functions, or for the integration of the local\nsub-domains. In this work, the American option which is a free boundary\nproblem, is reduced to a problem with fixed boundary using a Richardson\nextrapolation technique. Then the $\\theta$-weighted scheme is employed for the\ntime derivative. Stability analysis of the methods is analyzed and performed by\nthe matrix method. In fact, based on an analysis carried out in the present\npaper, the methods are unconditionally stable for implicit Euler (\\theta = 0)\nand Crank-Nicolson (\\theta = 0.5) schemes. It should be noted that LBIE and\nLRPI schemes lead to banded and sparse system matrices. Therefore, we use a\npowerful iterative algorithm named the Bi-conjugate gradient stabilized method\n(BCGSTAB) to get rid of this system. Numerical experiments are presented\nshowing that the LBIE and LRPI approaches are extremely accurate and fast.\n"
    },
    {
        "paper_id": 1412.6064,
        "authors": "Jamal Amani Rad and Kourosh Parand",
        "title": "Numerical pricing of American options under two stochastic factor models\n  with jumps using a meshless local Petrov-Galerkin method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The most recent update of financial option models is American options under\nstochastic volatility models with jumps in returns (SVJ) and stochastic\nvolatility models with jumps in returns and volatility (SVCJ). To evaluate\nthese options, mesh-based methods are applied in a number of papers but it is\nwell-known that these methods depend strongly on the mesh properties which is\nthe major disadvantage of them. Therefore, we propose the use of the meshless\nmethods to solve the aforementioned options models, especially in this work we\nselect and analyze one scheme of them, named local radial point interpolation\n(LRPI) based on Wendland's compactly supported radial basis functions\n(WCS-RBFs) with C6, C4 and C2 smoothness degrees. The LRPI method which is a\nspecial type of meshless local Petrov-Galerkin method (MLPG), offers several\nadvantages over the mesh-based methods, nevertheless it has never been applied\nto option pricing, at least to the very best of our knowledge. These schemes\nare the truly meshless methods, because, a traditional non-overlapping\ncontinuous mesh is not required, neither for the construction of the shape\nfunctions, nor for the integration of the local sub-domains. In this work, the\nAmerican option which is a free boundary problem, is reduced to a problem with\nfixed boundary using a Richardson extrapolation technique. Then the\nimplicit-explicit (IMEX) time stepping scheme is employed for the time\nderivative which allows us to smooth the discontinuities of the options'\npayoffs. Stability analysis of the method is analyzed and performed. In fact,\naccording to an analysis carried out in the present paper, the proposed method\nis unconditionally stable. Numerical experiments are presented showing that the\nproposed approaches are extremely accurate and fast.\n"
    },
    {
        "paper_id": 1412.6244,
        "authors": "Aleksejus Kononovicius, Julius Ruseckas",
        "title": "Nonlinear GARCH model and 1/f noise",
        "comments": "12 pages, 4 figures",
        "journal-ref": "Physica A 427, 2015, pp. 74-81",
        "doi": "10.1016/j.physa.2015.02.040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Auto-regressive conditionally heteroskedastic (ARCH) family models are still\nused, by practitioners in business and economic policy making, as a conditional\nvolatility forecasting models. Furthermore ARCH models still are attracting an\ninterest of the researchers. In this contribution we consider the well known\nGARCH(1,1) process and its nonlinear modifications, reminiscent of NGARCH\nmodel. We investigate the possibility to reproduce power law statistics,\nprobability density function and power spectral density, using ARCH family\nmodels. For this purpose we derive stochastic differential equations from the\nGARCH processes in consideration. We find the obtained equations to be similar\nto a general class of stochastic differential equations known to reproduce\npower law statistics. We show that linear GARCH(1,1) process has power law\ndistribution, but its power spectral density is Brownian noise-like. However,\nthe nonlinear modifications exhibit both power law distribution and power\nspectral density of the power law form, including 1/f noise.\n"
    },
    {
        "paper_id": 1412.6459,
        "authors": "Tomasz R. Bielecki, Igor Cialenco and Tao Chen",
        "title": "Dynamic Conic Finance via Backward Stochastic Difference Equations",
        "comments": "65 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an arbitrage free theoretical framework for modeling bid and ask\nprices of dividend paying securities in a discrete time setup using theory of\ndynamic acceptability indices. In the first part of the paper we develop the\ntheory of dynamic subscale invariant performance measures, on a general\nprobability space, and discrete time setup. We prove a representation theorem\nof such measures in terms of a family of dynamic convex risk measures, and\nprovide a representation of dynamic risk measures in terms of g-expectations,\nand solutions of BS$\\Delta$Es with convex drivers. We study the existence and\nuniqueness of the solutions, and derive a comparison theorem for corresponding\nBS$\\Delta$Es.\n  In the second part of the paper we discuss a market model for dividend paying\nsecurities by introducing the pricing operators that are defined in terms of\ndynamic acceptability indices, and find various properties of these operators.\nUsing these pricing operators, we define the bid and ask prices for the\nunderlying securities and then for derivatives in this market. We show that the\nobtained market model is arbitrage free, and we also prove a series of\nproperties of these prices.\n"
    },
    {
        "paper_id": 1412.6745,
        "authors": "Erindi Allaj",
        "title": "Risk measuring under liquidity risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general framework for measuring the liquidity risk. The\ntheoretical framework defines a class of risk measures that incorporate the\nliquidity risk into the standard risk measures. We consider a one-period risk\nmeasurement model. The liquidity risk is defined as the risk that a given\nsecurity or a portfolio of securities cannot be easily sold or bought by the\nfinancial institutions without causing significant changes in prices. The new\nrisk measures present some differences with respect to the standard risk\nmeasures. In particular, they are increasing monotonic and convex cash\nsub-additive on long positions. The contrary, in certain situations, holds for\nthe sell positions. For the long positions case, we provide these new risk\nmeasures with a dual representation. In some specific cases also the sell\npositions can be equipped with a dual representation. We apply our framework to\nthe situation in which financial institutions break up large trades into many\nsmall ones. Dual representation results are also obtained. We give many\npractical examples of risk measures and derive for each of them the respective\ncapital requirement. As a particular example, we discuss the VaR measure.\n"
    },
    {
        "paper_id": 1412.6924,
        "authors": "Klaus Jaffe",
        "title": "Visualizing the Invisible Hand of Markets: Simulating complex dynamic\n  economic interactions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In complex systems, many different parts interact in non-obvious ways.\nTraditional research focuses on a few or a single aspect of the problem so as\nto analyze it with the tools available. To get a better insight of phenomena\nthat emerge from complex interactions, we need instruments that can analyze\nsimultaneously complex interactions between many parts. Here, a simulator\nmodeling different types of economies, is used to visualize complex\nquantitative aspects that affect economic dynamics. The main conclusions are:\n1- Relatively simple economic settings produce complex non-linear dynamics and\ntherefore linear regressions are often unsuitable to capture complex economic\ndynamics; 2- Flexible pricing of goods by individual agents according to their\nmicro-environment increases the health and wealth of the society, but\nasymmetries in price sensitivity between buyers and sellers increase price\ninflation; 3- Prices for goods conferring risky long term benefits are not\ntracked efficiently by simple market forces. 4- Division of labor creates\nsynergies that improve enormously the health and wealth of the society by\nincreasing the efficiency of economic activity. 5- Stochastic modeling improves\nour understanding of real economies, and didactic games based on them might\nhelp policy makers and non specialists in grasping the complex dynamics\nunderlying even simple economic settings.\n"
    },
    {
        "paper_id": 1412.7058,
        "authors": "Andrei Lebedev, Petr Zabreiko",
        "title": "Fundamental theorem of asset pricing: a strengthened version and\n  $p$-summable markets",
        "comments": "10 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1410.4807",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the article a strenthened version of the 'Fundamental Theorem of asset\nPricing' for one-period market model is proven. The principal role in this\nresult play total and nonanihilating cones.\n"
    },
    {
        "paper_id": 1412.7096,
        "authors": "Emmanuel Bacry and Thibault Jaisson and Jean-Francois Muzy",
        "title": "Estimation of slowly decreasing Hawkes kernels: Application to high\n  frequency order book modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a modified version of the non parametric Hawkes kernel estimation\nprocedure studied in arXiv:1401.0903 that is adapted to slowly decreasing\nkernels. We show on numerical simulations involving a reasonable number of\nevents that this method allows us to estimate faithfully a power-law decreasing\nkernel over at least 6 decades. We then propose a 8-dimensional Hawkes model\nfor all events associated with the first level of some asset order book.\nApplying our estimation procedure to this model, allows us to uncover the main\nproperties of the coupled dynamics of trade, limit and cancel orders in\nrelationship with the mid-price variations.\n"
    },
    {
        "paper_id": 1412.7227,
        "authors": "Bruce M. Boghosian and Merek Johnson and Jeremy Marcq",
        "title": "An $H$ theorem for Boltzmann's equation for the Yard-Sale Model of asset\n  exchange",
        "comments": "10 pages, 1 figure, submitted to proceedings of the 23rd conference\n  on the Discrete Simulation of Fluid Dynamics (DSFD 2014)",
        "journal-ref": null,
        "doi": "10.1007/s10955-015-1316-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent work, Boltzmann and Fokker-Planck equations were derived for the\n\"Yard-Sale Model\" of asset exchange. For the version of the model without\nredistribution, it was conjectured, based on numerical evidence, that the\ntime-asymptotic state of the model was oligarchy -- complete concentration of\nwealth by a single individual. In this work, we prove that conjecture by\ndemonstrating that the Gini coefficient, a measure of inequality commonly used\nby economists, is an $H$ function of both the Boltzmann and Fokker-Planck\nequations for the model.\n"
    },
    {
        "paper_id": 1412.7269,
        "authors": "Mitsuaki Murota, Jun-ichi Inoue",
        "title": "Large-scale empirical study on pairs trading for all possible pairs of\n  stocks listed on the first section of the Tokyo Stock Exchange",
        "comments": "19 pages, 14 figures, using svjour3.cls",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We carry out a large-scale empirical data analysis to examine the efficiency\nof the so-called pairs trading. On the basis of relevant three thresholds,\nnamely, starting, profit-taking, and stop-loss for the `first-passage process'\nof the spread (gap) between two highly-correlated stocks, we construct an\neffective strategy to make a trade via `active' stock-pairs automatically. The\nalgorithm is applied to $1,784$ stocks listed on the first section of the Tokyo\nStock Exchange leading up to totally $1,590,436$ pairs. We are numerically\nconfirmed that the asset management by means of the pairs trading works\neffectively at least for the past three years (2010-2012) data sets in the\nsense that the profit rate becomes positive (totally positive arbitrage) in\nmost cases of the possible combinations of thresholds corresponding to\n`absorbing boundaries' in the literature of first-passage processes.\n"
    },
    {
        "paper_id": 1412.7412,
        "authors": "Abdelkoddousse Ahdida, Aur\\'elien Alfonsi, Ernesto Palidda",
        "title": "Smile with the Gaussian term structure model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an affine extension of the Linear Gaussian term structure Model\n(LGM) such that the instantaneous covariation of the factors is given by an\naffine process on semidefinite positive matrices. First, we set up the model\nand present some important properties concerning the Laplace transform of the\nfactors and the ergodicity of the model. Then, we present two main numerical\ntools to implement the model in practice. First, we obtain an expansion of\ncaplets and swaptions prices around the LGM. Such a fast and accurate\napproximation is useful for assessing the model behavior on the implied\nvolatility smile. Second, we provide a second order scheme for the weak error,\nwhich enables to calculate exotic options by a Monte-Carlo algorithm.\n"
    },
    {
        "paper_id": 1412.75,
        "authors": "Matheus Grasselli, Adrien Nguyen Huu (CERMICS)",
        "title": "Inflation and speculation in a dynamic macroeconomic model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a monetary version of the Keen model by merging two alternative\nextensions, namely the addition of a dynamic price level and the introduction\nof speculation. We recall and study old and new equilibria, together with their\nlocal stability analysis. This includes a state of recession associated with a\ndeflationary regime and characterized by falling employment but constant wage\nshares, with or without an accompanying debt crisis. We also emphasize some new\nqualitative behavior of the extended model, in particular its ability to\nproduce and describe repeated financial crises as a natural pace of the\neconomy, and its suitability to describe the relationship between economic\ngrowth and financial activities.\n"
    },
    {
        "paper_id": 1412.7562,
        "authors": "Christa Cuchiero, Irene Klein, Josef Teichmann",
        "title": "A new perspective on the fundamental theorem of asset pricing for large\n  financial markets",
        "comments": "Missing Assumption 2.5, which is used in Lemma 4.1, has been added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of large financial markets we formulate the notion of \\emph{no\nasymptotic free lunch with vanishing risk} (NAFLVR), under which we can prove a\nversion of the fundamental theorem of asset pricing (FTAP) in markets with an\n(even uncountably) infinite number of assets, as it is for instance the case in\nbond markets. We work in the general setting of admissible portfolio wealth\nprocesses as laid down by Y. Kabanov \\cite{kab:97} under a substantially\nrelaxed concatenation property and adapt the FTAP proof variant obtained in\n\\cite{CT:14} for the classical small market situation to large financial\nmarkets. In the case of countably many assets, our setting includes the large\nfinancial market model considered by M. De Donno et al. \\cite{DGP:05} and its\nabstract integration theory.\n  The notion of (NAFLVR) turns out to be an economically meaningful \"no\narbitrage\" condition (in particular not involving weak-$*$-closures), and,\n(NAFLVR) is equivalent to the existence of a separating measure. Furthermore we\nshow -- by means of a counterexample -- that the existence of an equivalent\nseparating measure does not lead to an equivalent $\\sigma$-martingale measure,\neven in a countable large financial market situation.\n"
    },
    {
        "paper_id": 1412.7647,
        "authors": "Donald Geman, H\\'elyette Geman, and Nassim Nicholas Taleb",
        "title": "Tail Risk Constraints and Maximum Entropy",
        "comments": null,
        "journal-ref": "Entropy 17 (6), 3724-3737, 2015",
        "doi": "10.3390/e17063724",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the world of modern financial theory, portfolio construction has\ntraditionally operated under at least one of two central assumptions: the\nconstraints are derived from a utility function and/or the multivariate\nprobability distribution of the underlying asset returns is fully known. In\npractice, both the performance criteria and the informational structure are\nmarkedly different: risk-taking agents are mandated to build portfolios by\nprimarily constraining the tails of the portfolio return to satisfy VaR, stress\ntesting, or expected shortfall (CVaR) conditions, and are largely ignorant\nabout the remaining properties of the probability distributions. As an\nalternative, we derive the shape of portfolio distributions which have maximum\nentropy subject to real-world left-tail constraints and other expectations. Two\nconsequences are (i) the left-tail constraints are sufficiently powerful to\noveride other considerations in the conventional theory, rendering individual\nportfolio components of limited relevance; and (ii) the \"barbell\" payoff\n(maximal certainty/low risk on one side, maximum uncertainty on the other)\nemerges naturally from this construction.\n"
    },
    {
        "paper_id": 1412.7649,
        "authors": "Minh Man Ngo and Huyen Pham",
        "title": "Optimal switching for pairs trading rule: a viscosity solutions approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the problem of determining the optimal cut-off for pairs\ntrading rules. We consider two correlated assets whose spread is modelled by a\nmean-reverting process with stochastic volatility, and the optimal pair trading\nrule is formulated as an optimal switching problem between three regimes: flat\nposition (no holding stocks), long one short the other and short one long the\nother. A fixed commission cost is charged with each transaction. We use a\nviscosity solutions approach to prove the existence and the explicit\ncharacterization of cut-off points via the resolution of quasi-algebraic\nequations. We illustrate our results by numerical simulations.\n"
    },
    {
        "paper_id": 1412.7943,
        "authors": "Fred Espen Benth and Paul Kr\\\"uhner",
        "title": "Derivatives pricing in energy markets: an infinite dimensional approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on forward curves modelled as Hilbert-space valued processes, we\nanalyse the pricing of various options relevant in energy markets. In\nparticular, we connect empirical evidence about energy forward prices known\nfrom the literature to propose stochastic models. Forward prices can be\nrepresented as linear functions on a Hilbert space, and options can thus be\nviewed as derivatives on the whole curve. The value of these options are\ncomputed under various specifications, in addition to their deltas. In a second\npart, cross-commodity models are investigated, leading to a study of square\nintegrable random variables with values in a \"two-dimensional\" Hilbert space.\nWe analyse the covariance operator and representations of such variables, as\nwell as presenting applications to pricing of spread and energy quanto options.\n"
    },
    {
        "paper_id": 1412.8017,
        "authors": "Semei Coronado-Ram\\'irez, Pedro Celso-Arellano and Omar Rojas",
        "title": "Adaptive Market Efficiency of Agricultural Commodity Futures Contracts",
        "comments": "11 pages",
        "journal-ref": "Contaduria y Administacion, 2015, 60(2), 372-382",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  In this paper we investigate the adaptive market efficiency of the\nagricultural commodity futures market, using a sample of eight futures\ncontracts. Using a battery of nonlinear tests, we uncover the nonlinear serial\ndependence in the returns series. We run the Hinich portmanteau bicorrelation\ntest to uncover the moments in which the nonlinear serial dependence, and\ntherefore adaptive market efficiency, occurs for our sample.\n"
    },
    {
        "paper_id": 1412.8414,
        "authors": "Tim Leung and Marco Santoli",
        "title": "Accounting for Earnings Announcements in the Pricing of Equity Options",
        "comments": "34 Pages",
        "journal-ref": "Journal of Financial Engineering, Vol. 1, No. 4 (December 2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an option pricing framework that accounts for the price impact of an\nearnings announcement (EA), and analyze the behavior of the implied volatility\nsurface prior to the event. On the announcement date, we incorporate a random\njump to the stock price to represent the shock due to earnings. We consider\ndifferent distributions of the scheduled earnings jump as well as different\nunderlying stock price dynamics before and after the EA date. Our main\ncontributions include analytical option pricing formulas when the underlying\nstock price follows the Kou model along with a double-exponential or Gaussian\nEA jump on the announcement date. Furthermore, we derive analytic bounds and\nasymptotics for the pre-EA implied volatility under various models. The\ncalibration results demonstrate adequate fit of the entire implied volatility\nsurface prior to an announcement. We also compare the risk-neutral distribution\nof the EA jump to its historical distribution. Finally, we discuss the\nvaluation and exercise strategy of pre-EA American options, and illustrate an\nanalytical approximation and numerical results.\n"
    },
    {
        "paper_id": 1412.8624,
        "authors": "James Fan and Christopher Griffin",
        "title": "Optimal Digital Product Maintenance with a Continuous Revenue Stream",
        "comments": "Updated version with improved proofs and sufficiency condition\n  discussion",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a control framework to analyze the digital vendor's profit\nmaximization problem. The vendor captures market share by focusing costly\neffort on post-launch product maintenance, which influences user perception of\nthe product and drives a revenue stream associated with product use. Our\ntheoretical results show necessary and sufficient conditions for product\nmaintenance to decline over a product's life-cycle, thus showing conditions\nwhen Lehman's 7th law of software evolution holds. We also numerically\nillustrate control paths under different market conditions.\n"
    },
    {
        "paper_id": 1412.8725,
        "authors": "F. Bagarello, E. Haven",
        "title": "Towards a formalization of a two traders market with information\n  exchange",
        "comments": "In press in Physica Scripta",
        "journal-ref": null,
        "doi": "10.1088/0031-8949/90/1/015203",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows that Hamiltonians and operators can also be put to good use\neven in contexts which are not purely physics based. Consider the world of\nfinance. The work presented here {models a two traders system with information\nexchange with the help of four fundamental operators: cash and share operators;\na portfolio operator and an operator reflecting the loss of information. An\ninformation Hamiltonian is considered and an additional Hamiltonian is\npresented which reflects the dynamics of selling/buying shares between traders.\nAn important result of the paper is that when the information Hamiltonian is\nzero, portfolio operators commute with the Hamiltonian and this suggests that\nthe dynamics are really due to the information. Under the assumption that the\ninteraction and information terms in the Hamiltonian have similar strength, a\nperturbation scheme is considered on the interaction parameter. Contrary to\nintuition, the paper shows that up to a second order in the interaction\nparameter, a key factor in the computation of the portfolios of traders will be\nthe initial values of the loss of information (rather than the initial\nconditions on the cash and shares). Finally, the paper shows that a natural\noutcome from the inequality of the variation of the portfolio of trader one\nversus the variation of the portfolio of trader two, begs for the introduction\nof `good' and `bad' information. It is shown that `good' information is related\nto the reservoirs (where an infinite set of bosonic operators are used) which\nmodel rumors/news and external facts, whilst `bad' information is associated\nwith a set of two modes bosonic operators.\n"
    },
    {
        "paper_id": 1501.00026,
        "authors": "Christoph K\\\"uhn and Budhi Arta Surya and Bj\\\"orn Ulbricht",
        "title": "Optimal Selling Time of a Stock under Capital Gains Taxes",
        "comments": "20 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the impact of capital gains taxes on optimal investment\ndecisions in a quite simple model. Namely, we consider a risk neutral investor\nwho owns one risky stock from which she assumes that it has a lower expected\nreturn than the riskless bank account and determine the optimal stopping time\nat which she sells the stock to invest the proceeds in the bank account up to\nthe maturity date. In the case of linear taxes and a positive riskless interest\nrate, the problem is nontrivial because at the selling time the investor has to\nrealize book profits which triggers tax payments. We derive a boundary that is\ncontinuous and increasing in time and decreasing in the volatility of the stock\nsuch that the investor sells the stock at the first time its price is smaller\nor equal to this boundary.\n"
    },
    {
        "paper_id": 1501.0004,
        "authors": "Marya Bazzi, Mason A. Porter, Stacy Williams, Mark McDonald, Daniel J.\n  Fenn, and Sam D. Howison",
        "title": "Community detection in temporal multilayer networks, with an application\n  to correlation networks",
        "comments": "42 pages, many figures, final accepted version before typesetting",
        "journal-ref": "Multiscale Modeling and Simulation: A SIAM Interdisciplinary\n  Journal, Vol. 14, No. 1: 1--41 (2016)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Networks are a convenient way to represent complex systems of interacting\nentities. Many networks contain \"communities\" of nodes that are more densely\nconnected to each other than to nodes in the rest of the network. In this\npaper, we investigate the detection of communities in temporal networks\nrepresented as multilayer networks. As a focal example, we study time-dependent\nfinancial-asset correlation networks. We first argue that the use of the\n\"modularity\" quality function---which is defined by comparing edge weights in\nan observed network to expected edge weights in a \"null network\"---is\napplication-dependent. We differentiate between \"null networks\" and \"null\nmodels\" in our discussion of modularity maximization, and we highlight that the\nsame null network can correspond to different null models. We then investigate\na multilayer modularity-maximization problem to identify communities in\ntemporal networks. Our multilayer analysis only depends on the form of the\nmaximization problem and not on the specific quality function that one chooses.\nWe introduce a diagnostic to measure \\emph{persistence} of community structure\nin a multilayer network partition. We prove several results that describe how\nthe multilayer maximization problem measures a trade-off between static\ncommunity structure within layers and larger values of persistence across\nlayers. We also discuss some computational issues that the popular \"Louvain\"\nheuristic faces with temporal multilayer networks and suggest ways to mitigate\nthem.\n"
    },
    {
        "paper_id": 1501.00273,
        "authors": "Ren\\'e A\\\"id, Luciano Campi, Delphine Lautier",
        "title": "On the spot-futures no-arbitrage relations in commodity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In commodity markets the convergence of futures towards spot prices, at the\nexpiration of the contract, is usually justified by no-arbitrage arguments. In\nthis article, we propose an alternative approach that relies on the expected\nprofit maximization problem of an agent, producing and storing a commodity\nwhile trading in the associated futures contracts. In this framework, the\nrelation between the spot and the futures prices holds through the\nwell-posedness of the maximization problem. We show that the futures price can\nstill be seen as the risk-neutral expectation of the spot price at maturity and\nwe propose an explicit formula for the forward volatility. Moreover, we provide\nan heuristic analysis of the optimal solution for the\nproduction/storage/trading problem, in a Markovian setting. This approach is\nparticularly interesting in the case of energy commodities, like electricity:\nthis framework indeed remains suitable for commodities characterized by\nstorability constraints, when standard no-arbitrage arguments cannot be safely\napplied.\n"
    },
    {
        "paper_id": 1501.00419,
        "authors": "Christopher J. Rook",
        "title": "Minimizing the Probability of Ruin in Retirement",
        "comments": "Proofs appendix with full C++ implementation is included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Retirees who exhaust their savings while still alive are said to experience\nfinancial ruin. These savings are typically grown during the accumulation phase\nthen spent during the retirement decumulation phase. Extensive research into\ninvest-and-harvest decumulation strategies has been conducted, but\nrecommendations differ markedly. This has likely been a source of concern and\nconfusion for the retiree. Our goal is to find what has heretofore been\nelusive, namely an optimal decumulation strategy. Optimality implies that no\nalternate strategy exists or can be constructed that delivers a lower\nprobability of ruin, given a fixed inflation-adjusted withdrawal rate.\n"
    },
    {
        "paper_id": 1501.00434,
        "authors": "Stanislao Gualdi, Marco Tarzia, Francesco Zamponi, Jean-Philippe\n  Bouchaud",
        "title": "Monetary Policy and Dark Corners in a stylized Agent-Based Model",
        "comments": "Contribution to the CRISIS project, 25 pages, 21 figures, pseudo-code\n  of the model, revised and improved version",
        "journal-ref": "Journal of Economic Interaction and Coordination 12, 507-537\n  (2017)",
        "doi": "10.1007/s11403-016-0174-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend in a minimal way the stylized model introduced in in \"Tipping\nPoints in Macroeconomic Agent Based Models\" [JEDC 50, 29-61 (2015)], with the\naim of investigating the role and efficacy of monetary policy of a `Central\nBank' that sets the interest rate such as to steer the economy towards a\nprescribed inflation and employment level. Our major finding is that provided\nits policy is not too aggressive (in a sense detailed in the paper) the Central\nBank is successful in achieving its goals. However, the existence of different\nequilibrium states of the economy, separated by phase boundaries (or \"dark\ncorners\"), can cause the monetary policy itself to trigger instabilities and be\ncounter-productive. In other words, the Central Bank must navigate in a narrow\nwindow: too little is not enough, too much leads to instabilities and wildly\noscillating economies. This conclusion strongly contrasts with the prediction\nof DSGE models.\n"
    },
    {
        "paper_id": 1501.00818,
        "authors": "Florian Ziel, Rick Steinert, Sven Husmann",
        "title": "Forecasting day ahead electricity spot prices: The impact of the EXAA to\n  other European electricity markets",
        "comments": null,
        "journal-ref": "Energy Economics, 51 (2015) 430-444",
        "doi": "10.1016/j.eneco.2015.08.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our paper we analyze the relationship between the day-ahead electricity\nprice of the Energy Exchange Austria (EXAA) and other day-ahead electricity\nprices in Europe. We focus on markets, which settle their prices after the\nEXAA, which enables traders to include the EXAA price into their calculations.\nFor each market we employ econometric models to incorporate the EXAA price and\ncompare them with their counterparts without the price of the Austrian\nexchange. By employing a forecasting study, we find that electricity price\nmodels can be improved when EXAA prices are considered.\n"
    },
    {
        "paper_id": 1501.00833,
        "authors": "Jonas Alm",
        "title": "Signs of dependence and heavy tails in non-life insurance data",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study data from the yearly reports the four major Swedish\nnon-life insurers have sent to the Swedish Financial Supervisory Authority\n(FSA). We aim at finding marginal distributions of, and dependence between,\nlosses on the five largest lines of business (LoBs) in order to create models\nfor Solvency Capital Requirement (SCR) calculation. We try to use data in an\noptimal way by sensibly defining an accounting year loss in terms of actuarial\nliability predictions, and by pooling observations from several companies when\npossible to decrease the uncertainty about the underlying distributions and\ntheir parameters. We find that dependence between LoBs is weaker in our data\nthan what is assumed in the Solvency II standard formula. We also find\ndependence between companies that may affect financial stability, and must be\ntaken into account when estimating loss distribution parameters. Moreover, we\ndiscuss under what circumstances an insurer is better (or worse) off using an\ninternal model for SCR calculation instead of the standard formula.\n"
    },
    {
        "paper_id": 1501.00837,
        "authors": "Alexander Schied",
        "title": "On a class of generalized Takagi functions with linear pathwise\n  quadratic variation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class $\\mathscr{X}$ of continuous functions on $[0,1]$ that is\nof interest from two different perspectives. First, it is closely related to\nsets of functions that have been studied as generalizations of the Takagi\nfunction. Second, each function in $\\mathscr{X}$ admits a linear pathwise\nquadratic variation and can thus serve as an integrator in F\\\"ollmer's pathwise\nIt\\=o calculus. We derive several uniform properties of the class\n$\\mathscr{X}$. For instance, we compute the overall pointwise maximum, the\nuniform maximal oscillation, and the exact uniform modulus of continuity for\nall functions in $\\mathscr{X}$. Furthermore, we give an example of a pair\n$x,y\\in\\mathscr{X}$ such that the quadratic variation of the sum $x+y$ does not\nexist.\n"
    },
    {
        "paper_id": 1501.00843,
        "authors": "Ulrich Horst and Michael Paulsen",
        "title": "A law of large numbers for limit order books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define a stochastic model of a two-sided limit order book in terms of its\nkey quantities \\textit{best bid [ask] price} and the \\textit{standing buy\n[sell] volume density}. For a simple scaling of the discreteness parameters,\nthat keeps the expected volume rate over the considered price interval\ninvariant, we prove a limit theorem. The limit theorem states that, given\nregularity conditions on the random order flow, the key quantities converge in\nprobability to a tractable continuous limiting model. In the limit model the\nbuy and sell volume densities are given as the unique solution to first-order\nlinear hyperbolic PDEs, specified by the expected order flow parameters.\n"
    },
    {
        "paper_id": 1501.00882,
        "authors": "Dominik Grafenhofer, Wolgang Kuhle",
        "title": "Observing Each Other's Observations in the Electronic Mail Game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a Bayesian coordination game where agents receive private\ninformation on the game's payoff structure. In addition, agents receive private\nsignals on each other's private information. We show that once agents possess\nthese different types of information, there exists a coordination game in the\nevaluation of this information. And even though the precisions of both signal\ntypes is exogenous, the precision with which agents predict each other's\nactions at equilibrium turns out to be endogenous. As a consequence, we find\nthat there exist multiple equilibria if the private signals' precision is high.\nThese equilibria differ with regard to the way that agents weight their private\ninformation to reason about each other's actions.\n"
    },
    {
        "paper_id": 1501.01126,
        "authors": "Pengyu Qian, Zizhuo Wang, Zaiwen Wen",
        "title": "A Composite Risk Measure Framework for Decision Making under Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a unified framework for decision making under\nuncertainty. Our framework is based on the composite of two risk measures,\nwhere the inner risk measure accounts for the risk of decision given the exact\ndistribution of uncertain model parameters, and the outer risk measure\nquantifies the risk that occurs when estimating the parameters of distribution.\nWe show that the model is tractable under mild conditions. The framework is a\ngeneralization of several existing models, including stochastic programming,\nrobust optimization, distributionally robust optimization, etc. Using this\nframework, we study a few new models which imply probabilistic guarantees for\nsolutions and yield less conservative results comparing to traditional models.\nNumerical experiments are performed on portfolio selection problems to\ndemonstrate the strength of our models.\n"
    },
    {
        "paper_id": 1501.01155,
        "authors": "Mihaly Ormos, David Zibriczky",
        "title": "Entropy-Based Financial Asset Pricing",
        "comments": "21 pages, 6 figures, 3 tables and 4 supporting files",
        "journal-ref": "PLoS ONE 9(12): e115742",
        "doi": "10.1371/journal.pone.0115742",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We investigate entropy as a financial risk measure. Entropy explains the\nequity premium of securities and portfolios in a simpler way and, at the same\ntime, with higher explanatory power than the beta parameter of the capital\nasset pricing model. For asset pricing we define the continuous entropy as an\nalternative measure of risk. Our results show that entropy decreases in the\nfunction of the number of securities involved in a portfolio in a similar way\nto the standard deviation, and that efficient portfolios are situated on a\nhyperbola in the expected return - entropy system. For empirical investigation\nwe use daily returns of 150 randomly selected securities for a period of 27\nyears. Our regression results show that entropy has a higher explanatory power\nfor the expected return than the capital asset pricing model beta. Furthermore\nwe show the time varying behaviour of the beta along with entropy.\n"
    },
    {
        "paper_id": 1501.01504,
        "authors": "Mikl\\'os R\\'asonyi and Jos\\'e Gregorio Rodr\\'iguez-Villarreal",
        "title": "Optimal investment under behavioural criteria in incomplete diffusion\n  market models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The most commonly accepted model for investors' preferences is expected\nutility theory. More recently, other theories have emerged and pose new\nchallenges to mathematics. The present paper treats preferences of cumulative\nprospect theory (CPT), where an \"S-shaped\" utility function is considered (i.e.\nconvex up to a certain point and concave from there on). Also, distorted\nprobability measures are applied for calculating the utility of a given\nposition with respect to a (possibly random) benchmark $G$. Such problems have\nheretofore been solved essentially for complete continuous-time market models\nonly. In the present paper we make a step forward and consider incomplete\nmodels of a diffusion type where the return of the investment in consideration\ndepends on some economic factors. Our main result asserts, under mild\nassumptions, the existence of an optimal strategy when the driving noise of the\neconomic factors is independent of that of the investment and the rate of\nreturn is non-negative. We are also able to accommodate models of a specific\ntype where the factor may have non-zero correlation with the investment.\n"
    },
    {
        "paper_id": 1501.01573,
        "authors": "Ola Mahmoud",
        "title": "The Temporal Dimension of Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-period measures of risk account for the path that the value of an\ninvestment portfolio takes. In the context of probabilistic risk measures, the\nfocus has traditionally been on the magnitude of investment loss and not on the\ndimension associated with the passage of time. In this paper, the concept of\ntemporal path-dependent risk measure is mathematically formalized to capture\nthe risk associated with the temporal dimension of a stochastic process. We\ndiscuss the properties of temporal measures of risk and show that they can\nnever be coherent. We then study the temporal dimension of investment drawdown,\nits duration, which measures the length of excursions below a running maximum.\nIts properties in the context of risk measures are analyzed both theoretically\nand empirically. In particular, we show that duration captures serial\ncorrelation in the returns of two major asset classes. We conclude by\ndiscussing the challenges of path-dependent temporal risk estimation in\npractice.\n"
    },
    {
        "paper_id": 1501.01892,
        "authors": "Dirk Becherer, Todor Bilarev, Peter Frentrup",
        "title": "Optimal Asset Liquidation with Multiplicative Transient Price Impact",
        "comments": "To appear in Applied Mathematics and Optimization. Model assumptions\n  relaxed; corrections and improvements on referees' suggestions",
        "journal-ref": "Appl Math Optim (2018), 78 : 643 - 676",
        "doi": "10.1007/s00245-017-9418-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a multiplicative transient price impact model for an illiquid\nfinancial market, where trading causes price impact which is multiplicative in\nrelation to the current price, transient over time with finite rate of\nresilience, and non-linear in the order size. We construct explicit solutions\nfor the optimal control and the value function of singular optimal control\nproblems to maximize expected discounted proceeds from liquidating a given\nasset position. A free boundary problem, describing the optimal control, is\nsolved for two variants of the problem where admissible controls are monotone\nor of bounded variation.\n"
    },
    {
        "paper_id": 1501.01954,
        "authors": "Sergey Sosnovskiy",
        "title": "On financial applications of the two-parameter Poisson-Dirichlet\n  distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Capital distribution curve is defined as log-log plot of normalized stock\ncapitalizations ranked in descending order. The curve displays remarkable\nstability over periods of time.\n  Theory of exchangeable distributions on set partitions, developed for\npurposes of mathematical genetics and recently applied in non-parametric\nBayesian statistics, provides probabilistic-combinatorial approach for analysis\nand modeling of the capital distribution curve. Framework of the two-parameter\nPoisson-Dirichlet distribution contains rich set of methods and tools,\nincluding infinite-dimensional diffusion process.\n  The purpose of this note is to introduce framework of exchangeable\ndistributions on partitions in the financial context. In particular, it is\nshown that averaged samples from the Poisson-Dirichlet distribution provide\napproximation to the capital distribution curves in equity markets. This\nsuggests that the two-parameter model can be employed for modelling evolution\nof market weights and prices fluctuating in stochastic equilibrium.\n"
    },
    {
        "paper_id": 1501.02007,
        "authors": "Marcelo Brutti Righi, Paulo Sergio Ceretta",
        "title": "Shortfall Deviation Risk: An alternative to risk measurement",
        "comments": null,
        "journal-ref": "Journal of Risk 19, 81-116, 2016",
        "doi": "10.21314/JOR.2016.349",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the Shortfall Deviation Risk (SDR), a risk measure that represents\nthe expected loss that occurs with certain probability penalized by the\ndispersion of results that are worse than such an expectation. SDR combines\nExpected Shortfall (ES) and Shortfall Deviation (SD), which we also introduce,\ncontemplating two fundamental pillars of the risk concept, the probability of\nadverse events and the variability of an expectation, and considers extreme\nresults. We demonstrate that SD is a generalized deviation measure, whereas SDR\nis a coherent risk measure. We achieve the dual representation of SDR, and we\ndiscuss issues such as its representation by a weighted ES, acceptance sets,\nconvexity, continuity and the relationship with stochastic dominance.\nIllustrations with real and simulated data allow us to conclude that SDR offers\ngreater protection in risk measurement compared with VaR and ES, especially in\ntimes of significant turbulence in riskier scenarios.\n"
    },
    {
        "paper_id": 1501.02216,
        "authors": "Frank W. K. Firk",
        "title": "Analyses of Statistical Structures in Economic Indices",
        "comments": "7 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1205.5820",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The complex, time-dependent statistical structures observed in the Dow Jones\nIndustrial Average on a typical trading day are modeled with Lorentzian\nfunctions. The resonant-like structures are characterized by the values of the\nbasic ratio: the average lifetime of the individual states associated with a\ngiven structural form divided by the average interval between adjacent states.\nValues of the ratio are determined for three structural forms characterized by\nthe average intervals: 50 to 100 seconds (the fine structure), approximately10\nminutes, and approximately1 hour (the intermediate structures I and II). During\nthe trading day the values of the basic ratio associated with the fine\nstructure of the index are found to lie in the narrow range from 0.49 to 0.52.\nThis finding is characteristic of the highly statistical nature of many-body\nsystems typified by daily trading. It is therefore proposed that the value of\nthis ratio, determined in the first hour-or-so on a given day, be used to\nprovide information concerning the likely performance of the fine, statistical\ncomponent of the index for the remainder of the trading day. For the\nintermediate structures the basic ratios are approximately 0.6 and therefore\nthey too can be analyzed as individual states.\n  Keywords: Analytical economics; Lorentzian analyses of statistical structures\nin the Dow Jones Industrial Average; basic parameters of economic indices.\n"
    },
    {
        "paper_id": 1501.02276,
        "authors": "Tim Leung and Brian Ward",
        "title": "The Golden Target: Analyzing the Tracking Performance of Leveraged Gold\n  ETFs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the empirical tracking performance of leveraged ETFs on\ngold, and their price relationships with gold spot and futures. For tracking\nthe gold spot, we find that our optimized portfolios with short-term gold\nfutures are highly effective in replicating prices. The market-traded gold ETF\n(GLD) also exhibits a similar tracking performance. However, we show that\nleveraged gold ETFs tend to underperform their corresponding leveraged\nbenchmark. Moreover, the underperformance worsens over a longer holding period.\nIn contrast, we illustrate that a dynamic portfolio of gold futures tracks\nsignificantly better than various static portfolios. The dynamic portfolio also\nconsistently outperforms the respective market-traded LETFs for different\nleverage ratios over multiple years.\n"
    },
    {
        "paper_id": 1501.02382,
        "authors": "Jianqing Fan, Fang Han, Han Liu, Byron Vickers",
        "title": "Robust Inference of Risks of Large Portfolios",
        "comments": "45 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a bootstrap-based robust high-confidence level upper bound (Robust\nH-CLUB) for assessing the risks of large portfolios. The proposed approach\nexploits rank-based and quantile-based estimators, and can be viewed as a\nrobust extension of the H-CLUB method (Fan et al., 2015). Such an extension\nallows us to handle possibly misspecified models and heavy-tailed data. Under\nmixing conditions, we analyze the proposed approach and demonstrate its\nadvantage over the H-CLUB. We further provide thorough numerical results to\nback up the developed theory. We also apply the proposed method to analyze a\nstock market dataset.\n"
    },
    {
        "paper_id": 1501.02447,
        "authors": "Efstathios Panayi, Gareth Peters",
        "title": "Stochastic simulation framework for the Limit Order Book using liquidity\n  motivated agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a new form of agent-based model for limit order\nbooks based on heterogeneous trading agents, whose motivations are liquidity\ndriven. These agents are abstractions of real market participants, expressed in\na stochastic model framework. We develop an efficient way to perform\nstatistical calibration of the model parameters on Level 2 limit order book\ndata from Chi-X, based on a combination of indirect inference and\nmulti-objective optimisation. We then demonstrate how such an agent-based\nmodelling framework can be of use in testing exchange regulations, as well as\ninforming brokerage decisions and other trading based scenarios.\n"
    },
    {
        "paper_id": 1501.02513,
        "authors": "Piotr Jaworski and Marcin Pitera",
        "title": "The 20-60-20 Rule",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we discuss an empirical phenomena known as the 20-60-20 rule.\nIt says that if we split the population into three groups, according to some\narbitrary benchmark criterion, then this particular ratio implies some sort of\nbalance. From practical point of view, this feature often leads to efficient\nmanagement or control. We provide a mathematical illustration, justifying the\noccurrence of this rule in many real world situations. We show that for any\npopulation, which could be described using multivariate normal vector, this\nfixed ratio leads to a global equilibrium state, when dispersion and linear\ndependance measurement is considered.\n"
    },
    {
        "paper_id": 1501.0275,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Self-Financing Trading and the Ito-Doeblin Lemma",
        "comments": "3 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of the note is to remind readers on how self-financing works in\nQuantitative Finance. The authors have observed continuing uncertainty on this\nissue which may be because it lies exactly at the intersection of stochastic\ncalculus and finance. The concept of a self-financing trading strategy was\noriginally, and carefully, introduced in (Harrison and Kreps 1979) and expanded\nvery generally in (Harrison and Pliska 1981).\n"
    },
    {
        "paper_id": 1501.03123,
        "authors": "Laurence Carassus, Mikl\\'os R\\'asonyi, Andrea M. Rodrigues",
        "title": "Non-concave utility maximisation on the positive real axis in discrete\n  time",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We treat a discrete-time asset allocation problem in an arbitrage-free,\ngenerically incomplete financial market, where the investor has a possibly\nnon-concave utility function and wealth is restricted to remain non-negative.\nUnder easily verifiable conditions, we establish the existence of optimal\nportfolios.\n"
    },
    {
        "paper_id": 1501.03371,
        "authors": "Leonardo Ermann and Dima L. Shepelyansky",
        "title": "Google matrix analysis of the multiproduct world trade network",
        "comments": "19 pages, 25 figures",
        "journal-ref": "Eur. Phys. J. B (2015) 88, 84",
        "doi": "10.1140/epjb/e2015-60047-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the United Nations COMTRADE database \\cite{comtrade} we construct the\nGoogle matrix $G$ of multiproduct world trade between the UN countries and\nanalyze the properties of trade flows on this network for years 1962 - 2010.\nThis construction, based on Markov chains, treats all countries on equal\ndemocratic grounds independently of their richness and at the same time it\nconsiders the contributions of trade products proportionally to their trade\nvolume. We consider the trade with 61 products for up to 227 countries. The\nobtained results show that the trade contribution of products is asymmetric:\nsome of them are export oriented while others are import oriented even if the\nranking by their trade volume is symmetric in respect to export and import\nafter averaging over all world countries. The construction of the Google matrix\nallows to investigate the sensitivity of trade balance in respect to price\nvariations of products, e.g. petroleum and gas, taking into account the world\nconnectivity of trade links. The trade balance based on PageRank and CheiRank\nprobabilities highlights the leading role of China and other BRICS countries in\nthe world trade in recent years. We also show that the eigenstates of $G$ with\nlarge eigenvalues select specific trade communities.\n"
    },
    {
        "paper_id": 1501.03387,
        "authors": "Francesco Caravenna and Jacopo Corbetta",
        "title": "The asymptotic smile of a multiscaling stochastic volatility model",
        "comments": "36 pages, 3 figures. Final version, to appear in Stochastic Process.\n  Appl",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic volatility model which captures relevant stylized\nfacts of financial series, including the multi-scaling of moments. The\nvolatility evolves according to a generalized Ornstein-Uhlenbeck processes with\nsuper-linear mean reversion.\n  Using large deviations techniques, we determine the asymptotic shape of the\nimplied volatility surface in any regime of small maturity $t \\to 0$ or extreme\nlog-strike $|\\kappa| \\to \\infty$ (with bounded maturity). Even if the price has\ncontinuous paths, out-of-the-money implied volatility diverges for small\nmaturity, producing a very pronounced smile.\n"
    },
    {
        "paper_id": 1501.03701,
        "authors": "Raphael Hauser and Sergey Shahverdyan",
        "title": "A New Approach to Model Free Option Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new approach to model-free path-dependent option\npricing. We first introduce a general duality result for linear optimisation\nproblems over signed measures introduced in [3] and show how the the problem of\nmodel-free option pricing can be formulated in the new framework. We then\nintroduce a model to solve the problem numerically when the only information\nprovided is the market data of vanilla call or put option prices. Compared to\nthe common approaches in the literature, e.g. [4], the model does not require\nthe marginal distributions of the stock price for different maturities. Though\nthe experiments are carried out for simple path-dependent options on a single\nstock, the model is easy to generalise for multi-asset framework.\n"
    },
    {
        "paper_id": 1501.03756,
        "authors": "Filippo Passerini and Samuel E. Vazquez",
        "title": "Optimal Trading with Alpha Predictors",
        "comments": "28 pages, 6 figures; v2: corrected typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of optimal trading using general alpha predictors with\nlinear costs and temporary impact. We do this within the framework of\nstochastic optimization with finite horizon using both limit and market orders.\nConsistently with other studies, we find that the presence of linear costs\ninduces a no-trading zone when using market orders, and a corresponding\nmarket-making zone when using limit orders. We show that, when combining both\nmarket and limit orders, the problem is further divided into zones in which we\ntrade more aggressively using market orders. Even though we do not solve\nanalytically the full optimization problem, we present explicit and simple\nanalytical recipes which approximate the full solution and are easy to\nimplement in practice. We test the algorithms using Monte Carlo simulations and\nshow how they improve our Profit and Losses.\n"
    },
    {
        "paper_id": 1501.03768,
        "authors": "Leslaw Gajek and Marek Kaluszka",
        "title": "On the martingale-fair index of return for investment funds",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A concept of martingale-fair index of return, consistent with Arbitrage Free\nPricing Theory, is introduced. An explicit formula for the average rate of\nreturn of a group of investment/pension funds in a discrete time stochastic\nmodel is derived and several properties of this index are shown. In particular,\nit is proven to be martingale-fair, i.e. be a martingale provided the prices of\nassets on the financial market form a vector martingale. The problem of merger\nof the funds is treated in detail.\n"
    },
    {
        "paper_id": 1501.04123,
        "authors": "Aurelio Fernandez Bariviera, M. Bel\\'en Guercio, Lisana B. Martinez",
        "title": "Data manipulation detection via permutation information theory\n  quantifiers",
        "comments": "Preprint presented at XVIII Conference on Nonequilibrium Statistical\n  Mechanics and Nonlinear Physics MEDYFINOL 2014. Macei\\'o (AL), Brazil",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent news cast doubts on London Interbank Offered Rate (LIBOR) integrity.\nGiven its economic importance and the delay with which authorities realize\nabout this situation, we aim to find an objective method in order to detect\ndepartures in the LIBOR rate that from the expected behavior. We analyze\nseveral interest rates time series and we detect an anomalous behavior in\nLIBOR, specially during the period of the financial crisis of 2008. Our\nfindings could be consistent with data manipulation.\n"
    },
    {
        "paper_id": 1501.04274,
        "authors": "Ioannis Karatzas and Constantinos Kardaras",
        "title": "Optional Decomposition for continuous semimartingales under arbitrary\n  filtrations",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an elementary treatment of the Optional Decomposition Theorem for\ncontinuous semimartingales and general filtrations. This treatment does not\nassume the existence of equivalent local martingale measure(s), only that of\nstrictly positive local martingale deflator(s).\n"
    },
    {
        "paper_id": 1501.04548,
        "authors": "Rohini Kumar",
        "title": "Effect of Volatility Clustering on Indifference Pricing of Options by\n  Convex Risk Measures",
        "comments": "Applied Mathematical Finance, 2014",
        "journal-ref": null,
        "doi": "10.1080/1350486X.2014.949805",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we look at the effect of volatility clustering on the risk\nindifference price of options described by Sircar and Sturm in their paper\n(Sircar, R., & Sturm, S. (2012). From smile asymptotics to market risk\nmeasures. Mathematical Finance. Advance online publication.\ndoi:10.1111/mafi.12015). The indifference price in their article is obtained by\nusing dynamic convex risk measures given by backward stochastic differential\nequations. Volatility clustering is modelled by a fast mean-reverting\nvolatility in a stochastic volatility model for stock price. Asymptotics of the\nindifference price of options and their corresponding implied volatility are\nobtained in this article, as the mean-reversion time approaches zero.\nCorrection terms to the asymptotic option price and implied volatility are also\nobtained.\n"
    },
    {
        "paper_id": 1501.04575,
        "authors": "Ren\\'e A\\\"id, Pierre Gruet, Huy\\^en Pham",
        "title": "An optimal trading problem in intraday electricity markets",
        "comments": "39 pages, 11 figures",
        "journal-ref": "Mathematics and Financial Economics, 2016, vol. 10, no 1, p. 49-85",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal trading for a power producer in the\ncontext of intraday electricity markets. The aim is to minimize the imbalance\ncost induced by the random residual demand in electricity, i.e. the consumption\nfrom the clients minus the production from renewable energy. For a simple\nlinear price impact model and a quadratic criterion, we explicitly obtain\napproximate optimal strategies in the intraday market and thermal power\ngeneration, and exhibit some remarkable properties of the trading rate.\nFurthermore, we study the case when there are jumps on the demand forecast and\non the intraday price, typically due to error in the prediction of wind power\ngeneration. Finally, we solve the problem when taking into account delay\nconstraints in thermal power production.\n"
    },
    {
        "paper_id": 1501.04682,
        "authors": "Markus Holopainen, Peter Sarlin",
        "title": "Toward robust early-warning models: A horse race, ensembles and model\n  uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents first steps toward robust models for crisis prediction.\nWe conduct a horse race of conventional statistical methods and more recent\nmachine learning methods as early-warning models. As individual models are in\nthe literature most often built in isolation of other methods, the exercise is\nof high relevance for assessing the relative performance of a wide variety of\nmethods. Further, we test various ensemble approaches to aggregating the\ninformation products of the built models, providing a more robust basis for\nmeasuring country-level vulnerabilities. Finally, we provide approaches to\nestimating model uncertainty in early-warning exercises, particularly model\nperformance uncertainty and model output uncertainty. The approaches put\nforward in this paper are shown with Europe as a playground. Generally, our\nresults show that the conventional statistical approaches are outperformed by\nmore advanced machine learning methods, such as k-nearest neighbors and neural\nnetworks, and particularly by model aggregation approaches through ensemble\nlearning.\n"
    },
    {
        "paper_id": 1501.04747,
        "authors": "Hao Xing",
        "title": "Consumption investment optimization with Epstein-Zin utility in\n  incomplete markets",
        "comments": "30 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a market with stochastic investment opportunities, we study an optimal\nconsumption investment problem for an agent with recursive utility of\nEpstein-Zin type. Focusing on the empirically relevant specification where both\nrisk aversion and elasticity of intertemporal substitution are in excess of\none, we characterize optimal consumption and investment strategies via backward\nstochastic differential equations. The supperdifferential of indirect utility\nis also obtained, meeting demands from applications in which Epstein-Zin\nutilities were used to resolve several asset pricing puzzles. The empirically\nrelevant utility specification introduces difficulties to the optimization\nproblem due to the fact that the Epstein-Zin aggregator is neither Lipschitz\nnor jointly concave in all its variables.\n"
    },
    {
        "paper_id": 1501.04992,
        "authors": "Franco Ruzzenenti, Andreas Joseph, Elisa Ticci, Pietro Vozzella,\n  Giampaolo Gabbi",
        "title": "Interactions between financial and environmental networks in OECD\n  countries",
        "comments": "Supplementary Information provided",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0136767",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse a multiplex of networks between OECD countries during the decade\n2002-2010, which consists of five financial layers, given by foreign direct\ninvestment, equity securities, short-term, long-term and total debt securities,\nand five environmental layers, given by emissions of N O x, P M 10 SO 2, CO 2\nequivalent and the water footprint associated with international trade. We\npresent a new measure of cross-layer correlations between flows in different\nlayers based on reciprocity. For the assessment of results, we implement a null\nmodel for this measure based on the exponential random graph theory. We find\nthat short-term financial flows are more correlated with environmental flows\nthan long-term investments. Moreover, the correlations between reverse\nfinancial and environmental flows (i.e. flows of different layers going in\nopposite directions) are generally stronger than correlations between synergic\nflows (flows going in the same direction). This suggests a trade-off between\nfinancial and environmental layers, where, more financialised countries display\nhigher correlations between outgoing financial flows and incoming environmental\nflows from lower financialised countries, which could have important policy\nimplications. Five countries are identified as hubs in this finance-environment\nmultiplex: The United States, France, Germany, Belgium-Luxembourg and the\nUnited Kingdom.\n"
    },
    {
        "paper_id": 1501.0504,
        "authors": "Filipi N. Silva, Cesar H. Comin, Thomas K. DM. Peron, Francisco A.\n  Rodrigues, Cheng Ye, Richard C. Wilson, Edwin Hancock, Luciano da F. Costa",
        "title": "Modular Dynamics of Financial Market Networks",
        "comments": "13 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial market is a complex dynamical system composed of a large\nvariety of intricate relationships between several entities, such as banks,\ncorporations and institutions. At the heart of the system lies the stock\nexchange mechanism, which establishes a time-evolving network of trades among\ncompanies and individuals. Such network can be inferred through correlations\nbetween time series of companies stock prices, allowing the overall system to\nbe characterized by techniques borrowed from network science. Here we study the\npresence of communities in the inferred stock market network, and show that the\nknowledge about the communities alone can provide a nearly complete\nrepresentation of the system topology. This is done by defining a simple null\nmodel, a randomized version of the studied network sharing only the sizes and\ninterconnectivity between communities observed. We show that many topological\ncharacteristics of the inferred networks are carried over the networks\ngenerated by the null model. In particular, we find that in periods of\ninstability, such as during a financial crisis, the network strays away from a\nstate of well-defined community structure to a much more uniform topological\norganization. We show that the framework presented here provides a good null\nmodel representation of topological variations taking place in the market\nduring crises. Also, the general approach used in this work can be extended to\nother systems.\n"
    },
    {
        "paper_id": 1501.05176,
        "authors": "Esteban Guevara Hidalgo",
        "title": "Bin Size Independence in Intra-day Seasonalities for Relative Prices",
        "comments": null,
        "journal-ref": "Physica A 468, 722-732 (2017)",
        "doi": "10.1016/j.physa.2016.11.128",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we perform a statistical analysis over the returns and relative\nprices of the CAC $40$ and the S\\&P $500$ with the purpose of analyzing the\nintra-day seasonalities of single and cross-sectional stock dynamics. In order\nto do that, we characterized the dynamics of a stock (or a set of stocks) by\nthe evolution of the moments of its returns (and relative prices) during a\ntypical day. We show that these intra-day seasonalities are independent of the\nsize of the bin, and the index we consider, (but characteristic for each index)\nfor the case of the relative prices but not for the case of the returns.\nFinally, we suggest how this bin size independence could be used to\ncharacterize \"atypical days\" for indexes and \"anomalous behaviours\" in stocks.\n"
    },
    {
        "paper_id": 1501.05381,
        "authors": "Zura Kakushadze",
        "title": "Combining Alphas via Bounded Regression",
        "comments": "20 pages; a clarifying footnote added, references updated, no other\n  changes; to appear in Risks",
        "journal-ref": "Risks 3(4) (2015) 474-490",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit algorithm and source code for combining alpha streams via\nbounded regression. In practical applications typically there is insufficient\nhistory to compute a sample covariance matrix (SCM) for a large number of\nalphas. To compute alpha allocation weights, one then resorts to (weighted)\nregression over SCM principal components. Regression often produces alpha\nweights with insufficient diversification and/or skewed distribution against,\ne.g., turnover. This can be rectified by imposing bounds on alpha weights\nwithin the regression procedure. Bounded regression can also be applied to\nstock and other asset portfolio construction. We discuss illustrative examples.\n"
    },
    {
        "paper_id": 1501.054,
        "authors": "Charles D. Brummitt, Teruyoshi Kobayashi",
        "title": "Cascades in multiplex financial networks with debts of different\n  seniority",
        "comments": "16 pages, 7 figures",
        "journal-ref": "Phys. Rev. E 91, 062813, 2015",
        "doi": "10.1103/PhysRevE.91.062813",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The seniority of debt, which determines the order in which a bankrupt\ninstitution repays its debts, is an important and sometimes contentious feature\nof financial crises, yet its impact on system-wide stability is not well\nunderstood. We capture seniority of debt in a multiplex network, a graph of\nnodes connected by multiple types of edges. Here, an edge between banks denotes\na debt contract of a certain level of seniority. Next we study cascading\ndefault. There exist multiple kinds of bankruptcy, indexed by the highest level\nof seniority at which a bank cannot repay all its debts. Self-interested banks\nwould prefer that all their loans be made at the most senior level. However,\nmixing debts of different seniority levels makes the system more stable, in\nthat it shrinks the set of network densities for which bankruptcies spread\nwidely. We compute the optimal ratio of senior to junior debts, which we call\nthe optimal seniority ratio, for two uncorrelated Erdos-Renyi networks. If\ninstitutions erode their buffer against insolvency, then this optimal seniority\nratio rises; in other words, if default thresholds fall, then more loans should\nbe senior. We generalize the analytical results to arbitrarily many levels of\nseniority and to heavy-tailed degree distributions.\n"
    },
    {
        "paper_id": 1501.05751,
        "authors": "Leonardo Bargigli, Giovanni di Iasio, Luigi Infante, Fabrizio Lillo\n  and Federico Pierobon",
        "title": "Interbank markets and multiplex networks: centrality measures and\n  statistical null models",
        "comments": "To appear in the book \"Interconnected Networks\", A. Garas e F.\n  Schweitzer (eds.), Springer Complexity Series",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The interbank market is considered one of the most important channels of\ncontagion. Its network representation, where banks and claims/obligations are\nrepresented by nodes and links (respectively), has received a lot of attention\nin the recent theoretical and empirical literature, for assessing systemic risk\nand identifying systematically important financial institutions. Different\ntypes of links, for example in terms of maturity and collateralization of the\nclaim/obligation, can be established between financial institutions. Therefore\na natural representation of the interbank structure which takes into account\nmore features of the market, is a multiplex, where each layer is associated\nwith a type of link. In this paper we review the empirical structure of the\nmultiplex and the theoretical consequences of this representation. We also\ninvestigate the betweenness and eigenvector centrality of a bank in the\nnetwork, comparing its centrality properties across different layers and with\nMaximum Entropy null models.\n"
    },
    {
        "paper_id": 1501.05771,
        "authors": "Nikolay Klemashev, Alexander Shananin",
        "title": "Positively-homogeneous Konus-Divisia indices and their applications to\n  demand analysis and forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to revealed preference theory and its applications to\ntesting economic data for consistency with utility maximization hypothesis,\nconstruction of index numbers, and forecasting. The quantitative measures of\ninconsistency of economic data with utility maximization behavior are also\ndiscussed. The structure of the paper is based on comparison between the two\ntests of revealed preference theory - generalized axiom of revealed preference\n(GARP) and homothetic axiom of revealed prefernce (HARP). We do this comparison\nboth theoretically and empirically. In particular we assess empirically the\npower of these tests for consistency with maximization behavior and the size of\nforecasting sets based on them. For the forecasting problem we show that when\nusing HARP there is an effective way of building the forecasting set since this\nset is given by the solution of the system of linear inequalities. The paper\nalso touches upon the question of testing a set of Engel curves rather than\nfinite set of observations for consistency with utility maximization behavior\nand shows that this question has effective solution when we require the\nrationalizing utility function to be positively homogeneous.\n"
    },
    {
        "paper_id": 1501.05893,
        "authors": "Maxim Bichuch, Agostino Capponi, Stephan Sturm",
        "title": "Arbitrage-Free Pricing of XVA -- Part I: Framework and Explicit Examples",
        "comments": "39 pages, 7 figures. The paper arXiv:1608.02690 subsumes the present\n  working paper as well as arXiv:1502.06106",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel framework for computing the total valuation adjustment\n(XVA) of a European claim accounting for funding costs, counterparty credit\nrisk, and collateralization. Based on no-arbitrage arguments, we derive the\nnonlinear backward stochastic differential equations (BSDEs) associated with\nthe replicating portfolios of long and short positions in the claim. This leads\nto the definition of buyer's and seller's XVA which in turn identify a\nno-arbitrage interval. When borrowing and lending rates coincide we provide a\nfully explicit expression for the uniquely determined price of XVA, expressed\nas a percentage of the price of the traded claim, and for the corresponding\nreplication strategies. This extends the result of Piterbarg by incorporating\nthe effect of premature contract termination due to default risk of the trader\nand of his counterparty.\n"
    },
    {
        "paper_id": 1501.06084,
        "authors": "Andrei Cozma and Matthieu Mariapragassam and Christoph Reisinger",
        "title": "Convergence of an Euler scheme for a hybrid stochastic-local volatility\n  model with stochastic rates in foreign exchange markets",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Heston-Cox-Ingersoll-Ross++ stochastic-local volatility model in\nthe context of foreign exchange markets and propose a Monte Carlo simulation\nscheme which combines the full truncation Euler scheme for the stochastic\nvolatility component and the stochastic domestic and foreign short interest\nrates with the log-Euler scheme for the exchange rate. We establish the\nexponential integrability of full truncation Euler approximations for the\nCox-Ingersoll-Ross process and find a lower bound on the explosion time of\nthese exponential moments. Under a full correlation structure and a realistic\nset of assumptions on the so-called leverage function, we prove the strong\nconvergence of the exchange rate approximations and deduce the convergence of\nMonte Carlo estimators for a number of vanilla and path-dependent options.\nThen, we perform a series of numerical experiments for an autocallable barrier\ndual currency note.\n"
    },
    {
        "paper_id": 1501.06221,
        "authors": "Jinbeom Kim, Tim Leung",
        "title": "Pricing Derivatives with Counterparty Risk and Collateralization: A\n  Fixed Point Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a valuation framework for financial contracts subject to\nreference and counterparty default risks with collateralization requirement. We\npropose a fixed point approach to analyze the mark-to-market contract value\nwith counterparty risk provision, and show that it is a unique bounded and\ncontinuous fixed point via contraction mapping. This leads us to develop an\naccurate iterative numerical scheme for valuation. Specifically, we solve a\nsequence of linear inhomogeneous PDEs, whose solutions converge to the fixed\npoint price function. We apply our methodology to compute the bid and ask\nprices for both defaultable equity and fixed-income derivatives, and illustrate\nthe non-trivial effects of counterparty risk, collateralization ratio and\nliquidation convention on the bid-ask spreads.\n"
    },
    {
        "paper_id": 1501.0698,
        "authors": "Masaaki Fukasawa",
        "title": "Short-time at-the-money skew and rough fractional volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Scholes implied volatility skew at the money of SPX options is\nknown to obey a power law with respect to the time-to-maturity. We construct a\nmodel of the underlying asset price process which is dynamically consistent to\nthe power law. The volatility process of the model is driven by a fractional\nBrownian motion with Hurst parameter less than half. The fractional Brownian\nmotion is correlated with a Brownian motion which drives the asset price\nprocess. We derive an asymptotic expansion of the implied volatility as the\ntime-to-maturity tends to zero. For this purpose we introduce a new approach to\nvalidate such an expansion, which enables us to treat more general models than\nin the literature. The local-stochastic volatility model is treated as well\nunder an essentially minimal regularity condition in order to show such a\nstandard model cannot be dynamically consistent to the power law.\n"
    },
    {
        "paper_id": 1501.07124,
        "authors": "O.S. Rozanova, G.S. Kambarbaeva",
        "title": "Optimal strategies of investment in a linear stochastic model of market",
        "comments": "35 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:0905.4740 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the continuous time portfolio optimization model on the market where\nthe mean returns of individual securities or asset categories are linearly\ndependent on underlying economic factors. We introduce the functional\n$Q_\\gamma$ featuring the expected earnings yield of portfolio minus a penalty\nterm proportional with a coefficient $\\gamma$ to the variance when we keep the\nvalue of the factor levels fixed. The coefficient $\\gamma$ plays the role of a\nrisk-aversion parameter. We find the optimal trading positions that can be\nobtained as the solution to a maximization problem for $Q_\\gamma$ at any moment\nof time. The single-factor case is analyzed in more details. We present a\nsimple asset allocation example featuring an interest rate which affects a\nstock index and also serves as a second investment opportunity. We consider two\npossibilities: the interest rate for the bank account is governed by\nVasicek-type and Cox-Ingersoll-Ross dynamics, respectively. Then we compare our\nresults with the theory of Bielecki and Pliska where the authors employ the\nmethods of the risk-sensitive control theory thereby using an infinite horizon\nobjective featuring the long run expected growth rate, the asymptotic variance,\nand a risk-aversion parameter similar to $\\gamma$.\n"
    },
    {
        "paper_id": 1501.07297,
        "authors": "Gildas Ratovomirija",
        "title": "Multivariate Stop loss Mixed Erlang Reinsurance risk: Aggregation,\n  Capital allocation and Default risk",
        "comments": "20 Pages, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address the aggregation of dependent stop loss reinsurance\nrisks where the dependence among the ceding insurer(s) risks is governed by the\nSarmanov distribution and each individual risk belongs to the class of Erlang\nmixtures. We investigate the effects of the ceding insurer(s) risk dependencies\non the reinsurer risk profile by deriving a closed formula for the distribution\nfunction of the aggregated stop loss reinsurance risk. Furthermore,\ndiversification effects from aggregating reinsurance risks are examined by\nderiving a closed expression for the risk capital needed for the whole\nportfolio of the reinsurer and also the allocated risk capital for each\nbusiness unit under the TVaR capital allocation principle. Moreover, given the\nrisk capital that the reinsurer holds, we express the default probability of\nthe reinsurer analytically. In case the reinsurer is in default, we determine\nanalytical expressions for the amount of the aggregate reinsured unpaid losses\nand the unpaid losses of each reinsured line of business of the ceding\ninsurer(s). These results are illustrated by numerical examples.\n"
    },
    {
        "paper_id": 1501.07402,
        "authors": "Johannes Hain, Tom Fischer",
        "title": "Valuation Algorithms for Structural Models of Financial\n  Interconnectedness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Much research in systemic risk is focused on default contagion. While this\ndemands an understanding of valuation, fewer articles specifically deal with\nthe existence, the uniqueness, and the computation of equilibrium prices in\nstructural models of interconnected financial systems. However, beyond\ncontagion research, these topics are also essential for risk-neutral pricing.\nIn this article, we therefore study and compare valuation algorithms in the\nstandard model of debt and equity cross-ownership which has crystallized in the\nwork of several authors over the past one and a half decades. Since known\nalgorithms have potentially infinite runtime, we develop a class of new\nalgorithms, which find exact solutions in finitely many calculation steps. A\nsimulation study for a range of financial system designs allows us to derive\nconclusions about the efficiency of different numerical methods under different\nsystem parameters.\n"
    },
    {
        "paper_id": 1501.07404,
        "authors": "Christophe Michel, Victor Reutenauer, Denis Talay and Etienne Tanr\\'e",
        "title": "Liquidity costs: a new numerical methodology and an empirical study",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/1350486X.2016.1164608",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider rate swaps which pay a fixed rate against a floating rate in\npresence of bid-ask spread costs. Even for simple models of bid-ask spread\ncosts, there is no explicit strategy optimizing an expected function of the\nhedging error. We here propose an efficient algorithm based on the stochastic\ngradient method to compute an approximate optimal strategy without solving a\nstochastic control problem. We validate our algorithm by numerical experiments.\nWe also develop several variants of the algorithm and discuss their\nperformances in terms of the numerical parameters and the liquidity cost.\n"
    },
    {
        "paper_id": 1501.07473,
        "authors": "Yannis G. Yatracos",
        "title": "Information in stock prices and some consequences: A model-free approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price of a stock will rarely follow the assumed model and a curious\ninvestor or a Regulatory Authority may wish to obtain a probability model the\nprices support. A risk neutral probability ${\\cal P}^*$ for the stock's price\nat time $T$ is determined in closed form from the prices before $T$ without\nassuming a price model. The findings indicate that ${\\cal P}^*$ may be a\nmixture. Under mild conditions on the prices the necessary and sufficient\ncondition to obtain ${\\cal P}^*$ is the coincidence at $T$ of the stock price\nranges assumed by the stock's trader and buyer. This result clarifies the\nrelation between market's informational efficiency and the arbitrage-free\noption pricing methodology. It also shows that in an incomplete market there\nare risk neutral probabilities not supported by each stock and their use can be\nlimited. ${\\cal P}^*$-price $C$ for the stock's European call option expiring\nat $T$ is obtained. Among other results it is shown for \"calm\" prices, like the\nlog-normal, that i) $C$ is the Black-Scholes-Merton price thus confirming its\nvalidity for various stock prices, ii) the buyer's price carries an\nexponentially increasing volatility premium and its difference with $C$\nprovides a measure of the market risk premium.\n"
    },
    {
        "paper_id": 1501.0748,
        "authors": "Oliver Janke and Qinghua Li",
        "title": "Portfolio Optimization under Shortfall Risk Constraint",
        "comments": "25 pages, Optimization, 2016",
        "journal-ref": "Optimization, 65(9), 1733-1755 (2016)",
        "doi": "10.1080/02331934.2016.1173693",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper solves a utility maximization problem under utility-based\nshortfall risk constraint, by proposing an approach using Lagrange multiplier\nand convex duality. Under mild conditions on the asymptotic elasticity of the\nutility function and the loss function, we find an optimal wealth process for\nthe constrained problem and characterize the bi-dual relation between the\nrespective value functions of the constrained problem and its dual. This\napproach applies to both complete and incomplete markets. Moreover, the\nextension to more complicated cases is illustrated by solving the problem with\na consumption process added. Finally, we give an example of utility and loss\nfunctions in the Black-Scholes market where the solutions have explicit forms.\n"
    },
    {
        "paper_id": 1501.07504,
        "authors": "J.E. Wesen, V.VV. Vermehren and H.M. de Oliveira",
        "title": "Adaptive Filter Design for Stock Market Prediction Using a\n  Correlation-based Criterion",
        "comments": "Quantitative Finance? 9 pages, 9 figures, 1 table. Conference: XLIII\n  Simposio Brasileiro de Pesquisa Operacional, August 2011, Ubatuba-SP, Brazil",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel adaptive-filter approach for predicting assets on\nthe stock markets. Concepts are introduced here, which allow understanding this\nmethod and computing of the corresponding forecast. This approach is applied,\nas an example, through the prediction over the actual valuation of the PETR3\nshares (Petrobras ON) traded in the Brazilian Stock Market. The first-rate\nchoices of the window length and the number of filter coefficient are\nevaluated. This is done by observing the correlation between the predictor\nsignal and the actual course performed by the market in terms of both the\nwindow prevision length and filter coefficient values. It is shown that such\nadaptive predictors furnish, on the average, very substantial profit on the\ninvested amount.\n"
    },
    {
        "paper_id": 1501.07778,
        "authors": "Patrick Steffen Michelberger and Jan Hendrik Witte",
        "title": "Foreign Exchange Market Microstructure and the WM/Reuters 4pm Fix",
        "comments": "20 Pages, 6 Figures, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A market fix serves as a benchmark for foreign exchange (FX) execution, and\nis employed by many institutional investors to establish an exact reference at\nwhich execution takes place. The currently most popular FX fix is the World\nMarket Reuters (WM/R) 4pm fix. Execution at the WM/R 4pm fix is a service\noffered by FX brokers (normally banks), who deliver execution at the fix\nprovided they obtain the trade order until a certain time prior to 4pm. In this\npaper, we study the market microstructure around 4pm. We demonstrate that\nmarket dynamics can be distinguished from other times during the day through\nincreased volatility and size of movements. Our findings question the aggregate\nbenefit to the client base of using the 4pm fix in its current form.\n"
    },
    {
        "paper_id": 1502.00104,
        "authors": "Michal Paulus and Ladislav Kristoufek",
        "title": "Worldwide clustering of the corruption perception",
        "comments": "16 pages, 3 figures",
        "journal-ref": "Physica A: Statistical Mechanics and Its Applications (2015), vol.\n  428, pp. 351-358",
        "doi": "10.1016/j.physa.2015.01.065",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We inspect a possible clustering structure of the corruption perception among\n134 countries. Using the average linkage clustering, we uncover a well-defined\nhierarchy in the relationships among countries. Four main clusters are\nidentified and they suggest that countries worldwide can be quite well\nseparated according to their perception of corruption. Moreover, we find a\nstrong connection between corruption levels and a stage of development inside\nthe clusters. The ranking of countries according to their corruption perfectly\ncopies the ranking according to the economic performance measured by the gross\ndomestic product per capita of the member states. To the best of our knowledge,\nthis study is the first one to present an application of hierarchical and\nclustering methods to the specific case of corruption.\n"
    },
    {
        "paper_id": 1502.00218,
        "authors": "Angus O. Unegbu, Augustine Okanlawon",
        "title": "Direct Foreign Investment in Kurdistan Region of Middle-East: Non-Oil\n  Sector Analysis",
        "comments": "38-49",
        "journal-ref": "BJEMT.2015.041",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kurdistan Region is a tourist hub. This research analyzes other Non-Oil\nSectors that have huge attractions of Foreign Direct Investments into the\nKurdistan Region from 2005 to 2013. Comparative analysis was carried out\nbetween Iraq and the Region, and among influential Sectors of the Economy.\nT-test and ANOVA are statistical tools employed in testing the research\nhypotheses. The research identify that there exist significant Foreign Direct\nInvestment inflows across the governorates in the region and among influential\nsectors of the Economy. The research also highlighted areas of high level of\ninvestment needs, sectors that have been crowded out and business opportunities\nin the region that requires huge Foreign Direct Investments. It is recommended\nthat the Regional Kurdistan Government should embark on fiscal Cashless\npolicies in order to stimulate further spill-off effects of attracting enormous\nNon-Oil Sectors of Foreign Direct Investments into the region.\n"
    },
    {
        "paper_id": 1502.00225,
        "authors": "Ladislav Kristoufek",
        "title": "Power-law correlations in finance-related Google searches, and their\n  cross-correlations with volatility and traded volume: Evidence from the Dow\n  Jones Industrial components",
        "comments": "22 pages, 6 figures",
        "journal-ref": "Physica A: Statistical Mechanics and Its Applications (2015), vol.\n  428, pp. 194-205",
        "doi": "10.1016/j.physa.2015.02.057",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We study power-law correlations properties of the Google search queries for\nDow Jones Industrial Average (DJIA) component stocks. Examining the daily data\nof the searched terms with a combination of the rescaled range and rescaled\nvariance tests together with the detrended fluctuation analysis, we show that\nthe searches are in fact power-law correlated with Hurst exponents between 0.8\nand 1.1. The general interest in the DJIA stocks is thus strongly persistent.\nWe further reinvestigate the cross-correlation structure between the searches,\ntraded volume and volatility of the component stocks using the detrended\ncross-correlation and detrending moving-average cross-correlation coefficients.\nContrary to the universal power-law correlations structure of the related\nGoogle searches, the results suggest that there is no universal relationship\nbetween the online search queries and the analyzed financial measures. Even\nthough we confirm positive correlation for a majority of pairs, there are\nseveral pairs with insignificant or even negative correlations. In addition,\nthe correlations vary quite strongly across scales.\n"
    },
    {
        "paper_id": 1502.00358,
        "authors": "Tim Leung and Yoshihiro Shirai",
        "title": "Optimal Derivative Liquidation Timing Under Path-Dependent Risk\n  Penalties",
        "comments": "26 pages, 11 figures",
        "journal-ref": "Journal of Financial Engineering vol 2, issue 1, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the risk-adjusted optimal timing to liquidate an option at\nthe prevailing market price. In addition to maximizing the expected discounted\nreturn from option sale, we incorporate a path-dependent risk penalty based on\nshortfall or quadratic variation of the option price up to the liquidation\ntime. We establish the conditions under which it is optimal to immediately\nliquidate or hold the option position through expiration. Furthermore, we study\nthe variational inequality associated with the optimal stopping problem, and\nprove the existence and uniqueness of a strong solution. A series of analytical\nand numerical results are provided to illustrate the non-trivial optimal\nliquidation strategies under geometric Brownian motion (GBM) and exponential\nOrnstein-Uhlenbeck models. We examine the combined effects of price dynamics\nand risk penalty on the sell and delay regions for various options. In\naddition, we obtain an explicit closed-form solution for the liquidation of a\nstock with quadratic penalty under the GBM model.\n"
    },
    {
        "paper_id": 1502.00674,
        "authors": "Michail Anthropelos, Michael Kupper, Antonis Papapantoleon",
        "title": "An equilibrium model for spot and forward prices of commodities",
        "comments": "42 pages, 7 figures. Final version, forthcoming in Mathematics of\n  Operations Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market model that consists of financial investors and producers\nof a commodity. Producers optionally store some production for future sale and\ngo short on forward contracts to hedge the uncertainty of the future commodity\nprice. Financial investors take positions in these contracts in order to\ndiversify their portfolios. The spot and forward equilibrium commodity prices\nare endogenously derived as the outcome of the interaction between producers\nand investors. Assuming that both are utility maximizers, we first prove the\nexistence of an equilibrium in an abstract setting. Then, in a framework where\nthe consumers' demand and the exogenously priced financial market are\ncorrelated, we provide semi-explicit expressions for the equilibrium prices and\nanalyze their dependence on the model parameters. The model can explain why\nincreased investors' participation in forward commodity markets and higher\ncorrelation between the commodity and the stock market could result in higher\nspot prices and lower forward premia.\n"
    },
    {
        "paper_id": 1502.0068,
        "authors": "Martin D. Gould, Mason A. Porter, Sam D. Howison",
        "title": "Quasi-Centralized Limit Order Books",
        "comments": "43 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A quasi-centralized limit order book (QCLOB) is a limit order book (LOB) in\nwhich financial institutions can only access the trading opportunities offered\nby counterparties with whom they possess sufficient bilateral credit. We\nperform an empirical analysis of a recent, high-quality data set from a large\nelectronic trading platform that utilizes QCLOBs to facilitate trade. We find\nmany significant differences between our results and those widely reported for\nother LOBs. We also uncover a remarkable empirical universality: although the\ndistributions describing order flow and market state vary considerably across\ndays, a simple, linear rescaling causes them to collapse onto a single curve.\nMotivated by this finding, we propose a semi-parametric model of order flow and\nmarket state in a QCLOB on a single trading day. Our model provides similar\nperformance to that of parametric curve-fitting techniques, while being simpler\nto compute and faster to implement.\n"
    },
    {
        "paper_id": 1502.00808,
        "authors": "Jo\\~ao P. da Cruz",
        "title": "On the multiplicative effect of government spending (or any other\n  spending for that matter)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is, among the economist ecosystem, the idea of virtuous public spending\nas a form of promotion of economic growth. If we think on the way GDP is\nmeasured, it is not possible to get that conclusion because it becomes\ncircular: measuring the money flow obviously will detect directly the public\nspending but always mixed with the flow of money from other sources. The\nquestion is how virtuous is public spending per se? Can it promote economic\ngrowth? Is there multiplicative effect in GDP bigger than 1? In this paper, we\nmake use of the first principles of Economics to show that government spending\nis, at the most, as virtuous as private consumption and can be a source of\neconomic depression and inequality if it is not restricted to fundamental\nservices.\n"
    },
    {
        "paper_id": 1502.00824,
        "authors": "Lei Tan, Bo Zheng, Jun-Jie Chen, Xiong-Fei Jiang",
        "title": "How volatilities nonlocal in time affect the price dynamics in complex\n  financial systems",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0118399",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What is the dominating mechanism of the price dynamics in financial systems\nis of great interest to scientists. The problem whether and how volatilities\naffect the price movement draws much attention. Although many efforts have been\nmade, it remains challenging. Physicists usually apply the concepts and methods\nin statistical physics, such as temporal correlation functions, to study\nfinancial dynamics. However, the usual volatility-return correlation function,\nwhich is local in time, typically fluctuates around zero. Here we construct\ndynamic observables nonlocal in time to explore the volatility-return\ncorrelation, based on the empirical data of hundreds of individual stocks and\n25 stock market indices in different countries. Strikingly, the correlation is\ndiscovered to be non-zero, with an amplitude of a few percent and a duration of\nover two weeks. This result provides compelling evidence that past volatilities\nnonlocal in time affect future returns. Further, we introduce an agent-based\nmodel with a novel mechanism, that is, the asymmetric trading preference in\nvolatile and stable markets, to understand the microscopic origin of the\nvolatility-return correlation nonlocal in time.\n"
    },
    {
        "paper_id": 1502.00861,
        "authors": "Eric Dahlgren and Tim Leung",
        "title": "An Optimal Multiple Stopping Approach to Infrastructure Investment\n  Decisions",
        "comments": "27 pages, 7 figures",
        "journal-ref": "Journal of Economic Dynamics & Control, vol. 53, pp.251-267, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The energy and material processing industries are traditionally characterized\nby very large-scale physical capital that is custom-built with long lead times\nand long lifetimes. However, recent technological advancement in low-cost\nautomation has made possible the parallel operation of large numbers of\nsmall-scale and modular production units. Amenable to mass-production, these\nunits can be more rapidly deployed but they are also likely to have a much\nquicker turnover. Such a paradigm shift motivates the analysis of the combined\neffect of lead time and lifetime on infrastructure investment decisions. In\norder to value the underlying real option, we introduce an optimal multiple\nstopping approach that accounts for operational flexibility, delay induced by\nlead time, and multiple (finite/infinite) future investment opportunities. We\nprovide an analytical characterization of the firm's value function and optimal\nstopping rule. This leads us to develop an iterative numerical scheme, and\nexamine how the investment decisions depend on lead time and lifetime, as well\nas other parameters. Furthermore, our model can be used to analyze the critical\ninvestment cost that makes small-scale (short lead time, short lifetime)\nalternatives competitive with traditional large-scale infrastructure.\n"
    },
    {
        "paper_id": 1502.00882,
        "authors": "M. Naresh Kumar and V. Sree Hari Rao",
        "title": "A New Methodology for Estimating Internal Credit Risk and Bankruptcy\n  Prediction under Basel II Regime",
        "comments": "14 pages, 1 figure in Computational Economics, 2014",
        "journal-ref": null,
        "doi": "10.1007/s10614-014-9452-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit estimation and bankruptcy prediction methods have been utilizing\nAltman's $z$ score method for the last several years. It is reported in many\nstudies that $z$ score is sensitive to changes in accounting figures.\nResearches have proposed different variations to conventional $z$ score that\ncan improve the prediction accuracy. In this paper we develop a new\nmultivariate non-linear model for computing the $z$ score. In addition we\ndevelop a new credit risk index by fitting a Pearson type-III distribution to\nthe transformed financial ratios. The results from our study have shown that\nthe new $z$ score can predict the bankruptcy with an accuracy of $98.6\\%$ as\ncompared to $93.5\\%$ by the Altman's $z$ score. Also, the discriminate analysis\nrevealed that the new transformed financial ratios could predict the bankruptcy\nprobability with an accuracy of $93.0\\%$ as compared to $87.4\\%$ using the\nweights of Altman's $z$ score.\n"
    },
    {
        "paper_id": 1502.00908,
        "authors": "Ra\\'ul Torres, Rosa E. Lillo and Henry Laniado",
        "title": "A Directional Multivariate Value at Risk",
        "comments": "30 pages, 9 figures",
        "journal-ref": "Insurance: Mathematics and Economics, Volume 65, November 2015,\n  Pages 111-123",
        "doi": "10.1016/j.insmatheco.2015.09.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In economics, insurance and finance, value at risk (VaR) is a widely used\nmeasure of the risk of loss on a specific portfolio of financial assets. For a\ngiven portfolio, time horizon, and probability $\\alpha$, the $100\\alpha\\%$ VaR\nis defined as a threshold loss value, such that the probability that the loss\non the portfolio over the given time horizon exceeds this value is $\\alpha$.\nThat is to say, it is a quantile of the distribution of the losses, which has\nboth good analytic properties and easy interpretation as a risk measure.\nHowever, its extension to the multivariate framework is not unique because a\nunique definition of multivariate quantile does not exist. In the current\nliterature, the multivariate quantiles are related to a specific partial order\nconsidered in $\\mathbb{R}^{n}$, or to a property of the univariate quantile\nthat is desirable to be extended to $\\mathbb{R}^{n}$. In this work, we\nintroduce a multivariate value at risk as a vector-valued directional risk\nmeasure, based on a directional multivariate quantile, which has recently been\nintroduced in the literature. The directional approach allows the manager to\nconsider external information or risk preferences in her/his analysis. We have\nderived some properties of the risk measure and we have compared the univariate\n\\textit{VaR} over the marginals with the components of the directional\nmultivariate VaR. We have also analyzed the relationship between some families\nof copulas, for which it is possible to obtain closed forms of the multivariate\nVaR that we propose. Finally, comparisons with other alternative multivariate\nVaR given in the literature, are provided in terms of robustness.\n"
    },
    {
        "paper_id": 1502.01125,
        "authors": "Frederik Meudt, Thilo A. Schmitt, Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Equilibrium Pricing in an Order Book Environment: Case Study for a Spin\n  Model",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.01.073",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When modelling stock market dynamics, the price formation is often based on\nan equilbrium mechanism. In real stock exchanges, however, the price formation\nis goverend by the order book. It is thus interesting to check if the resulting\nstylized facts of a model with equilibrium pricing change, remain the same or,\nmore generally, are compatible with the order book environment. We tackle this\nissue in the framework of a case study by embedding the\nBornholdt-Kaizoji-Fujiwara spin model into the order book dynamics. To this\nend, we use a recently developed agent based model that realistically\nincorporates the order book. We find realistic stylized facts. We conclude for\nthe studied case that equilibrium pricing is not needed and that the\ncorresponding assumption of a \"fundamental\" price may be abandoned.\n"
    },
    {
        "paper_id": 1502.01658,
        "authors": "Michael Ho, Zheng Sun, Jack Xin",
        "title": "Weighted Elastic Net Penalized Mean-Variance Portfolio Design and\n  Computation",
        "comments": "Added comparisons to SCAD and other techniques",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that the out-of-sample performance of Markowitz's\nmean-variance portfolio criterion can be negatively affected by estimation\nerrors in the mean and covariance. In this paper we address the problem by\nregularizing the mean-variance objective function with a weighted elastic net\npenalty. We show that the use of this penalty can be motivated by a robust\nreformulation of the mean-variance criterion that directly accounts for\nparameter uncertainty. With this interpretation of the weighted elastic net\npenalty we derive data driven techniques for calibrating the weighting\nparameters based on the level of uncertainty in the parameter estimates. We\ntest our proposed technique on US stock return data and our results show that\nthe calibrated weighted elastic net penalized portfolio outperforms both the\nunpenalized portfolio and uniformly weighted elastic net penalized portfolio.\n  This paper also introduces a novel Adaptive Support Split-Bregman approach\nwhich leverages the sparse nature of $\\ell_{1}$ penalized portfolios to\nefficiently compute a solution of our proposed portfolio criterion. Numerical\nresults show that this modification to the Split-Bregman algorithm results in\nsignificant improvements in computational speed compared with other techniques.\n"
    },
    {
        "paper_id": 1502.01735,
        "authors": "Yan Dolinsky and H. Mete Soner",
        "title": "Convex duality with transaction costs",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Convex duality for two two different super--replication problems in a\ncontinuous time financial market with proportional transaction cost is proved.\nIn this market, static hedging in a finite number of options, in addition to\nusual dynamic hedging with the underlying stock, are allowed. The first one the\nproblems considered is the model--independent hedging that requires the\nsuper--replication to hold for every continuous path. In the second one the\nmarket model is given through a probability measure P and the inequalities are\nunderstood P almost surely. The main result, using the convex duality, proves\nthat the two super--replication problems have the same value provided that P\nsatisfies the conditional full support property. Hence, the transaction costs\nprevents one from using the structure of a specific model to reduce the\nsuper--replication cost.\n"
    },
    {
        "paper_id": 1502.01912,
        "authors": "Sabrina Mulinacci",
        "title": "Archimedean-based Marshall-Olkin Distributions and Related Copula\n  Functions",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11009-016-9539-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new class of bivariate distributions is introduced that extends the\nGeneralized Marshall-Olkin distributions of Li and Pellerey (2011). Their\ndependence structure is studied through the analysis of the copula functions\nthat they induce. These copulas, that include as special cases the Generalized\nMarshall-Olkin copulas and the Scale Mixture of Marshall-Olkin copulas (see Li,\n2009),are obtained through suitable distortions of bivariate Archimedean\ncopulas: this induces asymmetry, and the corresponding Kendall's tau as well as\nthe tail dependence parameters are studied.\n"
    },
    {
        "paper_id": 1502.01918,
        "authors": "Umberto Cherubini and Sabrina Mulinacci",
        "title": "Systemic Risk with Exchangeable Contagion: Application to the European\n  Banking System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model and an estimation technique to distinguish systemic risk\nand contagion in credit risk. The main idea is to assume, for a set of $d$\nobligors, a set of $d$ idiosyncratic shocks and a shock that triggers the\ndefault of all them. All shocks are assumed to be linked by a dependence\nrelationship, that in this paper is assumed to be exchangeable and Archimedean.\nThis approach is able to encompass both systemic risk and contagion, with the\nMarshall-Olkin pure systemic risk model and the Archimedean contagion model as\nextreme cases. Moreover, we show that assuming an affine structure for the\nintensities of idiosyncratic and systemic shocks and a Gumbel copula, the\napproach delivers a complete multivariate distribution with exponential\nmarginal distributions. The model can be estimated by applying a moment\nmatching procedure to the bivariate marginals. We also provide an easy visual\ncheck of the good specification of the model. The model is applied to a\nselected sample of banks for 8 European countries, assuming a common shock for\nevery country. The model is found to be well specified for 4 of the 8\ncountries. We also provide the theoretical extension of the model to the\nnon-exchangeable case and we suggest possible avenues of research for the\nestimation.\n"
    },
    {
        "paper_id": 1502.02083,
        "authors": "Jin Hyuk Choi, Kasper Larsen, Duane J. Seppi",
        "title": "Information and Trading Targets in a Dynamic Market Equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the equilibrium interactions between trading targets\nand private information in a multi-period Kyle (1985) market. There are two\ninvestors who each follow dynamic trading strategies: A strategic portfolio\nrebalancer who engages in order splitting to reach a cumulative trading target\nand an unconstrained strategic insider who trades on long-lived information. We\nconsider cases in which the constrained rebalancer is partially informed as\nwell as the special case in which the rebalancer is ex ante uninformed. We\nderive a linear Bayesian Nash equilibrium, describe an algorithm for computing\nsuch equilibria, and present numerical results on properties of these\nequilibria.\n"
    },
    {
        "paper_id": 1502.02286,
        "authors": "Tatiana Belkina and Shangzhen Luo",
        "title": "Asymptotic Investment Behaviors under a Jump-Diffusion Risk Process",
        "comments": "23 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal investment control problem for an insurance company. The\nsurplus process follows the Cramer-Lundberg process with perturbation of a\nBrownian motion. The company can invest its surplus into a risk free asset and\na Black-Scholes risky asset. The optimization objective is to minimize the\nprobability of ruin. We show by new operators that the minimal ruin probability\nfunction is a classical solution to the corresponding HJB equation. Asymptotic\nbehaviors of the optimal investment control policy and the minimal ruin\nprobability function are studied for low surplus levels with a general claim\nsize distribution. Some new asymptotic results for large surplus levels in the\ncase with exponential claim distributions are obtained. We consider two cases\nof investment control - unconstrained investment and investment with a limited\namount.\n"
    },
    {
        "paper_id": 1502.02352,
        "authors": "Nikolai Dokuchaev",
        "title": "Optimal portfolio with unobservable market parameters and certainty\n  equivalence principle",
        "comments": "arXiv admin note: text overlap with arXiv:0804.4522",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a multi-stock continuous time incomplete market model with random\ncoefficients. We study the investment problem in the class of strategies which\ndo not use direct observations of the appreciation rates of the stocks, but\nrather use historical stock prices and an a priory given distribution of the\nappreciation rates. An explicit solution is found for case of power utilities\nand for a case when the problem can be embedded to a Markovian setting. Some\nnew estimates and filters for the appreciation rates are given.\n"
    },
    {
        "paper_id": 1502.02537,
        "authors": "Dimitri O. Ledenyov and Viktor O. Ledenyov",
        "title": "Mergers and acquisitions transactions strategies in diffusion - type\n  financial systems in highly volatile global capital markets with\n  nonlinearities",
        "comments": "160 pages, 9 figures, 37 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The M and A transactions represent a wide range of unique business\noptimization opportunities in the corporate transformation deals, which are\nusually characterized by the high level of total risk. The M and A transactions\ncan be successfully implemented by taking to an account the size of\ninvestments, purchase price, direction of transaction, type of transaction, and\nusing the modern comparable transactions analysis and the business valuation\ntechniques in the diffusion type financial systems in the finances. We\ndeveloped the MicroMA software program with the embedded optimized\nnear-real-time artificial intelligence algorithm to create the winning virtuous\nM and A strategies, using the financial performance characteristics of the\ninvolved firms, and to estimate the probability of the M and A transaction\ncompletion success. We believe that the fluctuating dependence of M and A\ntransactions number over the certain time period is quasi periodic. We think\nthat there are many factors, which can generate the quasi periodic oscillations\nof the M and A transactions number in the time domain, for example: the stock\nmarket bubble effects. We performed the research of the nonlinearities in the M\nand A transactions number quasi-periodic oscillations in Matlab, including the\nideal, linear, quadratic, and exponential dependences. We discovered that the\naverage of a sum of random numbers in the M and A transactions time series\nrepresents a time series with the quasi periodic systematic oscillations, which\ncan be finely approximated by the polynomial numbers. We think that, in the\ncourse of the M and A transaction implementation, the ability by the companies\nto absorb the newly acquired knowledge and to create the new innovative\nknowledge bases, is a key predeterminant of the M and A deal completion success\nas in Switzerland.\n"
    },
    {
        "paper_id": 1502.02595,
        "authors": "Jos\\'e E. Figueroa-L\\'opez and Sveinn \\'Olafsson",
        "title": "Short-time asymptotics for the implied volatility skew under a\n  stochastic volatility model with L\\'evy jumps",
        "comments": "29 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The implied volatility skew has received relatively little attention in the\nliterature on short-term asymptotics for financial models with jumps, despite\nits importance in model selection and calibration. We rectify this by providing\nhigh-order asymptotic expansions for the at-the-money implied volatility skew,\nunder a rich class of stochastic volatility models with independent stable-like\njumps of infinite variation. The case of a pure-jump stable-like L\\'evy model\nis also considered under the minimal possible conditions for the resulting\nexpansion to be well defined. Unlike recent results for \"near-the-money\" option\nprices and implied volatility, the results herein aid in understanding how the\nimplied volatility smile near expiry is affected by important features of the\ncontinuous component, such as the leverage and vol-of-vol parameters. As\nintermediary results we obtain high-order expansions for at-the-money digital\ncall option prices, which furthermore allow us to infer analogous results for\nthe delta of at-the-money options. Simulation results indicate that our\nasymptotic expansions give good fits for options with maturities up to one\nmonth, underpinning their relevance in practical applications, and an analysis\nof the implied volatility skew in recent S&P500 options data shows it to be\nconsistent with the infinite variation jump component of our models.\n"
    },
    {
        "paper_id": 1502.02819,
        "authors": "Karl Grosse-Erdmann and Fabien Heuwelyckx",
        "title": "The pricing of lookback options and binomial approximation",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Refining a discrete model of Cheuk and Vorst we obtain a closed formula for\nthe price of a European lookback option at any time between emission and\nmaturity. We derive an asymptotic expansion of the price as the number of\nperiods tends to infinity, thereby solving a problem posed by Lin and Palmer.\nWe prove, in particular, that the price in the discrete model tends to the\nprice in the continuous Black-Scholes model. Our results are based on an\nasymptotic expansion of the binomial cumulative distribution function that\nimproves several recent results in the literature.\n"
    },
    {
        "paper_id": 1502.02847,
        "authors": "Sara Biagini and Mustafa Pinar",
        "title": "The Robust Merton Problem of an Ambiguity Averse Investor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a closed form portfolio optimization rule for an investor who is\ndiffident about mean return and volatility estimates, and has a CRRA utility.\nThe novelty is that confidence is here represented using ellipsoidal\nuncertainty sets for the drift, given a volatility realization. This\nspecification affords a simple and concise analysis, as the optimal portfolio\nallocation policy is shaped by a rescaled market Sharpe ratio, computed under\nthe worst case volatility. The result is based on a max-min\nHamilton-Jacobi-Bellman-Isaacs PDE, which extends the classical Merton problem\nand reverts to it for an ambiguity-neutral investor.\n"
    },
    {
        "paper_id": 1502.02863,
        "authors": "M. Alessandra Crisafi, Andrea Macrina",
        "title": "Dark-Pool Perspective of Optimal Market Making",
        "comments": "27 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a finite-horizon market-making problem faced by a dark pool that\nexecutes incoming buy and sell orders. The arrival flow of such orders is\nassumed to be random and, for each transaction, the dark pool earns a per-share\ncommission no greater than the half bid-ask spread. Throughout the entire\nperiod, the main concern is inventory risk, which increases as the number of\nheld positions becomes critically small or large. The dark pool can control its\ninventory by choosing the size of the commission for each transaction, so to\nencourage, e.g., buy orders instead of sell orders. Furthermore, it can submit\nlit-pool limit orders, of which execution is uncertain, and market orders,\nwhich are expensive. In either case, the dark pool risks an information\nleakage, which we model via a fixed penalty for trading in the lit pool. We\nsolve a double-obstacle impulse-control problem associated with the optimal\nmanagement of the inventory, and we show that the value function is the unique\nviscosity solution of the associated system of quasi variational inequalities.\nWe explore various numerical examples of the proposed model, including one that\nadmits a semi-explicit solution.\n"
    },
    {
        "paper_id": 1502.02926,
        "authors": "Philipp Harms and David Stefanovits and Josef Teichmann and Mario\n  W\\\"uthrich",
        "title": "Consistent Recalibration of Yield Curve Models",
        "comments": "41 pages, 17 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The analytical tractability of affine (short rate) models, such as the\nVasicek and the Cox-Ingersoll-Ross models, has made them a popular choice for\nmodelling the dynamics of interest rates. However, in order to account properly\nfor the dynamics of real data, these models need to exhibit time-dependent or\neven stochastic parameters. This in turn breaks their tractability, and\nmodelling and simulating becomes an arduous task. We introduce a new class of\nHeath-Jarrow-Morton (HJM) models that both fit the dynamics of real market data\nand remain tractable. We call these models consistent recalibration (CRC)\nmodels. These CRC models appear as limits of concatenations of forward rate\nincrements, each belonging to a Hull-White extended affine factor model with\npossibly different parameters. That is, we construct HJM models from \"tangent\"\naffine models. We develop a theory for a continuous path version of such models\nand discuss their numerical implementations within the Vasicek and\nCox-Ingersoll-Ross frameworks.\n"
    },
    {
        "paper_id": 1502.02963,
        "authors": "Ricardo Crisostomo",
        "title": "An Analysis of the Heston Stochastic Volatility Model: Implementation\n  and Calibration using Matlab",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyses the implementation and calibration of the Heston\nStochastic Volatility Model. We first explain how characteristic functions can\nbe used to estimate option prices. Then we consider the implementation of the\nHeston model, showing that relatively simple solutions can lead to fast and\naccurate vanilla option prices. We also perform several calibration tests,\nusing both local and global optimization. Our analyses show that\nstraightforward setups deliver good calibration results. All calculations are\ncarried out in Matlab and numerical examples are included in the paper to\nfacilitate the understanding of mathematical concepts.\n"
    },
    {
        "paper_id": 1502.02968,
        "authors": "Michele Longo and Alessandra Mainini",
        "title": "Learning and Portfolio Decisions for HARA Investors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We maximize the expected utility from terminal wealth for an HARA investor\nwhen the market price of risk is an unobservable random variable. We compute\nthe optimal portfolio explicitly and explore the effects of learning by\ncomparing it with the corresponding myopic policy. In particular, we show that,\nfor a market price of risk constant in sign, the ratio between the portfolio\nunder partial observation and its myopic counterpart increases with respect to\nrisk tolerance. As a consequence, the absolute value of the partial observation\ncase is larger (smaller) than the myopic one if the investor is more (less)\nrisk tolerant than the logarithmic investor. Moreover, our explicit\ncomputations enable to study in details the so called hedging demand induced by\nlearning about market price of risk.\n"
    },
    {
        "paper_id": 1502.03018,
        "authors": "Nikolaos Halidias and Ioannis Stamatiou",
        "title": "Approximating explicitly the mean reverting CEV process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we want to exploit further the semi-discrete method appeared in\nHalidias and Stamatiou (2015). We are interested in the numerical solution of\nmean reverting CEV processes that appear in financial mathematics models and\nare described as non negative solutions of certain stochastic differential\nequations with sub-linear diffusion coefficients of the form $(x_t)^q,$ where\n$\\frac{1}{2}<q<1.$ Our goal is to construct explicit numerical schemes that\npreserve positivity. We prove convergence of the proposed SD scheme with rate\ndepending on the parameter $q.$ Furthermore, we verify our findings through\nnumerical experiments and compare with other positivity preserving schemes.\nFinally, we show how to treat the whole two-dimensional stochastic volatility\nmodel, with instantaneous variance process given by the above mean reverting\nCEV process.\n"
    },
    {
        "paper_id": 1502.03252,
        "authors": "Pablo Koch-Medina, Cosimo Munari, Mario Sikic",
        "title": "Diversification, protection of liability holders and regulatory\n  arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Any solvency regime for financial institutions should be aligned with the\nfundamental objectives of regulation: protecting liability holders and securing\nthe stability of the financial system. The first objective leads to consider\nsurplus-invariant capital adequacy tests, i.e. tests that do not depend on the\nsurplus of a financial institution. We provide a complete characterization of\nclosed, convex, surplus-invariant capital adequacy tests that highlights an\ninherent tension between surplus-invariance and the desire to give credit for\ndiversification. The second objective leads to requiring consistency of capital\nadequacy tests across jurisdictions. Of particular importance in this respect\nare capital adequacy tests that remain invariant under a change of\nnum\\'{e}raire. We establish an intimate link between surplus- and num\\'{e}raire\ninvariant tests.\n"
    },
    {
        "paper_id": 1502.03254,
        "authors": "Archil Gulisashvili, Blanka Horvath, Antoine Jacquier",
        "title": "Mass at zero in the uncorrelated SABR model and implied volatility\n  asymptotics",
        "comments": "15 pages, 2 tables, 8 figures This updated version concentrates on\n  the small- and large-time asymptotic behaviour of the mass at zero in the\n  uncorrelated SABR model. Some geometric considerations regarding the\n  correlated case are provided in a companion paper arXiv:1610.05636",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the mass at the origin in the uncorrelated SABR stochastic\nvolatility model, and derive several tractable expressions, in particular when\ntime becomes small or large. As an application--in fact the original motivation\nfor this paper--we derive small-strike expansions for the implied volatility\nwhen the maturity becomes short or large. These formulae, by definition\narbitrage free, allow us to quantify the impact of the mass at zero on existing\nimplied volatility approximations, and in particular how correct/erroneous\nthese approximations become.\n"
    },
    {
        "paper_id": 1502.03359,
        "authors": "Cl\\'ement M\\'enass\\'e and Peter Tankov",
        "title": "Asymptotic indifference pricing in exponential L\\'evy models",
        "comments": "Minor corrections",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets based on L\\'evy processes are typically incomplete and\noption prices depend on risk attitudes of individual agents. In this context,\nthe notion of utility indifference price has gained popularity in the academic\ncircles. Although theoretically very appealing, this pricing method remains\ndifficult to apply in practice, due to the high computational cost of solving\nthe nonlinear partial integro-differential equation associated to the\nindifference price. In this work, we develop closed form approximations to\nexponential utility indifference prices in exponential L\\'evy models. To this\nend, we first establish a new non-asymptotic approximation of the indifference\nprice which extends earlier results on small risk aversion asymptotics of this\nquantity. Next, we use this formula to derive a closed-form approximation of\nthe indifference price by treating the L\\'evy model as a perturbation of the\nBlack-Scholes model. This extends the methodology introduced in a recent paper\nfor smooth linear functionals of L\\'evy processes (A. \\v{C}ern\\'y, S. Denkl and\nJ. Kallsen, arXiv:1309.7833) to nonlinear and non-smooth functionals. Our\nclosed formula represents the indifference price as the linear combination of\nthe Black-Scholes price and correction terms which depend on the variance,\nskewness and kurtosis of the underlying L\\'evy process, and the derivatives of\nthe Black-Scholes price. As a by-product, we obtain a simple explicit formula\nfor the spread between the buyer's and the seller's indifference price. This\nformula allows to quantify, in a model-independent fashion, how sensitive a\ngiven product is to jump risk in the limit of small jump size.\n"
    },
    {
        "paper_id": 1502.03656,
        "authors": "Johan Dahlin, Fredrik Lindsten, Thomas B. Sch\\\"on",
        "title": "Quasi-Newton particle Metropolis-Hastings",
        "comments": "23 pages, 5 figures. Accepted for the 17th IFAC Symposium on System\n  Identification (SYSID), Beijing, China, October 2015",
        "journal-ref": null,
        "doi": "10.1016/j.ifacol.2015.12.258",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Particle Metropolis-Hastings enables Bayesian parameter inference in general\nnonlinear state space models (SSMs). However, in many implementations a random\nwalk proposal is used and this can result in poor mixing if not tuned correctly\nusing tedious pilot runs. Therefore, we consider a new proposal inspired by\nquasi-Newton algorithms that may achieve similar (or better) mixing with less\ntuning. An advantage compared to other Hessian based proposals, is that it only\nrequires estimates of the gradient of the log-posterior. A possible application\nis parameter inference in the challenging class of SSMs with intractable\nlikelihoods. We exemplify this application and the benefits of the new proposal\nby modelling log-returns of future contracts on coffee by a stochastic\nvolatility model with $\\alpha$-stable observations.\n"
    },
    {
        "paper_id": 1502.0384,
        "authors": "Zhe Yu, Shanjun Li and Lang Tong",
        "title": "Market Dynamics and Indirect Network Effects in Electric Vehicle\n  Diffusion",
        "comments": "20 pages, 8 figures, journal paper",
        "journal-ref": null,
        "doi": "10.1016/j.trd.2016.06.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The diffusion of electric vehicles (EVs) is studied in a two-sided market\nframework consisting of EVs on the one side and EV charging stations (EVCSs) on\nthe other. A sequential game is introduced as a model for the interactions\nbetween an EVCS investor and EV consumers. A consumer chooses to purchase an EV\nor a conventional gasoline alternative based on the upfront costs of purchase,\nthe future operating costs and the availability of charging stations. The\ninvestor, on the other hand, maximizes his profit by deciding whether to build\ncharging facilities at a set of potential EVCS sites or to defer his\ninvestments. The solution of the sequential game characterizes the EV-EVCS\nmarket equilibrium. The market solution is compared with that of a social\nplanner who invests in EVCSs with the goal of maximizing the social welfare. It\nis shown that the market solution underinvests EVCSs, leading to slower EV\ndiffusion. The effects of subsidies for EV purchase and EVCSs are also\nconsidered.\n"
    },
    {
        "paper_id": 1502.03871,
        "authors": "Ioane Muni Toke",
        "title": "Stationary distribution of the volume at the best quote in a Poisson\n  order book model",
        "comments": "19 pages, 3 figures, 2 tables",
        "journal-ref": "International Journal of Theoretical and Applied Finance, 20(06),\n  1750039 (2017)",
        "doi": "10.1142/S021902491750039X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a Markovian model that deals with the volume\noffered at the best quote of an electronic order book. The volume of the first\nlimit is a stochastic process whose paths are periodically interrupted and\nreset to a new value, either by a new limit order submitted inside the spread\nor by a market order that removes the first limit. Using applied probability\nresults on killing and resurrecting Markov processes, we derive the stationary\ndistribution of the volume offered at the best quote. All proposed models are\nempirically fitted and compared, stressing the importance of the proposed\nmechanisms.\n"
    },
    {
        "paper_id": 1502.03901,
        "authors": "Boris Buchmann, Benjamin Kaehler, Ross Maller, Alexander Szimayer",
        "title": "Multivariate Subordination using Generalised Gamma Convolutions with\n  Applications to V.G. Processes and Option Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We unify and extend a number of approaches related to constructing\nmultivariate Variance-Gamma (V.G.) models for option pricing. An overarching\nmodel is derived by subordinating multivariate Brownian motion to a\nsubordinator from the Thorin (1977) class of generalised Gamma convolution\nsubordinators. A class of models due to Grigelionis (2007), which contains the\nwell-known Madan-Seneta V.G. model, is of this type, but our multivariate\ngeneralization is considerably wider, allowing in particular for processes with\ninfinite variation and a variety of dependencies between the underlying\nprocesses. Multivariate classes developed by P\\'erez-Abreu and Stelzer (2012)\nand Semeraro (2008) and Guillaume (2013) are also submodels. The new models are\nshown to be invariant under Esscher transforms, and quite explicit expressions\nfor canonical measures (and transition densities in some cases) are obtained,\nwhich permit applications such as option pricing using PIDEs or tree based\nmethodologies. We illustrate with best-of and worst-of European and American\noptions on two assets.\n"
    },
    {
        "paper_id": 1502.03978,
        "authors": "Gianluca Cassese",
        "title": "Non Parametric Estimates of Option Prices Using Superhedging",
        "comments": "arXiv admin note: text overlap with arXiv:1406.0412",
        "journal-ref": null,
        "doi": "10.1142/S0219024919500407",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new non parametric technique to estimate the CALL function based\non the superhedging principle. Our approach does not require absence of\narbitrage and easily accommodates bid/ask spreads and other market\nimperfections. We prove some optimal statistical properties of our estimates.\nAs an application we first test the methodology on a simulated sample of option\nprices and then on the S\\&P 500 index options.\n"
    },
    {
        "paper_id": 1502.04359,
        "authors": "Ulrich Horst and D\\\"orte Kreher",
        "title": "A weak law of large numbers for a limit order book model with fully\n  state dependent order dynamics",
        "comments": "revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a limit order book (LOB) model, in which the order\ndynamics depend on both, the current best available prices and the current\nvolume density functions. For the joint dynamics of the best bid price, the\nbest ask price, and the standing volume densities on both sides of the LOB we\nderive a weak law of large numbers, which states that the LOB model converges\nto a continuous-time limit when the size of an individual order as well as the\ntick size tend to zero and the order arrival rate tends to infinity. In the\nscaling limit the two volume densities follow each a non-linear PDE coupled\nwith two non-linear ODEs that describe the best bid and ask price.\n"
    },
    {
        "paper_id": 1502.04521,
        "authors": "Masashi Ieda",
        "title": "A dynamic optimal execution strategy under stochastic price recovery",
        "comments": "24 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, we study the optimal execution problem under stochastic\nprice recovery based on limit order book dynamics. We model price recovery\nafter execution of a large order by accelerating the arrival of the refilling\norder, which is defined as a Cox process whose intensity increases by the\ndegree of the market impact. We include not only the market order but also the\nlimit order in our strategy in a restricted fashion. We formulate the problem\nas a combined stochastic control problem over a finite time horizon. The\ncorresponding Hamilton-Jacobi-Bellman quasi-variational inequality is solved\nnumerically. The optimal strategy obtained consists of three components: (i)\nthe initial large trade; (ii) the unscheduled small trades during the period;\n(iii) the terminal large trade. The size and timing of the trade is governed by\nthe tolerance for market impact depending on the state at each time step, and\nhence the strategy behaves dynamically. We also provide competitive results due\nto inclusion of the limit order, even though a limit order is allowed under\nconservative evaluation of the execution price.\n"
    },
    {
        "paper_id": 1502.04592,
        "authors": "Emmanuel Bacry, Iacopo Mastromatteo, and Jean-Fran\\c{c}ois Muzy",
        "title": "Hawkes processes in finance",
        "comments": "48 pages, 10 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose an overview of the recent academic literature\ndevoted to the applications of Hawkes processes in finance. Hawkes processes\nconstitute a particular class of multivariate point processes that has become\nvery popular in empirical high frequency finance this last decade. After a\nreminder of the main definitions and properties that characterize Hawkes\nprocesses, we review their main empirical applications to address many\ndifferent problems in high frequency finance. Because of their great\nflexibility and versatility, we show that they have been successfully involved\nin issues as diverse as estimating the volatility at the level of transaction\ndata, estimating the market stability, accounting for systemic risk contagion,\ndevising optimal execution strategies or capturing the dynamics of the full\norder book.\n"
    },
    {
        "paper_id": 1502.04909,
        "authors": "Robert Fernholz",
        "title": "Identification of Atlas models",
        "comments": "4 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Atlas models are systems of Ito processes with parameters that depend on\nrank. We show that the parameters of a simple Atlas model can be identified by\nmeasuring the variance of the top-ranked process for different sampling\nintervals.\n"
    },
    {
        "paper_id": 1502.05238,
        "authors": "Yakov Babichenko, Leonard J. Schulman",
        "title": "Pareto Efficient Nash Implementation Via Approval Voting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study implementation of a social choice correspondence in the case of two\nplayers who have von Neumann - Morgenstern utilities over a finite set of\nsocial alternatives, and the mechanism is allowed to output lotteries. Our main\npositive result shows that a close variant of the popular approval voting\nmechanism succeeds in selecting only Pareto efficient alternatives as pure Nash\nequilibria outcomes. Moreover, we provide an exact characterization of pure\nNash equilibria profiles and outcomes of the mechanism. The characterization\ndemonstrates a close connection between the approval voting mechanism and the\nnotion of average fixed point, which is a point that is equal to the average of\nall points that it does not Pareto dominate.\n"
    },
    {
        "paper_id": 1502.05274,
        "authors": "J. Doyne Farmer, Francois Lafond",
        "title": "How predictable is technological progress?",
        "comments": null,
        "journal-ref": "Research Policy, Volume 45, Issue 3, Pages 647-665 (April 2016)",
        "doi": "10.1016/j.respol.2015.11.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently it has become clear that many technologies follow a generalized\nversion of Moore's law, i.e. costs tend to drop exponentially, at different\nrates that depend on the technology. Here we formulate Moore's law as a\ncorrelated geometric random walk with drift, and apply it to historical data on\n53 technologies. We derive a closed form expression approximating the\ndistribution of forecast errors as a function of time. Based on hind-casting\nexperiments we show that this works well, making it possible to collapse the\nforecast errors for many different technologies at different time horizons onto\nthe same universal distribution. This is valuable because it allows us to make\nforecasts for any given technology with a clear understanding of the quality of\nthe forecasts. As a practical demonstration we make distributional forecasts at\ndifferent time horizons for solar photovoltaic modules, and show how our method\ncan be used to estimate the probability that a given technology will outperform\nanother technology at a given point in the future.\n"
    },
    {
        "paper_id": 1502.05367,
        "authors": "Damien Challet",
        "title": "One- and two-sample nonparametric tests for the signal-to-noise ratio\n  based on record statistics",
        "comments": "12 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new family of nonparametric statistics, the r-statistics, is introduced. It\nconsists of counting the number of records of the cumulative sum of the sample.\nThe single-sample r-statistic is almost as powerful as Student's t-statistic\nfor Gaussian and uniformly distributed variables, and more powerful than the\nsign and Wilcoxon signed-rank statistics as long as the data are not too\nheavy-tailed.\n  Three two-sample parametric r-statistics are proposed, one with a higher\nspecificity but a smaller sensitivity than Mann-Whitney U-test and the other\none a higher sensitivity but a smaller specificity. A nonparametric two-sample\nr-statistic is introduced, whose power is very close to that of Welch statistic\nfor Gaussian or uniformly distributed variables.\n"
    },
    {
        "paper_id": 1502.05442,
        "authors": "Archil Gulisashvili, Frederi Viens, Xin Zhang",
        "title": "Extreme-Strike Asymptotics for General Gaussian Stochastic Volatility\n  Models",
        "comments": "38 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic volatility asset price model in which the volatility\nis the absolute value of a continuous Gaussian process with arbitrary\nprescribed mean and covariance. By exhibiting a Karhunen-Lo\\`{e}ve expansion\nfor the integrated variance, and using sharp estimates of the density of a\ngeneral second-chaos variable, we derive asymptotics for the asset price\ndensity for large or small values of the variable, and study the wing behavior\nof the implied volatility in these models. Our main result provides explicit\nexpressions for the first five terms in the expansion of the implied\nvolatility. The expressions for the leading three terms are simple, and based\non three basic spectral-type statistics of the Gaussian process: the top\neigenvalue of its covariance operator, the multiplicity of this eigenvalue, and\nthe $L^{2}$ norm of the projection of the mean function on the top eigenspace.\nThe fourth term requires knowledge of all eigen-elements. We present detailed\nnumerics based on realistic liquidity assumptions in which classical and\nlong-memory volatility models are calibrated based on our expansion.\n"
    },
    {
        "paper_id": 1502.05603,
        "authors": "Paulo Ferreira, Andreia Dion\\'isio and S.M.S. Movahed",
        "title": "Assessment of 48 Stock markets using adaptive multifractal approach",
        "comments": "16 pages, 13 figures and 4 tables, major revision and match to\n  published version",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 486,\n  Pages 730-750 (15 November 2017)",
        "doi": "10.1016/j.physa.2017.05.046",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market comovements are examined using cointegration, Granger causality\ntests and nonlinear approaches in context of mutual information and\ncorrelations. Underlying data sets are affected by non-stationarities and\ntrends, we also apply AMF-DFA and AMF-DXA. We find only 170 pair of Stock\nmarkets cointegrated, and according to the Granger causality and mutual\ninformation, we realize that the strongest relations lies between emerging\nmarkets, and between emerging and frontier markets. According to scaling\nexponent given by AMF-DFA, $h(q=2)>1$, we find that all underlying data sets\nbelong to non-stationary process. According to EMH, only 8 markets are\nclassified in uncorrelated processes at $2\\sigma$ confidence interval. 6 Stock\nmarkets belong to anti-correlated class and dominant part of markets has memory\nin corresponding daily index prices during January 1995 to February 2014.\nNew-Zealand with $H=0.457\\pm0.004$ and Jordan with $H=0.602\\pm 0.006$ are far\nfrom EMH. The nature of cross-correlation exponents based on AMF-DXA is almost\nmultifractal for all pair of Stock markets. The empirical relation, $H_{xy}\\le\n[H_{xx}+H_{yy}]/2$, is confirmed. Mentioned relation for $q>0$ is also\nsatisfied while for $q<0$ there is a deviation from this relation confirming\nbehavior of markets for small fluctuations is affected by contribution of major\npair. For larger fluctuations, the cross-correlation contains information from\nboth local and global conditions. Width of singularity spectrum for\nauto-correlation and cross-correlation are $\\Delta \\alpha_{xx}\\in\n[0.304,0.905]$ and $\\Delta \\alpha_{xy}\\in [0.246,1.178]$, respectively. The\nwide range of singularity spectrum for cross-correlation confirms that the\nbilateral relation between Stock markets is more complex. The value of\n$\\sigma_{DCCA}$ indicates that all pairs of stock market studied in this time\ninterval belong to cross-correlated processes.\n"
    },
    {
        "paper_id": 1502.05743,
        "authors": "Parsiad Azimzadeh, Peter A. Forsyth",
        "title": "The existence of optimal bang-bang controls for GMxB contracts",
        "comments": "21 pages, 3 figures. Typo in proof of Lemma 4.19 in [v1] fixed.\n  Incorrect author data in [v1] fixed. Incorrect specification of assumption\n  (D2) in [v2] fixed. Bibliography abbreviations in [v3] fixed",
        "journal-ref": "SIAM.J.Finan.Math. 6.1 (2015) 117-139",
        "doi": "10.1137/140953885",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large collection of financial contracts offering guaranteed minimum\nbenefits are often posed as control problems, in which at any point in the\nsolution domain, a control is able to take any one of an uncountable number of\nvalues from the admissible set. Often, such contracts specify that the holder\nexert control at a finite number of deterministic times. The existence of an\noptimal bang-bang control, an optimal control taking on only a finite subset of\nvalues from the admissible set, is a common assumption in the literature. In\nthis case, the numerical complexity of searching for an optimal control is\nconsiderably reduced. However, no rigorous treatment as to when an optimal\nbang-bang control exists is present in the literature. We provide the reader\nwith a bang-bang principle from which the existence of such a control can be\nestablished for contracts satisfying some simple conditions. The bang-bang\nprinciple relies on the convexity and monotonicity of the solution and is\ndeveloped using basic results in convex analysis and parabolic partial\ndifferential equations. We show that a guaranteed lifelong withdrawal benefit\n(GLWB) contract admits an optimal bang-bang control. In particular, we find\nthat the holder of a GLWB can maximize a writer's losses by only ever\nperforming nonwithdrawal, withdrawal at exactly the contract rate, or full\nsurrender. We demonstrate that the related guaranteed minimum withdrawal\nbenefit contract is not convexity preserving, and hence does not satisfy the\nbang-bang principle other than in certain degenerate cases.\n"
    },
    {
        "paper_id": 1502.0592,
        "authors": "Ariel Neufeld, Marcel Nutz",
        "title": "Robust Utility Maximization with L\\'evy Processes",
        "comments": "Forthcoming in 'Mathematical Finance'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust portfolio optimization problem under model uncertainty for\nan investor with logarithmic or power utility. The uncertainty is specified by\na set of possible L\\'evy triplets; that is, possible instantaneous drift,\nvolatility and jump characteristics of the price process. We show that an\noptimal investment strategy exists and compute it in semi-closed form.\nMoreover, we provide a saddle point analysis describing a worst-case model.\n"
    },
    {
        "paper_id": 1502.06074,
        "authors": "Zura Kakushadze",
        "title": "Coping with Negative Short-Rates",
        "comments": "30 pages; no changes (excepting this line); to appear in Wilmott\n  Magazine",
        "journal-ref": "Wilmott Magazine 2016(81) (2016) 58-68",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a simple extension of the Ho and Lee model with generic\ntime-dependent drift in which: 1) we compute bond prices analytically; 2) the\nyield curve is sensible and the asymptotic yield is positive; and 3) our\nanalytical solution provides a clean and simple way of separating volatility\nfrom the drift in the short-rate process. Our extension amounts to introducing\none or two reflecting barriers for the underlying Brownian motion (as opposed\nto the short-rate), which allows to have more realistic time-dependent drift\n(as opposed to constant drift). In our model the spectrum -- or, roughly, the\nset of short-rate values contributing to bond and other claim prices -- is\ndiscrete and positive. We discuss how to calibrate our model using empirical\nyield data by fitting three parameters and then read off the time-dependent\ndrift.\n"
    },
    {
        "paper_id": 1502.06106,
        "authors": "Maxim Bichuch, Agostino Capponi, Stephan Sturm",
        "title": "Arbitrage-Free Pricing of XVA - Part II: PDE Representation and\n  Numerical Analysis",
        "comments": "19 pages, 3 figures, 2 tables. The paper arXiv:1608.02690 subsumes\n  the present working paper as well as arXiv:1501.05893",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the semilinear partial differential equation (PDE) associated with\nthe non-linear BSDE characterizing buyer's and seller's XVA in a framework that\nallows for asymmetries in funding, repo and collateral rates, as well as for\nearly contract termination due to counterparty credit risk. We show the\nexistence of a unique classical solution to the PDE by first proving the\nexistence and uniqueness of a viscosity solution and then its regularity. We\nuse the uniqueness result to conduct a thorough numerical study illustrating\nhow funding costs, repo rates, and counterparty credit risk contribute to\ndetermine the total valuation adjustment.\n"
    },
    {
        "paper_id": 1502.06163,
        "authors": "Jacky Mallett",
        "title": "Threadneedle: An Experimental Tool for the Simulation and Analysis of\n  Fractional Reserve Banking Systems",
        "comments": "19 pages, 7 figures, Eastern Economic Association, 41st Conference\n  2015 (Accepted)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Threadneedle is a multi-agent simulation framework, based on a full double\nentry book keeping implementation of the banking system's fundamental\ntransactions. It is designed to serve as an experimental test bed for economic\nsimulations that can explore the banking system's influence on the\nmacro-economy under varying assumptions for its regulatory framework, mix of\nfinancial instruments, and activities of borrowers and lenders. Support is\nprovided for Basel Capital and central bank reserve regulatory frameworks,\ninter-bank lending and correct handling of loan defaults within the bank\naccounting framework.\n  In this paper we provide an overview of the design of Threadneedle, and the\nrational for the double entry book keeping approach used in its implementation.\nWe then provide evidence from a series of experiments using the simulation that\nthe macro-economic behaviour of the banking system is in some cases sensitive\nto double entry book keeping ledger definitions, and in particular that loss\nprovisions can be systemically affecting. We also show that credit and money\nexpansion in Basel regulated systems is now dominated by the Basel capital\nrequirements, rather than the older central bank reserve requirements. This\nimplies that bank profitability is now the main factor in providing new capital\nto support lending, meaning that lowering interest rates can act to restrict\nloan supply, rather than increasing borrowing as currently believed. We also\nshow that long term liquidity flows due to interest repayment act in favour of\nthe bank making the loan, and do not provide any long term throttling effect on\nloan expansion and money expansion as has been claimed by Keynes and others.\n"
    },
    {
        "paper_id": 1502.06217,
        "authors": "Imre Kondor, Fabio Caccioli, G\\'abor Papp, Matteo Marsili",
        "title": "Contour map of estimation error for Expected Shortfall",
        "comments": "5 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The contour map of estimation error of Expected Shortfall (ES) is\nconstructed. It allows one to quantitatively determine the sample size (the\nlength of the time series) required by the optimization under ES of large\ninstitutional portfolios for a given size of the portfolio, at a given\nconfidence level and a given estimation error.\n"
    },
    {
        "paper_id": 1502.06349,
        "authors": "Antonio Dalessandro, Gareth W. Peters",
        "title": "Tensor Approximation of Generalized Correlated Diffusions and Functional\n  Copula Operators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate aspects of semimartingale decompositions, approximation and\nthe martingale representation for multidimensional correlated Markov processes.\nA new interpretation of the dependence among processes is given using the\nmartingale approach. We show that it is possible to represent, in both\ncontinuous and discrete space, that a multidimensional correlated generalized\ndiffusion is a linear combination of processes that originate from the\ndecomposition of the starting multidimensional semimartingale. This result not\nonly reconciles with the existing theory of diffusion approximations and\ndecompositions, but defines the general representation of infinitesimal\ngenerators for both multidimensional generalized diffusions and as we will\ndemonstrate also for the specification of copula density dependence structures.\nThis new result provides immediate representation of the approximate solution\nfor correlated stochastic differential equations. We demonstrate desirable\nconvergence results for the proposed multidimensional semimartingales\ndecomposition approximations.\n"
    },
    {
        "paper_id": 1502.06434,
        "authors": "B. W. Wanjawa and L. Muchemi",
        "title": "ANN Model to Predict Stock Prices at Stock Exchange Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock exchanges are considered major players in financial sectors of many\ncountries. Most Stockbrokers, who execute stock trade, use technical,\nfundamental or time series analysis in trying to predict stock prices, so as to\nadvise clients. However, these strategies do not usually guarantee good returns\nbecause they guide on trends and not the most likely price. It is therefore\nnecessary to explore improved methods of prediction.\n  The research proposes the use of Artificial Neural Network that is\nfeedforward multi-layer perceptron with error backpropagation and develops a\nmodel of configuration 5:21:21:1 with 80% training data in 130,000 cycles. The\nresearch develops a prototype and tests it on 2008-2012 data from stock markets\ne.g. Nairobi Securities Exchange and New York Stock Exchange, where prediction\nresults show MAPE of between 0.71% and 2.77%. Validation done with Encog and\nNeuroph realized comparable results. The model is thus capable of prediction on\ntypical stock markets.\n"
    },
    {
        "paper_id": 1502.06557,
        "authors": "Florian Ziel",
        "title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic\n  time series with applications to AR-ARCH type processes",
        "comments": null,
        "journal-ref": "Computational Statistics & Data Analysis, 100 (2016) 773-793",
        "doi": "10.1016/j.csda.2015.11.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Shrinkage algorithms are of great importance in almost every area of\nstatistics due to the increasing impact of big data. Especially time series\nanalysis benefits from efficient and rapid estimation techniques such as the\nlasso. However, currently lasso type estimators for autoregressive time series\nmodels still focus on models with homoscedastic residuals. Therefore, an\niteratively reweighted adaptive lasso algorithm for the estimation of time\nseries models under conditional heteroscedasticity is presented in a\nhigh-dimensional setting. The asymptotic behaviour of the resulting estimator\nis analysed. It is found that the proposed estimation procedure performs\nsubstantially better than its homoscedastic counterpart. A special case of the\nalgorithm is suitable to compute the estimated multivariate AR-ARCH type models\nefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH\nor ARMA-GARCH are discussed. Finally, different simulation results and\napplications to electricity market data and returns of metal prices are shown.\n"
    },
    {
        "paper_id": 1502.06681,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "Arbitrage, hedging and utility maximization using semi-static trading\n  strategies with American options",
        "comments": "Final version. To appear in the Annals of Applied Probability.\n  Keywords: Fundamental theorem of asset pricing, hedging duality, utility\n  maximization, semi-static trading strategies, American options, model\n  uncertainty",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial market where stocks are available for dynamic\ntrading, and European and American options are available for static trading\n(semi-static trading strategies). We assume that the American options are\ninfinitely divisible, and can only be bought but not sold. In the first part of\nthe paper, we work within the framework without model ambiguity. We first get\nthe fundamental theorem of asset pricing (FTAP). Using the FTAP, we get the\ndualities for the hedging prices of European and American options. Based on the\nhedging dualities, we also get the duality for the utility maximization. In the\nsecond part of the paper, we consider the market which admits non-dominated\nmodel uncertainty. We first establish the hedging result, and then using the\nhedging duality we further get the FTAP. Due to the technical difficulty\nstemming from the non-dominancy of the probability measure set, we use a\ndiscretization technique and apply the minimax theorem.\n"
    },
    {
        "paper_id": 1502.06736,
        "authors": "Jo\\\"el Bun, Romain Allez, Jean-Philippe Bouchaud, Marc Potters",
        "title": "Rotational invariant estimator for general noisy matrices",
        "comments": "19 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1109/TIT.2016.2616132",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the problem of estimating a given real symmetric signal matrix\n$\\textbf{C}$ from a noisy observation matrix $\\textbf{M}$ in the limit of large\ndimension. We consider the case where the noisy measurement $\\textbf{M}$ comes\neither from an arbitrary additive or multiplicative rotational invariant\nperturbation. We establish, using the Replica method, the asymptotic global law\nestimate for three general classes of noisy matrices, significantly extending\npreviously obtained results. We give exact results concerning the asymptotic\ndeviations (called overlaps) of the perturbed eigenvectors away from the true\nones, and we explain how to use these overlaps to \"clean\" the noisy eigenvalues\nof $\\textbf{M}$. We provide some numerical checks for the different estimators\nproposed in this paper and we also make the connection with some well known\nresults of Bayesian statistics.\n"
    },
    {
        "paper_id": 1502.06805,
        "authors": "Diego-Ivan Ruge-Leiva",
        "title": "International R&D Spillovers and other Unobserved Common Spillovers and\n  Shocks",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Studies which are based on Coe and Helpman (1995) and use weighted foreign\nR&D variables to estimate channel-specific R&D spillovers disregard the\ninteraction between international R&D spillovers and other unobserved common\nspillovers and shocks. Using a panel of 50 economies from 1970-2011, we find\nthat disregarding this interaction leads to inconsistent estimates whenever\nknowledge spillovers and other unobserved effects are correlated with foreign\nand domestic R&D. When this interaction is modeled, estimates are consistent;\nhowever, they confound foreign and domestic R&D effects with unobserved\neffects. Thus, the coefficient of a weighted foreign R&D variable cannot\ncapture genuine channel-specific R&D spillovers.\n"
    },
    {
        "paper_id": 1502.06901,
        "authors": "Ignacio Esponda and Demian Pouzo",
        "title": "Equilibrium in Misspecified Markov Decision Processes",
        "comments": "54 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study Markov decision problems where the agent does not know the\ntransition probability function mapping current states and actions to future\nstates. The agent has a prior belief over a set of possible transition\nfunctions and updates beliefs using Bayes' rule. We allow her to be\nmisspecified in the sense that the true transition probability function is not\nin the support of her prior. This problem is relevant in many economic settings\nbut is usually not amenable to analysis by the researcher. We make the problem\ntractable by studying asymptotic behavior. We propose an equilibrium notion and\nprovide conditions under which it characterizes steady state behavior. In the\nspecial case where the problem is static, equilibrium coincides with the\nsingle-agent version of Berk-Nash equilibrium (Esponda and Pouzo (2016)). We\nalso discuss subtle issues that arise exclusively in dynamic settings due to\nthe possibility of a negative value of experimentation.\n"
    },
    {
        "paper_id": 1502.06984,
        "authors": "J. Molins and E. Vives",
        "title": "Model risk on credit risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops the Jungle model in a credit portfolio framework. The\nJungle model is able to model credit contagion, produce doubly-peaked\nprobability distributions for the total default loss and endogenously generate\nquasi phase transitions, potentially leading to systemic credit events which\nhappen unexpectedly and without an underlying single cause. We show the Jungle\nmodel provides the optimal probability distribution for credit losses, under\nsome reasonable empirical constraints. The Dandelion model, a particular case\nof the Jungle model, is presented, motivated and exactly solved. The Dandelion\nmodel provides an explicit example of doubly-peaked probability distribution\nfor the credit losses. The Diamond model, another instance of the Jungle model,\nexperiences the so called quasi phase transitions; in particular, both the U.S.\nsubprime and the European sovereign crises are shown to be potential examples\nof quasi phase transitions. We argue the three known sources of default\nclustering (contagion, macroeconomic risk factors and frailty) can be\nunderstood under the unifying framework of contagion. We suggest how the Jungle\nmodel is able to explain a series of empirical stylized facts in credit\nportfolios, hard to reconcile by some standard credit portfolio models. We show\nthe Jungle model can handle inhomogeneous portfolios with state-dependent\nrecovery rates. We look at model risk in a credit risk framework under the\nJungle model, especially in relation to systemic risks posed by doubly-peaked\ndistributions and quasi phase transitions.\n"
    },
    {
        "paper_id": 1502.07265,
        "authors": "Andrew B. Whitford",
        "title": "Estimation of Several Political Action Effects of Energy Prices",
        "comments": "25 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One important effect of price shocks in the United States has been increased\npolitical attention paid to the structure and performance of oil and natural\ngas markets, along with some governmental support for energy conservation. This\npaper describes how price changes helped lead the emergence of a political\nagenda accompanied by several interventions, as revealed through Granger\ncausality tests on change in the legislative agenda.\n"
    },
    {
        "paper_id": 1502.07321,
        "authors": "Alexander Schnurr",
        "title": "An Ordinal Pattern Approach to Detect and to Model Leverage Effects and\n  Dependence Structures Between Financial Time Series",
        "comments": "13 pages, 3 figures",
        "journal-ref": "Stat. Papers 55(4) (2014), 919-931",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce two types of ordinal pattern dependence between time series.\nPositive (resp. negative) ordinal pattern dependence can be seen as a\nnon-paramatric and in particular non-linear counterpart to positive (resp.\nnegative) correlation. We show in an explorative study that both types of this\ndependence show up in real world financial data.\n"
    },
    {
        "paper_id": 1502.07367,
        "authors": "Jan Jurczyk, Alexander Eckrot",
        "title": "Cross correlations in European government bonds and EuroStoxx",
        "comments": "8 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use principle component analysis (PCA) of cross correlations in European\ngovernment bonds and European stocks to investigate the systemic risk contained\nin the European economy. We tackle the task to visualize the evolution of risk,\nintroducing the conditional average rolling sum (CARS). Using this tool we see\nthat the risk of government bonds and stocks had an independent movement. But\nin the course of the European sovereign debt crisis the coupling between bonds\nand stocks has strongly ncreased. This results in an in-phase oscillation of\nrisk for both markets since mid 2010. In our data, we observe a steep amplitude\nincrease, suggesting a high vulnerability of the two coupled systems.\n"
    },
    {
        "paper_id": 1502.07397,
        "authors": "Stephane Crepey, Andrea Macrina, Tuyet Mai Nguyen, David Skovmand",
        "title": "Rational Multi-Curve Models with Counterparty-Risk Valuation Adjustments",
        "comments": "34 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a multi-curve term structure setup in which the modelling\ningredients are expressed by rational functionals of Markov processes. We\ncalibrate to LIBOR swaptions data and show that a rational two-factor lognormal\nmulti-curve model is sufficient to match market data with accuracy. We\nelucidate the relationship between the models developed and calibrated under a\nrisk-neutral measure Q and their consistent equivalence class under the\nreal-world probability measure P. The consistent P-pricing models are applied\nto compute the risk exposures which may be required to comply with regulatory\nobligations. In order to compute counterparty-risk valuation adjustments, such\nas CVA, we show how positive default intensity processes with rational form can\nbe derived. We flesh out our study by applying the results to a basis swap\ncontract.\n"
    },
    {
        "paper_id": 1502.07522,
        "authors": "Philip Rinn, Yuriy Stepanov, Joachim Peinke, Thomas Guhr and Rudi\n  Sch\\\"afer",
        "title": "Dynamics of quasi-stationary systems: Finance as an example",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a combination of cluster analysis and stochastic process analysis\nto characterize high-dimensional complex dynamical systems by few dominating\nvariables. As an example, stock market data are analyzed for which the\ndynamical stability as well as transitions between different stable states are\nfound. This combined method also allows to set up new criteria for merging\nclusters to simplify the complexity of the system. The low-dimensional approach\nallows to recover the high-dimensional fixed points of the system by means of\nan optimization procedure.\n"
    },
    {
        "paper_id": 1502.07531,
        "authors": "Kathrin Glau",
        "title": "Feynman-Kac formula for L\\'evy processes with discontinuous killing rate",
        "comments": "Revision and a new section added: Numerical Example",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The challenge to fruitfully merge state-of-the-art techniques from\nmathematical finance and numerical analysis has inspired researchers to develop\nfast deterministic option pricing methods. As a result, highly efficient\nalgorithms to compute option prices in L\\'evy models by solving partial integro\ndifferential equations have been developed. In order to provide a solid\nmathematical foundation for these methods, we derive a Feynman-Kac\nrepresentation of variational solutions to partial integro differential\nequations that characterize conditional expectations of functionals of killed\ntime-inhomogeneous L\\'evy processes. We allow for a wide range of underlying\nstochastic processes, comprising processes with Brownian part, and a broad\nclass of pure jump processes such as generalized hyperbolic, multivariate\nnormal inverse Gaussian, tempered stable, and $\\alpha$-semi stable L\\'evy\nprocesses. By virtue of our mild regularity assumptions as to the killing rate\nand the initial condition of the partial differential equation, our results\nprovide a rigorous basis for numerous applications, not only in financial\nmathematics but also in probability theory and relativistic quantum mechanics.\n"
    },
    {
        "paper_id": 1502.07622,
        "authors": "Tihomir Gyulov, Lyuben Valkov",
        "title": "Well-Posedness and Comparison Principle for Option Pricing with\n  Switching Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an integro-differential equation derived from a system of coupled\nparabolic PDE and an ODE which describes an European option pricing with\nliquidity shocks. We study the well-posedness and prove comparison principle\nfor the corresponding initial value problem.\n"
    },
    {
        "paper_id": 1502.07625,
        "authors": "Derrick M. Anderson, Andrew B. Whitford",
        "title": "Developing Knowledge States: Technology and the Enhancement of National\n  Statistical Capacity",
        "comments": "32 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  National statistical systems are the enterprises tasked with collecting,\nvalidating and reporting societal attributes. These data serve many purposes -\nthey allow governments to improve services, economic actors to traverse\nmarkets, and academics to assess social theories. National statistical systems\nvary in quality, especially in developing countries. This study examines\ndeterminants of national statistical capacity in developing countries, focusing\non the impact of general purpose technologies (GPTs). Just as technological\nprogress helps to explain differences in economic growth, states with markets\nwith greater technological attainment (specifically, general purpose\ntechnologies) arguably have greater capacity for gathering and processing\nquality data. Analysis using panel methods shows a strong, statistically\nsignificant positive linear relationship between GPTs and national statistical\ncapacity. There is no evidence to support a non-linear function in this\nrelationship. Which is to say, there does not appear to be a marginal\ndepreciating National Statistical Capacity benefit associated with increases in\nGPTs.\n"
    },
    {
        "paper_id": 1502.07961,
        "authors": "Zachary Feinstein, Birgit Rudloff, Stefan Weber",
        "title": "Measures of Systemic Risk",
        "comments": "35 pages, 11 figures",
        "journal-ref": "SIAM Journal on Financial Mathematics 8 (1), 672-708 (2017)",
        "doi": "10.1137/16M1066087",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk refers to the risk that the financial system is susceptible to\nfailures due to the characteristics of the system itself. The tremendous cost\nof systemic risk requires the design and implementation of tools for the\nefficient macroprudential regulation of financial institutions. The current\npaper proposes a novel approach to measuring systemic risk.\n  Key to our construction is a rigorous derivation of systemic risk measures\nfrom the structure of the underlying system and the objectives of a financial\nregulator. The suggested systemic risk measures express systemic risk in terms\nof capital endowments of the financial firms. Their definition requires two\ningredients: a cash flow or value model that assigns to the capital allocations\nof the entities in the system a relevant stochastic outcome; and an\nacceptability criterion, i.e. a set of random outcomes that are acceptable to a\nregulatory authority. Systemic risk is measured by the set of allocations of\nadditional capital that lead to acceptable outcomes. We explain the conceptual\nframework and the definition of systemic risk measures, provide an algorithm\nfor their computation, and illustrate their application in numerical case\nstudies.\n  Many systemic risk measures in the literature can be viewed as the minimal\namount of capital that is needed to make the system acceptable after\naggregating individual risks, hence quantify the costs of a bail-out. In\ncontrast, our approach emphasizes operational systemic risk measures that\ninclude both ex post bailout costs as well as ex ante capital requirements and\nmay be used to prevent systemic crises.\n"
    },
    {
        "paper_id": 1503.00019,
        "authors": "Fabi\\'an Crocce, Juho H\\\"app\\\"ol\\\"a, Jonas Kiessling, Ra\\'ul Tempone",
        "title": "Error analysis in Fourier methods for option pricing",
        "comments": "21 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a bound for the error committed when using a Fourier method to\nprice European options when the underlying follows an exponential \\levy\ndynamic. The price of the option is described by a partial integro-differential\nequation (PIDE). Applying a Fourier transformation to the PIDE yields an\nordinary differential equation that can be solved analytically in terms of the\ncharacteristic exponent of the \\levy process. Then, a numerical inverse Fourier\ntransform allows us to obtain the option price. We present a novel bound for\nthe error and use this bound to set the parameters for the numerical method. We\nanalyse the properties of the bound for a dissipative and pure-jump example.\nThe bound presented is independent of the asymptotic behaviour of option prices\nat extreme asset prices. The error bound can be decomposed into a product of\nterms resulting from the dynamics and the option payoff, respectively. The\nanalysis is supplemented by numerical examples that demonstrate results\ncomparable to and superior to the existing literature.\n"
    },
    {
        "paper_id": 1503.00127,
        "authors": "Francesco Picciolo, Andreas Papandreou, Klaus Hubacek and Franco\n  Ruzzenenti",
        "title": "How crude oil prices shape the global division of labour",
        "comments": "11 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our work sheds new light on the role of oil prices in shaping the world\neconomy by investigating flows of goods and services through global value\nchains between 1960 and 2011, by means of Markov Chain and network analysis. We\nshow that over that time period the international division of labor and trade\npatterns are tightly linked to the price of oil. We demonstrate that this\ncorrelation does not depend on the balance of payments nor on the nominal value\nof trade or trade agreements; it is instead linked to the way the Global Value\nChains (GVCs) shape global trade. Our study suggests that transport played an\nimportant structural role in shaping GVCs.\n"
    },
    {
        "paper_id": 1503.00421,
        "authors": "Ashadun Nobi, Jae Woo Lee",
        "title": "State and group dynamics of world stock market by principal component\n  analysis",
        "comments": "13 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2015.12.144",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the dynamic interactions and structural changes in global financial\nindices in the years 1998-2012. We apply a principal component analysis (PCA)\nto cross-correlation coefficients of the stock indices. We calculate the\ncorrelations between principal components (PCs) and each asset, known as PC\ncoefficients. A change in market state is identified as a change in the first\nPC coefficients. Some indices do not show significant change of PCs in market\nstate during crises. The indices exposed to the invested capitals in the stock\nmarkets are at the minimum level of risk. Using the first two PC coefficients,\nwe identify indices that are similar and more strongly correlated than the\nothers. We observe that the European indices form a robust group over the\nobservation period. The dynamics of the individual indices within the group\nincrease in similarity with time, and the dynamics of indices are more similar\nduring the crises. Furthermore, the group formation of indices changes position\nin two-dimensional spaces due to crises. Finally, after a financial crisis, the\ndifference of PCs between the European and American indices narrows.\n"
    },
    {
        "paper_id": 1503.00529,
        "authors": "Sergei Maslov and Kim Sneppen",
        "title": "Diversity waves in collapse-driven population dynamics",
        "comments": "15 pages (including SI), 6 figures + 7 supplementary figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pcbi.1004440",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Populations of species in ecosystems are often constrained by availability of\nresources within their environment. In effect this means that a growth of one\npopulation, needs to be balanced by comparable reduction in populations of\nothers. In neutral models of biodiversity all populations are assumed to change\nincrementally due to stochastic births and deaths of individuals. Here we\npropose and model another redistribution mechanism driven by abrupt and severe\ncollapses of the entire population of a single species freeing up resources for\nthe remaining ones. This mechanism may be relevant e.g. for communities of\nbacteria, with strain-specific collapses caused e.g. by invading\nbacteriophages, or for other ecosystems where infectious diseases play an\nimportant role.\n  The emergent dynamics of our system is cyclic \"diversity waves\" triggered by\ncollapses of globally dominating populations. The population diversity peaks at\nthe beginning of each wave and exponentially decreases afterwards. Species\nabundances are characterized by a bimodal time-aggregated distribution with the\nlower peak formed by populations of recently collapsed or newly introduced\nspecies, while the upper peak - species that has not yet collapsed in the\ncurrent wave. In most waves both upper and lower peaks are composed of several\nsmaller peaks. This self-organized hierarchical peak structure has a long-term\nmemory transmitted across several waves. It gives rise to a scale-free tail of\nthe time-aggregated population distribution with a universal exponent of 1.7.\nWe show that diversity wave dynamics is robust with respect to variations in\nthe rules of our model such as diffusion between multiple environments,\nspecies-specific growth and extinction rates, and bet-hedging strategies.\n"
    },
    {
        "paper_id": 1503.00556,
        "authors": "Yuriy Stepanov, Philip Rinn, Thomas Guhr, Joachim Peinke and Rudi\n  Sch\\\"afer",
        "title": "Stability and Hierarchy of Quasi-Stationary States: Financial Markets as\n  an Example",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2015/08/P08011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We combine geometric data analysis and stochastic modeling to describe the\ncollective dynamics of complex systems. As an example we apply this approach to\nfinancial data and focus on the non-stationarity of the market correlation\nstructure. We identify the dominating variable and extract its explicit\nstochastic model. This allows us to establish a connection between its time\nevolution and known historical events on the market. We discuss the dynamics,\nthe stability and the hierarchy of the recently proposed quasi-stationary\nmarket states.\n"
    },
    {
        "paper_id": 1503.00621,
        "authors": "Stefano Battiston, Marco D'Errico, Stefano Gurciullo, Guido Caldarelli",
        "title": "Leveraging the network: a stress-test framework based on DebtRank",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel stress-test framework to monitor systemic risk in\nfinancial systems. The modular structure of the framework allows to accommodate\nfor a variety of shock scenarios, methods to estimate interbank exposures and\nmechanisms of distress propagation. The main features are as follows. First,\nthe framework allows to estimate and disentangle not only first-round effects\n(i.e. shock on external assets) and second-round effects (i.e. distress induced\nin the interbank network), but also third-round effects induced by possible\nfire sales. Second, it allows to monitor at the same time the impact of shocks\non individual or groups of financial institutions as well as their\nvulnerability to shocks on counterparties or certain asset classes. Third, it\nincludes estimates for loss distributions, thus combining network effects with\nfamiliar risk measures such as VaR and CVaR. Fourth, in order to perform\nrobustness analyses and cope with incomplete data, the framework features a\nmodule for the generation of sets of networks of interbank exposures that are\ncoherent with the total lending and borrowing of each bank. As an illustration,\nwe carry out a stress-test exercise on a dataset of listed European banks over\nthe years 2008-2013. We find that second-round and third-round effects dominate\nfirst-round effects, therefore suggesting that most current stress-test\nframeworks might lead to a severe underestimation of systemic risk.\n"
    },
    {
        "paper_id": 1503.00823,
        "authors": "Ya-Chun Gao, Yong Zeng, Shi-Min Cai",
        "title": "Influence network in Chinese stock market",
        "comments": "14 pages, 7 Figures",
        "journal-ref": "J. Stat. Mech. (2015) P03017",
        "doi": "10.1088/1742-5468/2015/03/P03017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a stock market, the price fluctuations are interactive, that is, one\nlisted company can influence others. In this paper, we seek to study the\ninfluence relationships among listed companies by constructing a directed\nnetwork on the basis of Chinese stock market. This influence network shows\ndistinct topological properties, particularly, a few large companies that can\nlead the tendency of stock market are recognized. Furthermore, by analyzing the\nsubnetworks of listed companies distributed in several significant economic\nsectors, it is found that the influence relationships are totally different\nfrom one economic sector to another, of which three types of connectivity as\nwell as hub-like listed companies are identified. In addition, the rankings of\nlisted companies obtained from the centrality metrics of influence network are\ncompared with that according to the assets, which gives inspiration to uncover\nand understand the importance of listed companies in the stock market. These\nempirical results are meaningful in providing these topological properties of\nChinese stock market and economic sectors as well as revealing the\ninteractively influence relationships among listed companies.\n"
    },
    {
        "paper_id": 1503.00864,
        "authors": "Stefan Waldenberger and Wolfgang M\\\"uller",
        "title": "Affine LIBOR models driven by real-valued affine processes",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The class of affine LIBOR models is appealing since it satisfies three\ncentral requirements of interest rate modeling. It is arbitrage-free, interest\nrates are nonnegative and caplet and swaption prices can be calculated\nanalytically. In order to guarantee nonnegative interest rates affine LIBOR\nmodels are driven by nonnegative affine processes, a restriction, which makes\nit hard to produce volatility smiles. We modify the affine LIBOR models in such\na way that real-valued affine processes can be used without destroying the\nnonnegativity of interest rates. Numerical examples show that in this class of\nmodels pronounced volatility smiles are possible.\n"
    },
    {
        "paper_id": 1503.00913,
        "authors": "Kyubin Yim, Gabjin Oh, Seunghwan Kim",
        "title": "Understanding Financial Market States Using Artificial Double Auction\n  Market",
        "comments": "17 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0152608",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ultimate value of theories of the fundamental mechanisms comprising the\nasset price in financial systems will be reflected in the capacity of such\ntheories to understand these systems. Although the models that explain the\nvarious states of financial markets offer substantial evidences from the fields\nof finance, mathematics, and even physics to explain states observed in the\nreal financial markets, previous theories that attempt to fully explain the\ncomplexities of financial markets have been inadequate. In this study, we\npropose an artificial double auction market as an agent-based model approach to\nstudy the origin of complex states in the financial markets, characterizing\nimportant parameters with an investment strategy that can cover the dynamics of\nthe financial market. The investment strategy of chartist traders after market\ninformation arrives should reduce market stability originating in the price\nfluctuations of risky assets. However, fundamentalist traders strategically\nsubmit orders with a fundamental value and, thereby stabilize the market. We\nconstruct a continuous double auction market and find that the market is\ncontrolled by a fraction of chartists, P_{c}. We show that mimicking real\nfinancial markets state, which emerges in real financial systems, is given\nbetween approximately P_{c} = 0.40 and P_{c} = 0.85, but that mimicking the\nefficient market hypothesis state can be generated in a range of less than\nP_{c} = 0.40. In particular, we observe that the mimicking market collapse\nstate created in a value greater than P_{c} = 0.85, in which a liquidity\nshortage occurs, and the phase transition behavior is P_{c} = 0.85.\n"
    },
    {
        "paper_id": 1503.00961,
        "authors": "Erhan Bayraktar and Virginia R. Young",
        "title": "Optimally Investing to Reach a Bequest Goal",
        "comments": "Final version. To appear in Insurance: Mathematics and Economics.\n  Keywords: Bequest motive, consumption, optimal investment, stochastic control",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the optimal strategy for investing in a Black-Scholes market in\norder to maximize the probability that wealth at death meets a bequest goal\n$b$, a type of goal-seeking problem, as pioneered by Dubins and Savage (1965,\n1976). The individual consumes at a constant rate $c$, so the level of wealth\nrequired for risklessly meeting consumption equals $c/r$, in which $r$ is the\nrate of return of the riskless asset.\n  Our problem is related to, but different from, the goal-reaching problems of\nBrowne (1997). First, Browne (1997, Section 3.1) maximizes the probability that\nwealth reaches $b < c/r$ before it reaches $a < b$. Browne's game ends when\nwealth reaches $b$. By contrast, for the problem we consider, the game\ncontinues until the individual dies or until wealth reaches 0; reaching $b$ and\nthen falling below it before death does not count.\n  Second, Browne (1997, Section 4.2) maximizes the expected discounted reward\nof reaching $b > c/r$ before wealth reaches $c/r$. If one interprets his\ndiscount rate as a hazard rate, then our two problems are {\\it mathematically}\nequivalent for the special case for which $b > c/r$, with ruin level $c/r$.\nHowever, we obtain different results because we set the ruin level at 0,\nthereby allowing the game to continue when wealth falls below $c/r$.\n"
    },
    {
        "paper_id": 1503.01584,
        "authors": "Frederik Meudt, Martin Theissen, Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Constructing Analytically Tractable Ensembles of Non-Stationary\n  Covariances with an Application to Financial Data",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2015/11/P11025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In complex systems, crucial parameters are often subject to unpredictable\nchanges in time. Climate, biological evolution and networks provide numerous\nexamples for such non-stationarities. In many cases, improved statistical\nmodels are urgently called for. In a general setting, we study systems of\ncorrelated quantities to which we refer as amplitudes. We are interested in the\ncase of non-stationarity, i.e., seemingly random covariances. We present a\ngeneral method to derive the distribution of the covariances from the\ndistribution of the amplitudes. To ensure analytical tractability, we construct\na properly deformed Wishart ensemble of random matrices. We apply our method to\nfinancial returns where the wealth of data allows us to carry out statistically\nsignificant tests. The ensemble that we find is characterized by an algebraic\ndistribution which improves the understanding of large events.\n"
    },
    {
        "paper_id": 1503.01754,
        "authors": "M. Bonollo, L. Di Persio, I. Oliva, A. Semmoloni",
        "title": "A Quantization Approach to the Counterparty Credit Exposure Estimation",
        "comments": "30 pages, 10 figures, 19 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During recent years the counterparty risk subject has received a growing\nattention because of the so called Basel Accord. In particular the Basel III\nAccord asks the banks to fulfill finer conditions concerning counterparty\ncredit exposures arising from banks' derivatives, securities financing\ntransactions, default and downgrade risks characterizing the Over The Counter\n(OTC) derivatives market, etc. Consequently the development of effective and\nmore accurate measures of risk have been pushed, particularly focusing on the\nestimate of the future fair value of derivatives with respect to prescribed\ntime horizon and fixed grid of time buckets. Standard methods used to treat the\nlatter scenario are mainly based on ad hoc implementations of the classic Monte\nCarlo (MC) approach, which is characterized by a high computational time,\nstrongly dependent on the number of considered assets. This is why many\nfinancial players moved to more enhanced Technologies, e.g., grid computing and\nGraphics Processing Units (GPUs) capabilities. In this paper we show how to\nimplement the quantization technique, in order to accurately estimate both\npricing and volatility values. Our approach is tested to produce effective\nresults for the counterparty risk evaluation, with a big improvement concerning\nrequired time to run when compared to MC approach.\n"
    },
    {
        "paper_id": 1503.01802,
        "authors": "Amogh Deshpande and Saul D. Jacka",
        "title": "Game-theoretic approach to risk-sensitive benchmarked asset management",
        "comments": "Forthcoming in Risk and Decision Analysis. arXiv admin note: text\n  overlap with arXiv:0905.4740 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider a game theoretic approach to the Risk-Sensitive\nBenchmarked Asset Management problem (RSBAM) of Davis and Lleo \\cite{DL}. In\nparticular, we consider a stochastic differential game between two players,\nnamely, the investor who has a power utility while the second player represents\nthe market which tries to minimize the expected payoff of the investor. The\nmarket does this by modulating a stochastic benchmark that the investor needs\nto outperform. We obtain an explicit expression for the optimal pair of\nstrategies as for both the players.\n"
    },
    {
        "paper_id": 1503.02034,
        "authors": "Alexander Sokol",
        "title": "A generic model for spouse's pensions with a view towards the\n  calculation of liabilities",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics, Vol. 65, pp. 198--207, 2015",
        "doi": "10.1016/j.insmatheco.2015.09.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a generic model for spouse's pensions. The generic model allows\nfor the modeling of various types of spouse's pensions with payments commencing\nat the death of the insured. We derive abstract formulas for cashflows and\nliabilities corresponding to common types of spouse's pensions. We show how the\nstandard formulas from the Danish G82 concession can be obtained as a special\ncase of our generic model. We also derive expressions for liabilities for\nspouse's pensions in models more advanced than found in the G82 concession. The\ngeneric nature of our model and results furthermore enable the calculation of\ncashflows and liabilities using simple estimates of marital behaviour among a\npopulation.\n"
    },
    {
        "paper_id": 1503.02177,
        "authors": "Rudi Sch\\\"afer, Sonja Barkhofen, Thomas Guhr, Hans-J\\\"urgen\n  St\\\"ockmann, Ulrich Kuhl",
        "title": "Compounding approach for univariate time series with non-stationary\n  variances",
        "comments": "7 pages",
        "journal-ref": "Phys. Rev. E 92, 062901 (2015)",
        "doi": "10.1103/PhysRevE.92.062901",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A defining feature of non-stationary systems is the time dependence of their\nstatistical parameters. Measured time series may exhibit Gaussian statistics on\nshort time horizons, due to the central limit theorem. The sample statistics\nfor long time horizons, however, averages over the time-dependent parameters.\nTo model the long-term statistical behavior, we compound the local distribution\nwith the distribution of its parameters. Here we consider two concrete, but\ndiverse examples of such non-stationary systems, the turbulent air flow of a\nfan and a time series of foreign exchange rates. Our main focus is to\nempirically determine the appropriate parameter distribution for the\ncompounding approach. To this end we have to estimate the parameter\ndistribution for univariate time series in a highly non-stationary situation.\n"
    },
    {
        "paper_id": 1503.02237,
        "authors": "Erhan Bayraktar, Virginia R. Young, David Promislow",
        "title": "Purchasing Term Life Insurance to Reach a Bequest Goal: Time-Dependent\n  Case",
        "comments": "To appear in North American Actuarial Journal. Keywords: Term life\n  insurance, bequest motive, deterministic control. arXiv admin note: text\n  overlap with arXiv:1402.5300",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of how an individual can use term life insurance to\nmaximize the probability of reaching a given bequest goal, an important problem\nin financial planning. We assume that the individual buys instantaneous term\nlife insurance with a premium payable continuously. By contrast with Bayraktar\net al. (2014), we allow the force of mortality to vary with time, which, as we\nshow, greatly complicates the problem.\n"
    },
    {
        "paper_id": 1503.02405,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Pawe{\\l} O\\'swi\\k{e}cimka",
        "title": "Detecting and interpreting distortions in hierarchical organization of\n  complex time series",
        "comments": "5 pages, 4 figures. Accepted for publication in Physical Review E as\n  a Rapid Communication",
        "journal-ref": "Phys. Rev. E 91, 030902(R) (2015)",
        "doi": "10.1103/PhysRevE.91.030902",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hierarchical organization is a cornerstone of complexity and multifractality\nconstitutes its central quantifying concept. For model uniform cascades the\ncorresponding singularity spectra are symmetric while those extracted from\nempirical data are often asymmetric. Using the selected time series\nrepresenting such diverse phenomena like price changes and inter-transaction\ntimes in the financial markets, sentence length variability in the narrative\ntexts, Missouri River discharge and Sunspot Number variability as examples, we\nshow that the resulting singularity spectra appear strongly asymmetric, more\noften left-sided but in some cases also right-sided. We present a unified view\non the origin of such effects and indicate that they may be crucially\ninformative for identifying composition of the time series. One particularly\nintriguing case of this later kind of asymmetry is detected in the daily\nreported Sunspot Number variability. This signals that either the commonly used\nfamous Wolf formula distorts the real dynamics in expressing the largest\nSunspot Numbers or, if not, that their dynamics is governed by a somewhat\ndifferent mechanism.\n"
    },
    {
        "paper_id": 1503.02479,
        "authors": "Baosen Zhang and Ramesh Johari and Ram Rajagopal",
        "title": "Competition and Efficiency of Coalitions in Cournot Games with\n  Uncertainty",
        "comments": "Submitted to IEEE TCNS",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the impact of coalition formation on the efficiency of Cournot\ngames where producers face uncertainties. In particular, we study a market\nmodel where firms must determine their output before an uncertain production\ncapacity is realized. In contrast to standard Cournot models, we show that the\ngame is not efficient when there are many small firms. Instead, producers tend\nto act conservatively to hedge against their risks. We show that in the\npresence of uncertainty, the game becomes efficient when firms are allowed to\ntake advantage of diversity to form groups of certain sizes. We characterize\nthe tradeoff between market power and uncertainty reduction as a function of\ngroup size. In particular, we compare the welfare and output obtained with\ncoalitional competition, with the same benchmarks when output is controlled by\na single system operator. We show when there are $N$ firms present, competition\nbetween groups of size $\\Omega(\\sqrt{N})$ results in equilibria that are\nsocially optimal in terms of welfare and groups of size $\\Omega(N^{2/3})$ are\nsocially optimal in terms of production. We also extend our results to the case\nof uncertain demand by establishing an equivalency between Cournot oligopoly\nand Cournot Oligopsony. We demonstrate our results with real data from\nelectricity markets with significant wind power penetration.\n"
    },
    {
        "paper_id": 1503.02822,
        "authors": "Zhaoxu Hou, Jan Obloj",
        "title": "On robust pricing-hedging duality in continuous time",
        "comments": "Section 3.4 added containing martingale optimal transport duality for\n  multiple maturities with exact matching of intermediate laws under stronger\n  continuity assumptions on the payoff",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We pursue robust approach to pricing and hedging in mathematical finance. We\nconsider a continuous time setting in which some underlying assets and options,\nwith continuous paths, are available for dynamic trading and a further set of\nEuropean options, possibly with varying maturities, is available for static\ntrading. Motivated by the notion of prediction set in Mykland (2003), we\ninclude in our setup modelling beliefs by allowing to specify a set of paths to\nbe considered, e.g. super-replication of a contingent claim is required only\nfor paths falling in the given set. Our framework thus interpolates between\nmodel-independent and model-specific settings and allows to quantify the impact\nof making assumptions or gaining information. We obtain a general\npricing-hedging duality result: the infimum over superhedging prices is equal\nto supremum over calibrated martingale measures. In presence of non-trivial\nbeliefs, the equality is between limiting values of perturbed problems. In\nparticular, our results include the martingale optimal transport duality of\nDolinsky and Soner (2013) and extend it to multiple dimensions and multiple\nmaturities.\n"
    },
    {
        "paper_id": 1503.03006,
        "authors": "Alain B\\'elanger, Gaston Giroux and Ndoun\\'e Ndoun\\'e",
        "title": "Some new results on Dufffie-type OTC markets",
        "comments": "14 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1202.5251",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The extended Wild sums considered in this article generalize the classi- cal\nWild sums of statistical physics. We first show how to obtain explicit\nsolutions for the evolution equation of a large system where the interactions\nare given by a single, but general, interacting kernel which involves m\ncomponents, for a fixed m >= 2. We then show how to retain the explicit\nformulas for the case of OTC market models where the dynamics is more directly\ndescribed by two (or more) kernels.\n"
    },
    {
        "paper_id": 1503.0318,
        "authors": "Jae Youn Ahn",
        "title": "Negative Dependence Concept in Copulas and the Marginal Free Herd\n  Behavior Index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a set of copulas that can be interpreted as having the negative\nextreme dependence. This set of copulas is interesting because it coincides\nwith countermonotonic copula for a bivariate case, and more importantly, is\nshown to be minimal in concordance ordering in the sense that no copula exists\nwhich is strictly smaller than the given copula outside the proposed copula\nset. Admitting the absence of the minimum copula in multivariate dimensions\ngreater than 2, the study of the set of minimal copulas can be important in the\ninvestigation of various optimization problems. To demonstrate the importance\nof the proposed copula set, we provide the variance minimization problem of the\naggregated sum with arbitrarily given uniform marginals. As a\nfinancial/actuarial application of these copulas, we define a new herd behavior\nindex using weighted Spearman's rho, and determine the sharp lower bound of the\nindex using the proposed set of copulas.\n"
    },
    {
        "paper_id": 1503.03194,
        "authors": "Michael Okelola and Keshlan Govinder",
        "title": "Symmetry structure and solution of evolution-type equations with time\n  dependent parameters in financial Mathematics",
        "comments": "6 pages of original research article",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical models with time dependent parameters are of great interest in\nfinancial Mathematics because they capture real life scenarios in the financial\nmarket. In this study, via the Lie group technique, we analyse evolution-type\nequations with time dependent parameters and give the general symmetry\nstructure of these equations. In addition, we illustrate this method by looking\nat an example of exotic options called the power options. Our model parameters\nare time dependent and the option gives a continuous yield dividend at\ndifferent time intervals. We present new solutions, satisfying the boundary\nconditions, to this important problem.\n"
    },
    {
        "paper_id": 1503.03548,
        "authors": "Yu-Lei Wan (ECUST), Wen-Jie Xie (ECUST), Gao-Feng Gu (ECUST),\n  Zhi-Qiang Jiang (ECUST), Wei Chen (SZSE), Xiong Xiong (TJU), Wei Zhang (TJU),\n  Wei-Xing Zhou (ECUST)",
        "title": "Statistical Properties and Pre-hit Dynamics of Price Limit Hits in the\n  Chinese Stock Markets",
        "comments": "18 pages including 8 figures and 2 tables",
        "journal-ref": "PLoS ONE 10 (2), e0120312 (2015)",
        "doi": "10.1371/journal.pone.0120312",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price limit trading rules are adopted in some stock markets (especially\nemerging markets) trying to cool off traders' short-term trading mania on\nindividual stocks and increase market efficiency. Under such a microstructure,\nstocks may hit their up-limits and down-limits from time to time. However, the\nbehaviors of price limit hits are not well studied partially due to the fact\nthat main stock markets such as the US markets and most European markets do not\nset price limits. Here, we perform detailed analyses of the high-frequency data\nof all A-share common stocks traded on the Shanghai Stock Exchange and the\nShenzhen Stock Exchange from 2000 to 2011 to investigate the statistical\nproperties of price limit hits and the dynamical evolution of several important\nfinancial variables before stock price hits its limits. We compare the\nproperties of up-limit hits and down-limit hits. We also divide the whole\nperiod into three bullish periods and three bearish periods to unveil possible\ndifferences during bullish and bearish market states. To uncover the impacts of\nstock capitalization on price limit hits, we partition all stocks into six\nportfolios according to their capitalizations on different trading days. We\nfind that the price limit trading rule has a cooling-off effect (object to the\nmagnet effect), indicating that the rule takes effect in the Chinese stock\nmarkets. We find that price continuation is much more likely to occur than\nprice reversal on the next trading day after a limit-hitting day, especially\nfor down-limit hits, which has potential practical values for market\npractitioners.\n"
    },
    {
        "paper_id": 1503.03567,
        "authors": "Michael V. Klibanov and Andrey V. Kuzhuget",
        "title": "Profitable forecast of prices of stock options on real market data via\n  the solution of an ill-posed problem for the Black-Scholes equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new mathematical model for the Black-Scholes equation is proposed to\nforecast option prices. This model includes new interval for the price of the\nunderlying stock as well as new initial and boundary conditions. Conventional\nnotions of maturity time and strike prices are not used. The Black-Scholes\nequation is solved as a parabolic equation with the reversed time, which is an\nill-posed problem. Thus, a regularization method is used to solve it. This idea\nis verified on real market data for twenty liquid options. A trading strategy\nis proposed. This strategy indicates that our method is profitable on at least\nthose twenty options. We conjecture that our method might lead to significant\nprofits of those financial institutions which trade large amounts of options.\nWe caution, however, that detailed further studies are necessary to verify this\nconjecture.\n"
    },
    {
        "paper_id": 1503.03705,
        "authors": "M. Briani, L. Caramellino, A. Zanette",
        "title": "A hybrid tree/finite-difference approach for Heston-Hull-White type\n  models",
        "comments": null,
        "journal-ref": "The Journal of Computational Finance 2017 Volume 21 Number 3 Pages\n  1-45",
        "doi": "10.21314/JCF.2017.333",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a hybrid tree-finite difference method which permits to obtain\nefficient and accurate European and American option prices in the Heston\nHull-White and Heston Hull-White2d models. Moreover, as a by-product, we\nprovide a new simulation scheme to be used for Monte Carlo evaluations.\nNumerical results show the reliability and the efficiency of the proposed\nmethods\n"
    },
    {
        "paper_id": 1503.03726,
        "authors": "Oliver Kley and Claudia Kluppelberg",
        "title": "Bounds for randomly shared risk of heavy-tailed loss factors",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10687-016-0248-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a risk vector $V$, whose components are shared among agents by some\nrandom mechanism, we obtain asymptotic lower and upper bounds for the\nindividual agents' exposure risk and the aggregated risk in the market. Risk is\nmeasured by Value-at-Risk or Conditional Tail Expectation. We assume Pareto\ntails for the components of $V$ and arbitrary dependence structure in a\nmultivariate regular variation setting. Upper and lower bounds are given by\nasymptotically independent and fully dependent components of $V$ with respect\nto the tail index $\\alpha$ being smaller or larger than 1. Counterexamples,\nwhere for non-linear aggregation functions no bounds are available, complete\nthe picture.\n"
    },
    {
        "paper_id": 1503.03902,
        "authors": "D.J. Manuge",
        "title": "L\\'evy Processes For Finance: An Introduction In R",
        "comments": "18 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This brief manuscript provides an introduction to L\\'evy processes and their\napplications in finance as the random process that drives asset models.\nCharacteristic functions and random variable generators of popular L\\'evy\nprocesses are presented in R.\n"
    },
    {
        "paper_id": 1503.03986,
        "authors": "Jan Jurczyk",
        "title": "Measuring switching processes in financial markets with the\n  Mean-Variance spin glass approach",
        "comments": "7 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we use the Mean-Variance Model in order to measure the\ncurrent market state. In our study we take the approach of detecting the\noverall alignment of portfolios in the spin picture. The projection to the\nground-states enables us to use physical observables in order to describe the\ncurrent state of the explored market. The defined magnetization of portfolios\nshows cursor effects, which we use to detect turmoils.\n"
    },
    {
        "paper_id": 1503.0446,
        "authors": "Hirbod Assa",
        "title": "Optimal risk allocation in a market with non-convex preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aims of this study are twofold. First, we consider an optimal risk\nallocation problem with non-convex preferences. By establishing an infimal\nrepresentation for distortion risk measures, we give some necessary and\nsufficient conditions for the existence of optimal and asymptotic optimal\nallocations. We will show that, similar to a market with convex preferences, in\na non-convex framework with distortion risk measures the boundedness of the\noptimal risk allocation problem depends only on the preferences. Second, we\nconsider the same optimal allocation problem by adding a further assumption\nthat allocations are co-monotone. We characterize the co-monotone optimal risk\nallocations within which we prove the \"marginal risk allocations\" take only the\nvalues zero or one. Remarkably, we can separate the role of the market\npreferences and the total risk in our representation.\n"
    },
    {
        "paper_id": 1503.04772,
        "authors": "Mehrnoosh Khademi, Massimiliano Ferrara, Bruno Pansera, Mehdi Salimi",
        "title": "A dynamic game on Green Supply Chain Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we establish a dynamic game to allocate CSR (Corporate Social\nResponsibility) to the members of a supply chain. We propose a model of\nthree-tier supply chain in decentralized state that is including supplier,\nmanufacturer and retailer. For analyzing supply chain performance in\ndecentralized state and the relationships between the members of supply chain,\nwe use Stackelberg game and we consider in this paper a hierarchical\nequilibrium solution for a two-level game. Specially, we formulate a model that\ncrosses through multi-periods by a dynamic discreet Stackelberg game. We try to\nobtain an equilibrium point at where both the profits of members and the level\nof CSR taken by supply chains are maximized.\n"
    },
    {
        "paper_id": 1503.04799,
        "authors": "G\\'erard Weisbuch",
        "title": "From anti-conformism to extremism",
        "comments": "37 pages, 20 figures, fortran demo program",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We here present a model of the dynamics of extremism based on opinion\ndynamics in order to understand the circumstances which favour its emergence\nand development in large fractions of the general public. Our model is based on\nthe bounded confidence hypothesis and on the evolution of initially\nanti-conformist agents to extreme positions.\n  Numerical analyses demonstrate that a few anti-conformists are able to drag a\nlarge fraction of conformists agents to their position provided that they\nexpress their views more often than the conformists. The most influential\nparameter controlling the outcome of the dynamics is the uncertainty of the\nconformist agents; the higher their uncertainty, the higher is the influence of\nanti-conformists. Systematic scans of the parameter space show the existence of\ntwo regime transitions, one following the conformists uncertainty parameter and\nthe other one following the anti-conformism strength.\n"
    },
    {
        "paper_id": 1503.04841,
        "authors": "Deokjae Lee, Jae-Young Kim, Jeho Lee, B. Kahng",
        "title": "Forest Fire Model as a Supercritical Dynamic Model in Financial Systems",
        "comments": null,
        "journal-ref": "Phys. Rev. E 91, 022806 (2015)",
        "doi": "10.1103/PhysRevE.91.022806",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, large-scale cascading failures in complex systems have garnered\nsubstantial attention. Such extreme events have been treated as an integral\npart of the self-organized criticality (SOC). Recent empirical work has\nsuggested that some extreme events systematically deviate from the SOC\nparadigm, requiring a different theoretical framework. We shed additional\ntheoretical light on this possibility by studying financial crisis. We build\nour model of financial crisis on the well-known forest fire model in scale-free\nnetworks. Our analysis shows a non-trivial scaling feature indicating\nsupercritical behavior, which is independent of system size. Extreme events in\nthe supercritical state result from bursting of a fat bubble, seeds of which\nare sown by a protracted period of a benign financial environment with few\nshocks. Our findings suggest that policymakers can control the magnitude of\nfinancial meltdowns by keeping the economy operating within reasonable duration\nof a benign environment.\n"
    },
    {
        "paper_id": 1503.04979,
        "authors": "Stefan Waldenberger",
        "title": "The affine inflation market models",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interest rate market models, like the LIBOR market model, have the advantage\nthat the basic model quantities are directly observable in financial markets.\nInflation market models extend this approach to inflation markets, where\nzero-coupon and year-on-year inflation-indexed swaps are the basic observable\nproducts. For inflation market models considered so far closed formulas exist\nfor only one type of swap, but not for both. The model in this paper uses\naffine processes in such a way that prices for both types of swaps can be\ncalculated explicitly. Furthermore call and put options on both types of swap\nrates can be calculated using one-dimensional Fourier inversion formulas. Using\nthe derived formulas we present an example calibration to market data.\n"
    },
    {
        "paper_id": 1503.05098,
        "authors": "Fabio Saracco, Riccardo Di Clemente, Andrea Gabrielli, Tiziano\n  Squartini",
        "title": "Randomizing bipartite networks: the case of the World Trade Web",
        "comments": "22 pages, 13 figures",
        "journal-ref": "Sci. Rep. 5 (10595) (2015)",
        "doi": "10.1038/srep10595",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the last fifteen years, network theory has been successfully applied\nboth to natural sciences and to socioeconomic disciplines. In particular,\nbipartite networks have been recognized to provide a particularly insightful\nrepresentation of many systems, ranging from mutualistic networks in ecology to\ntrade networks in economy, whence the need of a pattern detection-oriented\nanalysis in order to identify statistically-significant structural properties.\nSuch an analysis rests upon the definition of suitable null models, i.e. upon\nthe choice of the portion of network structure to be preserved while\nrandomizing everything else. However, quite surprisingly, little work has been\ndone so far to define null models for real bipartite networks. The aim of the\npresent work is to fill this gap, extending a recently-proposed method to\nrandomize monopartite networks to bipartite networks. While the proposed\nformalism is perfectly general, we apply our method to the binary, undirected,\nbipartite representation of the World Trade Web, comparing the observed values\nof a number of structural quantities of interest with the expected ones,\ncalculated via our randomization procedure. Interestingly, the behavior of the\nWorld Trade Web in this new representation is strongly different from the\nmonopartite analogue, showing highly non-trivial patterns of self-organization.\n"
    },
    {
        "paper_id": 1503.05127,
        "authors": "Chiara Corini, Guglielmo D'Amico, Filippo Petroni, Flavio Prattico,\n  Raimondo Manca",
        "title": "Tornadoes and related damage costs: statistical modeling with a\n  semi-Markov approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a statistical approach to tornadoes modeling for predicting and\nsimulating occurrences of tornadoes and accumulated cost distributions over a\ntime interval. This is achieved by modeling the tornadoes intensity, measured\nwith the Fujita scale, as a stochastic process. Since the Fujita scale divides\ntornadoes intensity into six states, it is possible to model the tornadoes\nintensity by using Markov and semi-Markov models. We demonstrate that the\nsemi-Markov approach is able to reproduce the duration effect that is detected\nin tornadoes occurrence. The superiority of the semi-Markov model as compared\nto the Markov chain model is also affirmed by means of a statistical test of\nhypothesis. As an application we compute the expected value and the variance of\nthe costs generated by the tornadoes over a given time interval in a given\narea. he paper contributes to the literature by demonstrating that semi-Markov\nmodels represent an effective tool for physical analysis of tornadoes as well\nas for the estimation of the economic damages to human things.\n"
    },
    {
        "paper_id": 1503.05139,
        "authors": "Ander Olvik and Raul Kangro",
        "title": "Pricing of Warrants with Stock Price Dependent Threshold Conditions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Warrants with stock price dependent threshold conditions give the right to\nbuy specially issued stocks, if the performance of the stock price satisfies\nsome requirements. Existence of these derivatives changes the price process of\nthe underlying. We show that in the presence of such warrants one cannot assume\nthat the stock market is arbitrage free and that the stock is tradeable at\nevery time moment with the same price for buying and selling. This means that\nthe usual methods for deriving fair prices for such warrants cannot be used. We\nstart from a simple model for the firm's value process and discuss some ways to\nspecify a related model for the stock price process in the presence of warrants\nwith threshold conditions. We also discuss how indifference pricing approach\ncan be used for pricing such warrants.\n"
    },
    {
        "paper_id": 1503.05283,
        "authors": "Haonan Wu",
        "title": "Re-visiting the Distance Coefficient in Gravity Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper revisits the classic gravity model in international trade and\nreexamines the distance coefficient. As pointed out by Frankel (1997), this\ncoefficient measures the relative unit transportation cost between short\ndistance and long distance rather than the absolute level of average\ntransportation cost. Our results confirm this point in the sense that the\ncoefficient has been very stable between 1991-2006, despite the obvious\ntechnological progress taken place during this period. Moreover, by comparing\nthe sensitivity of these coefficients to change in oil prices at short periods\nof time, in which technology remained unchanged, we conclude that the average\ntechnology has indeed reduced the average trading cost. The results are robust\nwhen we divide the aggregate international trades into different industries.\n"
    },
    {
        "paper_id": 1503.05343,
        "authors": "Youssouf A. F. Toukourou and Fran\\c{c}ois Dufresne",
        "title": "ON Integrated Chance Constraints in ALM for Pension Funds",
        "comments": "29 pages, 16 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the role of integrated chance constraints (ICC) as quantitative\nrisk constraints in asset and liability management (ALM) for pension funds. We\ndefine two types of ICC: the one period integrated chance constraint (OICC) and\nthe multiperiod integrated chance constraint (MICC). As their names suggest,\nthe OICC covers only one period whereas several periods are taken into account\nwith the MICC. A multistage stochastic linear programming model is therefore\ndeveloped for this purpose and a special mention is paid to the modeling of the\nMICC.\n  Based on a numerical example, we firstly analyse the effects of the OICC and\nthe MICC on the optimal decisions (asset allocation and contribution rate) of a\npension fund. By definition, the MICC is more restrictive and safer compared to\nthe OICC. Secondly, we quantify this MICC safety increase. The results show\nthat although the optimal decisions from the OICC and the MICC differ, the\ntotal costs are very close, showing that the MICC is definitely a better\napproach since it is more prudent.\n"
    },
    {
        "paper_id": 1503.05416,
        "authors": "Boualem Djehiche and Peter Helgesson",
        "title": "The Principal-Agent Problem With Time Inconsistent Utility Functions",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1410.6392",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a generalization of the continuous time\nPrincipal-Agent problem allowing for time inconsistent utility functions, for\ninstance of mean-variance type. Using recent results on the Pontryagin maximum\nprinciple for FBSDEs we suggest a method of characterizing optimal contracts\nfor such models. To illustrate this we consider a fully solved explicit example\nin the linear quadratic setting.\n"
    },
    {
        "paper_id": 1503.05475,
        "authors": "B. Bouchard and G. Loeper and Y. Zou",
        "title": "Almost-sure hedging with permanent price impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial model with permanent price impact. Continuous time\ntrading dynamics are derived as the limit of discrete rebalancing policies. We\nthen study the problem of super-hedging a European option. Our main result is\nthe derivation of a quasi-linear pricing equation. It holds in the sense of\nviscosity solutions. When it admits a smooth solution, it provides a perfect\nhedging strategy.\n"
    },
    {
        "paper_id": 1503.0555,
        "authors": "Hao Meng, Wen-Jie Xie, Wei-Xing Zhou (ECUST)",
        "title": "Club Convergence of House Prices: Evidence from China's Ten Key Cities",
        "comments": "16 Latex pages including 6 figures and 4 tables",
        "journal-ref": "International Journal of Modern Physics B 29 (24), 1550181 (2015)",
        "doi": "10.1142/S0217979215501817",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The latest global financial tsunami and its follow-up global economic\nrecession has uncovered the crucial impact of housing markets on financial and\neconomic systems. The Chinese stock market experienced a markedly fall during\nthe global financial tsunami and China's economy has also slowed down by about\n2\\%-3\\% when measured in GDP. Nevertheless, the housing markets in diverse\nChinese cities seemed to continue the almost nonstop mania for more than ten\nyears. However, the structure and dynamics of the Chinese housing market are\nless studied. Here we perform an extensive study of the Chinese housing market\nby analyzing ten representative key cities based on both linear and nonlinear\neconophysical and econometric methods. We identify a common collective driving\nforce which accounts for 96.5\\% of the house price growth, indicating very high\nsystemic risk in the Chinese housing market. The ten key cities can be\ncategorized into clubs and the house prices of the cities in the same club\nexhibit an evident convergence. These findings from different methods are\nbasically consistent with each other. The identified city clubs are also\nconsistent with the conventional classification of city tiers. The house prices\nof the first-tier cities grow the fastest, and those of the third- and\nfourth-tier cities rise the slowest, which illustrates the possible presence of\na ripple effect in the diffusion of house prices in different cities.\n"
    },
    {
        "paper_id": 1503.05655,
        "authors": "Hagen Kleinert and Jan Korbel",
        "title": "Option Pricing Beyond Black-Scholes Based on Double-Fractional Diffusion",
        "comments": "16 pages, 5 figures",
        "journal-ref": "Physica A 449, 2016, 200-214",
        "doi": "10.1016/j.physa.2015.12.125",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how the prices of options can be determined with the help of\ndouble-fractional differential equation in such a way that their inclusion in a\nportfolio of stocks provides a more reliable hedge against dramatic price drops\nthat the use of options whose prices were fixed by the Black-Scholes formula.\n"
    },
    {
        "paper_id": 1503.05769,
        "authors": "Erhan Bayraktar and Asaf Cohen",
        "title": "Risk Sensitive Control of the Lifetime Ruin Problem",
        "comments": "Final version. To appear in Applied Mathematics and Optimization.\n  Keywords: Probability of lifetime ruin, optimal investment, risk sensitive\n  control, large deviations, differential games",
        "journal-ref": "Applied Mathematics and Optimization, 77 (2), 229-252, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a risk sensitive control version of the lifetime ruin probability\nproblem. We consider a sequence of investments problems in Black-Scholes market\nthat includes a risky asset and a riskless asset. We present a differential\ngame that governs the limit behavior. We solve it explicitly and use it in\norder to find an asymptotically optimal policy.\n"
    },
    {
        "paper_id": 1503.05909,
        "authors": "Alberto Ohashi, Alexandre B Simas",
        "title": "Principal Components Analysis for Semimartingales and Stochastic PDE",
        "comments": "54 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we develop a novel principal component analysis (PCA) for\nsemimartingales by introducing a suitable spectral analysis for the quadratic\nvariation operator. Motivated by high-dimensional complex systems typically\nfound in interest rate markets, we investigate correlation in high-dimensional\nhigh-frequency data generated by continuous semimartingales. In contrast to the\ntraditional PCA methodology, the directions of large variations are not\ndeterministic, but rather they are bounded variation adapted processes which\nmaximize quadratic variation almost surely. This allows us to reduce\ndimensionality from high-dimensional semimartingale systems in terms of\nquadratic covariation rather than the usual covariance concept.\n  The proposed methodology allows us to investigate space-time data driven by\nmulti-dimensional latent semimartingale state processes. The theory is applied\nto discretely-observed stochastic PDEs which admit finite-dimensional\nrealizations. In particular, we provide consistent estimators for\nfinite-dimensional invariant manifolds for Heath-Jarrow-Morton models. More\nimportantly, components of the invariant manifold associated to volatility and\ndrift dynamics are consistently estimated and identified. The proposed\nmethodology is illustrated with both simulated and real data sets.\n"
    },
    {
        "paper_id": 1503.0602,
        "authors": "Elena Agliari, Adriano Barra, Andrea Galluzzi, Francisco\n  Requena-Silvente, Daniele Tantari",
        "title": "Insights in Economical Complexity in Spain: the hidden boost of migrants\n  in international tradings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider extensive data on Spanish international trades and population\ncomposition and, through statistical-mechanics and graph-theory driven\nanalysis, we unveil that the social network made of native and foreign-born\nindividuals plays a role in the evolution and in the diversification of trades.\nIndeed, migrants naturally provide key information on policies and needs in\ntheir native countries, hence allowing firm's holders to leverage transactional\ncosts of exports and duties. As a consequence, international trading is\naffordable for a larger basin of firms and thus results in an increased number\nof transactions, which, in turn, implies a larger diversification of\ninternational traded products. These results corroborate the novel scenario\ndepicted by \"Economical Complexity\", where the pattern of production and trade\nof more developed countries is highly diversified. We also address a central\nquestion in Economics, concerning the existence of a critical threshold for\nmigrants (within a given territorial district) over which they effectively\ncontribute to boost international trades: in our physically-driven picture,\nthis phenomenon corresponds to the emergence of a phase transition and,\ntackling the problem from this perspective, results in a novel successful\nquantitative route. Finally, we can infer that the pattern of interaction\nbetween native and foreign-born population exhibits small-world features as\nsmall diameter, large clustering, and weak ties working as optimal cut-edge, in\ncomplete agreement with findings in \"Social Complexity\".\n"
    },
    {
        "paper_id": 1503.06205,
        "authors": "Lorien X. Hayden, Ricky Chachra, Alexander A. Alemi, Paul H. Ginsparg,\n  James P. Sethna",
        "title": "Canonical Sectors and Evolution of Firms in the US Stock Markets",
        "comments": "Some new figures; minor textual changes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A classification of companies into sectors of the economy is important for\nmacroeconomic analysis and for investments into the sector-specific financial\nindices and exchange traded funds (ETFs). Major industrial classification\nsystems and financial indices have historically been based on expert opinion\nand developed manually. Here we show how unsupervised machine learning can\nprovide a more objective and comprehensive broad-level sector decomposition of\nstocks. An emergent low-dimensional structure in the space of historical stock\nprice returns automatically identifies \"canonical sectors\" in the market, and\nassigns every stock a participation weight into these sectors. Furthermore, by\nanalyzing data from different periods, we show how these weights for listed\nfirms have evolved over time.\n"
    },
    {
        "paper_id": 1503.06317,
        "authors": "Amirhossein Sadoghi",
        "title": "Measuring Systemic Risk: Robust Ranking Techniques Approach",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research, we introduce a robust metric to identify Systemically\nImportant Financial Institution (SIFI) in a financial network by taking into\naccount both common idiosyncratic shocks and contagion through counterparty\nexposures. We develop an efficient algorithm to rank financial institutions by\nformulating a fixed point problem and reducing it to a non-smooth convex\noptimization problem. We then study the underlying distribution of the proposed\nmetric and analyze the performance of the algorithm by using different\nfinancial network structures. Overall, our findings suggest that the level of\ninterconnection and position of institutions in the financial network are\nimportant elements to measure systemic risk and identify SIFIs. Results show\nthat increasing the levels of out- and in-degree connections of an institution\ncan have a diverse impact on its systemic ranking. Additionally, on the\nempirical side, we investigate the factors which lead to the identification of\nGlobal Systemic Important Banks (G-SIB) by using a panel dataset of the largest\nbanks in each country. Our empirical results supports the main findings of the\ntheoretical model.\n"
    },
    {
        "paper_id": 1503.06354,
        "authors": "Francesca Biagini, Jean-Pierre Fouque, Marco Frittelli, Thilo\n  Meyer-Brandis",
        "title": "A Unified Approach to Systemic Risk Measures via Acceptance Sets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial crisis has dramatically demonstrated that the traditional\napproach to apply univariate monetary risk measures to single institutions does\nnot capture sufficiently the perilous systemic risk that is generated by the\ninterconnectedness of the system entities and the corresponding contagion\neffects. This has brought awareness of the urgent need for novel approaches\nthat capture systemic riskiness. The purpose of this paper is to specify a\ngeneral methodological framework that is flexible enough to cover a wide range\nof possibilities to design systemic risk measures via multi-dimensional\nacceptance sets and aggregation functions, and to study corresponding examples.\nExisting systemic risk measures can usually be interpreted as the minimal\ncapital needed to secure the system after aggregating individual risks. In\ncontrast, our approach also includes systemic risk measures that can be\ninterpreted as the minimal capital funds that secure the aggregated system by\nallocating capital to the single institutions before aggregating the individual\nrisks. This allows for a possible ranking of the institutions in terms of\nsystemic riskiness measured by the optimal allocations. Moreover, we also allow\nfor the possibility of allocating the funds according to the future state of\nthe system (random allocation). We provide conditions which ensure\nmonotonicity, convexity, or quasi-convexity properties of our systemic risk\nmeasures.\n"
    },
    {
        "paper_id": 1503.06704,
        "authors": "Jonathan Donier and Jean-Philippe Bouchaud",
        "title": "Why Do Markets Crash? Bitcoin Data Offers Unprecedented Insights",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0139356",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crashes have fascinated and baffled many canny observers of financial\nmarkets. In the strict orthodoxy of the efficient market theory, crashes must\nbe due to sudden changes of the fundamental valuation of assets. However,\ndetailed empirical studies suggest that large price jumps cannot be explained\nby news and are the result of endogenous feedback loops. Although plausible, a\nclear-cut empirical evidence for such a scenario is still lacking. Here we show\nhow crashes are conditioned by the market liquidity, for which we propose a new\nmeasure inspired by recent theories of market impact and based on readily\navailable, public information. Our results open the possibility of a dynamical\nevaluation of liquidity risk and early warning signs of market instabilities,\nand could lead to a quantitative description of the mechanisms leading to\nmarket crashes.\n"
    },
    {
        "paper_id": 1503.06926,
        "authors": "Semei Coronado, Omar Rojas, Rafael Romero-Meza and Francisco\n  Venegas-Martinez",
        "title": "A study of co-movements between USA and Latin American stock markets: a\n  cross-bicorrelations perspective",
        "comments": "Working paper, 9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we use the Brooks and Hinich cross-bicorrelation test in order\nto uncover nonlinear dependence periods between USA Standard and Poor 500\n(SP500), used as benchmark, and six Latin American stock markets indexes:\nMexico (BMV), Brazil (BOVESPA), Chile (IPSA), Colombia (COLCAP), Peru (IGBVL)\nand Argentina (MERVAL). We have found windows of nonlinear dependence and\nco-movement between the SP500 and the Latin American stock markets, some of\nwhich coincide with periods of crisis, giving way to a possible contagion or\ninterdependence interpretation.\n"
    },
    {
        "paper_id": 1503.07007,
        "authors": "Masaaki Fujii",
        "title": "Optimal Position Management for a Market Maker with Stochastic Price\n  Impacts",
        "comments": "Revised and extended. Convergence of the approximation scheme is\n  added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with an optimal position management problem for a market\nmaker who has to face uncertain customer order flows in an illiquid market,\nwhere the market maker's continuous trading incurs a stochastic linear price\nimpact. Although the execution timing is uncertain, the market maker can also\nask its OTC counterparties to transact a block trade without causing a direct\nprice impact. We adopt quite generic stochastic processes of the securities,\norder flows, price impacts, quadratic penalties as well as security\nborrowing/lending rates. The solution of the market maker's optimal\nposition-management strategy is represented by a stochastic\nHamilton-Jacobi-Bellman equation, which can be decomposed into three (one\nnon-linear and two linear) backward stochastic differential equations (BSDEs).\nWe provide the verification using the standard BSDE techniques for a single\nsecurity case. For a multiple-security case, we make use of the connection of\nthe non-linear BSDE to a special type of backward stochastic Riccati\ndifferential equation (BSRDE) whose properties were studied by Bismut(1976). We\nalso propose a perturbative approximation scheme for the resultant BSRDE, which\nonly requires a system of linear ODEs to be solved at each expansion order. Its\njustification and the convergence rate are also given.\n"
    },
    {
        "paper_id": 1503.07389,
        "authors": "Andreas Bjerre-Nielsen",
        "title": "Sorting in Networks: Adversity and Structure",
        "comments": "49 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  People choose friendships with people similar to themselves, i.e. they sort\nby resemblence. Economic studies have shown when sorting is optimal and\nconstitute an equilibrium, however, this presumes lack of beneficial\nspillovers. We investigate formation of economic and social networks where\nagents may form or cut ties. We combine a setup with link formation where\nagents have types that determine the value of a connection. We provide\nconditions for sorting in friendships, i.e. that agents tend to partner only\nwith those with those sufficiently similar to themselves. Conditions are\nprovided with and without beneficial spillovers from indirect connections. We\nshow that sorting may be suboptimal, yet a socially stable outcome, despite\notherwise obeying the conditions for sorting in Becker (1973). We analyze\npolicy tools to mitigate suboptimal sorting. Another feature is that agents\nwith higher value are more central in networks under certain conditions; a side\neffect is sorting by degree centrality under certain conditions. Finally we\nillustrate the limits to patterns of sorting and centrality.\n"
    },
    {
        "paper_id": 1503.07495,
        "authors": "Marcin Makowski, Edward W. Piotrowski, Jan S{\\l}adkowski, Jacek Syska",
        "title": "The intensity of the random variable intercept in the sector of negative\n  probabilities",
        "comments": "11 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider properties of the measurement intensity $\\rho$ of a random\nvariable for which the probability density function represented by the\ncorresponding Wigner function attains negative values on a part of the domain.\nWe consider a simple economic interpretation of this problem. This model is\nused to present the applicability of the method to the analysis of the negative\nprobability on markets where there are anomalies in the law of supply and\ndemand (e.g. Giffen's goods). It turns out that the new conditions to optimize\nthe intensity $\\rho$ require a new strategy. We propose a strategy (so-called\n$\\grave{a}$ rebours strategy) based on the fixed point method and explore its\neffectiveness.\n"
    },
    {
        "paper_id": 1503.07676,
        "authors": "Brett Hemenway and Sanjeev Khanna",
        "title": "Sensitivity and Computational Complexity in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern financial networks exhibit a high degree of interconnectedness and\ndetermining the causes of instability and contagion in financial networks is\nnecessary to inform policy and avoid future financial collapse. In the American\nEconomic Review, Elliott, Golub and Jackson proposed a simple model for\ncapturing the dynamics of complex financial networks. In Elliott, Golub and\nJackson's model, each institution in the network can buy underlying assets or\npercentage shares in other institutions (cross-holdings) and if any\ninstitution's value drops below a critical threshold value, its value suffers\nan additional failure cost.\n  This work shows that even in simple model put forward by Elliott, Golub and\nJackson there are fundamental barriers to understanding the risks that are\ninherent in a network. First, if institutions are not required to maintain a\nminimum amount of self-holdings, an $\\epsilon$ change in investments by a\nsingle institution can have an arbitrarily magnified influence on the net worth\nof the institutions in the system. This sensitivity result shows that if\ninstitutions have small self-holdings, then estimating the market value of an\ninstitution requires almost perfect information about every cross-holding in\nthe system. Second, we show that even if a regulator has complete information\nabout all cross-holdings in the system, it may be computationally intractable\nto even estimate the number of failures that could be caused by an arbitrarily\nsmall shock to the system. Together, these results show that any uncertainty in\nthe cross-holdings or values of the underlying assets can be amplified by the\nnetwork to arbitrarily large uncertainty in the valuations of institutions in\nthe network.\n"
    },
    {
        "paper_id": 1503.08013,
        "authors": "Liusha Yang, Romain Couillet, Matthew R. McKay",
        "title": "A Robust Statistics Approach to Minimum Variance Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/TSP.2015.2474298",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the design of portfolios under a minimum risk criterion. The\nperformance of the optimized portfolio relies on the accuracy of the estimated\ncovariance matrix of the portfolio asset returns. For large portfolios, the\nnumber of available market returns is often of similar order to the number of\nassets, so that the sample covariance matrix performs poorly as a covariance\nestimator. Additionally, financial market data often contain outliers which, if\nnot correctly handled, may further corrupt the covariance estimation. We\naddress these shortcomings by studying the performance of a hybrid covariance\nmatrix estimator based on Tyler's robust M-estimator and on Ledoit-Wolf's\nshrinkage estimator while assuming samples with heavy-tailed distribution.\nEmploying recent results from random matrix theory, we develop a consistent\nestimator of (a scaled version of) the realized portfolio risk, which is\nminimized by optimizing online the shrinkage intensity. Our portfolio\noptimization method is shown via simulations to outperform existing methods\nboth for synthetic and real market data.\n"
    },
    {
        "paper_id": 1503.08032,
        "authors": "Filippo Petroni and Maurizio Serva",
        "title": "Observability of Market Daily Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the price dynamics of 65 stocks from the Dow Jones Composite Average\nfrom 1973 until 2014. We show that it is possible to define a Daily Market\nVolatility $\\sigma(t)$ which is directly observable from data. This quantity is\nusually indirectly defined by $r(t)=\\sigma(t) \\omega(t)$ where the $r(t)$ are\nthe daily returns of the market index and the $\\omega(t)$ are i.i.d. random\nvariables with vanishing average and unitary variance. The relation\n$r(t)=\\sigma(t) \\omega(t)$ alone is unable to give an operative definition of\nthe index volatility, which remains unobservable. On the contrary, we show that\nusing the whole information available in the market, the index volatility can\nbe operatively defined and detected.\n"
    },
    {
        "paper_id": 1503.08082,
        "authors": "Antoine Jacquier and Patrick Roome",
        "title": "Black-Scholes in a CEV random environment",
        "comments": "25 pages. Typos corrected, introduction updated, and title shortened",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical (It\\^o diffusions) stochastic volatility models are not able to\ncapture the steepness of small-maturity implied volatility smiles. Jumps, in\nparticular exponential L\\'evy and affine models, which exhibit small-maturity\nexploding smiles, have historically been proposed to remedy this (see\n\\cite{Tank} for an overview), and more recently rough volatility models\n\\cite{AlosLeon, Fukasawa}. We suggest here a different route, randomising the\nBlack-Scholes variance by a CEV-generated distribution, which allows us to\nmodulate the rate of explosion (through the CEV exponent) of the implied\nvolatility for small maturities. The range of rates includes behaviours similar\nto exponential L\\'evy models and fractional stochastic volatility models.\n"
    },
    {
        "paper_id": 1503.08119,
        "authors": "Raul Merino, Josep Vives",
        "title": "About the decomposition of pricing formulas under stochastic volatility\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain a decomposition of the call option price for a very general\nstochastic volatility diffusion model extending the decomposition obtained by\nE. Al\\`os in [2] for the Heston model. We realize that a new term arises when\nthe stock price does not follow an exponential model. The techniques used are\nnon anticipative. In particular, we see also that equivalent results can be\nobtained using Functional It\\^o Calculus. Using the same generalizing ideas we\nalso extend to non exponential models the alternative call option price\ndecompostion formula obtained in [1] and [3] written in terms of the Malliavin\nderivative of the volatility process. Finally, we give a general expression for\nthe derivative of the implied volatility under both, the anticipative and the\nnon anticipative case.\n"
    },
    {
        "paper_id": 1503.08123,
        "authors": "Tobias Fissler, Johanna F. Ziegel",
        "title": "Higher order elicitability and Osband's principle",
        "comments": "32 pages",
        "journal-ref": "The Annals of Statistics 2016, Vol. 44, No. 4, 1680-1707",
        "doi": "10.1214/16-AOS1439",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A statistical functional, such as the mean or the median, is called\nelicitable if there is a scoring function or loss function such that the\ncorrect forecast of the functional is the unique minimizer of the expected\nscore. Such scoring functions are called strictly consistent for the\nfunctional. The elicitability of a functional opens the possibility to compare\ncompeting forecasts and to rank them in terms of their realized scores. In this\npaper, we explore the notion of elicitability for multi-dimensional functionals\nand give both necessary and sufficient conditions for strictly consistent\nscoring functions. We cover the case of functionals with elicitable components,\nbut we also show that one-dimensional functionals that are not elicitable can\nbe a component of a higher order elicitable functional. In the case of the\nvariance this is a known result. However, an important result of this paper is\nthat spectral risk measures with a spectral measure with finite support are\njointly elicitable if one adds the `correct' quantiles. A direct consequence of\napplied interest is that the pair (Value at Risk, Expected Shortfall) is\njointly elicitable under mild conditions that are usually fulfilled in risk\nmanagement applications.\n"
    },
    {
        "paper_id": 1503.08441,
        "authors": "Andrey Korotayev and Julia Zinkina",
        "title": "East africa in the Malthusian trap? A statistical analysis of financial,\n  economic, and demographic indicators",
        "comments": "30 pages, 29 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  A statistical analysis of financial, economic, and demographic indicators\nperformed by the authors demonstrates (1) that the main countries of East\nAfrica (Uganda, Kenya, and Tanzania) have not escaped the Malthusian Trap yet;\n(2) that this countries are not likely to follow the \"North African path\" and\nto achieve this escape before they achieve serious successes in their fertility\ntransition; (3) that East Africa is unlikely to achieve this escape if it does\nnot follow the \"Bangladeshi path\" and does not achieve really substantial\nfertility declines in the foreseeable future, which would imply the\nintroduction of compulsory universal secondary education, serious family\nplanning programs of the Rwandan type, and the rise of legal age of marriage\nwith parental consent. Such measures should of course be accompanied by the\nsubstantial increases in the agricultural labor productivity and the decline of\nthe percentage of population employed in agriculture.\n"
    },
    {
        "paper_id": 1503.08465,
        "authors": "Noemi Nava, T. Di Matteo, Tomaso Aste",
        "title": "Anomalous volatility scaling in high frequency financial data",
        "comments": "25 pages, 11 figure, 5 tables",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2015.12.022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility of intra-day stock market indices computed at various time\nhorizons exhibits a scaling behaviour that differs from what would be expected\nfrom fractional Brownian motion (fBm). We investigate this anomalous scaling by\nusing empirical mode decomposition (EMD), a method which separates time series\ninto a set of cyclical components at different time-scales. By applying the EMD\nto fBm, we retrieve a scaling law that relates the variance of the components\nto a power law of the oscillating period. In contrast, when analysing 22\ndifferent stock market indices, we observe deviations from the fBm and Brownian\nmotion scaling behaviour. We discuss and quantify these deviations, associating\nthem to the characteristics of financial markets, with larger deviations\ncorresponding to less developed markets.\n"
    },
    {
        "paper_id": 1503.08586,
        "authors": "Chuancun Yin, Dan Zhu",
        "title": "New class of distortion risk measures and their tail asymptotics with\n  emphasis on VaR",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distortion risk measures are extensively used in finance and insurance\napplications because of their appealing properties. We present three methods to\nconstruct new class of distortion functions and measures. The approach involves\nthe composting methods, the mixing methods and the approach that based on the\ntheory of copula. Subadditivity is an important property when aggregating risks\nin order to preserve the benefits of diversification. However, Value at risk\n(VaR), as the most well-known example of distortion risk measure is not always\nglobally subadditive, except of elliptically distributed risks. In this paper,\ninstead of study subadditivity we investigate the tail subadditivity for VaR\nand other distortion risk measures. In particular, we demonstrate that VaR is\ntail subadditive for the case where the support of risk is bounded. Various\nexamples are also presented to illustrate the results.\n"
    },
    {
        "paper_id": 1503.08589,
        "authors": "Takuji Arai, Yuto Imai and Ryoichi Suzuki",
        "title": "Local risk-minimization for Barndorff-Nielsen and Shephard models",
        "comments": "39 pages, 2figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain explicit representations of locally risk-minimizing strategies of\ncall and put options for the Barndorff-Nielsen and Shephard models, which are\nOrnstein--Uhlenbeck-type stochastic volatility models. Using Malliavin calculus\nfor Levy processes, Arai and Suzuki (2015) obtained a formula for locally\nrisk-minimizing strategies for Levy markets under many additional conditions.\nSupposing mild conditions, we make sure that the Barndorff-Nielsen and Shephard\nmodels satisfy all the conditions imposed in Arai and Suzuki (2015). Among\nothers, we investigate the Malliavin differentiability of the density of the\nminimal martingale measure. Moreover, some numerical experiments for locally\nrisk-minimizing strategies are introduced.\n"
    },
    {
        "paper_id": 1503.08628,
        "authors": "Qian Lin",
        "title": "Dynamic indifference pricing via the G-expectation",
        "comments": "This paper has been withdrawn by the authors due to some error in\n  some statements",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the dynamic indifference pricing with ambiguity preferences. For\nthis, we introduce the dynamic expected utility with ambiguity via the\nnonlinear expectation--G-expectation, introduced by Peng (2007). We also study\nthe risk aversion and certainty equivalent for the agents with ambiguity. We\nobtain the dynamic consistency of indifference pricing with ambiguity\npreferences. Finally, we obtain comparative statics.\n"
    },
    {
        "paper_id": 1503.08785,
        "authors": "Elad Oster and Alexander Feigel",
        "title": "Prices of Options as Opinion Dynamics of the Market Players with Limited\n  Social Influence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamics of market prices is described as the evolution of opinions in\nthe trading community regarding future market behavior. The price then is a\nfunction of the voting process of the market players in favor to raise or\nreduce the value of a stock. The model presented in this paper is suited for\npricing of options and was verified against real market data. The model allows\nderiving the parameters of market players from available real market data,\nespecially maximum possible correlation (herding) and anti-correlation between\nthe players' opinions. The deviations of market prices from those predicted by\nthe Black-Scholes model, such as smile and skew implied volatilities, are\ninterpreted as the current values and limits of social influence of the market\nplayers, respectively. To the best of our knowledge, this is the first work\nthat discriminates skew and smile phenomena. Our approach unifies and develops\na further connection between trading, voters' model, and statistical physics\nanalogies of opinion dynamics.\n"
    },
    {
        "paper_id": 1503.089,
        "authors": "Wei He, Yeneng Sun",
        "title": "Dynamic Games with Almost Perfect Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to solve two fundamental problems on finite or infinite\nhorizon dynamic games with perfect or almost perfect information. Under some\nmild conditions, we prove (1) the existence of subgame-perfect equilibria in\ngeneral dynamic games with almost perfect information, and (2) the existence of\npure-strategy subgame-perfect equilibria in perfect-information dynamic games\nwith uncertainty. Our results go beyond previous works on continuous dynamic\ngames in the sense that public randomization and the continuity requirement on\nthe state variables are not needed. As an illustrative application, a dynamic\nstochastic oligopoly market with intertemporally dependent payoffs is\nconsidered.\n"
    },
    {
        "paper_id": 1503.08961,
        "authors": "Huiwen Yan, Zhou Yang, Fahuai Yi, Gechun Liang",
        "title": "Dynkin Game of Convertible Bonds and Their Optimal Strategy",
        "comments": "28 pages, 9 figures in Journal of Mathematical Analysis and\n  Application, 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the valuation and optimal strategy of convertible bonds as\na Dynkin game by using the reflected backward stochastic differential equation\nmethod and the variational inequality method. We first reduce such a Dynkin\ngame to an optimal stopping time problem with state constraint, and then in a\nMarkovian setting, we investigate the optimal strategy by analyzing the\nproperties of the corresponding free boundary, including its position,\nasymptotics, monotonicity and regularity. We identify situations when call\nprecedes conversion, and vice versa. Moreover, we show that the irregular\npayoff results in the possibly non-monotonic conversion boundary. Surprisingly,\nthe price of the convertible bond is not necessarily monotonic in time: it may\neven increase when time approaches maturity.\n"
    },
    {
        "paper_id": 1503.08969,
        "authors": "Huiwen Yan, Gechun Liang, Zhou Yang",
        "title": "Indifference Pricing and Hedging in a Multiple-Priors Model with Trading\n  Constraints",
        "comments": "28 pages in Science China Mathematics, 2015",
        "journal-ref": null,
        "doi": "10.1007/s11425-014-4885-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers utility indifference valuation of derivatives under\nmodel uncertainty and trading constraints, where the utility is formulated as\nan additive stochastic differential utility of both intertemporal consumption\nand terminal wealth, and the uncertain prospects are ranked according to a\nmultiple-priors model of Chen and Epstein (2002). The price is determined by\ntwo optimal stochastic control problems (mixed with optimal stopping time in\nthe case of American option) of forward-backward stochastic differential\nequations. By means of backward stochastic differential equation and partial\ndifferential equation methods, we show that both bid and ask prices are closely\nrelated to the Black-Scholes risk-neutral price with modified dividend rates.\nThe two prices will actually coincide with each other if there is no trading\nconstraint or the model uncertainty disappears. Finally, two applications to\nEuropean option and American option are discussed.\n"
    },
    {
        "paper_id": 1503.09004,
        "authors": "Desislava Chetalova, Marcel Wollschl\\\"ager and Rudi Sch\\\"afer",
        "title": "Dependence structure of market states",
        "comments": "19 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2015/08/P08012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the dependence structure of market states by estimating empirical\npairwise copulas of daily stock returns. We consider both original returns,\nwhich exhibit time-varying trends and volatilities, as well as locally\nnormalized ones, where the non-stationarity has been removed. The empirical\npairwise copula for each state is compared with a bivariate K-copula. This\ncopula arises from a recently introduced random matrix model, in which\nnon-stationary correlations between returns are modeled by an ensemble of\nrandom matrices. The comparison reveals overall good agreement between\nempirical and analytical copulas, especially for locally normalized returns.\nStill, there are some deviations in the tails. Furthermore, we find an\nasymmetry in the dependence structure of market states. The empirical pairwise\ncopulas exhibit a stronger lower tail dependence, particularly in times of\ncrisis.\n"
    },
    {
        "paper_id": 1503.09008,
        "authors": "W. Mudzimbabwe, Lubin G. Vulkov",
        "title": "IMEX schemes for a Parabolic-ODE system of European Options with\n  Liquidity Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The coupled system, where one is a degenerate parabolic equation and the\nother has not a diffusion term arises in the modeling of European options with\nliquidity shocks. Two implicit-explicit (IMEX) schemes that preserve the\npositivity of the differential problem solution are constructed and analyzed.\nNumerical experiments confirm the theoretical results and illustrate the high\naccuracy and efficiency of the schemes in combination with Richardson\nextrapolation\n"
    },
    {
        "paper_id": 1504.00276,
        "authors": "Hyungbin Park",
        "title": "The Martin Integral Representation of Markovian Pricing Kernels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this article is to describe all possible beliefs of market\nparticipants on objective measures under Markovian environments when a\nrisk-neutral measure is given. To achieve this, we employ the Martin integral\nrepresentation of Markovian pricing kernels. Then, we offer economic and\nfinancial implications of this representation. This representation is useful to\nanalyze the long-term behavior of the state variable in the market. The Ross\nrecovery theorem and the long-term behavior of cash flows are discussed as\napplications.\n"
    },
    {
        "paper_id": 1504.0031,
        "authors": "Erhan Bayraktar, Xiang Yu",
        "title": "Optimal Investment with Random Endowments and Transaction Costs: Duality\n  Theory and Shadow Prices",
        "comments": "Final version. To appear in Mathematics and Financial Economics.\n  Keywords: Proportional Transaction Costs, Unbounded Random Endowments,\n  Acceptable Portfolios, Super-hedging Theorem, Utility Maximization, Shadow\n  Prices, Convex Duality",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the utility maximization on the terminal wealth with\nrandom endowments and proportional transaction costs. To deal with unbounded\nrandom payoffs from some illiquid claims, we propose to work with the\nacceptable portfolios defined via the consistent price system (CPS) such that\nthe liquidation value processes stay above some stochastic thresholds. In the\nmarket consisting of one riskless bond and one risky asset, we obtain a type of\nsuper-hedging result. Based on this characterization of the primal space, the\nexistence and uniqueness of the optimal solution for the utility maximization\nproblem are established using the duality approach. As an important application\nof the duality theorem, we provide some sufficient conditions for the existence\nof a shadow price process with random endowments in a generalized form as well\nas in the usual sense using acceptable portfolios.\n"
    },
    {
        "paper_id": 1504.00334,
        "authors": "Rene Carmona, Yi Ma, Sergey Nadtochiy",
        "title": "Simulation of Implied Volatility Surfaces via Tangent Levy Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we implement and test two types of market-based models for\nEuropean-type options, based on the tangent Levy models proposed recently by R.\nCarmona and S. Nadtochiy. As a result, we obtain a method for generating Monte\nCarlo samples of future paths of implied volatility surfaces. These paths and\nthe surfaces themselves are free of arbitrage, and are constructed in a way\nthat is consistent with the past and present values of implied volatility. We\nuse a real market data to estimate the parameters of these models and conduct\nan empirical study, to compare the performance of market-based models with the\nperformance of classical stochastic volatility models. We choose the problem of\nminimal-variance portfolio choice as a measure of model performance and compare\nthe two tangent Levy models to SABR model. Our study demonstrates that the\ntangent Levy models do a much better job at finding a portfolio with smallest\nvariance, their predictions for the variance are more reliable, and the\nportfolio weights are more stable. To the best of our knowledge, this is the\nfirst example of empirical analysis that provides a convincing evidence of the\nsuperior performance of the market-based models for European options using real\nmarket data.\n"
    },
    {
        "paper_id": 1504.00428,
        "authors": "Alexander Badran and Beniamin Goldys",
        "title": "A Market Model for VIX Futures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new modelling approach that directly prescribes dynamics to the term\nstructure of VIX futures is proposed in this paper. The approach is motivated\nby the tractability enjoyed by models that directly prescribe dynamics to the\nVIX, practices observed in interest-rate modelling, and the desire to develop a\nplatform to better understand VIX option implied volatilities. The main\ncontribution of the paper is the derivation of necessary conditions for there\nto be no arbitrage between the joint market of VIX and equity derivatives. The\narbitrage conditions are analogous to the well-known HJM drift restrictions in\ninterest-rate modelling. The restrictions also address a fundamental open\nproblem related to an existing modelling approach, in which the dynamics of the\nVIX are specified directly. The paper is concluded with an application of the\nmain result, which demonstrates that when modelling VIX futures directly, the\ndrift and diffusion of the corresponding stochastic volatility model must be\nrestricted to preclude arbitrage.\n"
    },
    {
        "paper_id": 1504.00579,
        "authors": "Frank Kelly and Elena Yudovina",
        "title": "A Markov model of a limit order book: thresholds, recurrence, and\n  trading strategies",
        "comments": "Revision of the submitted version: correcting an error (caught by Jan\n  Swart) in the statement of Proposition 4.1 (1), propagating to Theorem 2.1\n  (1)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a tractable model of a limit order book on short time scales,\nwhere the dynamics are driven by stochastic fluctuations between supply and\ndemand. We establish the existence of a limiting distribution for the highest\nbid, and for the lowest ask, where the limiting distributions are confined\nbetween two thresholds. We make extensive use of fluid limits in order to\nestablish recurrence properties of the model. We use the model to analyze\nvarious high-frequency trading strategies, and comment on the Nash equilibria\nthat emerge between high-frequency traders when a market in continuous time is\nreplaced by frequent batch auctions.\n"
    },
    {
        "paper_id": 1504.0059,
        "authors": "Assaf Almog, Ferry Besamusca, Mel MacMahon, Diego Garlaschelli",
        "title": "Mesoscopic Community Structure of Financial Markets Revealed by Price\n  and Sign Fluctuations",
        "comments": "15 pages, 7 figures",
        "journal-ref": "PLoS ONE 10: 7. e0133679 (2015)",
        "doi": "10.1371/journal.pone.0133679",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The mesoscopic organization of complex systems, from financial markets to the\nbrain, is an intermediate between the microscopic dynamics of individual units\n(stocks or neurons, in the mentioned cases), and the macroscopic dynamics of\nthe system as a whole. The organization is determined by \"communities\" of units\nwhose dynamics, represented by time series of activity, is more strongly\ncorrelated internally than with the rest of the system. Recent studies have\nshown that the binary projections of various financial and neural time series\nexhibit nontrivial dynamical features that resemble those of the original data.\nThis implies that a significant piece of information is encoded into the binary\nprojection (i.e. the sign) of such increments. Here, we explore whether the\nbinary signatures of multiple time series can replicate the same complex\ncommunity organization of the financial market, as the original weighted time\nseries. We adopt a method that has been specifically designed to detect\ncommunities from cross-correlation matrices of time series data. Our analysis\nshows that the simpler binary representation leads to a community structure\nthat is almost identical with that obtained using the full weighted\nrepresentation. These results confirm that binary projections of financial time\nseries contain significant structural information.\n"
    },
    {
        "paper_id": 1504.0064,
        "authors": "Freddy Delbaen",
        "title": "Remark on the Paper \"Entropic Value-at-Risk: A New Coherent Risk\n  Measure\" by Amir Ahmadi-Javid, J. Opt. Theory and Appl., 155\n  (2001),1105--1123",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper mentioned in the title introduces the entropic value at risk. I\ngive some extra comments and using the general theory make a relation with some\ncommonotone risk measures.\n"
    },
    {
        "paper_id": 1504.01022,
        "authors": "Karel in 't Hout, Jari Toivanen",
        "title": "Application of Operator Splitting Methods in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial derivatives pricing aims to find the fair value of a financial\ncontract on an underlying asset. Here we consider option pricing in the partial\ndifferential equations framework. The contemporary models lead to\none-dimensional or multidimensional parabolic problems of the\nconvection-diffusion type and generalizations thereof. An overview of various\noperator splitting methods is presented for the efficient numerical solution of\nthese problems.\n  Splitting schemes of the Alternating Direction Implicit (ADI) type are\ndiscussed for multidimensional problems, e.g. given by stochastic volatility\n(SV) models. For jump models Implicit-Explicit (IMEX) methods are considered\nwhich efficiently treat the nonlocal jump operator. For American options an\neasy-to-implement operator splitting method is described for the resulting\nlinear complementarity problems.\n  Numerical experiments are presented to illustrate the actual stability and\nconvergence of the splitting schemes. Here European and American put options\nare considered under four asset price models: the classical Black-Scholes\nmodel, the Merton jump-diffusion model, the Heston SV model, and the Bates SV\nmodel with jumps.\n"
    },
    {
        "paper_id": 1504.01026,
        "authors": "Alexander Vervuurt and Ioannis Karatzas",
        "title": "Diversity-Weighted Portfolios with Negative Parameter",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": "10.1007/s10436-015-0263-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a negative-parameter variant of the diversity-weighted portfolio\nstudied by Fernholz, Karatzas, and Kardaras (Finance Stoch 9(1):1-27, 2005),\nwhich invests in each company a fraction of wealth inversely proportional to\nthe company's market weight (the ratio of its capitalization to that of the\nentire market). We show that this strategy outperforms the market with\nprobability one, under a non-degeneracy assumption on the volatility structure\nand the assumption that the market weights admit a positive lower bound.\nSeveral modifications of this portfolio, which outperform the market under\nmilder versions of this \"no-failure\" condition, are put forward, one of which\nis rank-based. An empirical study suggests that such strategies as studied here\nhave indeed the potential to outperform the market and to be preferable\ninvestment opportunities, even under realistic proportional transaction costs.\n"
    },
    {
        "paper_id": 1504.0115,
        "authors": "T Kruse, A Popier (LMM)",
        "title": "Minimal supersolutions for BSDEs with singular terminal condition and\n  application to optimal position targeting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the existence of a minimal supersolution for backward stochastic\ndifferential equations when the terminal data can take the value +$\\infty$ with\npositive probability. We deal with equations on a general filtered probability\nspace and with generators satisfying a general monotonicity assumption. With\nthis minimal supersolution we then solve an optimal stochastic control problem\nrelated to portfolio liquidation problems. We generalize the existing results\nin three directions: firstly there is no assumption on the underlying\nfiltration (except completeness and quasi-left continuity), secondly we relax\nthe terminal liquidation constraint and finally the time horizon can be random.\n"
    },
    {
        "paper_id": 1504.01152,
        "authors": "Ying Hu (IRMAR), Hanqing Jin, Xun Yu Zhou",
        "title": "Time-Inconsistent Stochastic Linear--Quadratic Control: Characterization\n  and Uniqueness of Equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we continue our study on a general time-inconsistent\nstochastic linear--quadratic (LQ) control problem originally formulated in [6].\nWe derive a necessary and sufficient condition for equilibrium controls via a\nflow of forward--backward stochastic differential equations. When the state is\none dimensional and the coefficients in the problem are all deterministic, we\nprove that the explicit equilibrium control constructed in \\cite{HJZ} is indeed\nunique. Our proof is based on the derived equivalent condition for equilibria\nas well as a stochastic version of the Lebesgue differentiation theorem.\nFinally, we show that the equilibrium strategy is unique for a mean--variance\nportfolio selection model in a complete financial market where the risk-free\nrate is a deterministic function of time but all the other market parameters\nare possibly stochastic processes.\n"
    },
    {
        "paper_id": 1504.01542,
        "authors": "Akihiko Inoue, Shingo Moriuchi and Yusuke Nakamura",
        "title": "A Vasicek-type short rate model with memory effect",
        "comments": "16 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a Vasicek-type short rate model which has two additional\nparameters representing memory effect. This model presents better results in\nyield curve fitting than the classical Vasicek model. We derive closed-form\nexpressions for the prices of bonds and bond options. Though the model is\nnon-Markov, there exists an associated Markov process which allows one to apply\nusual numerical methods to the model. We derive analogs of an affine term\nstructure and term structure equations for the model, and, using them, we\npresent a numerical method to evaluate contingent claims.\n"
    },
    {
        "paper_id": 1504.01811,
        "authors": "Jun-Jie Chen, Lei Tan, Bo Zheng",
        "title": "Agent-based model with multi-level herding for complex financial systems",
        "comments": "14 pages, 5 figures",
        "journal-ref": "Sci. Rep. 5, 8399(2015)",
        "doi": "10.1038/srep08399",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In complex financial systems, the sector structure and volatility clustering\nare respectively important features of the spatial and temporal correlations.\nHowever, the microscopic generation mechanism of the sector structure is not\nyet understood. Especially, how to produce these two features in one model\nremains challenging. We introduce a novel interaction mechanism, i.e., the\nmulti-level herding, in constructing an agent-based model to investigate the\nsector structure combined with volatility clustering. According to the previous\nmarket performance, agents trade in groups, and their herding behavior\ncomprises the herding at stock, sector and market levels. Further, we propose\nmethods to determine the key model parameters from historical market data,\nrather than from statistical fitting of the results. From the simulation, we\nobtain the sector structure and volatility clustering, as well as the\neigenvalue distribution of the cross-correlation matrix, for the New York and\nHong Kong stock exchanges. These properties are in agreement with the empirical\nones. Our results quantitatively reveal that the multi-level herding is the\nmicroscopic generation mechanism of the sector structure, and provide new\ninsight into the spatio-temporal interactions in financial systems at the\nmicroscopic level.\n"
    },
    {
        "paper_id": 1504.01857,
        "authors": "Marco Bardoscia, Stefano Battiston, Fabio Caccioli, Guido Caldarelli",
        "title": "DebtRank: A microscopic foundation for shock propagation",
        "comments": "10 pages, 2 figures",
        "journal-ref": "PLoS ONE 10(6): e0130406 (2015)",
        "doi": "10.1371/journal.pone.0130406",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The DebtRank algorithm has been increasingly investigated as a method to\nestimate the impact of shocks in financial networks, as it overcomes the\nlimitations of the traditional default-cascade approaches. Here we formulate a\ndynamical \"microscopic\" theory of instability for financial networks by\niterating balance sheet identities of individual banks and by assuming a simple\nrule for the transfer of shocks from borrowers to lenders. By doing so, we\ngeneralise the DebtRank formulation, both providing an interpretation of the\neffective dynamics in terms of basic accounting principles and preventing the\nunderestimation of losses on certain network topologies. Depending on the\nstructure of the interbank leverage matrix the dynamics is either stable, in\nwhich case the asymptotic state can be computed analytically, or unstable,\nmeaning that at least one bank will default. We apply this framework to a\ndataset of the top listed European banks in the period 2008 - 2013. We find\nthat network effects can generate an amplification of exogenous shocks of a\nfactor ranging between three (in normal periods) and six (during the crisis)\nwhen we stress the system with a 0.5% shock on external (i.e. non-interbank)\nassets for all banks.\n"
    },
    {
        "paper_id": 1504.0228,
        "authors": "Stanislav S. Borysov and Yasser Roudi and Alexander V. Balatsky",
        "title": "U.S. stock market interaction network as learned by the Boltzmann\n  Machine",
        "comments": "15 pages, 17 figures, 1 table",
        "journal-ref": "Eur. Phys. J. B (2015) 88: 321",
        "doi": "10.1140/epjb/e2015-60282-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study historical dynamics of joint equilibrium distribution of stock\nreturns in the U.S. stock market using the Boltzmann distribution model being\nparametrized by external fields and pairwise couplings. Within Boltzmann\nlearning framework for statistical inference, we analyze historical behavior of\nthe parameters inferred using exact and approximate learning algorithms. Since\nthe model and inference methods require use of binary variables, effect of this\nmapping of continuous returns to the discrete domain is studied. The presented\nanalysis shows that binarization preserves market correlation structure.\nProperties of distributions of external fields and couplings as well as\nindustry sector clustering structure are studied for different historical dates\nand moving window sizes. We found that a heavy positive tail in the\ndistribution of couplings is responsible for the sparse market clustering\nstructure. We also show that discrepancies between the model parameters might\nbe used as a precursor of financial instabilities.\n"
    },
    {
        "paper_id": 1504.02361,
        "authors": "Bin Shen, Jiang Zhang, Qiuhua Zheng",
        "title": "Exploring multi-layer flow network of international trade based on flow\n  distances",
        "comments": "21 pages, 9 figures",
        "journal-ref": "PLoS ONE 2015, 10(11): e0142936",
        "doi": "10.1371/journal.pone.0142936",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the approach of flow distances, the international trade flow system\nis studied from the perspective of multi-layer flow network. A model of\nmulti-layer flow network is proposed for modelling and analyzing multiple types\nof flows in flow systems. Then, flow distances are introduced, and symmetric\nminimum flow distance is presented. Subsequently, we discuss the establishment\nof the multi-layer flow networks of international trade from two coupled\nviewpoints, i.e., the viewpoint of commodity flow and that of money flow. Thus,\nthe multi-layer flow networks of international trade is explored. First,\ntrading \"trophic levels\" are adopted to depict positions that economies\noccupied in the flow network. We find that the distributions of trading\n\"trophic levels\" have the similar clustering pattern for different types of\ncommodity, and there are some regularities between money flow network and\ncommodity flow network. Second, we find that active and competitive countries\ntrade a wide spectrum of products, while inactive and underdeveloped countries\ntrade a limited variety of products. Besides, some abnormal countries import\nmany types of goods, which the vast majority of countries do not need to\nimport. It may indicate an abnormal economic status. Third, harmonic node\ncentrality is proposed and we find the phenomenon of centrality stratification.\nIt means that competitive countries tend to occupy the central positions in the\ntrading of a large variety of commodities, while underdeveloped countries\nlikely in the peripheral positions in the trading of their limited varieties of\nproducts. Fourth, we find that manufactured products have significant larger\nmean first-passage flow distances from the source to the sink than that of\nprimary products.\n"
    },
    {
        "paper_id": 1504.02435,
        "authors": "Xi-Yuan Qian (ECUST), Ya-Min Liu (ECUST), Zhi-Qiang Jiang (ECUST),\n  Boris Podobnik (BU and ZSEM), Wei-Xing Zhou (ECUST), H. Eugene Stanley (BU)",
        "title": "Detrended partial cross-correlation analysis of two nonstationary time\n  series influenced by common external forces",
        "comments": "7 Latex pages including 3 figures",
        "journal-ref": "Physical Review E 91 (6), 062816 (2015)",
        "doi": "10.1103/PhysRevE.91.062816",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When common factors strongly influence two power-law cross-correlated time\nseries recorded in complex natural or social systems, using classic detrended\ncross-correlation analysis (DCCA) without considering these common factors will\nbias the results. We use detrended partial cross-correlation analysis (DPXA) to\nuncover the intrinsic power-law cross-correlations between two simultaneously\nrecorded time series in the presence of nonstationarity after removing the\neffects of other time series acting as common forces. The DPXA method is a\ngeneralization of the detrended cross-correlation analysis that takes into\naccount partial correlation analysis. We demonstrate the method by using\nbivariate fractional Brownian motions contaminated with a fractional Brownian\nmotion. We find that the DPXA is able to recover the analytical cross Hurst\nindices, and thus the multi-scale DPXA coefficients are a viable alternative to\nthe conventional cross-correlation coefficient. We demonstrate the advantage of\nthe DPXA coefficients over the DCCA coefficients by analyzing contaminated\nbivariate fractional Brownian motions. We calculate the DPXA coefficients and\nuse them to extract the intrinsic cross-correlation between crude oil and gold\nfutures by taking into consideration the impact of the US dollar index. We\ndevelop the multifractal DPXA (MF-DPXA) method in order to generalize the DPXA\nmethod and investigate multifractal time series. We analyze multifractal\nbinomial measures masked with strong white noises and find that the MF-DPXA\nmethod quantifies the hidden multifractal nature while the MF-DCCA method\nfails.\n"
    },
    {
        "paper_id": 1504.02511,
        "authors": "Manuel G. Ch\\'avez-Angeles and Patricia S. S\\'anchez-Medina",
        "title": "Application of the war of attrition game to the analysis of intellectual\n  property disputes",
        "comments": null,
        "journal-ref": "International Journal of Game Theory and Technology (IJGTT),\n  Vol.3, No.1, March 2015",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  In many developing countries intellectual property infringement and the\ncommerce of pirate goods is an entrepreneurial activity. Digital piracy is very\noften the only media for having access to music, cinema, books and software. At\nthe same time, bio-prospecting and infringement of indigenous knowledge rights\nby international consortiums is usual in places with high biodiversity. In\nthese arenas transnational actors interact with local communities. Accusations\nof piracy often go both ways. This article analyzes the case of southeast\nMexico. Using a war of attrition game theory model it explains different\nsituations of intellectual property rights piracy and protection. It analyzes\ndifferent levels of interaction and institutional settings from the global to\nthe very local. The article proposes free IP zones as a solution of IP\ndisputes. The formation of technological local clusters through Free\nIntellectual Property Zones (FIPZ) would allow firms to copy and share de facto\npublic domain content for developing new products inside the FIPZ. Enforcement\nof intellectual property could be pursuit outside of the FIPZ. FIPZ are\nenvisioned as a new type of a sui generis intellectual property regime.\n"
    },
    {
        "paper_id": 1504.02516,
        "authors": "Gaurab Aryal and Dong-Hyuk Kim",
        "title": "Empirical Relevance of Ambiguity in First Price Auction Models",
        "comments": "43 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the identification and estimation of first-price auction models\nwhere bidders have ambiguity about the valuation distribution and their\npreferences are represented by maxmin expected utility. When entry is\nexogenous, the distribution and ambiguity structure are nonparametrically\nidentified, separately from risk aversion (CRRA). We propose a flexible\nBayesian method based on Bernstein polynomials. Monte Carlo experiments show\nthat our method estimates parameters precisely, and chooses reserve prices with\n(nearly) optimal revenues, whether there is ambiguity or not. Furthermore, if\nthe model is misspecified -- incorrectly assuming no ambiguity among bidders --\nit may induce estimation bias with a substantial revenue loss.\n"
    },
    {
        "paper_id": 1504.02734,
        "authors": "Julio Backhoff Veraguas and Francisco Silva",
        "title": "Sensitivity analysis for expected utility maximization in incomplete\n  Brownian market models",
        "comments": "Improves and highlights the discussion on weak vs strong\n  perturbations",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the issue of sensitivity with respect to model parameters for the\nproblem of utility maximization from final wealth in an incomplete Samuelson\nmodel and mainly, but not exclusively, for utility functions of positive\npower-type. The method consists in moving the parameters through change of\nmeasure, which we call a weak perturbation, decoupling the usual wealth\nequation from the varying parameters. By rewriting the maximization problem in\nterms of a convex-analytical support function of a weakly-compact set,\ncrucially leveraging on the work of Backhoff and Fontbona (SIFIN 2016), the\nprevious formulation let us prove the Hadamard directional differentiability of\nthe value function w.r.t. the drift and interest rate parameters, as well as\nfor volatility matrices under a stability condition on their Kernel, and derive\nexplicit expressions for the directional derivatives. We contrast our proposed\nweak perturbations against what we call strong perturbations, where the wealth\nequation is directly influenced by the changing parameters. Contrary to\nconventional wisdom, we find that both points of view generally yield different\nsensitivities unless e.g. if initial parameters and their perturbations are\ndeterministic.\n"
    },
    {
        "paper_id": 1504.02896,
        "authors": "Marco Bianchetti, Sergei Kucherenko, Stefano Scoleri",
        "title": "Pricing and Risk Management with High-Dimensional Quasi Monte Carlo and\n  Global Sensitivity Analysis",
        "comments": "43 pages, 21 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review and apply Quasi Monte Carlo (QMC) and Global Sensitivity Analysis\n(GSA) techniques to pricing and risk management (greeks) of representative\nfinancial instruments of increasing complexity. We compare QMC vs standard\nMonte Carlo (MC) results in great detail, using high-dimensional Sobol' low\ndiscrepancy sequences, different discretization methods, and specific analyses\nof convergence, performance, speed up, stability, and error optimization for\nfinite differences greeks. We find that our QMC outperforms MC in most cases,\nincluding the highest-dimensional simulations and greeks calculations, showing\nfaster and more stable convergence to exact or almost exact results. Using GSA,\nwe are able to fully explain our findings in terms of reduced effective\ndimension of our QMC simulation, allowed in most cases, but not always, by\nBrownian bridge discretization. We conclude that, beyond pricing, QMC is a very\npromising technique also for computing risk figures, greeks in particular, as\nit allows to reduce the computational effort of high-dimensional Monte Carlo\nsimulations typical of modern risk management.\n"
    },
    {
        "paper_id": 1504.02956,
        "authors": "Francesco Corradi, Andrea Zaccaria, and Luciano Pietronero",
        "title": "Liquidity crises on different time scales",
        "comments": "23 pages, 10 figures",
        "journal-ref": "Phys. Rev. E 92, 062802 (2015)",
        "doi": "10.1103/PhysRevE.92.062802",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an empirical analysis of the microstructure of financial markets\nand, in particular, of the static and dynamic properties of liquidity. We find\nthat on relatively large time scales (15 minutes) large price fluctuations are\nconnected to the failure of the subtle mechanism of compensation between the\nflows of market and limit orders: in other words, the missed revelation of the\nlatent order book breaks the dynamical equilibrium between the flows,\ntriggering the large price jumps. On smaller time scales (30 seconds), instead,\nthe static depletion of the limit order book is an indicator of an intrinsic\nfragility of the system, which is related to a strongly non linear enhancement\nof the response. In order to quantify this phenomenon we introduce a measure of\nthe liquidity imbalance present in the book and we show that it is correlated\nto both the sign and the magnitude of the next price movement. These findings\nprovide a quantitative definition of the effective liquidity, which results to\nbe strongly dependent on the considered time scales.\n"
    },
    {
        "paper_id": 1504.02972,
        "authors": "Ronald Hochreiter",
        "title": "Computing trading strategies based on financial sentiment data using\n  evolutionary optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we apply evolutionary optimization techniques to compute\noptimal rule-based trading strategies based on financial sentiment data. The\nsentiment data was extracted from the social media service StockTwits to\naccommodate the level of bullishness or bearishness of the online trading\ncommunity towards certain stocks. Numerical results for all stocks from the Dow\nJones Industrial Average (DJIA) index are presented and a comparison to\nclassical risk-return portfolio selection is provided.\n"
    },
    {
        "paper_id": 1504.02988,
        "authors": "Alexander Vervuurt",
        "title": "Topics in Stochastic Portfolio Theory",
        "comments": "62 pages, first-year transfer thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is an overview of the area of Stochastic Portfolio Theory, and can be\nseen as an updated and extended version of the survey paper by Fernholz and\nKaratzas (Handbook of Numerical Analysis Vol.15:89-167, 2009).\n"
    },
    {
        "paper_id": 1504.03074,
        "authors": "Binur Yermukanova, Laila Zhexembay, Natanael Karjanto",
        "title": "On a method of solving the Black-Scholes Equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a different method of solving a simplified version of the\nBlack-Scholes equation. This paper will discuss the importance of the\nBlack-Scholes equation and its applications in finance.\n"
    },
    {
        "paper_id": 1504.03079,
        "authors": "Fran\\c{c}ois Legendre (ERUDITE), Djibril Togola (ERUDITE)",
        "title": "Explicit solution to dynamic portfolio choice problem : The\n  continuous-time detour",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/2.1.4715.3449",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper solves the dynamic portfolio choice problem. Using an explicit\nsolution with a power utility, we construct a bridge between a continuous and\ndiscrete VAR model to assess portfolio sensitivities. We find, from a well\nanalyzed example that the optimal allocation to stocks is particularly\nsensitive to Sharpe ratio. Our quantitative analysis highlights that this\nsensitivity increases when the risk aversion decreases and/or when the time\nhorizon increases. This finding explains the low accuracy of discrete numerical\nmethods especially along the tails of the unconditional distribution of the\nstate variable.\n"
    },
    {
        "paper_id": 1504.031,
        "authors": "Thibault Jaisson and Mathieu Rosenbaum",
        "title": "Rough fractional diffusions as scaling limits of nearly unstable heavy\n  tailed Hawkes processes",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the asymptotic behavior as time goes to infinity of Hawkes\nprocesses whose regression kernel has $L^1$ norm close to one and power law\ntail of the form $x^{-(1+\\alpha)}$, with $\\alpha\\in(0,1)$. We in particular\nprove that when $\\alpha\\in(1/2,1)$, after suitable rescaling, their law\nconverges to that of a kind of integrated fractional Cox-Ingersoll-Ross\nprocess, with associated Hurst parameter $H=\\alpha-1/2$. This result is in\ncontrast to the case of a regression kernel with light tail, where a classical\nBrownian CIR process is obtained at the limit. Interestingly, it shows that\npersistence properties in the point process can lead to an irregular behavior\nof the limiting process. This theoretical result enables us to give an\nagent-based foundation to some recent findings about the rough nature of\nvolatility in financial markets.\n"
    },
    {
        "paper_id": 1504.03209,
        "authors": "Mykhaylo Shkolnikov, Ronnie Sircar, Thaleia Zariphopoulou",
        "title": "Asymptotic analysis of forward performance processes in incomplete\n  markets and their ill-posed HJB equations",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal portfolio selection under forward\ninvestment performance criteria in an incomplete market. The dynamics of the\nprices of the traded assets depend on a pair of stochastic factors, namely, a\nslow factor (e.g. a macroeconomic indicator) and a fast factor (e.g. stochastic\nvolatility). We analyze the associated forward performance SPDE and provide\nexplicit formulae for the leading order and first order correction terms for\nthe forward investment process and the optimal feedback portfolios. They both\ndepend on the investor's initial preferences and the dynamically changing\ninvestment opportunities. The leading order terms resemble their time-monotone\ncounterparts, but with the appropriate stochastic time changes resulting from\naveraging phenomena. The first-order terms compile the reaction of the investor\nto both the changes in the market input and his recent performance. Our\nanalysis is based on an expansion of the underlying ill-posed HJB equation, and\nit is justified by means of an appropriate remainder estimate.\n"
    },
    {
        "paper_id": 1504.03232,
        "authors": "Maria Letizia Bertotti and Giovanni Modanese",
        "title": "Economic inequality and mobility in kinetic models for social sciences",
        "comments": "11 pages, 6 figures. Proceedings of the Sigma-Phi Conference on\n  Statistical Physics, Rhodes, 2014",
        "journal-ref": "Europ. Phys. J. ST, 225 (2016) 1945-1958",
        "doi": "10.1140/epjst/e2015-50117-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Statistical evaluations of the economic mobility of a society are more\ndifficult than measurements of the income distribution, because they require to\nfollow the evolution of the individuals' income for at least one or two\ngenerations. In micro-to-macro theoretical models of economic exchanges based\non kinetic equations, the income distribution depends only on the asymptotic\nequilibrium solutions, while mobility estimates also involve the detailed\nstructure of the transition probabilities of the model, and are thus an\nimportant tool for assessing its validity. Empirical data show a remarkably\ngeneral negative correlation between economic inequality and mobility, whose\nexplanation is still unclear. It is therefore particularly interesting to study\nthis correlation in analytical models. In previous work we investigated the\nbehavior of the Gini inequality index in kinetic models in dependence on\nseveral parameters which define the binary interactions and the taxation and\nredistribution processes: saving propensity, taxation rates gap, tax evasion\nrate, welfare means-testing etc. Here, we check the correlation of mobility\nwith inequality by analyzing the mobility dependence from the same parameters.\nAccording to several numerical solutions, the correlation is confirmed to be\nnegative.\n"
    },
    {
        "paper_id": 1504.03238,
        "authors": "Si Cheng and Michael R. Tehranchi",
        "title": "Polynomial term structure models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1404.6190",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we explore a class of tractable interest rate models that\nhave the property that the price of a zero-coupon bond can be expressed as a\npolynomial of a state diffusion process. Our results include a classification\nof all such time-homogeneous single-factor models in the spirit of Filipovic's\nmaximal degree theorem for exponential polynomial models, as well as an\nexplicit characterisation of the set of feasible parameters in the case when\nthe factor process is bounded. Extensions to time-inhomogeneous and\nmulti-factor polynomial models are also considered.\n"
    },
    {
        "paper_id": 1504.03508,
        "authors": "Peter Klimek, Michael Obersteiner, Stefan Thurner",
        "title": "Systemic trade-risk of critical resources",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the wake of the 2008 financial crisis the role of strongly interconnected\nmarkets in fostering systemic instability has been increasingly acknowledged.\nTrade networks of commodities are susceptible to deleterious cascades of supply\nshocks that increase systemic trade-risks and pose a threat to geopolitical\nstability. On a global and a regional level we show that supply risk, scarcity,\nand price volatility of non-fuel mineral resources are intricately connected\nwith the structure of the world-trade network of or spanned by these resources.\nOn the global level we demonstrate that the scarcity of a resource, as measured\nby its trade volume compared to extractable reserves, is closely related to the\nsusceptibility of the trade network with respect to cascading shocks. On the\nregional level we find that to some extent the region-specific price volatility\nand supply risk can be understood by centrality measures that capture systemic\ntrade-risk. The resources associated with the highest systemic trade-risk\nindicators are often those that are produced as byproducts of major metals. We\nidentify significant shortcomings in the management of systemic trade-risk, in\nparticular in the EU.\n"
    },
    {
        "paper_id": 1504.03552,
        "authors": "Fabio Antonelli, Alessandro Ramponi, Sergio Scarlatti",
        "title": "Random Time Forward Starting Options",
        "comments": "19 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a natural generalization of the forward-starting options, first\ndiscussed by M. Rubinstein. The main feature of the contract presented here is\nthat the strike-determination time is not fixed ex-ante, but allowed to be\nrandom, usually related to the occurrence of some event, either of financial\nnature or not. We will call these options {\\bf Random Time Forward Starting\n(RTFS)}. We show that, under an appropriate \"martingale preserving\" hypothesis,\nwe can exhibit arbitrage free prices, which can be explicitly computed in many\nclassical market models, at least under independence between the random time\nand the assets' prices. Practical implementations of the pricing methodologies\nare also provided. Finally a credit value adjustment formula for these OTC\noptions is computed for the unilateral counterparty credit risk.\n"
    },
    {
        "paper_id": 1504.03644,
        "authors": "Mathias Beiglb\\\"ock, Alexander M. G. Cox, Martin Huesmann, Nicolas\n  Perkowski, David J. Pr\\\"omel",
        "title": "Pathwise super-replication via Vovk's outer measure",
        "comments": "18 pages",
        "journal-ref": "Finance Stoch., 21(4):1141-1166, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since Hobson's seminal paper [D. Hobson: Robust hedging of the lookback\noption. In: Finance Stoch. (1998)] the connection between model-independent\npricing and the Skorokhod embedding problem has been a driving force in robust\nfinance. We establish a general pricing-hedging duality for financial\nderivatives which are susceptible to the Skorokhod approach.\n  Using Vovk's approach to mathematical finance we derive a model-independent\nsuper-replication theorem in continuous time, given information on finitely\nmany marginals. Our result covers a broad range of exotic derivatives,\nincluding lookback options, discretely monitored Asian options, and options on\nrealized variance.\n"
    },
    {
        "paper_id": 1504.03733,
        "authors": "Mauro Bernardi and Leopoldo Catania",
        "title": "Switching-GAS Copula Models With Application to Systemic Risk",
        "comments": "59 pages, 60 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent financial disasters have emphasised the need to accurately predict\nextreme financial losses and their consequences for the institutions belonging\nto a given financial market. The ability of econometric models to predict\nextreme events strongly relies on their flexibility to account for the highly\nnonlinear and asymmetric dependence observed in financial returns. We develop a\nnew class of flexible Copula models where the evolution of the dependence\nparameters follow a Markov-Switching Generalised Autoregressive Score (SGASC)\ndynamics. Maximum Likelihood estimation is consistently performed using the\nInference Functions for Margins (IFM) approach and a version of the\nExpectation-Maximisation (EM) algorithm specifically tailored to this class of\nmodels. The SGASC models are then used to estimate the Conditional\nValue-at-Risk (CoVaR), which is defined as the VaR of a given asset conditional\non another asset (or portfolio) being in financial distress, and the\nConditional Expected Shortfall (CoES). Our empirical investigation shows that\nthe proposed SGASC models are able to explain and predict the systemic risk\ncontribution of several European countries. Moreover, we also find that the\nSGASC models outperform competitors using several CoVaR backtesting procedures.\n"
    },
    {
        "paper_id": 1504.03822,
        "authors": "Vadim Nastasiuk",
        "title": "Fisher information and quantum mechanical models for finance",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The probability distribution function (PDF) for prices on financial markets\nis derived by extremization of Fisher information. It is shown how on that\nbasis the quantum-like description for financial markets arises and different\nfinancial market models are mapped by quantum mechanical ones.\n"
    },
    {
        "paper_id": 1504.03895,
        "authors": "Cyril Pitrou",
        "title": "Graph representation of balance sheets: from exogenous to endogenous\n  money",
        "comments": "25 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The nature of monetary arrangements is often discussed without any reference\nto its detailed construction. We present a graph representation that allows for\na clear understanding of modern monetary systems. First, we show that systems\nbased on commodity money are incompatible with credit. We then study the\ncurrent chartalist systems based on pure fiat money, and we discuss the\nconsolidation of the central bank with the Treasury. We obtain a visual\nexplanation about how commercial banks are responsible for endogenous money\ncreation whereas the Treasury and the central bank are in charge of the total\namount of net money. Finally we draw an analogy between systems based on gold\nconvertibility and currency pegs to show that fixed exchange rates can never be\nmaintained.\n"
    },
    {
        "paper_id": 1504.03934,
        "authors": "Ahmed Bel Hadj Ayed, Gr\\'egoire Loeper, Fr\\'ed\\'eric Abergel",
        "title": "Forecasting trends with asset prices",
        "comments": "26 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a stochastic asset price model where the trend is\nan unobservable Ornstein Uhlenbeck process. We first review some classical\nresults from Kalman filtering. Expectedly, the choice of the parameters is\ncrucial to put it into practice. For this purpose, we obtain the likelihood in\nclosed form, and provide two on-line computations of this function. Then, we\ninvestigate the asymptotic behaviour of statistical estimators. Finally, we\nquantify the effect of a bad calibration with the continuous time mis-specified\nKalman filter. Numerical examples illustrate the difficulty of trend\nforecasting in financial time series.\n"
    },
    {
        "paper_id": 1504.04102,
        "authors": "Zhiwu Zheng",
        "title": "The Equilibrium Statistical Model of Economic Systems using Concepts and\n  Theorems of Statistical Physics",
        "comments": "18 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic systems are similar with physic systems for their large number of\nindividuals and the exist of equilibrium. In this paper, we present a model\napplying the equilibrium statistical model in economic systems. Consistent with\nstatistical physics, we define a series of concepts, such as economic\ntemperature, economic pressure, economic potential, wealth and population.\nMoreover, we suggest that these parameters show pretty close relationship with\nthe concepts in economy. This paper presents related concepts in the\nequilibrium economic model and constructs significant theorems and corollaries,\nwhich is derived from the priori possibility postulate, getting theorems\nincluding the equilibrium theorem between open systems, the distribution\ntheorem of wealth and population along with related corollaries. More\nimportantly, we demonstrate a method constructing optimal density of states,\noptimizing a macroscopic parameter depending on need to get the distribution of\ndensity of states utilizing the variation method, which is significant for the\ndevelopment of a society. In addition, we calculate a simple economic system as\nan example, indicating that the system occupied mostly by the middle class\ncould develop stably and soundly, explaining the reason for resulting\ndistributions of macroscopic parameters.\n"
    },
    {
        "paper_id": 1504.04254,
        "authors": "Hong Zhu (ECUST), Zhi-Qiang Jiang (ECUST), Sai-Ping Li (Academia\n  Sinica), Wei-Xing Zhou (ECUST)",
        "title": "Profitability of simple technical trading rules of Chinese stock\n  exchange indexes",
        "comments": "12 pages including 1 figure and 4 tables",
        "journal-ref": "Physica A 439, 75-84 (2015)",
        "doi": "10.1016/j.physa.2015.07.032",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although technical trading rules have been widely used by practitioners in\nfinancial markets, their profitability still remains controversial. We here\ninvestigate the profitability of moving average (MA) and trading range break\n(TRB) rules by using the Shanghai Stock Exchange Composite Index (SHCI) from\nMay 21, 1992 through December 31, 2013 and Shenzhen Stock Exchange Composite\nIndex (SZCI) from April 3, 1991 through December 31, 2013. The $t$-test is\nadopted to check whether the mean returns which are conditioned on the trading\nsignals are significantly different from unconditioned returns and whether the\nmean returns conditioned on the buy signals are significantly different from\nthe mean returns conditioned on the sell signals. We find that TRB rules\noutperform MA rules and short-term variable moving average (VMA) rules\noutperform long-term VMA rules. By applying White's Reality Check test and\naccounting for the data snooping effects, we find that the best trading rule\noutperforms the buy-and-hold strategy when transaction costs are not taken into\nconsideration. Once transaction costs are included, trading profits will be\neliminated completely. Our analysis suggests that simple trading rules like MA\nand TRB cannot beat the standard buy-and-hold strategy for the Chinese stock\nexchange indexes.\n"
    },
    {
        "paper_id": 1504.04296,
        "authors": "Olivier Brandouy, Jean-Paul Delahaye, Lin Ma",
        "title": "Estimating the Algorithmic Complexity of Stock Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Randomness and regularities in Finance are usually treated in probabilistic\nterms. In this paper, we develop a completely different approach in using a\nnon-probabilistic framework based on the algorithmic information theory\ninitially developed by Kolmogorov (1965). We present some elements of this\ntheory and show why it is particularly relevant to Finance, and potentially to\nother sub-fields of Economics as well. We develop a generic method to estimate\nthe Kolmogorov complexity of numeric series. This approach is based on an\niterative \"regularity erasing procedure\" implemented to use lossless\ncompression algorithms on financial data. Examples are provided with both\nsimulated and real-world financial time series. The contributions of this\narticle are twofold. The first one is methodological : we show that some\nstructural regularities, invisible with classical statistical tests, can be\ndetected by this algorithmic method. The second one consists in illustrations\non the daily Dow-Jones Index suggesting that beyond several well-known\nregularities, hidden structure may in this index remain to be identified.\n"
    },
    {
        "paper_id": 1504.04354,
        "authors": "Martin D. Gould and Mason A. Porter and Sam D. Howison",
        "title": "The Long Memory of Order Flow in the Foreign Exchange Spot Market",
        "comments": "38 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the long memory of order flow for each of three liquid currency\npairs on a large electronic trading platform in the foreign exchange (FX) spot\nmarket. Due to the extremely high levels of market activity on the platform,\nand in contrast to existing empirical studies of other markets, our data\nenables us to perform statistically stable estimation without needing to\naggregate data from different trading days. We find strong evidence of long\nmemory, with a Hurst exponent of approximately 0.7, for each of the three\ncurrency pairs and on each trading day in our sample. We repeat our\ncalculations using data that spans different trading days, and we find no\nsignificant differences in our results. We test and reject the hypothesis that\nthe apparent long memory of order flow is an artifact caused by structural\nbreaks, in favour of the alternative hypothesis of true long memory. We\ntherefore conclude that the long memory of order flow in the FX spot market is\na robust empirical property that persists across daily boundaries.\n"
    },
    {
        "paper_id": 1504.04388,
        "authors": "Gilberto Gonz\\'alez-Parra, Benito Chen-Charpentier, Abraham J. Arenas\n  and Miguel Diaz-Rodriguez",
        "title": "Mathematical modeling of physical capital using the spatial Solow model",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research deals with the mathematical modeling of the physical capital\ndiffusion through the borders of the countries. The physical capital is\nconsidered an important variable for the economic growth of a country. Here we\nuse an extension of the economic Solow model to describe how the smuggling\naffects the economic growth of the countries. In this study we rely on a\nproduction function that is non-concave instead of the classical Cobb-Douglas\nproduction function. In order to model the physical capital diffusion through\nthe borders of the country, we developed a model based on a parabolic partial\ndifferential equation that describes the dynamics of physical capital and\nboundary conditions of Neumann type. Smuggling is present in many borders\nbetween countries and may include fuel, machinery and food. This smuggling\nthrough the borders is a problematic issue for the country's economies. The\nsmuggling problem usually is related mainly to a non-official exchange rate\nthat is different than the official rate or subsides. Numerical simulations are\nobtained using an explicit finite difference scheme that shows how the physical\ncapital diffusion through the border of the countries. The study of physical\ncapital is a paramount issue for the economic growth of many countries for the\nnext years. The results show that the dynamics of the physical capital when\nboundary conditions of Neumann type are different than zero differ from the\nclassical economic behavior observed in the classical spatial Solow model\nwithout physical capital flux through the borders of countries. Finally, it can\nbe concluded that avoiding the smuggling through the frontiers is an important\nfactor that affects the economic growth of the countries.\n"
    },
    {
        "paper_id": 1504.04581,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Dirac Processes and Default Risk",
        "comments": "30 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce Dirac processes, using Dirac delta functions, for\nshort-rate-type pricing of financial derivatives. Dirac processes add spikes to\nthe existing building blocks of diffusions and jumps. Dirac processes are\nGeneralized Processes, which have not been used directly before because the\ndollar value of non-Real numbers is meaningless. However, short-rate pricing is\nbased on integrals so Dirac processes are natural. This integration directly\nimplies that jumps are redundant whilst Dirac processes expand expressivity of\nshort-rate approaches. Practically, we demonstrate that Dirac processes enable\nhigh implied volatility for CDS swaptions that has been otherwise problematic\nin hazard rate setups.\n"
    },
    {
        "paper_id": 1504.04594,
        "authors": "Riccardo Fazio",
        "title": "A Posteriori Error Estimator for a Front-Fixing Finite Difference Scheme\n  for American Options",
        "comments": "6 pages,3 figures, 2 tables. World Congress on Engineering 2015.\n  London 1-3 July 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the numerical solution of the American option valuation problem, we\nprovide a script written in MATLAB implementing an explicit finite difference\nscheme. Our main contribute is the definition of a posteriori error estimator\nfor the American options pricing which is based on Richardson's extrapolation\ntheory. This error estimator allows us to find a suitable grid where the\ncomputed solution, both the option price field variable and the free boundary\nposition, verify a prefixed error tolerance.\n"
    },
    {
        "paper_id": 1504.04682,
        "authors": "Tim Leung, Xin Li, Zheng Wang",
        "title": "Optimal Multiple Trading Times Under the Exponential OU Model with\n  Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the timing of trades under mean-reverting price dynamics\nsubject to fixed transaction costs. We solve an optimal double stopping problem\nto determine the optimal times to enter and subsequently exit the market, when\nprices are driven by an exponential Ornstein-Uhlenbeck process. In addition, we\nanalyze a related optimal switching problem that involves an infinite sequence\nof trades, and identify the conditions under which the double stopping and\nswitching problems admit the same optimal entry and/or exit timing strategies.\nAmong our results, we find that the investor generally enters when the price is\nlow, but may find it optimal to wait if the current price is sufficiently close\nto zero. In other words, the continuation (waiting) region for entry is\ndisconnected. Numerical results are provided to illustrate the dependence of\ntiming strategies on model parameters and transaction costs.\n"
    },
    {
        "paper_id": 1504.04774,
        "authors": "Claudia Kl\\\"uppelberg, Jianing Zhang",
        "title": "Time-consistency of risk measures with GARCH volatilities and their\n  estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study time-consistent risk measures for returns that are\ngiven by a GARCH(1,1) model. We present a construction of risk measures based\non their static counterparts that overcomes the lack of time-consistency. We\nthen study in detail our construction for the risk measures Value-at-Risk (VaR)\nand Average Value-at-Risk (AVaR). While in the VaR case we can derive an\nanalytical formula for its time-consistent counterpart, in the AVaR case we\nderive lower and upper bounds to its time-consistent version. Furthermore, we\nincorporate techniques from Extreme Value Theory (EVT) to allow for a more\ntail-geared statistical analysis of the corresponding risk measures. We\nconclude with an application of our results to a data set of stock prices.\n"
    },
    {
        "paper_id": 1504.04819,
        "authors": "Jozef Barunik and Barbora Malinska",
        "title": "Forecasting the term structure of crude oil futures prices with neural\n  networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper contributes to the rare literature modeling term structure of crude\noil markets. We explain term structure of crude oil prices using dynamic\nNelson-Siegel model, and propose to forecast them with the generalized\nregression framework based on neural networks. The newly proposed framework is\nempirically tested on 24 years of crude oil futures prices covering several\nimportant recessions and crisis periods. We find 1-month, 3-month, 6-month and\n12-month-ahead forecasts obtained from focused time-delay neural network to be\nsignificantly more accurate than forecasts from other benchmark models. The\nproposed forecasting strategy produces the lowest errors across all times to\nmaturity.\n"
    },
    {
        "paper_id": 1504.05309,
        "authors": "Andrew Papanicolaou",
        "title": "Introduction to Stochastic Differential Equations (SDEs) for Finance",
        "comments": "These are an evolving set of course notes. Eventually I hope to make\n  them a book. They are posted on the arXiv so that others may see my approach\n  to the topic",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  These are course notes on the application of SDEs to options pricing. The\nauthor was partially supported by NSF grant DMS-0739195.\n"
    },
    {
        "paper_id": 1504.05723,
        "authors": "Saikat Saha",
        "title": "Noise Robust Online Inference for Linear Dynamic Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the Bayesian online inference problems for the linear dynamic\nsystems (LDS) under non- Gaussian environment. The noises can naturally be\nnon-Gaussian (skewed and/or heavy tailed) or to accommodate spurious\nobservations, noises can be modeled as heavy tailed. However, at the cost of\nsuch noise robustness, the performance may degrade when such spurious\nobservations are absent. Therefore, any inference engine should not only be\nrobust to noise outlier, but also be adaptive to potentially unknown and time\nvarying noise parameters; yet it should be scalable and easy to implement.\n  To address them, we envisage here a new noise adaptive Rao-Blackwellized\nparticle filter (RBPF), by leveraging a hierarchically Gaussian model as a\nproxy for any non-Gaussian (process or measurement) noise density. This leads\nto a conditionally linear Gaussian model (CLGM), that is tractable. However,\nthis framework requires a valid transition kernel for the intractable state,\ntargeted by the particle filter (PF). This is typically unknown. We outline how\nsuch kernel can be constructed provably, at least for certain classes\nencompassing many commonly occurring non-Gaussian noises, using auxiliary\nlatent variable approach. The efficacy of this RBPF algorithm is demonstrated\nthrough numerical studies.\n"
    },
    {
        "paper_id": 1504.05737,
        "authors": "Joung-Hun Lee, Marko Jusup, Boris Podobnik, Yoh Iwasa",
        "title": "Agent-based mapping of credit risk for sustainable microfinance",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0126447",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by recent ideas on how the analysis of complex financial risks can\nbenefit from analogies with independent research areas, we propose an\nunorthodox framework for mapping microfinance credit risk---a major obstacle to\nthe sustainability of lenders outreaching to the poor. Specifically, using the\nelements of network theory, we constructed an agent-based model that obeys the\nstylised rules of microfinance industry. We found that in a deteriorating\neconomic environment confounded with adverse selection, a form of latent moral\nhazard may cause a regime shift from a high to a low loan repayment\nprobability. An after-the-fact recovery, when possible, required the economic\nenvironment to improve beyond that which led to the shift in the first place.\nThese findings suggest a small set of measurable quantities for mapping\nmicrofinance credit risk and, consequently, for balancing the requirements to\nreasonably price loans and to operate on a fully self-financed basis. We\nillustrate how the proposed mapping works using a 10-year monthly data set from\none of the best-known microfinance representatives, Grameen Bank in Bangladesh.\nFinally, we discuss an entirely new perspective for managing microfinance\ncredit risk based on enticing spontaneous cooperation by building social\ncapital.\n"
    },
    {
        "paper_id": 1504.05806,
        "authors": "Gareth W. Peters and Efstathios Panayi and Francois Septier",
        "title": "SMC-ABC methods for the estimation of stochastic simulation models of\n  the limit order book",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider classes of models that have been recently developed\nfor quantitative finance that involve modelling a highly complex multivariate,\nmulti-attribute stochastic process known as the Limit Order Book (LOB). The LOB\nis the primary data structure recorded each day intra-daily for all assets on\nevery electronic exchange in the world in which trading takes place. As such,\nit represents one of the most important fundamental structures to study from a\nstochastic process perspective if one wishes to characterize features of\nstochastic dynamics for price, volume, liquidity and other important attributes\nfor a traded asset. In this paper we aim to adopt the model structure which\ndevelops a stochastic model framework for the LOB of a given asset and to\nexplain how to perform calibration of this stochastic model to real observed\nLOB data for a range of different assets.\n"
    },
    {
        "paper_id": 1504.05844,
        "authors": "Chandradew Sharma, Kinjal Banerjee",
        "title": "A Study of Correlations in the Stock Market",
        "comments": "12 pages, 10 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications (2015), pp.\n  321-330",
        "doi": "10.1016/j.physa.2015.03.061",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the various sectors of the Bombay Stock Exchange(BSE) for a period\nof 8 years from April 2006 - March 2014. Using the data of daily returns of a\nperiod of eight years we make a direct model free analysis of the pattern of\nthe sectorial indices movement and the correlations among them. Our analysis\nshows significant auto correlation among the individual sectors and also strong\ncross-correlation among sectors. We also find that auto correlations in some of\nthe sectors persist in time. This is a very significant result and has not been\nreported so far in Indian context These findings will be very useful in model\nbuilding for prediction of price movement of equities, derivatives and\nportfolio management. We show that the Random Walk Hypothesis is not applicable\nin modeling the Indian market and Mean-Variance-Skewness-Kurtosis based\nportfolio optimization might be required. We also find that almost all sectors\nare highly correlated during large fluctuation periods and have only moderate\ncorrelation during normal periods.\n"
    },
    {
        "paper_id": 1504.06031,
        "authors": "Eyal Neuman and Alexander Schied",
        "title": "Optimal Portfolio Liquidation in Target Zone Models and Catalytic\n  Superprocesses",
        "comments": "16 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal buying and selling strategies in target zone models. In\nthese models the price is modeled by a diffusion process which is reflected at\none or more barriers. Such models arise for example when a currency exchange\nrate is kept above a certain threshold due to central bank intervention. We\nconsider the optimal portfolio liquidation problem for an investor for whom\nprices are optimal at the barrier and who creates temporary price impact. This\nproblem will be formulated as the minimization of a cost-risk functional over\nstrategies that only trade when the price process is located at the barrier. We\nsolve the corresponding singular stochastic control problem by means of a\nscaling limit of critical branching particle systems, which is known as a\ncatalytic superprocess. In this setting the catalyst is a set of points which\nis given by the barriers of the price process. For the cases in which the\nunaffected price process is a reflected arithmetic or geometric Brownian motion\nwith drift, we moreover give a detailed financial justification of our cost\nfunctional by means of an approximation with discrete-time models.\n"
    },
    {
        "paper_id": 1504.06094,
        "authors": "Miryana Grigorova, Peter Imkeller, Elias Offen, Youssef Ouknine,\n  Marie-Claire Quenez (LPMA)",
        "title": "Reflected BSDEs when the obstacle is not right-continuous and optimal\n  stopping",
        "comments": null,
        "journal-ref": "The Annals of Applied Probability : an official journal of the\n  institute of mathematical statistics, The Institute of Mathematical\n  Statistics, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the first part of the paper, we study reflected backward stochastic\ndifferential equations (RBSDEs) with lower obstacle which is assumed to be\nright upper-semicontinuous but not necessarily right-continuous. We prove\nexistence and uniqueness of the solutions to such RBSDEs in appropriate Banach\nspaces. The result is established by using some tools from the general theory\nof processes such as Mertens decomposition of optional strong (but not\nnecessarily right-continuous) supermartingales, some tools from optimal\nstopping theory, as well as an appropriate generalization of It{\\^o}'s formula\ndue to Gal'chouk and Lenglart. In the second part of the paper, we provide some\nlinks between the RBSDE studied in the first part and an optimal stopping\nproblem in which the risk of a financial position $\\xi$ is assessed by an\n$f$-conditional expectation $\\mathcal{E}^f(\\cdot)$ (where $f$ is a Lipschitz\ndriver). We characterize the \"value function\" of the problem in terms of the\nsolution to our RBSDE. Under an additional assumption of left\nupper-semicontinuity on $\\xi$, we show the existence of an optimal stopping\ntime. We also provide a generalization of Mertens decomposition to the case of\nstrong $\\mathcal{E}^f$-supermartingales.\n"
    },
    {
        "paper_id": 1504.06113,
        "authors": "Matthias Raddant and Friedrich Wagner",
        "title": "Transitions in the Stock Markets of the US, UK, and Germany",
        "comments": null,
        "journal-ref": "Quantitative Finance, 17(2), 289-297 (2017)",
        "doi": "10.1080/14697688.2016.1183812",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an analysis of the US, the UK, and the German stock market we find a\nchange in the behavior based on the stock's beta values. Before 2006 risky\ntrades were concentrated on stocks in the IT and technology sector. Afterwards\nrisky trading takes place for stocks from the financial sector. We show that an\nagent-based model can reproduce these changes. We further show that the initial\nimpulse for the transition might stem from the increase of high frequency\ntrading at that time.\n"
    },
    {
        "paper_id": 1504.06235,
        "authors": "Stanislaus Maier-Paape, Andreas Platen",
        "title": "Lead-Lag Relationship using a Stop-and-Reverse-MinMax Process",
        "comments": "22 pages, 21 figures, 3 tables; Keywords: lead-lag relationship,\n  intermarket analysis, local extrema, empirical distribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The intermarket analysis, in particular the lead-lag relationship, plays an\nimportant role within financial markets. Therefore a mathematical approach to\nbe able to find interrelations between the price development of two different\nfinancial underlyings is developed in this paper. Computing the differences of\nthe relative positions of relevant local extrema of two charts, i.e., the local\nphase shifts of these underlyings, gives us an empirical distribution on the\nunit circle. With the aid of directional statistics such angular distributions\nare studied for many pairs of markets. It is shown that there are several very\nstrongly correlated underlyings in the field of foreign exchange, commodities\nand indexes. In some cases one of the two underlyings is significantly ahead\nwith respect to the relevant local extrema, i.e., there is a phase shift\nunequal to zero between these two underlyings.\n"
    },
    {
        "paper_id": 1504.06397,
        "authors": "Shan Wang (ECUST), Zhi-Qiang Jiang (ECUST), Sai-Ping Li (Academia\n  Sinica), Wei-Xing Zhou (ECUST)",
        "title": "Testing the performance of technical trading rules in the Chinese market",
        "comments": "11 Latex pages including 2 figures and two tables",
        "journal-ref": "Physica A 439, 114-123 (2015)",
        "doi": "10.1016/j.physa.2015.07.029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technical trading rules have a long history of being used by practitioners in\nfinancial markets. Their profitable ability and efficiency of technical trading\nrules are yet controversial. In this paper, we test the performance of more\nthan seven thousands traditional technical trading rules on the Shanghai\nSecurities Composite Index (SSCI) from May 21, 1992 through June 30, 2013 and\nShanghai Shenzhen 300 Index (SHSZ 300) from April 8, 2005 through June 30, 2013\nto check whether an effective trading strategy could be found by using the\nperformance measurements based on the return and Sharpe ratio. To correct for\nthe influence of the data-snooping effect, we adopt the Superior Predictive\nAbility test to evaluate if there exists a trading rule that can significantly\noutperform the benchmark. The result shows that for SSCI, technical trading\nrules offer significant profitability, while for SHSZ 300, this ability is\nlost. We further partition the SSCI into two sub-series and find that the\nefficiency of technical trading in sub-series, which have exactly the same\nspanning period as that of SHSZ 300, is severely weakened. By testing the\ntrading rules on both indexes with a five-year moving window, we find that the\nfinancial bubble from 2005 to 2007 greatly improve the effectiveness of\ntechnical trading rules. This is consistent with the predictive ability of\ntechnical trading rules which appears when the market is less efficient.\n"
    },
    {
        "paper_id": 1504.06563,
        "authors": "Alexandre Boumezoued",
        "title": "Population viewpoint on Hawkes processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on a class of linear Hawkes processes with general\nimmigrants. These are counting processes with shot noise intensity, including\nself-excited and externally excited patterns. For such processes, we introduce\nthe concept of age pyramid which evolves according to immigration and births.\nThe virtue if this approach that combines an intensity process definition and a\nbranching representation is that the population age pyramid keeps track of all\npast events. This is used to compute new distribution properties for a class of\nlinear Hawkes processes with general immigrants which generalize the popular\nexponential fertility function. The pathwise construction of the Hawkes process\nand its underlying population is also given.\n"
    },
    {
        "paper_id": 1504.06634,
        "authors": "Babak Heydari, Mohsen Mosleh, Kia Dalili",
        "title": "Efficient Network Structures with Separable Heterogeneous Connection\n  Costs",
        "comments": "9 pages",
        "journal-ref": "Economics Letters, Vol. 134, September 2015, 82-85",
        "doi": "10.1016/j.econlet.2015.06.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a heterogeneous connection model for network formation to\ncapture the effect of cost heterogeneity on the structure of efficient\nnetworks. In the proposed model, connection costs are assumed to be separable,\nwhich means the total connection cost for each agent is uniquely proportional\nto its degree. For these sets of networks, we provide the analytical solution\nfor the efficient network and discuss stability impli- cations. We show that\nthe efficient network exhibits a core-periphery structure, and for a given\ndensity, we find a lower bound for clustering coefficient of the efficient\nnetwork.\n"
    },
    {
        "paper_id": 1504.06773,
        "authors": "V.Kandiah, H.Escaith and D.L.Shepelyansky",
        "title": "Google matrix of the world network of economic activities",
        "comments": "more data at http://www.quantware.ups-tlse.fr/QWLIB/wneamatrix/",
        "journal-ref": "Eur. Phys. J. B (2015) 88: 186",
        "doi": "10.1140/epjb/e2015-60324-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the new data from the OECD-WTO world network of economic activities we\nconstruct the Google matrix $G$ of this directed network and perform its\ndetailed analysis. The network contains 58 countries and 37 activity sectors\nfor years 1995 and 2008. The construction of $G$, based on Markov chain\ntransitions, treats all countries on equal democratic grounds while the\ncontribution of activity sectors is proportional to their exchange monetary\nvolume. The Google matrix analysis allows to obtain reliable ranking of\ncountries and activity sectors and to determine the sensitivity of\nCheiRank-PageRank commercial balance of countries in respect to price\nvariations and labor cost in various countries. We demonstrate that the\ndeveloped approach takes into account multiplicity of network links with\neconomy interactions between countries and activity sectors thus being more\nefficient compared to the usual export-import analysis. The spectrum and\neigenstates of $G$ are also analyzed being related to specific activity\ncommunities of countries.\n"
    },
    {
        "paper_id": 1504.06789,
        "authors": "Alexander von Felbert",
        "title": "Network Structure and Counterparty Credit Risk",
        "comments": "27 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we offer a novel type of network model which can capture the\nprecise structure of a financial market based, for example, on empirical\nfindings. With the attached stochastic framework it is further possible to\nstudy how an arbitrary network structure and its expected counterparty credit\nrisk are analytically related to each other. This allows us, for the first\ntime, to model the precise structure of an arbitrary financial market and to\nderive the corresponding expected exposure in a closed-form expression. It\nfurther enables us to draw implications for the study of systemic risk. We\napply the powerful theory of characteristic functions and Hilbert transforms.\nThe latter concept is used to express the characteristic function (c.f.) of the\nrandom variable (r.v.) $\\max(Y, 0)$ in terms of the c.f. of the r.v. $Y$. The\npresent paper applies this concept for the first time in mathematical finance.\nWe then characterise Eulerian digraphs as distinguished exposure structures and\nshow that considering the precise network structures is crucial for the study\nof systemic risk. The introduced network model is then applied to study the\nfeatures of an over-the-counter and a centrally cleared market. We also give a\nmore general answer to the question of whether it is more advantageous for the\noverall counterparty credit risk to clear via a central counterparty or\nclassically bilateral between the two involved counterparties. We then show\nthat the exact market structure is a crucial factor in answering the raised\nquestion.\n"
    },
    {
        "paper_id": 1504.06909,
        "authors": "Yongyang Cai, Kenneth L. Judd, Thomas S. Lontzek",
        "title": "The Social Cost of Carbon with Economic and Climate Risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is great uncertainty about future climate conditions and the\nappropriate policies for managing interactions between the climate and the\neconomy. We develop a multidimensional computational model to examine how\nuncertainties and risks in the economic and climate systems affect the social\ncost of carbon (SCC)---that is, the present value of the marginal damage to\neconomic output caused by carbon emissions. The SCC is substantially increased\nby economic and climate risks at both current and future times. Furthermore,\nthe SCC is itself a stochastic process with significant variation; for example,\nthe basic elements of risk incorporated into our model cause the SCC in 2100 to\nbe, with significant probability, ten times what it would be without those\nrisks. We have only imprecise information about what parameter values are best\nfor approximating reality. To deal with this parametric uncertainty we perform\nextensive uncertainty quantification and show that these findings are robust\nfor a wide range of alternative specifications. More generally, this work shows\nthat large-scale computing can enable economists to examine substantially more\ncomplex and realistic models for the purposes of policy analysis.\n"
    },
    {
        "paper_id": 1504.07152,
        "authors": "Aki-Hiro Sato, Paolo Tasca, Takashi Isogai",
        "title": "Dynamic Interaction Between Asset Prices and Bank Behavior: A Systemic\n  Risk Perspective",
        "comments": "26 pages, 35 figures, 21st Computing in Economics and Finance\n  (CEF2015)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk in banking systems remains a crucial issue that it has not been\ncompletely understood. In our toy model, banks are exposed to two sources of\nrisks, namely, market risk from their investments in assets external to the\nbanking system and credit risk from their lending in the interbank market. By\nand large, both risks increase during severe financial turmoil. Under this\nscenario, the paper shows the conditions under which both the individual and\nthe systemic default tend to coincide.\n"
    },
    {
        "paper_id": 1504.07227,
        "authors": "Mina Karzand and Lav R. Varshney",
        "title": "Communication Strategies for Low-Latency Trading",
        "comments": "Will appear in IEEE International Symposium on Information Theory\n  (ISIT), 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The possibility of latency arbitrage in financial markets has led to the\ndeployment of high-speed communication links between distant financial centers.\nThese links are noisy and so there is a need for coding. In this paper, we\ndevelop a gametheoretic model of trading behavior where two traders compete to\ncapture latency arbitrage opportunities using binary signalling. Different\ncoding schemes are strategies that trade off between reliability and latency.\nWhen one trader has a better channel, the second trader should not compete.\nWith statistically identical channels, we find there are two different regimes\nof channel noise for which: there is a unique Nash equilibrium yielding ties;\nand there are two Nash equilibria with different winners.\n"
    },
    {
        "paper_id": 1504.07604,
        "authors": "Ilona Bednarek, Marcin Makowski, Edward W. Piotrowski, Jan\n  S{\\l}adkowski, Jacek Syska",
        "title": "Generalization of the Aoki-Yoshikawa sectoral productivity model based\n  on extreme physical information principle",
        "comments": "24 pages, 1 figure",
        "journal-ref": "Physica A 428 (2015) 161-172",
        "doi": "10.1016/j.physa.2015.02.033",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a continuous variable generalization of the\nAoki-Yoshikawa sectoral productivity model. Information theoretical methods\nfrom the Frieden-Soffer extreme physical information statistical estimation\nmethodology were used to construct exact solutions. Both approaches coincide in\nfirst order approximation. The approach proposed here can be successfully\napplied in other fields of research.\n"
    },
    {
        "paper_id": 1504.07666,
        "authors": "Eduardo Zambrano, Alberto Hernando, Aurelio Fernandez-Bariviera,\n  Ricardo Hernando, Angelo Plastino",
        "title": "Thermodynamics of firms' growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  The distribution of firms' growth and firms' sizes is a topic under intense\nscrutiny. In this paper we show that a thermodynamic model based on the Maximum\nEntropy Principle, with dynamical prior information, can be constructed that\nadequately describes the dynamics and distribution of firms' growth. Our\ntheoretical framework is tested against a comprehensive data-base of Spanish\nfirms, which covers to a very large extent Spain's economic activity with a\ntotal of 1,155,142 firms evolving along a full decade. We show that the\nempirical exponent of Pareto's law, a rule often observed in the rank\ndistribution of large-size firms, is explained by the capacity of the economic\nsystem for creating/destroying firms, and can be used to measure the health of\na capitalist-based economy. Indeed, our model predicts that when the exponent\nis larger that 1, creation of firms is favored; when it is smaller that 1,\ndestruction of firms is favored instead; and when it equals 1 (matching Zipf's\nlaw), the system is in a full macroeconomic equilibrium, entailing \"free\"\ncreation and/or destruction of firms. For medium and smaller firm-sizes, the\ndynamical regime changes; the whole distribution can no longer be fitted to a\nsingle simple analytic form and numerical prediction is required. Our model\nconstitutes the basis of a full predictive framework for the economic evolution\nof an ensemble of firms that can be potentially used to develop simulations and\ntest hypothetical scenarios, as economic crisis or the response to specific\npolicy measures.\n"
    },
    {
        "paper_id": 1504.07805,
        "authors": "Vivien Brunel",
        "title": "Operational risk modeled analytically II: the consequences of\n  classification invariance",
        "comments": "7 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most of the banks' operational risk internal models are based on loss pooling\nin risk and business line categories. The parameters and outputs of operational\nrisk models are sensitive to the pooling of the data and the choice of the risk\nclassification. In a simple model, we establish the link between the number of\nrisk cells and the model parameters by requiring invariance of the bank's loss\ndistribution upon a change in classification. We provide details on the impact\nof this requirement on the domain of attraction of the loss distribution, on\ndiversification effects and on cell risk correlations.\n"
    },
    {
        "paper_id": 1504.0792,
        "authors": "Alet Roux",
        "title": "Pricing and hedging game options in currency models with proportional\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The pricing, hedging, optimal exercise and optimal cancellation of game or\nIsraeli options are considered in a multi-currency model with proportional\ntransaction costs. Efficient constructions for optimal hedging, cancellation\nand exercise strategies are presented, together with numerical examples, as\nwell as probabilistic dual representations for the bid and ask price of a game\noption.\n"
    },
    {
        "paper_id": 1504.08073,
        "authors": "Tim Leung and Haohua Wan",
        "title": "ESO Valuation with Job Termination Risk and Jumps in Stock Price",
        "comments": "28 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Employee stock options (ESOs) are American-style call options that can be\nterminated early due to employment shock. This paper studies an ESO valuation\nframework that accounts for job termination risk and jumps in the company stock\nprice. Under general L\\'evy stock price dynamics, we show that a higher job\ntermination risk induces the ESO holder to voluntarily accelerate exercise,\nwhich in turn reduces the cost to the company. The holder's optimal exercise\nboundary and ESO cost are determined by solving an inhomogeneous partial\nintegro-differential variational inequality (PIDVI). We apply Fourier transform\nto simplify the variational inequality and develop accurate numerical methods.\nFurthermore, when the stock price follows a geometric Brownian motion, we\nprovide closed-form formulas for both the vested and unvested perpetual ESOs.\nOur model is also applied to evaluate the probabilities of understating ESO\nexpenses and contract termination.\n"
    },
    {
        "paper_id": 1504.08136,
        "authors": "Wendong Zheng and Pingping Zeng",
        "title": "Pricing timer options and variance derivatives with closed-form partial\n  transform under the 3/2 model",
        "comments": "33 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most of the empirical studies on stochastic volatility dynamics favor the 3/2\nspecification over the square-root (CIR) process in the Heston model. In the\ncontext of option pricing, the 3/2 stochastic volatility model is reported to\nbe able to capture the volatility skew evolution better than the Heston model.\nIn this article, we make a thorough investigation on the analytic tractability\nof the 3/2 stochastic volatility model by proposing a closed-form formula for\nthe partial transform of the triple joint transition density $(X,I,V)$ which\nstand for the log asset price, the quadratic variation (continuous realized\nvariance) and the instantaneous variance, respectively. Two distinct\nformulations are provided for deriving the main result. The closed-form partial\ntransform enables us to deduce a variety of marginal partial transforms and\ncharacteristic functions and plays a crucial role in pricing discretely sampled\nvariance derivatives and exotic options that depend on both the asset price and\nquadratic variation. Various applications and numerical examples on pricing\nexotic derivatives with discrete monitoring feature are given to demonstrate\nthe versatility of the partial transform under the 3/2 model.\n"
    },
    {
        "paper_id": 1504.0817,
        "authors": "Kristina R. Dahl, Bernt {\\O}ksendal",
        "title": "Singular recursive utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the concept of singular recursive utility. This leads to a kind\nof singular BSDE which, to the best of our knowledge, has not been studied\nbefore. We show conditions for existence and uniqueness of a solution for this\nkind of singular BSDE. Furthermore, we analyze the problem of maximizing the\nsingular recursive utility. We derive sufficient and necessary maximum\nprinciples for this problem, and connect it to the Skorohod reflection problem.\nFinally, we apply our results to a specific cash flow. In this case, we find\nthat the optimal consumption rate is given by the solution to the corresponding\nSkorohod reflection problem.\n"
    },
    {
        "paper_id": 1505.00003,
        "authors": "G. Papadopoulos and D. Kugiumtzis",
        "title": "Estimation of connectivity measures in gappy time series",
        "comments": "20 pages, 10 figures, submitted to Physica A",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new method is proposed to compute connectivity measures on multivariate\ntime series with gaps. Rather than removing or filling the gaps, the rows of\nthe joint data matrix containing empty entries are removed and the calculations\nare done on the remainder matrix. The method, called measure adapted gap\nremoval (MAGR), can be applied to any connectivity measure that uses a joint\ndata matrix, such as cross correlation, cross mutual information and transfer\nentropy. MAGR is favorably compared using these three measures to a number of\nknown gap-filling techniques, as well as the gap closure. The superiority of\nMAGR is illustrated on time series from synthetic systems and financial time\nseries.\n"
    },
    {
        "paper_id": 1505.00288,
        "authors": "Dietmar Pfeifer, Herv\\'e Awoumlac Tsatedem, Andreas M\\\"andle, C\\^ome\n  Girschig",
        "title": "New copulas based on general partitions-of-unity and their applications\n  to risk management",
        "comments": "23 pages, 24 figures, 4 tables, 2o references",
        "journal-ref": "Dependence Modeling (2016), 123 - 140",
        "doi": "10.1515/demo-2016-0006",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We construct new multivariate copulas on the basis of a generalized infinite\npartition-of-unity approach. This approach allows - in contrast to finite\npartition-of-unity copulas - for tail-dependence as well as for asymmetry. A\npossibility of fitting such copulas to real data from quantitative risk\nmanagement is also pointed out.\n"
    },
    {
        "paper_id": 1505.00328,
        "authors": "Huai-Long Shi, Zhi-Qiang Jiang, Wei-Xing Zhou (ECUST)",
        "title": "Profitability of contrarian strategies in the Chinese stock market",
        "comments": "24 pages (including 4 figures and 9 tables) + 5 supplementary figures\n  + 10 supplementary tables",
        "journal-ref": "PLoS ONE 10(9), e0137892 (2015)",
        "doi": "10.1371/journal.pone.0137892",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper reexamines the profitability of loser, winner and contrarian\nportfolios in the Chinese stock market using monthly data of all stocks traded\non the Shanghai Stock Exchange and Shenzhen Stock Exchange covering the period\nfrom January 1997 to December 2012. We find evidence of short-term and\nlong-term contrarian profitability in the whole sample period when the\nestimation and holding horizons are 1 month or longer than 12 months and the\nannualized returns of contrarian portfolios increases with the estimation and\nholding horizons. We perform subperiod analysis and find that the long-term\ncontrarian effect is significant in both bullish and bearish states while the\nshort-term contrarian effect disappears in bullish states. We compare the\nperformance of contrarian portfolios based on different grouping manners in the\nestimation period and unveil that decile grouping outperforms quintile grouping\nand tertile grouping, which is more evident and robust in the long run.\nGenerally, loser portfolios and winner portfolios have positive returns and\nloser portfolios perform much better than winner portfolios. Both loser and\nwinner portfolios in bullish states perform better than those in the whole\nsample period. In contrast, loser and winner portfolios have smaller returns in\nbearish states in which loser portfolio returns are significant only in the\nlong term and winner portfolio returns become insignificant. These results are\nrobust to the one-month skipping between the estimation and holding periods and\nfor the two stock exchanges. Our findings show that the Chinese stock market is\nnot efficient in the weak form. These findings also have obvious practical\nimplications for financial practitioners.\n"
    },
    {
        "paper_id": 1505.00471,
        "authors": "J. L. Subias",
        "title": "Phase Transitions, Renormalization and Yang-Lee Zeros in Stock Markets",
        "comments": "11 pages, 8 figures, LaTeX preprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper analyses the formal parallelism existing between the laws\nof thermodynamics and some economic principles. Based on previous works, we\nshall show how the existence in Economics of principles analogous to those in\nthermodynamics involves the occurrence of economic events that remind of\nwell-known phenomenological thermodynamic paradigms (i.e., the magnetocaloric\neffect and population inversion). We shall also show how the phase transition\nand renormalization theory provides a natural framework to understand and\npredict trend changes in stock markets. Finally, current negotiation strategies\nin financial markets are briefly reviewed.\n"
    },
    {
        "paper_id": 1505.00475,
        "authors": "Wei Qian, Craig A. Rolling, Gang Cheng, Yuhong Yang",
        "title": "On the Forecast Combination Puzzle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is often reported in forecast combination literature that a simple average\nof candidate forecasts is more robust than sophisticated combining methods.\nThis phenomenon is usually referred to as the \"forecast combination puzzle\".\nMotivated by this puzzle, we explore its possible explanations including\nestimation error, invalid weighting formulas and model screening. We show that\nexisting understanding of the puzzle should be complemented by the distinction\nof different forecast combination scenarios known as combining for adaptation\nand combining for improvement. Applying combining methods without consideration\nof the underlying scenario can itself cause the puzzle. Based on our new\nunderstandings, both simulations and real data evaluations are conducted to\nillustrate the causes of the puzzle. We further propose a multi-level AFTER\nstrategy that can integrate the strengths of different combining methods and\nadapt intelligently to the underlying scenario. In particular, by treating the\nsimple average as a candidate forecast, the proposed strategy is shown to avoid\nthe heavy cost of estimation error and, to a large extent, solve the forecast\ncombination puzzle.\n"
    },
    {
        "paper_id": 1505.00597,
        "authors": "Bruno Bouchard and Dylan Possama\\\"i and Xiaolu Tan",
        "title": "A general Doob-Meyer-Mertens decomposition for $g$-supermartingale\n  systems",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a general Doob-Meyer decomposition for $g$-supermartingale\nsystems, which does not require any right-continuity on the system. In\nparticular, it generalizes the Doob-Meyer decomposition of Mertens (1972) for\nclassical supermartingales, as well as Peng's (1999) version for\nright-continuous $g$-supermartingales. As examples of application, we prove an\noptional decomposition theorem for $g$-supermartingale systems, and also obtain\na general version of the well-known dual formation for BSDEs with constraint on\nthe gains-process, using very simple arguments.\n"
    },
    {
        "paper_id": 1505.00638,
        "authors": "Nikolai Dokuchaev",
        "title": "On statistical indistinguishability of complete and incomplete discrete\n  time market models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the possibility of statistical evaluation of the market\ncompleteness for discrete time stock market models. It is known that the market\ncompleteness is not a robust property: small random deviations of the\ncoefficients convert a complete market model into a incomplete one. The paper\nshows that market incompleteness is also non-robust. We show that, for any\nincomplete market from a wide class of discrete time models, there exists a\ncomplete market model with arbitrarily close stock prices. This means that\nincomplete markets are indistinguishable from the complete markets in the terms\nof the market statistics.\n"
    },
    {
        "paper_id": 1505.00704,
        "authors": "Lucio Maria Calcagnile, Giacomo Bormetti, Michele Treccani, Stefano\n  Marmi, Fabrizio Lillo",
        "title": "Collective synchronization and high frequency systemic instabilities in\n  financial markets",
        "comments": "28 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent years have seen an unprecedented rise of the role that technology\nplays in all aspects of human activities. Unavoidably, technology has heavily\nentered the Capital Markets trading space, to the extent that all major\nexchanges are now trading exclusively using electronic platforms. The ultra\nfast speed of information processing, order placement, and cancelling generates\nnew dynamics which is still not completely deciphered. Analyzing a large\ndataset of stocks traded on the US markets, our study evidences that since 2001\nthe level of synchronization of large price movements across assets has\nsignificantly increased. Even though the total number of over-threshold events\nhas diminished in recent years, when an event occurs, the average number of\nassets swinging together has increased. Quite unexpectedly, only a minor\nfraction of these events -- regularly less than 40% along all years -- can be\nconnected with the release of pre-announced macroeconomic news. We also\ndocument that the larger is the level of sistemicity of an event, the larger is\nthe probability -- and degree of sistemicity -- that a new event will occur in\nthe near future. This opens the way to the intriguing idea that systemic events\nemerge as an effect of a purely endogenous mechanism. Consistently, we present\na high-dimensional, yet parsimonious, model based on a class of self- and\ncross-exciting processes, termed Hawkes processes, which reconciles the\nmodeling effort with the empirical evidence.\n"
    },
    {
        "paper_id": 1505.00829,
        "authors": "Steven E. Pav",
        "title": "Inference on the Sharpe ratio via the upsilon distribution",
        "comments": "Proposed statistical methodology is severely flawed and should not be\n  used",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The upsilon distribution, the sum of independent chi random variates and a\nnormal, is introduced. As a special case, the upsilon distribution includes\nLecoutre's lambda-prime distribution. The upsilon distribution finds\napplication in Frequentist inference on the Sharpe ratio, including hypothesis\ntests on independent samples, confidence intervals, and prediction intervals,\nas well as their Bayesian counterparts. These tests are extended to the case of\nfactor models of returns.\n"
    },
    {
        "paper_id": 1505.00965,
        "authors": "Desmond J. Higham",
        "title": "An Introduction to Multilevel Monte Carlo for Option Valuation",
        "comments": "Submitted to International Journal of Computer Mathematics, special\n  issue on Computational Methods in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monte Carlo is a simple and flexible tool that is widely used in\ncomputational finance. In this context, it is common for the quantity of\ninterest to be the expected value of a random variable defined via a stochastic\ndifferential equation. In 2008, Giles proposed a remarkable improvement to the\napproach of discretizing with a numerical method and applying standard Monte\nCarlo. His multilevel Monte Carlo method offers an order of speed up given by\nthe inverse of epsilon, where epsilon is the required accuracy. So computations\ncan run 100 times more quickly when two digits of accuracy are required. The\nmultilevel philosophy has since been adopted by a range of researchers and a\nwealth of practically significant results has arisen, most of which have yet to\nmake their way into the expository literature.\n  In this work, we give a brief, accessible, introduction to multilevel Monte\nCarlo and summarize recent results applicable to the task of option evaluation.\n"
    },
    {
        "paper_id": 1505.00997,
        "authors": "Anna Aksamit, Tahir Choulli, Jun Deng and Monique Jeanblanc",
        "title": "Non-Arbitrage Under Additional Information for Thin Semimartingale\n  Models",
        "comments": "This paper develops the part of thin and single jump processes\n  mentioned in our earlier version: \"Non-arbitrage up to random horizon and\n  after honest times for semimartingale models\", Available at:\n  arXiv:1310.1142v1. arXiv admin note: text overlap with arXiv:1404.0410",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper completes the two studies undertaken in\n\\cite{aksamit/choulli/deng/jeanblanc2} and\n\\cite{aksamit/choulli/deng/jeanblanc3}, where the authors quantify the impact\nof a random time on the No-Unbounded-Risk-with-Bounded-Profit concept (called\nNUPBR hereafter) when the stock price processes are quasi-left-continuous (do\nnot jump on predictable stopping times). Herein, we focus on the NUPBR for\nsemimartingales models that live on thin predictable sets only and the\nprogressive enlargement with a random time. For this flow of information, we\nexplain how far the NUPBR property is affected when one stops the model by an\narbitrary random time or when one incorporates fully an honest time into the\nmodel. This also generalizes \\cite{choulli/deng} to the case when the jump\ntimes are not ordered in anyway. Furthermore, for the current context, we show\nhow to construct explicitly local martingale deflator under the bigger\nfiltration from those of the smaller filtration.\n"
    },
    {
        "paper_id": 1505.01274,
        "authors": "Els Heinsalu and Marco Patriarca",
        "title": "Kinetic models of immediate exchange",
        "comments": "10 pages, 7 figures",
        "journal-ref": "Eur. Phys. J. B 87, 170 (2014)",
        "doi": "10.1140/epjb/e2014-50270-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel kinetic exchange model differing from previous ones in two\nmain aspects. First, the basic dynamics is modified in order to represent\neconomies where immediate wealth exchanges are carried out, instead of\nreshufflings or uni-directional movements of wealth. Such dynamics produces\nwealth distributions that describe more faithfully real data at small values of\nwealth. Secondly, a general probabilistic trading criterion is introduced, so\nthat two economic units can decide independently whether to trade or not\ndepending on their profit. It is found that the type of the equilibrium wealth\ndistribution is the same for a large class of trading criteria formulated in a\nsymmetrical way with respect to the two interacting units. This establishes\nunexpected links between and provides a microscopic foundations of various\nkinetic exchange models in which the existence of a saving propensity is\npostulated. We also study the generalized heterogeneous version of the model in\nwhich units use different trading criteria and show that suitable sets of\ndiversified parameter values with a moderate level of heterogeneity can\nreproduce realistic wealth distributions with a Pareto power law.\n"
    },
    {
        "paper_id": 1505.01333,
        "authors": "Damien Challet",
        "title": "Sharper asset ranking from total drawdown durations",
        "comments": "21 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The total duration of drawdowns is shown to provide a moment-free, unbiased,\nefficient and robust estimator of Sharpe ratios both for Gaussian and\nheavy-tailed price returns. We then use this quantity to infer an analytic\nexpression of the bias of moment-based Sharpe ratio estimators as a function of\nthe return distribution tail exponent. The heterogeneity of tail exponents at\nany given time among assets implies that our new method yields significantly\ndifferent asset rankings than those of moment-based methods, especially in\nperiods large volatility. This is fully confirmed by using 20 years of\nhistorical data on 3449 liquid US equities.\n"
    },
    {
        "paper_id": 1505.0155,
        "authors": "Gordon J. Ross",
        "title": "Dynamic Multi-Factor Clustering of Financial Networks",
        "comments": "9 pages",
        "journal-ref": "Physical Review E (2014), 89: 022809",
        "doi": "10.1103/PhysRevE.89.022809",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the tendency for financial instruments to form clusters when\nthere are multiple factors influencing the correlation structure. Specifically,\nwe consider a stock portfolio which contains companies from different\nindustrial sectors, located in several different countries. Both sector\nmembership and geography combine to create a complex clustering structure where\ncompanies seem to first be divided based on sector, with geographical\nsubclusters emerging within each industrial sector. We argue that standard\ntechniques for detecting overlapping clusters and communities are not able to\ncapture this type of structure, and show how robust regression techniques can\ninstead be used to remove the influence of both sector and geography from the\ncorrelation matrix separately. Our analysis reveals that prior to the 2008\nfinancial crisis, companies did not tend to form clusters based on geography.\nThis changed immediately following the crisis, with geography becoming a more\nimportant determinant of clustering.\n"
    },
    {
        "paper_id": 1505.02039,
        "authors": "Andrey Itkin and Alexander Lipton",
        "title": "Structural default model with mutual obligations",
        "comments": "27 pages, 13 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers mutual obligations in the interconnected bank system and\nanalyzes their influence on joint and marginal survival probabilities as well\nas CDS and FTD prices for the individual banks. To make the role of mutual\nobligations more transparent, a simple structural default model with banks'\nassets driven by correlated multidimensional Brownian motion with drift is\nconsidered. This model enables a closed form representation for many quantities\nof interest, at least in a 2D case, to be obtained, and moreover, model\ncalibration is provided. Finally, we demonstrate that mutual obligations have\nto be taken into account in order to get correct values for model parameters.\n"
    },
    {
        "paper_id": 1505.02274,
        "authors": "Takayuki Mizuno, Takaaki Ohnishi and Tsutomu Watanabe",
        "title": "Structure of global buyer-supplier networks and its implications for\n  conflict minerals regulations",
        "comments": "18 pages, 7 tables, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the structure of global inter-firm linkages using a dataset\nthat contains information on business partners for about 400,000 firms\nworldwide, including all the firms listed on the major stock exchanges. Among\nthe firms, we examine three networks, which are based on customer-supplier,\nlicensee-licensor, and strategic alliance relationships. First, we show that\nthese networks all have scale-free topology and that the degree distribution\nfor each follows a power law with an exponent of 1.5. The shortest path length\nis around six for all three networks. Second, we show through community\nstructure analysis that the firms comprise a community with those firms that\nbelong to the same industry but different home countries, indicating the\nglobalization of firms' production activities. Finally, we discuss what such\nproduction globalization implies for the proliferation of conflict minerals\n(i.e., minerals extracted from conflict zones and sold to firms in other\ncountries to perpetuate fighting) through global buyer-supplier linkages. We\nshow that a limited number of firms belonging to some specific industries and\ncountries plays an important role in the global proliferation of conflict\nminerals. Our numerical simulation shows that regulations on the purchases of\nconflict minerals by those firms would substantially reduce their worldwide\nuse.\n"
    },
    {
        "paper_id": 1505.02281,
        "authors": "Marius Hofert, Amir Memartoluie, David Saunders, Tony Wirjanto",
        "title": "Improved Algorithms for Computing Worst Value-at-Risk: Numerical\n  Challenges and the Adaptive Rearrangement Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Numerical challenges inherent in algorithms for computing worst Value-at-Risk\nin homogeneous portfolios are identified and solutions as well as words of\nwarning concerning their implementation are provided. Furthermore, both\nconceptual and computational improvements to the Rearrangement Algorithm for\napproximating worst Value-at-Risk for portfolios with arbitrary marginal loss\ndistributions are given. In particular, a novel Adaptive Rearrangement\nAlgorithm is introduced and investigated. These algorithms are implemented\nusing the R package qrmtools.\n"
    },
    {
        "paper_id": 1505.02292,
        "authors": "Amir Memartoluie, David Saunders, Tony Wirjanto",
        "title": "Wrong-Way Bounds in Counterparty Credit Risk Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of finding the worst-case joint distribution of a set of\nrisk factors given prescribed multivariate marginals and a nonlinear loss\nfunction. We show that when the risk measure is CVaR, and the distributions are\ndiscretized, the problem can be conveniently solved using linear programming\ntechnique. The method has applications to any situation where marginals are\nprovided, and bounds need to be determined on total portfolio risk. This arises\nin many financial contexts, including pricing and risk management of exotic\noptions, analysis of structured finance instruments, and aggregation of\nportfolio risk across risk types. Applications to counterparty credit risk are\nemphasized, and they include assessing wrong-way risk in the credit valuation\nadjustment, and counterparty credit risk measurement. Lastly a detailed\napplication of the algorithm for counterparty risk measurement to a real\nportfolio case is also presented in this paper.\n"
    },
    {
        "paper_id": 1505.02305,
        "authors": "Robin L. Lumsdaine, Daniel N. Rockmore, Nicholas Foti, Gregory Leibon,\n  J. Doyne Farmer",
        "title": "The Intrafirm Complexity of Systemically Important Financial\n  Institutions",
        "comments": "33 pages, 7 Figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In November, 2011, the Financial Stability Board, in collaboration with the\nInternational Monetary Fund, published a list of 29 \"systemically important\nfinancial institutions\" (SIFIs). This designation reflects a concern that the\nfailure of any one of them could have dramatic negative consequences for the\nglobal economy and is based on \"their size, complexity, and systemic\ninterconnectedness\". While the characteristics of \"size\" and \"systemic\ninterconnectedness\" have been the subject of a good deal of quantitative\nanalysis, less attention has been paid to measures of a firm's \"complexity.\" In\nthis paper we take on the challenges of measuring the complexity of a financial\ninstitution and to that end explore the use of the structure of an individual\nfirm's control hierarchy as a proxy for institutional complexity. The control\nhierarchy is a network representation of the institution and its subsidiaries.\nWe show that this mathematical representation (and various associated metrics)\nprovides a consistent way to compare the complexity of firms with often very\ndisparate business models and as such may provide the foundation for\ndetermining a SIFI designation. By quantifying the level of complexity of a\nfirm, our approach also may prove useful should firms need to reduce their\nlevel of complexity either in response to business or regulatory needs. Using a\ndata set containing the control hierarchies of many of the designated SIFIs, we\nfind that in the past two years, these firms have decreased their level of\ncomplexity, perhaps in response to regulatory requirements.\n"
    },
    {
        "paper_id": 1505.02416,
        "authors": "Christoph Czichowsky and Walter Schachermayer",
        "title": "Portfolio optimisation beyond semimartingales: shadow prices and\n  fractional Brownian motion",
        "comments": "To appear in Annals of Applied Probability. We would like to thank\n  Junjian Yang for careful reading of the manuscript and pointing out a mistake\n  in an earlier version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While absence of arbitrage in frictionless financial markets requires price\nprocesses to be semimartingales, non-semimartingales can be used to model\nprices in an arbitrage-free way, if proportional transaction costs are taken\ninto account. In this paper, we show, for a class of price processes which are\nnot necessarily semimartingales, the existence of an optimal trading strategy\nfor utility maximisation under transaction costs by establishing the existence\nof a so-called shadow price. This is a semimartingale price process, taking\nvalues in the bid ask spread, such that frictionless trading for that price\nprocess leads to the same optimal strategy and utility as the original problem\nunder transaction costs. Our results combine arguments from convex duality with\nthe stickiness condition introduced by P. Guasoni. They apply in particular to\nexponential utility and geometric fractional Brownian motion. In this case, the\nshadow price is an Ito process. As a consequence we obtain a rather surprising\nresult on the pathwise behaviour of fractional Brownian motion: the\ntrajectories may touch an Ito process in a one-sided manner without reflection.\n"
    },
    {
        "paper_id": 1505.02431,
        "authors": "Elena Boguslavskaya and Dmitry Muravey",
        "title": "An explicit solution for optimal investment in Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a variation of the Merton's problem with added\nstochastic volatility and finite time horizon. It is known that the\ncorresponding optimal control problem may be reduced to a linear parabolic\nboundary problem under some assumptions on the underlying process and the\nutility function. The resulting parabolic PDE is often quite difficult to\nsolve, even when it is linear. The present paper contributes to the pool of\nexplicit solutions for stochastic optimal control problems. Our main result is\nthe exact solution for optimal investment in Heston model.\n"
    },
    {
        "paper_id": 1505.02546,
        "authors": "Thai Huu Nguyen and Serguei Pergamenshchikov",
        "title": "Approximate hedging problem with transaction costs in stochastic\n  volatility markets",
        "comments": "30 pages; 5 figures",
        "journal-ref": null,
        "doi": "10.1111/mafi.12094",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the problem of option replication in general stochastic\nvolatility markets with transaction costs, using a new specification for the\nvolatility adjustment in Leland's algorithm \\cite{Leland}. We prove several\nlimit theorems for the normalized replication error of Leland's strategy, as\nwell as that of the strategy suggested by L\\'epinette. The asymptotic results\nobtained not only generalize the existing results, but also enable us to fix\nthe under-hedging property pointed out by Kabanov and Safarian. We also discuss\npossible methods to improve the convergence rate and to reduce the option price\ninclusive of transaction costs.\n"
    },
    {
        "paper_id": 1505.02627,
        "authors": "Thai Huu Nguyen and Serguei Pergamenschchikov",
        "title": "Approximate hedging with proportional transaction costs in stochastic\n  volatility models with jumps",
        "comments": null,
        "journal-ref": "SIAM Theory of Probability and Its Applications (TVP), 65 (2),\n  2020",
        "doi": "10.4213/tvp5352",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of option replication under constant proportional\ntransaction costs in models where stochastic volatility and jumps are combined\nto capture the market's important features. Assuming some mild condition on the\njump size distribution we show that transaction costs can be approximately\ncompensated by applying the Leland adjusting volatility principle and the\nasymptotic property of the hedging error due to discrete readjustments is\ncharacterized. In particular, the jump risk can be approximately eliminated and\nthe results established in continuous diffusion models are recovered. The study\nalso confirms that for the case of constant trading cost rate, the approximate\nresults established by Kabanov and Safarian (1997)and by Pergamenschikov (2003)\nare still valid in jump-diffusion models with deterministic volatility using\nthe classical Leland parameter in Leland (1986).\n"
    },
    {
        "paper_id": 1505.02644,
        "authors": "Dragos-Patru Covei",
        "title": "A Profit-maximization Model for a Company that Sells an Arbitrary Number\n  of Products",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the problems faced by a firm that sells certain commodities is to\ndetermine the number of products that it must supply in order to maximize its\nprofit. In this article, the authors give an answer to this problem of economic\ninterest. The proposed problem is a generalization of the results obtained by\nStirzaker (Probability and Random Variables: A Beginner's Guide, 1999) and\nKupferman (Lecture Notes in Probability, 2009) where the authors do not present\na situation where the sale of a quantity from some commodities is constrained\nby the marketing of another. In addition, the described procedure is simple and\ncan be successfully applied to any number of commodities. The obtained results\ncan be easily put into practice.\n"
    },
    {
        "paper_id": 1505.0275,
        "authors": "Ying-Hui Shao, Gao-Feng Gu, Zhi-Qiang Jiang, Wei-Xing Zhou (ECUST)",
        "title": "Effects of polynomial trends on detrending moving average analysis",
        "comments": "13 Latex pages including 3 figures",
        "journal-ref": "Fractals 23 (3), 1550034 (2015)",
        "doi": "10.1142/S0218348X15500346",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The detrending moving average (DMA) algorithm is one of the best performing\nmethods to quantify the long-term correlations in nonstationary time series.\nMany long-term correlated time series in real systems contain various trends.\nWe investigate the effects of polynomial trends on the scaling behaviors and\nthe performances of three widely used DMA methods including backward algorithm\n(BDMA), centered algorithm (CDMA) and forward algorithm (FDMA). We derive a\ngeneral framework for polynomial trends and obtain analytical results for\nconstant shifts and linear trends. We find that the behavior of the CDMA method\nis not influenced by constant shifts. In contrast, linear trends cause a\ncrossover in the CDMA fluctuation functions. We also find that constant shifts\nand linear trends cause crossovers in the fluctuation functions obtained from\nthe BDMA and FDMA methods. When a crossover exists, the scaling behavior at\nsmall scales comes from the intrinsic time series while that at large scales is\ndominated by the constant shifts or linear trends. We also derive analytically\nthe expressions of crossover scales and show that the crossover scale depends\non the strength of the polynomial trend, the Hurst index, and in some cases\n(linear trends for BDMA and FDMA) the length of the time series. In all cases,\nthe BDMA and the FDMA behave almost the same under the influence of constant\nshifts or linear trends. Extensive numerical experiments confirm excellently\nthe analytical derivations. We conclude that the CDMA method outperforms the\nBDMA and FDMA methods in the presence of polynomial trends.\n"
    },
    {
        "paper_id": 1505.0303,
        "authors": "Murray Pollock",
        "title": "On the Exact Simulation of (Jump) Diffusion Bridges",
        "comments": "12 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we outline methodology to efficiently simulate (jump) diffusion\nbridge sample paths without discretisation error. We achieve this by\nconsidering the simulation of conditioned (jump) diffusion bridge sample paths\nin light of recent work developing a mathematical framework for simulating\nfinite dimensional sample path skeletons (which flexibly characterise the\nentirety of sample paths).\n"
    },
    {
        "paper_id": 1505.03501,
        "authors": "Ramin Okhrati, Alejandro Balb\\'as and Jos\\'e Garrido",
        "title": "Hedging of defaultable claims in a structural model using a locally\n  risk-minimizing approach",
        "comments": null,
        "journal-ref": "Stochastic Processes and their Applications, 124, (9), 2868-2891\n  (2014)",
        "doi": "10.1016/j.spa.2014.04.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of a locally risk-minimizing approach, the problem of hedging\ndefaultable claims and their Follmer-Schweizer decompositions are discussed in\na structural model. This is done when the underlying process is a finite\nvariation Levy process and the claims pay a predetermined payout at maturity,\ncontingent on no prior default. More precisely, in this particular framework,\nthe locally risk-minimizing approach is carried out when the underlying process\nhas jumps, the derivative is linked to a default event, and the probability\nmeasure is not necessarily risk-neutral.\n"
    },
    {
        "paper_id": 1505.03587,
        "authors": "Malihe Alikhani, Bj{\\o}rn Kjos-Hanssen, Amirarsalan Pakravan, and\n  Babak Saadat",
        "title": "Pricing complexity options",
        "comments": null,
        "journal-ref": "Algorithmic Finance (2015), 4:3-4, 127-137",
        "doi": "10.3233/AF-150050",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider options that pay the complexity deficiency of a sequence of up\nand down ticks of a stock upon exercise. We study the price of European and\nAmerican versions of this option numerically for automatic complexity, and\ntheoretically for Kolmogorov complexity. We also consider run complexity, which\nis a restricted form of automatic complexity.\n"
    },
    {
        "paper_id": 1505.03874,
        "authors": "Johannes Freiesleben and Nicolas Gu\\'erin",
        "title": "Homogenization and Clustering as a Non-Statistical Methodology to Assess\n  Multi-Parametrical Chain Problems",
        "comments": "32 pages, 8 figures, 2 tables, full paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new theoretical and numerical assessment methodology for a\none-dimensional process chain with general applicability to management problems\nsuch as the optimization of decision chains or production chains. The process\nis thereby seen as a chain of subsequently arranged units with random\nparameters influencing the objective function. For solving such complex chain\nproblems, analytical methods usually fail and statistical methods only provide\napproximate solutions while requiring massive computing power. We took insights\nfrom physics to develop a new methodology based on homogenization and\nclustering. The core idea is to replace the complex real chain with a virtual\nchain that homogenizes the involved parameters and clusters the working units\ninto global units to facilitate computation. This methodology drastically\nreduces computing time, allows for the derivation of analytical formulas, and\nprovides fast and objective insights about the optimization problem under\ninvestigation. We illustrate the analytical potency of this methodology by\napplying it to the production problem of selecting the economically superior\nquality maintenance strategy. It can further be applied to all sequential\nmulti-parametrical chain problems commonly found in business.\n"
    },
    {
        "paper_id": 1505.03956,
        "authors": "Dieter Grass",
        "title": "From 0D to 1D spatial models using OCMat",
        "comments": "34 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the standard class of optimal control models in OCMat can be\nused to analyze 1D spatial distributed systems. This approach is an\nintermediate step on the way to the FEM discretization approach presented in\nGrass and Uecker (2015). Therefore, the spatial distributed model is\ntransformed into a standard model by a finite difference discretization. This\n(high dimensional) standard model is then analyzed using OCMAT. As an example\nwe apply this method to the spatial distributed shallow lake model formulated\nin Brock and Xepapadeas (2008). The results are then compared with those of the\nFEM discretization in GRass and Uecker (2015)\n"
    },
    {
        "paper_id": 1505.0398,
        "authors": "Hansjoerg Albrecher and Pablo Azcue and Nora Muler",
        "title": "Optimal Dividend Strategies for Two Collaborating Insurance Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a two-dimensional optimal dividend problem in the context of two\ninsurance companies with compound Poisson surplus processes, who collaborate by\npaying each other's deficit when possible. We solve the stochastic control\nproblem of maximizing the weighted sum of expected discounted dividend payments\n(among all admissible dividend strategies) until ruin of both companies, by\nextending results of univariate optimal control theory. In the case that the\ndividends paid by the two companies are equally weighted, the value function of\nthis problem compares favorably with the one of merging the two companies\ncompletely. We identify this optimal value function as the smallest viscosity\nsupersolution of the respective Hamilton-Jacobi-Bellman equation and provide an\niterative approach to approximate it numerically. Curve strategies are\nidentified as the natural analogue of barrier strategies in this\ntwo-dimensional context. A numerical example is given for which such a curve\nstrategy is indeed optimal among all admissible dividend strategies, and for\nwhich this collaboration mechanism also outperforms the suitably weighted\noptimal dividend strategies of the two stand-alone companies.\n"
    },
    {
        "paper_id": 1505.04045,
        "authors": "Georg Mainik, Georgi Mitov, Ludger R\\\"uschendorf",
        "title": "Portfolio optimization for heavy-tailed assets: Extreme Risk Index vs.\n  Markowitz",
        "comments": "Manuscript accepted in the Journal of Empirical Finance",
        "journal-ref": null,
        "doi": "10.1016/j.jempfin.2015.03.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using daily returns of the S&P 500 stocks from 2001 to 2011, we perform a\nbacktesting study of the portfolio optimization strategy based on the extreme\nrisk index (ERI). This method uses multivariate extreme value theory to\nminimize the probability of large portfolio losses. With more than 400 stocks\nto choose from, our study seems to be the first application of extreme value\ntechniques in portfolio management on a large scale. The primary aim of our\ninvestigation is the potential of ERI in practice. The performance of this\nstrategy is benchmarked against the minimum variance portfolio and the equally\nweighted portfolio. These fundamental strategies are important benchmarks for\nlarge-scale applications. Our comparison includes annualized portfolio returns,\nmaximal drawdowns, transaction costs, portfolio concentration, and asset\ndiversity in the portfolio. In addition to that we study the impact of an\nalternative tail index estimator. Our results show that the ERI strategy\nsignificantly outperforms both the minimum-variance portfolio and the equally\nweighted portfolio on assets with heavy tails.\n"
    },
    {
        "paper_id": 1505.0406,
        "authors": "Wanfeng Yan and Edgar van Tuyll van Serooskerken",
        "title": "Forecasting Financial Extremes: A Network Degree Measure of\n  Super-exponential Growth",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0128908",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Investors in stock market are usually greedy during bull markets and scared\nduring bear markets. The greed or fear spreads across investors quickly. This\nis known as the herding effect, and often leads to a fast movement of stock\nprices. During such market regimes, stock prices change at a super-exponential\nrate and are normally followed by a trend reversal that corrects the previous\nover reaction. In this paper, we construct an indicator to measure the\nmagnitude of the super-exponential growth of stock prices, by measuring the\ndegree of the price network, generated from the price time series. Twelve major\ninternational stock indices have been investigated. Error diagram tests show\nthat this new indicator has strong predictive power for financial extremes,\nboth peaks and troughs. By varying the parameters used to construct the error\ndiagram, we show the predictive power is very robust. The new indicator has a\nbetter performance than the LPPL pattern recognition indicator.\n"
    },
    {
        "paper_id": 1505.04276,
        "authors": "Sebastian Poledna, Jos\\'e Luis Molina-Borboa, Seraf\\'in\n  Mart\\'inez-Jaramillo, Marco van der Leij, and Stefan Thurner",
        "title": "The multi-layer network nature of systemic risk and its implications for\n  the costs of financial crises",
        "comments": "15 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The inability to see and quantify systemic financial risk comes at an immense\nsocial cost. Systemic risk in the financial system arises to a large extent as\na consequence of the interconnectedness of its institutions, which are linked\nthrough networks of different types of financial contracts, such as credit,\nderivatives, foreign exchange and securities. The interplay of the various\nexposure networks can be represented as layers in a financial multi-layer\nnetwork. In this work we quantify the daily contributions to systemic risk from\nfour layers of the Mexican banking system from 2007-2013. We show that focusing\non a single layer underestimates the total systemic risk by up to 90%. By\nassigning systemic risk levels to individual banks we study the systemic risk\nprofile of the Mexican banking system on all market layers. This profile can be\nused to quantify systemic risk on a national level in terms of nation-wide\nexpected systemic losses. We show that market-based systemic risk indicators\nsystematically underestimate expected systemic losses. We find that expected\nsystemic losses are up to a factor four higher now than before the financial\ncrisis of 2007-2008. We find that systemic risk contributions of individual\ntransactions can be up to a factor of thousand higher than the corresponding\ncredit risk, which creates huge risks for the public. We find an intriguing\nnon-linear effect whereby the sum of systemic risk of all layers underestimates\nthe total risk. The method presented here is the first objective data driven\nquantification of systemic risk on national scales that reveal its true levels.\n"
    },
    {
        "paper_id": 1505.04459,
        "authors": "Jos\\'e E. Figueroa-L\\'opez and Yankeng Luo",
        "title": "Small-time expansions for state-dependent local jump-diffusion models\n  with infinite jump activity",
        "comments": "This revision corrects some typos and simplifies and relaxes several\n  conditions and key arguments",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we consider a Markov process X, starting from x and solving\na stochastic differential equation, which is driven by a Brownian motion and an\nindependent pure jump component exhibiting state-dependent jump intensity and\ninfinite jump activity. A second order expansion is derived for the tail\nprobability P[X(t)>x+y] in small time t, for y>0. As an application of this\nexpansion and a suitable change of the underlying probability measure, a second\norder expansion, near expiration, for out-of-the-money European call option\nprices is obtained when the underlying stock price is modeled as the\nexponential of the jump-diffusion process X under the risk-neutral probability\nmeasure.\n"
    },
    {
        "paper_id": 1505.04485,
        "authors": "Chuancun Yin",
        "title": "Remarks on equality of two distributions under some partial orders",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we establish some appropriate conditions for stochastic equality\nof two random variables/vectors which are ordered with respect to convex\nordering or with respect to supermodular ordering. Multivariate extensions of\nthis result are also considered.\n"
    },
    {
        "paper_id": 1505.04573,
        "authors": "Hyong-chol O, Song-gon Jang, Il-Gwang Jon, Mun-Chol Kim, Gyong-Ryol\n  Kim, Hak-Yong Kim",
        "title": "The Binomial Tree Method and Explicit Difference Schemes for American\n  Options with Time Dependent Coefficients",
        "comments": "39 pages, 4 figures; In this version, some new results for American\n  call options are added in Sections 6,7 and 8",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Binomial tree methods (BTM) and explicit difference schemes (EDS) for the\nvariational inequality model of American options with time dependent\ncoefficients are studied. When volatility is time dependent, it is not\nreasonable to assume that the dynamics of the underlying asset's price forms a\nbinomial tree if a partition of time interval with equal parts is used. A time\ninterval partition method that allows binomial tree dynamics of the underlying\nasset's price is provided. Conditions under which the prices of American option\nby BTM and EDS have the monotonic property on time variable are found. Using\nconvergence of EDS for variational inequality model of American options to\nviscosity solution the decreasing property of the price of American put options\nand increasing property of the optimal exercise boundary on time variable are\nproved. First, put options are considered. Then the linear homogeneity and\ncall-put symmetry of the price functions in the BTM and the EDS for the\nvariational inequality model of American options with time dependent\ncoefficients are studied and using them call options are studied.\n"
    },
    {
        "paper_id": 1505.04587,
        "authors": "David Lagziel and Ehud Lehrer",
        "title": "On the Failures of Bonus Plans",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A decision maker (DM) has some funds invested through two investment firms.\nShe wishes to allocate additional funds according to the firms' earnings. The\nDM, on the one hand, tries to maximize the total expected earnings, while the\nfirms, on the other hand, try to maximize the overall expected funds they\nmanage. In this paper we prove that, for every market, the DM has an optimal\nbonus policy such that the firms are motivated to act according to the\ninterests of the DM. On the other hand, we also prove that the only policy that\nis optimal in every market, is independent of the actions and earnings of the\nfirms.\n"
    },
    {
        "paper_id": 1505.04593,
        "authors": "M. Formenti, L. Spadafora, M. Terraneo, F. Ramponi",
        "title": "The efficiency of Anderson-Darling test with limited sample size: an\n  application to Backtesting Counterparty Credit Risk internal model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents a theoretical and empirical evaluation of Anderson-Darling\ntest when the sample size is limited. The test can be applied in order to\nbacktest the risk factors dynamics in the context of Counterparty Credit Risk\nmodelling. We show the limits of such test when backtesting the distributions\nof an interest rate model over long time horizons and we propose a modified\nversion of the test that is able to detect more efficiently an underestimation\nof the model's volatility. Finally we provide an empirical application.\n"
    },
    {
        "paper_id": 1505.04648,
        "authors": "Maximilian Ga{\\ss}, Kathrin Glau, Mirco Mahlstedt, Maximilian Mair",
        "title": "Chebyshev Interpolation for Parametric Option Pricing",
        "comments": "Multivariate Option Pricing, Complexity Reduction, (Tensorized)\n  Chebyshev Polynomials, Polynomial Interpolation, Fourier Transform Methods,\n  Monte Carlo, Affine Processes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recurrent tasks such as pricing, calibration and risk assessment need to be\nexecuted accurately and in real-time. Simultaneously we observe an increase in\nmodel sophistication on the one hand and growing demands on the quality of risk\nmanagement on the other. To address the resulting computational challenges, it\nis natural to exploit the recurrent nature of these tasks. We concentrate on\nParametric Option Pricing (POP) and show that polynomial interpolation in the\nparameter space promises to reduce run-times while maintaining accuracy. The\nattractive properties of Chebyshev interpolation and its tensorized extension\nenable us to identify criteria for (sub)exponential convergence and explicit\nerror bounds. We show that these results apply to a variety of European\n(basket) options and affine asset models. Numerical experiments confirm our\nfindings. Exploring the potential of the method further, we empirically\ninvestigate the efficiency of the Chebyshev method for multivariate and\npath-dependent options.\n"
    },
    {
        "paper_id": 1505.04757,
        "authors": "Jonas Hirz, Uwe Schmock, Pavel V. Shevchenko",
        "title": "Actuarial Applications and Estimation of Extended~CreditRisk$^+$",
        "comments": "34 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an additive stochastic mortality model which allows joint\nmodelling and forecasting of underlying death causes. Parameter families for\nmortality trends can be chosen freely. As model settings become high\ndimensional, Markov chain Monte Carlo (MCMC) is used for parameter estimation.\nWe then link our proposed model to an extended version of the credit risk model\nCreditRisk$^+$. This allows exact risk aggregation via an efficient numerically\nstable Panjer recursion algorithm and provides numerous applications in credit,\nlife insurance and annuity portfolios to derive P\\&L distributions.\nFurthermore, the model allows exact (without Monte Carlo simulation error)\ncalculation of risk measures and their sensitivities with respect to model\nparameters for P\\&L distributions such as value-at-risk and expected shortfall.\nNumerous examples, including an application to partial internal models under\nSolvency II, using Austrian and Australian data are shown.\n"
    },
    {
        "paper_id": 1505.0481,
        "authors": "Xin Guo, Zhao Ruan, Lingjiong Zhu",
        "title": "Dynamics of Order Positions and Related Queues in a Limit Order Book",
        "comments": "42 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Order positions are key variables in algorithmic trading. This paper studies\nthe limiting behavior of order positions and related queues in a limit order\nbook. In addition to the fluid and diffusion limits for the processes,\nfluctuations of order positions and related queues around their fluid limits\nare analyzed. As a corollary, explicit analytical expressions for various\nquantities of interests in a limit order book are derived.\n"
    },
    {
        "paper_id": 1505.04914,
        "authors": "Enrico Biffis, Beniamin Goldys, Cecilia Prosdocimi, Margherita Zanella",
        "title": "A pricing formula for delayed claims: Appreciating the past to value the\n  future",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the valuation of contingent claims with delayed dynamics in a\nBlack&Scholes complete market model. We find a pricing formula that can be\ndecomposed into terms reflecting the market values of the past and the present,\nshowing how the valuation of future cashflows cannot abstract away from the\ncontribution of the past. As a practical application, we provide an explicit\nexpression for the market value of human capital in a setting with wage\nrigidity. The formula we derive has successfully been used to explicitly solve\nthe infinite dimensional stochastic control problems addressed in different\nsettings.\n"
    },
    {
        "paper_id": 1505.04921,
        "authors": "Bernt {\\O}ksendal and Agn\\`es Sulem",
        "title": "Optimal control of predictive mean-field equations and applications to\n  finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a coupled system of controlled stochastic differential equations\n(SDEs) driven by a Brownian motion and a compensated Poisson random measure,\nconsisting of a forward SDE in the unknown process $X(t)$ and a\n\\emph{predictive mean-field} backward SDE (BSDE) in the unknowns $Y(t), Z(t),\nK(t,\\cdot)$. The driver of the BSDE at time $t$ may depend not just upon the\nunknown processes $Y(t), Z(t), K(t,\\cdot)$, but also on the predicted future\nvalue $Y(t+\\delta)$, defined by the conditional expectation $A(t):=\nE[Y(t+\\delta) | \\mathcal{F}_t]$. \\\\ We give a sufficient and a necessary\nmaximum principle for the optimal control of such systems, and then we apply\nthese results to the following two problems:\\\\ (i) Optimal portfolio in a\nfinancial market with an \\emph{insider influenced asset price process.} \\\\ (ii)\n  Optimal consumption rate from a cash flow modeled as a geometric It\\^ o-L\\'\nevy SDE, with respect to \\emph{predictive recursive utility}.\n"
    },
    {
        "paper_id": 1505.04936,
        "authors": "Weibing Huang and Mathieu Rosenbaum",
        "title": "Ergodicity and diffusivity of Markovian order book models: a general\n  framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general Markovian framework for order book modeling. Through our\napproach, we aim at providing a tool enabling to get a better understanding of\nthe price formation process and of the link between microscopic and macroscopic\nfeatures of financial assets. To do so, we propose a new method of order book\nrepresentation, and decompose the problem of order book modeling into two\nsub-problems: dynamics of a continuous-time double auction system with a fixed\nreference price; interactions between the double auction system and the\nreference price movements. State dependency is included in our framework by\nallowing the order flow intensities to depend on the order book state.\nFurthermore, contrary to most existing models, the impact of the order book\nupdates on the reference price dynamics is not assumed to be instantaneous. We\nfirst prove that under general assumptions, our system is ergodic. Then we\ndeduce the convergence towards a Brownian motion of the rescaled price process.\n"
    },
    {
        "paper_id": 1505.05046,
        "authors": "Neda Esmaeeli, Peter Imkeller",
        "title": "American Options with Asymmetric Information and Reflected BSDE",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an American contingent claim on a financial market where the\nbuyer has additional information. Both agents (seller and buyer) observe the\nsame prices, while the information available to them may differ due to some\nextra exogenous knowledge the buyer has. The buyer's information flow is\nmodeled by an initial enlargement of the reference filtration. It seems natural\nto investigate the value of the American contingent claim with asymmetric\ninformation. We provide a representation for the cost of the additional\ninformation relying on some results on reflected backward stochastic\ndifferential equations (RBSDE). This is done by using an interpretation of\nprices of American contingent claims with extra information for the buyer by\nsolutions of appropriate RBSDE.\n"
    },
    {
        "paper_id": 1505.05089,
        "authors": "Ruonan Lin and Yi Gu",
        "title": "CEI: a new indicator measuring City Commercial Credit Risk initiated in\n  China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aiming at quantifying and evaluating the regional commercial environment\nalong with the level of economic development among cities in mainland China,\nthe concept of China City Commercial Environment Credit Index(CEI) was first\nintroduced and established in 2010. In this manuscript, a historical review and\ndetailed introduction of CEI is included, followed by statistical studies. In\nparticular, an independent statistical cross-check for the existing CEI-2012 is\nperformed and significant factors that play the most in influential roles are\ndiscussed.\n"
    },
    {
        "paper_id": 1505.05182,
        "authors": "Romeil Sandhu, Tryphon Georgiou, Allen Tannenbaum",
        "title": "Market Fragility, Systemic Risk, and Ricci Curvature",
        "comments": "6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Measuring systemic risk or fragility of financial systems is a ubiquitous\ntask of fundamental importance in analyzing market efficiency, portfolio\nallocation, and containment of financial contagions. Recent attempts have shown\nthat representing such systems as a weighted graph characterizing the complex\nweb of interacting agents over some information flow (e.g., debt, stock\nreturns, shareholder ownership) may provide certain keen insights. Here, we\nshow that fragility, or the ability of system to be prone to failures in the\nface of random perturbations, is negatively correlated with geometric notion of\nRicci curvature. The key ingredient relating fragility and curvature is\nentropy. As a proof of concept, we examine returns from a set of stocks\ncomprising the S\\&P 500 over a 15 year span to show that financial crashes are\nmore robust compared to normal \"business as usual\" fragile market behavior -\ni.e., Ricci curvature is a \"crash hallmark.\" Perhaps more importantly, this\nwork lays the foundation of understanding of how to design systems and policy\nregulations in a manner that can combat financial instabilities exposed during\nthe 2007-2008 crisis.\n"
    },
    {
        "paper_id": 1505.05256,
        "authors": "Archil Gulisashvili, Frederi Viens, Xin Zhang",
        "title": "Small-time asymptotics for Gaussian self-similar stochastic volatility\n  models",
        "comments": "40 pages, 6 included pdf images",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the class of self-similar Gaussian stochastic volatility models,\nand compute the small-time (near-maturity) asymptotics for the corresponding\nasset price density, the call and put pricing functions, and the implied\nvolatilities. Unlike the well-known model-free behavior for extreme-strike\nasymptotics, small-time behaviors of the above depend heavily on the model, and\nrequire a control of the asset price density which is uniform with respect to\nthe asset price variable, in order to translate into results for call prices\nand implied volatilities. Away from the money, we express the asymptotics\nexplicitly using the volatility process' self-similarity parameter $H$, its\nfirst Karhunen-Loeve eigenvalue at time 1, and the latter's multiplicity.\nSeveral model-free estimators for $H$ result. At the money, a separate study is\nrequired: the asymptotics for small time depend instead on the integrated\nvariance's moments of orders 1/2 and 3/2, and the estimator for $H$ sees an\naffine adjustment, while remaining model-free.\n"
    },
    {
        "paper_id": 1505.05491,
        "authors": "Aizhan Issagali, Damira Alshimbayeva and Aidana Zhalgas",
        "title": "Portfolio Optimization",
        "comments": "The article is not peer-reviewed, and does not contain original\n  research. It might contain errors, and should not be used as a basis for\n  further research. It is a very rough draft, which has not been checked for\n  mistakes and inconsistencies. It should be seen as an initial attempt of\n  students to write on the topic of Portfolio Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper Portfolio Optimization techniques were used to determine the\nmost favorable investment portfolio. In particular, stock indices of three\ncompanies, namely Microsoft Corporation, Christian Dior Fashion House and\nShevron Corporation were evaluated. Using this data the amounts invested in\neach asset when a portfolio is chosen on the efficient frontier were\ncalculated. In addition, the Portfolio with minimum variance, tangency\nportfolio and optimal Markowitz portfolio are presented.\n"
    },
    {
        "paper_id": 1505.05639,
        "authors": "Elena Lucia Harpa, Liviu Marian, Sorina Moica, Iulia Elena Apavaloaie",
        "title": "Analysis of the most important variables which determine innovation\n  among rural entrepreneurs",
        "comments": "4th RMEE Conference The Management Between Profit and Social\n  Responsibility, UT Cluj, 18th-20th September 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present research aims to highlight the main factors influencing the\ndevelopment of entrepreneurial innovation in a rural environment and to perform\nan empirical study with the purpose of assessing the main problems in rural\ndevelopment. The research performed is mostly of a quantitative nature, being\nbased on the use of the questionnaire as research tool, although some of the\nquestions were raised in order to collect respondents impressions and opinions\nwhich would form the object fo qualitative research. The research outlines the\nfact that in the rural entrepreneurship innovation is performed with minimal\ninvestment in new technologies and depends on the entrepreneur's involvement in\nInnovation Systems Network. It was also noted that most of the entrepreneurs in\nrural environment are non-innovators. The research question started to assess\nthe key elements which identify the role of innovation among entrepreneurs in\nrural areas. Based on these facts, we determined the variables that make\nentrepreneurial innovation in rural areas, followed by the analysis of the most\nsignificant variable rural entrepreneurs in the Mures county. This result can\nsupport the creation of the future model of innovation in rural\nentrepreneurship. There are relatively few studies addressing the problem of\nresearch regarding innovation in rural areas. The emphasis is on national and\nregional studies, without differentiating between the rural and urban areas.\nThus, the purpose of the analysis is to add a descriptive background related to\nthe innovation at a micro-economic level in the rural areas\n"
    },
    {
        "paper_id": 1505.05669,
        "authors": "Tommi Ekholm",
        "title": "Optimal forest rotation age under efficient climate change mitigation",
        "comments": "in Forest Policy and Economics, 2015",
        "journal-ref": null,
        "doi": "10.1016/j.forpol.2015.10.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the optimal rotation of forests when the carbon flows of\nforest growth and harvest are priced with an increasing price. Such an\nevolution of carbon price is generally associated with economically efficient\nclimate change mitigation, and would provide incentives for the land-owner for\nenhanced carbon sequestration. With an infinitely long sequence of even-aged\nforest rotations, the optimal harvest age changes with subsequent rotations due\nto the changing carbon price. The first-order optimality conditions therefore\nalso involve an infinite chain of lengths for consecutive forest rotations, and\nallow the approximation of the infinite-time problem with a truncated series of\nforest rotations.\n  Illustrative numerical calculations show that when starting from bare land,\nthe initial carbon price and its growth rate both primarily increase the length\nof the first rotation. With some combinations of the carbon pricing parameters,\nthe optimal harvest age can be several hundred years if the forest carbon is\nreleased to the atmosphere upon harvest. This effect is not, however, entirely\nmonotonous. Consequently, the currently optimal harvest ages are generally\nlower with higher rates of carbon price increase. This creates an interesting\ntemporal aspect, suggesting that the supply of wood and carbon sequestration by\nforests can change considerably during subsequent rotations under an increasing\nprice on carbon.\n"
    },
    {
        "paper_id": 1505.05737,
        "authors": "Ngoc Mai Tran and Josephine Yu",
        "title": "Product-Mix Auctions and Tropical Geometry",
        "comments": "23 pages plus references, 4 figures. Minor edits for journal\n  publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a recent and ongoing work, Baldwin and Klemperer explored a connection\nbetween tropical geometry and economics. They gave a sufficient condition for\nthe existence of competitive equilibrium in product-mix auctions of indivisible\ngoods. This result, which we call the Unimodularity Theorem, can also be traced\nback to the work of Danilov, Koshevoy, and Murota in discrete convex analysis.\nWe give a new proof of the Unimodularity Theorem via the classical\nunimodularity theorem in integer programming. We give a unified treatment of\nthese results via tropical geometry and formulate a new sufficient condition\nfor competitive equilibrium when there are only two types of product.\nGeneralizations of our theorem in higher dimensions are equivalent to various\nforms of the Oda conjecture in algebraic geometry.\n"
    },
    {
        "paper_id": 1505.06053,
        "authors": "Claude Godreche, Satya N. Majumdar, Gregory Schehr",
        "title": "Record statistics for random walk bridges",
        "comments": "40 pages, 11 figures, contribution to the JSTAT Special Issue based\n  on the Galileo Galilei Institute Workshop \"Advances in Nonequilibrium\n  Statistical Mechanics\". Published version",
        "journal-ref": "J. Stat. Mech. P07026 (2015)",
        "doi": "10.1088/1742-5468/2015/07/P07026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the statistics of records in a random sequence\n$\\{x_B(0)=0,x_B(1),\\cdots, x_B(n)=x_B(0)=0\\}$ of $n$ time steps. The sequence\n$x_B(k)$'s represents the position at step $k$ of a random walk `bridge' of $n$\nsteps that starts and ends at the origin. At each step, the increment of the\nposition is a random jump drawn from a specified symmetric distribution. We\nstudy the statistics of records and record ages for such a bridge sequence, for\ndifferent jump distributions. In absence of the bridge condition, i.e., for a\nfree random walk sequence, the statistics of the number and ages of records\nexhibits a `strong' universality for all $n$, i.e., they are completely\nindependent of the jump distribution as long as the distribution is continuous.\nWe show that the presence of the bridge constraint destroys this strong `all\n$n$' universality. Nevertheless a `weaker' universality still remains for large\n$n$, where we show that the record statistics depends on the jump distributions\nonly through a single parameter $0<\\mu\\le 2$, known as the L\\'evy index of the\nwalk, but are insensitive to the other details of the jump distribution. We\nderive the most general results (for arbitrary jump distributions) wherever\npossible and also present two exactly solvable cases. We present numerical\nsimulations that verify our analytical results.\n"
    },
    {
        "paper_id": 1505.06216,
        "authors": "Yuji Hirono, Yoshimasa Hidaka",
        "title": "Jarzynski-type equalities in gambling: role of information in capital\n  growth",
        "comments": "32 pages, no figure; ver2: version to appear in Journal of\n  Statistical Physics",
        "journal-ref": null,
        "doi": "10.1007/s10955-015-1348-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the capital growth in gambling with (and without) side information\nand memory effects. We derive several equalities for gambling, which are of\nsimilar form to the Jarzynski equality and its extension to systems with\nfeedback controls. Those relations provide us with new measures to quantify the\neffects of information on the statistics of capital growth in gambling. We\ndiscuss the implications of the equalities and show that they reproduce the\nknown upper bounds of average capital growth rates.\n"
    },
    {
        "paper_id": 1505.06946,
        "authors": "Runhuan Feng and Hans W. Volkmer",
        "title": "Conditional Asian Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conditional Asian options are recent market innovations, which offer cheaper\nand long-dated alternatives to regular Asian options. In contrast with payoffs\nfrom regular Asian options which are based on average asset prices, the payoffs\nfrom conditional Asian options are determined only by average prices above\ncertain threshold. Due to the limited inclusion of prices, conditional Asian\noptions further reduce the volatility in the payoffs than their regular\ncounterparts and have been promoted in the market as viable hedging and risk\nmanagement instruments for equity-linked life insurance products. There has\nbeen no previous academic literature on this subject and practitioners have\nonly been known to price these products by simulations. We propose the first\nanalytical approach to computing prices and deltas of conditional Asian options\nin comparison with regular Asian options. In the numerical examples, we put to\nthe test some cost-benefit claims by practitioners. As a by-product, the work\nalso presents some distributional properties of the occupation time and the\ntime-integral of geometric Brownian motion during the occupation time.\n"
    },
    {
        "paper_id": 1505.0721,
        "authors": "Dominique Pepin (CRIEF)",
        "title": "Intertemporal Substitutability, Risk Aversion and Asset Prices",
        "comments": null,
        "journal-ref": "Economics Bulletin, Economics Bulletin, 2015, 35 (4), pp.2233-2241",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is the elasticity of intertemporal substitution (EIS) more or less than one?\nThis question can be answered by confronting theoretical results of asset\npricing models with investor behaviour during episodes of stock market panic.\nIf we consider these episodes as periods of high risk aversion, then lower\nasset prices are in fact associated with higher risk aversion. However,\naccording to theoretical models, risky asset price is an increasing function of\nthe coefficient of risk aversion only if the EIS exceeds unity. It may\ntherefore be concluded that the EIS must be more than one to reconcile theory\nwith the observed stock price decline during periods of panic.\n"
    },
    {
        "paper_id": 1505.07224,
        "authors": "Constantinos Kardaras, Hao Xing, Gordan \\v{Z}itkovi\\'c",
        "title": "Incomplete stochastic equilibria for dynamic monetary utility",
        "comments": "33 pages - significantly revised version, extending from exponential\n  to general dynamic monetary utilities",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study existence and uniqueness of continuous-time stochastic Radner\nequilibria in an incomplete market model among a group of agents whose\npreference is characterized by cash invariant time-consistent monetary\nutilities. An assumption of \"smallness\" type is shown to be sufficient for\nexistence and uniqueness. In particular, this assumption encapsulates settings\nwith small endowments, small time-horizon, or a large population of weakly\nheterogeneous agents. Central role in our analysis is played by a fully-coupled\nnonlinear system of quadratic BSDEs.\n"
    },
    {
        "paper_id": 1505.07313,
        "authors": "Tim Leung and Kazutoshi Yamazaki and Hongzhong Zhang",
        "title": "Optimal Multiple Stopping with Negative Discount Rate and Random\n  Refraction Times under Levy Models",
        "comments": "25 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.1137/140957317",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a class of optimal multiple stopping problems driven by\nL\\'evy processes. Our model allows for a negative effective discount rate,\nwhich arises in a number of financial applications, including stock loans and\nreal options, where the strike price can potentially grow at a higher rate than\nthe original discount factor. Moreover, successive exercise opportunities are\nseparated by i.i.d. random refraction times. Under a wide class of two-sided\nL\\'evy models with a general random refraction time, we rigorously show that\nthe optimal strategy to exercise successive call options is uniquely\ncharacterized by a sequence of up-crossing times. The corresponding optimal\nthresholds are determined explicitly in the single stopping case and\nrecursively in the multiple stopping case.\n"
    },
    {
        "paper_id": 1505.07484,
        "authors": "Dirk Tasche",
        "title": "Fitting a distribution to Value-at-Risk and Expected Shortfall, with an\n  application to covered bonds",
        "comments": "27 pages, 2 figures, 3 tables",
        "journal-ref": "Journal of Credit Risk 12(2), 1-34, 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Covered bonds are a specific example of senior secured debt. If the issuer of\nthe bonds defaults the proceeds of the assets in the cover pool are used for\ntheir debt service. If in this situation the cover pool proceeds do not suffice\nfor the debt service, the creditors of the bonds have recourse to the issuer's\nassets and their claims are pari passu with the claims of the creditors of\nsenior unsecured debt. Historically, covered bonds have been very safe\ninvestments. During their more than two hundred years of existence, investors\nnever suffered losses due to missed payments from covered bonds. From a risk\nmanagement perspective, therefore modelling covered bonds losses is mainly of\ninterest for estimating the impact that the asset encumbrance by the cover pool\nhas on the loss characteristics of the issuer's senior unsecured debt. We\nexplore one-period structural modelling approaches for covered bonds and senior\nunsecured debt losses with one and two asset value variables respectively.\nObviously, two-assets models with separate values of the cover pool and the\nissuer's remaining portfolio allow for more realistic modelling. However, we\ndemonstrate that exact calibration of such models may be impossible. We also\ninvestigate a one-asset model in which the riskiness of the cover pool is\nreflected by a risk-based adjustment of the encumbrance ratio of the issuer's\nassets.\n"
    },
    {
        "paper_id": 1505.07533,
        "authors": "Erhan Bayraktar and Song Yao",
        "title": "Optimal Stopping with Random Maturity under Nonlinear Expectations",
        "comments": "Keywords: discretionary stopping, random maturity, controls in weak\n  formulation, optimal stopping, nonlinear expectation, weak stability under\n  pasting, Lipschitz continuous stopping time, dynamic programming principle,\n  martingale approach",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze an optimal stopping problem with random maturity under a nonlinear\nexpectation with respect to a weakly compact set of mutually singular\nprobabilities $\\mathcal{P}$. The maturity is specified as the hitting time to\nlevel $0$ of some continuous index process at which the payoff process is even\nallowed to have a positive jump. When $\\mathcal{P}$ is a collection of\nsemimartingale measures, the optimal stopping problem can be viewed as a {\\it\ndiscretionary} stopping problem for a player who can influence both drift and\nvolatility of the dynamic of underlying stochastic flow.\n"
    },
    {
        "paper_id": 1505.07613,
        "authors": "Bertram D\\\"uring and Christof Heuer",
        "title": "High-order compact schemes for Black-Scholes basket options",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new high-order compact scheme for the multi-dimensional\nBlack-Scholes model with application to European Put options on a basket of two\nunderlying assets. The scheme is second-order accurate in time and fourth-order\naccurate in space. Numerical examples confirm that a standard second-order\nfinite difference scheme is significantly outperformed.\n"
    },
    {
        "paper_id": 1505.07705,
        "authors": "Tim Leung and Kazutoshi Yamazaki and Hongzhong Zhang",
        "title": "An analytic recursive method for optimal multiple stopping: Canadization\n  and phase-type fitting",
        "comments": "30 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1142/S0219024915500326",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal multiple stopping problem for call-type payoff driven by\na spectrally negative Levy process. The stopping times are separated by\nconstant refraction times, and the discount rate can be positive or negative.\nThe computation involves a distribution of the Levy process at a constant\nhorizon and hence the solutions in general cannot be attained analytically.\nMotivated by the maturity randomization (Canadization) technique by Carr\n(1998), we approximate the refraction times by independent, identically\ndistributed Erlang random variables. In addition, fitting random jumps to\nphase-type distributions, our method involves repeated integrations with\nrespect to the resolvent measure written in terms of the scale function of the\nunderlying Levy process. We derive a recursive algorithm to compute the value\nfunction in closed form, and sequentially determine the optimal exercise\nthresholds. A series of numerical examples are provided to compare our analytic\nformula to results from Monte Carlo simulation.\n"
    },
    {
        "paper_id": 1505.07907,
        "authors": "D. Hartmann, M.R. Guevara, C. Jara-Figueroa, M. Aristaran, C.A.\n  Hidalgo",
        "title": "Linking Economic Complexity, Institutions and Income Inequality",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.worlddev.2016.12.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A country's mix of products predicts its subsequent pattern of\ndiversification and economic growth. But does this product mix also predict\nincome inequality? Here we combine methods from econometrics, network science,\nand economic complexity to show that countries exporting complex products (as\nmeasured by the Economic Complexity Index) have lower levels of income\ninequality than countries exporting simpler products. Using multivariate\nregression analysis, we show that economic complexity is a significant and\nnegative predictor of income inequality and that this relationship is robust to\ncontrolling for aggregate measures of income, institutions, export\nconcentration, and human capital. Moreover, we introduce a measure that\nassociates a product to a level of income inequality equal to the average GINI\nof the countries exporting that product (weighted by the share the product\nrepresents in that country's export basket). We use this measure together with\nthe network of related products (or product space) to illustrate how the\ndevelopment of new products is associated with changes in income inequality.\nThese findings show that economic complexity captures information about an\neconomy's level of development that is relevant to the ways an economy\ngenerates and distributes its income. Moreover, these findings suggest that a\ncountry's productive structure may limit its range of income inequality.\nFinally, we make our results available through an online resource that allows\nfor its users to visualize the structural transformation of over 150 countries\nand their associated changes in income inequality between 1963 and 2008.\n"
    },
    {
        "paper_id": 1505.08117,
        "authors": "Olga Y. Uritskaya and Vadim M. Uritsky",
        "title": "Predictability of price movements in deregulated electricity markets",
        "comments": "15 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate predictability of electricity prices in the\nCanadian provinces of Alberta and Ontario, as well as in the US Mid-C market.\nUsing scale-dependent detrended fluctuation analysis, spectral analysis, and\nthe probability distribution analysis we show that the studied markets exhibit\nstrongly anti-persistent properties suggesting that their dynamics can be\npredicted based on historic price records across the range of time scales from\none hour to one month. For both Canadian markets, the price movements reveal\nthree types of correlated behavior which can be used for forecasting. The\ndiscovered scenarios remain the same on different time scales up to one month\nas well as for on- and off- peak electricity data. These scenarios represent\nsharp increases of prices and are not present in the Mid-C market due to its\nlower volatility. We argue that extreme price movements in this market should\nfollow the same tendency as the more volatile Canadian markets. The estimated\nvalues of the Pareto indices suggest that the prediction of these events can be\nstatistically stable. The results obtained provide new relevant information for\nmanaging financial risks associated with the dynamics of electricity\nderivatives over time frame exceeding one day.\n"
    },
    {
        "paper_id": 1505.08136,
        "authors": "Provash Mali, Amitabha Mukhopadhyay",
        "title": "Long-range memory and multifractality in gold markets",
        "comments": "21 pages, 10 figures, 2 tables",
        "journal-ref": "Phys. Scr. 90 (2015) 035209",
        "doi": "10.1088/0031-8949/90/3/035209",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long-range correlation and fluctuation in the gold market time series of\nworld's two leading gold consuming countries, namely China and India, are\nstudied. For both the market series during the period 1985-2013 we observe a\nlong-range persistence of memory in the sequences of maxima (minima) of returns\nin successive time windows of fixed length, but the series as a whole are found\nto be uncorrelated. Multifractal analysis for these series as well as for the\nsequences of maxima (minima) is carried out in terms of the multifractal\ndetrended fluctuation analysis (MF-DFA) method. We observe a weak multifractal\nstructure for the original series that is mainly originated from the fat-tailed\nprobability distribution function of the values, and the multifractal nature of\nthe original time series is enriched into their sequences of maximal (minimal)\nreturns. A quantitative measure of multifractality is provided by using a set\nof \"complexity parameters\".\n"
    },
    {
        "paper_id": 1506.00082,
        "authors": "Yao Tung Huang, Qingshuo Song, Harry Zheng",
        "title": "Weak Convergence of Path-Dependent SDEs in Basket CDS Pricing with\n  Contagion Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the computational aspects of the basket CDS pricing with\ncounterparty risk under a credit contagion model of multinames. This model\nenables us to capture the systematic volatility increases in the market\ntriggered by a particular bankruptcy. The drawback of this problem is its\nanalytical complication due to its path-dependent functional, which bears a\npotential failure in its convergence of numerical approximation under standing\nassumptions. In this paper we find sufficient conditions for the desired\nconvergence of the functionals associated with a class of path-dependent\nstochastic differential equations. The main ingredient is to identify the weak\nconvergence of the approximated solution to the underlying path-dependent\nstochastic differential equation.\n"
    },
    {
        "paper_id": 1506.00088,
        "authors": "Adam D. Bull",
        "title": "Semimartingale detection and goodness-of-fit tests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In quantitative finance, we often fit a parametric semimartingale model to\nasset prices. To ensure our model is correct, we must then perform\ngoodness-of-fit tests. In this paper, we give a new goodness-of-fit test for\nvolatility-like processes, which is easily applied to a variety of\nsemimartingale models. In each case, we reduce the problem to the detection of\na semimartingale observed under noise. In this setting, we then describe a\nwavelet-thresholding test, which obtains adaptive and near-optimal detection\nrates.\n"
    },
    {
        "paper_id": 1506.00146,
        "authors": "Thomas Tarler",
        "title": "The Theory of a Heliospheric Economy",
        "comments": "Needs revision, not comfortable with having it up as is. For example,\n  citation errors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite more than 50 years of human space exploration, no paper in the field\nof economics has been published regarding the theory of a space-based economy.\nThe aim of this paper is to develop quantitative techniques to estimate\nconditions of the human heliospheric expansion. An empirical analysis of\ncurrent space commercialization and reasoning from first economic principles\nyields an evolutionary prisoner's dilemma game on a dynamically scaled\nheterogeneous Newman-Watts Small World Network to generate a new space. The\nanalysis allows for scalar measurements of behavior, market structures, wealth,\nand technological prowess, with time measured relative to the system. Four\nmajor phases of heliospheric expansion become evident, in which the dynamic of\nthe economic environment drives further exploration. Further research could\ncombine empirical estimations of parameters with computer simulations to prove\nresults to inform long-term business plans or public policy to further\nincentivize human heliospheric domination.\n"
    },
    {
        "paper_id": 1506.00166,
        "authors": "Bahman Angoshtari, Erhan Bayraktar, Virginia R. Young",
        "title": "Optimal Investment to Minimize the Probability of Drawdown",
        "comments": "To appear in Stochastics. Keywords: Optimal investment, stochastic\n  optimal control, probability of drawdown",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the optimal investment strategy in a Black-Scholes financial\nmarket to minimize the so-called {\\it probability of drawdown}, namely, the\nprobability that the value of an investment portfolio reaches some fixed\nproportion of its maximum value to date. We assume that the portfolio is\nsubject to a payout that is a deterministic function of its value, as might be\nthe case for an endowment fund paying at a specified rate, for example, at a\nconstant rate or at a rate that is proportional to the fund's value.\n"
    },
    {
        "paper_id": 1506.00188,
        "authors": "Daniel C. Schwarz",
        "title": "Market Completion with Derivative Securities",
        "comments": null,
        "journal-ref": "Finance and Stochastics, 21(1), pp. 263-284, 2017",
        "doi": "10.1007/s00780-016-0317-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $S^F$ be a $\\mathbb{P}$-martingale representing the price of a primitive\nasset in an incomplete market framework. We present easily verifiable\nconditions on model coefficients which guarantee the completeness of the market\nin which in addition to the primitive asset one may also trade a derivative\ncontract $S^B$. Both $S^F$ and $S^B$ are defined in terms of the solution $X$\nto a $2$-dimensional stochastic differential equation: $S^F_t = f(X_t)$ and\n$S^B_t:=\\mathbb{E}[g(X_1) | \\mathcal{F}_t]$. From a purely mathematical point\nof view we prove that every local martingale under $\\mathbb{P}$ can be\nrepresented as a stochastic integral with respect to the\n$\\mathbb{P}$-martingale $S := (S^F\\ S^B)$. Notably, in contrast to recent\nresults on the endogenous completeness of equilibria markets, our conditions\nallow the Jacobian matrix of $(f,g)$ to be singular everywhere on\n$\\mathbf{R}^2$. Hence they cover, as a special case, the prominent example of a\nstochastic volatility model being completed with a European call (or put)\noption.\n"
    },
    {
        "paper_id": 1506.00236,
        "authors": "Ryohei Hisano, Tsutomu Watanabe, Takayuki Mizuno, Takaaki Ohnishi,\n  Didier Sornette",
        "title": "The gradual evolution of buyer--seller networks and their role in\n  aggregate fluctuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Buyer--seller relationships among firms can be regarded as a longitudinal\nnetwork in which the connectivity pattern evolves as each firm receives\nproductivity shocks. Based on a data set describing the evolution of\nbuyer--seller links among 55,608 firms over a decade and structural equation\nmodeling, we find some evidence that interfirm networks evolve reflecting a\nfirm's local decisions to mitigate adverse effects from neighbor firms through\ninterfirm linkage, while enjoying positive effects from them. As a result, link\nrenewal tends to have a positive impact on the growth rates of firms. We also\ninvestigate the role of networks in aggregate fluctuations.\n"
    },
    {
        "paper_id": 1506.00348,
        "authors": "Assaf Almog, Rhys Bird, Diego Garlaschelli",
        "title": "Enhanced Gravity Model of trade: reconciling macroeconomic and network\n  models",
        "comments": null,
        "journal-ref": "Front. Phys., 16 ( 2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The structure of the International Trade Network (ITN), whose nodes and links\nrepresent world countries and their trade relations respectively, affects key\neconomic processes worldwide, including globalization, economic integration,\nindustrial production, and the propagation of shocks and instabilities.\nCharacterizing the ITN via a simple yet accurate model is an open problem. The\ntraditional Gravity Model (GM) successfully reproduces the volume of trade\nbetween connected countries, using macroeconomic properties such as GDP,\ngeographic distance, and possibly other factors. However, it predicts a network\nwith complete or homogeneous topology, thus failing to reproduce the highly\nheterogeneous structure of the ITN. On the other hand, recent maximum-entropy\nnetwork models successfully reproduce the complex topology of the ITN, but\nprovide no information about trade volumes. Here we integrate these two\ncurrently incompatible approaches via the introduction of an Enhanced Gravity\nModel (EGM) of trade. The EGM is the simplest model combining the GM with the\nnetwork approach within a maximum-entropy framework. Via a unified and\nprincipled mechanism that is transparent enough to be generalized to any\neconomic network, the EGM provides a new econometric framework wherein trade\nprobabilities and trade volumes can be separately controlled by any combination\nof dyadic and country-specific macroeconomic variables. The model successfully\nreproduces both the global topology and the local link weights of the ITN,\nparsimoniously reconciling the conflicting approaches. It also indicates that\nthe probability that any two countries trade a certain volume should follow a\ngeometric or exponential distribution with an additional point mass at zero\nvolume.\n"
    },
    {
        "paper_id": 1506.00396,
        "authors": "Takuji Arai",
        "title": "Good deal bounds with convex constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the structure of good deal bounds, which are subintervals of a\nno-arbitrage pricing bound, for financial market models with convex constraints\nas an extension of Arai and Fukasawa (2014). The upper and lower bounds of a\ngood deal bound are naturally described by a convex risk measure. We call such\na risk measure a good deal valuation; and study its properties. We also discuss\nsuperhedging cost and Fundamental Theorem of Asset Pricing for convex\nconstrained markets.\n"
    },
    {
        "paper_id": 1506.00535,
        "authors": "Moawia Alghalith",
        "title": "New exact Taylor's expansions and simple solutions to PDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide new exact Taylor's series with fixed coefficients and without the\nremainder. We demonstrate the usefulness of this contribution by using it to\nobtain very simple solutions to (non-linear) PDEs. We also apply the method to\nthe portfolio model.\n"
    },
    {
        "paper_id": 1506.00686,
        "authors": "Damiano Brigo, Marco Francischello, Andrea Pallavicini",
        "title": "Invariance, existence and uniqueness of solutions of nonlinear valuation\n  PDEs and FBSDEs inclusive of credit risk, collateral and funding costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study conditions for existence, uniqueness and invariance of the\ncomprehensive nonlinear valuation equations first introduced in Pallavicini et\nal (2011). These equations take the form of semilinear PDEs and\nForward-Backward Stochastic Differential Equations (FBSDEs). After summarizing\nthe cash flows definitions allowing us to extend valuation to credit risk and\ndefault closeout, including collateral margining with possible\nre-hypothecation, and treasury funding costs, we show how such cash flows, when\npresent-valued in an arbitrage free setting, lead to semi-linear PDEs or more\ngenerally to FBSDEs. We provide conditions for existence and uniqueness of such\nsolutions in a viscosity and classical sense, discussing the role of the\nhedging strategy. We show an invariance theorem stating that even though we\nstart from a risk-neutral valuation approach based on a locally risk-free bank\naccount growing at a risk-free rate, our final valuation equations do not\ndepend on the risk free rate. Indeed, our final semilinear PDE or FBSDEs and\ntheir classical or viscosity solutions depend only on contractual, market or\ntreasury rates and we do not need to proxy the risk free rate with a real\nmarket rate, since it acts as an instrumental variable. The equations\nderivations, their numerical solutions, the related XVA valuation adjustments\nwith their overlap, and the invariance result had been analyzed numerically and\nextended to central clearing and multiple discount curves in a number of\nprevious works, including Pallavicini et al (2011), Pallavicini et al (2012),\nBrigo et al (2013), Brigo and Pallavicini (2014), and Brigo et al (2014).\n"
    },
    {
        "paper_id": 1506.00697,
        "authors": "Andrzej Daniluk, Rafa{\\l} Muchorski",
        "title": "Approximations of Bond and Swaption Prices in a Black-Karasi\\'{n}ski\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive semi-analytic approximation formulae for bond and swaption prices\nin a Black-Karasi\\'{n}ski interest rate model. Approximations are obtained\nusing a novel technique based on the Karhunen-Lo\\`{e}ve expansion. Formulas are\neasily computable and prove to be very accurate in numerical tests. This makes\nthem useful for numerically efficient calibration of the model.\n"
    },
    {
        "paper_id": 1506.00806,
        "authors": "Antoine Kornprobst, Raphael Douady",
        "title": "An Empirical Approach to Financial Crisis Indicators Based on Random\n  Matrices",
        "comments": "Added better graphics for the protective put strategy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this work is to build financial crisis indicators based on\nspectral properties of the dynamics of market data. After choosing an optimal\nsize for a rolling window, the historical market data in this window is seen\nevery trading day as a random matrix from which a covariance and a correlation\nmatrix are obtained. The financial crisis indicators that we have built deal\nwith the spectral properties of these covariance and correlation matrices and\nthey are of two kinds. The first one is based on the Hellinger distance,\ncomputed between the distribution of the eigenvalues of the empirical\ncovariance matrix and the distribution of the eigenvalues of a reference\ncovariance matrix representing either a calm or agitated market. The idea\nbehind this first type of indicators is that when the empirical distribution of\nthe spectrum of the covariance matrix is deviating from the reference in the\nsense of Hellinger, then a crisis may be forthcoming. The second type of\nindicators is based on the study of the spectral radius and the trace of the\ncovariance and correlation matrices as a mean to directly study the volatility\nand correlations inside the market. The idea behind the second type of\nindicators is the fact that large eigenvalues are a sign of dynamic\ninstability. The predictive power of the financial crisis indicators in this\nframework is then demonstrated, in particular by using them as decision-making\ntools in a protective-put strategy.\n"
    },
    {
        "paper_id": 1506.00937,
        "authors": "Zachary Feinstein",
        "title": "Financial Contagion and Asset Liquidation Strategies",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a framework for modeling the financial system with\nmultiple illiquid assets during a crisis. This work generalizes the paper by\nAmini, Filipovic and Minca (2016) by allowing for differing liquidation\nstrategies. The main result is a proof of sufficient conditions for the\nexistence of an equilibrium liquidation strategy with corresponding unique\nclearing payments and liquidation prices. An algorithm for computing the\nmaximal clearing payments and prices is provided.\n"
    },
    {
        "paper_id": 1506.01315,
        "authors": "Mario Szegedy and Yixin Xu",
        "title": "Impossibility Theorems and the Universal Algebraic Toolkit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We elucidate a close connection between the Theory of Judgment Aggregation\n(more generally, Evaluation Aggregation), and a relatively young but rapidly\ngrowing field of universal algebra, that was primarily developed to investigate\nconstraint satisfaction problems. Our connection yields a full classification\nof non-binary evaluations into possibility and impossibility domains both under\nthe idempotent and the supportive conditions. Prior to the current result E.\nDokow and R. Holzman nearly classified non-binary evaluations in the supportive\ncase, by combinatorial means. The algebraic approach gives us new insights to\nthe easier binary case as well, which had been fully classified by the above\nauthors. Our algebraic view lets us put forth a suggestion about a\nstrengthening of the Non-dictatorship criterion, that helps us avoid \"outliers\"\nlike the affine subspace. Finally, we give upper bounds on the complexity of\ncomputing if a domain is impossible or not (to our best knowledge no finite\ntime bounds were given earlier).\n"
    },
    {
        "paper_id": 1506.01467,
        "authors": "Anindya Goswami, Jeeten Patel, Poorva Shevgaonkar",
        "title": "A system of non-local parabolic PDE and application to option pricing",
        "comments": "7 pages. arXiv admin note: substantial text overlap with\n  arXiv:1408.5266",
        "journal-ref": "Stoch. Anal. Appl. 34(2016) no. 5, 893-905",
        "doi": "10.1080/07362994.2016.1189340",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper includes a proof of well-posedness of an initial-boundary value\nproblem involving a system of degenerate non-local parabolic PDE which\nnaturally arises in the study of derivative pricing in a generalized market\nmodel. In a semi-Markov modulated GBM model the locally risk minimizing price\nfunction satisfies a special case of this problem. We study the well-posedness\nof the problem via a Volterra integral equation of second kind. A probabilistic\napproach, in particular the method of conditioning on stopping times is used\nfor showing uniqueness.\n"
    },
    {
        "paper_id": 1506.01477,
        "authors": "Takuji Arai",
        "title": "Local risk-minimization for Barndorff-Nielsen and Shephard models with\n  volatility risk premium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive representations of local risk-minimization of call and put options\nfor Barndorff-Nielsen and Shephard models: jump type stochastic volatility\nmodels whose squared volatility process is given by a non-Gaussian\nrnstein-Uhlenbeck process. The general form of Barndorff-Nielsen and Shephard\nmodels includes two parameters: volatility risk premium $\\beta$ and leverage\neffect $\\rho$. Arai and Suzuki (2015, arxiv:1503.08589) dealt with the same\nproblem under constraint $\\beta=-\\frac{1}{2}$. In this paper, we relax the\nrestriction on $\\beta$; and restrict $\\rho$ to $0$ instead. We introduce a\nMalliavin calculus under the minimal martingale measure to solve the problem.\n"
    },
    {
        "paper_id": 1506.01513,
        "authors": "David Garcia, Frank Schweitzer",
        "title": "Social signals and algorithmic trading of Bitcoin",
        "comments": "http://rsos.royalsocietypublishing.org/content/2/9/150288",
        "journal-ref": "Royal Society Open Science, 2:150288, 2015",
        "doi": "10.1098/rsos.150288",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The availability of data on digital traces is growing to unprecedented sizes,\nbut inferring actionable knowledge from large-scale data is far from being\ntrivial. This is especially important for computational finance, where digital\ntraces of human behavior offer a great potential to drive trading strategies.\nWe contribute to this by providing a consistent approach that integrates\nvarious datasources in the design of algorithmic traders. This allows us to\nderive insights into the principles behind the profitability of our trading\nstrategies. We illustrate our approach through the analysis of Bitcoin, a\ncryptocurrency known for its large price fluctuations. In our analysis, we\ninclude economic signals of volume and price of exchange for USD, adoption of\nthe Bitcoin technology, and transaction volume of Bitcoin. We add social\nsignals related to information search, word of mouth volume, emotional valence,\nand opinion polarization as expressed in tweets related to Bitcoin for more\nthan 3 years. Our analysis reveals that increases in opinion polarization and\nexchange volume precede rising Bitcoin prices, and that emotional valence\nprecedes opinion polarization and rising exchange volumes. We apply these\ninsights to design algorithmic trading strategies for Bitcoin, reaching very\nhigh profits in less than a year. We verify this high profitability with robust\nstatistical methods that take into account risk and trading costs, confirming\nthe long-standing hypothesis that trading based social media sentiment has the\npotential to yield positive returns on investment.\n"
    },
    {
        "paper_id": 1506.0166,
        "authors": "Dan Xu and Christian Beck",
        "title": "Transition from lognormal to chi-square superstatistics for financial\n  time series",
        "comments": "8 pages, 15 figures, 1 table. Replaced by final version published in\n  Physica A",
        "journal-ref": "Physica A 453, 173 (2016)",
        "doi": "10.1016/j.physa.2016.02.057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Share price returns on different time scales can be well modelled by a\nsuperstatistical dynamics. Here we provide an investigation which type of\nsuperstatistics is most suitable to properly describe share price dynamics on\nvarious time scales. It is shown that while chi-square superstatistics works\nwell on a time scale of days, on a much smaller time scale of minutes the price\nchanges are better described by lognormal superstatistics. The system dynamics\nthus exhibits a transition from lognormal to chi-square superstatistics as a\nfunction of time scale. We discuss a more general model interpolating between\nboth statistics which fits the observed data very well. We also present results\non correlation functions of the extracted superstatistical volatility\nparameter, which exhibits exponential decay for returns on large time scales,\nwhereas for returns on small time scales there are long-range correlations and\npower-law decay.\n"
    },
    {
        "paper_id": 1506.01734,
        "authors": "Natasa Golo, Guy Kelman, David S. Bree, Leanne Usher, Marco Lamieri\n  and Sorin Solomon",
        "title": "Many-to-one contagion of economic growth rate across trade credit\n  network of firms",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach and an empirical procedure to test direct\ncontagion of growth rate in a trade credit network of firms. Our hypotheses are\nthat the use of trade credit contributes to contagion (from many customers to a\nsingle supplier - \"many to one\" contagion) and amplification (through their\ninteraction with the macrocopic variables, such as interest rate) of growth\nrate. In this paper we test the contagion hypothesis, measuring empirically the\nmesoscopic \"many-to-one\" effect. The effect of amplification has been dealt\nwith in another paper.\n  Our empirical analysis is based on the delayed payments between trading\npartners across many different industrial sectors, intermediated by a large\nItalian bank during the year 2007. The data is used to create a weighted and\ndirected trade credit network. Assuming that the linkages are static, we look\nat the dynamics of the nodes/firms. On the ratio of the 2007 trade credit in\nSales and Purchases items on the profit and loss statements, we estimate the\ntrade credit in 2006 and 2008.\n  Applying the \"many to one\" approach we compare such predicted growth of trade\n(demand) aggregated per supplier, and compare it with the real growth of Sales\nof the supplier. We analyze the correlation of these two growth rates over two\nyearly periods, 2007/2006 and 2008/2007, and in this way we test our contagion\nhypotheses. We could not find strong correlations between the predicted and the\nactual growth rates. We provide an evidence of contagion only in restricted\nsub-groups of our network, and not in the whole network. We do find a strong\nmacroscopic effect of the crisis, indicated by a coincident negative drift in\nthe growth of sales of nearly all the firms in our sample.\n"
    },
    {
        "paper_id": 1506.01837,
        "authors": "Tom Fischer",
        "title": "No-Arbitrage Prices of Cash Flows and Forward Contracts as Choquet\n  Representations",
        "comments": "JEL Classification: G12, G13",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a market of deterministic cash flows, given as an additive, symmetric\nrelation of exchangeability on the finite signed Borel measures on the\nnon-negative real time axis, it is shown that the only arbitrage-free price\nfunctional that fulfills some additional mild requirements is the integral of\nthe unit zero-coupon bond prices with respect to the payment measures. For\nprobability measures, this is a Choquet representation, where the Dirac\nmeasures, as unit zero-coupon bonds, are the extreme points. Dropping one of\nthe requirements, the Lebesgue decomposition is used to construct\ncounterexamples, where the Choquet price formula does not hold despite of an\narbitrage-free market model. The concept is then extended to deterministic\nstreams of assets and currencies in general, yielding a valuation principle for\nforward markets. Under mild assumptions, it is shown that a foreign cash flow's\nworth in local currency is identical to the value of the cash flow in local\ncurrency for which the Radon-Nikodym derivative with respect to the foreign\ncash flow is the forward FX rate.\n"
    },
    {
        "paper_id": 1506.01984,
        "authors": "Luca Di Persio, Chiara Segala",
        "title": "Autoregressive approaches to import--export time series II: a concrete\n  case study",
        "comments": "Published at http://dx.doi.org/10.15559/15-VMSTA25 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2015, Vol. 2, No. 1,\n  67-93",
        "doi": "10.15559/15-VMSTA25",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present work constitutes the second part of a two-paper project that, in\nparticular, deals with an in-depth study of effective techniques used in\neconometrics in order to make accurate forecasts in the concrete framework of\none of the major economies of the most productive Italian area, namely the\nprovince of Verona. It is worth mentioning that this region is indubitably\nrecognized as the core of the commercial engine of the whole Italian country.\nThis is why our analysis has a concrete impact; it is based on real data, and\nthis is also the reason why particular attention has been taken in treating the\nrelevant economical data and in choosing the right methods to manage them to\nobtain good forecasts. In particular, we develop an approach mainly based on\nvector autoregression where lagged values of two or more variables are\nconsidered, Granger causality, and the stochastic trend approach useful to work\nwith the cointegration phenomenon.\n"
    },
    {
        "paper_id": 1506.02013,
        "authors": "James Li, Eric Bax, Nilanjan Roy, Andrea Leistra",
        "title": "VCG Payments for Portfolio Allocations in Online Advertising",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some online advertising offers pay only when an ad elicits a response.\nRandomness and uncertainty about response rates make showing those ads a risky\ninvestment for online publishers. Like financial investors, publishers can use\nportfolio allocation over multiple advertising offers to pursue revenue while\ncontrolling risk. Allocations over multiple offers do not have a distinct\nwinner and runner-up, so the usual second-price mechanism does not apply. This\npaper develops a pricing mechanism for portfolio allocations. The mechanism is\nefficient, truthful, and rewards offers that reduce risk.\n"
    },
    {
        "paper_id": 1506.0202,
        "authors": "Ragavendran Gopalakrishnan, Eric Bax, Krishna Prasad Chitrapura,\n  Sachin Garg",
        "title": "Portfolio Allocation for Sellers in Online Advertising",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In markets for online advertising, some advertisers pay only when users\nrespond to ads. So publishers estimate ad response rates and multiply by\nadvertiser bids to estimate expected revenue for showing ads. Since these\nestimates may be inaccurate, the publisher risks not selecting the ad for each\nad call that would maximize revenue. The variance of revenue can be decomposed\ninto two components -- variance due to `uncertainty' because the true response\nrate is unknown, and variance due to `randomness' because realized response\nstatistics fluctuate around the true response rate. Over a sequence of many ad\ncalls, the variance due to randomness nearly vanishes due to the law of large\nnumbers. However, the variance due to uncertainty doesn't diminish.\n  We introduce a technique for ad selection that augments existing estimation\nand explore-exploit methods. The technique uses methods from portfolio\noptimization to produce a distribution over ads rather than selecting the\nsingle ad that maximizes estimated expected revenue. Over a sequence of similar\nad calls, ads are selected according to the distribution. This approach\ndecreases the effects of uncertainty and increases revenue.\n"
    },
    {
        "paper_id": 1506.02074,
        "authors": "Tim Leung, Matthew Lorig",
        "title": "Optimal Static Quadratic Hedging",
        "comments": "33 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a flexible framework for hedging a contingent claim by holding\nstatic positions in vanilla European calls, puts, bonds, and forwards. A\nmodel-free expression is derived for the optimal static hedging strategy that\nminimizes the expected squared hedging error subject to a cost constraint. The\noptimal hedge involves computing a number of expectations that reflect the\ndependence among the contingent claim and the hedging assets. We provide a\ngeneral method for approximating these expectations analytically in a general\nMarkov diffusion market. To illustrate the versatility of our approach, we\npresent several numerical examples, including hedging path-dependent options\nand options written on a correlated asset.\n"
    },
    {
        "paper_id": 1506.02414,
        "authors": "Roy Cerqueti and Marcel Ausloos",
        "title": "Cross Ranking of Cities and Regions: Population vs. Income",
        "comments": "34 pages, 13 figures, 6 tables, 81 references; prepared for Journal\n  of Statistical Mechanics: Theory and Experiment (JSTAT)",
        "journal-ref": "J. Stat. Mech. (2015) P07002",
        "doi": "10.1088/1742-5468/2015/00/000000",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the relationship between the inner economical structure\nof communities and their population distribution through a rank-rank analysis\nof official data, along statistical physics ideas within two techniques. The\ndata is taken on Italian cities. The analysis is performed both at a global\n(national) and at a more local (regional) level in order to distinguish \"macro\"\nand \"micro\" aspects. First, the rank-size rule is found not to be a standard\npower law, as in many other studies, but a doubly decreasing power law. Next,\nthe Kendall and the Spearman rank correlation coefficients which measure pair\nconcordance and the correlation between fluctuations in two rankings,\nrespectively, - as a correlation function does in thermodynamics, are\ncalculated for finding rank correlation (if any) between demography and wealth.\nResults show non only global disparities for the whole (country) set, but also\n(regional) disparities, when comparing the number of cities in regions, the\nnumber of inhabitants in cities and that in regions, as well as when comparing\nthe aggregated tax income of the cities and that of regions. Different outliers\nare pointed out and justified. Interestingly, two classes of cities in the\ncountry and two classes of regions in the country are found. \"Common sense\"\nsocial, political, and economic considerations sustain the findings. More\nimportantly, the methods show that they allow to distinguish communities, very\nclearly, when specific criteria are numerically sound. A specific modeling for\nthe findings is presented, i.e. for the doubly decreasing power law and the two\nphase system, based on statistics theory, e.g., urn filling. The model ideas\ncan be expected to hold when similar rank relationship features are observed in\nfields. It is emphasized that the analysis makes more sense than one through a\nPearson value-value correlation analysis.\n"
    },
    {
        "paper_id": 1506.02507,
        "authors": "Thibault Jaisson",
        "title": "Liquidity and Impact in Fair Markets",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a theory which applies to any market dynamics that satisfy a fair\nmarket assumption on the nullity of the average profit of simple market making\nstrategies. We show that for any such fair market, there exists a martingale\nfair price which corresponds to the average liquidation value (at the ask or\nthe bid) of an infinitesimal quantity of stock. We show that this fair price is\na natural reference price to compute the ex post gain of limit orders. Using\nonly the fair market assumption, we link the spread to the impact of market\norders on the fair price. We use our definition of the fair price to build\nempirical tests of the relevance of this notion whose results are consistent\nwith our theoretical predictions.\n"
    },
    {
        "paper_id": 1506.02521,
        "authors": "Viktors Ajevskis",
        "title": "Nonlocal Solutions to Dynamic Equilibrium Models: The Approximate Stable\n  Manifolds Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents a method for constructing a sequence of approximate\nsolutions of increasing accuracy to general equilibrium models on nonlocal\ndomains. The method is based on a technique originated from dynamical systems\ntheory. The approximate solutions are constructed employing the Contraction\nMapping Theorem and the fact that solutions to general equilibrium models\nconverge to a steady state. The approach allows deriving the a priori and a\nposteriori approximation errors of the solutions. Under certain nonlocal\nconditions we prove the convergence of the approximate solutions to the true\nsolution and hence the Stable Manifold Theorem. We also show that the proposed\napproach can be treated as a rigorous proof of convergence for the extended\npath algorithm to the true solution in a class of nonlinear rational\nexpectation models.\n"
    },
    {
        "paper_id": 1506.02522,
        "authors": "Viktors Ajevskis",
        "title": "Semi-Global Solutions to DSGE Models: Perturbation around a\n  Deterministic Path",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes an approach based on a perturbation technique to\nconstruct global solutions to dynamic stochastic general equilibrium models\n(DSGE). The main idea is to expand a solution in a series of powers of a small\nparameter scaling the uncertainty in the economy around a solution to the\ndeterministic model, i.e. the model where the volatility of the shocks\nvanishes. If a deterministic path is global in state variables, then so are the\nconstructed solutions to the stochastic model, whereas these solutions are\nlocal in the scaling parameter. Under the assumption that a deterministic path\nis already known the higher order terms in the expansion are obtained\nrecursively by solving linear rational expectations models with time-varying\nparameters. The present work also proposes a method rested on backward\nrecursion for solving general systems of linear rational expectations models\nwith time-varying parameters and determines the conditions under which the\nsolutions of the method exist.\n"
    },
    {
        "paper_id": 1506.02789,
        "authors": "Kensuke Ishitani and Takashi Kato",
        "title": "Theoretical and Numerical Analysis of an Optimal Execution Problem with\n  Uncertain Market Impact",
        "comments": "24 pages, 14 figures. Continuation of the paper arXiv:1301.6485",
        "journal-ref": "Communications on Stochastic Analysis, 9(3), 343-366 (2015)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is a continuation of Ishitani and Kato (2015), in which we derived\na continuous-time value function corresponding to an optimal execution problem\nwith uncertain market impact as the limit of a discrete-time value function.\nHere, we investigate some properties of the derived value function. In\nparticular, we show that the function is continuous and has the semigroup\nproperty, which is strongly related to the Hamilton-Jacobi-Bellman\nquasi-variational inequality. Moreover, we show that noise in market impact\ncauses risk-neutral assessment to underestimate the impact cost. We also study\ntypical examples under a log-linear/quadratic market impact function with\nGamma-distributed noise.\n"
    },
    {
        "paper_id": 1506.02802,
        "authors": "Paolo Guasoni, Eberhard Mayerhofer",
        "title": "The Limits of Leverage",
        "comments": "4 figures",
        "journal-ref": "Mathematical Finance, Vol 29, 249-284, 2019",
        "doi": "10.1111/mafi.12172",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When trading incurs proportional costs, leverage can scale an asset's return\nonly up to a maximum multiple, which is sensitive to its volatility and\nliquidity. In a model with one safe and one risky asset, with constant\ninvestment opportunities and proportional costs, we find strategies that\nmaximize long term returns given average volatility. As leverage increases,\nrising rebalancing costs imply declining Sharpe ratios. Beyond a critical\nlevel, even returns decline. Holding the Sharpe ratio constant, higher asset\nvolatility leads to superior returns through lower costs.\n"
    },
    {
        "paper_id": 1506.0294,
        "authors": "Luca Di Persio",
        "title": "Autoregressive approaches to import-export time series I: basic\n  techniques",
        "comments": "Published at http://dx.doi.org/10.15559/15-VMSTA22 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2015, Vol. 2, No. 1,\n  51-65",
        "doi": "10.15559/15-VMSTA22",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is the first part of a project dealing with an in-depth study of\neffective techniques used in econometrics in order to make accurate forecasts\nin the concrete framework of one of the major economies of the most productive\nItalian area, namely the province of Verona. In particular, we develop an\napproach mainly based on vector autoregressions, where lagged values of two or\nmore variables are considered, Granger causality, and the stochastic trend\napproach useful to work with the cointegration phenomenon. Latter techniques\nconstitute the core of the present paper, whereas in the second part of the\nproject, we present how these approaches can be applied to economic data at our\ndisposal in order to obtain concrete analysis of import--export behavior for\nthe considered productive area of Verona.\n"
    },
    {
        "paper_id": 1506.03106,
        "authors": "Lubos Hanus and Lukas Vacha",
        "title": "Business cycle synchronization within the European Union: A wavelet\n  cohesion approach",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we map the process of business cycle synchronization across\nthe European Union. We study this synchronization by applying wavelet\ntechniques, particularly the cohesion measure with time-varying weights. This\nnovel approach allows us to study the dynamic relationship among selected\ncountries from a different perspective than the usual time-domain models.\nAnalyzing monthly data from 1990 to 2014, we show an increasing co-movement of\nthe Visegrad countries with the European Union after the countries began\npreparing for the accession to the European Union. With particular focus on the\nVisegrad countries we show that participation in a currency union possibly\nincreases the co-movement. Furthermore, we find a high degree of\nsynchronization in long-term horizons by analyzing the Visegrad Four and\nSouthern European countries' synchronization with the core countries of the\nEuropean Union.\n"
    },
    {
        "paper_id": 1506.03347,
        "authors": "Filip Smolik and Lukas Vacha",
        "title": "Time-scale analysis of co-movement in EU sovereign bond markets",
        "comments": "new version is too large",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the co-movement of the 10-year sovereign bond yields of 11 EU\ncountries. Our analysis is focused mainly on changes in co-movement during the\nfinancial crisis period, especially around two significant dates - the fall of\nLehman Brothers, September 15, 2008, and the announcement of the increase of\nGreece's public deficit on October 20, 2009. We study co-movement dynamics\nusing wavelet analysis, which allows us to observe how co-movement changes\nacross frequencies and over time. We divide the countries into three groups:\nthe core of the Eurozone, the periphery of the Eurozone and the states outside\nthe Eurozone. The results indicate that co-movement decreased considerably\nduring the crisis period for all country pairs but that there are significant\ndifferences among the groups. Furthermore, we demonstrate that the co-movement\nof bond yields is frequency (scale) dependent.\n"
    },
    {
        "paper_id": 1506.03414,
        "authors": "Ole Peters and Alexander Adamou",
        "title": "An evolutionary advantage of cooperation",
        "comments": "16 pages, 2 figures",
        "journal-ref": "Phil. Trans. R. Soc. A. 380:20200425 (2022)",
        "doi": "10.1098/rsta.2020.0425",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cooperation is a persistent behavioral pattern of entities pooling and\nsharing resources. Its ubiquity in nature poses a conundrum. Whenever two\nentities cooperate, one must willingly relinquish something of value to the\nother. Why is this apparent altruism favored in evolution? Classical solutions\nassume a net fitness gain in a cooperative transaction which, through\nreciprocity or relatedness, finds its way back from recipient to donor. We seek\nthe source of this fitness gain. Our analysis rests on the insight that\nevolutionary processes are typically multiplicative and noisy. Fluctuations\nhave a net negative effect on the long-time growth rate of resources but no\neffect on the growth rate of their expectation value. This is an example of\nnon-ergodicity. By reducing the amplitude of fluctuations, pooling and sharing\nincreases the long-time growth rate for cooperating entities, meaning that\ncooperators outgrow similar non-cooperators. We identify this increase in\ngrowth rate as the net fitness gain, consistent with the concept of geometric\nmean fitness in the biological literature. This constitutes a fundamental\nmechanism for the evolution of cooperation. Its minimal assumptions make it a\ncandidate explanation of cooperation in settings too simple for other fitness\ngains, such as emergent function and specialization, to be probable. One such\nexample is the transition from single cells to early multicellular life.\n"
    },
    {
        "paper_id": 1506.03564,
        "authors": "Fabio Derendinger",
        "title": "Copula based hierarchical risk aggregation - Tree dependent sampling and\n  the space of mild tree dependence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ability to adequately model risks is crucial for insurance companies. The\nmethod of \"Copula-based hierarchical risk aggregation\" by Arbenz et al. offers\na flexible way in doing so and has attracted much attention recently. We\nbriefly introduce the aggregation tree model as well as the sampling algorithm\nproposed by they authors.\n  An important characteristic of the model is that the joint distribution of\nall risk is not fully specified unless an additional assumption (known as\n\"conditional independence assumption\") is added. We show that there is\nnumerical evidence that the sampling algorithm yields an approximation of the\ndistribution uniquely specified by the conditional independence assumption. We\npropose a modified algorithm and provide a proof that under certain conditions\nthe said distribution is indeed approximated by our algorithm.\n  We further determine the space of feasible distributions for a given\naggregation tree model in case we drop the conditional independence assumption.\nWe study the impact of the input parameters and the tree structure, which\nallows conclusions of the way the aggregation tree should be designed.\n"
    },
    {
        "paper_id": 1506.03597,
        "authors": "Mario Alberto Annunziata, Alberto Petri, Giorgio Pontuale, and Andrea\n  Zaccaria",
        "title": "How log-normal is your country? An analysis of the statistical\n  distribution of the exported volumes of products",
        "comments": "10 pages, 5 figures, submitted to IWcee15 conference",
        "journal-ref": null,
        "doi": "10.1140/epjst/e2015-50320-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have considered the statistical distributions of the volumes of the\ndifferent products exported by 148 countries. We have found that the form of\nthese distributions is not unique but heavily depends on the level of\ndevelopment of the nation, as expressed by macroeconomic indicators like GDP,\nGDP per capita, total export and a recently introduced measure for countries'\neconomic complexity called fitness. We have identified three major classes: a)\nan incomplete log-normal shape, truncated on the left side, for the less\ndeveloped countries, b) a complete log-normal, with a wider range of volumes,\nfor nations characterized by intermediate economy, and c) a strongly asymmetric\nshape for countries with a high degree of development. The ranking curves of\nthe exported volumes from each country seldom cross each other, showing a clear\nhierarchy of export volumes. Finally, the log-normality hypothesis has been\nchecked for the distributions of all the 148 countries through different tests,\nKolmogorov-Smirnov and Cramer-Von Mises, confirming that it cannot be rejected\nonly for the countries of intermediate economy.\n"
    },
    {
        "paper_id": 1506.03621,
        "authors": "Anindya Goswami and Sanket Nandan",
        "title": "Convergence of Estimated Option Price in a Regime switching Market",
        "comments": "11 pages, 2 figures",
        "journal-ref": "Indian Journal of Pure and Applied Mathematics, 47(2016), no. 2,\n  169-182",
        "doi": "10.1007/s13226-016-0182-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an observed generalized semi-Markov regime, estimation of transition rate\nof regime switching leads towards calculation of locally risk minimizing option\nprice. Despite the uniform convergence of estimated step function of transition\nrate, to meet the existence of classical solution of the modified price\nequation, the estimator is approximated in the class of smooth functions and\nfurthermore, the convergence is established. Later, the existence of the\nsolution of the modified price equation is verified and the point-wise\nconvergence of such approximation of option price is proved to answer the\ntractability of its application in Finance. To demonstrate the consistency in\nresult a numerical experiment has been reported.\n"
    },
    {
        "paper_id": 1506.03708,
        "authors": "Adri\\'an Carro, Ra\\'ul Toral, Maxi San Miguel",
        "title": "Markets, herding and response to external information",
        "comments": "30 pages, 8 figures. Thoroughly revised and updated version of\n  arXiv:1302.6477",
        "journal-ref": "PLoS ONE 10(7): e0133287, July 23, 2015",
        "doi": "10.1371/journal.pone.0133287",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on the influence of external sources of information upon financial\nmarkets. In particular, we develop a stochastic agent-based market model\ncharacterized by a certain herding behavior as well as allowing traders to be\ninfluenced by an external dynamic signal of information. This signal can be\ninterpreted as a time-varying advertising, public perception or rumor, in favor\nor against one of two possible trading behaviors, thus breaking the symmetry of\nthe system and acting as a continuously varying exogenous shock. As an\nillustration, we use a well-known German Indicator of Economic Sentiment as\ninformation input and compare our results with Germany's leading stock market\nindex, the DAX, in order to calibrate some of the model parameters. We study\nthe conditions for the ensemble of agents to more accurately follow the\ninformation input signal. The response of the system to the external\ninformation is maximal for an intermediate range of values of a market\nparameter, suggesting the existence of three different market regimes:\namplification, precise assimilation and undervaluation of incoming information.\n"
    },
    {
        "paper_id": 1506.03758,
        "authors": "Jonathan Donier and Jean-Philippe Bouchaud",
        "title": "From Walras' auctioneer to continuous time double auctions: A general\n  dynamic theory of supply and demand",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa4e8e",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In standard Walrasian auctions, the price of a good is defined as the point\nwhere the supply and demand curves intersect. Since both curves are generically\nregular, the response to small perturbations is linearly small. However, a\ncrucial ingredient is absent of the theory, namely transactions themselves.\nWhat happens after they occur? To answer the question, we develop a dynamic\ntheory for supply and demand based on agents with heterogeneous beliefs. When\nthe inter-auction time is infinitely long, the Walrasian mechanism is\nrecovered. When transactions are allowed to happen in continuous time, a\npeculiar property emerges: close to the price, supply and demand vanish\nquadratically, which we empirically confirm on the Bitcoin. This explains why\nprice impact in financial markets is universally observed to behave as the\nsquare root of the excess volume. The consequences are important, as they imply\nthat the very fact of clearing the market makes prices hypersensitive to small\nfluctuations.\n"
    },
    {
        "paper_id": 1506.03898,
        "authors": "Takuji Arai, Yuto Imai and Ryoichi Suzuki",
        "title": "Numerical analysis on local risk-minimization forexponential L\\'evy\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We illustrate how to compute local risk minimization (LRM) of call options\nfor exponential L\\'evy models. We have previously obtained a representation of\nLRM for call options; here we transform it into a form that allows use of the\nfast Fourier transform method suggested by Carr & Madan. In particular, we\nconsider Merton jump-diffusion models and variance gamma models as concrete\napplications.\n"
    },
    {
        "paper_id": 1506.03917,
        "authors": "Norbert Agbeko",
        "title": "On the Characteristics of the Free Market in a Cooperative Society",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The key characteristic of a true free market economy is that exchanges are\nentirely voluntary. When there is a monopoly in the creation of currency as we\nhave in today's markets, you no longer have a true free market. Features of the\ncurrent economic system such as central banking and taxation would be\nnonexistent in a free market. This paper examines how currency monopoly leads\nto the instabilities and imbalances that we see in today's economy. It also\nproposes that currencies should emerge from the voluntary exchange of goods and\nservices, and studies economic interaction across all scales, by considering\neconomic action in cases where the self-interests of individuals are\ncoincident. By examining the voluntary exchange of goods and services at the\nscale of an entire society, it is shown that a new currency system, which\nresolves a lot of the problems caused by the current fiat currency system,\nemerges naturally from the free market. The new currency system is robust and\nefficient, and provides a way for public goods and services to be provided, and\nits providers compensated, without the need for direct taxation.\n"
    },
    {
        "paper_id": 1506.04063,
        "authors": "Gaoyue Guo, Xiaolu Tan and Nizar Touzi",
        "title": "Optimal Skorokhod embedding under finitely-many marginal constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Skorokhod embedding problem aims to represent a given probability measure\non the real line as the distribution of Brownian motion stopped at a chosen\nstopping time. In this paper, we consider an extension of the optimal Skorokhod\nembedding problem to the case of finitely-many marginal constraints. Using the\nclassical convex duality approach together with the optimal stopping theory, we\nobtain the duality results which are formulated by means of probability\nmeasures on an enlarged space. We also relate these results to the problem of\nmartingale optimal transport under multiple marginal constraints.\n"
    },
    {
        "paper_id": 1506.04125,
        "authors": "V\\'eronique Maume-Deschamps (ICJ), Didier Rulli\\`ere (SAF), Khalil\n  Said (SAF)",
        "title": "A risk management approach to capital allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European insurance sector will soon be faced with the application of\nSolvency 2 regulation norms. It will create a real change in risk management\npractices. The ORSA approach of the second pillar makes the capital allocation\nan important exercise for all insurers and specially for groups. Considering\nmulti-branches firms, capital allocation has to be based on a multivariate risk\nmodeling. Several allocation methods are present in the literature and insurers\npractices. In this paper, we present a new risk allocation method, we study its\ncoherence using an axiomatic approach, and we try to define what the best\nallocation choice for an insurance group is.\n"
    },
    {
        "paper_id": 1506.04227,
        "authors": "Steven E. Pav",
        "title": "Safety Third: Roy's Criterion and Higher Order Moments",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Roy's `Safety First' criterion for selecting one risky asset from many is\nadapted to the case of non-normal returns, via Cornish Fisher expansion. The\nresulting investment objective is consistent with first order stochastic\ndominance, and is equal to the Sharpe ratio for the case of normal returns. An\ninvestor selecting assets via this objective is not universally attracted to\npositive skew, rather the preference for skew depends on term, the expected\nreturn and the disastrous rate of return.\n"
    },
    {
        "paper_id": 1506.04663,
        "authors": "Vahan Nanumyan, Antonios Garas, Frank Schweitzer",
        "title": "The Network of Counterparty Risk: Analysing Correlations in OTC\n  Derivatives",
        "comments": "36 pages, 18 figures, 2 tables",
        "journal-ref": "PLoS ONE 10(9): e0136638, 2015",
        "doi": "10.1371/journal.pone.0136638",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Counterparty risk denotes the risk that a party defaults in a bilateral\ncontract. This risk not only depends on the two parties involved, but also on\nthe risk from various other contracts each of these parties holds. In rather\ninformal markets, such as the OTC (over-the-counter) derivative market,\ninstitutions only report their aggregated quarterly risk exposure, but no\ndetails about their counterparties. Hence, little is known about the\ndiversification of counterparty risk. In this paper, we reconstruct the\nweighted and time-dependent network of counterparty risk in the OTC derivatives\nmarket of the United States between 1998 and 2012. To proxy unknown bilateral\nexposures, we first study the co-occurrence patterns of institutions based on\ntheir quarterly activity and ranking in the official report. The network\nobtained this way is further analysed by a weighted k-core decomposition, to\nreveal a core-periphery structure. This allows us to compare the activity-based\nranking with a topology-based ranking, to identify the most important\ninstitutions and their mutual dependencies. We also analyse correlations in\nthese activities, to show strong similarities in the behavior of the core\ninstitutions. Our analysis clearly demonstrates the clustering of counterparty\nrisk in a small set of about a dozen US banks. This not only increases the\ndefault risk of the central institutions, but also the default risk of\nperipheral institutions which have contracts with the central ones. Hence, all\ninstitutions indirectly have to bear (part of) the counterparty risk of all\nothers, which needs to be better reflected in the price of OTC derivatives.\n"
    },
    {
        "paper_id": 1506.04698,
        "authors": "Jan-Henrik Steg and Jacco Thijssen",
        "title": "Quick or Persistent? Strategic Investment Demanding Versatility",
        "comments": "34 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyse a dynamic model of investment under uncertainty in a\nduopoly, in which each firm has an option to switch from the present market to\na new market. We construct a subgame perfect equilibrium in mixed strategies\nand show that both preemption and attrition can occur along typical equilibrium\npaths. In order to determine the attrition region a two-dimensional constrained\noptimal stopping problem needs to be solved, for which we characterize the\nnon-trivial stopping boundary in the state space. We explicitly determine\nMarkovian equilibrium stopping rates in the attrition region and show that\nthere is always a positive probability of eventual preemption, contrasting the\ndeterministic version of the model. A simulation-based numerical example\nillustrates the model and shows the relative likelihoods of investment taking\nplace in attrition and preemption regions.\n"
    },
    {
        "paper_id": 1506.04869,
        "authors": "Shuhua Chang, Xinyu Wang and Alexander Shananin",
        "title": "Modeling and Computation of Mean Field Equilibria in Producers' Game\n  with Emission Permits Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  In this paper, we present a mean field game to model the production behaviors\nof a very large number of producers, whose carbon emissions are regulated by\ngovernment. Especially, an emission permits trading scheme is considered in our\nmodel, in which each enterprise can trade its own permits flexibly. By means of\nthe mean field equilibrium, we obtain a Hamilton-Jacobi-Bellman (HJB) equation\ncoupled with a Kolmogorov equation, which are satisfied by the adjoint state\nand the density of producers (agents), respectively. Then, we propose a\nso-called fitted finite volume method to solve the HJB equation and the\nKolmogorov equation. The efficiency and the usefulness of this method are\nillustrated by the numerical experiments. Under different conditions, the\nequilibrium states as well as the effects of the emission permits price are\nexamined, which demonstrates that the emission permits trading scheme\ninfluences the producers' behaviors, that is, more populations would like to\nchoose a lower rather than a higher emission level when the emission permits\nare expensive.\n"
    },
    {
        "paper_id": 1506.0488,
        "authors": "Viktors Ajevskis",
        "title": "An Exchange Rate Target Zone Model with a Terminal Condition and\n  Mean-Reverting Fundamentals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a target zones exchange rate model with a terminal\ncondition of entering a currency zone. It is assumed that the exchange rate is\na function of the fundamental and time. Another essential assumptions of the\nmodel is that the fundamental process is bounded inside a band and that\nterminal condition for the exchange rate holds. The fundamental is specified in\ntwo ways: as a regulated Brownian motion and Ornstein-Uhlenbeck processes. For\nthe case of the Brownian motion process the closed form solution of the problem\nis obtained, whereas for the Ornstein-Uhlenbeck process the closed form\nsolution does not exist, therefore we had to use numerical method for solving\nof the problem. Both specifications are compared numerically.\n"
    },
    {
        "paper_id": 1506.05376,
        "authors": "Jonathan K. Budd and Peter G. Taylor",
        "title": "Calculating optimal limits for transacting credit card customers",
        "comments": "17 pages. Submitted to the Journal of the Operational Research\n  Society on 20th May 2015. This version updated with minor corrections:\n  corrected typo on pg. 3; fixed incorrect index in equation 1; added a\n  definition for the profit function on pg. 3",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a model of credit card profitability, assuming that the\ncard-holder always pays the full outstanding balance. The motivation for the\nmodel is to calculate an optimal credit limit, which requires an expression for\nthe expected outstanding balance. We derive its Laplace transform, assuming\nthat purchases are made according to a marked point process and that there is a\nsimplified balance control policy in place to prevent the credit limit being\nexceeded. We calculate optimal limits for a compound Poisson process example\nand show that the optimal limit scales with the distribution of the purchasing\nprocess and that the probability of exceeding the optimal limit remains\nconstant. We establish a connection with the classic newsvendor model and use\nthis to calculate bounds on the optimal limit for a more complicated balance\ncontrol policy. Finally, we apply our model to real credit card purchase data.\n"
    },
    {
        "paper_id": 1506.05418,
        "authors": "Yong Tao",
        "title": "Universal Laws of Human Society's Income Distribution",
        "comments": "Newtonian equations in Economic system",
        "journal-ref": "Physica A 435 (2015) 89-94",
        "doi": "10.1016/j.physa.2015.05.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  General equilibrium equations in economics play the same role with many-body\nNewtonian equations in physics. Accordingly, each solution of the general\nequilibrium equations can be regarded as a possible microstate of the economic\nsystem. Since Arrow's Impossibility Theorem and Rawls' principle of social\nfairness will provide a powerful support for the hypothesis of equal\nprobability, then the principle of maximum entropy is available in a just and\nequilibrium economy so that an income distribution will occur spontaneously\n(with the largest probability). Remarkably, some scholars have observed such an\nincome distribution in some democratic countries, e.g. USA. This result implies\nthat the hypothesis of equal probability may be only suitable for some \"fair\"\nsystems (economic or physical systems). From this meaning, the non-equilibrium\nsystems may be \"unfair\" so that the hypothesis of equal probability is\nunavailable.\n"
    },
    {
        "paper_id": 1506.05497,
        "authors": "Christopher W. Miller and Insoon Yang",
        "title": "Optimal Dynamic Contracts for a Large-Scale Principal-Agent Hierarchy: A\n  Concavity-Preserving Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a continuous-time contract whereby a top-level player can\nincentivize a hierarchy of players below him to act in his best interest\ndespite only observing the output of his direct subordinate. This paper extends\nSannikov's approach from a situation of asymmetric information between a\nprincipal and an agent to one of hierarchical information between several\nplayers. We develop an iterative algorithm for constructing an incentive\ncompatible contract and define the correct notion of concavity which must be\npreserved during iteration. We identify conditions under which a dynamic\nprogramming construction of an optimal dynamic contract can be reduced to only\na one-dimensional state space and one-dimensional control set, independent of\nthe size of the hierarchy. In this sense, our results contribute to the\napplicability of dynamic programming on dynamic contracts for a large-scale\nprincipal-agent hierarchy.\n"
    },
    {
        "paper_id": 1506.05895,
        "authors": "Paolo Guasoni, Mikl\\'os R\\'asonyi",
        "title": "Hedging, arbitrage and optimality with superlinear frictions",
        "comments": "Published at http://dx.doi.org/10.1214/14-AAP1043 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 4, 2066-2095",
        "doi": "10.1214/14-AAP1043",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a continuous-time model with multiple assets described by c\\`{a}dl\\`{a}g\nprocesses, this paper characterizes superhedging prices, absence of arbitrage,\nand utility maximizing strategies, under general frictions that make execution\nprices arbitrarily unfavorable for high trading intensity. Such frictions\ninduce a duality between feasible trading strategies and shadow execution\nprices with a martingale measure. Utility maximizing strategies exist even if\narbitrage is present, because it is not scalable at will.\n"
    },
    {
        "paper_id": 1506.05911,
        "authors": "Lorenz Schneider and Bertrand Tavin",
        "title": "Seasonal Stochastic Volatility and Correlation together with the\n  Samuelson Effect in Commodity Futures Markets",
        "comments": "24 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a multi-factor stochastic volatility model based on the\nCIR/Heston volatility process that incorporates seasonality and the Samuelson\neffect. First, we give conditions on the seasonal term under which the\ncorresponding volatility factor is well-defined. These conditions appear to be\nrather mild. Second, we calculate the joint characteristic function of two\nfutures prices for different maturities in the proposed model. This\ncharacteristic function is analytic. Finally, we provide numerical\nillustrations in terms of implied volatility and correlation produced by the\nproposed model with five different specifications of the seasonality pattern.\nThe model is found to be able to produce volatility smiles at the same time as\na volatility term-structure that exhibits the Samuelson effect with a seasonal\ncomponent. Correlation, instantaneous or implied from calendar spread option\nprices via a Gaussian copula, is also found to be seasonal.\n"
    },
    {
        "paper_id": 1506.0599,
        "authors": "Moshe A. Milevsky, Virginia R. Young",
        "title": "Annuitization and asset allocation",
        "comments": null,
        "journal-ref": "J of Economic Dynamics and Control 31(9) (2007), 3138-3177",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the optimal annuitization, investment and consumption\nstrategies of a utility-maximizing retiree facing a stochastic time of death\nunder a variety of institutional restrictions. We focus on the impact of aging\non the optimal purchase of life annuities which form the basis of most Defined\nBenefit pension plans. Due to adverse selection, acquiring a lifetime payout\nannuity is an irreversible transaction that creates an incentive to delay.\nUnder the institutional all-or-nothing arrangement where annuitization must\ntake place at one distinct point in time (i.e. retirement), we derive the\noptimal age at which to annuitize and develop a metric to capture the loss from\nannuitizing prematurely. In contrast, under an open-market structure where\nindividuals can annuitize any fraction of their wealth at anytime, we locate a\ngeneral optimal annuity purchasing policy. In this case, we find that an\nindividual will initially annuitize a lump sum and then buy annuities to keep\nwealth to one side of a separating ray in wealth-annuity space. We believe our\npaper is the first to integrate life annuity products into the portfolio choice\nliterature while taking into account realistic institutional restrictions which\nare unique to the market for mortality-contingent claims.\n"
    },
    {
        "paper_id": 1506.06069,
        "authors": "Daniela Bubboloni and Michele Gori",
        "title": "Resolute refinements of social choice correspondences",
        "comments": "arXiv admin note: text overlap with arXiv:1503.04028",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many classical social choice correspondences are resolute only in the case of\ntwo alternatives and an odd number of individuals. Thus, in most cases, they\nadmit several resolute refinements, each of them naturally interpreted as a\ntie-breaking rule, satisfying different properties. In this paper we look for\nclasses of social choice correspondences which admit resolute refinements\nfulfilling suitable versions of anonymity and neutrality. In particular,\nsupposing that individuals and alternatives have been exogenously partitioned\ninto subcommittees and subclasses, we find out arithmetical conditions on the\nsizes of subcommittees and subclasses that are necessary and sufficient for\nmaking any social choice correspondence which is efficient, anonymous with\nrespect to subcommittees, neutral with respect to subclasses and possibly\nimmune to the reversal bias admit a resolute refinement sharing the same\nproperties.\n"
    },
    {
        "paper_id": 1506.0618,
        "authors": "Matthew Lorig and Ronnie Sircar",
        "title": "Portfolio Optimization under Local-Stochastic Volatility: Coefficient\n  Taylor Series Approximations & Implied Sharpe Ratio",
        "comments": "27 pages, 2figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the finite horizon Merton portfolio optimization problem in a\ngeneral local-stochastic volatility setting. Using model coefficient expansion\ntechniques, we derive approximations for the both the value function and the\noptimal investment strategy. We also analyze the `implied Sharpe ratio' and\nderive a series approximation for this quantity. The zeroth-order approximation\nof the value function and optimal investment strategy correspond to those\nobtained by Merton (1969) when the risky asset follows a geometric Brownian\nmotion. The first-order correction of the value function can, for general\nutility functions, be expressed as a differential operator acting on the\nzeroth-order term. For power utility functions, higher order terms can also be\ncomputed as a differential operator acting on the zeroth-order term. We give a\nrigorous accuracy bound for the higher order approximations in this case in\npure stochastic volatility models. A number of examples are provided in order\nto demonstrate numerically the accuracy of our approximations.\n"
    },
    {
        "paper_id": 1506.06568,
        "authors": "Carlo Marinelli, Stefano d'Addona",
        "title": "Nonparametric estimates of pricing functionals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the empirical performance of several non-parametric estimators of\nthe pricing functional for European options, using historical put and call\nprices on the S&P500 during the year 2012. Two main families of estimators are\nconsidered, obtained by estimating the pricing functional directly, and by\nestimating the (Black-Scholes) implied volatility surface, respectively. In\neach case simple estimators based on linear interpolation are constructed, as\nwell as more sophisticated ones based on smoothing kernels, \\`a la\nNadaraya-Watson. The results based on the analysis of the empirical pricing\nerrors in an extensive out-of-sample study indicate that a simple approach\nbased on the Black-Scholes formula coupled with linear interpolation of the\nvolatility surface outperforms, both in accuracy and computational speed, all\nother methods.\n"
    },
    {
        "paper_id": 1506.06608,
        "authors": "Matteo Burzoni, Marco Frittelli, Marco Maggis",
        "title": "Model-free Superhedging Duality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a model free discrete time financial market, we prove the superhedging\nduality theorem, where trading is allowed with dynamic and semi-static\nstrategies. We also show that the initial cost of the cheapest portfolio that\ndominates a contingent claim on every possible path $\\omega \\in \\Omega$, might\nbe strictly greater than the upper bound of the no-arbitrage prices. We\ntherefore characterize the subset of trajectories on which this duality gap\ndisappears and prove that it is an analytic set.\n"
    },
    {
        "paper_id": 1506.06624,
        "authors": "J.L. Bretagnolle, P. Ouwehand",
        "title": "The Levy-Ito Decomposition theorem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This a free translation with additional explanations of {\\em Processus \\`a\nAccroissement Independants Chapitre I: La D\\'ecomposition de Paul L\\'evy}, by\nJ.L. Bretagnolle, in {\\em Ecole d'Et\\'e de Probabilit\\'es}, Lecture Notes in\nMathematics 307, Springer 1973. The L\\'evy-Khintchine representation of\ninfinitely divisible distributions is obtained as a by-product.\n  As this proof makes use of martingale methods, it is pedagogically more\nsuitable for students of financial mathematics than some other approaches. It\nis hoped that the end notes will also help to make the proof more accessible to\nthis audience.\n"
    },
    {
        "paper_id": 1506.06664,
        "authors": "Rebekka Burkholz, Matt V. Leduc, Antonios Garas, Frank Schweitzer",
        "title": "Systemic risk in multiplex networks with asymmetric coupling and\n  threshold feedback",
        "comments": "18 pages, 5 figures",
        "journal-ref": "Physica D: Nonlinear Phenomena, Vol. 323--324, 64--72 (2016)",
        "doi": "10.1016/j.physd.2015.10.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study cascades on a two-layer multiplex network, with asymmetric feedback\nthat depends on the coupling strength between the layers. Based on an\nanalytical branching process approximation, we calculate the systemic risk\nmeasured by the final fraction of failed nodes on a reference layer. The\nresults are compared with the case of a single layer network that is an\naggregated representation of the two layers. We find that systemic risk in the\ntwo-layer network is smaller than in the aggregated one only if the coupling\nstrength between the two layers is small. Above a critical coupling strength,\nsystemic risk is increased because of the mutual amplification of cascades in\nthe two layers. We even observe sharp phase transitions in the cascade size\nthat are less pronounced on the aggregated layer. Our insights can be applied\nto a scenario where firms decide whether they want to split their business into\na less risky core business and a more risky subsidiary business. In most cases,\nthis may lead to a drastic increase of systemic risk, which is underestimated\nin an aggregated approach.\n"
    },
    {
        "paper_id": 1506.06669,
        "authors": "Rachael Meager",
        "title": "Understanding the Impact of Microcredit Expansions: A Bayesian\n  Hierarchical Analysis of 7 Randomised Experiments",
        "comments": "This draft is preliminary and incomplete; future versions of this\n  paper will contain substantive additional results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bayesian hierarchical models are a methodology for aggregation and synthesis\nof data from heterogeneous settings, used widely in statistics and other\ndisciplines. I apply this framework to the evidence from 7 randomized\nexperiments of expanding access to microcredit to assess the general impact of\nthe intervention on household outcomes and the heterogeneity in this impact\nacross sites. The results suggest that the effect of microcredit is likely to\nbe positive but small relative to control group average levels, and the\npossibility of a negative impact cannot be ruled out. By contrast, common\nmeta-analytic methods that pool all the data without assessing the\nheterogeneity misleadingly produce \"statistically significant\" results in 2 of\nthe 6 household outcomes. Standard pooling metrics for the studies indicate on\naverage 60% pooling on the treatment effects, suggesting that the site-specific\neffects are reasonably externally valid, and thus informative for each other\nand for the general case. The cross-study heterogeneity is almost entirely\ngenerated by heterogeneous effects for the 27% households who previously\noperated businesses before microcredit expansion, although this group is likely\nto see much larger impacts overall. A Ridge regression procedure to assess the\ncorrelations between site-specific covariates and treatment effects indicates\nthat the remaining heterogeneity is strongly correlated with differences in\neconomic variables, but not with differences in study design protocols. The\naverage interest rate and the average loan size have the strongest correlation\nwith the treatment effects, and both are negative.\n"
    },
    {
        "paper_id": 1506.06975,
        "authors": "Johan Dahlin, Mattias Villani and Thomas B. Sch\\\"on",
        "title": "Bayesian optimisation for fast approximate inference in state-space\n  models with intractable likelihoods",
        "comments": "24 pages, 7 figures. Submitted to journal for review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of approximate Bayesian parameter inference in\nnon-linear state-space models with intractable likelihoods. Sequential Monte\nCarlo with approximate Bayesian computations (SMC-ABC) is one approach to\napproximate the likelihood in this type of models. However, such approximations\ncan be noisy and computationally costly which hinders efficient implementations\nusing standard methods based on optimisation and Monte Carlo methods. We\npropose a computationally efficient novel method based on the combination of\nGaussian process optimisation and SMC-ABC to create a Laplace approximation of\nthe intractable posterior. We exemplify the proposed algorithm for inference in\nstochastic volatility models with both synthetic and real-world data as well as\nfor estimating the Value-at-Risk for two portfolios using a copula model. We\ndocument speed-ups of between one and two orders of magnitude compared to\nstate-of-the-art algorithms for posterior inference.\n"
    },
    {
        "paper_id": 1506.06979,
        "authors": "Dmitry Lesnik",
        "title": "Intrinsic Storage Valuation by Variational Analysis",
        "comments": "arXiv admin note: text overlap with arXiv:1011.1234",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The mathematical problem concerning intrinsic storage optimisation is\nformulated and solved by means of variational analysis. The solution, though\nobtained in implicit form, still sheds light on many important features of the\noptimal exercise strategy. It is shown how the solution depends on different\nconstraint types including carry cost and cycle constraint. Additionally, the\nrelationship between intrinsic and stochastic solutions is investigated. In\nparticular, we show that the optimal stochastic exercise decision is always\nclose to the intrinsic one.\n"
    },
    {
        "paper_id": 1506.06997,
        "authors": "Pierre M. Blacque-Florentin and Badr Missaoui",
        "title": "Nonparametric and arbitrage-free construction of call surfaces using\n  l1-recovery",
        "comments": "27 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to the application of an $l_1$ -minimisation technique\nto construct an arbitrage-free call-option surface. We propose a\nnononparametric approach to obtaining model-free call option surfaces that are\nperfectly consistent with market quotes and free of static arbitrage. The\napproach is inspired from the compressed-sensing framework that is used in\nsignal processing to deal with under-sampled signals. We address the problem of\nfitting the call-option surface to sparse option data. To illustrate the\nmethodology, we proceed to the construction of the whole call-price surface of\nthe S\\&P500 options, taking into account the arbitrage possibilities in the\ntime direction. The resulting object is a surface free of both butterfly and\ncalendar-spread arbitrage that matches the original market points. We then move\non to an FX application, namely the HKD/USD call-option surface.\n"
    },
    {
        "paper_id": 1506.07163,
        "authors": "Sergey Sosnovskiy",
        "title": "Market shape formation, statistical equilibrium and neutral evolution\n  theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical methods of population genetics and framework of exchangeability\nprovide a Markov chain model for analysis and interpretation of stochastic\nbehaviour of equity markets, explaining, in particular, market shape formation,\nstatistical equilibrium and temporal stability of market weights.\n"
    },
    {
        "paper_id": 1506.07212,
        "authors": "Rafael Frongillo, Ian A. Kash",
        "title": "Elicitation Complexity of Statistical Properties",
        "comments": "This version fixes an error in the condition needed for the main\n  lower bound and adds an application to Range Value at Risk, along with a\n  substantial reorganization of the paper and numerous smaller changes. A\n  previous version appeared in Neural Information Processing Systems 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A property, or statistical functional, is said to be elicitable if it\nminimizes expected loss for some loss function. The study of which properties\nare elicitable sheds light on the capabilities and limitations of point\nestimation and empirical risk minimization. While recent work asks which\nproperties are elicitable, we instead advocate for a more nuanced question: how\nmany dimensions are required to indirectly elicit a given property? This number\nis called the elicitation complexity of the property. We lay the foundation for\na general theory of elicitation complexity, including several basic results\nabout how elicitation complexity behaves, and the complexity of standard\nproperties of interest. Building on this foundation, our main result gives\ntight complexity bounds for the broad class of Bayes risks. We apply these\nresults to several properties of interest, including variance, entropy, norms,\nand several classes of financial risk measures. We conclude with discussion and\nopen directions.\n"
    },
    {
        "paper_id": 1506.07368,
        "authors": "Stefan Rass",
        "title": "On Game-Theoretic Risk Management (Part One) -- Towards a Theory of\n  Games with Payoffs that are Probability-Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal behavior in (competitive) situation is traditionally determined with\nthe help of utility functions that measure the payoff of different actions.\nGiven an ordering on the space of revenues (payoffs), the classical axiomatic\napproach of von Neumann and Morgenstern establishes the existence of suitable\nutility functions, and yields to game-theory as the most prominent\nmaterialization of a theory to determine optimal behavior. Although this\nappears to be a most natural approach to risk management too, applications in\ncritical infrastructures often violate the implicit assumption of actions\nleading to deterministic consequences. In that sense, the gameplay in a\ncritical infrastructure risk control competition is intrinsically random in the\nsense of actions having uncertain consequences. Mathematically, this takes us\nto utility functions that are probability-distribution-valued, in which case we\nloose the canonic (in fact every possible) ordering on the space of payoffs,\nand the original techniques of von Neumann and Morgenstern no longer apply.\n  This work introduces a new kind of game in which uncertainty applies to the\npayoff functions rather than the player's actions (a setting that has been\nwidely studied in the literature, yielding to celebrated notions like the\ntrembling hands equilibrium or the purification theorem). In detail, we show\nhow to fix the non-existence of a (canonic) ordering on the space of\nprobability distributions by only mildly restricting the full set to a subset\nthat can be totally ordered. Our vehicle to define the ordering and establish\nbasic game-theory is non-standard analysis and hyperreal numbers.\n"
    },
    {
        "paper_id": 1506.07432,
        "authors": "J.-F. Mercure, H. Pollitt, A. M. Bassi, J. E Vi\\~nuales and N. R.\n  Edwards",
        "title": "Modelling complex systems of heterogeneous agents to better design\n  sustainability transitions policy",
        "comments": "20 pages, 5 figures, to appear in Global Environmental Change",
        "journal-ref": null,
        "doi": "10.1016/j.gloenvcha.2016.02.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article proposes a fundamental methodological shift in the modelling of\npolicy interventions for sustainability transitions in order to account for\ncomplexity (e.g. self-reinforcing mechanism arising from multi-agent\ninteractions) and agent heterogeneity (e.g. differences in consumer and\ninvestment behaviour). We first characterise the uncertainty faced by climate\npolicy-makers and its implications for investment decision-makers. We then\nidentify five shortcomings in the equilibrium and optimisation-based approaches\nmost frequently used to inform sustainability policy: (i) their normative,\noptimisation-based nature, (ii) their unrealistic reliance on the\nfull-rationality of agents, (iii) their inability to account for mutual\ninfluences among agents and capture related self-reinforcing (positive\nfeedback) processes, (iv) their inability to represent multiple solutions and\npath-dependency, and (v) their inability to properly account for agent\nheterogeneity. The aim of this article is to introduce an alternative modelling\napproach based on complexity dynamics and agent heterogeneity, and explore its\nuse in four key areas of sustainability policy, namely (1) technology adoption\nand diffusion, (2) macroeconomic impacts of low-carbon policies, (3)\ninteractions between the socio-economic system and the natural environment, and\n(4) the anticipation of policy outcomes. The practical relevance of the\nproposed methodology is discussed by reference to four applications: the\ndiffusion of transport technology, the impact of low-carbon investment on\nincome and employment, the management of cascading uncertainties, and the\ncross-sectoral impact of biofuels policies. The article calls for a fundamental\nmethodological shift aligning the modelling of the socio-economic system with\nthat of the climatic system, for a combined and realistic understanding of the\nimpact of sustainability policies.\n"
    },
    {
        "paper_id": 1506.07554,
        "authors": "Xin Zang, Jun Ni, Jing-Zhi Huang and Lan Wu",
        "title": "Double-jump stochastic volatility model for VIX: evidence from VVIX",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies the continuous-time dynamics of VIX with stochastic\nvolatility and jumps in VIX and volatility. Built on the general parametric\naffine model with stochastic volatility and jump in logarithm of VIX, we derive\na linear relation between the stochastic volatility factor and VVIX index. We\ndetect the existence of co-jump of VIX and VVIX and put forward a double-jump\nstochastic volatility model for VIX through its joint property with VVIX. With\nVVIX index as a proxy for the stochastic volatility, we use MCMC method to\nestimate the dynamics of VIX. Comparing nested models on VIX, we show the jump\nin VIX and the volatility factor is statistically significant. The jump\nintensity is also statedependent. We analyze the impact of jump factor on the\nVIX dynamics.\n"
    },
    {
        "paper_id": 1506.07582,
        "authors": "Natasa Golo, David S. Bree, Guy Kelman, Leanne Usher, Marco Lamieri\n  and Sorin Solomon",
        "title": "Too dynamic to fail. Empirical support for an autocatalytic model of\n  Minsky's financial instability hypothesis",
        "comments": "28 pages, 10 figures. in Journal of Economic Interaction and\n  Coordination, 2015",
        "journal-ref": null,
        "doi": "10.1007/s11403-015-0163-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Solomon and Golo [1] have recently proposed an autocatalytic\n(self-reinforcing) feedback model which couples a macroscopic system parameter\n(the interest rate), a microscopic parameter that measures the distribution of\nthe states of the individual agents (the number of firms in financial\ndifficulty) and a peer-to-peer network effect (contagion across supply chain\nfinancing). In this model, each financial agent is characterized by its\nresilience to the interest rate. Above a certain rate the interest due on the\nfirm's financial costs exceeds its earnings and the firm becomes susceptible to\nfailure (ponzi). For the interest rate levels under a certain threshold level,\nthe firm loans are smaller then its earnings and the firm becomes 'hedge.' In\nthis paper, we fit the historical data (2002-2009) on interest rate data into\nour model, in order to predict the number of the ponzi firms. We compare the\nprediction with the data taken from a large panel of Italian firms over a\nperiod of 9 years. We then use trade credit linkages to discuss the connection\nbetween the ponzi density and the network percolation.\n  We find that the 'top-down'-'bottom-up' positive feedback loop accounts for\nmost of the Minsky crisis accelerator dynamics. The peer-to-peer ponzi\ncompanies contagion becomes significant only in the last stage of the crisis\nwhen the ponzi density is above a critical value. Moreover the ponzi contagion\nis limited only to the companies that were not dynamic enough to substitute\ntheir distressed clients with new ones. In this respect the data support a view\nin which the success of the economy depends on substituting the static\n'supply-network' picture with an interacting dynamic agents one.\n"
    },
    {
        "paper_id": 1506.07854,
        "authors": "Enrique Guerra-Pujol",
        "title": "A Bayesian Model of the Litigation Game",
        "comments": "24 pages, 1 table",
        "journal-ref": "European Journal of Legal Studies, vol. 4, no. 2 (Autumn/Winter\n  2011), pp. 220-240",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over a century ago, Oliver Wendell Holmes invited scholars to look at the law\nthrough the lens of probability theory: \"The prophecies of what the courts will\ndo in fact, and nothing more pretentious, are what I mean by the law.\" Yet few\nlegal scholars have taken up this intriguing invitation. As such, in place of\nprevious approaches to the study of law, this paper presents a non-normative,\nmathematical approach to law and the legal process. Specifically, we present a\nformal Bayesian model of civil and criminal litigation, or what we refer to as\nthe litigation game; that is, instead of focusing on the rules of civil or\ncriminal procedure or substantive legal doctrine, we ask and attempt to answer\na mathematical question: what is the posterior probability that a defendant in\na civil or criminal trial will be found liable, given that the defendant has,\nin fact, committed a wrongful act?\n"
    },
    {
        "paper_id": 1506.08054,
        "authors": "Marcel Wollschl\\\"ager and Rudi Sch\\\"afer",
        "title": "Impact of non-stationarity on estimating and modeling empirical copulas\n  of daily stock returns",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": "10.21314/JOR.2016.342",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  All too often measuring statistical dependencies between financial time\nseries is reduced to a linear correlation coefficient. However this may not\ncapture all facets of reality. We study empirical dependencies of daily stock\nreturns by their pairwise copulas. Here we investigate particularly to which\nextent the non-stationarity of financial time series affects both the\nestimation and the modeling of empirical copulas. We estimate empirical copulas\nfrom the non-stationary, original return time series and stationary, locally\nnormalized ones. Thereby we are able to explore the empirical dependence\nstructure on two different scales: a global and a local one. Additionally the\nasymmetry of the empirical copulas is emphasized as a fundamental\ncharacteristic. We compare our empirical findings with a single Gaussian\ncopula, with a correlation-weighted average of Gaussian copulas, with the\nK-copula directly addressing the non-stationarity of dependencies as a model\nparameter, and with the skewed Student's t-copula. The K-copula covers the\nempirical dependence structure on the local scale most adequately, whereas the\nskewed Student's t-copula best captures the asymmetry of the empirical copula\non the global scale.\n"
    },
    {
        "paper_id": 1506.08127,
        "authors": "David Criens, Kathrin Glau, Zorana Grbac",
        "title": "Martingale property of exponential semimartingales: a note on explicit\n  conditions and applications to financial models",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a collection of explicit sufficient conditions for the true\nmartingale property of a wide class of exponentials of semimartingales. We\nexpress the conditions in terms of semimartingale characteristics. This turns\nout to be very convenient in financial modeling in general. Especially it\nallows us to carefully discuss the question of well-definedness of\nsemimartingale Libor models, whose construction crucially relies on a sequence\nof measure changes.\n"
    },
    {
        "paper_id": 1506.0836,
        "authors": "Jinxia Zhu and Hailiang Yang",
        "title": "Optimal financing and dividend distribution in a general diffusion model\n  with regime switching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal financing and dividend distribution problem with\nrestricted dividend rates in a diffusion type surplus model where the drift and\nvolatility coefficients are general functions of the level of surplus and the\nexternal environment regime. The environment regime is modeled by a Markov\nprocess. Both capital injections and dividend payments incur expenses. The\nobjective is to maximize the expectation of the total discounted dividends\nminus the total cost of capital injections. We prove that it is optimal to\ninject capitals only when the surplus tends to fall below zero and to pay out\ndividends at the maximal rate when the surplus is at or above the threshold\ndependent on the environment regime.\n"
    },
    {
        "paper_id": 1506.084,
        "authors": "Christopher J. Rook",
        "title": "Optimal Equity Glidepaths in Retirement",
        "comments": "Fully documented source code from a C++ implementation is included in\n  the attached proofs appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic retirement glidepaths evolve over time based on some measure such as\nthe retiree's funded status or current market valuations. Conversely, static\nglidepaths are fixed at a starting point and selected under the assumption that\nthey will not change. In practice, new static glidepaths may be derived\nperiodically making them more flexible. The optimal static retirement glidepath\nwould be the one that performs better than all others with respect to some\nmetric. When systematic withdrawals are made from a retirement portfolio,\nglidepaths are often assessed via the probability of ruin (or success). Our\ngoal here is to derive the optimal static glidepath with respect to this\nmetric. It is a result new to the literature and the shape will be of special\ninterest to retirees, financial advisors, retirement researchers, and\ntarget-date fund providers.\n"
    },
    {
        "paper_id": 1506.08408,
        "authors": "David Landriault, Bin Li, Hongzhong Zhang",
        "title": "On magnitude, asymptotics and duration of drawdowns for L\\'{e}vy models",
        "comments": "Published at http://dx.doi.org/10.3150/15-BEJ748 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2017, Vol. 23, No. 1, 432-458",
        "doi": "10.3150/15-BEJ748",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers magnitude, asymptotics and duration of drawdowns for\nsome L\\'{e}vy processes. First, we revisit some existing results on the\nmagnitude of drawdowns for spectrally negative L\\'{e}vy processes using an\napproximation approach. For any spectrally negative L\\'{e}vy process whose\nscale functions are well-behaved at $0+$, we then study the asymptotics of\ndrawdown quantities when the threshold of drawdown magnitude approaches zero.\nWe also show that such asymptotics is robust to perturbations of additional\npositive compound Poisson jumps. Finally, thanks to the asymptotic results and\nsome recent works on the running maximum of L\\'{e}vy processes, we derive the\nlaw of duration of drawdowns for a large class of L\\'{e}vy processes (with a\ngeneral spectrally negative part plus a positive compound Poisson structure).\nThe duration of drawdowns is also known as the \"Time to Recover\" (TTR) the\nhistorical maximum, which is a widely used performance measure in the fund\nmanagement industry. We find that the law of duration of drawdowns\nqualitatively depends on the path type of the spectrally negative component of\nthe underlying L\\'{e}vy process.\n"
    },
    {
        "paper_id": 1506.08595,
        "authors": "Yannick Armenti (LaMME), St\\'ephane Cr\\'epey (LaMME)",
        "title": "Central Clearing Valuation Adjustment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops an XVA (costs) analysis of centrally cleared trading,\nparallel to the one that has been developed in the last years for bilateral\ntransactions. We introduce a dynamic framework that incorporates the sequence\nof cash-flows involved in the waterfall of resources of a clearing house. The\ntotal cost of the clearance framework for a clearing member, called CCVA for\ncentral clearing valuation adjustment, is decomposed into a CVA corresponding\nto the cost of its losses on the default fund in case of defaults of other\nmember, an MVA corresponding to the cost of funding its margins and a KVA\ncorresponding to the cost of the regulatory capital and also of the capital at\nrisk that the member implicitly provides to the CCP through its default fund\ncontribution. In the end the structure of the XVA equations for bilateral and\ncleared portfolios is similar, but the input data to these equations are not\nthe same, reflecting different financial network structures. The resulting XVA\nnumbers differ, but, interestingly enough, they become comparable after scaling\nby a suitable netting ratio.\n"
    },
    {
        "paper_id": 1506.0869,
        "authors": "Gabor Nagy and Gergo Barta and Tamas Henk",
        "title": "Portfolio optimization using local linear regression ensembles in\n  RapidMiner",
        "comments": "RCOMM 2012: Rapidminer Community Meeting and Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we implement a Local Linear Regression Ensemble Committee\n(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. The\nestimates and the historical returns of the committees are used to compute the\nweights of the portfolio from the 453 stock. The proposed method outperforms\nbenchmark portfolio selection strategies that optimize the growth rate of the\ncapital. We investigate the effect of algorithm parameter m: the number of\nselected stocks on achieved average annual yields. Results suggest the\nalgorithm's practical usefulness in everyday trading.\n"
    },
    {
        "paper_id": 1506.08692,
        "authors": "Jaroslaw Kwapien, Pawel Oswiecimka, Stanislaw Drozdz",
        "title": "Detrended fluctuation analysis made flexible to detect range of\n  cross-correlated fluctuations",
        "comments": null,
        "journal-ref": "Phys. Rev. E 92, 052815 (2015)",
        "doi": "10.1103/PhysRevE.92.052815",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The detrended cross-correlation coefficient $\\rho_{\\rm DCCA}$ has recently\nbeen proposed to quantify the strength of cross-correlations on different\ntemporal scales in bivariate, non-stationary time series. It is based on the\ndetrended cross-correlation and detrended fluctuation analyses (DCCA and DFA,\nrespectively) and can be viewed as an analogue of the Pearson coefficient in\nthe case of the fluctuation analysis. The coefficient $\\rho_{\\rm DCCA}$ works\nwell in many practical situations but by construction its applicability is\nlimited to detection of whether two signals are generally cross-correlated,\nwithout possibility to obtain information on the amplitude of fluctuations that\nare responsible for those cross-correlations. In order to introduce some\nrelated flexibility, here we propose an extension of $\\rho_{\\rm DCCA}$ that\nexploits the multifractal versions of DFA and DCCA: MFDFA and MFCCA,\nrespectively. The resulting new coefficient $\\rho_q$ not only is able to\nquantify the strength of correlations, but also it allows one to identify the\nrange of detrended fluctuation amplitudes that are correlated in two signals\nunder study. We show how the coefficient $\\rho_q$ works in practical situations\nby applying it to stochastic time series representing processes with long\nmemory: autoregressive and multiplicative ones. Such processes are often used\nto model signals recorded from complex systems and complex physical phenomena\nlike turbulence, so we are convinced that this new measure can successfully be\napplied in time series analysis. In particular, we present an example of such\napplication to highly complex empirical data from financial markets. The\npresent formulation can straightforwardly be extended to multivariate data in\nterms of the $q$-dependent counterpart of the correlation matrices and then to\nthe network representation.\n"
    },
    {
        "paper_id": 1506.0874,
        "authors": "Aur\\'elien Alfonsi and Pierre Blanc",
        "title": "Extension and calibration of a Hawkes-based optimal execution model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide some theoretical extensions and a calibration protocol for our\nformer dynamic optimal execution model. The Hawkes parameters and the\npropagator are estimated independently on financial data from stocks of the\nCAC40. Interestingly, the propagator exhibits a smoothly decaying form with one\nor two dominant time scales, but only so after a few seconds that the market\nneeds to adjust after a large trade. Motivated by our estimation results, we\nderive the optimal execution strategy for a multi-exponential Hawkes kernel and\nbacktest it on the data for round trips. We find that the strategy is\nprofitable on average when trading at the midprice, which is in accordance with\nviolated martingale conditions. However, in most cases, these profits vanish\nwhen we take bid-ask costs into account.\n"
    },
    {
        "paper_id": 1506.08743,
        "authors": "Alex Augusto Timm Rathke",
        "title": "Note on tax enforcement and transfer pricing manipulation",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note proposes the segregation of independent endogenous and exogenous\ncomponents of tax penalty probability to introduce a formal demonstration that\nenforcement and tax penalties are negatively related with income shifting. JEL\nF23; H26.\n"
    },
    {
        "paper_id": 1506.08847,
        "authors": "Provash Mali and Amitabha Mukhopadhyay",
        "title": "Multifractal characterization of gold market: a multifractal detrended\n  fluctuation analysis",
        "comments": "20 pages, 7 figures, 1 table",
        "journal-ref": "Physica A 413,2014,361-372",
        "doi": "10.1016/j.physa.2014.06.076",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multifractal detrended fluctuation analysis technique is employed to\nanalyze the time series of gold consumer price index (CPI) and the market trend\nof three world's highest gold consuming countries, namely China, India and\nTurkey for the period: 1993-July 2013. Various multifractal variables, such as\nthe generalized Hurst exponent, the multifractal exponent and the singularity\nspectrum, are calculated and the results are fitted to the generalized binomial\nmultifractal (GBM) series that consists of only two parameters. Special\nemphasis is given to identify the possible source(s) of multifractality in\nthese series. Our analysis shows that the CPI series and all three market\nseries are of multifractal nature. The origin of multifractality for the CPI\ntime series and Indian market series is found due to a long-range time\ncorrelation, whereas it is mostly due to the fat-tailed probability\ndistributions of the values for the Chinese and Turkey markets. The GBM model\nseries more or less describes all the time series analyzed here.\n"
    },
    {
        "paper_id": 1506.09184,
        "authors": "Erhan Bayraktar and Song Yao",
        "title": "On the Robust Dynkin Game",
        "comments": "To appear in Annals of Applied Probability. Keywords: robust Dynkin\n  game, nonlinear expectation, dynamic programming principle, controls in weak\n  formulation, weak stability under pasting, martingale approach,\n  path-dependent stochastic differential equations with controls, optimal\n  triplet, optimal stopping with random maturity",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust Dynkin game over a set of mutually singular probabilities.\nWe first prove that for the conservative player of the game, her lower and\nupper value processes coincide (i.e. She has a value process $V $ in the game).\nSuch a result helps people connect the robust Dynkin game with second-order\ndoubly reflected backward stochastic differential equations. Also, we show that\nthe value process $V$ is a submartingale under an appropriately defined\nnonlinear expectations up to the first time $\\tau_*$ when $V$ meets the lower\npayoff process $L$. If the probability set is weakly compact, one can even find\nan optimal triplet. The mutual singularity of probabilities in causes major\ntechnical difficulties. To deal with them, we use some new methods including\ntwo approximations with respect to the set of stopping times. The mutual\nsingularity of probabilities causes major technical difficulties. To deal with\nthem, we use some new methods including two approximations with respect to the\nset of stopping times\n"
    },
    {
        "paper_id": 1507.00208,
        "authors": "Francesca Biagini, Alessandro Gnoatto, Maximilian H\\\"artel",
        "title": "The Long-Term Swap Rate and a General Analysis of Long-Term Interest\n  Rates",
        "comments": "29 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce here for the first time the long-term swap rate, characterised\nas the fair rate of an overnight indexed swap with infinitely many exchanges.\nFurthermore we analyse the relationship between the long-term swap rate, the\nlong-term yield, see Biagini et al. [2018], Biagini and H\\\"artel [2014], and El\nKaroui et al. [1997], and the long-term simple rate, considered in Brody and\nHughston [2016] as long-term discounting rate. We finally investigate the\nexistence of these long-term rates in two term structure methodologies, the\nFlesaker-Hughston model and the linear-rational model. A numerical example\nillustrates how our results can be used to estimate the non-optional component\nof a CoCo bond.\n"
    },
    {
        "paper_id": 1507.00244,
        "authors": "Tobias Fissler, Johanna F. Ziegel, Tilmann Gneiting",
        "title": "Expected Shortfall is jointly elicitable with Value at Risk -\n  Implications for backtesting",
        "comments": null,
        "journal-ref": "Risk, January 2016, 58-61",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we comment on the relevance of elicitability for backtesting\nrisk measure estimates. In particular, we propose the use of Diebold-Mariano\ntests, and show how they can be implemented for Expected Shortfall (ES), based\non the recent result of Fissler and Ziegel (2015) that ES is jointly elicitable\nwith Value at Risk.\n"
    },
    {
        "paper_id": 1507.0025,
        "authors": "Giovanni Bonaccolto, Massimiliano Caporin and Sandra Paterlini",
        "title": "Asset Allocation Strategies Based on Penalized Quantile Regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that quantile regression model minimizes the portfolio\nextreme risk, whenever the attention is placed on the estimation of the\nresponse variable left quantiles. We show that, by considering the entire\nconditional distribution of the dependent variable, it is possible to optimize\ndifferent risk and performance indicators. In particular, we introduce a\nrisk-adjusted profitability measure, useful in evaluating financial portfolios\nunder a pessimistic perspective, since the reward contribution is net of the\nmost favorable outcomes. Moreover, as we consider large portfolios, we also\ncope with the dimensionality issue by introducing an l1-norm penalty on the\nassets weights.\n"
    },
    {
        "paper_id": 1507.00294,
        "authors": "Ramin Okhrati and Uwe Schmock",
        "title": "It\\^o's formula for finite variation L\\'evy processes: The case of\n  non-smooth functions",
        "comments": null,
        "journal-ref": "Journal of Mathematical Analysis and Applications, 430, (2),\n  1163-1174 (2015)",
        "doi": "10.1016/j.jmaa.2015.05.025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Extending It\\^o's formula to non-smooth functions is important both in theory\nand applications. One of the fairly general extensions of the formula, known as\nMeyer-It\\^o, applies to one dimensional semimartingales and convex functions.\nThere are also satisfactory generalizations of It\\^o's formula for diffusion\nprocesses where the Meyer-It\\^o assumptions are weakened even further. We study\na version of It\\^o's formula for multi-dimensional finite variation L\\'evy\nprocesses assuming that the underlying function is continuous and admits weak\nderivatives. We also discuss some applications of this extension, particularly\nin finance.\n"
    },
    {
        "paper_id": 1507.00578,
        "authors": "Etienne C\\^ome (IFSTTAR/COSYS/GRETTIA), Marie Cottrell (SAMM), Patrice\n  Gaubert (ERUDITE)",
        "title": "Analysis of Professional Trajectories using Disconnected Self-Organizing\n  Maps",
        "comments": null,
        "journal-ref": "Neurocomputing, Elsevier, 2014, 147, pp.185-196",
        "doi": "10.1016/j.neucom.2013.12.058",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we address an important economic question. Is there, as\nmainstream economic theory asserts it, an homogeneous labor market with\nmechanisms which govern supply and demand for work, producing an equilibrium\nwith its remarkable properties? Using the Panel Study of Income Dynamics (PSID)\ncollected on the period 1984-2003, we study the situations of American workers\nwith respect to employment. The data include all heads of household (men or\nwomen) as well as the partners who are on the labor market, working or not.\nThey are extracted from the complete survey and we compute a few relevant\nfeatures which characterize the worker's situations. To perform this analysis,\nwe suggest using a Self-Organizing Map (SOM, Kohonen algorithm) with specific\nstructure based on planar graphs, with disconnected components (called D-SOM),\nespecially interesting for clustering. We compare the results to those obtained\nwith a classical SOM grid and a star-shaped map (called SOS). Each component of\nD-SOM takes the form of a string and corresponds to an organized cluster. From\nthis clustering, we study the trajectories of the individuals among the classes\nby using the transition probability matrices for each period and the\ncorresponding stationary distributions. As a matter of fact, we find clear\nevidence of heterogeneous parts, each one with high homo-geneity, representing\nsituations well identified in terms of activity and wage levels and in degree\nof stability in the workplace. These results and their interpretation in\neconomic terms contribute to the debate about flexibility which is commonly\nseen as a way to obtain a better level of equilibrium on the labor market.\n"
    },
    {
        "paper_id": 1507.00671,
        "authors": "Mathias Beiglb\\\"ock, Marcel Nutz, Nizar Touzi",
        "title": "Complete Duality for Martingale Optimal Transport on the Line",
        "comments": "42 pages; forthcoming in 'Annals of Probability'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal transport between two probability measures on the real\nline, where the transport plans are laws of one-step martingales. A quasi-sure\nformulation of the dual problem is introduced and shown to yield a complete\nduality theory for general marginals and measurable reward (cost) functions:\nabsence of a duality gap and existence of dual optimizers. Both properties are\nshown to fail in the classical formulation. As a consequence of the duality\nresult, we obtain a general principle of cyclical monotonicity describing the\ngeometry of optimal transports.\n"
    },
    {
        "paper_id": 1507.00784,
        "authors": "Th\\'arsis Tuani Pinto Souza, Olga Kolchyna, Philip C. Treleaven and\n  Tomaso Aste",
        "title": "Twitter Sentiment Analysis Applied to Finance: A Case Study in the\n  Retail Industry",
        "comments": "23 pages, 5 figures, 9 tables",
        "journal-ref": "In: Handbook of Sentiment Analysis in Finance. Mitra, G. and Yu,\n  X. (Eds.). (2016). ISBN 1910571571., 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a financial analysis over Twitter sentiment analytics\nextracted from listed retail brands. We investigate whether there is\nstatistically-significant information between the Twitter sentiment and volume,\nand stock returns and volatility. Traditional newswires are also considered as\na proxy for the market sentiment for comparative purpose. The results suggest\nthat social media is indeed a valuable source in the analysis of the financial\ndynamics in the retail sector even when compared to mainstream news such as the\nWall Street Journal and Dow Jones Newswires.\n"
    },
    {
        "paper_id": 1507.00846,
        "authors": "Florent S\\'egonne",
        "title": "Variance Dynamics - An empirical journey",
        "comments": "22 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the joint dynamics of spot and implied volatility from an\nempirical perspective. We focus on the equity market with the SPX Index our\nunderlying of choice. Using only observable quantities, we extract the\ninstantaneous variance curves implied by the market and study their daily\nvariations jointly with spot returns. We analyze the characteristics of their\nindividual and joint densities, quantify the non-linear relationship between\nspot and volatility, and discuss the modeling implications on the implied\nleverage and the volatility clustering effects. We show that non-linearities\nhave little impact on the dynamics of at-the-money volatilities, but can have a\nsignificant effect on the pricing and hedging of volatility derivatives.\n"
    },
    {
        "paper_id": 1507.00894,
        "authors": "Eleonora Perversi, Eugenio Regazzini",
        "title": "Inequality and risk aversion in economies open to altruistic attitudes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper attempts to find a relationship between agents' risk aversion and\ninequality of incomes. Specifically, a model is proposed for the evolution in\ntime of surplus/deficit distribution, and the long-time distributions are\ncharacterized almost completely. They turn out to be weak Pareto laws with\nexponent linked to the relative risk aversion index which, in turn, is supposed\nto be the same for every agent. On the one hand, the aforesaid link is\nexpressed by an affine transformation. On the other hand, the level of the\nrelative risk aversion index results from a frequency distribution of\nobservable quantities stemming from how agents interact in an economic sense.\nCombination of these facts is conducive to the specification of qualitative and\nquantitative characteristics of actions fit for the control of income\nconcentration.\n"
    },
    {
        "paper_id": 1507.01033,
        "authors": "Yoann Potiron, Per Mykland",
        "title": "Estimation of integrated quadratic covariation with endogenous sampling\n  times",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jeconom.2016.10.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When estimating high-frequency covariance (quadratic covariation) of two\narbitrary assets observed asynchronously, simple assumptions, such as\nindependence, are usually imposed on the relationship between the prices\nprocess and the observation times. In this paper, we introduce a general\nendogenous two-dimensional nonparametric model. Because an observation is\ngenerated whenever an auxiliary process called observation time process hits\none of the two boundary processes, it is called the hitting boundary process\nwith time process (HBT) model. We establish a central limit theorem for the\nHayashi-Yoshida (HY) estimator under HBT in the case where the price process\nand the observation price process follow a continuous Ito process. We obtain an\nasymptotic bias. We provide an estimator of the latter as well as a\nbias-corrected HY estimator of the high-frequency covariance. In addition, we\ngive a consistent estimator of the associated standard error.\n"
    },
    {
        "paper_id": 1507.01125,
        "authors": "Gaoyue Guo, Xiaolu Tan and Nizar Touzi",
        "title": "Tightness and duality of martingale transport on the Skorokhod space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The martingale optimal transport aims to optimally transfer a probability\nmeasure to another along the class of martingales. This problem is mainly\nmotivated by the robust superhedging of exotic derivatives in financial\nmathematics, which turns out to be the corresponding Kantorovich dual. In this\npaper we consider the continuous-time martingale transport on the Skorokhod\nspace of cadlag paths. Similar to the classical setting of optimal transport,\nwe introduce different dual problems and establish the corresponding dualities\nby a crucial use of the S-topology and the dynamic programming principle.\n"
    },
    {
        "paper_id": 1507.01175,
        "authors": "V\\'eronique Maume-Deschamps (ICJ), Didier Rulli\\`ere (SAF), Khalil\n  Said (SAF)",
        "title": "Impact of dependence on some multivariate risk indicators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The minimization of some multivariate risk indicators may be used as an\nallocation method, as proposed in C\\'enac et al. [6]. The aim of capital\nallocation is to choose a point in a simplex, according to a given criterion.\nIn a previous paper [17] we proved that the proposed allocation technique\nsatisfies a set of coherence axioms. In the present one, we study the\nproperties and asymptotic behavior of the allocation for some distribution\nmodels. We analyze also the impact of the dependence structure on the\nallocation using some copulas.\n"
    },
    {
        "paper_id": 1507.0161,
        "authors": "Grigory Temnov",
        "title": "Analysis of Ornstein-Uhlenbeck process stopped at maximum drawdown and\n  application to trading strategies with trailing stops",
        "comments": "19 pages, 10 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a strategy for automated trading, outline theoretical\njustification of the profitability of this strategy and overview the\nhypothetical results in application to currency pairs trading. The proposed\nmethodology relies on the assumption that processes reflecting the dynamics of\ncurrency exchange rates are in a certain sense similar to the class of\nOrnstein-Uhlenbeck processes and exhibits the mean reverting property. In order\nto describe the quantitative characteristics of the projected return of the\nstrategy, we derive the explicit expression for the running maximum of the\nOrnstein-Uhlenbeck process stopped at maximum drawdown and look at the\ncorrespondence between derived characteristics and the observed ones.\n"
    },
    {
        "paper_id": 1507.01729,
        "authors": "Jozef Barunik and Tomas Krehlik",
        "title": "Measuring the frequency dynamics of financial connectedness and systemic\n  risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new framework for measuring connectedness among financial\nvariables that arises due to heterogeneous frequency responses to shocks. To\nestimate connectedness in short-, medium-, and long-term financial cycles, we\nintroduce a framework based on the spectral representation of variance\ndecompositions. In an empirical application, we document the rich\ntime-frequency dynamics of volatility connectedness in US financial\ninstitutions. Economically, periods in which connectedness is created at high\nfrequencies are periods when stock markets seem to process information rapidly\nand calmly, and a shock to one asset in the system will have an impact mainly\nin the short term. When the connectedness is created at lower frequencies, it\nsuggests that shocks are persistent and are being transmitted for longer\nperiods.\n"
    },
    {
        "paper_id": 1507.01847,
        "authors": "Zachary Feinstein and Fatena El-Masri",
        "title": "The Effects of Leverage Requirements and Fire Sales on Financial\n  Contagion via Asset Liquidation Strategies in Financial Networks",
        "comments": "38 pages, 23 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a framework for modeling the financial system with\nmultiple illiquid assets when liquidation of illiquid assets is caused by\nfailure to meet a leverage requirement. This extends the network model of\nCifuentes, Shin & Ferrucci (2005) which incorporates a single asset with fire\nsales and capital adequacy ratio. This also extends the network model of\nFeinstein (2015) which incorporates multiple illiquid assets with fire sales\nand no leverage ratios. We prove existence of equilibrium clearing payments and\nliquidation prices for a known liquidation strategy when leverage requirements\nare required. We also prove sufficient conditions for the existence of an\nequilibrium liquidation strategy with corresponding clearing payments and\nliquidation prices. Finally we calibrate network models to asset and liability\ndata for 50 banks in the United States from 2007-2014 in order to draw\nconclusions on systemic risk as a function of leverage requirements.\n"
    },
    {
        "paper_id": 1507.01901,
        "authors": "Diego Aparicio, Daniel Fraiman",
        "title": "Banking Networks and Leverage Dependence: Evidence from Selected\n  Emerging Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use bank-level balance sheet data from 2005 to 2010 to study interactions\nwithin the banking system of five emerging countries: Argentina, Brazil,\nMexico, South Africa, and Taiwan. For each country we construct a financial\nnetwork based on the leverage ratio dependence between each pair of banks, and\nfind results that are comparable across countries. Banks present a variety of\nleverage ratio behaviors. This leverage diversity produces financial networks\nthat exhibit a modular structure characterized by one large bank community,\nsome small ones and isolated banks. There exist compact structures that have\nsynchronized dynamics. Many groups of banks merge together creating a financial\nnetwork topology that converges to a unique big cluster at a relatively low\nleverage dependence level. Finally, we propose a model that includes corporate\nand interbank loans for studying the banking system. This model generates\nnetworks similar to the empirical ones. Moreover, we find that faster-growing\nbanks tend to be more highly interconnected between them, and this is also\nobserved in empirical data.\n"
    },
    {
        "paper_id": 1507.02025,
        "authors": "Enrico G. De Giorgi and Ola Mahmoud",
        "title": "Diversification Preferences in the Theory of Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Diversification represents the idea of choosing variety over uniformity.\nWithin the theory of choice, desirability of diversification is axiomatized as\npreference for a convex combination of choices that are equivalently ranked.\nThis corresponds to the notion of risk aversion when one assumes the\nvon-Neumann-Morgenstern expected utility model, but the equivalence fails to\nhold in other models. This paper studies axiomatizations of the concept of\ndiversification and their relationship to the related notions of risk aversion\nand convex preferences within different choice theoretic models. Implications\nof these notions on portfolio choice are discussed. We cover model-independent\ndiversification preferences, preferences within models of choice under risk,\nincluding expected utility theory and the more general rank-dependent expected\nutility theory, as well as models of choice under uncertainty axiomatized via\nChoquet expected utility theory. Remarks on interpretations of diversification\npreferences within models of behavioral choice are given in the conclusion.\n"
    },
    {
        "paper_id": 1507.02203,
        "authors": "Gurjeet Dhesi, Muhammad Bilal Shakeel and Ling Xiao",
        "title": "Modified Brownian Motion Approach to Modelling Returns Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An innovative extension of Geometric Brownian Motion model is developed by\nincorporating a weighting factor and a stochastic function modelled as a\nmixture of power and trigonometric functions. Simulations based on this\nModified Brownian Motion Model with optimal weighting factors selected by\ngoodness of fit tests, substantially outperform the basic Geometric Brownian\nMotion model in terms of fitting the returns distribution of historic data\nprice indices. Furthermore we attempt to provide an interpretation of the\nadditional stochastic term in relation to irrational behaviour in financial\nmarkets and outline the importance of this novel model.\n"
    },
    {
        "paper_id": 1507.0231,
        "authors": "Ovidiu Racorean",
        "title": "Quantum Gates and Quantum Circuits of Stock Portfolio",
        "comments": "41 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In quantum computation, series of quantum gates have to be arranged in a\npredefined sequence that led to a quantum circuit in order to solve a\nparticular problem. What if the sequence of quantum gates is known but both the\nproblem to be solved and the outcome of the so defined quantum circuit remain\nin the shadow? This is the situation of the stock market. The price time series\nof a portfolio of stocks are organized in braids that effectively simulate\nquantum gates in the hypothesis of Ising anyons quantum computational model.\nFollowing the prescriptions of Ising anyons model, 1-qubit quantum gates are\nconstructed for portfolio composed of four stocks. Adding two additional stocks\nat the initial portfolio result in 2-qubits quantum gates and circuits.\nHadamard gate, Pauli gates or controlled-Z gate are some of the elementary\nquantum gates that are identified in the stock market structure. Addition of\nother pairs of stocks, that eventually represent a market index, like Dow Jones\nindustrial Average, it results in a sequence of n-qubits quantum gates that\nform a quantum code. Deciphering this mysterious quantum code of the stock\nmarket is an issue for future investigations.\n"
    },
    {
        "paper_id": 1507.02651,
        "authors": "Alexander M. G. Cox, Sigrid K\\\"allblad",
        "title": "Model-independent bounds for Asian options: a dynamic programming\n  approach",
        "comments": "Updated version with some technical changes, and a new appendix\n  containing a proof of the DPP",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of finding model-independent bounds on the price of\nan Asian option, when the call prices at the maturity date of the option are\nknown. Our methods differ from most approaches to model-independent pricing in\nthat we consider the problem as a dynamic programming problem, where the\ncontrolled process is the conditional distribution of the asset at the maturity\ndate. By formulating the problem in this manner, we are able to determine the\nmodel-independent price through a PDE formulation. Notably, this approach does\nnot require specific constraints on the payoff function (e.g. convexity), and\nwould appear to generalise to many related problems.\n"
    },
    {
        "paper_id": 1507.02822,
        "authors": "Patrick J. Laub, Thomas Taimre, and Philip K. Pollett",
        "title": "Hawkes Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hawkes processes are a particularly interesting class of stochastic process\nthat have been applied in diverse areas, from earthquake modelling to financial\nanalysis. They are point processes whose defining characteristic is that they\n'self-excite', meaning that each arrival increases the rate of future arrivals\nfor some period of time. Hawkes processes are well established, particularly\nwithin the financial literature, yet many of the treatments are inaccessible to\none not acquainted with the topic. This survey provides background, introduces\nthe field and historical developments, and touches upon all major aspects of\nHawkes processes.\n"
    },
    {
        "paper_id": 1507.02847,
        "authors": "Nicolas Langren\\'e, Geoffrey Lee, Zili Zhu",
        "title": "Switching to non-affine stochastic volatility: A closed-form expansion\n  for the Inverse Gamma model",
        "comments": "30 pages, 6 figures",
        "journal-ref": "International Journal of Theoretical and Applied Finance 19(5)\n  1-37 (2016)",
        "doi": "10.1142/S021902491650031X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces the Inverse Gamma (IGa) stochastic volatility model\nwith time-dependent parameters, defined by the volatility dynamics\n$dV_{t}=\\kappa_{t}\\left(\\theta_{t}-V_{t}\\right)dt+\\lambda_{t}V_{t}dB_{t}$. This\nnon-affine model is much more realistic than classical affine models like the\nHeston stochastic volatility model, even though both are as parsimonious (only\nfour stochastic parameters). Indeed, it provides more realistic volatility\ndistribution and volatility paths, which translate in practice into more robust\ncalibration and better hedging accuracy, explaining its popularity among\npractitioners. In order to price vanilla options with IGa volatility, we\npropose a closed-form volatility-of-volatility expansion. Specifically, the\nprice of a European put option with IGa volatility is approximated by a\nBlack-Scholes price plus a weighted combination of Black-Scholes greeks, where\nthe weights depend only on the four time-dependent parameters of the model.\nThis closed-form pricing method allows for very fast pricing and calibration to\nmarket data. The overall quality of the approximation is very good, as shown by\nseveral calibration tests on real-world market data where expansion prices are\ncompared favorably with Monte Carlo simulation results. This paper shows that\nthe IGa model is as simple, more realistic, easier to implement and faster to\ncalibrate than classical transform-based affine models. We therefore hope that\nthe present work will foster further research on non-affine models like the\nInverse Gamma stochastic volatility model, all the more so as this robust model\nis of great interest to the industry.\n"
    },
    {
        "paper_id": 1507.02974,
        "authors": "Kasper Larsen, Tanawit Sae Sue",
        "title": "Radner equilibrium in incomplete Levy models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct continuous-time equilibrium models based on a finite number of\nexponential utility investors. The investors' income rates as well as the\nstock's dividend rate are governed by discontinuous Levy processes. Our main\nresult provides the equilibrium (i.e., bond and stock price dynamics) in\nclosed-form. As an application, we show that the equilibrium Sharpe ratio can\nbe increased and the equilibrium interest rate can be decreased\n(simultaneously) when the investors' income streams cannot be traded.\n"
    },
    {
        "paper_id": 1507.03004,
        "authors": "Mikkel Bennedsen, Asger Lunde, Mikko S. Pakkanen",
        "title": "Hybrid scheme for Brownian semistationary processes",
        "comments": "33 pages, 4 figures, v4: minor revision, in particular we have\n  derived a new expression (3.5), equivalent to the previous one but\n  numerically more convenient, for the off-diagonal elements of the covariance\n  matrix Sigma",
        "journal-ref": "Finance and Stochastics 2017, Vol. 21, No. 4, 931-965",
        "doi": "10.1007/s00780-017-0335-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a simulation scheme for Brownian semistationary processes, which\nis based on discretizing the stochastic integral representation of the process\nin the time domain. We assume that the kernel function of the process is\nregularly varying at zero. The novel feature of the scheme is to approximate\nthe kernel function by a power function near zero and by a step function\nelsewhere. The resulting approximation of the process is a combination of\nWiener integrals of the power function and a Riemann sum, which is why we call\nthis method a hybrid scheme. Our main theoretical result describes the\nasymptotics of the mean square error of the hybrid scheme and we observe that\nthe scheme leads to a substantial improvement of accuracy compared to the\nordinary forward Riemann-sum scheme, while having the same computational\ncomplexity. We exemplify the use of the hybrid scheme by two numerical\nexperiments, where we examine the finite-sample properties of an estimator of\nthe roughness parameter of a Brownian semistationary process and study Monte\nCarlo option pricing in the rough Bergomi model of Bayer et al. [Quant. Finance\n16(6), 887-904, 2016], respectively.\n"
    },
    {
        "paper_id": 1507.03141,
        "authors": "Sergey Kamenshchikov",
        "title": "Bifurcation patterns of market regime transition",
        "comments": "9 pages,4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper mechanisms of reversion - momentum transition are considered.\nTwo basic nonlinear mechanisms are highlighted: a slow and fast bifurcation. A\nslow bifurcation leads to the equilibrium evolution, preceded by stability loss\ndelay of a control parameter. A single order parameter is introduced by\nMarkovian chain diffusion, which plays a role of a precursor. A fast\nbifurcation is formed by a singular fusion of unstable and stable equilibrium\nstates. The effect of a precatastrophic range compression is observed before\nthe discrete change of a system. A diffusion time scaling is presented as a\nprecursor of the fast bifurcation. The efficiency of both precursors in a\ncurrency market was illustrated by simulation of a prototype of a trading\nsystem.\n"
    },
    {
        "paper_id": 1507.03169,
        "authors": "A. Y. Klimenko",
        "title": "Intransitivity in Theory and in the Real World",
        "comments": "44 pages, 14 figures, 47 references, 6 appendices",
        "journal-ref": "Entropy 2015, 17, 4364-4412",
        "doi": "10.3390/e17064364",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work considers reasons for and implications of discarding the assumption\nof transitivity, which (transitivity) is the fundamental postulate in the\nutility theory of Von Neumann and Morgenstern, the adiabatic accessibility\nprinciple of Caratheodory and most other theories related to preferences or\ncompetition. The examples of intransitivity are drawn from different fields,\nsuch as law, biology, game theory, economics and competitive evolutionary\ndynamic. This work is intended as a common platform that allows us to discuss\nintransitivity in the context of different disciplines. The basic concepts and\nterms that are needed for consistent treatment of intransitivity in various\napplications are presented and analysed in a unified manner. The analysis\npoints out conditions that necessitate appearance of intransitivity, such as\nmultiplicity of preference criteria and imperfect (i.e. approximate)\ndiscrimination of different cases. The present work observes that with\nincreasing presence and strength of intransitivity, thermodynamics gradually\nfades away leaving space for more general kinetic considerations.\nIntransitivity in competitive systems is linked to complex phenomena that would\nbe difficult or impossible to explain on the basis of transitive assumptions.\nHuman preferences that seem irrational from the perspective of the conventional\nutility theory, become perfectly logical in the intransitive and relativistic\nframework suggested here. The example of competitive simulations for the\nrisk/benefit dilemma demonstrates the significance of intransitivity in cyclic\nbehaviour and abrupt changes in the system. The evolutionary intransitivity\nparameter, which is introduced in the Appendix, is a general measure of\nintransitivity, which is particularly useful in evolving competitive systems.\nQuantum preferences are also considered in the Appendix.\n"
    },
    {
        "paper_id": 1507.03278,
        "authors": "V.Kandiah, H.Escaith and D.L.Shepelyansky",
        "title": "Contagion effects in the world network of economic activities",
        "comments": "this work is linked with arXiv:1504.06773 [q-fin.ST]",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the new data from the OECD-WTO world network of economic activities we\nconstruct the Google matrix $G$ of this directed network and perform its\ndetailed analysis. The network contains 58 countries and 37 activity sectors\nfor years 1995, 2000, 2005, 2008, 2009. The construction of $G$, based on\nMarkov chain transitions, treats all countries on equal democratic grounds\nwhile the contribution of activity sectors is proportional to their exchange\nmonetary volume. The Google matrix analysis allows to obtain reliable ranking\nof countries and activity sectors and to determine the sensitivity of\nCheiRank-PageRank commercial balance of countries in respect to price\nvariations and labor cost in various countries. We demonstrate that the\ndeveloped approach takes into account multiplicity of network links with\neconomy interactions between countries and activity sectors thus being more\nefficient compared to the usual export-import analysis. Our results highlight\nthe striking increase of the influence of German economic activity on other\ncountries during the period 1995 to 2009 while the influence of Eurozone\ndecreases during the same period. We compare our results with the similar\nanalysis of the world trade network from the UN COMTRADE database. We argue\nthat the knowledge of network structure allows to analyze the effects of\neconomic influence and contagion propagation over the world economy.\n"
    },
    {
        "paper_id": 1507.03378,
        "authors": "Djordje Stratimirovic, Darko Sarvan, Vladimir Miljkovic, Suzana Blesic",
        "title": "Analysis of cyclical behavior in time series of stock market returns",
        "comments": "25 pages, 7 figures, 5 tables, Communications in Nonlinear Science\n  and Numerical Simulation, 2017",
        "journal-ref": null,
        "doi": "10.1016/j.cnsns.2017.05.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we have analyzed scaling properties and cyclical behavior of\nthe three types of stock market indexes (SMI) time series: data belonging to\nstock markets of developed economies, emerging economies, and of the\nunderdeveloped or transitional economies. We have used two techniques of data\nanalysis to obtain and verify our findings: the wavelet spectral analysis to\nstudy SMI returns data, and the Hurst exponent formalism to study local\nbehavior around market cycles and trends. We have found cyclical behavior in\nall SMI data sets that we have analyzed. Moreover, the positions and the\nboundaries of cyclical intervals that we have found seam to be common for all\nmarkets in our dataset. We list and illustrate the presence of nine such\nperiods in our SMI data. We also report on the possibilities to differentiate\nbetween the level of growth of the analyzed markets by way of statistical\nanalysis of the properties of wavelet spectra that characterize particular peak\nbehaviors. Our results show that measures like the relative WT energy content\nand the relative WT amplitude for the peaks in the small scales region could be\nused for partial differentiation between market economies. Finally, we propose\na way to quantify the level of development of a stock market based on the Hurst\nscaling exponent approach. From the local scaling exponents calculated for our\nnine peak regions we have defined what we named the Development Index, which\nproved, at least in the case of our dataset, to be suitable to rank the SMI\nseries that we have analyzed in three distinct groups.\n"
    },
    {
        "paper_id": 1507.04065,
        "authors": "Simpson Zhang and Mihaela van der Schaar",
        "title": "Reputational Learning and Network Dynamics",
        "comments": "Added Agent Re-Entry Section Added Simulations Modified Literature\n  Review Expanded Star Networks Section",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many real world networks agents are initially unsure of each other's\nqualities and must learn about each other over time via repeated interactions.\nThis paper is the first to provide a methodology for studying the dynamics of\nsuch networks, taking into account that agents differ from each other, that\nthey begin with incomplete information, and that they must learn through past\nexperiences which connections/links to form and which to break. The network\ndynamics in our model vary drastically from the dynamics in models of complete\ninformation. With incomplete information and learning, agents who provide high\nbenefits will develop high reputations and remain in the network, while agents\nwho provide low benefits will drop in reputation and become ostracized. We\nshow, among many other things, that the information to which agents have access\nand the speed at which they learn and act can have a tremendous impact on the\nresulting network dynamics. Using our model, we can also compute the ex ante\nsocial welfare given an arbitrary initial network, which allows us to\ncharacterize the socially optimal network structures for different sets of\nagents. Importantly, we show through examples that the optimal network\nstructure depends sharply on both the initial beliefs of the agents, as well as\nthe rate of learning by the agents. Due to the potential negative consequences\nof ostracism, it may be necessary to place agents with lower initial\nreputations at less central positions within the network.\n"
    },
    {
        "paper_id": 1507.04136,
        "authors": "Christoph Aymanns, Fabio Caccioli, J. Doyne Farmer, Vincent W.C. Tan",
        "title": "Taming the Basel Leverage Cycle",
        "comments": "41 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Effective risk control must make a tradeoff between the microprudential risk\nof exogenous shocks to individual institutions and the macroprudential risks\ncaused by their systemic interactions. We investigate a simple dynamical model\nfor understanding this tradeoff, consisting of a bank with a leverage target\nand an unleveraged fundamental investor subject to exogenous noise with\nclustered volatility. The parameter space has three regions: (i) a stable\nregion, where the system always reaches a fixed point equilibrium; (ii) a\nlocally unstable region, characterized by cycles and chaotic behavior; and\n(iii) a globally unstable region. A crude calibration of parameters to data\nputs the model in region (ii). In this region there is a slowly building price\nbubble, resembling a \"Great Moderation\", followed by a crash, with a period of\napproximately 10-15 years, which we dub the \"Basel leverage cycle\". We propose\na criterion for rating macroprudential policies based on their ability to\nminimize risk for a given average leverage. We construct a one parameter family\nof leverage policies that allows us to vary from the procyclical policies of\nBasel II or III, in which leverage decreases when volatility increases, to\ncountercyclical policies in which leverage increases when volatility increases.\nWe find the best policy depends critically on three parameters: The average\nleverage used by the bank; the relative size of the bank and the\nfundamentalist, and the amplitude of the exogenous noise. Basel II is optimal\nwhen the exogenous noise is high, the bank is small and leverage is low; in the\nopposite limit where the bank is large or leverage is high the optimal policy\nis closer to constant leverage. We also find that systemic risk can be\ndramatically decreased by lowering the leverage target adjustment speed of the\nbanks.\n"
    },
    {
        "paper_id": 1507.04167,
        "authors": "Mikhail Timonin",
        "title": "Axiomatization of the Choquet integral for 2-dimensional heterogeneous\n  product sets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove a representation theorem for the Choquet integral model. The\npreference relation is defined on a two-dimensional heterogeneous product set\n$X = X_1 \\times X_2$ where elements of $X_1$ and $X_2$ are not necessarily\ncomparable with each other. However, making such comparisons in a meaningful\nway is necessary for the construction of the Choquet integral (and any\nrank-dependent model). We construct the representation, study its uniqueness\nproperties, and look at applications in multicriteria decision analysis,\nstate-dependent utility theory, and social choice. Previous axiomatizations of\nthis model, developed for decision making under uncertainty, relied heavily on\nthe notion of comonotocity and that of a \"constant act\". However, that requires\n$X$ to have a special structure, namely, all factors of this set must be\nidentical. Our characterization does not assume commensurateness of criteria a\npriori, so defining comonotonicity becomes impossible.\n"
    },
    {
        "paper_id": 1507.04236,
        "authors": "Arnab Chatterjee, Anindya S. Chakrabarti, Asim Ghosh, Anirban\n  Chakraborti, Tushar K. Nandi",
        "title": "Invariant features of spatial inequality in consumption: the case of\n  India",
        "comments": "14 pages, 5 figs, 4 tables; Accepted in Physica A",
        "journal-ref": "Physica A 442 (2016) 169-181",
        "doi": "10.1016/j.physa.2015.09.019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the distributional features and inequality of consumption\nexpenditure across India, for different states, castes, religion and\nurban-rural divide. We find that even though the aggregate measures of\ninequality are fairly diversified across states, the consumption distributions\nshow near identical statistics, once properly normalized. This feature is seen\nto be robust with respect to variations in sociological and economic factors.\nWe also show that state-wise inequality seems to be positively correlated with\ngrowth which is in accord with the traditional idea of Kuznets' curve. We\npresent a brief model to account for the invariance found empirically and show\nthat better but riskier technology draws can create a positive correlation\nbetween inequality and growth.\n"
    },
    {
        "paper_id": 1507.04298,
        "authors": "A. E. Biondo, A. Pluchino, A. Rapisarda",
        "title": "Modelling Financial Markets by Self-Organized Criticality",
        "comments": "10 pages, 10 figures",
        "journal-ref": "Phys. Rev. E 92, 042814 (2015)",
        "doi": "10.1103/PhysRevE.92.042814",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a financial market model, characterized by self-organized\ncriticality, that is able to generate endogenously a realistic price dynamics\nand to reproduce well-known stylized facts. We consider a community of\nheterogeneous traders, composed by chartists and fundamentalists, and focus on\nthe role of informative pressure on market participants, showing how the\nspreading of information, based on a realistic imitative behavior, drives\ncontagion and causes market fragility. In this model imitation is not intended\nas a change in the agent's group of origin, but is referred only to the price\nformation process. We introduce in the community also a variable number of\nrandom traders in order to study their possible beneficial role in stabilizing\nthe market, as found in other studies. Finally we also suggest some\ncounterintuitive policy strategies able to dampen fluctuations by means of a\npartial reduction of information.\n"
    },
    {
        "paper_id": 1507.04387,
        "authors": "Traian A. Pirvu and Elena Cristina Canepa",
        "title": "One bank problem in the federal funds market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The model of this paper gives a convenient strategy that a bank in the\nfederal funds market can use in order to maximize its profit in a\ncontemporaneous reserve requirement (CRR) regime. The reserve requirements are\ndetermined by the demand deposit process, modelled as a Brownian motion with\ndrift. We propose a new model in which the cumulative funds purchases and sales\nare discounted at possible different rates. We formulate and solve the problem\nof finding the bank's optimal strategy. The model can be extended to involve\nthe bank's asset size and we obtain that, under some conditions, the optimal\nupper barrier for fund sales is a linear function of the asset size. As a\nconsequence, the bank net purchase amount is linear in the asset size.\n"
    },
    {
        "paper_id": 1507.04478,
        "authors": "Vadim Borokhov",
        "title": "Antimonopoly regulation method in energy markets based on the\n  Vickrey-Clarke-Groves mechanism",
        "comments": "21 pages",
        "journal-ref": "the pre-peer-review version of the paper published in Electric\n  Power Systems Research Volume 209, August 2022, 107964",
        "doi": "10.1016/j.epsr.2022.107964",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We evaluate the applicability of the generic Vickrey-Clarke-Groves (VCG)\nmechanism as an antimonopoly measure against a profit-maximizing producer with\nmarket power operating a portfolio of generating units at the centralized\ntwo-settlement energy market. The producer may indicate in its bid not only the\naltered cost function but also the distorted values of the technical parameters\nof its generating units, which enter the system-wide constraints of the\ncentralized dispatch optimization problem. To ensure the applicability of the\nVCG method in this setting, we identify an additional assumption on the changes\nof the feasible set of the centralized dispatch optimization problem induced by\nvariations of the producer's technical parameters. In the framework of the\ngeneric VCG mechanism, we propose an antimonopoly regulation method based on a\nregulator estimate of the producer's truthful bid. If this estimate is exact,\nthe producer's maximum profit coincides with that in the case of the truthful\nbidding when no antimonopoly measure is applied. If the estimate is not exact,\nthe error affects neither the producer's (weakly) dominant bid nor its optimal\nnodal output but manifests itself in the total uplift payment. This ensures an\nefficient allocation in the form of the optimal output/consumption schedule and\nshields the (pre-uplift) market prices from the producer's market power. We\ncompare the suggested method with the alternative antimonopoly regulation\napproach based on the replacement of the producer's bid by a bid composed by\nthe regulator.\n"
    },
    {
        "paper_id": 1507.04655,
        "authors": "Ole Peters and Alexander Adamou",
        "title": "Insurance makes wealth grow faster",
        "comments": "27 pages, 3 figures, 3 tables, 1 glossary",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Voluntary insurance contracts constitute a puzzle because they increase the\nexpectation value of one party's wealth, whereas both parties must sign for\nsuch contracts to exist. Classically, the puzzle is resolved by introducing\nnon-linear utility functions, which encode asymmetric risk preferences; or by\nassuming the parties have asymmetric information. Here we show the puzzle goes\naway if contracts are evaluated by their effect on the time-average growth rate\nof wealth. Our solution assumes only knowledge of wealth dynamics. Time\naverages and expectation values differ because wealth changes are non-ergodic.\nOur reasoning is generalisable: business happens when both parties grow faster.\n"
    },
    {
        "paper_id": 1507.04767,
        "authors": "Antony Ware, Ilnaz Asadzadeh",
        "title": "Semi-parametric time series modelling with autocopulas",
        "comments": "10 pages, AMMCS-CAIMS 2015 Congress",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present an application of the use of autocopulas for\nmodelling financial time series showing serial dependencies that are not\nnecessarily linear. The approach presented here is semi-parametric in that it\nis characterized by a non-parametric autocopula and parametric marginals. One\nadvantage of using autocopulas is that they provide a general representation of\nthe auto-dependency of the time series, in particular making it possible to\nstudy the interdependence of values of the series at different extremes\nseparately. The specific time series that is studied here comes from daily cash\nflows involving the product of daily natural gas price and daily temperature\ndeviations from normal levels. Seasonality is captured by using a time\ndependent normal inverse Gaussian (NIG) distribution fitted to the raw values.\n"
    },
    {
        "paper_id": 1507.04797,
        "authors": "Jan-Henrik Steg",
        "title": "Symmetric Equilibria in Stochastic Timing Games",
        "comments": "This version: May 20, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct subgame-perfect equilibria with mixed strategies for symmetric\nstochastic timing games with arbitrary strategic incentives. The strategies are\nqualitatively different for local first- or second-mover advantages, which we\nanalyse in turn. When there is a local second-mover advantage, the players may\nconduct a war of attrition with stopping rates that we characterize in terms of\nthe Snell envelope from the theory of optimal stopping. This is a very general\nresult, but it provides a clear interpretation. When there is a local\nfirst-mover advantage, stopping typically results from preemption and is\nabrupt. Equilibria may differ in the degree of preemption, precisely when it is\ntriggered or not. We develop an algorithm to characterize when preemption is\ninevitable and to construct corresponding payoff-maximal symmetric equilibria.\n"
    },
    {
        "paper_id": 1507.04848,
        "authors": "Ali Hosseiny",
        "title": "Violation of Invariance of Measurement for GDP Growth Rate and its\n  Consequences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim here is to address the origins of sustainability for the real growth\nrate in the United States. For over a century of observations on the real GDP\nper capita of the United States a sustainable two percent growth rate has been\nobserved. To find an explanation for this observation I consider the impact of\nutility preferences and the effect of mobility of labor \\& capital on every\nprovided measurement. Mobility of labor results in heterogenous rates of\nincrease in prices which is called Baumol's cost disease phenomenon.\nHeterogeneous rates of inflation then make it impossible to define an invariant\nmeasure for the real growth rate. Paradoxical and ambiguous results already\nhave been observed when different measurements provided by the World Bank have\nbeen compared with the ones from the systems of national accounts (SNA). Such\nambiguity is currently being discussed in economy. I define a toy model for\ncaring out measurements in order to state that this ambiguity can be very\nsignificant. I provide examples in which GDP expands 5 folds while measurements\npercept an expansion around 2 folds. Violation of invariance of the\nmeasurements leads to state that it is hard to compare the growth rate of GDP\nfor a smooth growing country such as the U.S. with a fast growing country such\nas China. Besides, I state that to extrapolate the time that economy of China\npasses the economy of the US we need to consider local metric of the central\nbanks of both countries. Finally I conclude that it is our method of\nmeasurements that leads us to percept the sustainable growth rate.\n"
    },
    {
        "paper_id": 1507.04934,
        "authors": "Wolfgang Kuhle",
        "title": "Darwinian Adverse Selection",
        "comments": "Maximization, Rationality, Economics, Biology, Group Selection",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model to study the role of rationality in economics and biology.\nThe model's agents differ continuously in their ability to make rational\nchoices. The agents' objective is to ensure their individual survival over time\nor, equivalently, to maximize profits. In equilibrium, however, rational agents\nwho maximize their objective survival probability are, individually and\ncollectively, eliminated by the forces of competition. Instead of rationality,\nthere emerges a unique distribution of irrational players who are individually\nnot fit for the struggle of survival. The selection of irrational players over\nrational ones relies on the fact that all rational players coordinate on the\nsame optimal action, which leaves them collectively undiversified and thus\nvulnerable to aggregate risks.\n"
    },
    {
        "paper_id": 1507.0499,
        "authors": "Thilo A. Schmitt, Rudi Sch\\\"afer, Holger Dette, Thomas Guhr",
        "title": "Quantile Correlations: Uncovering temporal dependencies in financial\n  time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct an empirical study using the quantile-based correlation function\nto uncover the temporal dependencies in financial time series. The study uses\nintraday data for the S\\&P 500 stocks from the New York Stock Exchange. After\nestablishing an empirical overview we compare the quantile-based correlation\nfunction to stochastic processes from the GARCH family and find striking\ndifferences. This motivates us to propose the quantile-based correlation\nfunction as a powerful tool to assess the agreements between stochastic\nprocesses and empirical data.\n"
    },
    {
        "paper_id": 1507.05055,
        "authors": "Pat Muldowney",
        "title": "Pricing American and Asian Options",
        "comments": "21 pages, 10 figures,\n  https://sites.google.com/site/stieltjescomplete/home/finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An analytic method for pricing American call options is provided; followed by\nan empirical method for pricing Asian call options. The methodology is the\npricing theory presented in \"A Modern Theory of Random Variation\", by Patrick\nMuldowney, 2012.\n"
    },
    {
        "paper_id": 1507.05203,
        "authors": "Vygintas Gontis, Shlomo Havlin, Aleksejus Kononovicius, Boris\n  Podobnik, H. Eugene Stanley",
        "title": "Stochastic model of financial markets reproducing scaling and memory in\n  volatility return intervals",
        "comments": "19 pages, 8 figures",
        "journal-ref": "Physica A 462 (2016) 1091-1102",
        "doi": "10.1016/j.physa.2016.06.143",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the volatility return intervals in the NYSE and FOREX markets.\nWe explain previous empirical findings using a model based on the interacting\nagent hypothesis instead of the widely-used efficient market hypothesis. We\nderive macroscopic equations based on the microscopic herding interactions of\nagents and find that they are able to reproduce various stylized facts of\ndifferent markets and different assets with the same set of model parameters.\nWe show that the power-law properties and the scaling of return intervals and\nother financial variables have a similar origin and could be a result of a\ngeneral class of non-linear stochastic differential equations derived from a\nmaster equation of an agent system that is coupled by herding interactions.\nSpecifically, we find that this approach enables us to recover the volatility\nreturn interval statistics as well as volatility probability and spectral\ndensities for the NYSE and FOREX markets, for different assets, and for\ndifferent time-scales. We find also that the historical S\\&P500 monthly series\nexhibits the same volatility return interval properties recovered by our\nproposed model. Our statistical results suggest that human herding is so strong\nthat it persists even when other evolving fluctuations perturbate the financial\nsystem.\n"
    },
    {
        "paper_id": 1507.05311,
        "authors": "V.I. Yukalov, E.P. Yukalova, and D. Sornette",
        "title": "Dynamical system theory of periodically collapsing bubbles",
        "comments": "Latex file, 35 pages, 16 figures",
        "journal-ref": "Eur. Phys. J. B 88 (2015) 179",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a reduced form set of two coupled continuous time equations\nlinking the price of a representative asset and the price of a bond, the later\nquantifying the cost of borrowing. The feedbacks between asset prices and bonds\nare mediated by the dependence of their \"fundamental values\" on past asset\nprices and bond themselves. The obtained nonlinear self-referencing price\ndynamics can induce, in a completely objective deterministic way, the\nappearance of periodically exploding bubbles ending in crashes. Technically,\nthe periodically explosive bubbles arise due to the proximity of two types of\nbifurcations as a function of the two key control parameters $b$ and $g$, which\nrepresent, respectively, the sensitivity of the fundamental asset price on past\nasset and bond prices and of the fundamental bond price on past asset prices.\nOne is a Hopf bifurcation, when a stable focus transforms into an unstable\nfocus and a limit cycle appears. The other is a rather unusual bifurcation,\nwhen a stable node and a saddle merge together and disappear, while an unstable\nfocus survives and a limit cycle develops. The lines, where the periodic\nbubbles arise, are analogous to the critical lines of phase transitions in\nstatistical physics. The amplitude of bubbles and waiting times between them\nrespectively diverge with the critical exponents $\\gamma = 1$ and $\\nu = 1/2$,\nas the critical lines are approached.\n"
    },
    {
        "paper_id": 1507.05351,
        "authors": "Yannick Armenti, Stephane Crepey, Samuel Drapeau, Antonis\n  Papapantoleon",
        "title": "Multivariate Shortfall Risk Allocation and Systemic Risk",
        "comments": "Code, results and figures can also be consulted at\n  https://github.com/yarmenti/MSRA",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ongoing concern about systemic risk since the outburst of the global\nfinancial crisis has highlighted the need for risk measures at the level of\nsets of interconnected financial components, such as portfolios, institutions\nor members of clearing houses. The two main issues in systemic risk measurement\nare the computation of an overall reserve level and its allocation to the\ndifferent components according to their systemic relevance. We develop here a\npragmatic approach to systemic risk measurement and allocation based on\nmultivariate shortfall risk measures, where acceptable allocations are first\ncomputed and then aggregated so as to minimize costs. We analyze the\nsensitivity of the risk allocations to various factors and highlight its\nrelevance as an indicator of systemic risk. In particular, we study the\ninterplay between the loss function and the dependence structure of the\ncomponents. Moreover, we address the computational aspects of risk allocation.\nFinally, we apply this methodology to the allocation of the default fund of a\nCCP on real data.\n"
    },
    {
        "paper_id": 1507.05376,
        "authors": "Misha Perepelitsa",
        "title": "The time scales of the aggregate learning and sorting in market entry\n  games with large number of players",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the dynamics of player's strategies in repeated market games,\nwhere the selection of strategies is determined by a learning model. Prior\ntheoretical analysis and experimental data show that after large number of\nplays the average number of agents who decide to enter, per round of the game,\napproaches the market capacity and, after a longer wait, agents are being\nsorted into two groups: the agents in one group rarely enter the market, and in\nthe other, the agents enter almost all the time. In this paper we obtain\nestimates of the characteristic times it takes for both patterns to emerge in\nthe repeated plays of the game. The estimates are given in terms of the\nparameters of the game, assuming that the number of agents is large, the number\nof rounds of the game per unit of time is large, and the characteristic change\nof the propensity per game is small. Our approach is based on the analysis of\nthe partial differential equation for the function $f(t,q)$ that describes the\ndistribution of agents according to their level of propensity to enter the\nmarket, $q,$ at time $t.$\n"
    },
    {
        "paper_id": 1507.05415,
        "authors": "Volodymyr Perederiy",
        "title": "Endogenous Derivation and Forecast of Lifetime PDs",
        "comments": "Keywords: Prediction, Probability of Default, PD, Default Rates,\n  Through-the-Cycle, TtC, Point-in-Time, PiT, Credit Portfolio Model,\n  Systematic Factor, Macroeconomic Factor, Time Series, Autoregression,\n  Bayesian Analysis, IFRS 9, Accounting, Financial Instruments, Lifetime,\n  Expected Credit Losses",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a simple technical approach for the analytical derivation\nof Point-in-Time PD (probability of default) forecasts, with minimal data\nrequirements. The inputs required are the current and future Through-the-Cycle\nPDs of the obligors, their last known default rates, and a measurement of the\nsystematic dependence of the obligors. Technically, the forecasts are made from\nwithin a classical asset-based credit portfolio model, with the additional\nassumption of a simple (first/second order) autoregressive process for the\nsystematic factor. This paper elaborates in detail on the practical issues of\nimplementation, especially on the parametrization alternatives. We also show\nhow the approach can be naturally extended to low-default portfolios with\nvolatile default rates, using Bayesian methodology. Furthermore, expert\njudgments on the current macroeconomic state, although not necessary for the\nforecasts, can be embedded into the model using the Bayesian technique. The\nresulting PD forecasts can be used for the derivation of expected lifetime\ncredit losses as required by the newly adopted accounting standard IFRS 9. In\ndoing so, the presented approach is endogenous, as it does not require any\nexogenous macroeconomic forecasts, which are notoriously unreliable and often\nsubjective. Also, it does not require any dependency modeling between PDs and\nmacroeconomic variables, which often proves to be cumbersome and unstable.\n"
    },
    {
        "paper_id": 1507.05687,
        "authors": "Xiao Fan Liu and Chi Kong Tse",
        "title": "A General Framework for Complex Network Applications",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex network theory has been applied to solving practical problems from\ndifferent domains. In this paper, we present a general framework for complex\nnetwork applications. The keys of a successful application are a thorough\nunderstanding of the real system and a correct mapping of complex network\ntheory to practical problems in the system. Despite of certain limitations\ndiscussed in this paper, complex network theory provides a foundation on which\nto develop powerful tools in analyzing and optimizing large interconnected\nsystems.\n"
    },
    {
        "paper_id": 1507.05865,
        "authors": "Dmitry Kramkov and Kim Weston",
        "title": "Muckenhoupt's $(A_p)$ condition and the existence of the optimal\n  martingale measure",
        "comments": "24 pages",
        "journal-ref": "Stochastic Processes and their Applications, 126(9), 2016, p.\n  2615-2633",
        "doi": "10.1016/j.spa.2016.02.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the problem of optimal investment with utility function defined on\n$(0,\\infty)$, we formulate sufficient conditions for the dual optimizer to be a\nuniformly integrable martingale. Our key requirement consists of the existence\nof a martingale measure whose density process satisfies the probabilistic\nMuckenhoupt $(A_p)$ condition for the power $p=1/(1-a)$, where $a\\in (0,1)$ is\na lower bound on the relative risk-aversion of the utility function. We\nconstruct a counterexample showing that this $(A_p)$ condition is sharp.\n"
    },
    {
        "paper_id": 1507.06015,
        "authors": "Helin Zhu, Tianyi Liu and Enlu Zhou",
        "title": "Risk Quantification in Stochastic Simulation under Input Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When simulating a complex stochastic system, the behavior of output response\ndepends on input parameters estimated from finite real-world data, and the\nfiniteness of data brings input uncertainty into the system. The quantification\nof the impact of input uncertainty on output response has been extensively\nstudied. Most of the existing literature focuses on providing inferences on the\nmean response at the true but unknown input parameter, including point\nestimation and confidence interval construction. Risk quantification of mean\nresponse under input uncertainty often plays an important role in system\nevaluation and control, because it provides inferences on extreme scenarios of\nmean response in all possible input models. To the best of our knowledge, it\nhas rarely been systematically studied in the literature. In this paper, first\nwe introduce risk measures of mean response under input uncertainty, and\npropose a nested Monte Carlo simulation approach to estimate them. Then we\ndevelop asymptotical properties such as consistency and asymptotic normality\nfor the proposed nested risk estimators. We further study the associated budget\nallocation problem for efficient nested risk simulation, and finally use a\nsharing economy example to illustrate the importance of accessing and\ncontrolling risk due to input uncertainty.\n"
    },
    {
        "paper_id": 1507.06219,
        "authors": "Francesco Caravelli, James Requeima, Cozmin Ududec, Ali Ashtari,\n  Tiziana Di Matteo, Tomaso Aste",
        "title": "Multi-scaling of wholesale electricity prices",
        "comments": "14 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We empirically analyze the most volatile component of the electricity price\ntime series from two North-American wholesale electricity markets. We show that\nthese time series exhibit fluctuations which are not described by a Brownian\nMotion, as they show multi-scaling, high Hurst exponents and sharp price\nmovements. We use the generalized Hurst exponent (GHE, $H(q)$) to show that\nalthough these time-series have strong cyclical components, the fluctuations\nexhibit persistent behaviour, i.e., $H(q)>0.5$. We investigate the\neffectiveness of the GHE as a predictive tool in a simple linear forecasting\nmodel, and study the forecast error as a function of $H(q)$, with $q=1$ and\n$q=2$. Our results suggest that the GHE can be used as prediction tool for\nthese time series when the Hurst exponent is dynamically evaluated on rolling\ntime windows of size $\\approx 50 - 100$ hours. These results are also compared\nto the case in which the cyclical components have been subtracted from the time\nseries, showing the importance of cyclicality in the prediction power of the\nHurst exponent.\n"
    },
    {
        "paper_id": 1507.06242,
        "authors": "Stefan Lyocsa, Tomas Vyrost, Eduard Baumohl",
        "title": "Return spillovers around the globe: A network approach",
        "comments": "This work was supported by the Slovak Research and Development Agency\n  under the contract No. APVV-0666-11 and No. APVV-14-0357. Authors also\n  appreciate the support provided from the Slovak Grant Agency for Science\n  (VEGA project No. 1/0392/15)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a rolling windows analysis of filtered and aligned stock index returns\nfrom 40 countries during the period 2006-2014, we construct Granger causality\nnetworks and investigate the ensuing structure of the relationships by studying\nnetwork properties and fitting spatial probit models. We provide evidence that\nstock market volatility and market size increases, while foreign exchange\nvolatility decreases the probability of return spillover from a given market.\nWe also show that market development and returns on the foreign exchange market\nand stock market also matter, but they exhibit significant time-varying\nbehaviour with alternating effects. These results suggest that higher market\nintegration periods are alternated with periods where investors appear to be\nchasing returns. Despite the significance of market characteristics and market\nconditions, what in reality matters for information propagation is the temporal\ndistance between closing hours, i.e. the temporal proximity effect. This\nimplies that choosing markets which trade in similar hours bears additional\ncosts to investors, as the probability of return spillovers increases. The same\neffect was observed with regard to the temporal distance to the US market.\nFinally, we confirm the existence of the preferential attachment effect, i.e.\nthe probability of a given market to propagate return spillovers to a new\nmarket depends endogenously and positively on the existing number of return\nspillovers from that market.\n"
    },
    {
        "paper_id": 1507.06477,
        "authors": "Takayuki Mizuno, Takaaki Ohnishi, Tsutomu Watanabe",
        "title": "Novel and topical business news and their impact on stock market\n  activities",
        "comments": "8 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an indicator to measure the degree to which a particular news\narticle is novel, as well as an indicator to measure the degree to which a\nparticular news item attracts attention from investors. The novelty measure is\nobtained by comparing the extent to which a particular news article is similar\nto earlier news articles, and an article is regarded as novel if there was no\nsimilar article before it. On the other hand, we say a news item receives a lot\nof attention and thus is highly topical if it is simultaneously reported by\nmany news agencies and read by many investors who receive news from those\nagencies. The topicality measure for a news item is obtained by counting the\nnumber of news articles whose content is similar to an original news article\nbut which are delivered by other news agencies. To check the performance of the\nindicators, we empirically examine how these indicators are correlated with\nintraday financial market indicators such as the number of transactions and\nprice volatility. Specifically, we use a dataset consisting of over 90 million\nbusiness news articles reported in English and a dataset consisting of\nminute-by-minute stock prices on the New York Stock Exchange and the NASDAQ\nStock Market from 2003 to 2014, and show that stock prices and transaction\nvolumes exhibited a significant response to a news article when it is novel and\ntopical.\n"
    },
    {
        "paper_id": 1507.06514,
        "authors": "A. Sadoghi, J. Vecer",
        "title": "Optimum Liquidation Problem Associated with the Poisson Cluster Process",
        "comments": "46 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research, we develop a trading strategy for the discrete-time optimal\nliquidation problem of large order trading with different market\nmicrostructures in an illiquid market. In this framework, the flow of orders\ncan be viewed as a point process with stochastic intensity. We model the price\nimpact as a linear function of a self-exciting dynamic process. We formulate\nthe liquidation problem as a discrete-time Markov Decision Processes, where the\nstate process is a Piecewise Deterministic Markov Process (PDMP). The numerical\nresults indicate that an optimal trading strategy is dependent on\ncharacteristics of the market microstructure. When no orders above certain\nvalue come the optimal solution takes offers in the lower levels of the limit\norder book in order to prevent not filling of orders and facing final inventory\ncosts.\n"
    },
    {
        "paper_id": 1507.0685,
        "authors": "Xun Li and Zuo Quan Xu",
        "title": "Continuous-Time Mean-Variance Portfolio Selection with Constraints on\n  Wealth and Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider continuous-time mean-variance portfolio selection with bankruptcy\nprohibition under convex cone portfolio constraints. This is a long-standing\nand difficult problem not only because of its theoretical significance, but\nalso for its practical importance. First of all, we transform the above problem\ninto an equivalent mean-variance problem with bankruptcy prohibition without\nportfolio constraints. The latter is then treated using martingale theory. Our\nfindings indicate that we can directly present the semi-analytical expressions\nof the pre-committed efficient mean-variance policy without a viscosity\nsolution technique but within a general framework of the cone portfolio\nconstraints. The numerical simulation also sheds light on results established\nin this paper.\n"
    },
    {
        "paper_id": 1507.07052,
        "authors": "Weibing Huang, Charles-Albert Lehalle, Mathieu Rosenbaum",
        "title": "How to predict the consequences of a tick value change? Evidence from\n  the Tokyo Stock Exchange pilot program",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The tick value is a crucial component of market design and is often\nconsidered the most suitable tool to mitigate the effects of high frequency\ntrading. The goal of this paper is to demonstrate that the approach introduced\nin Dayri and Rosenbaum (2015) allows for an ex ante assessment of the\nconsequences of a tick value change on the microstructure of an asset. To that\npurpose, we analyze the pilot program on tick value modifications started in\n2014 by the Tokyo Stock Exchange in light of this methodology. We focus on\nforecasting the future cost of market and limit orders after a tick value\nchange and show that our predictions are very accurate. Furthermore, for each\nasset involved in the pilot program, we are able to define (ex ante) an optimal\ntick value. This enables us to classify the stocks according to the relevance\nof their tick value, before and after its modification.\n"
    },
    {
        "paper_id": 1507.07162,
        "authors": "Pavel V. Shevchenko, Jonas Hirz and Uwe Schmock",
        "title": "Forecasting Leading Death Causes in Australia using Extended\n  CreditRisk$+$",
        "comments": "arXiv admin note: text overlap with arXiv:1505.04757",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently we developed a new framework in Hirz et al (2015) to model\nstochastic mortality using extended CreditRisk$^+$ methodology which is very\ndifferent from traditional time series methods used for mortality modelling\npreviously. In this framework, deaths are driven by common latent stochastic\nrisk factors which may be interpreted as death causes like neoplasms,\ncirculatory diseases or idiosyncratic components. These common factors\nintroduce dependence between policyholders in annuity portfolios or between\ndeath events in population. This framework can be used to construct life tables\nbased on mortality rate forecast. Moreover this framework allows stress testing\nand, therefore, offers insight into how certain health scenarios influence\nannuity payments of an insurer. Such scenarios may include improvement in\nhealth treatments or better medication. In this paper, using publicly available\ndata for Australia, we estimate the model using Markov chain Monte Carlo method\nto identify leading death causes across all age groups including long term\nforecast for 2031 and 2051. On top of general reduced mortality, the proportion\nof deaths for certain certain causes has changed massively over the period 1987\nto 2011. Our model forecasts suggest that if these trends persist, then the\nfuture gives a whole new picture of mortality for people aged above 40 years.\nNeoplasms will become the overall number-one death cause. Moreover, deaths due\nto mental and behavioural disorders are very likely to surge whilst deaths due\nto circulatory diseases will tend to decrease. This potential increase in\ndeaths due to mental and behavioural disorders for older ages will have a\nmassive impact on social systems as, typically, such patients need long-term\ngeriatric care.\n"
    },
    {
        "paper_id": 1507.07214,
        "authors": "Andrei N. Soklakov",
        "title": "One trade at a time -- unraveling the Equity Premium Puzzle",
        "comments": "21 pages, 4 figures",
        "journal-ref": "published as Supplementary materials for \"Economics of\n  Disagreement\", Entropy 22 (8), 860 (2020)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets provide a natural quantitative lab for understanding some\nof the most advanced human behaviours. Among them is the use of mathematical\ntools known as financial instruments. Besides money, the two most fundamental\nfinancial instruments are bonds and equities. More than 30 years ago Mehra and\nPrescott found the numerical performance of equities relative to government\nbonds could not be explained by consumption-based (mainstream) economic\ntheories. This empirical observation, known as the Equity Premium Puzzle, has\nbeen defying mainstream economics ever since. The recent financial crisis\nrevealed an even deeper need for understanding financial products. We show how\nunderstanding the rational nature of product design resolves the Equity Premium\nPuzzle. In doing so we obtain an experimentally tested theory of product\ndesign.\n"
    },
    {
        "paper_id": 1507.07216,
        "authors": "Andrei N. Soklakov",
        "title": "Model Risk Analysis via Investment Structuring",
        "comments": "11 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \"What are the origins of risks?\" and \"How material are they?\" -- these are\nthe two most fundamental questions of any risk analysis. Quantitative\nStructuring -- a technology for building financial products -- provides\neconomically meaningful answers for both of these questions. It does so by\nconsidering risk as an investment opportunity. The structure of the investment\nreveals the precise sources of risk and its expected performance measures\nmateriality. We demonstrate these capabilities of Quantitative Structuring\nusing a concrete practical example -- model risk in options on vol-targeted\nindices.\n"
    },
    {
        "paper_id": 1507.07219,
        "authors": "Andrei N. Soklakov",
        "title": "Why Quantitative Structuring?",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quality-designed consumer products are easy to recognize. Wouldn't it be\ngreat if the quality of financial products became just as apparent? This paper\nis addressed to financial practitioners. It provides an informal introduction\nto Quantitative Structuring -- a technology of manufacturing quality financial\nproducts (information derivatives). The presentation is arranged in three\nparts: the main text assumes no prior knowledge of the topic; important\ndetailed discussions are arranged as a set of appendices; finally, a list of\nreferences provides further details including applications beyond product\ndesign: from model risk to economics and statistics.\n"
    },
    {
        "paper_id": 1507.0787,
        "authors": "Samuel R\\\"onnqvist and Peter Sarlin",
        "title": "Detect & Describe: Deep learning of bank stress in the news",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  News is a pertinent source of information on financial risks and stress\nfactors, which nevertheless is challenging to harness due to the sparse and\nunstructured nature of natural text. We propose an approach based on\ndistributional semantics and deep learning with neural networks to model and\nlink text to a scarce set of bank distress events. Through unsupervised\ntraining, we learn semantic vector representations of news articles as\npredictors of distress events. The predictive model that we learn can signal\ncoinciding stress with an aggregated index at bank or European level, while\ncrucially allowing for automatic extraction of text descriptions of the events,\nbased on passages with high stress levels. The method offers insight that\nmodels based on other types of data cannot provide, while offering a general\nmeans for interpreting this type of semantic-predictive model. We model bank\ndistress with data on 243 events and 6.6M news articles for 101 large European\nbanks.\n"
    },
    {
        "paper_id": 1507.08333,
        "authors": "Josselin Garnier, George Papanicolaou, Tzu-Wei Yang",
        "title": "A risk analysis for a system stabilized by a central agent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate and analyze a multi-agent model for the evolution of individual\nand systemic risk in which the local agents interact with each other through a\ncentral agent who, in turn, is influenced by the mean field of the local\nagents. The central agent is stabilized by a bistable potential, the only\nstabilizing force in the system. The local agents derive their stability only\nfrom the central agent. In the mean field limit of a large number of local\nagents we show that the systemic risk decreases when the strength of the\ninteraction of the local agents with the central agent increases. This means\nthat the probability of transition from one of the two stable quasi-equilibria\nto the other one decreases. We also show that the systemic risk increases when\nthe strength of the interaction of the central agent with the mean field of the\nlocal agents increases. Following the financial interpretation of such models\nand their behavior given in our previous paper (Garnier, Papanicolaou and Yang,\nSIAM J. Fin. Math. 4, 2013, 151-184), we may interpret the results of this\npaper in the following way. From the point of view of systemic risk, and while\nkeeping the perceived risk of the local agents approximately constant, it is\nbetter to strengthen the interaction of the local agents with the central agent\nthan the other way around.\n"
    },
    {
        "paper_id": 1507.08713,
        "authors": "Bahman Angoshtari, Erhan Bayraktar, Virginia R. Young",
        "title": "Minimizing the Probability of Lifetime Drawdown under Constant\n  Consumption",
        "comments": "To appear in Insurance: Mathematics and Economics. Keywords: Optimal\n  investment, stochastic optimal control, probability of drawdown. arXiv admin\n  note: text overlap with arXiv:0806.2358",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We assume that an individual invests in a financial market with one riskless\nand one risky asset, with the latter's price following geometric Brownian\nmotion as in the Black-Scholes model. Under a constant rate of consumption, we\nfind the optimal investment strategy for the individual who wishes to minimize\nthe probability that her wealth drops below some fixed proportion of her\nmaximum wealth to date, the so-called probability of {\\it lifetime drawdown}.\nIf maximum wealth is less than a particular value, $m^*$, then the individual\noptimally invests in such a way that maximum wealth never increases above its\ncurrent value. By contrast, if maximum wealth is greater than $m^*$ but less\nthan the safe level, then the individual optimally allows the maximum to\nincrease to the safe level.\n"
    },
    {
        "paper_id": 1507.08738,
        "authors": "Xiaolin Luo and Pavel Shevchenko",
        "title": "Variable Annuity with GMWB: surrender or not, that is the question",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1410.8609",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under the optimal withdrawal strategy of a policyholder, the pricing of\nvariable annuities with Guaranteed Minimum Withdrawal Benefit (GMWB) is an\noptimal stochastic control problem. The surrender feature available in marketed\nproducts allows termination of the contract before maturity, making it also an\noptimal stopping problem. Although the surrender feature is quite common in\nvariable annuity contracts, there appears to be no published analysis and\nresults for this feature in GMWB under optimal policyholder behaviour - results\nfound in the literature so far are consistent with the absence of such a\nfeature. Also, it is of practical interest to see how the much simpler\nbang-bang strategy, although not optimal for GMWB, compares with optimal GMWB\nstrategy with surrender option.\n  In this paper we extend our recently developed algorithm (Luo and Shevchenko\n2015a) to include surrender option in GMWB and compare prices under different\npolicyholder strategies: optimal, static and bang-bang. Results indicate that\nfollowing a simple but sub-optimal bang-bang strategy does not lead to\nsignificant reduction in the price or equivalently in the fee, in comparison\nwith the optimal strategy. We observed that the extra value added by the\nsurrender option could add very significant value to the GMWB contract. We also\nperformed calculations for static withdrawal with surrender option, which is\nthe same as bang-bang minus the \"no-withdrawal\" choice. We find that the fee\nfor such contract is only less than 1% smaller when compared to the case of\nbang-bang strategy, meaning that th \"no-withdrawal\" option adds little value to\nthe contract.\n"
    },
    {
        "paper_id": 1507.08779,
        "authors": "Giacomo Bormetti, Damiano Brigo, Marco Francischello, Andrea\n  Pallavicini",
        "title": "Impact of Multiple Curve Dynamics in Credit Valuation Adjustments under\n  Collateralization",
        "comments": "arXiv admin note: text overlap with arXiv:1304.1397",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a detailed analysis of interest rate derivatives valuation under\ncredit risk and collateral modeling. We show how the credit and collateral\nextended valuation framework in Pallavicini et al (2011), and the related\ncollateralized valuation measure, can be helpful in defining the key market\nrates underlying the multiple interest rate curves that characterize current\ninterest rate markets. A key point is that spot Libor rates are to be treated\nas market primitives rather than being defined by no-arbitrage relationships.\nWe formulate a consistent realistic dynamics for the different rates emerging\nfrom our analysis and compare the resulting model performances to simpler\nmodels used in the industry. We include the often neglected margin period of\nrisk, showing how this feature may increase the impact of different rates\ndynamics on valuation. We point out limitations of multiple curve models with\ndeterministic basis considering valuation of particularly sensitive products\nsuch as basis swaps. We stress that a proper wrong way risk analysis for such\nproducts requires a model with a stochastic basis and we show numerical results\nconfirming this fact.\n"
    },
    {
        "paper_id": 1507.08863,
        "authors": "Fabio Sabatini, Francesco Sarracino",
        "title": "Keeping up with the e-Joneses: Do online social networks raise social\n  comparisons?",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online social networks such as Facebook disclose an unprecedented volume of\npersonal information amplifying the occasions for social comparisons. We test\nthe hypothesis that the use of social networking sites (SNS) increases people's\ndissatisfaction with their income. After addressing endogeneity issues, our\nresults suggest that SNS users have a higher probability to compare their\nachievements with those of others. This effect seems stronger than the one\nexerted by TV watching, it is particularly strong for younger people, and it\naffects men and women in a similar way.\n"
    },
    {
        "paper_id": 1507.08937,
        "authors": "Stefan Haring and Ronald Hochreiter",
        "title": "Efficient and robust calibration of the Heston option pricing model for\n  American options using an improved Cuckoo Search Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper an improved Cuckoo Search Algorithm is developed to allow for\nan efficient and robust calibration of the Heston option pricing model for\nAmerican options. Calibration of stochastic volatility models like the Heston\nis significantly harder than classical option pricing models as more parameters\nhave to be estimated. The difficult task of calibrating one of these models to\nAmerican Put options data is the main objective of this paper. Numerical\nresults are shown to substantiate the suitability of the chosen method to\ntackle this problem.\n"
    },
    {
        "paper_id": 1508.0009,
        "authors": "Man Chung Fung, Katja Ignatieva, Michael Sherris",
        "title": "Managing Systematic Mortality Risk in Life Annuities: An Application of\n  Longevity Derivatives",
        "comments": "23 pages; under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper assesses the hedge effectiveness of an index-based longevity swap\nand a longevity cap. Although swaps are a natural instrument for hedging\nlongevity risk, derivatives with non-linear pay-offs, such as longevity caps,\nalso provide downside protection. A tractable stochastic mortality model with\nage dependent drift and volatility is developed and analytical formulae for\nprices of these longevity derivatives are derived. Hedge effectiveness is\nconsidered for a hypothetical life annuity portfolio. The hedging of the life\nannuity portfolio is comprehensively assessed for a range of assumptions for\nthe market price of longevity risk, the term to maturity of the hedging\ninstruments, as well as the size of the underlying annuity portfolio. The model\nis calibrated using Australian mortality data. The results provide a\ncomprehensive analysis of longevity hedging, highlighting the risk management\nbenefits and costs of linear and nonlinear payoff structures.\n"
    },
    {
        "paper_id": 1508.00108,
        "authors": "Andr\\'es Sosa and Ernesto Mordecki",
        "title": "Modelling the Uruguayan debt through gaussians models",
        "comments": "18 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model bond's price curves corresponding to the sovereign uruguayan debt\nnominated in USD, as an alternative to the official bond prices publication\nreleased by the Central Bank of Uruguay (CBU). Four different gaussian models\nare fitted, based on historical data issued by the CBU, corresponding to some\nof the more frequently traded bonds. The main difficulty we approach is the\nabsence of liquidity in the bond market. Nevertheless the adjustment is\nrelatively good, giving the possibility of non-arbitrage pricing of the whole\nfamily of non traded instruments, and also the possibility of pricing\nderivative securities.\n"
    },
    {
        "paper_id": 1508.00275,
        "authors": "Jean-Philippe Bouchaud (Capital Fund Management and Ecole\n  Polytechnique)",
        "title": "On growth-optimal tax rates and the issue of wealth inequalities",
        "comments": "9 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2015/11/P11011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a highly stylized, yet non trivial model of the economy, with a\npublic and private sector coupled through a wealth tax and a redistribution\npolicy. The model can be fully solved analytically, and allows one to address\nthe question of optimal taxation and of wealth inequalities. We find that\naccording to the assumption made on the relative performance of public and\nprivate sectors, three situations are possible. Not surprisingly, the optimal\nwealth tax rate is either 0% for a deeply dysfunctional government and/or\nhighly productive private sector, or 100 % for a highly efficient public sector\nand/or debilitated/risk averse private investors. If the gap between the\npublic/private performance is moderate, there is an optimal positive wealth tax\nrate maximizing economic growth, even -- counter-intuitively -- when the\nprivate sector generates more growth. The compromise between profitable private\ninvestments and taxation however leads to a residual level of inequalities. The\nmechanism leading to an optimal growth rate is related the well-known\nexplore/exploit trade-off.\n"
    },
    {
        "paper_id": 1508.0031,
        "authors": "James Risk and Michael Ludkovski",
        "title": "Statistical Emulators for Pricing and Hedging Longevity Risk Products",
        "comments": "29 pages and 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the use of statistical emulators for the purpose of valuing\nmortality-linked contracts in stochastic mortality models. Such models\ntypically require (nested) evaluation of expected values of nonlinear\nfunctionals of multi-dimensional stochastic processes. Except in the simplest\ncases, no closed-form expressions are available, necessitating numerical\napproximation. Rather than building ad hoc analytic approximations, we advocate\nthe use of modern statistical tools from machine learning to generate a\nflexible, non-parametric surrogate for the true mappings. This method allows\nperformance guarantees regarding approximation accuracy and removes the need\nfor nested simulation. We illustrate our approach with case studies involving\n(i) a Lee-Carter model with mortality shocks, (ii) index-based static hedging\nwith longevity basis risk; (iii) a Cairns-Blake-Dowd stochastic survival\nprobability model.\n"
    },
    {
        "paper_id": 1508.00322,
        "authors": "Man Chung Fung, Gareth W. Peters, Pavel V. Shevchenko",
        "title": "A State-Space Estimation of the Lee-Carter Mortality Model and\n  Implications for Annuity Pricing",
        "comments": "9 pages; conference paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we investigate a state-space representation of the Lee-Carter\nmodel which is a benchmark stochastic mortality model for forecasting\nage-specific death rates. Existing relevant literature focuses mainly on\nmortality forecasting or pricing of longevity derivatives, while the full\nimplications and methods of using the state-space representation of the\nLee-Carter model in pricing retirement income products is yet to be examined.\nThe main contribution of this article is twofold. First, we provide a rigorous\nand detailed derivation of the posterior distributions of the parameters and\nthe latent process of the Lee-Carter model via Gibbs sampling. Our assumption\nfor priors is slightly more general than the current literature in this area.\nMoreover, we suggest a new form of identification constraint not yet utilised\nin the actuarial literature that proves to be a more convenient approach for\nestimating the model under the state-space framework. Second, by exploiting the\nposterior distribution of the latent process and parameters, we examine the\npricing range of annuities, taking into account the stochastic nature of the\ndynamics of the mortality rates. In this way we aim to capture the impact of\nlongevity risk on the pricing of annuities. The outcome of our study\ndemonstrates that an annuity price can be more than 4% under-valued when\ndifferent assumptions are made on determining the survival curve constructed\nfrom the distribution of the forecasted death rates. Given that a typical\nannuity portfolio consists of a large number of policies with maturities which\nspan decades, we conclude that the impact of longevity risk on the accurate\npricing of annuities is a significant issue to be further researched. In\naddition, we find that mis-pricing is increasingly more pronounced for older\nages as well as for annuity policies having a longer maturity.\n"
    },
    {
        "paper_id": 1508.00511,
        "authors": "Smicha Ait Amokthar, Nadjia El Saadi, Yacine Belarbi",
        "title": "Mod\\'{e}lisation spatiale de la formation des agglom\\'{e}rations dans la\n  zone alg\\'{e}roise",
        "comments": "12 pages, in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this study is to analyze the dynamics underlying Algiers urban\narea formation with reference to The New Economic Geography (NEG) theories and\nmore precisely to the paper of Paul Krugman (1991), \"Increasing returns and\neconomic geography\" which explains the mechanisms of economic activities\nconcentration through two types of forces: centripetal forces enhancing the\neconomic activities concentration and centrifugal forces hindering the\nagglomeration process. In fact, these mechanisms are translated into a system\nof nonlinear equations which is very hard to solve analytically. As a\nconsequence, the use of numerical methods is highly advocated. We present some\nnumerical simulations using real Algerian data.\n"
    },
    {
        "paper_id": 1508.00607,
        "authors": "Lawrence Carr",
        "title": "Existence of continuous euclidean embeddings for a weak class of orders",
        "comments": "8 pages, edited for a mathematical audience",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that if $X$ is a topological space that admits Debreu's classical\nutility theorem (eg.\\ $X$ is separable and connected, second countable, etc.),\nthen order relations on $X$ satisfying milder completeness conditions can be\ncontinuously embedded in $\\mathbb R^I$ for $I$ some index set. In the\nparticular case where $X$ is a compact metric space, this closes a conjecture\nof Nishimura \\& Ok (2015). We also show that when $\\mathbb R^I$ is given a\nnon-standard partial order coinciding with Pareto improvement, the analogous\nembedding theorem fails to hold in the continuous case.\n"
    },
    {
        "paper_id": 1508.00632,
        "authors": "Peter Carr, Roger Lee, Matthew Lorig",
        "title": "Robust replication of barrier-style claims on price and volatility",
        "comments": "24 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how to price and replicate a variety of barrier-style claims written\non the $\\log$ price $X$ and quadratic variation $\\langle X \\rangle$ of a risky\nasset. Our framework assumes no arbitrage, frictionless markets and zero\ninterest rates. We model the risky asset as a strictly positive continuous\nsemimartingale with an independent volatility process. The volatility process\nmay exhibit jumps and may be non-Markovian. As hedging instruments, we use only\nthe underlying risky asset, zero-coupon bonds, and European calls and puts with\nthe same maturity as the barrier-style claim. We consider knock-in, knock-out\nand rebate claims in single and double barrier varieties.\n"
    },
    {
        "paper_id": 1508.00668,
        "authors": "Xiaolin Luo and Pavel V. Shevchenko",
        "title": "Valuation of capital protection options",
        "comments": "This working paper formed part of a larger published paper P.V.\n  Shevchenko and X. Luo (2016). A unified pricing of variable annuity\n  guarantees under the optimal stochastic control framework. Risks 4(3),\n  22:1-22:31, doi:10.3390/risks4030022, arXiv:1605.00339",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents numerical algorithm and results for pricing a capital\nprotection option offered by many asset managers for investment portfolios to\ntake advantage of market growth and protect savings. Under optimal withdrawal\npolicyholder behaviour the pricing of such a product is an optimal stochastic\ncontrol problem that cannot be solved using Monte Carlo method. In low\ndimension case, it can be solved using PDE based methods such as finite\ndifference. In this paper, we develop a much more efficient Gauss-Hermite\nquadrature method with a one-dimensional cubic spline for calculation of the\nexpectation between withdrawal/reset dates, and a bi-cubic spline interpolation\nfor applying the jump conditions across withdrawal/reset dates. We show results\nfor both static and dynamic withdrawals and for both the asset accumulation and\nthe pension phases (different penalties for any excessive withdrawal) in the\nretirement investment cycle. To evaluate products with capital protection\noption, it is common industry practice to assume static withdrawals and use\nMonte Carlo method. As a result, the fair fee is underpriced if policyholder\nbehaves optimally. We found that extra fee that has to be charged to counter\nthe optimal policyholder behaviour is most significant at smaller interest rate\nand higher volatility levels, and it is sensitive to the penalty threshold. At\nlow interest rate and a moderate penalty threshold level (15% of the portfolio\nvalue per annum) typically set in practice, the extra fee due to optimal\nwithdrawal can be as high as 40% and more on top of the base case of no\nwithdrawal or the case of fixed withdrawals at the penalty threshold.\n"
    },
    {
        "paper_id": 1508.00893,
        "authors": "Oussama Fadil, Jake Soloff",
        "title": "Information Cascades and Online Rating Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through mathematical analysis and simulations, online ratings and their\nimpact on businesses are characterized through two parameters: an inherent and\nobjective restaurant quality factor, and the accuracy of customers' gut feeling\nabout a business. Within this model, it is found that online ratings are seldom\naccurate mainly because of the low or high accuracy in customers' gut feelings.\n"
    },
    {
        "paper_id": 1508.00975,
        "authors": "Su Do Yi, Seung Ki Baek, Guillaume Chevereau, and Eric Bertin",
        "title": "Symmetry restoration by pricing in a duopoly of perishable goods",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2015/11/P11001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Competition is a main tenet of economics, and the reason is that a perfectly\ncompetitive equilibrium is Pareto-efficient in the absence of externalities and\npublic goods. Whether a product is selected in a market crucially relates to\nits competitiveness, but the selection in turn affects the landscape of\ncompetition. Such a feedback mechanism has been illustrated in a duopoly model\nby Lambert et al., in which a buyer's satisfaction is updated depending on the\n{\\em freshness} of a purchased product. The probability for buyer $n$ to select\nseller $i$ is assumed to be $p_{n,i} \\propto e^{ S_{n,i}/T}$, where $S_{n,i}$\nis the buyer's satisfaction and $T$ is an effective temperature to introduce\nstochasticity. If $T$ decreases below a critical point $T_c$, the system\nundergoes a transition from a symmetric phase to an asymmetric one, in which\nonly one of the two sellers is selected. In this work, we extend the model by\nincorporating a simple price system. By considering a greed factor $g$ to\ncontrol how the satisfaction depends on the price, we argue the existence of an\noscillatory phase in addition to the symmetric and asymmetric ones in the\n$(T,g)$ plane, and estimate the phase boundaries through mean-field\napproximations. The analytic results show that the market preserves the\ninherent symmetry between the sellers for lower $T$ in the presence of the\nprice system, which is confirmed by our numerical simulations.\n"
    },
    {
        "paper_id": 1508.01661,
        "authors": "Jaroslava Hlouskova and Leopold S\\\"ogner",
        "title": "GMM Estimation of Affine Term Structure Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates parameter estimation of affine term structure\nmodels by means of the generalized method of moments. Exact moments of the\naffine latent process as well as of the yields are obtained by using results\nderived for p-polynomial processes. Then the generalized method of moments,\ncombined with Quasi-Bayesian methods, is used to get reliable parameter\nestimates and to perform inference. After a simulation study, the estimation\nprocedure is applied to empirical interest rate data.\n"
    },
    {
        "paper_id": 1508.01914,
        "authors": "Bahman Angoshtari, Erhan Bayraktar, Virginia R. Young",
        "title": "Minimizing the Expected Lifetime Spent in Drawdown under Proportional\n  Consumption",
        "comments": "This paper is to appear in Finance Research Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the optimal amount to invest in a Black-Scholes financial market\nfor an individual who consumes at a rate equal to a constant proportion of her\nwealth and who wishes to minimize the expected time that her wealth spends in\ndrawdown during her lifetime. Drawdown occurs when wealth is less than some\nfixed proportion of maximum wealth. We compare the optimal investment strategy\nwith those for three related goal-seeking problems and learn that the\nindividual is myopic in her investing behavior, as expected from other\ngoal-seeking research.\n"
    },
    {
        "paper_id": 1508.02056,
        "authors": "S.A. Mukul, A.Z.M.M. Rashid, M.B. Uddin, N.A. Khan",
        "title": "Role of non-timber forest products in sustaining forest-based\n  livelihoods and rural households' resilience capacity in and around protected\n  area- a Bangladesh study",
        "comments": "To appear in Journal of Environmental Planning and Management, 2015",
        "journal-ref": null,
        "doi": "10.1080/09640568.2015.1035774",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  People in developing world derive a significant part of their livelihoods\nfrom various forest products, particularly non-timber forest products. This\narticle attempts to explore the contribution of NTFPs in sustaining\nforest-based rural livelihood in and around a protected area of Bangladesh, and\ntheir potential role in enhancing households resilience capacity. Based on\nempirical investigation our study revealed that, local communities gather a\nsubstantial amount of NTFPs from national park despite the official\nrestrictions. 27 percent households of the area received at least some cash\nbenefit from the collection, processing and selling of NTFPs, and NTFPs\ncontribute as HHs primary, supplementary and emergency sources of income. NTFPs\nalso constituted an estimated 19 percent of HHs net annual income, and were the\nprimary occupation for about 18 percent of the HHs. HHs dependency on nearby\nforests for various NTFPs varied vis-a-vis their socio-economic condition as\nwell as with their location from the park. Based on our case study the article\nalso offers some clues for improving the situation in PA.\n"
    },
    {
        "paper_id": 1508.02203,
        "authors": "Sabiou Inoua",
        "title": "The Intrinsic Instability of Financial Markets",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we explain the wild fluctuations of financial prices from the\nintrinsic amplifying feedback of speculative supply and demand. Formally, we\nshow that an asset return follows a multiplicative random growth with exogenous\ninput, which is well-known to be a generic power-law generating process, and\nwhich could thus easily explain the well-established power-law distribution of\nreturns, and other related variables. Moreover, the theory we develop here is a\ngeneral framework where competing ideas can be discussed in a unified way. The\ndominant random walk model, for instance, is easily derived in this framework\nif we superimpose market clearing (central to neoclassical economics). It\ncorresponds to the case where the feedback in price dynamics is ignored in\nfavor of the external input, namely the random inflow of news from the real\neconomy.\n"
    },
    {
        "paper_id": 1508.02367,
        "authors": "Zachary Feinstein and Birgit Rudloff",
        "title": "A recursive algorithm for multivariate risk measures and a set-valued\n  Bellman's principle",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1007/s10898-016-0459-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A method for calculating multi-portfolio time consistent multivariate risk\nmeasures in discrete time is presented. Market models for $d$ assets with\ntransaction costs or illiquidity and possible trading constraints are\nconsidered on a finite probability space. The set of capital requirements at\neach time and state is calculated recursively backwards in time along the event\ntree. We motivate why the proposed procedure can be seen as a set-valued\nBellman's principle, that might be of independent interest within the growing\nfield of set optimization. We give conditions under which the backwards\ncalculation of the sets reduces to solving a sequence of linear, respectively\nconvex vector optimization problems. Numerical examples are given and include\nsuperhedging under illiquidity, the set-valued entropic risk measure, and the\nmulti-portfolio time consistent version of the relaxed worst case risk measure\nand of the set-valued average value at risk.\n"
    },
    {
        "paper_id": 1508.02473,
        "authors": "Jie Ding, Vahid Tarokh, Yuhong Yang",
        "title": "Bridging AIC and BIC: a new criterion for autoregression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new criterion to determine the order of an autoregressive\nmodel fitted to time series data. It has the benefits of the two well-known\nmodel selection techniques, the Akaike information criterion and the Bayesian\ninformation criterion. When the data is generated from a finite order\nautoregression, the Bayesian information criterion is known to be consistent,\nand so is the new criterion. When the true order is infinity or suitably high\nwith respect to the sample size, the Akaike information criterion is known to\nbe efficient in the sense that its prediction performance is asymptotically\nequivalent to the best offered by the candidate models; in this case, the new\ncriterion behaves in a similar manner. Different from the two classical\ncriteria, the proposed criterion adaptively achieves either consistency or\nefficiency depending on the underlying true model. In practice where the\nobserved time series is given without any prior information about the model\nspecification, the proposed order selection criterion is more flexible and\nrobust compared with classical approaches. Numerical results are presented\ndemonstrating the adaptivity of the proposed technique when applied to various\ndatasets.\n"
    },
    {
        "paper_id": 1508.02476,
        "authors": "Richard Vale",
        "title": "A Model for Tax Evasion with Some Realistic Properties",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a discrete-time dynamic model of income tax evasion. The model is\nsolved exactly in the case of a single taxpayer and shown to have some\nrealistic properties, including avoiding the Yitzhaki paradox. The extension to\nan agent-based model with a network of taxpayers is also investigated.\n"
    },
    {
        "paper_id": 1508.02636,
        "authors": "Maojiao Ye and Guoqiang Hu",
        "title": "Game Design and Analysis for Price based Demand Response: An Aggregate\n  Game Approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/TCYB.2016.2524452",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, an aggregate game approach is proposed for the modeling and\nanalysis of energy consumption control in smart grid. Since the electricity\nuser's cost function depends on the aggregate load, which is unknown to the end\nusers, an aggregate load estimator is employed to estimate it. Based on the\ncommunication among the users about their estimations on the aggregate load,\nNash equilibrium seeking strategies are proposed for the electricity users. By\nusing singular perturbation analysis and Lyapunov stability analysis, a local\nconvergence result to the Nash equilibrium is presented for the energy\nconsumption game that may have multiple Nash equilibria. For the energy\nconsumption game with a unique Nash equilibrium, it is shown that the players'\nstrategies converge to the Nash equilibrium non-locally. More specially, if the\nunique Nash equilibrium is an inner Nash equilibrium, then the convergence rate\ncan be quantified. Energy consumption game with stubborn players is also\ninvestigated. Convergence to the best response strategies for the rational\nplayers is ensured. Numerical examples are provided to verify the effectiveness\nof the proposed methods.\n"
    },
    {
        "paper_id": 1508.02749,
        "authors": "Georg Mainik",
        "title": "Risk aggregation with empirical margins: Latin hypercubes, empirical\n  copulas, and convergence of sum distributions",
        "comments": "Manuscript accepted in the Journal of Multivariate Analysis",
        "journal-ref": null,
        "doi": "10.1016/j.jmva.2015.07.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies convergence properties of multivariate distributions\nconstructed by endowing empirical margins with a copula. This setting includes\nLatin Hypercube Sampling with dependence, also known as the Iman--Conover\nmethod. The primary question addressed here is the convergence of the component\nsum, which is relevant to risk aggregation in insurance and finance.\n  This paper shows that a CLT for the aggregated risk distribution is not\navailable, so that the underlying mathematical problem goes beyond classic\nfunctional CLTs for empirical copulas. This issue is relevant to Monte-Carlo\nbased risk aggregation in all multivariate models generated by plugging\nempirical margins into a copula.\n  Instead of a functional CLT, this paper establishes strong uniform\nconsistency of the estimated sum distribution function and provides a\nsufficient criterion for the convergence rate $O(n^{-1/2})$ in probability.\nThese convergence results hold for all copulas with bounded densities. Examples\nwith unbounded densities include bivariate Clayton and Gauss copulas. The\nconvergence results are not specific to the component sum and hold also for any\nother componentwise non-decreasing aggregation function. On the other hand,\nconvergence of estimates for the joint distribution is much easier to prove,\nincluding CLTs.\n  Beyond Iman--Conover estimates, the results of this paper apply to\nmultivariate distributions obtained by plugging empirical margins into an exact\ncopula or by plugging exact margins into an empirical copula.\n"
    },
    {
        "paper_id": 1508.02824,
        "authors": "Paul Larsen",
        "title": "Asyptotic Normality for Maximum Likelihood Estimation and Operational\n  Risk",
        "comments": "Split previous arXiv submission into two parts for journal\n  submission. A slightly modified version of this paper will appear in the\n  Journal of Operational Risk. The second part on stability of OpVar will be\n  posted separately",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Operational risk models commonly employ maximum likelihood estimation (MLE)\nto fit loss data to heavy-tailed distributions. Yet several desirable\nproperties of MLE (e.g. asymptotic normality) are generally valid only for\nlarge sample-sizes, a situation rarely encountered in operational risk. In this\npaper, we study how asymptotic normality does--or does not--hold for common\nseverity distributions in operational risk models. We then apply these results\nto evaluate errors caused by failure of asymptotic normality in constructing\nconfidence intervals around the MLE fitted parameters.\n"
    },
    {
        "paper_id": 1508.02919,
        "authors": "Gaurab Aryal, Isabelle Perrigne and Quang Vuong",
        "title": "Identification of Insurance Models with Multidimensional Screening",
        "comments": "55 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the identification of insurance models with\nmultidimensional screening where insurees have private information about their\nrisk and risk aversion. The model includes a random damage and the possibility\nof several claims. Screening of insurees relies on their certainty equivalence.\nThe paper then investigates how data availability on the number of offered\ncoverages and reported claims affects the identification of the model\nprimitives under four different scenarios. We show that the model structure is\nidentified despite bunching due to multidimensional screening and/or a finite\nnumber of offered coverages. The observed number of claims plays a key role in\nthe identification of the joint distribution of risk and risk aversion. In\naddition, the paper derives all the restrictions imposed by the model on\nobservables. Our results are constructive with explicit equations for\nestimation and model testing.\n"
    },
    {
        "paper_id": 1508.03282,
        "authors": "Claudio Fontana",
        "title": "The strong predictable representation property in initially enlarged\n  filtrations under the density hypothesis",
        "comments": "30 pages, revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the strong predictable representation property in filtrations\ninitially enlarged with a random variable L. We prove that the strong\npredictable representation property can always be transferred to the enlarged\nfiltration as long as the classical density hypothesis of Jacod (1985) holds.\nThis generalizes the existing martingale representation results and does not\nrely on the equivalence between the conditional and the unconditional laws of\nL. Depending on the behavior of the density process at zero, different forms of\nmartingale representation are established. The results are illustrated in the\ncontext of hedging contingent claims under insider information.\n"
    },
    {
        "paper_id": 1508.03373,
        "authors": "Vaibhav Srivastava and Samuel F. Feng and Jonathan D. Cohen and Naomi\n  Ehrich Leonard and Amitai Shenhav",
        "title": "A martingale analysis of first passage times of time-dependent Wiener\n  diffusion models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research in psychology and neuroscience has successfully modeled decision\nmaking as a process of noisy evidence accumulation to a decision bound. While\nthere are several variants and implementations of this idea, the majority of\nthese models make use of a noisy accumulation between two absorbing boundaries.\nA common assumption of these models is that decision parameters, e.g., the rate\nof accumulation (drift rate), remain fixed over the course of a decision,\nallowing the derivation of analytic formulas for the probabilities of hitting\nthe upper or lower decision threshold, and the mean decision time. There is\nreason to believe, however, that many types of behavior would be better\ndescribed by a model in which the parameters were allowed to vary over the\ncourse of the decision process.\n  In this paper, we use martingale theory to derive formulas for the mean\ndecision time, hitting probabilities, and first passage time (FPT) densities of\na Wiener process with time-varying drift between two time-varying absorbing\nboundaries. This model was first studied by Ratcliff (1980) in the two-stage\nform, and here we consider the same model for an arbitrary number of stages\n(i.e. intervals of time during which parameters are constant). Our calculations\nenable direct computation of mean decision times and hitting probabilities for\nthe associated multistage process. We also provide a review of how martingale\ntheory may be used to analyze similar models employing Wiener processes by\nre-deriving some classical results. In concert with a variety of numerical\ntools already available, the current derivations should encourage mathematical\nanalysis of more complex models of decision making with time-varying evidence.\n"
    },
    {
        "paper_id": 1508.03533,
        "authors": "Fabio Saracco, Riccardo Di Clemente, Andrea Gabrielli, Tiziano\n  Squartini",
        "title": "Detecting early signs of the 2007-2008 crisis in the world trade",
        "comments": "18 pages, 9 figures",
        "journal-ref": "Sci. Rep. 6 (30286) (2016)",
        "doi": "10.1038/srep30286",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since 2007, several contributions have tried to identify early-warning\nsignals of the financial crisis. However, the vast majority of analyses has\nfocused on financial systems and little theoretical work has been done on the\neconomic counterpart. In the present paper we fill this gap and employ the\ntheoretical tools of network theory to shed light on the response of world\ntrade to the financial crisis of 2007 and the economic recession of 2008-2009.\nWe have explored the evolution of the bipartite World Trade Web (WTW) across\nthe years 1995-2010, monitoring the behavior of the system both before and\nafter 2007. Our analysis shows early structural changes in the WTW topology:\nsince 2003, the WTW becomes increasingly compatible with the picture of a\nnetwork where correlations between countries and products are progressively\nlost. Moreover, the WTW structural modification can be considered as concluded\nin 2010, after a seemingly stationary phase of three years. We have also\nrefined our analysis by considering specific subsets of countries and products:\nthe most statistically significant early-warning signals are provided by the\nmost volatile macrosectors, especially when measured on developing countries,\nsuggesting the emerging economies as being the most sensitive ones to the\nglobal economic cycles.\n"
    },
    {
        "paper_id": 1508.03571,
        "authors": "Fabio Saracco, Riccardo Di Clemente, Andrea Gabrielli and Luciano\n  Pietronero",
        "title": "From innovation to diversification: a simple competitive model",
        "comments": "8 figures, 8 tables",
        "journal-ref": "PloS One 10(11): e0140420 (2015)",
        "doi": "10.1371/journal.pone.0140420",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Few attempts have been proposed in order to describe the statistical features\nand historical evolution of the export bipartite matrix countries/products. An\nimportant standpoint is the introduction of a products network, namely a\nhierarchical forest of products that models the formation and the evolution of\ncommodities. In the present article, we propose a simple dynamical model where\ncountries compete with each other to acquire the ability to produce and export\nnew products. Countries will have two possibilities to expand their export:\ninnovating, i.e. introducing new goods, namely new nodes in the product\nnetworks, or copying the productive process of others, i.e. occupying a node\nalready present in the same network. In this way, the topology of the products\nnetwork and the country-product matrix evolve simultaneously, driven by the\ncountries push toward innovation.\n"
    },
    {
        "paper_id": 1508.03651,
        "authors": "Endre Cs\\'oka",
        "title": "A conjecture about the efficiency of first price mechanisms",
        "comments": "The conjecture is disproved in its original form by the author. The\n  current status of the question is a part of the paper \"Efficient Teamwork\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present different versions of a conjecture which would express that first\nprice mechanisms never work very badly in a very general class of problems. The\ndefinitions include most of the problems where there is a principal (seller)\nwho has the right to exclude others from the game. The exact definitions are\nmotivated by the \"first price mechanism\" in E Cs: \"Efficient Teamwork\", but the\nconjecture is relevant for most auction problems, e.g. for combinatorial\nauctions.\n"
    },
    {
        "paper_id": 1508.03677,
        "authors": "Bin Li (1), K. Y. Michael Wong (1), Amos H. M. Chan (1), Tsz Yan So\n  (1), Hermanni Heimonen (1), Junyi Wei (1), David Saad (2) ((1) Department of\n  Physics, The Hong Kong University of Science and Technology, Clear Water Bay,\n  Hong Kong (2) The Nonlinearity and Complexity Research Group, Aston\n  University, Birmingham B4 7ET, United Kingdom)",
        "title": "How Market Structure Drives Commodity Prices",
        "comments": "17 pages, 5 figures, 1 table, 15 pages of Supporting Information",
        "journal-ref": "J. Stat. Mech. (2017) 113405",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an agent-based model, in which agents set their prices to\nmaximize profit. At steady state the market self-organizes into three groups:\nexcess producers, consumers and balanced agents, with prices determined by\ntheir own resource level and a couple of macroscopic parameters that emerge\nnaturally from the analysis, akin to mean-field parameters in statistical\nmechanics. When resources are scarce prices rise sharply below a turning point\nthat marks the disappearance of excess producers. To compare the model with\nreal empirical data, we study the relations between commodity prices and\nstock-to-use ratios of a range of commodities such as agricultural products and\nmetals. By introducing an elasticity parameter to mitigate noise and long-term\nchanges in commodities data, we confirm the trend of rising prices, provide\nevidence for turning points, and indicate yield points for less essential\ncommodities.\n"
    },
    {
        "paper_id": 1508.03841,
        "authors": "Juan Ospina",
        "title": "New Analytical Solutions of a Modified Black-Scholes Equation with the\n  European Put Option",
        "comments": "16 pages,4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Maple, we compute some analytical solutions of a modified Black-Scholes\nequation, recently proposed, in the case of the European put option. We show\nthat the modified Black-Scholes equation with the European put option is\nexactly solvable in terms of associated Laguerre polynomials. We make some\nnumerical experiments with the analytical solutions and we compare our results\nwith the results derived from numerical experiments using the standard\nBlack-Scholes equation.\n"
    },
    {
        "paper_id": 1508.03853,
        "authors": "Alex Augusto Timm Rathke",
        "title": "Transfer pricing manipulation, tax penalty cost and the impact of\n  foreign profit taxation",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analizes the optimal level of transfer pricing manipulation when\nthe expected tax penalty is a function of the tax enforcement and the market\nprice parameter. The arm's length principle implies the existence of a range of\nacceptable prices shaped by market, and firms can manipulate transfer prices\nmore freely if market price range is wide, or if its delimitations are\ndifficult to determine. Home taxation of foreign profits can reduce income\nshifting incentive, depending on the portion of repatriation for tax purposes.\nWe find that the limited tax credit rule tends to be a less efficient measure,\nnonetheless it is the most widely adopted rule by countries, so to spark the\nperspective of more powerful approaches for taxation of foreign profits.\n"
    },
    {
        "paper_id": 1508.03924,
        "authors": "Demian Pouzo and Ignacio Presno",
        "title": "Optimal Taxation with Endogenous Default under Incomplete Markets",
        "comments": "55 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a dynamic economy, we characterize the fiscal policy of the government\nwhen it levies distortionary taxes and issues defaultable bonds to finance its\nstochastic expenditure. Default may occur in equilibrium as it prevents the\ngovernment from incurring in future tax distortions that would come along with\nthe service of the debt. Households anticipate the possibility of default\ngenerating endogenous credit limits. These limits hinder the government's\nability to smooth taxes using debt, implying more volatile and less serially\ncorrelated fiscal policies, higher borrowing costs and lower levels of\nindebtedness. In order to exit temporary financial autarky following a default\nevent, the government has to repay a random fraction of the defaulted debt. We\nshow that the optimal fiscal and renegotiation policies have implications\naligned with the data.\n"
    },
    {
        "paper_id": 1508.04246,
        "authors": "J\\\"org D. Becker (Institut f\\\"ur Cybernetische Anthropologie\n  Starnberg)",
        "title": "Why is GDP growth linear?",
        "comments": "4 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many European countries the growth of the real GDP per capita has been\nlinear since 1950. An explanation for this linearity is still missing. We\npropose that in artificial intelligence we may find models for a linear growth\nof performance. We also discuss possible consequences of the fact that in\nsystems with linear growth the percentage growth goes to zero.\n"
    },
    {
        "paper_id": 1508.04321,
        "authors": "Nicola Moreni and Andrea Pallavicini",
        "title": "FX Modelling in Collateralized Markets: foreign measures, basis curves,\n  and pricing formulae",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general derivation of the arbitrage-free pricing framework for\nmultiple-currency collateralized products. We include the impact on option\npricing of the policy adopted to fund in foreign currency, so that we are able\nto price contracts with cash flows and/or collateral accounts expressed in\nforeign currencies inclusive of funding costs originating from dislocations in\nthe FX market. Then, we apply these results to price cross-currency swaps under\ndifferent market situations, to understand how to implement a feasible curve\nbootstrap procedure. We present the main practical problems arising from the\nway the market is quoting liquid instruments: uncertainties about collateral\ncurrencies and renotioning features. We discuss the theoretical requirements to\nimplement curve bootstrapping and the approximations usually taken to\npractically implement the procedure. We also provide numerical examples based\non real market data.\n"
    },
    {
        "paper_id": 1508.04332,
        "authors": "Dimitri Kroujiline, Maxim Gusev, Dmitry Ushanov, Sergey V. Sharov and\n  Boris Govorkov",
        "title": "Forecasting stock market returns over multiple time horizons",
        "comments": "This is the version accepted for publication in a journal\n  Quantitative Finance. A draft was posted here on 18 August 2015. 50 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we seek to demonstrate the predictability of stock market\nreturns and explain the nature of this return predictability. To this end, we\nintroduce investors with different investment horizons into the news-driven,\nanalytic, agent-based market model developed in Gusev et al. (2015). This\nheterogeneous framework enables us to capture dynamics at multiple timescales,\nexpanding the model's applications and improving precision. We study the\nheterogeneous model theoretically and empirically to highlight essential\nmechanisms underlying certain market behaviors, such as transitions between\nbull- and bear markets and the self-similar behavior of price changes. Most\nimportantly, we apply this model to show that the stock market is nearly\nefficient on intraday timescales, adjusting quickly to incoming news, but\nbecomes inefficient on longer timescales, where news may have a long-lasting\nnonlinear impact on dynamics, attributable to a feedback mechanism acting over\nthese horizons. Then, using the model, we design algorithmic strategies that\nutilize news flow, quantified and measured, as the only input to trade on\nmarket return forecasts over multiple horizons, from days to months. The\nbacktested results suggest that the return is predictable to the extent that\nsuccessful trading strategies can be constructed to harness this\npredictability.\n"
    },
    {
        "paper_id": 1508.04348,
        "authors": "Efstathios Panayi, Gareth W. Peters, Jon Danielsson, Jean-Pierre\n  Zigrand",
        "title": "Designating market maker behaviour in Limit Order Book markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial exchanges provide incentives for limit order book (LOB) liquidity\nprovision to certain market participants, termed designated market makers or\ndesignated sponsors. While quoting requirements typically enforce the activity\nof these participants for a certain portion of the day, we argue that liquidity\ndemand throughout the trading day is far from uniformly distributed, and thus\nthis liquidity provision may not be calibrated to the demand. We propose that\nquoting obligations also include requirements about the speed of liquidity\nreplenishment, and we recommend use of the Threshold Exceedance Duration (TED)\nfor this purpose. We present a comprehensive regression modelling approach\nusing GLM and GAMLSS models to relate the TED to the state of the LOB and\nidentify the regression structures that are best suited to modelling the TED.\nSuch an approach can be used by exchanges to set target levels of liquidity\nreplenishment for designated market makers.\n"
    },
    {
        "paper_id": 1508.04351,
        "authors": "Antoine Jacquier and Martin Keller-Ressel",
        "title": "Implied volatility in strict local martingale models",
        "comments": "21 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider implied volatilities in asset pricing models, where the\ndiscounted underlying is a strict local martingale under the pricing measure.\nOur main result gives an asymptotic expansion of the right wing of the implied\nvolatility smile and shows that the strict local martingale property can be\ndetermined from this expansion. This result complements the well-known\nasymptotic results of Lee and Benaim-Friz, which apply only to true\nmartingales. This also shows that `price bubbles' in the sense of strict local\nmartingale behaviour can in principle be detected by an analysis of implied\nvolatility. Finally we relate our results to left-wing expansions of implied\nvolatilities in models with mass at zero by a duality method based on an\nabsolutely continuous measure change.\n"
    },
    {
        "paper_id": 1508.04392,
        "authors": "Zhen Zhu, Greg Morrison, Michelangelo Puliga, Alessandro Chessa,\n  Massimo Riccaboni",
        "title": "The Similarity of Global Value Chains: A Network-Based Measure",
        "comments": "30 pages, 14 figures",
        "journal-ref": "Net Sci 6 (2018) 607-632",
        "doi": "10.1017/nws.2018.8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  International trade has been increasingly organized in the form of global\nvalue chains (GVCs) where different stages of production are located in\ndifferent countries. This recent phenomenon has substantial consequences for\nboth trade policy design at the national or regional level and business\ndecision making at the firm level. In this paper, we provide a new method for\ncomparing GVCs across countries and over time. First, we use the World\nInput-Output Database (WIOD) to construct both the upstream and downstream\nglobal value networks, where the nodes are individual sectors in different\ncountries and the links are the value-added contribution relationships. Second,\nwe introduce a network-based measure of node similarity to compare the GVCs\nbetween any pair of countries for each sector and each year available in the\nWIOD. Our network-based similarity is a better measure for node comparison than\nthe existing ones because it takes into account all the direct and indirect\nrelationships between country-sector pairs, is applicable to both directed and\nweighted networks with self-loops, and takes into account externally defined\nnode attributes. As a result, our measure of similarity reveals the most\nintensive interactions among the GVCs across countries and over time. From 1995\nto 2011, the average similarity between sectors and countries have clear\nincreasing trends, which are temporarily interrupted by the recent economic\ncrisis. This measure of the similarity of GVCs provides quantitative answers to\nimportant questions about dependency, sustainability, risk, and competition in\nthe global production system.\n"
    },
    {
        "paper_id": 1508.04487,
        "authors": "Jordan Mann and J. Nathan Kutz",
        "title": "Dynamic Mode Decomposition for Financial Trading Strategies",
        "comments": "18 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1506.00564",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate the application of an algorithmic trading strategy based upon\nthe recently developed dynamic mode decomposition (DMD) on portfolios of\nfinancial data. The method is capable of characterizing complex dynamical\nsystems, in this case financial market dynamics, in an equation-free manner by\ndecomposing the state of the system into low-rank terms whose temporal\ncoefficients in time are known. By extracting key temporal coherent structures\n(portfolios) in its sampling window, it provides a regression to a best fit\nlinear dynamical system, allowing for a predictive assessment of the market\ndynamics and informing an investment strategy. The data-driven analytics\ncapitalizes on stock market patterns, either real or perceived, to inform\nbuy/sell/hold investment decisions. Critical to the method is an associated\nlearning algorithm that optimizes the sampling and prediction windows of the\nalgorithm by discovering trading hot-spots. The underlying mathematical\nstructure of the algorithms is rooted in methods from nonlinear dynamical\nsystems and shows that the decomposition is an effective mathematical tool for\ndata-driven discovery of market patterns.\n"
    },
    {
        "paper_id": 1508.04512,
        "authors": "Aurelio F. Bariviera, M.T. Martin, A. Plastino, V. Vampa",
        "title": "LIBOR troubles: anomalous movements detection based on Maximum Entropy",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.01.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  According to the definition of the London Interbank Offered Rate (LIBOR),\ncontributing banks should give fair estimates of their own borrowing costs in\nthe interbank market. Between 2007 and 2009, several banks made inappropriate\nsubmissions of LIBOR, sometimes motivated by profit-seeking from their trading\npositions. In 2012, several newspapers' articles began to cast doubt on LIBOR\nintegrity, leading surveillance authorities to conduct investigations on banks'\nbehavior. Such procedures resulted in severe fines imposed to involved banks,\nwho recognized their financial inappropriate conduct. In this paper, we uncover\nsuch unfair behavior by using a forecasting method based on the Maximum Entropy\nprinciple. Our results are robust against changes in parameter settings and\ncould be of great help for market surveillance.\n"
    },
    {
        "paper_id": 1508.04748,
        "authors": "Aurelio F. Bariviera, M. Bel\\'en Guercio, Lisana B. Martinez, Osvaldo\n  A. Rosso",
        "title": "The (in)visible hand in the Libor market: an Information Theory approach",
        "comments": "PACS 89.65.Gh Econophysics; 74.40.De noise and chaos",
        "journal-ref": "The European Physical Journal B (2015) 88(8):208",
        "doi": "10.1140/epjb/e2015-60410-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes several interest rates time series from the United\nKingdom during the period 1999 to 2014. The analysis is carried out using a\npioneering statistical tool in the financial literature: the complexity-entropy\ncausality plane. This representation is able to classify different stochastic\nand chaotic regimes in time series. We use sliding temporal windows to assess\nchanges in the intrinsic stochastic dynamics of the time series. Anomalous\nbehavior in the Libor is detected, especially around the time of the last\nfinancial crisis, that could be consistent with data manipulation.\n"
    },
    {
        "paper_id": 1508.04754,
        "authors": "Sandro Claudio Lera and Didier Sornette",
        "title": "Currency target zone modeling: An interplay between physics and\n  economics",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.92.062828",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the performance of the euro/Swiss franc exchange rate in the\nextraordinary period from September 6, 2011 and January 15, 2015 when the Swiss\nNational Bank enforced a minimum exchange rate of 1.20 Swiss francs per euro.\nBased on the analogy between Brownian motion in finance and physics, the\nfirst-order effect of such a steric constraint would enter a priori in the form\nof a repulsive entropic force associated with the paths crossing the barrier\nthat are forbidden. Non-parametric empirical estimates of drift and volatility\nshow that the predicted first-order analogy between economics and physics are\nincorrect. The clue is to realise that the random walk nature of financial\nprices results from the continuous anticipations of traders about future\nopportunities, whose aggregate actions translate into an approximate efficient\nmarket with almost no arbitrage opportunities. With the Swiss National Bank\nstated commitment to enforce the barrier, traders's anticipation of this action\nleads to a vanishing drift together with a volatility of the exchange rate that\ndepends on the distance to the barrier. We give direct quantitative empirical\nevidence that this effect is well described by Krugman's target zone model\n[P.R. Krugman. The Quarterly Journal of Economics, 106(3):669-682, 1991].\nMotivated by the insights from this economical model, we revise the initial\neconomics-physics analogy and show that, within the context of hindered\ndiffusion, the two systems can be described with the same mathematics after\nall. Using a recently proposed extended analogy in terms of a colloidal\nBrownian particle embedded in a fluid of molecules associated with the\nunderlying order book, we derive that, close to the restricting boundary, the\ndynamics of both systems is described by a stochastic differential equation\nwith a very small constant drift and a linear diffusion coefficient.\n"
    },
    {
        "paper_id": 1508.04883,
        "authors": "Zura Kakushadze",
        "title": "Heterotic Risk Models",
        "comments": "41 pages; a trivial typo corrected",
        "journal-ref": "Wilmott Magazine 2015(80) (2015) 40-55",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a complete algorithm and source code for constructing what we refer\nto as heterotic risk models (for equities), which combine: i) granularity of an\nindustry classification; ii) diagonality of the principal component factor\ncovariance matrix for any sub-cluster of stocks; and iii) dramatic reduction of\nthe factor covariance matrix size in the Russian-doll risk model construction.\nThis appears to prove a powerful approach for constructing out-of-sample stable\nshort-lookback risk models. Thus, for intraday mean-reversion alphas based on\novernight returns, Sharpe ratio optimization using our heterotic risk models\nsizably improves the performance characteristics compared to weighted\nregressions based on principal components or industry classification. We also\ngive source code for: a) building statistical risk models; and ii) Sharpe ratio\noptimization with homogeneous linear constraints and position bounds.\n"
    },
    {
        "paper_id": 1508.049,
        "authors": "Dieter Hendricks, Tim Gebbie, Diane Wilcox",
        "title": "Detecting intraday financial market states using temporal clustering",
        "comments": "30 pages, 16 figures, 8 tables, published in Quantitative Finance",
        "journal-ref": "Quantitative Finance, (2016),16:11, 1657-1678",
        "doi": "10.1080/14697688.2016.1171378",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the application of a high-speed maximum likelihood clustering\nalgorithm to detect temporal financial market states, using correlation\nmatrices estimated from intraday market microstructure features. We first\ndetermine the ex-ante intraday temporal cluster configurations to identify\nmarket states, and then study the identified temporal state features to extract\nstate signature vectors which enable online state detection. The state\nsignature vectors serve as low-dimensional state descriptors which can be used\nin learning algorithms for optimal planning in the high-frequency trading\ndomain. We present a feasible scheme for real-time intraday state detection\nfrom streaming market data feeds. This study identifies an interesting\nhierarchy of system behaviour which motivates the need for time-scale-specific\nstate space reduction for participating agents.\n"
    },
    {
        "paper_id": 1508.05114,
        "authors": "Alfred Galichon, Scott Kominers, Simon Weber",
        "title": "The nonlinear Bernstein-Schr\u007f\\\"odinger equation in Economics",
        "comments": "8 pages, submitted to Lecture Notes in Computer Science",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we relate the Equilibrium Assignment Problem (EAP), which is\nunderlying in several economics models, to a system of nonlinear equations that\nwe call the \"nonlinear Bernstein-Schr\u007f\\\"odinger system\", which is well-known in\nthe linear case, but whose nonlinear extension does not seem to have been\nstudied. We apply this connection to derive an existence result for the EAP,\nand an efficient computational method.\n"
    },
    {
        "paper_id": 1508.05233,
        "authors": "Yan Dolinsky and Ariel Neufeld",
        "title": "Super-replication in Fully Incomplete Markets",
        "comments": "Former titles: Super-replication in Extremely Incomplete Markets;\n  Super-replication of Game Options in Stochastic Volatility Models",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we introduce the notion of fully incomplete markets. We prove\nthat for these markets the super-replication price coincide with the model free\nsuper-replication price. Namely, the knowledge of the model does not reduce the\nsuper-replication price. We provide two families of fully incomplete models:\nstochastic volatility models and rough volatility models. Moreover, we give\nseveral computational examples. Our approach is purely probabilistic.\n"
    },
    {
        "paper_id": 1508.05241,
        "authors": "Jan Hendrik Witte",
        "title": "Volatility Harvesting: Extracting Return from Randomness",
        "comments": "10 Pages, 6 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Studying Binomial and Gaussian return dynamics in discrete time, we show how\nexcess volatility can be traded to create growth. We test our results on real\nworld data to confirm the observed model phenomena while also highlighting\nimplicit risks.\n"
    },
    {
        "paper_id": 1508.05353,
        "authors": "Gaurab Aryal and Maria F. Gabrielli",
        "title": "Is Collusion-Proof Procurement Expensive?",
        "comments": "I was a co-author in this earlier version, but I left the paper as a\n  co-author. This paper is now a different paper developed by Dr. Maria F.\n  Gabrielli",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Collusion among bidders adversely affects procurement cost and in some cases\nefficiency, and it seems collusion is more prevalent that we would like.\nStatistical methods of detecting collusion just using bid data, in a hope to\ndeter future collusion, is perilous, and access to additional data is rare and\noften always after the fact. In this paper, we estimate the extra cost of\nimplementing a new procurement rule proposed by Chen and Micali [2012] that is\nrobust to collusion and always guarantees the efficient outcome. The rule\nrequires bidders to report their coalition and to ensure\nincentive-compatibility, the mechanism allows them to attain rents. We estimate\nthis rent using data from California highway construction and find it to be\nanywhere between 1.6% to 5%. Even after we factor in the marginal excess burden\nof taxes needed to finance these rents, the cost ranges between 2.08% and 6.5%,\nsuggesting that there is a room to think about running this new auction,\nsuggesting we should consider this auction.\n"
    },
    {
        "paper_id": 1508.05355,
        "authors": "Benjamin Munro, Julia McLachlan",
        "title": "Autonomics: an autonomous and intelligent economic platform and next\n  generation money tool",
        "comments": "55 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a high level network architecture for an economic system that\nintegrates money, governance and reputation. We introduce a method for issuing,\nand redeeming a digital coin using a mechanism to create a sustainable global\neconomy and a free market. To maintain a currency's value over time, and\ntherefore be money proper, we claim it must be issued by the buyer and backed\nfor value by the seller, exchanging the products of labour, in a free market.\nWe also claim that a free market and sustainable economy cannot be maintained\nusing economically arbitrary creation and allocation of money. Nakamoto, with\nBitcoin, introduced a new technology called the cryptographic blockchain to\noperate a decentralised and distributed accounts ledger without the need for an\nuntrusted third party. This blockchain technology creates and allocates new\ndigital currency as a reward for \"proof-of-work\", to secure the network.\nHowever, no currency, digital or otherwise, has solved how to create and\nallocate money in an economically non-arbitrary way, or how to govern and trust\na world-scale free enterprise money system. We propose an \"Ontologically\nNetworked Exchange\" (ONE), with purpose as its highest order domain. Each\npurpose is defined in a contract, and the entire economy of contracts is\nstructured in a unified ontology. We claim to secure the ONE network using\neconomically non-arbitrary methodologies and economically incented human\nbehaviour. Decisions influenced by reputation help to secure the network\nwithout an untrusted third party. The stack of contracts, organised in a\nunified ontology, functions as a super recursive algorithm, with individual use\nprogramming the algorithm, acting as the \"oracle\". The state of the algorithm\nbecomes the \"memory\" of a scalable and trustable artificial intelligence (AI).\nThis AI offers a new platform for what we call the \"Autonomy-of-Things\" (AoT).\n"
    },
    {
        "paper_id": 1508.05357,
        "authors": "Paul Ormerod, Rickard Nyman, David Tuckett",
        "title": "Measuring Financial Sentiment to Predict Financial Instability: A New\n  Approach based on Text Analysis",
        "comments": "14 pages plus Supplementary Material",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the financial crisis of the late 2000s, policy makers have shown\nconsiderable interest in monitoring financial stability. Several central banks\nnow publish indices of financial stress, which are essentially based upon\nmarket related data. In this paper, we examine the potential for improving the\nindices by deriving information about emotion shifts in the economy. We report\non a new approach, based on the content analysis of very large text databases,\nand termed directed algorithmic text analysis. The algorithm identifies, very\nrapidly, shifts through time in the relations between two core emotional\ngroups. The method is robust. The same word-list is used to identify the two\nemotion groups across different studies. Membership of the words in the lists\nhas been validated in psychological experiments. The words consist of everyday\nEnglish words with no specific economic meaning. Initial results show promise.\nAn emotion index capturing shifts between the two emotion groups in texts\npotentially referring to the whole US economy improves the one-quarter ahead\nconsensus forecasts for real GDP growth. More specifically, the same indices\nare shown to Granger cause both the Cleveland and St Louis Federal Reserve\nIndices of Financial Stress.\n"
    },
    {
        "paper_id": 1508.0546,
        "authors": "Marcin Pitera and {\\L}ukasz Stettner",
        "title": "Long run risk sensitive portfolio with general factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper portfolio optimization over long run risk sensitive criterion is\nconsidered. It is assumed that economic factors which stimulate asset prices\nare ergodic but non necessarily uniformly ergodic. Solution to suitable Bellman\nequation using local span contraction with weighted norms is shown. The form of\noptimal strategy is presented and examples of market models satisfying imposed\nassumptions are shown.\n"
    },
    {
        "paper_id": 1508.05751,
        "authors": "Daniel Martin Katz, Michael J Bommarito II, Tyler Soellinger, James\n  Ming Chen",
        "title": "Law on the Market? Abnormal Stock Returns and Supreme Court\n  Decision-Making",
        "comments": "25 pages, 6 figures; first presented in brief at the 14th Annual\n  Finance, Risk and Accounting Conference, Oriel College - Oxford University\n  (2014)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What happens when the Supreme Court of the United States decides a case\nimpacting one or more publicly-traded firms? While many have observed anecdotal\nevidence linking decisions or oral arguments to abnormal stock returns, few\nhave rigorously or systematically investigated the behavior of equities around\nSupreme Court actions. In this research, we present the first comprehensive,\nlongitudinal study on the topic, spanning over 15 years and hundreds of cases\nand firms. Using both intra- and interday data around decisions and oral\narguments, we evaluate the frequency and magnitude of statistically-significant\nabnormal return events after Supreme Court action. On a per-term basis, we find\n5.3 cases and 7.8 stocks that exhibit abnormal returns after decision. In\ntotal, across the cases we examined, we find 79 out of the 211 cases (37%)\nexhibit an average abnormal return of 4.4% over a two-session window with an\naverage $|t|$-statistic of 2.9. Finally, we observe that abnormal returns\nfollowing Supreme Court decisions materialize over the span of hours and days,\nnot minutes, yielding strong implications for market efficiency in this\ncontext. While we cannot causally separate substantive legal impact from mere\nrevision of beliefs, we do find strong evidence that there is indeed a \"law on\nthe market\" effect as measured by the frequency of abnormal return events, and\nthat these abnormal returns are not immediately incorporated into prices.\n"
    },
    {
        "paper_id": 1508.05837,
        "authors": "Simone Farinelli and Luisa Tibiletti",
        "title": "Hydroassets Portfolio Management for Intraday Electricity Trading from a\n  Discrete Time Stochastic Optimization Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hydro storage system optimization is becoming one of the most challenging\ntasks in Energy Finance. While currently the state-of-the-art of the commercial\nsoftware in the industry implements mainly linear models, we would like to\nintroduce risk aversion and a generic utility function. At the same time, we\naim to develop and implement a computational efficient algorithm, which is not\naffected by the curse of dimensionality and does not utilize subjective\nheuristics to prevent it. For the short term power market we propose a\nsimultaneous solution for both dispatch and bidding problems.\n  Following the Blomvall and Lindberg (2002) interior point model, we set up a\nstochastic multiperiod optimization procedure by means of a \"bushy\" recombining\ntree that provides fast computational results. Inequality constraints are\npacked into the objective function by the logarithmic barrier approach and the\nutility function is approximated by its second order Taylor polynomial. The\noptimal solution for the original problem is obtained as a diagonal sequence\nwhere the first diagonal dimension is the parameter controlling the logarithmic\npenalty and the second is the parameter for the Newton step in the construction\nof the approximated solution. Optimal intraday electricity trading and water\nvalues for hydro assets as shadow prices are computed. The algorithm is\nimplemented in Mathematica.\n"
    },
    {
        "paper_id": 1508.05948,
        "authors": "Daniela Bubboloni, Michele Gori",
        "title": "On the reversal bias of the Minimax social choice correspondence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce three different qualifications of the reversal bias in the\nframework of social choice correspondences. For each of them, we prove that the\nMinimax social choice correspondence is immune to it if and only if the number\nof voters and the number of alternatives satisfy suitable arithmetical\nconditions. We prove those facts thanks to a new characterization of the\nMinimax social choice correspondence and using a graph theory approach. We\ndiscuss the same issue for the Borda and Copeland social choice\ncorrespondences.\n"
    },
    {
        "paper_id": 1508.06024,
        "authors": "Yoshihiro Yura and Hideki Takayasu and Didier Sornette and Misako\n  Takayasu",
        "title": "Financial Knudsen number: breakdown of continuous price dynamics and\n  asymmetric buy and sell structures confirmed by high precision order book\n  information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalise the description of the dynamics of the order book of financial\nmarkets in terms of a Brownian particle embedded in a fluid of incoming,\nexiting and annihilating particles by presenting a model of the velocity on\neach side (buy and sell) independently. The improved model builds on the\ntime-averaged number of particles in the inner layer and its change per unit\ntime, where the inner layer is revealed by the correlations between price\nvelocity and change in the number of particles (limit orders). This allows us\nto introduce the Knudsen number of the financial Brownian particle motion and\nits asymmetric version (on the buy and sell sides). Not being considered\npreviously, the asymmetric Knudsen numbers are crucial in finance in order to\ndetect asymmetric price changes. The Knudsen numbers allows us to characterise\nthe conditions for the market dynamics to be correctly described by a\ncontinuous stochastic process. Not questioned until now for large liquid\nmarkets such as the USD/JPY and EUR/USD exchange rates, we show that there are\nregimes when the Knudsen numbers are so high that discrete particle effects\ndominate, such as during market stresses and crashes. We document the presence\nof imbalances of particles depletion rates on the buy and sell sides that are\nassociated with high Knudsen numbers and violent directional price changes.\nThis indicator can detect the direction of the price motion at the early stage\nwhile the usual volatility risk measure is blind to the price direction.\n"
    },
    {
        "paper_id": 1508.06032,
        "authors": "Zhou Zhou",
        "title": "Non-zero-sum stopping games in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider two-player non-zero-sum stopping games in discrete time. Unlike\nDynkin games, in our games the payoff of each player is revealed after both\nplayers stop. Moreover, each player can adjust her own stopping strategy\naccording to the other player's action. In the first part of the paper, we\nconsider the game where players act simultaneously at each stage. We show that\nthere exists a Nash equilibrium in mixed stopping strategies. In the second\npart, we assume that one player has to act first at each stage. In this case,\nwe show the existence of a Nash equilibrium in pure stopping strategies.\n"
    },
    {
        "paper_id": 1508.06117,
        "authors": "L. C. G. Rogers",
        "title": "Bermudan options by simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this study is to devise numerical methods for dealing with very\nhigh-dimensional Bermudan-style derivatives. For such problems, we quickly see\nthat we can at best hope for price bounds, and we can only use a simulation\napproach. We use the approach of Barraquand & Martineau which proposes that the\nreward process should be treated as if it were Markovian, and then uses this to\ngenerate a stopping rule and hence a lower bound on the price. Using the dual\napproach introduced by Rogers, and Haugh & Kogan, this approximate Markov\nprocess leads us to hedging strategies, and upper bounds on the price. The\nmethodology is generic, and is illustrated on eight examples of varying levels\nof difficulty. Run times are largely insensitive to dimension.\n"
    },
    {
        "paper_id": 1508.06182,
        "authors": "Gili Rosenberg, Poya Haghnegahdar, Phil Goddard, Peter Carr, Kesheng\n  Wu and Marcos L\\'opez de Prado",
        "title": "Solving the Optimal Trading Trajectory Problem Using a Quantum Annealer",
        "comments": "7 pages; expanded and updated",
        "journal-ref": "IEEE Journal of Selected Topics in Signal Processing (JSTSP),\n  Volume 10, Issue 6, 2016, and Proc. of the 8th Workshop on High Performance\n  Computational Finance (WHPCF), p. 7, ACM, 2015",
        "doi": "10.1109/JSTSP.2016.2574703",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve a multi-period portfolio optimization problem using D-Wave Systems'\nquantum annealer. We derive a formulation of the problem, discuss several\npossible integer encoding schemes, and present numerical examples that show\nhigh success rates. The formulation incorporates transaction costs (including\npermanent and temporary market impact), and, significantly, the solution does\nnot require the inversion of a covariance matrix. The discrete multi-period\nportfolio optimization problem we solve is significantly harder than the\ncontinuous variable problem. We present insight into how results may be\nimproved using suitable software enhancements, and why current quantum\nannealing technology limits the size of problem that can be successfully solved\ntoday. The formulation presented is specifically designed to be scalable, with\nthe expectation that as quantum annealing technology improves, larger problems\nwill be solvable using the same techniques.\n"
    },
    {
        "paper_id": 1508.06225,
        "authors": "S.I. Melnyk, I.G. Tuluzov",
        "title": "Theory of pricing as relativistic kinematics",
        "comments": "18 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The algebra of transactions as fundamental measurements is constructed on the\nbasis of the analysis of their properties and represents an expansion of the\nBoolean algebra. The notion of the generalized economic measurements of the\neconomic quantity and quality of objects of transactions is introduced. It has\nbeen shown that the vector space of economic states constructed on the basis of\nthese measurements is relativistic. The laws of kinematics of economic objects\nin this space have been analyzed and the stages of constructing the dynamics\nhave been formulated. In particular, the principle of maximum benefit, which\nrepresents an economic analog of the principle of least action in the classical\nmechanics, and the principle of relativity as the principle of equality of all\npossible consumer preferences have been formulated. The notion of economic\ninterval between two economic objects invariant to the selection of the vector\nof consumer preferences has been introduced. Methods of experimental\nverification of the principle of relativity in the space of economic states\nhave been proposed.\n"
    },
    {
        "paper_id": 1508.06236,
        "authors": "Luca Di Persio and Michele Bonollo and Gregorio Pellegrini",
        "title": "A computational spectral approach to interest rate models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Polynomial Chaos Expansion (PCE) technique recovers a finite second order\nrandom variable exploiting suitable linear combinations of orthogonal\npolynomials which are functions of a given stochas- tic quantity {\\xi}, hence\nacting as a kind of random basis. The PCE methodology has been developed as a\nmathematically rigorous Uncertainty Quantification (UQ) method which aims at\nproviding reliable numerical estimates for some uncertain physical quantities\ndefining the dynamic of certain engineering models and their related\nsimulations. In the present paper we exploit the PCE approach to analyze some\nequity and interest rate models considering, without loss of generality, the\none dimensional case. In particular we will take into account those models\nwhich are based on the Geometric Brownian Motion (gBm), e.g. the Vasicek model,\nthe CIR model, etc. We also provide several numerical applications and results\nwhich are discussed for a set of volatility values. The latter allows us to\ntest the PCE technique on a quite large set of different scenarios, hence\nproviding a rather complete and detailed investigation on PCE-approximation's\nfeatures and properties, such as the convergence of statistics, distribution\nand quantiles. Moreover we give results concerning both an efficiency and an\naccuracy study of our approach by comparing our outputs with the ones obtained\nadopting the Monte Carlo approach in its standard form as well as in its\nenhanced version.\n"
    },
    {
        "paper_id": 1508.06339,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "A General Framework for the Benchmark pricing in a Fully Collateralized\n  Market",
        "comments": "Revised version. Initial title was \"choice of collateral currency\n  updated.\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Collateralization with daily margining has become a new standard in the\npost-crisis market. Although there appeared vast literature on a so-called\nmulti-curve framework, a complete picture of a multi-currency setup with\ncross-currency basis can be rarely found since our initial attempts. This work\ngives its extension regarding a general framework of interest rates in a fully\ncollateralized market. It gives a new formulation of the currency funding\nspread which is better suited for the general dependence. In the last half, it\ndevelops a discretization of the HJM framework with a fixed tenor structure,\nwhich makes it implementable as a traditional Market Model.\n"
    },
    {
        "paper_id": 1508.06376,
        "authors": "Bernt {\\O}ksendal, Elin R{\\o}se",
        "title": "A white noise approach to insider trading",
        "comments": "arXiv admin note: text overlap with arXiv:1504.02581",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new approach to the optimal portfolio problem for an insider\nwith logarithmic utility. Our method is based on white noise theory, stochastic\nforward integrals, Hida-Malliavin calculus and the Donsker delta function.\n"
    },
    {
        "paper_id": 1508.06492,
        "authors": "Anis Al Gerbi, Benjamin Jourdain, Emmanuelle Cl\\'ement",
        "title": "Ninomiya-Victoir scheme: strong convergence, antithetic version and\n  application to multilevel estimators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we are interested in the strong convergence properties of the\nNinomiya-Victoir scheme which is known to exhibit weak convergence with order\n2. We prove strong convergence with order $1/2$. This study is aimed at\nanalysing the use of this scheme either at each level or only at the finest\nlevel of a multilevel Monte Carlo estimator: indeed, the variance of a\nmultilevel Monte Carlo estimator is related to the strong error between the two\nschemes used on the coarse and fine grids at each level. Recently, Giles and\nSzpruch proposed a scheme permitting to construct a multilevel Monte Carlo\nestimator achieving the optimal complexity $O\\left(\\epsilon^{-2}\\right)$ for\nthe precision $\\epsilon$. In the same spirit, we propose a modified\nNinomiya-Victoir scheme, which may be strongly coupled with order $1$ to the\nGiles-Szpruch scheme at the finest level of a multilevel Monte Carlo estimator.\nNumerical experiments show that this choice improves the efficiency, since the\norder $2$ of weak convergence of the Ninomiya-Victoir scheme permits to reduce\nthe number of discretization levels.\n"
    },
    {
        "paper_id": 1508.06586,
        "authors": "Carlos Pedro Gon\\c{c}alves",
        "title": "Financial Market Modeling with Quantum Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Econophysics has developed as a research field that applies the formalism of\nStatistical Mechanics and Quantum Mechanics to address Economics and Finance\nproblems. The branch of Econophysics that applies of Quantum Theory to\nEconomics and Finance is called Quantum Econophysics. In Finance, Quantum\nEconophysics' contributions have ranged from option pricing to market dynamics\nmodeling, behavioral finance and applications of Game Theory, integrating the\nempirical finding, from human decision analysis, that shows that nonlinear\nupdate rules in probabilities, leading to non-additive decision weights, can be\ncomputationally approached from quantum computation, with resulting quantum\ninterference terms explaining the non-additive probabilities. The current work\ndraws on these results to introduce new tools from Quantum Artificial\nIntelligence, namely Quantum Artificial Neural Networks as a way to build and\nsimulate financial market models with adaptive selection of trading rules,\nleading to turbulence and excess kurtosis in the returns distributions for a\nwide range of parameters.\n"
    },
    {
        "paper_id": 1508.06797,
        "authors": "A. Paliathanasis, K. Krishnakumar, K.M. Tamizhmani and P.G.L. Leach",
        "title": "Lie Symmetry Analysis of the Black-Scholes-Merton Model for European\n  Options with Stochastic Volatility",
        "comments": "Published version, 14pages, 4 figures",
        "journal-ref": "Mathematics 2016, 4(2), 28 (Special Issue: Mathematical Finance)",
        "doi": "10.3390/math4020028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a classification of the Lie point symmetries for the\nBlack--Scholes--Merton Model for European options with stochastic volatility,\n$\\sigma$, in which the last is defined by a stochastic differential equation\nwith an Orstein--Uhlenbeck term. In this model, the value of the option is\ngiven by a linear (1 + 2) evolution partial differential equation in which the\nprice of the option depends upon two independent variables, the value of the\nunderlying asset, $S$, and a new variable, $y$. We find that for arbitrary\nfunctional form of the volatility, $\\sigma(y)$, the (1 + 2) evolution equation\nalways admits two Lie point symmetries in addition to the automatic linear\nsymmetry and the infinite number of solution symmetries. However, when\n$\\sigma(y)=\\sigma_{0}$ and as the price of the option depends upon the second\nBrownian motion in which the volatility is defined, the (1 + 2) evolution is\nnot reduced to the Black--Scholes--Merton Equation, the model admits five Lie\npoint symmetries in addition to the linear symmetry and the infinite number of\nsolution symmetries. We apply the zeroth-order invariants of the Lie symmetries\nand we reduce the (1 + 2) evolution equation to a linear second-order ordinary\ndifferential equation. Finally, we study two models of special interest, the\nHeston model and the Stein--Stein model.\n"
    },
    {
        "paper_id": 1508.07428,
        "authors": "Noemi Nava, Tiziana Di Matteo and Tomaso Aste",
        "title": "Time-dependent scaling patterns in high frequency financial data",
        "comments": "28 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1140/epjst/e2015-50328-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We measure the influence of different time-scales on the dynamics of\nfinancial market data. This is obtained by decomposing financial time series\ninto simple oscillations associated with distinct time-scales. We propose two\nnew time-varying measures: 1) an amplitude scaling exponent and 2) an\nentropy-like measure. We apply these measures to intraday, 30-second sampled\nprices of various stock indices. Our results reveal intraday trends where\ndifferent time-horizons contribute with variable relative amplitudes over the\ncourse of the trading day. Our findings indicate that the time series we\nanalysed have a non-stationary multifractal nature with predominantly\npersistent behaviour at the middle of the trading session and anti-persistent\nbehaviour at the open and close. We demonstrate that these deviations are\nstatistically significant and robust.\n"
    },
    {
        "paper_id": 1508.07505,
        "authors": "Zhi-Qiang Jiang (ECUST, BU), Askery A. Canabarro (UFAL, BU), Boris\n  Podobnik (UR), H. Eugene Stanley (BU), and Wei-Xing Zhou (ECUST)",
        "title": "Early warning of large volatilities based on recurrence interval\n  analysis in Chinese stock markets",
        "comments": "13 pages and 7 figures",
        "journal-ref": "Quantitative Finance 16 (11), 1713-1724 (2016)",
        "doi": "10.1080/14697688.2016.1175656",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Being able to forcast extreme volatility is a central issue in financial risk\nmanagement. We present a large volatility predicting method based on the\ndistribution of recurrence intervals between volatilities exceeding a certain\nthreshold $Q$ for a fixed expected recurrence time $\\tau_Q$. We find that the\nrecurrence intervals are well approximated by the $q$-exponential distribution\nfor all stocks and all $\\tau_Q$ values. Thus a analytical formula for\ndetermining the hazard probability $W(\\Delta t |t)$ that a volatility above $Q$\nwill occur within a short interval $\\Delta t$ if the last volatility exceeding\n$Q$ happened $t$ periods ago can be directly derived from the $q$-exponential\ndistribution, which is found to be in good agreement with the empirical hazard\nprobability from real stock data. Using these results, we adopt a\ndecision-making algorithm for triggering the alarm of the occurrence of the\nnext volatility above $Q$ based on the hazard probability. Using a \"receiver\noperator characteristic\" (ROC) analysis, we find that this predicting method\nefficiently forecasts the occurrance of large volatility events in real stock\ndata. Our analysis may help us better understand reoccurring large volatilities\nand more accurately quantify financial risks in stock markets.\n"
    },
    {
        "paper_id": 1508.07534,
        "authors": "Daniya Tlegenova",
        "title": "Forecasting Exchange Rates Using Time Series Analysis: The sample of the\n  currency of Kazakhstan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper models yearly exchange rates between USD/KZT, EUR/KZT and SGD/KZT,\nand compares the actual data with developed forecasts using time series\nanalysis over the period from 2006 to 2014. The official yearly data of\nNational Bank of the Republic of Kazakhstan is used for present study. The main\ngoal of this paper is to apply the ARIMA model for forecasting of yearly\nexchange rates of USD/KZT, EUR/KZT and SGD/KZT. The accuracy of the forecast is\ncompared with Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE)\nand Root Mean Squared Error (RMSE).\n"
    },
    {
        "paper_id": 1508.07561,
        "authors": "Carla Mereu and Robert Stelzer",
        "title": "A BSDE arising in an exponential utility maximization problem in a pure\n  jump market model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of utility maximization with exponential preferences\nin a market where the traded stock/risky asset price is modelled as a\nL\\'evy-driven pure jump process (i.e. the driving L\\'evy process has no\nBrownian component). In this setting, we study the terminal utility\noptimization problem in the presence of a European contingent claim. We\nconsider in detail the BSDE (backward stochastic differential equation)\ncharacterising the value function when using an exponential utility function.\nFirst we analyse the well-definedness of the generator. This leads to some\nconditions on the market model related to conditions for the market to admit no\nfree lunches. Then we give bounds on the candidate optimal strategy.\n  Thereafter, we discuss the example of a cross-hedging problem and, under\nsevere assumptions on the structure of the claim, we give explicit solutions.\nFinally, we establish an explicit solution for a related BSDE with a suitable\nterminal condition but a simpler generator.\n"
    },
    {
        "paper_id": 1508.07582,
        "authors": "Christopher J. Rook, Mitchell Kerman",
        "title": "Approximating the Sum of Correlated Lognormals: An Implementation",
        "comments": "Fully documented source code is included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lognormal random variables appear naturally in many engineering disciplines,\nincluding wireless communications, reliability theory, and finance. So, too,\ndoes the sum of (correlated) lognormal random variables. Unfortunately, no\nclosed form probability distribution exists for such a sum, and it requires\napproximation. Some approximation methods date back over 80 years and most take\none of two approaches, either: 1) an approximate probability distribution is\nderived mathematically, or 2) the sum is approximated by a single lognormal\nrandom variable. In this research, we take the latter approach and review a\nfairly recent approximation procedure proposed by Mehta, Wu, Molisch, and Zhang\n(2007), then implement it using C++. The result is applied to a discrete time\nmodel commonly encountered within the field of financial economics.\n"
    },
    {
        "paper_id": 1508.07761,
        "authors": "Miklos Rasonyi",
        "title": "Maximizing expected utility in the Arbitrage Pricing Model",
        "comments": "Several corrections, Section 5 added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an infinite dimensional optimization problem motivated by\nmathematical economics. Within the celebrated \"Arbitrage Pricing Model\", we use\nprobabilistic and functional analytic techniques to show the existence of\noptimal strategies for investors who maximize their expected utility.\n"
    },
    {
        "paper_id": 1508.07891,
        "authors": "Tzu-Wei Yang and Lingjiong Zhu",
        "title": "A reduced-form model for level-1 limit order books",
        "comments": null,
        "journal-ref": "Market Microstructure and Liquidity, Volume 02, Issue 02,\n  September 2016",
        "doi": "10.1142/S2382626616500088",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One popular approach to model the limit order books dynamics of the best bid\nand ask at level-1 is to use the reduced-form diffusion approximations. It is\nwell known that the biggest contributing factor to the price movement is the\nimbalance of the best bid and ask. We investigate the data of the level-1 limit\norder books of a basket of stocks and study the numerical evidence of drift,\ncorrelation, volatility and their dependence on the imbalance. Based on the\nnumerical discoveries, we develop a nonparametric discrete model for the\ndynamics of the best bid and ask, which can be approximated by a reduced-form\nmodel with analytical tractability that can fit the empirical data of\ncorrelation, volatilities and probability of price movement simultaneously.\n"
    },
    {
        "paper_id": 1508.07914,
        "authors": "Roman Gayduk and Sergey Nadtochiy",
        "title": "Liquidity Effects of Trading Frequency",
        "comments": "Accepted in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we present a discrete time modeling framework, in which the\nshape and dynamics of a Limit Order Book (LOB) arise endogenously from an\nequilibrium between multiple market participants (agents). We use the proposed\nmodeling framework to analyze the effects of trading frequency on market\nliquidity in a very general setting. In particular, we demonstrate the dual\neffect of high trading frequency. On the one hand, the higher frequency\nincreases market efficiency, if the agents choose to provide liquidity in\nequilibrium. On the other hand, it also makes markets more fragile, in the\nsense that the agents choose to provide liquidity in equilibrium only if they\nare market-neutral (i.e., their beliefs satisfy certain martingale property).\nEven a very small deviation from market-neutrality may cause the agents to stop\nproviding liquidity, if the trading frequency is sufficiently high, which\nrepresents an endogenous liquidity crisis (aka flash crash) in the market. This\nframework enables us to provide more insight into how such a liquidity crisis\nunfolds, connecting it to the so-called adverse selection effect.\n"
    },
    {
        "paper_id": 1509.00136,
        "authors": "Alex Young",
        "title": "The effect of stock market indexing on corporate tax avoidance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Membership in the Russell 1000 and 2000 Indices is based on a ranking of\nmarket capitalization in May. Each index is separately value weighted such that\nfirms just inside the Russell 2000 are comparable in size to firms just outside\n(i.e. at the bottom of the Russell 1000) but have much higher index weights.\nThese features allow for the the annual reconstitution of these indices to be\nused as part of a regression discontinuity design to identify the effect of\nstock market indexing. Using this design, I investigate whether stock market\nindexing affects corporate tax avoidance. I find no evidence that firms just\ninside the Russell 2000 have significantly different effective tax rates than\nfirms just outside.\n"
    },
    {
        "paper_id": 1509.00217,
        "authors": "Aurelio F. Bariviera, M. Belen Guercio, Lisana B. Martinez, Osvaldo A.\n  Rosso",
        "title": "A permutation Information Theory tour through different interest rate\n  maturities: the Libor case",
        "comments": "arXiv admin note: text overlap with arXiv:1304.0399",
        "journal-ref": null,
        "doi": "10.1098/rsta.2015.0119",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes Libor interest rates for seven different maturities and\nreferred to operations in British Pounds, Euro, Swiss Francs and Japanese Yen,\nduring the period years 2001 to 2015. The analysis is performed by means of two\nquantifiers derived from Information Theory: the permutation Shannon entropy\nand the permutation Fisher information measure. An anomalous behavior in the\nLibor is detected in all currencies except Euro during the years 2006--2012.\nThe stochastic switch is more severe in 1, 2 and 3 months maturities. Given the\nspecial mechanism of Libor setting, we conjecture that the behavior could have\nbeen produced by the manipulation that was uncovered by financial authorities.\nWe argue that our methodology is pertinent as a market overseeing instrument.\n"
    },
    {
        "paper_id": 1509.00372,
        "authors": "Florian Ziel, Rick Steinert",
        "title": "Electricity Price Forecasting using Sale and Purchase Curves: The\n  X-Model",
        "comments": "Online appendix is partially provided",
        "journal-ref": "Energy Economics, 59 (2016) 435-454",
        "doi": "10.1016/j.eneco.2016.08.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our paper aims to model and forecast the electricity price by taking a\ncompletely new perspective on the data. It will be the first approach which is\nable to combine the insights of market structure models with extensive and\nmodern econometric analysis. Instead of directly modeling the electricity price\nas it is usually done in time series or data mining approaches, we model and\nutilize its true source: the sale and purchase curves of the electricity\nexchange. We will refer to this new model as X-Model, as almost every\nderegulated electricity price is simply the result of the intersection of the\nelectricity supply and demand curve at a certain auction. Therefore we show an\napproach to deal with a tremendous amount of auction data, using a subtle data\nprocessing technique as well as dimension reduction and lasso based estimation\nmethods. We incorporate not only several known features, such as seasonal\nbehavior or the impact of other processes like renewable energy, but also\ncompletely new elaborated stylized facts of the bidding structure. Our model is\nable to capture the non-linear behavior of the electricity price, which is\nespecially useful for predicting huge price spikes. Using simulation methods we\nshow how to derive prediction intervals for probabilistic forecasting. We\ndescribe and show the proposed methods for the day-ahead EPEX spot price of\nGermany and Austria.\n"
    },
    {
        "paper_id": 1509.00607,
        "authors": "Domenico Di Gangi, Fabrizio Lillo, Davide Pirino",
        "title": "Assessing systemic risk due to fire sales spillover through maximum\n  entropy network reconstruction",
        "comments": "36 pages, 6 figures, Accepted on Journal of Economic Dynamics and\n  Control",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assessing systemic risk in financial markets is of great importance but it\noften requires data that are unavailable or available at a very low frequency.\nFor this reason, systemic risk assessment with partial information is\npotentially very useful for regulators and other stakeholders. In this paper we\nconsider systemic risk due to fire sales spillover and portfolio rebalancing by\nusing the risk metrics defined by Greenwood et al. (2015). By using the Maximum\nEntropy principle we propose a method to assess aggregated and single bank's\nsystemicness and vulnerability and to statistically test for a change in these\nvariables when only the information on the size of each bank and the\ncapitalization of the investment assets are available. We prove the\neffectiveness of our method on 2001-2013 quarterly data of US banks for which\nportfolio composition is available.\n"
    },
    {
        "paper_id": 1509.00629,
        "authors": "Nicola Cufaro Petroni and Piergiacomo Sabino",
        "title": "Correlated Poisson processes and self-decomposable laws",
        "comments": "45 pages; lengthy calculations in the appendices; 7 figures; in press\n  on Med. J. Math",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a method to produce pairs of non independent Poisson processes\n$M(t),N(t)$ from positively correlated, self-decomposable, exponential\nrenewals. In particular the present paper provides the family of copulas\npairing the renewals, along with the closed form for the joint distribution\n$p_{m,n}(s,t)$ of the pair $\\big(M(s),N(t)\\big)$, an outcome which turns out to\nbe instrumental to produce explicit algorithms for applications in finance and\nqueuing theory. We finally discuss the cross-correlation properties of the two\nprocesses and the relative timing of their jumps\n"
    },
    {
        "paper_id": 1509.00686,
        "authors": "Erik Ekstr\\\"om and Juozas Vaicenavicius",
        "title": "Optimal liquidation of an asset under drift uncertainty",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a problem of finding an optimal stopping strategy to liquidate an\nasset with unknown drift. Taking a Bayesian approach, we model the initial\nbeliefs of an individual about the drift parameter by allowing an arbitrary\nprobability distribution to characterise the uncertainty about the drift\nparameter. Filtering theory is used to describe the evolution of the posterior\nbeliefs about the drift once the price process is being observed. An optimal\nstopping time is determined as the first passage time of the posterior mean\nbelow a monotone boundary, which can be characterised as the unique solution to\na non-linear integral equation. We also study monotonicity properties with\nrespect to the prior distribution and the asset volatility.\n"
    },
    {
        "paper_id": 1509.00959,
        "authors": "Somwrita Sarkar, Peter Phibbs, Roderick Simpson, Sachin Wasnik",
        "title": "The scaling of income inequality in cities",
        "comments": "10 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Developing a scientific understanding of cities in a fast urbanizing world is\nessential for planning sustainable urban systems. Recently, it was shown that\nincome and wealth creation follow increasing returns, scaling superlinearly\nwith city size. We study scaling of per capita incomes for separate census\ndefined income categories against population size for the whole of Australia.\nAcross several urban area definitions, we find that lowest incomes grow just\nlinearly or sublinearly ($\\beta = 0.94$ to $1.00$), whereas highest incomes\ngrow superlinearly ($\\beta = 1.00$ to $1.21$), with total income just\nsuperlinear ($\\beta = 1.03$ to $1.05$). These findings support the earlier\nfinding: the bigger the city, the richer the city. But, we also see an emergent\nmetric of inequality: the larger the population size and densities of a city,\nhigher incomes grow more quickly than lower, suggesting a disproportionate\nagglomeration of incomes in the highest income categories in big cities.\nBecause there are many more people on lower incomes that scale sublinearly as\ncompared to the highest that scale superlinearly, these findings suggest a\nscaling of inequality: the larger the population, the greater the inequality.\nUrban and economic planning will need to examine ways in which larger cities\ncan be made more equitable.\n"
    },
    {
        "paper_id": 1509.0098,
        "authors": "Ruimeng Hu and Mike Ludkovski",
        "title": "Sequential Design for Ranking Response Surfaces",
        "comments": "26 pages, 7 figures (updated several sections and figures)",
        "journal-ref": null,
        "doi": "10.1137/15M1045168",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and analyze sequential design methods for the problem of ranking\nseveral response surfaces. Namely, given $L \\ge 2$ response surfaces over a\ncontinuous input space $\\cal X$, the aim is to efficiently find the index of\nthe minimal response across the entire $\\cal X$. The response surfaces are not\nknown and have to be noisily sampled one-at-a-time. This setting is motivated\nby stochastic control applications and requires joint experimental design both\nin space and response-index dimensions. To generate sequential design\nheuristics we investigate stepwise uncertainty reduction approaches, as well as\nsampling based on posterior classification complexity. We also make connections\nbetween our continuous-input formulation and the discrete framework of pure\nregret in multi-armed bandits. To model the response surfaces we utilize\nkriging surrogates. Several numerical examples using both synthetic data and an\nepidemics control problem are provided to illustrate our approach and the\nefficacy of respective adaptive designs.\n"
    },
    {
        "paper_id": 1509.01144,
        "authors": "Nicola Cufaro Petroni and Piergiacomo Sabino",
        "title": "Cointegrating Jumps: an Application to Energy Facilities",
        "comments": "25 pages, 4 figures; Revised Section 5 with new and better data",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the concept of self-decomposable random variables we discuss the\napplication of a model for a pair of dependent Poisson processes to energy\nfacilities. Due to the resulting structure of the jump events we can see the\nself-decomposability as a form of cointegration among jumps. In the context of\nenergy facilities, the application of our approach to model power or gas\ndynamics and to evaluate transportation assets seen as spread options is\nstraightforward. We study the applicability of our methodology first assuming a\nMerton market model with two underlying assets; in a second step we consider\nprice dynamics driven by an exponential mean-reverting Geometric\nOrnstein-Uhlenbeck plus compound Poisson that are commonly used in the energy\nfield. In this specific case we propose a price spot dynamics for each\nunderlying that has the advantage of being treatable to find non-arbitrage\nconditions. In particular we can find close-form formulas for vanilla options\nso that the price and the Greeks of spread options can be calculated in close\nform using the Margrabe formula (if the strike is zero) or some other well\nknown approximation.\n"
    },
    {
        "paper_id": 1509.01157,
        "authors": "Anthony J. Webster and Richard H. Clarke",
        "title": "An Insurance-Led Response to Climate Change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate change is widely expected to increase weather related damage and the\ninsurance claims that result from it. This will increase insurance premiums, in\na way that is independent of a customer's contribution to the causes of climate\nchange. Insurance provides a financial mechanism that mitigates some of the\nconsequences of climate change, allowing damage from increasingly frequent\nevents to be repaired. We observe that the insurance industry could reclaim any\nincrease in claims due to climate change, by increasing the insurance premiums\non energy producers for example, without needing government intervention or a\nnew tax. We argue that this insurance-led levy must acknowledge both present\ncarbon emissions and a modern industry's carbon inheritance, that is, to\nrecognise that fossil-fuel driven industrial growth has provided the\ninnovations and conditions needed for modern civilisation to exist and develop.\nA tax or levy on energy production is one mechanism that would recognise carbon\ninheritance through the increased (energy) costs for manufacturing and using\nmodern technology, and can also provide an incentive to minimise carbon\nemissions, through higher costs for the most polluting industries. The\nnecessary increases in insurance premiums would initially be small, and will\nrequire an event attribution (EA) methodology to determine their size. We\npropose that the levies can be phased in as the science of event attribution\nbecomes sufficiently robust for each claim type, to ultimately provide a global\ninsurance-led response to climate change.\n"
    },
    {
        "paper_id": 1509.01175,
        "authors": "Josselin Garnier and Knut Solna",
        "title": "Correction to Black-Scholes formula due to fractional stochastic\n  volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical studies show that the volatility may exhibit correlations that\ndecay as a fractional power of the time offset. The paper presents a rigorous\nanalysis for the case when the stationary stochastic volatility model is\nconstructed in terms of a fractional Ornstein Uhlenbeck process to have such\ncorrelations. It is shown how the associated implied volatility has a term\nstructure that is a function of maturity to a fractional power.\n"
    },
    {
        "paper_id": 1509.01212,
        "authors": "Maria Ramos-Escamilla",
        "title": "Stochastic Frontier I & D of fractal dimensions for technological\n  innovation",
        "comments": "21 pages, in Spanish",
        "journal-ref": null,
        "doi": "10.1016/S0185-0849(13)71335-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an analysis of the study variables such as gdp,\nemployment levels, the level of R & D and technology that will serve as the\nbasis for stochastic modeling of production possibilities frontier in the\ngoodness of fractal dimensions Ex Ante and Ex Post a priori to determine the\nlevels of causality immediately and check its accuracy and power of indexing,\nusing high frequency data and thus address the response this assumption of\nstochastic frontiers with level N of partitions in time.\n"
    },
    {
        "paper_id": 1509.01213,
        "authors": "Tshilidzi Marwala",
        "title": "Impact of Artificial Intelligence on Economic Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial intelligence has impacted many aspects of human life. This paper\nstudies the impact of artificial intelligence on economic theory. In particular\nwe study the impact of artificial intelligence on the theory of bounded\nrationality, efficient market hypothesis and prospect theory.\n"
    },
    {
        "paper_id": 1509.01214,
        "authors": "Enrique Guerra-Pujol",
        "title": "The Poker-Litigation Game",
        "comments": "17 pages, 1 appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is litigation a serious search for truth or simply a game of skill or luck?\nAlthough the process of litigation has been modeled as a Prisoner's Dilemma, as\na War of Attrition, as a Game of Chicken and even as a simple coin toss, no one\nhas formally modeled litigation as a game of poker. This paper is the first to\ndo so. We present a simple \"poker-litigation game\" and find the optimal\nstrategy for playing this game.\n"
    },
    {
        "paper_id": 1509.01215,
        "authors": "Sunil Kumar, Zakir Husain and Diganta Mukherjee",
        "title": "Assessing Consistency of Consumer Confidence Data using Dynamic Latent\n  Class Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many countries information on expectations collected through consumer\nconfidence surveys are used in macroeconomic policy formulation. Unfortunately,\nbefore doing so, the consistency of responses is often not taken into account,\nleading to biases creeping in and affecting the reliability of the indices\nhence created. This paper describes how latent class analysis may be used to\ncheck the consistency of responses and ensure a parsimonious questionnaire. In\nparticular, we examine how temporal changes may be incorporated into the model.\nOur methodology is illustrated using three rounds of Consumer Confidence Survey\n(CCS) conducted by Reserve Bank of India (RBI).\n"
    },
    {
        "paper_id": 1509.01216,
        "authors": "Joachim Kaldasch",
        "title": "Dynamic Model of the Price Dispersion of Homogeneous Goods",
        "comments": null,
        "journal-ref": "British Journal of Economics, Management & Trade,8(2): 120-131,\n  2015, Article no.BJEMT.2015.104",
        "doi": "10.9734/BJEMT/2015/17849",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Presented is an analytic microeconomic model of the temporal price dispersion\nof homogeneous goods in polypoly markets. This new approach is based on the\nidea that the price dispersion has its origin in the dynamics of the purchase\nprocess. The price dispersion is determined by the chance that demanded and\nsupplied product units meet in a given price interval. It can be characterized\nby a fat-tailed Laplace distribution for short and by a lognormal distribution\nfor long time horizons. Taking random temporal variations of demanded and\nsupplied units into account both the mean price and also the standard deviation\nof the price dispersion are governed by a lognormal distribution. A comparison\nwith empirical investigations confirms the model statements.\n"
    },
    {
        "paper_id": 1509.01217,
        "authors": "Pietro DeLellis, Franco Garofalo, Francesco Lo Iudice, Elena\n  Napoletano",
        "title": "Wealth distribution across communities of adaptive financial agents",
        "comments": "18 pages, 7 figures, published in New Journal of Physics",
        "journal-ref": "New Journal of Physics, Volume 17, 083003, August 2015",
        "doi": "10.1088/1367-2630/17/8/083003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the trading volumes and wealth distribution of a novel\nagent-based model of an artificial financial market. In this model,\nheterogeneous agents, behaving according to the Von Neumann and Morgenstern\nutility theory, may mutually interact. A Tobin-like tax (TT) on successful\ninvestments and a flat tax are compared to assess the effects on the agents'\nwealth distribution. We carry out extensive numerical simulations in two\nalternative scenarios: i) a reference scenario, where the agents keep their\nutility function fixed, and ii) a focal scenario, where the agents are adaptive\nand self-organize in communities, emulating their neighbours by updating their\nown utility function. Specifically, the interactions among the agents are\nmodelled through a directed scale-free network to account for the presence of\ncommunity leaders, and the herding-like effect is tested against the reference\nscenario. We observe that our model is capable of replicating the benefits and\ndrawbacks of the two taxation systems and that the interactions among the\nagents strongly affect the wealth distribution across the communities.\nRemarkably, the communities benefit from the presence of leaders with\nsuccessful trading strategies, and are more likely to increase their average\nwealth. Moreover, this emulation mechanism mitigates the decrease in trading\nvolumes, which is a typical drawback of TTs.\n"
    },
    {
        "paper_id": 1509.01218,
        "authors": "Suren Harutyunyan",
        "title": "Tax Bond Creation Using a Structural Model and its Extensions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article describes and explores taxes and debt in finance. Here a\nsituation is thought about, where tax payments would qualify to be considered\nas debt. Using this principle we can infer that it is possible to create and\nprice a type of bond (Tax Normalization Guarantee) for companies, which would\nallow them to enter in temporary tax breaks to allow them to free capital.\nFinally it is explored a way to structure these bonds in financial products and\nvaluate them.\n"
    },
    {
        "paper_id": 1509.01479,
        "authors": "Andrei Cozma, Christoph Reisinger",
        "title": "A mixed Monte Carlo and PDE variance reduction method for foreign\n  exchange options under the Heston-CIR model",
        "comments": "52 pages, 7 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the valuation of European and path-dependent options in\nforeign exchange (FX) markets is considered when the currency exchange rate\nevolves according to the Heston model combined with the Cox-Ingersoll-Ross\ndynamics for the stochastic domestic and foreign short interest rates. The\nmixed Monte Carlo/PDE method requires that we simulate only the paths of the\nsquared volatility and the two interest rates, while an \"inner\"\nBlack-Scholes-type expectation is evaluated by means of a PDE. This can lead to\na substantial variance reduction and complexity improvements under certain\ncircumstances depending on the contract and the model parameters. In this work,\nwe establish the uniform boundedness of moments of the exchange rate process\nand its approximation, and prove strong convergence in $L^p$ ($p\\geq1$) of the\nlatter. Then, we carry out a variance reduction analysis and obtain accurate\napproximations for quantities of interest. All theoretical contributions can be\nextended to multi-factor short rates in a straightforward manner. Finally, we\nillustrate the efficiency of the method for the four-factor Heston-CIR model\nthrough a detailed quantitative assessment.\n"
    },
    {
        "paper_id": 1509.01482,
        "authors": "Manuel Sebastian Mariani, Alexandre Vidmer, Matus Medo, Yi-Cheng Zhang",
        "title": "Measuring economic complexity of countries and products: which metric to\n  use?",
        "comments": null,
        "journal-ref": "The European Physical Journal B 88 (11), 1-9 (2015)",
        "doi": "10.1140/epjb/e2015-60298-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Evaluating the economies of countries and their relations with products in\nthe global market is a central problem in economics, with far-reaching\nimplications to our theoretical understanding of the international trade as\nwell as to practical applications, such as policy making and financial\ninvestment planning. The recent Economic Complexity approach aims to quantify\nthe competitiveness of countries and the quality of the exported products based\non the empirical observation that the most competitive countries have\ndiversified exports, whereas developing countries only export few low quality\nproducts -- typically those exported by many other countries. Two different\nmetrics, Fitness-Complexity and the Method of Reflections, have been proposed\nto measure country and product score in the Economic Complexity framework. We\nuse international trade data and a recent ranking evaluation measure to\nquantitatively compare the ability of the two metrics to rank countries and\nproducts according to their importance in the network. The results show that\nthe Fitness-Complexity metric outperforms the Method of Reflections in both the\nranking of products and the ranking of countries. We also investigate a\nGeneralization of the Fitness-Complexity metric and show that it can produce\nimproved rankings provided that the input data are reliable.\n"
    },
    {
        "paper_id": 1509.01483,
        "authors": "Stanislao Gualdi and Antoine Mandel",
        "title": "On the emergence of scale-free production networks",
        "comments": "31 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple dynamical model of the formation of production networks\namong monopolistically competitive firms. The model subsumes the standard\ngeneral equilibrium approach \\`a la Arrow-Debreu but displays a wide set of\npotential dynamic behaviors. It robustly reproduces key stylized facts of\nfirms' demographics. Our main result is that competition between intermediate\ngood producers generically leads to the emergence of scale-free production\nnetworks.\n"
    },
    {
        "paper_id": 1509.01484,
        "authors": "Boris Bolshakov, Ekaterina Shamaeva, Eugene Popov",
        "title": "Interdisciplinary Business Games on Sustainable Development: Theoretical\n  Foundations and Prospects of Implementation",
        "comments": "Report at the XXIII International Scientific Symposium 'Miner's Week\n  - 2015' (MISiS, Moscow, 26-30 January 2015) pp 319-325",
        "journal-ref": null,
        "doi": "10.5281/zenodo.16849",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article defines the place of business games among all games in general\nbased on the classification by F.G. Junger; it provides critical analysis of\nexisting business games types; it also formulates requirements and lays\ntheoretical foundations and elements of the methodology and organization of\ninterdisciplinary business games (IBG) on sustainable development as a special\ntype of business games. In addition, it examines the prospects of IBG\nimplementation in higher education for sustainable development, using\ninformation technology and computer resources.\n"
    },
    {
        "paper_id": 1509.01526,
        "authors": "Bent Flyvbjerg and Cass R. Sunstein",
        "title": "The Principle of the Malevolent Hiding Hand; or, the Planning Fallacy\n  Writ Large",
        "comments": null,
        "journal-ref": "Social Research, vol. 83, no. 4, Winter, 2016, pp. 979-1004",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We identify and document a new principle of economic behavior: the principle\nof the Malevolent Hiding Hand. In a famous discussion, Albert Hirschman\ncelebrated the Hiding Hand, which he saw as a benevolent mechanism by which\nunrealistically optimistic planners embark on unexpectedly challenging plans,\nonly to be rescued by human ingenuity, which they could not anticipate, but\nwhich ultimately led to success, principally in the form of unexpectedly high\nnet benefits. Studying eleven projects, Hirschman suggested that the Hiding\nHand is a general phenomenon. But the Benevolent Hiding Hand has an evil twin,\nthe Malevolent Hiding Hand, which blinds excessively optimistic planners not\nonly to unexpectedly high costs but also to unexpectedly low net benefits.\nStudying a much larger sample than Hirschman did, we find that the Malevolent\nHiding Hand is common and that the phenomenon that Hirschman identified is\nrare. This sobering finding suggests that Hirschman's phenomenon is a special\ncase; it attests to the pervasiveness of the planning fallacy, writ very large.\nOne implication involves the continuing need for unbiased cost-benefit analyses\nand other economic decision support tools; another is that such tools might\nsometimes prove unreliable.\n"
    },
    {
        "paper_id": 1509.01672,
        "authors": "Huy N. Chau, Andrea Cosso, Claudio Fontana, Oleksii Mostovyi",
        "title": "Optimal investment with intermediate consumption under no unbounded\n  profit with bounded risk",
        "comments": "10 pages, revised version, to appear in the Applied Probability\n  Journals",
        "journal-ref": "Journal of Applied Probability, 2017, 54(3): 710-719",
        "doi": "10.1017/jpr.2017.29",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal investment with intermediate consumption\nin a general semimartingale model of an incomplete market, with preferences\nbeing represented by a utility stochastic field. We show that the key\nconclusions of the utility maximization theory hold under the assumptions of no\nunbounded profit with bounded risk (NUPBR) and of the finiteness of both primal\nand dual value functions.\n"
    },
    {
        "paper_id": 1509.01694,
        "authors": "Asaf Cohen, Virginia R. Young",
        "title": "Minimizing Lifetime Poverty with a Penalty for Bankruptcy",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics, 69, 156-167, 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide investment advice for an individual who wishes to minimize her\nlifetime poverty, with a penalty for bankruptcy or ruin. We measure poverty via\na non-negative, non-increasing function of (running) wealth. Thus, the lower\nwealth falls and the longer wealth stays low, the greater the penalty. This\npaper generalizes the problems of minimizing the probability of lifetime ruin\nand minimizing expected lifetime occupation, with the poverty function serving\nas a bridge between the two. To illustrate our model, we compute the optimal\ninvestment strategies for a specific poverty function and two consumption\nfunctions, and we prove some interesting properties of those investment\nstrategies.\n"
    },
    {
        "paper_id": 1509.01741,
        "authors": "Roman Kononenko",
        "title": "IMF Lending and Economic Growth: An Empirical Analysis of Ukraine",
        "comments": "Master's Thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study uses Vector Autoregression (VAR) Methodology as well as Vector\nError Correction (VEC) Methodology to examine the existence and direction of\ncausality between economic growth and IMF lending for Ukraine. The paper\nexamines the IMF lending data for the period of 1991-2010. Robust empirical\nanalysis indicates that IMF lending has a negative effect of on Ukraine's\neconomic growth in the short term. Policy implications of this finding are\nthat, despite short-run decline in economic growth, IMF lending can result in a\nlong-run sustainable growth for Ukraine. For this, policymakers need to ensure\nthat fund's money are used not only to cover budget's deficit, but also to\nfinance institutional reforms.\n"
    },
    {
        "paper_id": 1509.01839,
        "authors": "Aurelio F. Bariviera, Luciano Zunino, M. Belen Guercio, Lisana B.\n  Martinez, Osvaldo A. Rosso",
        "title": "Efficiency and credit ratings: a permutation-information-theory analysis",
        "comments": null,
        "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment, Vol 2013,\n  Number 08, P08007, 2013",
        "doi": "10.1088/1742-5468/2013/08/P08007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The role of credit rating agencies has been under severe scrutiny after the\nsubprime crisis. In this paper we explore the relationship between credit\nratings and informational efficiency of a sample of thirty nine corporate bonds\nof US oil and energy companies from April 2008 to November 2012. For that\npurpose, we use a powerful statistical tool relatively new in the financial\nliterature: the complexity-entropy causality plane. This representation space\nallows to graphically classify the different bonds according to their degree of\ninformational efficiency. We find that this classification agrees with the\ncredit ratings assigned by Moody's. Particularly, we detect the formation of\ntwo clusters, that correspond to the global categories of investment and\nspeculative grades. Regarding to the latter cluster, two subgroups reflect\ndistinct levels of efficiency. Additionally, we also find an intriguing absence\nof correlation between informational efficiency and firm characteristics. This\nallows us to conclude that the proposed permutation-information-theory approach\nprovides an alternative practical way to justify bond classification.\n"
    },
    {
        "paper_id": 1509.01966,
        "authors": "Florian Ziel",
        "title": "Forecasting Electricity Spot Prices using Lasso: On Capturing the\n  Autoregressive Intraday Structure",
        "comments": null,
        "journal-ref": "IEEE Transactions on Power Systems, 31.6 (2016) 4977-4987",
        "doi": "10.1109/TPWRS.2016.2521545",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a regression based model for day-ahead electricity\nspot prices. We estimate the considered linear regression model by the lasso\nestimation method. The lasso approach allows for many possible parameters in\nthe model, but also shrinks and sparsifies the parameters automatically to\navoid overfitting. Thus, it is able to capture the autoregressive intraday\ndependency structure of the electricity price well. We discuss in detail the\nestimation results which provide insights to the intraday behavior of\nelectricity prices. We perform an out-of-sample forecasting study for several\nEuropean electricity markets. The results illustrate well that the efficient\nlasso based estimation technique can exhibit advantages from two popular model\napproaches.\n"
    },
    {
        "paper_id": 1509.02179,
        "authors": "Michael Ludkovski",
        "title": "Kriging Metamodels and Experimental Design for Bermudan Option Pricing",
        "comments": "27 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate two new strategies for the numerical solution of optimal\nstopping problems within the Regression Monte Carlo (RMC) framework of\nLongstaff and Schwartz. First, we propose the use of stochastic kriging\n(Gaussian process) meta-models for fitting the continuation value. Kriging\noffers a flexible, nonparametric regression approach that quantifies\napproximation quality. Second, we connect the choice of stochastic grids used\nin RMC to the Design of Experiments paradigm. We examine space-filling and\nadaptive experimental designs; we also investigate the use of batching with\nreplicated simulations at design sites to improve the signal-to-noise ratio.\nNumerical case studies for valuing Bermudan Puts and Max-Calls under a variety\nof asset dynamics illustrate that our methods offer significant reduction in\nsimulation budgets over existing approaches.\n"
    },
    {
        "paper_id": 1509.02686,
        "authors": "Ludovic Goudenege, Andrea Molent, Antonino Zanette",
        "title": "Pricing and Hedging GLWB in the Heston and in the Black-Scholes with\n  Stochastic Interest Rate Models",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics. Volume 70, September 2016,\n  Pages 38-57",
        "doi": "10.1016/j.insmatheco.2016.05.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Valuing Guaranteed Lifelong Withdrawal Benefit (GLWB) has attracted\nsignificant attention from both the academic field and real world financial\nmarkets. As remarked by Forsyth and Vetzal the Black and Scholes framework\nseems to be inappropriate for such long maturity products. They propose to use\na regime switching model. Alternatively, we propose here to use a stochastic\nvolatility model (Heston model) and a Black Scholes model with stochastic\ninterest rate (Hull White model). For this purpose we present four numerical\nmethods for pricing GLWB variables annuities: a hybrid tree-finite difference\nmethod and a hybrid Monte Carlo method, an ADI finite difference scheme, and a\nstandard Monte Carlo method. These methods are used to determine the\nno-arbitrage fee for the most popular versions of the GLWB contract, and to\ncalculate the Greeks used in hedging. Both constant withdrawal and optimal\nwithdrawal (including lapsation) strategies are considered. Numerical results\nare presented which demonstrate the sensitivity of the no-arbitrage fee to\neconomic, contractual and longevity assumptions.\n"
    },
    {
        "paper_id": 1509.02711,
        "authors": "Asim Ghosh, Arnab Chatterjee, Jun-ichi Inoue, Bikas K. Chakrabarti",
        "title": "Inequality measures in kinetic exchange models of wealth distributions",
        "comments": "10 pages, 7 figures; accepted in Physica A",
        "journal-ref": "Physica A 451 (2016) 465-474",
        "doi": "10.1016/j.physa.2016.01.081",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the inequality indices for some models of wealth\nexchange. We calculated Gini index and newly introduced k-index and compare the\nresults with reported empirical data available for different countries. We have\nfound lower and upper bounds for the indices and discuss the efficiencies of\nthe models. Some exact analytical calculations are given for a few cases. We\nalso exactly compute the quantities for Gamma and double Gamma distributions.\n"
    },
    {
        "paper_id": 1509.02727,
        "authors": "Lioudmila Vostrikova",
        "title": "Utility Maximisation for Exponential Levy Models with option and\n  information processes",
        "comments": "27 pages, no figures",
        "journal-ref": "Theory Probab. Appl. 61-1 (2017), pp. 107-128",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider expected utility maximisation problem for exponential Levy models\nand HARA utilities in presence of illiquid asset in portfolio. This illiquid\nasset is modelled by an option of European type on another risky asset which is\ncorrelated with the first one. Under some hypothesis on Levy processes, we give\nthe expressions of information processes figured in maximum utility formula. As\napplications, we consider Black-Scholes models with correlated Brownian\nMotions, and also Black-Scholes models with jump part represented by Poisson\nprocess.\n"
    },
    {
        "paper_id": 1509.03264,
        "authors": "Simone Farinelli and Hideyuki Takada",
        "title": "Can You hear the Shape of a Market? Geometric Arbitrage and Spectral\n  Theory",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1406.6805,\n  arXiv:0910.1671",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric Arbitrage Theory reformulates a generic asset model possibly\nallowing for arbitrage by packaging all assets and their forwards dynamics into\na stochastic principal fibre bundle, with a connection whose parallel transport\nencodes discounting and portfolio rebalancing, and whose curvature measures, in\nthis geometric language, the 'instantaneous arbitrage capability' generated by\nthe market itself. The cashflow bundle is the vector bundle associated to this\nstochastic principal fibre bundle for the natural choice of the vector space\nfibre. The cashflow bundle carries a stochastic covariant differentiation\ninduced by the connection on the principal fibre bundle. The link between\narbitrage theory and spectral theory of the connection Laplacian on the vector\nbundle is given by the zero eigenspace resulting in a parametrization of all\nrisk neutral measures equivalent to the statistical one. This indicates that a\nmarket satisfies the (NFLVR) condition if and only if $0$ is in the discrete\nspectrum of the connection Laplacian on the cash flow bundle or of the Dirac\nLaplacian of the twisted cash flow bundle with the exterior algebra bundle. We\napply this result by extending Jarrow-Protter-Shimbo theory of asset bubbles\nfor complete arbitrage free markets to markets not satisfying the (NFLVR).\nMoreover, by means of the Atiyah-Singer index theorem, we prove that the Euler\ncharacteristic of the asset nominal space is a topological obstruction to the\nthe (NFLVR) condition, and, by means of the Bochner-Weitzenb\\\"ock formula, the\nnon vanishing of the homology group of the cash flow bundle is revealed to be a\ntopological obstruction to (NFLVR), too. Asset bubbles are defined, classified\nand decomposed for markets allowing arbitrage.\n"
    },
    {
        "paper_id": 1509.03577,
        "authors": "Edgardo Brigatti, Felipe Macias, Max O. Souza, and Jorge P. Zubelli",
        "title": "A Hedged Monte Carlo Approach to Real Option Pricing",
        "comments": "25 pages, 14 figures",
        "journal-ref": "Commodities, Energy and Environmental Finance, Fields Institute\n  Communications, 74, 275-299 (2015)",
        "doi": "10.1007/978-1-4939-2733-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we are concerned with valuing optionalities associated to invest\nor to delay investment in a project when the available information provided to\nthe manager comes from simulated data of cash flows under historical (or\nsubjective) measure in a possibly incomplete market. Our approach is suitable\nalso to incorporating subjective views from management or market experts and to\nstochastic investment costs. It is based on the Hedged Monte Carlo strategy\nproposed by Potters et al (2001) where options are priced simultaneously with\nthe determination of the corresponding hedging. The approach is particularly\nwell-suited to the evaluation of commodity related projects whereby the\navailability of pricing formulae is very rare, the scenario simulations are\nusually available only in the historical measure, and the cash flows can be\nhighly nonlinear functions of the prices.\n"
    },
    {
        "paper_id": 1509.03703,
        "authors": "Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Production Function of the Mining Sector of Iran",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this study is to estimate the production function and examine\nthe structure of production in the mining sector of Iran. Several studies have\nalready been conducted in estimating production functions of various economic\nsectors; however, less attention has been paid to mining sectors. After\nexamining the stationarity of variables using augmented Dickey-Fuller and\nPhillips-Perron tests, this study estimated the production function of the\nmining sector of Iran under different scenarios using the co-integration method\nand time-series data for 1976-2006. The unconstrained Cobb-Douglas production\nfunction in Tinbergen form provided better results in terms of theoretical\nfoundations of economics, statistics, and econometrics. These results show that\nthe structure of the mining sector of Iran is both capital-intensive and\nlabour-intensive. Based on the findings of this study, the elasticity of\nproduction with respect to capital and labour have been 0.44 and 0.41,\nrespectively. In addition, the coefficient of time variable, as an indicator of\ntechnological progress in the production process, is statistically significant\nrepresenting a positive effect of technological changes on the output of Iran's\nmining sector.\n"
    },
    {
        "paper_id": 1509.03864,
        "authors": "Paul M.N. Feehan, Ruoting Gong and Jian Song",
        "title": "Feynman-Kac Formulas for Solutions to Degenerate Elliptic and Parabolic\n  Boundary-Value and Obstacle Problems with Dirichlet Boundary Conditions",
        "comments": "41 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove Feynman-Kac formulas for solutions to elliptic and parabolic\nboundary value and obstacle problems associated with a general Markov diffusion\nprocess. Our diffusion model covers several popular stochastic volatility\nmodels, such as the Heston model, the CEV model and the SABR model, which are\nwidely used as asset pricing models in mathematical finance. The generator of\nthis Markov process with killing is a second-order, degenerate, elliptic\npartial differential operator, where the degeneracy in the operator symbol is\nproportional to the $2\\alpha$-power of the distance to the boundary of the\nhalf-plane, with $\\alpha\\in(0,1]$. Our stochastic representation formulas\nprovide the unique solutions to the elliptic boundary value and obstacle\nproblems, when we seek solutions which are suitably smooth up to the boundary\nportion $\\Gamma_{0}$ contained in the boundary of the upper half-plane. In the\ncase when the full Dirichlet condition is given, our stochastic representation\nformulas provide the unique solutions which are not guaranteed to be any more\nthan continuous up to the boundary portion $\\Gamma_{0}$.\n"
    },
    {
        "paper_id": 1509.04135,
        "authors": "Cl\\'audia Nunes and Rita Pimentel",
        "title": "Analytical solution to an investment problem under uncertainties with\n  shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive the optimal investment decision in a project where both demand and\ninvestment costs are stochastic processes, eventually subject to shocks. We\nextend the approach used in Dixit and Pindyck (1994), chapter 6.5, to deal with\ntwo sources of uncertainty, but assuming that the underlying processes are no\nlonger geometric Brownian diffusions but rather jump diffusion processes. For\nthe class of isoelastic functions that we address in this paper, it is still\npossible to derive a closed expression for the value of the firm. We prove\nformally that the result we get is indeed the solution of the optimization\nproblem.\n"
    },
    {
        "paper_id": 1509.04264,
        "authors": "Klaus Jaffe",
        "title": "Agent based simulations visualize Adam Smith's invisible hand by solving\n  Friedrich Hayek's Economic Calculus",
        "comments": "Econophysics, Complexity, Synergy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by Adam Smith and Friedrich Hayek, many economists have postulated\nthe existence of invisible forces that drive economic markets. These market\nforces interact in complex ways making it difficult to visualize or understand\nthe interactions in every detail. Here I show how these forces can transcend a\nzero-sum game and become a win-win business interaction, thanks to emergent\nsocial synergies triggered by division of labor. Computer simulations with the\nmodel Sociodynamica show here the detailed dynamics underlying this phenomenon\nin a simple virtual economy. In these simulations, independent agents act in an\neconomy exploiting and trading two different goods in a heterogeneous\nenvironment. All and each of the various forces and individuals were tracked\ncontinuously, allowing to unveil a synergistic effect on economic output\nproduced by the division of labor between agents. Running simulations in a\nhomogeneous environment, for example, eliminated all benefits of division of\nlabor. The simulations showed that the synergies unleashed by division of labor\narise if: Economies work in a heterogeneous environment; agents engage in\ncomplementary activities whose optimization processes diverge; agents have\nmeans to synchronize their activities. This insight, although trivial if viewed\na posteriori, improve our understanding of the source and nature of synergies\nin real economic markets and might render economic and natural sciences more\nconsilient.\n"
    },
    {
        "paper_id": 1509.04333,
        "authors": "Henk van Elst (Karlshochschule International University)",
        "title": "An Introduction to Business Mathematics",
        "comments": "87 pages, LaTeX2e, hyperlinked references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  These lecture notes provide a self-contained introduction to the mathematical\nmethods required in a Bachelor degree programme in Business, Economics, or\nManagement. In particular, the topics covered comprise real-valued vector and\nmatrix algebra, systems of linear algebraic equations, Leontief's stationary\ninput-output matrix model, linear programming, elementary financial\nmathematics, as well as differential and integral calculus of real-valued\nfunctions of one real variable. A special focus is set on applications in\nquantitative economical modelling.\n"
    },
    {
        "paper_id": 1509.04564,
        "authors": "Claudiu Herteliu, Bogdan Vasile Ileanu, Marcel Ausloos, Giulia Rotundo",
        "title": "Effect of religious rules on time of conception in Romania from 1905 to\n  2001",
        "comments": "new version correcting notation confusion between Tables and Figures,\n  correcting misprints in text, including formulae, with title and text closer\n  to publication, but yet with slightly different notations; now 28 pages, 3\n  Tables, 7 figures, 38 references",
        "journal-ref": "Human Reproduction, Vol.30, No.9 pp. 2202-2214, 2015",
        "doi": "10.1093/humrep/dev129",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Population growth (or decay) in a country can be due to various f\nsocio-economic constraints, as demonstrated in this paper. For example, sexual\nintercourse is banned in various religions, during Nativity and Lent fasting\nperiods. Data consisting of registered daily birth records for very long\n(35,429 points) time series and many (24,947,061) babies in Romania between\n1905 and 2001 (97 years) is analyzed. The data was obtained from the 1992 and\n2002 censuses, thus on persons alive at that time.\n  We grouped the population into two categories (Eastern Orthodox and\nNon-Orthodox) in order to distinguish religious constraints and performed\nextensive data analysis in a comparative manner for both groups. From such a\nlong time series data analysis, it seems that the Lent fast has a more drastic\neffect than the Nativity fast over baby conception within the Eastern Orthodox\npopulation, thereby differently increasing the population ratio. Thereafter, we\ndeveloped and tested econometric models where the dependent variable is the\nbaby conception deduced day, while the independent variables are: (i) religious\naffiliation; (ii) Nativity and Lent fast time intervals; (iii) rurality; (iv)\nday length; (v) weekend, and (vi) a trend background. Our findings are\nconcordant with other papers, proving differences between religious groups on\nconception, - although reaching different conclusions regarding the influence\nof weather on fertility. The approach seems a useful hint for developing\neconometric-like models in other sociophysics prone cases.\n"
    },
    {
        "paper_id": 1509.04839,
        "authors": "Xu Zuo Quan, Zhou Xun Yu, Zhuang Sheng Chao",
        "title": "Optimal Insurance with Rank-Dependent Utility and Increasing Indemnities",
        "comments": "33 pages, 3 figures",
        "journal-ref": "Mathematical Finance, Vol.29 (2019), 659-692",
        "doi": "10.1111/mafi.12185",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bernard et al. (2015) study an optimal insurance design problem where an\nindividual's preference is of the rank-dependent utility (RDU) type, and show\nthat in general an optimal contract covers both large and small losses.\nHowever, their contracts suffer from a problem of moral hazard for paying more\ncompensation for a smaller loss. This paper addresses this setback by\nexogenously imposing the constraint that both the indemnity function and the\ninsured's retention function be increasing with respect to the loss. We\ncharacterize the optimal solutions via calculus of variations, and then apply\nthe result to obtain explicitly expressed contracts for problems with Yaari's\ndual criterion and general RDU. Finally, we use a numerical example to compare\nthe results between ours and that of Bernard et al. (2015).\n"
    },
    {
        "paper_id": 1509.04952,
        "authors": "Zvonko Kostanjcar, Stjepan Begusic, H. E. Stanley, and Boris Podobnik",
        "title": "Estimating Tipping Points in Feedback-Driven Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/JSTSP.2016.2593099",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Much research has been conducted arguing that tipping points at which complex\nsystems experience phase transitions are difficult to identify. To test the\nexistence of tipping points in financial markets, based on the alternating\noffer strategic model we propose a network of bargaining agents who mutually\neither cooperate or where the feedback mechanism between trading and price\ndynamics is driven by an external \"hidden\" variable R that quantifies the\ndegree of market overpricing. Due to the feedback mechanism, R fluctuates and\noscillates over time, and thus periods when the market is underpriced and\noverpriced occur repeatedly. As the market becomes overpriced, bubbles are\ncreated that ultimately burst in a market crash. The probability that the index\nwill drop in the next year exhibits a strong hysteresis behavior from which we\ncalculate the tipping point. The probability distribution function of R has a\nbimodal shape characteristic of small systems near the tipping point. By\nexamining the S&P500 index we illustrate the applicability of the model and\ndemonstate that the financial data exhibits a hysteresis and a tipping point\nthat agree with the model predictions. We report a cointegration between the\nreturns of the S&P 500 index and its intrinsic value.\n"
    },
    {
        "paper_id": 1509.05024,
        "authors": "Valery Vilisov",
        "title": "Modeling Concordances of Company's Investment Directions With Its Market\n  Attraction",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.1.1707.8248",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work models the interconnection of company's investment managers'\nrepresentations and the market attraction of its shares. The models that\nreflect the connection of the company's market effectiveness indices and\nparameters of its economic activity are created on the basis of the\nMean-Variance Analysis and Regression Analysis. On another side, expert\nevaluation methods also clarified the same influence parameters, but it was\nmade according to the opinion of company managers. These two evaluation rows\nare used when making managerial decisions.\n"
    },
    {
        "paper_id": 1509.05471,
        "authors": "Riccardo Junior Buonocore, Tomaso Aste, Tiziana Di Matteo",
        "title": "Measuring multiscaling in financial time-series",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the origin of multiscaling in financial time-series and\ninvestigate how to best quantify it. Our methodology consists in separating the\ndifferent sources of measured multifractality by analysing the\nmulti/uni-scaling behaviour of synthetic time-series with known properties. We\nuse the results from the synthetic time-series to interpret the measure of\nmultifractality of real log-returns time-series. The main finding is that the\naggregation horizon of the returns can introduce a strong bias effect on the\nmeasure of multifractality. This effect can become especially important when\nreturns distributions have power law tails with exponents in the range [2,5].\nWe discuss the right aggregation horizon to mitigate this bias.\n"
    },
    {
        "paper_id": 1509.05475,
        "authors": "Gautier Marti, Philippe Very, Philippe Donnat, Frank Nielsen",
        "title": "A proposal of a methodological framework with experimental guidelines to\n  investigate clustering stability on financial time series",
        "comments": "Accepted at ICMLA 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present in this paper an empirical framework motivated by the practitioner\npoint of view on stability. The goal is to both assess clustering validity and\nyield market insights by providing through the data perturbations we propose a\nmulti-view of the assets' clustering behaviour. The perturbation framework is\nillustrated on an extensive credit default swap time series database available\nonline at www.datagrapple.com.\n"
    },
    {
        "paper_id": 1509.05638,
        "authors": "Nicole B\\\"auerle and Anna Ja\\'skiewicz",
        "title": "Stochastic Optimal Growth Model with Risk Sensitive Preferences",
        "comments": null,
        "journal-ref": "Journal of Economic Theory 173, 181-200, 2018",
        "doi": "10.1016/j.jet.2017.11.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a one-sector optimal growth model with i.i.d. productivity\nshocks that are allowed to be unbounded. The utility function is assumed to be\nnon-negative and unbounded from above. The novel feature in our framework is\nthat the agent has risk sensitive preferences in the sense of Hansen and\nSargent (1995). Under mild assumptions imposed on the productivity and utility\nfunctions we prove that the maximal discounted non-expected utility in the\ninfinite time horizon satisfies the optimality equation and the agent possesses\na stationary optimal policy. A new point used in our analysis is an inequality\nfor the so-called associated random variables. We also establish the Euler\nequation that incorporates the solution to the optimality equation.\n"
    },
    {
        "paper_id": 1509.05894,
        "authors": "Franco Ruzzenenti, Francesco Picciolo, Andreas Papandreou",
        "title": "A network analysis of the global energy market: an insight on the\n  entanglement between crude oil and the world economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One major hurdle in the road toward a low carbon economy is the present\nentanglement of developed economies with oil. This tight relationship is\nmirrored in the correlation between most of economic indicators with oil price.\nThis paper addresses the role of oil compared to the other three main energy\ncommodities -coal, gas and electricity, in shaping the international trading\nnetwork (ITW or WTW, world trade web) in the light of network theory. It\ninitially surveys briefly the literature on the correlation between oil prices\nwith economic growth and compares the concepts of time correlation with the\nconcept of spatial correlation brought about by network theory. It outlines the\nconceptual framework underpinning the network measures adopted in the analysis\nand results are presented. Three measures are taken into account: the ratio of\nmutual exchanges in the network (reciprocity); the role of distances in\ndetermining trades (spatial filling); and the spatial correlation of energy\ncommodities with the whole trade network and with four trading categories:\nfood, capital goods, intermediate goods and consumption goods. The analysis\ndeliver five main results:1) the the energy commodities network was\nstructurally stable amid dramatic growth during the decade considered; 2) that\noil is the most correlated energy commodity to the world trade web; 3) that oil\nis the most pervasive network, though coal is the less affected by distances;\n4) that oil has a remarkably high level of internal reciprocity and external\noverlapping 5) that the reciprocity of the trade network is negative correlated\nin time with the price of oil.\n"
    },
    {
        "paper_id": 1509.05943,
        "authors": "Valery Vilisov",
        "title": "Managing Cellular Billing Plan Switchings",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.1.3318.0642",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we shall consider a very popular practical applied problem of managing\nmode switching (in this work we are considering managing billing plans). Out of\nthe two parties (service provider and service consumer), participating in the\nprocesses modelled here, we shall consider only a consumer type of a problem.\nHerein we provide formal characterization of the problem as well as the\nelements necessary for its solution. We shall consider full predicted costs,\noriginating when switching to a billing plan as a target index. The work\ncontains an example that provides a detailed view of the application technology\nreferring to the suggested problem solution algorithm. Using the example's data\nwe have performed the analysis measuring the problem's sensitivity in relation\nto the growth of the traffic volume. Herein we provided a polynomial\napproximation of the target index value depending on the traffic volume.\n"
    },
    {
        "paper_id": 1509.05952,
        "authors": "Wen-Jie Xie and Zhi-Qiang Jiang and Gao-Feng Gu and Xiong Xiong and\n  Wei-Xing Zhou",
        "title": "Joint multifractal analysis based on the partition function approach:\n  Analytical analysis, numerical simulation and empirical application",
        "comments": "19 pages, 5 figures",
        "journal-ref": "New Journal of Physics 17 (10), 103020 (2015)",
        "doi": "10.1088/1367-2630/17/10/103020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many complex systems generate multifractal time series which are long-range\ncross-correlated. Numerous methods have been proposed to characterize the\nmultifractal nature of these long-range cross correlations. However, several\nimportant issues about these methods are not well understood and most methods\nconsider only one moment order. We study the joint multifractal analysis based\non partition function with two moment orders, which was initially invented to\ninvestigate fluid fields, and derive analytically several important properties.\nWe apply the method numerically to binomial measures with multifractal cross\ncorrelations and bivariate fractional Brownian motions without multifractal\ncross correlations. For binomial multifractal measures, the explicit\nexpressions of mass function, singularity strength and multifractal spectrum of\nthe cross correlations are derived, which agree excellently with the numerical\nresults. We also apply the method to stock market indexes and unveil intriguing\nmultifractality in the cross correlations of index volatilities.\n"
    },
    {
        "paper_id": 1509.05954,
        "authors": "Marco Cuturi, Alexandre d'Aspremont",
        "title": "Mean-Reverting Portfolios: Tradeoffs Between Sparsity and Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mean-reverting assets are one of the holy grails of financial markets: if\nsuch assets existed, they would provide trivially profitable investment\nstrategies for any investor able to trade them, thanks to the knowledge that\nsuch assets oscillate predictably around their long term mean. The modus\noperandi of cointegration-based trading strategies [Tsay, 2005, {\\S}8] is to\ncreate first a portfolio of assets whose aggregate value mean-reverts, to\nexploit that knowledge by selling short or buying that portfolio when its value\ndeviates from its long-term mean. Such portfolios are typically selected using\ntools from cointegration theory [Engle and Granger, 1987, Johansen, 1991],\nwhose aim is to detect combinations of assets that are stationary, and\ntherefore mean-reverting. We argue in this work that focusing on stationarity\nonly may not suffice to ensure profitability of cointegration-based strategies.\nWhile it might be possible to create syn- thetically, using a large array of\nfinancial assets, a portfolio whose aggre- gate value is stationary and\ntherefore mean-reverting, trading such a large portfolio incurs in practice\nimportant trade or borrow costs. Looking for stationary portfolios formed by\nmany assets may also result in portfolios that have a very small volatility and\nwhich require significant leverage to be profitable. We study in this work\nalgorithmic approaches that can take mitigate these effects by searching for\nmaximally mean-reverting portfo- lios which are sufficiently sparse and/or\nvolatile.\n"
    },
    {
        "paper_id": 1509.0621,
        "authors": "Michail Anthropelos, Scott Robertson, Konstantinos Spiliopoulos",
        "title": "The pricing of contingent claims and optimal positions in asymptotically\n  complete markets",
        "comments": "Final version of a paper to appear in Annals of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study utility indifference prices and optimal purchasing quantities for a\ncontingent claim, in an incomplete semi-martingale market, in the presence of\nvanishing hedging errors and/or risk aversion. Assuming that the average\nindifference price converges to a well defined limit, we prove that optimally\ntaken positions become large in absolute value at a specific rate. We draw\nmotivation from and make connections to Large Deviations theory, and in\nparticular, the celebrated G\\\"{a}rtner-Ellis theorem. We analyze a series of\nwell studied examples where this limiting behavior occurs, such as fixed\nmarkets with vanishing risk aversion, the basis risk model with high\ncorrelation, models of large markets with vanishing trading restrictions and\nthe Black-Scholes-Merton model with either vanishing default probabilities or\nvanishing transaction costs. Lastly, we show that the large claim regime could\nnaturally arise in partial equilibrium models.\n"
    },
    {
        "paper_id": 1509.06315,
        "authors": "Mateusz Denys, Maciej Jagielski, Tomasz Gubiec, Ryszard Kutner, H.\n  Eugene Stanley",
        "title": "Universality of market superstatistics",
        "comments": null,
        "journal-ref": "Phys. Rev. E 94, 042305 (2016)",
        "doi": "10.1103/PhysRevE.94.042305",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a continuous-time random walk (CTRW) to model market fluctuation data\nfrom times when traders experience excessive losses or excessive profits. We\nanalytically derive \"superstatistics\" that accurately model empirical market\nactivity data (supplied by Bogachev, Ludescher, Tsallis, and Bunde)that exhibit\ntransition thresholds. We measure the interevent times between excessive losses\nand excessive profits, and use the mean interevent time as a control variable\nto derive a universal description of empirical data collapse. Our\nsuperstatistic value is a weighted sum of two components, (i) a powerlaw\ncorrected by the lower incomplete gamma function, which asymptotically tends\ntoward robustness but initially gives an exponential, and (ii) a powerlaw\ndamped by the upper incomplete gamma function, which tends toward the power-law\nonly during short interevent times. We find that the scaling shape exponents\nthat drive both components subordinate themselves and a \"superscaling\"\nconfiguration emerges. We use superstatistics to describe the hierarchical\nactivity when component (i) reproduces the negative feedback and component (ii)\nreproduces the stylized fact of volatility clustering. Our results indicate\nthat there is a functional (but not literal) balance between excessive profits\nand excessive losses that can be described using the same body of\nsuperstatistics, but different calibration values and driving parameters.\n"
    },
    {
        "paper_id": 1509.06457,
        "authors": "Suneel Sarswat, Kandathil Mathew Abraham, Subir Kumar Ghosh",
        "title": "Identifying collusion groups using spectral clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an illiquid stock, traders can collude and place orders on a predetermined\nprice and quantity at a fixed schedule. This is usually done to manipulate the\nprice of the stock or to create artificial liquidity in the stock, which may\nmislead genuine investors. Here, the problem is to identify such group of\ncolluding traders. We modeled the problem instance as a graph, where each\ntrader corresponds to a vertex of the graph and trade corresponds to edges of\nthe graph. Further, we assign weights on edges depending on total volume, total\nnumber of trades, maximum change in the price and commonality between two\nvertices. Spectral clustering algorithms are used on the constructed graph to\nidentify colluding group(s). We have compared our results with simulated data\nto show the effectiveness of spectral clustering to detecting colluding groups.\nMoreover, we also have used parameters of real data to test the effectiveness\nof our algorithm.\n"
    },
    {
        "paper_id": 1509.06472,
        "authors": "Nikolai Dokuchaev",
        "title": "On the no-arbitrage market and continuity in the Hurst parameter",
        "comments": "arXiv admin note: text overlap with arXiv:1509.06112",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market with fractional Brownian motion with stochastic\nintegrals generated by the Riemann sums. We found that this market is arbitrage\nfree if admissible strategies that are using observations with an arbitrarily\nsmall delay. Moreover, we found that this approach eliminates the discontinuity\nof the stochastic integrals with respect to the Hurst parameter H at H=1/2.\n"
    },
    {
        "paper_id": 1509.06504,
        "authors": "Henry Ngongo (UEA)",
        "title": "Les indicateus avanc\\'es de l'inflation en RDCongo",
        "comments": "in French",
        "journal-ref": "Annales de l'U.E.A, Universit\\'e Evang\\'elique en Afrique, 2015,\n  Annales de l'U.E.A Num\\'ero 5, Volume 4, 4 (5), pp.23 - 40",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to identify the leading of inflation indicators of monetary\npolicy in DRC. The results reveal that the most relevant inflation indicators\nusually come from the monetary origin than the real sector. Variance\ndecomposition analyzes place in the foreground the rate of exchange, the money\nsupply and the public consumption like the most relevant indicators. In order\nto achieve its goal of price stability and to support a strong economic growth,\nthe intermediate objective of the Central Bank baseded on the controle of the\nmoney supply seems to be less relevant. Relates to a high level of the\ndollarization, the central bank will be able to adopt either the strategy of\nnominal anchoring of the rate of exchange, this calls the return of the fixed\nexchange regime or to adopt a strategy of inflation targeting is to restore the\ncredibility of the monetary policy\n"
    },
    {
        "paper_id": 1509.06524,
        "authors": "Maurizio Naldi, Giuseppe D'Acquisto",
        "title": "Option contracts for a privacy-aware market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Suppliers (including companies and individual prosumers) may wish to protect\ntheir private information when selling items they have in stock. A market is\nenvisaged where private information can be protected through the use of\ndifferential privacy and option contracts, while privacy-aware suppliers\ndeliver their stock at a reduced price. In such a marketplace a broker acts as\nintermediary between privacy-aware suppliers and end customers, providing the\nextra items possibly needed to fully meet the customers' demand, while end\ncustomers book the items they need through an option contract. All stakeholders\nmay benefit from such a marketplace. A formula is provided for the option\nprice, and a budget equation is set for the mechanism to be profitable for the\nbroker/producer.\n"
    },
    {
        "paper_id": 1509.06544,
        "authors": "Matt V. Leduc, Matthew O. Jackson, Ramesh Johari",
        "title": "Pricing and Referrals in Diffusion on Networks",
        "comments": "44 pages, 3 tables, 8 figures",
        "journal-ref": "Games and Economic Behavior 104 (2017) 568-594",
        "doi": "10.1016/j.geb.2017.05.011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When a new product or technology is introduced, potential consumers can learn\nits quality by trying the product, at a risk, or by letting others try it and\nfree-riding on the information that they generate. We propose a dynamic game to\nstudy the adoption of technologies of uncertain value, when agents are\nconnected by a network and a monopolist seller chooses a policy to maximize\nprofits. Consumers with low degree (few friends) have incentives to adopt\nearly, while consumers with high degree have incentives to free ride. The\nseller can induce high-degree consumers to adopt early by offering referral\nincentives - rewards to early adopters whose friends buy in the second period.\nReferral incentives thus lead to a `double-threshold strategy' by which low and\nhigh-degree agents adopt the product early while middle-degree agents wait. We\nshow that referral incentives are optimal on certain networks while\ninter-temporal price discrimination (i.e., a first-period price discount) is\noptimal on others, and discuss welfare implications.\n"
    },
    {
        "paper_id": 1509.06612,
        "authors": "Ron W. Nielsen",
        "title": "Mathematical Analysis of the Historical Economic Growth",
        "comments": "23 pages, 23 figures, 1 table, 7150 words. Contains important\n  clarification",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data describing historical economic growth are analysed. Included in the\nanalysis is the world and regional economic growth. The analysis demonstrates\nthat historical economic growth had a natural tendency to follow hyperbolic\ndistributions. Parameters describing hyperbolic distributions have been\ndetermined. A search for takeoffs from stagnation to growth produced negative\nresults. This analysis throws a new light on the interpretation of the\nmechanism of the historical economic growth and suggests new lines of research.\n"
    },
    {
        "paper_id": 1509.07155,
        "authors": "Hee Su Roh, Yinyu Ye",
        "title": "Market Making with Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pari-mutuel markets are trading platforms through which the common market\nmaker simultaneously clears multiple contingent claims markets. This market has\nseveral distinctive properties that began attracting the attention of the\nfinancial industry in the 2000s. For example, the platform aggregates liquidity\nfrom the individual contingent claims market into the common pool while\nshielding the market maker from potential financial loss. The contribution of\nthis paper is two-fold. First, we provide a new economic interpretation of the\nmarket-clearing strategy of a pari-mutuel market that is well known in the\nliterature. The pari-mutuel auctioneer is shown to be equivalent to the market\nmaker with extreme ambiguity aversion for the future contingent event. Second,\nbased on this theoretical understanding, we present a new market-clearing\nalgorithm called the Knightian Pari-mutuel Mechanism (KPM). The KPM retains\nmany interesting properties of pari-mutuel markets while explicitly controlling\nfor the market maker's ambiguity aversion. In addition, the KPM is\ncomputationally efficient in that it is solvable in polynomial time.\n"
    },
    {
        "paper_id": 1509.0771,
        "authors": "Pierre Blanc, Jonathan Donier, Jean-Philippe Bouchaud",
        "title": "Quadratic Hawkes processes for financial prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and establish the main properties of QHawkes (\"Quadratic\"\nHawkes) models. QHawkes models generalize the Hawkes price models introduced in\nE. Bacry et al. (2014), by allowing all feedback effects in the jump intensity\nthat are linear and quadratic in past returns. A non-parametric fit on NYSE\nstock data shows that the off-diagonal component of the quadratic kernel indeed\nhas a structure that standard Hawkes models fail to reproduce. Our model\nexhibits two main properties, that we believe are crucial in the modelling and\nthe understanding of the volatility process: first, the model is time-reversal\nasymmetric, similar to financial markets whose time evolution has a preferred\ndirection. Second, it generates a multiplicative, fat-tailed volatility\nprocess, that we characterize in detail in the case of exponentially decaying\nkernels, and which is linked to Pearson diffusions in the continuous limit.\nSeveral other interesting properties of QHawkes processes are discussed, in\nparticular the fact that they can generate long memory without necessarily be\nat the critical point. Finally, we provide numerical simulations of our\ncalibrated QHawkes model, which is indeed seen to reproduce, with only a small\namount of quadratic non-linearity, the correct magnitude of fat-tails and time\nreversal asymmetry seen in empirical time series.\n"
    },
    {
        "paper_id": 1509.07751,
        "authors": "Lars Josef H\\\"o\\\"ok and Erik Lindstr\\\"om",
        "title": "Efficient Computation of the Quasi Likelihood function for Discretely\n  Observed Diffusion Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a simple method for nearly simultaneous computation of all\nmoments needed for quasi maximum likelihood estimation of parameters in\ndiscretely observed stochastic differential equations commonly seen in finance.\nThe method proposed in this papers is not restricted to any particular dynamics\nof the differential equation and is virtually insensitive to the sampling\ninterval. The key contribution of the paper is that computational complexity is\nsublinear in the number of observations as we compute all moments through a\nsingle operation. Furthermore, that operation can be done offline. The\nsimulations show that the method is unbiased for all practical purposes for any\nsampling design, including random sampling, and that the computational cost is\ncomparable (actually faster for moderate and large data sets) to the simple,\noften severely biased, Euler-Maruyama approximation.\n"
    },
    {
        "paper_id": 1509.07953,
        "authors": "Peter A. Bebbington and Reimer Kuehn",
        "title": "Optimal trading strategies - a time series approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2016/05/053209",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by recent advances in the spectral theory of auto-covariance\nmatrices, we are led to revisit a reformulation of Markowitz' mean-variance\nportfolio optimization approach in the time domain. In its simplest incarnation\nit applies to a single traded asset and allows to find an optimal trading\nstrategy which - for a given return - is minimally exposed to market price\nfluctuations. The model is initially investigated for a range of synthetic\nprice processes, taken to be either second order stationary, or to exhibit\nsecond order stationary increments. Attention is paid to consequences of\nestimating auto-covariance matrices from small finite samples, and\nauto-covariance matrix cleaning strategies to mitigate against these are\ninvestigated. Finally we apply our framework to real world data.\n"
    },
    {
        "paper_id": 1509.08079,
        "authors": "Rubina Zadourian, Peter Grassberger",
        "title": "Asymmetry of cross correlations between intra-day and overnight\n  volatilities",
        "comments": "4 pages, including 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We point out a stunning time asymmetry in the short time cross correlations\nbetween intra-day and overnight volatilities (absolute values of log-returns of\nstock prices). While overnight volatility is significantly (and positively)\ncorrelated with the intra-day volatility during the \\textit{following} day\n(allowing thus non-trivial predictions), it is much less correlated with the\nintra-day volatility during the \\textit{preceding} day. While the effect is not\nunexpected in view of previous observations, its robustness and extreme\nsimplicity are remarkable.\n"
    },
    {
        "paper_id": 1509.0811,
        "authors": "Zura Kakushadze and Igor Tulchinsky",
        "title": "Performance v. Turnover: A Story by 4,000 Alphas",
        "comments": "17 pages; 2 trivial typos fixed",
        "journal-ref": "The Journal of Investment Strategies 5(2) (2016) 75-89, Invited\n  Investment Strategy Forum Paper",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze empirical data for 4,000 real-life trading portfolios (U.S.\nequities) with holding periods of about 0.7-19 trading days. We find a simple\nscaling C ~ 1/T, where C is cents-per-share, and T is the portfolio turnover.\nThus, the portfolio return R has no statistically significant dependence on the\nturnover T. We also find a scaling R ~ V^X, where V is the portfolio\nvolatility, and the power X is around 0.8-0.85 for holding periods up to 10\ndays or so. To our knowledge, this is the only publicly available empirical\nstudy on such a large number of real-life trading portfolios/alphas.\n"
    },
    {
        "paper_id": 1509.08248,
        "authors": "Robert L\\\"ow, Stanislaus Maier-Paape, Andreas Platen",
        "title": "Correctness of Backtest Engines",
        "comments": "15 pages, 6 figures; Keywords: backtest evaluation, historical\n  simulation, trading system, candle chart, imperfect data, price model,\n  correctness test, backtest correctness",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years several trading platforms appeared which provide a backtest\nengine to calculate historic performance of self designed trading strategies on\nunderlying candle data. The construction of a correct working backtest engine\nis, however, a subtle task as shown by Maier-Paape and Platen (cf.\narXiv:1412.5558 [q-fin.TR]). Several platforms are struggling on the\ncorrectness.\n  In this work, we discuss the problem how the correctness of backtest engines\ncan be verified. We provide models for candles and for intra-period prices\nwhich will be applied to conduct a proof of correctness for a given backtest\nengine if the here provided tests on specific model candles are successful.\nFurthermore, we hint to algorithmic considerations in order to allow for a fast\nimplementation of these tests necessary for the proof of correctness.\n"
    },
    {
        "paper_id": 1509.08272,
        "authors": "Fred Espen Benth and Heidar Eyjolfsson",
        "title": "Representation and approximation of ambit fields in Hilbert space",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We lift ambit fields as introduced by Barndorff-Nielsen and Schmiegel to a\nclass of Hilbert space-valued volatility modulated Volterra processes. We name\nthis class Hambit fields, and show that they can be expressed as a countable\nsum of weighted real-valued volatility modulated Volterra processes. Moreover,\nHambit fields can be interpreted as the boundary of the mild solution of a\ncertain first order stochastic partial differential equation. This stochastic\npartial differential equation is formulated on a suitable Hilbert space of\nfunctions on the positive real line with values in the state space of the\nHambit field. We provide an explicit construction of such a space. Finally, we\napply this interpretation of Hambit fields to develop a finite difference\nscheme, for which we prove convergence under some Lipschitz conditions.\n"
    },
    {
        "paper_id": 1509.0828,
        "authors": "Mikl\\'os R\\'asonyi and Hasanjan Sayit",
        "title": "Sticky processes, local and true martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that for a so-called sticky process $S$ there exists an equivalent\nprobability $Q$ and a $Q$-martingale $\\tilde{S}$ that is arbitrarily close to\n$S$ in $L^p(Q)$ norm. For continuous $S$, $\\tilde{S}$ can be chosen arbitrarily\nclose to $S$ in supremum norm. In the case where $S$ is a local martingale we\nmay choose $Q$ arbitrarily close to the original probability in the total\nvariation norm. We provide examples to illustrate the power of our results and\npresent applications in mathematical finance.\n"
    },
    {
        "paper_id": 1509.08281,
        "authors": "Alexander Schied, Elias Strehle, and Tao Zhang",
        "title": "High-frequency limit of Nash equilibria in a market impact game with\n  transient price impact",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1137/16M107030X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the high-frequency limits of strategies and costs in a Nash\nequilibrium for two agents that are competing to minimize liquidation costs in\na discrete-time market impact model with exponentially decaying price impact\nand quadratic transaction costs of size $\\theta\\ge0$. We show that, for\n$\\theta=0$, equilibrium strategies and costs will oscillate indefinitely\nbetween two accumulation points. For $\\theta>0$, however, strategies, costs,\nand total transaction costs will converge towards limits that are independent\nof $\\theta$. We then show that the limiting strategies form a Nash equilibrium\nfor a continuous-time version of the model with $\\theta$ equal to a certain\ncritical value $\\theta^*>0$, and that the corresponding expected costs coincide\nwith the high-frequency limits of the discrete-time equilibrium costs. For\n$\\theta\\neq\\theta^*$, however, continuous-time Nash equilibria will typically\nnot exist. Our results permit us to give mathematically rigorous proofs of\nnumerical observations made in Schied and Zhang (2013). In particular, we\nprovide a range of model parameters for which the limiting expected costs of\nboth agents are decreasing functions of $\\theta$. That is, for sufficiently\nhigh trading speed, raising additional transaction costs can reduce the\nexpected costs of all agents.\n"
    },
    {
        "paper_id": 1509.08291,
        "authors": "Tobias Scholl, Antonios Garas, Frank Schweitzer",
        "title": "The spatial component of R&D networks",
        "comments": "Working paper, 22 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the role of geography in R&D networks by means of a quantitative,\nmicro-geographic approach. Using a large database that covers international R&D\ncollaborations from 1984 to 2009, we localize each actor precisely in space\nthrough its latitude and longitude. This allows us to analyze the R&D network\nat all geographic scales simultaneously. Our empirical results show that\ndespite the high importance of the city level, transnational R&D collaborations\nat large distances are much more frequent than expected from similar networks.\nThis provides evidence for the ambiguity of distance in economic cooperation\nwhich is also suggested by the existing literature. In addition we test whether\nthe hypothesis of local buzz and global pipelines applies to the observed R&D\nnetwork by calculating well-defined metrics from network theory.\n"
    },
    {
        "paper_id": 1509.08503,
        "authors": "Enzo Busseti and Stephen Boyd",
        "title": "Volume Weighted Average Price Optimal Execution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of optimal execution of a trading order under Volume\nWeighted Average Price (VWAP) benchmark, from the point of view of a\nrisk-averse broker. The problem consists in minimizing mean-variance of the\nslippage, with quadratic transaction costs. We devise multiple ways to solve\nit, in particular we study how to incorporate the information coming from the\nmarket during the schedule. Most related works in the literature eschew the\nissue of imperfect knowledge of the total market volume. We instead incorporate\nit in our model. We validate our method with extensive simulation of order\nexecution on real NYSE market data. Our proposed solution, using a simple model\nfor market volumes, reduces by 10% the VWAP deviation RMSE of the standard\n\"static\" solution (and can simultaneously reduce transaction costs).\n"
    },
    {
        "paper_id": 1509.08869,
        "authors": "Matyas Barczy, Mohamed Ben Alaya, Ahmed Kebaier, Gyula Pap",
        "title": "Asymptotic behavior of maximum likelihood estimators for a jump-type\n  Heston model",
        "comments": "51 pages, 3 figures",
        "journal-ref": "Journal of Statistical Planning and Inference 198, (2019), 139-164",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study asymptotic properties of maximum likelihood estimators of drift\nparameters for a jump-type Heston model based on continuous time observations,\nwhere the jump process can be any purely non-Gaussian L\\'evy process of not\nnecessarily bounded variation with a L\\'evy measure concentrated on\n$(-1,\\infty)$. We prove strong consistency and asymptotic normality for all\nadmissible parameter values except one, where we show only weak consistency and\nmixed normal (but non-normal) asymptotic behavior. It turns out that the\nvolatility of the price process is a measurable function of the price process.\nWe also present some numerical illustrations to confirm our results.\n"
    },
    {
        "paper_id": 1509.09133,
        "authors": "Nicole El Karoui (LPMA), Monique Jeanblanc (LaMME), Ying Jiao (SAF)",
        "title": "Dynamics of multivariate default system in random environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a multivariate default system where random environmental\ninformation is available. We study the dynamics of the system in a general\nsetting and adopt the point of view of change of probability measures. We also\nmake a link with the density approach in the credit risk modelling. In the\nparticular case where no environmental information is concerned, we pay a\nspecial attention to the phenomenon of system weakened by failures as in the\nclassical reliability system.\n"
    },
    {
        "paper_id": 1510.00237,
        "authors": "Michel Fliess, C\\'edric Join",
        "title": "Seasonalities and cycles in time series: A fresh look with computer\n  experiments",
        "comments": "in 2015 Paris Financial Management Conference (14-15 December 2015)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in the understanding of time series permit to clarify\nseasonalities and cycles, which might be rather obscure in today's literature.\nA theorem due to P. Cartier and Y. Perrin, which was published only recently,\nin 1995, and several time scales yield, perhaps for the first time, a clear-cut\ndefinition of seasonalities and cycles. Their detection and their extraction,\nmoreover, become easy to implement. Several computer experiments with concrete\ndata from various fields are presented and discussed. The conclusion suggests\nthe application of this approach to the debatable Kondriatev waves.\n"
    },
    {
        "paper_id": 1510.00352,
        "authors": "Dmitry Lesnik",
        "title": "Retarded action principle and self-financing portfolio dynamics",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a consistent differential representation for the dynamics of a\nself-financing portfolio for different hedging strategies. In the basis of the\nderivation there is the so called \"retarded action principle\", which represents\nthe causality in the evolution of dependent stochastic variables. We\ndemonstrate this principle on example of a vanilla and a storage option.\n"
    },
    {
        "paper_id": 1510.00616,
        "authors": "Oliver Kley and Claudia Kl\\\"uppelberg and Gesine Reinert",
        "title": "Conditional risk measures in a bipartite market structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the effect of network structure between agents and\nobjects on measures for systemic risk. We model the influence of sharing large\nexogeneous losses to the financial or (re)insuance market by a bipartite graph.\nUsing Pareto-tailed losses and multivariate regular variation we obtain\nasymptotic results for systemic conditional risk measures based on the\nValue-at-Risk and the Conditional Tail Expectation. These results allow us to\nassess the influence of an individual institution on the systemic or market\nrisk and vice versa through a collection of conditional systemic risk measures.\nFor large markets Poisson approximations of the relevant constants are provided\nin the example of an insurance market. The example of an underlying homogeneous\nrandom graph is analysed in detail, and the results are illustrated through\nsimulations.\n"
    },
    {
        "paper_id": 1510.00665,
        "authors": "Paul Studtmann",
        "title": "Universalized Prisoner's Dilemma With Risk",
        "comments": "Includes four pages of Mathematica worksheets that verify the\n  mathematical claims made in the paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper I present a mathematically novel approach to the Prisoner's\nDilemma. I do so by first defining recursively a distinct action type, what I\ncall 'universalizing', that I add to the original prisoner's dilemma. Such a\nmodified version of the Prisoner's Dilemma provides a very food productive\nmodel of the choices that would be made in a prisoner's dilemma by agents who\ntrust each other. As I show, players playing a universalized prisoner's dilemma\nget as far out of the dilemma as is mathematically possible. I then add the\nconcept of risk to the universalized version of prisoner's dilemma. Doing so\nprovide a model that is sensitive to the trustworthiness of the agents in any\nprisoner's dilemma. As I show, with no risk, agents get out of the prisoners\ndilemma; and with maximal risk, the succumb to it. succumb to it.\n"
    },
    {
        "paper_id": 1510.00698,
        "authors": "Benoit Mahault, Avadh Saxena and Cristiano Nisoli",
        "title": "More Opportunities than Wealth: A Network of Power and Frustration",
        "comments": "10 pages, 4 figures, supplementary material, and 3 animations",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a minimal agent-based model to qualitatively conceptualize the\nallocation of limited wealth among more abundant opportunities. We study the\ninterplay of power, satisfaction and frustration in distribution,\nconcentration, and inequality of wealth. Our framework allows us to compare\nsubjective measures of frustration and satisfaction to collective measures of\nfairness in wealth distribution, such as the Lorenz curve and the Gini index.\nWe find that a completely libertarian, law-of-the-jungle setting, where every\nagent can acquire wealth from, or lose wealth to, anybody else invariably leads\nto a complete polarization of the distribution of wealth vs. opportunity. The\npicture is however dramatically modified when hard constraints are imposed over\nagents, and they are limited to share wealth with neighbors on a network. We\nthen propose an out of equilibrium dynamics {\\it of} the networks, based on a\ncompetition between power and frustration in the decision-making of agents that\nleads to network coevolution. We show that the ratio of power and frustration\ncontrols different dynamical regimes separated by kinetic transitions and\ncharacterized by drastically different values of the indices of equality. The\ninterplay of power and frustration leads to the emergence of three\nself-organized social classes, lower, middle, and upper class, whose\ninteractions drive a cyclical regime.\n"
    },
    {
        "paper_id": 1510.00876,
        "authors": "I.A. Molotkov, A.I. Osin",
        "title": "Analysis of the particle transfer between two systems under unification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate unification of two systems of identical elements having\ndifferent dimensions which may be of interest for both physics and economics.\nCharacteristic parameters as well as explicit formulae for the temperature (in\neconomics - capital turnover) and dimension of the united system are obtained\nas functions of parameters of the initial systems. Expressions also found for\nthe entropies of initial and united system. The process of unification is\naccompanied by the transfer of particles (money) between the systems and\nexplicit expression is obtained for the transferred number of particles (size\nof the capital). A relation between parameters of the initial systems also\nfound which defines the regime with zero particle transfer.\n"
    },
    {
        "paper_id": 1510.00941,
        "authors": "Matthew Ginley",
        "title": "Shortfall from Maximum Convexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review the dynamics of the returns of Leveraged Exchange Traded Funds\n(LETFs) and propose a new measure of realized volatility: Shortfall from\nMaximum Convexity. We show that SMC has a more intuitive interpretation and\nprovides more statistical information compared to the traditionally used sample\nstandard deviation when applied to LETF returns, a dataset where normality and\nindependence do not hold.\n"
    },
    {
        "paper_id": 1510.01172,
        "authors": "Wei Lin, Shenghong Li, Xingguo Luo, and Shane Chern",
        "title": "Consistent Pricing of VIX and Equity Derivatives with the 4/2 Stochastic\n  Volatility Plus Jumps Model",
        "comments": "18 pages. Submitted. arXiv admin note: text overlap with\n  arXiv:1203.5903 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a 4/2 stochastic volatility plus jumps model,\nnamely, a new stochastic volatility model including the Heston model and 3/2\nmodel as special cases. Our model is highly tractable by applying the Lie\nsymmetries theory for PDEs, which means that the pricing procedure can be\nperformed efficiently. In fact, we obtain a closed-form solution for the joint\nFourier-Laplace transform so that equity and realized-variance derivatives can\nbe priced. We also employ our model to consistently price equity and VIX\nderivatives. In this process, the quasi-closed-form solutions for future and\noption prices are derived. Furthermore, through adopting data on daily VIX\nfuture and option prices, we investigate our model along with the Heston model\nand 3/2 model and compare their different performance in practice. Our result\nillustrates that the 4/2 model with an instantaneous volatility of the form\n$(a\\sqrt{V_t}+b/\\sqrt{V_t})$ for some constants $a, b$ presents considerable\nadvantages in pricing VIX derivatives.\n"
    },
    {
        "paper_id": 1510.0121,
        "authors": "Tam\\'as Fleiner, Zsuzsanna Jank\\'o, Akihisa Tamura, Alexander\n  Teytelboym",
        "title": "Trading Networks with Bilateral Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model of matching in trading networks in which firms can enter\ninto bilateral contracts. In trading networks, stable outcomes, which are\nimmune to deviations of arbitrary sets of firms, may not exist. We define a new\nsolution concept called trail stability. Trail-stable outcomes are immune to\nconsecutive, pairwise deviations between linked firms. We show that any trading\nnetwork with bilateral contracts has a trail-stable outcome whenever firms'\nchoice functions satisfy the full substitutability condition. For trail-stable\noutcomes, we prove results on the lattice structure, the rural hospitals\ntheorem, strategy-proofness, and comparative statics of firm entry and exit. We\nalso introduce weak trail stability which is implied by trail stability under\nfull substitutability. We describe relationships between the solution concepts.\n"
    },
    {
        "paper_id": 1510.01593,
        "authors": "Halis Sak and \\.Ismail Ba\\c{s}o\\u{g}lu",
        "title": "Efficient Randomized Quasi-Monte Carlo Methods For Portfolio Market Risk",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2017.07.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of simulating loss probabilities and conditional\nexcesses for linear asset portfolios under the t-copula model. Although in the\nliterature on market risk management there are papers proposing efficient\nvariance reduction methods for Monte Carlo simulation of portfolio market risk,\nthere is no paper discussing combining the randomized quasi-Monte Carlo method\nwith variance reduction techniques. In this paper, we combine the randomized\nquasi-Monte Carlo method with importance sampling and stratified importance\nsampling. Numerical results for realistic portfolio examples suggest that\nreplacing pseudorandom numbers (Monte Carlo) with quasi-random sequences\n(quasi-Monte Carlo) in the simulations increases the robustness of the\nestimates once we reduce the effective dimension and the non-smoothness of the\nintegrands.\n"
    },
    {
        "paper_id": 1510.01675,
        "authors": "Thomas Kruse, Judith C. Schneider, Nikolaus Schweizer",
        "title": "What's in a ball? Constructing and characterizing uncertainty sets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the presence of model risk, it is well-established to replace classical\nexpected values by worst-case expectations over all models within a fixed\nradius from a given reference model. This is the \"robustness\" approach. We show\nthat previous methods for measuring this radius, e.g. relative entropy or\npolynomial divergences, are inadequate for reference models which are\nmoderately heavy-tailed such as lognormal models. Worst cases are either\ninfinitely pessimistic, or they rule out the possibility of fat-tailed \"power\nlaw\" models as plausible alternatives. We introduce a new family of divergence\nmeasures which captures intermediate levels of pessimism.\n"
    },
    {
        "paper_id": 1510.01679,
        "authors": "S. Ciliberti, Y. Lemp\\'eri\\`ere, A. Beveratos, G. Simon, L. Laloux, M.\n  Potters, J. P. Bouchaud",
        "title": "Deconstructing the Low-Vol Anomaly",
        "comments": "21 pages, 7 figures, 7 tables -- 1 figure added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study several aspects of the so-called low-vol and low-beta anomalies,\nsome already documented (such as the universality of the effect over different\ngeographical zones), others hitherto not clearly discussed in the literature.\nOur most significant message is that the low-vol anomaly is the result of two\nindependent effects. One is the striking negative correlation between past\nrealized volatility and dividend yield. Second is the fact that ex-dividend\nreturns themselves are weakly dependent on the volatility level, leading to\nbetter risk-adjusted returns for low-vol stocks. This effect is further\namplified by compounding. We find that the low-vol strategy is not associated\nto short term reversals, nor does it qualify as a Risk-Premium strategy, since\nits overall skewness is slightly positive. For practical purposes, the strong\ndividend bias and the resulting correlation with other valuation metrics (such\nas Earnings to Price or Book to Price) does make the low-vol strategies to some\nextent redundant, at least for equities.\n"
    },
    {
        "paper_id": 1510.01848,
        "authors": "Sergii Kuchuk-Iatsenko, Yuliya Mishura",
        "title": "Pricing the European call option in the model with stochastic volatility\n  driven by Ornstein--Uhlenbeck process. Exact formulas",
        "comments": "Published at http://dx.doi.org/10.15559/15-VMSTA36CNF in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2015, Vol. 2, No. 3,\n  233-249",
        "doi": "10.15559/15-VMSTA36CNF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the Black--Scholes model of financial market modified to capture\nthe stochastic nature of volatility observed at real financial markets. For\nvolatility driven by the Ornstein--Uhlenbeck process, we establish the\nexistence of equivalent martingale measure in the market model. The option is\npriced with respect to the minimal martingale measure for the case of\nuncorrelated processes of volatility and asset price, and an analytic\nexpression for the price of European call option is derived. We use the inverse\nFourier transform of a characteristic function and the Gaussian property of the\nOrnstein--Uhlenbeck process.\n"
    },
    {
        "paper_id": 1510.0189,
        "authors": "Beatrice Acciaio, Martin Larsson",
        "title": "Semi-static completeness and robust pricing by informed investors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a continuous-time financial market that consists of securities\navailable for dynamic trading, and securities only available for static\ntrading. We work in a robust framework where a set of non-dominated models is\ngiven. The concept of semi-static completeness is introduced: it corresponds to\nhaving exact replication by means of semi-static strategies. We show that\nsemi-static completeness is equivalent to an extremality property, and give a\ncharacterization of the induced filtration structure. Furthermore, we consider\ninvestors with additional information and, for specific types of extra\ninformation, we characterize the models that are semi-statically complete for\nthe informed investors. Finally, we provide some examples where robust pricing\nfor informed and uninformed agents can be done over semi-statically complete\nmodels.\n"
    },
    {
        "paper_id": 1510.0201,
        "authors": "Scott Robertson, Zhe Cheng",
        "title": "Endogenous Current Coupons",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of identifying current coupons for Agency backed\nTo-be-Announced (TBA) Mortgage Backed Securities. In a doubly stochastic factor\nbased model which allows for prepayment intensities to depend upon current and\norigination mortgage rates, as well as underlying investment factors, we\nidentify the current coupon with solutions to a degenerate elliptic, non-linear\nfixed point problem. Using Schaefer's theorem we prove existence of current\ncoupons. We also provide an explicit approximation to the fixed point, valid\nfor compact perturbations off a baseline factor-based intensity model.\nNumerical examples are provided which show the approximation performs\nremarkably well in estimating the current coupon.\n"
    },
    {
        "paper_id": 1510.02013,
        "authors": "Yusuke Morimoto and Makiko Sasada",
        "title": "Algebraic Structure of Vector Fields in Financial Diffusion Models and\n  its Applications",
        "comments": "21 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1080/14697688.2016.1264618",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High order discretization schemes of SDEs by using free Lie algebra valued\nrandom variables are introduced by Kusuoka, Lyons-Victoir, Ninomiya-Victoir and\nNinomiya-Ninomiya. These schemes are called KLNV methods. They involve solving\nthe flows of vector fields associated with SDEs and it is usually done by\nnumerical methods. The authors found a special Lie algebraic structure on the\nvector fields in the major financial diffusion models. Using this structure, we\ncan solve the flows associated with vector fields analytically and efficiently.\nNumerical examples show that our method saves the computation time drastically.\n"
    },
    {
        "paper_id": 1510.02292,
        "authors": "Robert Fernholz",
        "title": "An example of short-term relative arbitrage",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long-term relative arbitrage exists in markets where the excess growth rate\nof the market portfolio is bounded away from zero. Here it is shown that under\na time-homogeneity hypothesis this condition will also imply the existence of\nrelative arbitrage over arbitrarily short intervals.\n"
    },
    {
        "paper_id": 1510.02435,
        "authors": "Jason Smith",
        "title": "Information equilibrium as an economic principle",
        "comments": "44 pages, 26 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A general information equilibrium model in the case of ideal information\ntransfer is defined and then used to derive the relationship between supply\n(information destination) and demand (information source) with the price as the\ndetector of information exchange between demand and supply. We recover the\nproperties of the traditional economic supply-demand diagram. Information\nequilibrium is then applied to macroeconomic problems, recovering some common\nmacroeconomic models in particular limits like the AD-AS model, IS-LM model (in\na low inflation limit), the quantity theory of money (in a high inflation\nlimit) and the Solow-Swan growth model. Information equilibrium results in\nempirically accurate models of inflation and interest rates, and can be used to\nmotivate a 'statistical economics', analogous to statistical mechanics for\nthermodynamics.\n"
    },
    {
        "paper_id": 1510.02752,
        "authors": "Ivan Kitov, Oleg Kitov",
        "title": "Gender income disparity in the USA: analysis and dynamic modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze and develop a quantitative model describing the evolution of\npersonal income distribution, PID, for males and females in the U.S. between\n1930 and 2014. The overall microeconomic model, which we introduced ten years\nago, accurately predicts the change in mean income as a function of age as well\nas the dependence on age of the portion of people distributed according to the\nPareto law. As a result, we have precisely described the change in Gini ratio\nsince the start of income measurements in 1947. The overall population consists\nof two genders, however, which have different income distributions. The\ndifference between incomes earned by male and female population has been\nexperiencing dramatic changes over time. Here, we model the internal dynamics\nof men and women PIDs separately and then describe their relative contribution\nto the overall PID. Our original model is refined to match all principal\ngender-dependent observations. We found that women in the U.S. are deprived of\nhigher job positions. This is the cause of the long term income inequality\nbetween males and females in the U.S. It is unjust to women and has a negative\neffect on real economic growth. Women have been catching up since the 1960s and\nthat improves the performance of the U.S. economy. It will take decades,\nhowever, to full income equality between genders. There are no new defining\nparameters included in the model except the critical age, when people start to\nlose their incomes, was split into two critical ages for low-middle incomes and\nthe highest incomes, which obey a power law distribution. Such an extension\nbecomes necessary in order to match the observation that the female population\nin the earlier 1960s was practically not represented in the highest incomes.\n"
    },
    {
        "paper_id": 1510.02754,
        "authors": "Ivan Kitov, Oleg Kitov",
        "title": "How universal is the law of income distribution? Cross country\n  comparison",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The evolution of personal income distribution (PID) in four countries:\nCanada, New Zealand, the UK, and the USA follows a unique trajectory. We have\nrevealed precise match in the shape of two age-dependent features of the PID:\nmean income and the portion of people with the highest incomes (2 to 5% of the\nworking age population). Because of the U.S. economic superiority, as expressed\nby real GDP per head, the curves of mean income and the portion of rich people\ncurrently observed in three chasing countries one-to-one reproduce the curves\nmeasured in the USA 15 to 25 years before. This result of cross country\ncomparison implies that the driving force behind the PID evolution is the same\nin four studied countries. Our parsimonious microeconomic model, which links\nthe change in PID only with one exogenous parameter - real GDP per capita,\naccurately predicts all studied features for the U.S. This study proves that\nour quantitative model, based on one first-order differential equation, is\nuniversal. For example, new observations in Canada, New Zealand, and the UK\nconfirm our previous finding that the age of maximum mean income is defined by\nthe root-square dependence on real GDP per capita.\n"
    },
    {
        "paper_id": 1510.02768,
        "authors": "Mauricio Contreras, Alejandro Llanquihu\\'en and Marcelo Villena",
        "title": "On the Solution of the Multi-asset Black-Scholes model: Correlations,\n  Eigenvalues and Geometry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the multi-asset Black-Scholes model in terms of the\nimportance that the correlation parameter space (equivalent to an $N$\ndimensional hypercube) has in the solution of the pricing problem. We show that\ninside of this hypercube there is a surface, called the Kummer surface\n$\\Sigma_K$, where the determinant of the correlation matrix $\\rho$ is zero, so\nthe usual formula for the propagator of the $N$ asset Black-Scholes equation is\nno longer valid. Worse than that, in some regions outside this surface, the\ndeterminant of $\\rho$ becomes negative, so the usual propagator becomes complex\nand divergent. Thus the option pricing model is not well defined for these\nregions outside $\\Sigma_K$. On the Kummer surface instead, the rank of the\n$\\rho$ matrix is a variable number. By using the Wei-Norman theorem, we compute\nthe propagator over the variable rank surface $\\Sigma_K$ for the general $N$\nasset case. We also study in detail the three assets case and its implied\ngeometry along the Kummer surface.\n"
    },
    {
        "paper_id": 1510.02808,
        "authors": "Ting-Kam Leonard Wong",
        "title": "Universal portfolios in stochastic portfolio theory",
        "comments": "25 pages; revised",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider a family of portfolio strategies with the aim of achieving the\nasymptotic growth rate of the best one. The idea behind Cover's universal\nportfolio is to build a wealth-weighted average which can be viewed as a\nbuy-and-hold portfolio of portfolios. When an optimal portfolio exists, the\nwealth-weighted average converges to it by concentration of wealth. Working\nunder a discrete time and pathwise setup, we show under suitable conditions\nthat the distribution of wealth in the family satisfies a pathwise large\ndeviation principle as time tends to infinity. Our main result extends Cover's\nportfolio to the nonparametric family of functionally generated portfolios in\nstochastic portfolio theory and establishes its asymptotic universality.\n"
    },
    {
        "paper_id": 1510.0304,
        "authors": "Z. Koohi Lai, S. Vasheghani Farahani, S.M.S. Movahed and G.R. Jafari",
        "title": "Coupled uncertainty provided by a multifractal random walker",
        "comments": "9 pages and 4 figures",
        "journal-ref": "Physics Letters A 379 (2015) 2284-2290",
        "doi": "10.1016/j.physleta.2015.07.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim here is to study the concept of pairing multifractality between time\nseries possessing non-Gaussian distributions. The increasing number of rare\nevents creates \"criticality\". We show how the pairing between two series is\naffected by rare events, which we call \"coupled criticality\". A method is\nproposed for studying the coupled criticality born out of the interaction\nbetween two series, using the bivariate multifractal random walk (BiMRW). This\nmethod allows studying dependence of the coupled criticality on the criticality\nof each individual system. This approach is applied to data sets of gold and\noil markets, and inflation and unemployment.\n"
    },
    {
        "paper_id": 1510.03079,
        "authors": "Mourad Lazgham",
        "title": "Regularity properties in a state-constrained expected utility\n  maximization problem",
        "comments": "29 pages, supported by Deutsche Forschungsgemeinschaft through Grant\n  SCHI 500/3-1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic optimal control problem in a market model with\ntemporary and permanent price impact, which is related to an expected utility\nmaximization problem under finite fuel constraint. We establish the initial\ncondition fulfilled by the corresponding value function and show its first\nregularity property. Moreover, we can prove the existence and uniqueness of\noptimal strategies under rather mild model assumptions. On the one hand, this\nresult is of independent interest. On the other hand, it will then allow us to\nderive further regularity properties of the corresponding value function, in\nparticular its continuity and partial differentiability. As a consequence of\nthe continuity of the value function, we will prove the dynamic programming\nprinciple without appealing to the classical measurable selection arguments.\n"
    },
    {
        "paper_id": 1510.03205,
        "authors": "Shanshan Wang, Rudi Sch\\\"afer, and Thomas Guhr",
        "title": "Price response in correlated financial markets: empirical results",
        "comments": "improved analysis and new versions in arXiv:1603.01580 and\n  arXiv:1603.01586",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous studies of the stock price response to individual trades focused on\nsingle stocks. We empirically investigate the price response of one stock to\nthe trades of other stocks. How large is the impact of one stock on others and\nvice versa? -- This impact of trades on the price change across stocks appears\nto be transient instead of permanent. Performing different averages, we\ndistinguish active and passive responses. The two average responses show\ndifferent characteristic dependences on the time lag. The passive response\nexhibits a shorter response period with sizeable volatilities, and the active\nresponse a longer period. We also study the response for a given stock with\nrespect to different sectors and to the whole market. Furthermore, we compare\nthe self-response with the various cross-responses. The correlation of the\ntrade signs is a short-memory process for a pair of stocks, but it turns into a\nlong-memory process when averaged over different pairs of stocks.\n"
    },
    {
        "paper_id": 1510.0322,
        "authors": "Masaaki Fujii and Akihiko Takahashi",
        "title": "Asymptotic Expansion for Forward-Backward SDEs with Jumps",
        "comments": "Forthcoming in Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work provides a semi-analytic approximation method for decoupled\nforwardbackward SDEs (FBSDEs) with jumps. In particular, we construct an\nasymptotic expansion method for FBSDEs driven by the random Poisson measures\nwith {\\sigma}-finite compensators as well as the standard Brownian motions\naround the small-variance limit of the forward SDE. We provide a semi-analytic\nsolution technique as well as its error estimate for which we only need to\nsolve essentially a system of linear ODEs. In the case of a finite jump measure\nwith a bounded intensity, the method can also handle state-dependent and hence\nnon-Poissonian jumps, which are quite relevant for many practical applications.\n"
    },
    {
        "paper_id": 1510.03223,
        "authors": "Peter Bank, Mete Soner, Moritz Vo{\\ss}",
        "title": "Hedging with Temporary Price Impact",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of hedging a European contingent claim in a Bachelier\nmodel with transient price impact as proposed by Almgren and Chriss. Following\nthe approach of Rogers and Singh and Naujokat and Westray, the hedging problem\ncan be regarded as a cost optimal tracking problem of the frictionless hedging\nstrategy. We solve this problem explicitly for general predictable target\nhedging strategies. It turns out that, rather than towards the current target\nposition, the optimal policy trades towards a weighted average of expected\nfuture target positions. This generalizes an observation of Garleanu and\nPedersen from their homogenous Markovian optimal investment problem to a\ngeneral hedging problem. Our findings complement a number of previous studies\nin the literature on optimal strategies in illiquid markets where the\nfrictionless strategy is confined to diffusions. The consideration of general\npredictable reference strategies is made possible by the use of a convex\nanalysis approach instead of the more common dynamic programming methods.\n"
    },
    {
        "paper_id": 1510.03385,
        "authors": "David Puelz, Carlos M. Carvalho, P. Richard Hahn",
        "title": "Optimal ETF Selection for Passive Investing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the problem of isolating a small number of exchange\ntraded funds (ETFs) that suffice to capture the fundamental dimensions of\nvariation in U.S. financial markets. First, the data is fit to a vector-valued\nBayesian regression model, which is a matrix-variate generalization of the well\nknown stochastic search variable selection (SSVS) of George and McCulloch\n(1993). ETF selection is then performed using the decoupled shrinkage and\nselection (DSS) procedure described in Hahn and Carvalho (2015), adapted in two\nways: to the vector-response setting and to incorporate stochastic covariates.\nThe selected set of ETFs is obtained under a number of different penalty and\nmodeling choices. Optimal portfolios are constructed from selected ETFs by\nmaximizing the Sharpe ratio posterior mean, and they are compared to the\n(unknown) optimal portfolio based on the full Bayesian model. We compare our\nselection results to popular ETF advisor Wealthfront.com. Additionally, we\nconsider selecting ETFs by modeling a large set of mutual funds.\n"
    },
    {
        "paper_id": 1510.03398,
        "authors": "Nazaria Solferino, Viviana Solferino",
        "title": "The Corporate Social Responsibility is just a twist in a M\\\"obius Strip",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years economics agents and systems have became more and more\ninteracting and juxtaposed, therefore the social sciences need to rely on the\nstudies of physical sciences to analyze this complexity in the relationships.\nAccording to this point of view we rely on the geometrical model of the\nM\\\"obius strip used in the electromagnetism which analyzes the moves of the\nelectrons that produce energy. We use a similar model in a Corporate Social\nResponsibility context to devise a new cost function in order to take into\naccount of three positive crossed effects on the efficiency: i)cooperation\namong stakeholders in the same sector, ii)cooperation among similar\nstakeholders in different sectors and iii)the stakeholders' loyalty towards the\ncompany. By applying this new cost function to a firm's decisional problem we\nfind that investing in Corporate Social Responsibility activities is ever\nconvenient depending on the number of sectors, the stakeholders' sensitivity to\nthese investments and the decay rate to alienation. Our work suggests a new\nmethod of analysis which should be developed not only at a theoretical but also\nat an empirical level.\n"
    },
    {
        "paper_id": 1510.0355,
        "authors": "J. B. Heaton, N. G. Polson, J. H. Witte",
        "title": "Why Indexing Works",
        "comments": "5 Pages, 1 Figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a simple stock selection model to explain why active equity\nmanagers tend to underperform a benchmark index. We motivate our model with the\nempirical observation that the best performing stocks in a broad market index\noften perform much better than the other stocks in the index. Randomly\nselecting a subset of securities from the index may dramatically increase the\nchance of underperforming the index. The relative likelihood of\nunderperformance by investors choosing active management likely is much more\nimportant than the loss to those same investors from the higher fees for active\nmanagement relative to passive index investing. Thus, active management may be\neven more challenging than previously believed, and the stakes for finding the\nbest active managers may be larger than previously assumed.\n"
    },
    {
        "paper_id": 1510.03584,
        "authors": "Mourad Lazgham",
        "title": "Viscosity properties with singularities in a state-constrained expected\n  utility maximization problem",
        "comments": "23 pages, supported by Deutsche Forschungsgemeinschaft through Grant\n  SCHI 500/3-1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the value function originating from an expected utility\nmaximization problem with finite fuel constraint and show its close relation to\na nonlinear parabolic degenerated Hamilton-Jacobi-Bellman (HJB) equation with\nsingularity. On one hand, we give a so-called verification argument based on\nthe dynamic programming principle, which allows us to derive conditions under\nwhich a classical solution of the HJB equation coincides with our value\nfunction (provided that it is smooth enough). On the other hand, we establish a\ncomparison principle, which allows us to characterize our value function as the\nunique viscosity solution of the HJB equation.\n"
    },
    {
        "paper_id": 1510.0359,
        "authors": "Ahmed Kebaier (LAGA), J\\'er\\^ome Lelong (DAO, MATHRISK)",
        "title": "Coupling Importance Sampling and Multilevel Monte Carlo using Sample\n  Average Approximation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose a smart idea to couple importance sampling and\nMultilevel Monte Carlo (MLMC). We advocate a per level approach with as many\nimportance sampling parameters as the number of levels, which enables us to\ncompute the different levels independently. The search for parameters is\ncarried out using sample average approximation, which basically consists in\napplying deterministic optimisation techniques to a Monte Carlo approximation\nrather than resorting to stochastic approximation. Our innovative estimator\nleads to a robust and efficient procedure reducing both the discretization\nerror (the bias) and the variance for a given computational effort. In the\nsetting of discretized diffusions, we prove that our estimator satisfies a\nstrong law of large numbers and a central limit theorem with optimal limiting\nvariance, in the sense that this is the variance achieved by the best\nimportance sampling measure (among the class of changes we consider), which is\nhowever non tractable. Finally, we illustrate the efficiency of our method on\nseveral numerical challenges coming from quantitative finance and show that it\noutperforms the standard MLMC estimator.\n"
    },
    {
        "paper_id": 1510.03596,
        "authors": "Ahmed Bel Hadj Ayed, Gr\\'egoire Loeper, Sofiene El Aoud, Fr\\'ed\\'eric\n  Abergel",
        "title": "Performance analysis of the optimal strategy under partial information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The question addressed in this paper is the performance of the optimal\nstrategy, and the impact of partial information. The setting we consider is\nthat of a stochastic asset price model where the trend follows an unobservable\nOrnstein-Uhlenbeck process. We focus on the optimal strategy with a logarithmic\nutility function under full or partial information. For both cases, we provide\nthe asymptotic expectation and variance of the logarithmic return as functions\nof the signal-to-noise ratio and of the trend mean reversion speed. Finally, we\ncompare the asymptotic Sharpe ratios of these strategies in order to quantify\nthe loss of performance due to partial information.\n"
    },
    {
        "paper_id": 1510.03704,
        "authors": "Achal Awasthi and Oleg Malafeyev",
        "title": "Is the Indian Stock Market efficient - A comprehensive study of Bombay\n  Stock Exchange Indices",
        "comments": "8 pages, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How an investor invests in the market is largely influenced by the market\nefficiency because if a market is efficient, it is extremely difficult to make\nexcessive returns because in an efficient market there will be no undervalued\nsecurities i.e. securities whose value is less than its assumed intrinsic\nvalue, which offer returns that are higher than the deserved expected returns,\ngiven their risk. However, there is a possibility of making excessive returns\nif the market is not efficient. This article analyses the five popular stock\nindices of BSE. This would not only test the efficiency of the Indian Stock\nMarket but also test the random walk nature of the stock market. The study\nundertaken in this paper has provided strong evidence in favor of the\ninefficient form of the Indian Stock Market. The series of stock indices in the\nIndian Stock Market are found to be biased random time series and the random\nwalk model can't be applied in the Indian Stock Market. This study confirms\nthat there is a drift in market efficiency and investors can capitalize on this\nby correctly choosing the securities that are undervalued.\n"
    },
    {
        "paper_id": 1510.0392,
        "authors": "Lingjiong Zhu",
        "title": "A State-Dependent Dual Risk Model",
        "comments": "20 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a dual risk model, the premiums are considered as the costs and the claims\nare regarded as the profits. The surplus can be interpreted as the wealth of a\nventure capital, whose profits depend on research and development. In most of\nthe existing literature of dual risk models, the profits follow the compound\nPoisson model and the cost is constant. In this paper, we develop a\nstate-dependent dual risk model, in which the arrival rate of the profits and\nthe costs depend on the current state of the wealth process. Ruin probabilities\nare obtained in closed-forms. Further properties and results will also be\ndiscussed.\n"
    },
    {
        "paper_id": 1510.03926,
        "authors": "Roberto Ortiz, Mauricio Contreras and Marcelo Villena",
        "title": "On the Efficient Market Hypothesis of Stock Market Indexes: The Role of\n  Non-synchronous Trading and Portfolio Effects",
        "comments": "25 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, the long-term behavior of the stock market index of the New\nYork Stock Exchange is studied, for the period 1950 to 2013. Specifically, the\nCRSP Value-Weighted and CRSP Equal-Weighted index are analyzed in terms of\nmarket efficiency, using the standard ratio variance test, considering over\n1600 one week rolling windows. For the equally weighted index, the null\nhypothesis of random walk is rejected in the whole period, while for the\nweighted market value index, the null hypothesis start to be accepted from the\n1990s. In order to explain this difference, we raised the hypothesis that this\nbehavior can be explained by the joint action of portfolios and non-synchronous\ntrading effects. To check the feasibility of the above assumption, we performed\na simulation of both effects, on two- and six-asset portfolios. The results\nshowed that it is possible to explain the empirical difference between the two\nindex, almost entirely by the joint effects of portfolio and non-synchronous\ntrading.\n"
    },
    {
        "paper_id": 1510.03928,
        "authors": "Parsiad Azimzadeh, Peter A. Forsyth",
        "title": "Weakly chained matrices, policy iteration, and impulse control",
        "comments": "30 pages, 4 figures; error in proof of Theorem 3.5 fixed as of v3;\n  error in statement of Lemma 4.6 fixed as of v4",
        "journal-ref": "SIAM.J.Numer.Anal. 54.3 (2016) 1341-1364",
        "doi": "10.1137/15M1043431",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is motivated by numerical solutions to Hamilton-Jacobi-Bellman\nquasi-variational inequalities (HJBQVIs) associated with combined stochastic\nand impulse control problems. In particular, we consider (i) direct control,\n(ii) penalized, and (iii) semi-Lagrangian discretization schemes applied to the\nHJBQVI problem. Scheme (i) takes the form of a Bellman problem involving an\noperator which is not necessarily contractive. We consider the well-posedness\nof the Bellman problem and give sufficient conditions for convergence of the\ncorresponding policy iteration. To do so, we use weakly chained diagonally\ndominant matrices, which give a graph-theoretic characterization of weakly\ndiagonally dominant M-matrices. We compare schemes (i)--(iii) under the\nfollowing examples: (a) optimal control of the exchange rate, (b) optimal\nconsumption with fixed and proportional transaction costs, and (c) pricing\nguaranteed minimum withdrawal benefits in variable annuities. We find that one\nshould abstain from using scheme (i).\n"
    },
    {
        "paper_id": 1510.04061,
        "authors": "Philipp Harms and David Stefanovits",
        "title": "Affine representations of fractional processes with applications in\n  mathematical finance",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fractional processes have gained popularity in financial modeling due to the\ndependence structure of their increments and the roughness of their sample\npaths. The non-Markovianity of these processes gives, however, rise to\nconceptual and practical difficulties in computation and calibration. To\naddress these issues, we show that a certain class of fractional processes can\nbe represented as linear functionals of an infinite dimensional affine process.\nThis can be derived from integral representations similar to those of Carmona,\nCoutin, Montseny, and Muravlev. We demonstrate by means of several examples\nthat this allows one to construct tractable financial models with fractional\nfeatures.\n"
    },
    {
        "paper_id": 1510.04295,
        "authors": "Jiatu Cai, Mathieu Rosenbaum and Peter Tankov",
        "title": "Asymptotic Lower Bounds for Optimal Tracking: a Linear Programming\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of tracking a target whose dynamics is modeled by a\ncontinuous It\\=o semi-martingale. The aim is to minimize both deviation from\nthe target and tracking efforts. We establish the existence of asymptotic lower\nbounds for this problem, depending on the cost structure. These lower bounds\ncan be related to the time-average control of Brownian motion, which is\ncharacterized as a deterministic linear programming problem. A comprehensive\nlist of examples with explicit expressions for the lower bounds is provided.\n"
    },
    {
        "paper_id": 1510.04346,
        "authors": "Xiongzhi Chen",
        "title": "Explicit solutions to a vector time series model and its induced model\n  for business cycles",
        "comments": "16 page2, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This article gives the explicit solution to a general vector time series\nmodel that describes interacting, heterogeneous agents that operate under\nuncertainties but according to Keynesian principles, from which a model for\nbusiness cycle is induced by a weighted average of the growth rates of the\nagents in the model. The explicit solution enables a direct simulation of the\ntime series defined by the model and better understanding of the joint behavior\nof the growth rates. In addition, the induced model for business cycles and its\nsolutions are explicitly given and analyzed. The explicit solutions provide a\nbetter understanding of the mathematics of these models and the econometric\nproperties they try to incorporate.\n"
    },
    {
        "paper_id": 1510.0437,
        "authors": "Wujiang Lou",
        "title": "Extending the Black-Scholes Option Pricing Theory to Account for an\n  Option Market Maker's Funding Costs",
        "comments": "13 pages, 5 figures, 2 tables, presented at 2nd WBS US Fixed Income\n  Conference, 2014 New York. in Lou, Wujiang, Funding in option pricing: the\n  Black-Scholes Framework Extended, Risk, April 2015,pp 56-61",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An option market maker incurs funding costs when carrying and hedging\ninventory. To hedge a net long delta inventory, for example, she pays a fee to\nborrow stock from the securities lending market. Because of haircuts, she posts\nadditional cash margin to the lender which needs to be financed at her\nunsecured debt rate. This paper incorporates funding asymmetry (borrowed cash\nand invested cash earning different interest rates) and realistic stock\nfinancing cost into the classic option pricing theory. It is shown that an\noption position can be dynamically replicated and self financed in the presence\nof these funding costs. Noting that the funding amounts and costs are different\nfor long and short positions, we extend Black-Scholes partial differential\nequations (PDE) per position side. The PDE's nonlinear funding cost terms\ncreate a free funding boundary and would result in the bid price for a long\nposition on an option lower than the ask price for a short position. An\niterative Crank-Nicholson finite difference method is developed to compute\nEuropean and American vanilla option prices. Numerical results show that\nreasonable funding cost parameters can easily produce same magnitude of bid/ask\nspread of less liquid, longer term options as observed in the market place.\nPortfolio level pricing examples show the netting effect of hedges, which could\nmoderate impact of funding costs.\n"
    },
    {
        "paper_id": 1510.0455,
        "authors": "Marcelo J. Villena and Axel A. Araneda",
        "title": "Dynamics and Stability in Retail Competition",
        "comments": "18 pages, 4 figures",
        "journal-ref": "Mathematics and computers in simulation 2017",
        "doi": "10.1016/j.matcom.2016.09.011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Retail competition today can be described by three main features: i)\noligopolistic competition, ii) multi-store settings, and iii) the presence of\nlarge economies of scale. In these markets, firms usually apply a centralized\ndecisions making process in order to take full advantage of economies of\nscales, e.g. retail distribution centers. In this paper, we model and analyze\nthe stability and chaos of retail competition considering all these issues. In\nparticular, a dynamic multi-market Cournot-Nash equilibrium with global\neconomies and diseconomies of scale model is developed. We confirm the\nnon-intuitive hypothesis that retail multi-store competition is more unstable\nthat traditional small business that cover the same demand. The main sources of\nstability are the scale parameter and the number of markets\n"
    },
    {
        "paper_id": 1510.04588,
        "authors": "Yusuke Morimoto",
        "title": "Application of Stochastic Mesh Method to Efficient Approximation of CVA",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the author considers the numerical computation of CVA for\nlarge systems by Mote Carlo methods. He introduces two types of stochastic mesh\nmethods for the computations of CVA. In the first method, stochastic mesh\nmethod is used to obtain the future value of the derivative contracts. In the\nsecond method, stochastic mesh method is used only to judge whether future\nvalue of the derivative contracts is positive or not. He discusses the rate of\nconvergence to the real CVA value of these methods.\n"
    },
    {
        "paper_id": 1510.0469,
        "authors": "Hokky Situngkir",
        "title": "On Capturing the Spreading Dynamics over Trading Prices in the Market",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While market is a social field where information flows over the interacting\nagents, there have been not so many methods to observe the spreading\ninformation in the prices comprising the market. By incorporating the entropy\ntransfer in information theory in its relation to the Granger causality, the\npaper proposes a tree of weighted directed graph of market to detect the\nchanges of price might affect other price changes. We compare the proposed\nanalysis with the similar tree representation built from the correlation\ncoefficients of stock prices in order to have insight of possibility in seeing\nthe collective behavior of the market in general.\n"
    },
    {
        "paper_id": 1510.04841,
        "authors": "Nassim Nicholas Taleb",
        "title": "How to (Not) Estimate Gini Coefficients for Fat Tailed Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Direct measurements of Gini coefficients by conventional arithmetic\ncalculations are a poor estimator, even if paradoxically, they include the\nentire population, as because of super-additivity they cannot lend themselves\nto comparisons between units of different size, and intertemporal analyses are\nvitiated by the population changes. The Gini of aggregated units A and B will\nbe higher than those of A and B computed separately. This effect becomes more\nacute with fatness of tails. When the sample size is smaller than entire\npopulation, the error is extremely high. The conventional literature on Gini\ncoefficients cannot be trusted and comparing countries of different sizes makes\nno sense; nor does it make sense to make claims of \"changes in inequality\"\nbased on conventional measures. We compare the standard methodologies to the\nindirect methods via maximum likelihood estimation of tail exponent. We compare\nto the tail method which is unbiased, with considerably lower error rate. We\nalso consider measurement errors of the tail exponent and suggest a simple but\nefficient methodology to calculate Gini coefficients.\n"
    },
    {
        "paper_id": 1510.04899,
        "authors": "Andrey Itkin",
        "title": "Nonlinear PDEs risen when solving some optimization problems in finance,\n  and their solutions",
        "comments": "20 pages, 3 figs",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a specific type of nonlinear partial differential equations (PDE)\nthat appear in mathematical finance as the result of solving some optimization\nproblems. We review some existing in the literature examples of such problems,\nand discuss the properties of these PDEs. We also demonstrate how to solve them\nnumerically in a general case, and analytically in some particular case.\n"
    },
    {
        "paper_id": 1510.0491,
        "authors": "Rafal Rak, Stanislaw Drozdz, Jaroslaw Kwapien, Pawel Oswiecimka",
        "title": "Detrended cross-correlations between returns, volatility, trading\n  activity, and volume traded for the stock market companies",
        "comments": null,
        "journal-ref": "EPL 112, 48001 (2015)",
        "doi": "10.1209/0295-5075/112/48001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a few quantities that characterize trading on a stock market in a\nfixed time interval: logarithmic returns, volatility, trading activity (i.e.,\nthe number of transactions), and volume traded. We search for the power-law\ncross-correlations among these quantities aggregated over different time units\nfrom 1 min to 10 min. Our study is based on empirical data from the American\nstock market consisting of tick-by-tick recordings of 31 stocks listed in Dow\nJones Industrial Average during the years 2008-2011. Since all the considered\nquantities except the returns show strong daily patterns related to the\nvariable trading activity in different parts of a day, which are the best\nevident in the autocorrelation function, we remove these patterns by detrending\nbefore we proceed further with our study. We apply the multifractal detrended\ncross-correlation analysis with sign preserving (MFCCA) and show that the\nstrongest power-law cross-correlations exist between trading activity and\nvolume traded, while the weakest ones exist (or even do not exist) between the\nreturns and the remaining quantities. We also show that the strongest\ncross-correlations are carried by those parts of the signals that are\ncharacterized by large and medium variance. Our observation that the most\nconvincing power-law cross-correlations occur between trading activity and\nvolume traded reveals the existence of strong fractal-like coupling between\nthese quantities.\n"
    },
    {
        "paper_id": 1510.04924,
        "authors": "Arash Fahim, Lingjiong Zhu",
        "title": "Optimal Investment in a Dual Risk Model",
        "comments": "37 pages, 5 figures",
        "journal-ref": "Risks 11(2), 41, 2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dual risk models are popular for modeling a venture capital or high tech\ncompany, for which the running cost is deterministic and the profits arrive\nstochastically over time. Most of the existing literature on dual risk models\nconcentrated on the optimal dividend strategies. In this paper, we propose to\nstudy the optimal investment strategy on research and development for the dual\nrisk models to minimize the ruin probability of the underlying company. We will\nalso study the optimization problem when in addition the investment in a risky\nasset is allowed.\n"
    },
    {
        "paper_id": 1510.04943,
        "authors": "Fabio Caccioli, Imre Kondor, G\\'abor Papp",
        "title": "Portfolio Optimization under Expected Shortfall: Contour Maps of\n  Estimation Error",
        "comments": "47 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The contour maps of the error of historical resp. parametric estimates for\nlarge random portfolios optimized under the risk measure Expected Shortfall\n(ES) are constructed. Similar maps for the sensitivity of the portfolio weights\nto small changes in the returns as well as the VaR of the ES-optimized\nportfolio are also presented, along with results for the distribution of\nportfolio weights over the random samples and for the out-of-sample and\nin-the-sample estimates for ES. The contour maps allow one to quantitatively\ndetermine the sample size (the length of the time series) required by the\noptimization for a given number of different assets in the portfolio, at a\ngiven confidence level and a given level of relative estimation error. The\nnecessary sample sizes invariably turn out to be unrealistically large for any\nreasonable choice of the number of assets and the confidence level. These\nresults are obtained via analytical calculations based on methods borrowed from\nthe statistical physics of random systems, supported by numerical simulations.\n"
    },
    {
        "paper_id": 1510.04967,
        "authors": "Bernardo Alves Furtado and Isaque Daniel Rocha Eberhardt",
        "title": "A simple agent-based spatial model of the economy: tools for policy",
        "comments": "27 pages, 25 figures, includes ODD Protocol and pseudocodes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study simulates the evolution of artificial economies in order to\nunderstand the tax relevance of administrative boundaries in the quality of\nlife of its citizens. The modeling involves the construction of a computational\nalgorithm, which includes citizens, bounded into families; firms and\ngovernments; all of them interacting in markets for goods, labor and real\nestate. The real estate market allows families to move to dwellings with higher\nquality or lower price when the families capitalize property values. The goods\nmarket allows consumers to search on a flexible number of firms choosing by\nprice and proximity. The labor market entails a matching process between firms\n(location) and candidates (qualification). The government may be configured\ninto one, four or seven distinct sub-national governments. The role of\ngovernment is to collect taxes on the value added of firms in its territory and\ninvest the taxes into higher levels of quality of life for residents. The model\ndoes not have a credit market. The results suggest that the configuration of\nadministrative boundaries is relevant to the levels of quality of life arising\nfrom the reversal of taxes. The model with seven regions is more dynamic, with\nhigher GDP values, but more unequal and heterogeneous across regions. The\nsimulation with only one region is more homogeneously poor. The study seeks to\ncontribute to a theoretical and methodological framework as well as to\ndescribe, operationalize and test computer models of public finance analysis,\nwith explicitly spatial and dynamic emphasis. Several alternatives of expansion\nof the model for future research are described. Moreover, this study adds to\nthe existing literature in the realm of simple microeconomic computational\nmodels, specifying structural relationships between local governments and\nfirms, consumers and dwellings mediated by distance.\n"
    },
    {
        "paper_id": 1510.05097,
        "authors": "Ibrahim Ekren, Ren Liu, Johannes Muhle-Karbe",
        "title": "Optimal Rebalancing Frequencies for Multidimensional Portfolios",
        "comments": "25 pages, 1 figure, to appear in \"Mathematics and Financial\n  Economics\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal investment with multiple assets in the presence of small\nproportional transaction costs. Rather than computing an asymptotically optimal\nno-trade region, we optimize over suitable trading frequencies. We derive\nexplicit formulas for these and the associated welfare losses due to small\ntransaction costs in a general, multidimensional diffusion setting, and compare\ntheir performance to a number of alternatives using Monte Carlo simulations.\n"
    },
    {
        "paper_id": 1510.05115,
        "authors": "Rafal Rak, Pawel Zi\\k{e}ba",
        "title": "Multifractal Flexibly Detrended Fluctuation Analysis",
        "comments": "15 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1212.0354 by other authors",
        "journal-ref": "Acta Physica Polonica B, Vol.46, No.10, p.1925 (2015)",
        "doi": "10.5506/APhysPolB.46.1925",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multifractal time series analysis is a approach that shows the possible\ncomplexity of the system. Nowadays, one of the most popular and the best\nmethods for determining multifractal characteristics is Multifractal Detrended\nFluctuation Analysis (MFDFA). However, it has some drawback. One of its core\nelements is detrending of the series. In the classical MFDFA a trend is\nestimated by fitting a polynomial of degree $m$ where $m=const$. We propose\nthat the degree $m$ of a polynomial was not constant ($m\\neq const$) and its\nselection was ruled by an established criterion. Taking into account the above\namendment, we examine the multifractal spectra both for artificial and\nreal-world mono- and the multifractal time series. Unlike classical MFDFA\nmethod, obtained singularity spectra almost perfectly reflects the theoretical\nresults and for real time series we observe a significant right side shift of\nthe spectrum.\n"
    },
    {
        "paper_id": 1510.05118,
        "authors": "Matteo Barigozzi and Marc Hallin",
        "title": "Networks, Dynamic Factors, and the Volatility Analysis of\n  High-Dimensional Financial Series",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/rssc.12177",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider weighted directed networks for analysing, over the period\n2000-2013, the interdependencies between volatilities of a large panel of\nstocks belonging to the S\\&P100 index. In particular, we focus on the so-called\n{\\it Long-Run Variance Decomposition Network} (LVDN), where the nodes are\nstocks, and the weight associated with edge $(i,j)$ represents the proportion\nof $h$-step-ahead forecast error variance of variable $i$ accounted for by\nvariable $j$'s innovations. To overcome the curse of dimensionality, we\ndecompose the panel into a component driven by few global, market-wide,\nfactors, and an idiosyncratic one modelled by means of a sparse vector\nautoregression (VAR) model. Inversion of the VAR together with suitable\nidentification restrictions, produces the estimated network, by means of which\nwe can assess how {\\it systemic} each firm is.~Our analysis demonstrates the\nprominent role of financial firms as sources of contagion, especially during\nthe~2007-2008 crisis.\n"
    },
    {
        "paper_id": 1510.05123,
        "authors": "Francesco Caravelli, Lorenzo Sindoni, Fabio Caccioli, Cozmin Ududec",
        "title": "Optimal growth trajectories with finite carrying capacity",
        "comments": "10 pages, two columns, 9 figures; title changed, presentation made\n  more general; results untouched",
        "journal-ref": "Phys. Rev. E 94, 022315 (2016)",
        "doi": "10.1103/PhysRevE.94.022315",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of finding optimal strategies that maximize the\naverage growth-rate of multiplicative stochastic processes. For a geometric\nBrownian motion the problem is solved through the so-called Kelly criterion,\naccording to which the optimal growth rate is achieved by investing a constant\ngiven fraction of resources at any step of the dynamics. We generalize these\nfinding to the case of dynamical equations with finite carrying capacity, which\ncan find applications in biology, mathematical ecology, and finance. We\nformulate the problem in terms of a stochastic process with multiplicative\nnoise and a non-linear drift term that is determined by the specific functional\nform of carrying capacity. We solve the stochastic equation for two classes of\ncarrying capacity functions (power laws and logarithmic), and in both cases\ncompute optimal trajectories of the control parameter. We further test the\nvalidity of our analytical results using numerical simulations.\n"
    },
    {
        "paper_id": 1510.0551,
        "authors": "Vladislav Gennadievich Malyshkin and Ray Bakhramov",
        "title": "Mathematical Foundations of Realtime Equity Trading. Liquidity Deficit\n  and Market Dynamics. Automated Trading Machines",
        "comments": "Grammar fixes. Better described numerical calculations in appendix. A\n  link to source code of general basis polynomial library required for\n  Radon-Nikodym numerical calculation added. More literature references.\n  Volatility trading section added. Relation between matrix averages and\n  quantum mechanics density matrix added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We postulates, and then show experimentally, that liquidity deficit is the\ndriving force of the markets. In the first part of the paper a kinematic of\nliquidity deficit is developed. The calculus-like approach, which is based on\nRadon--Nikodym derivatives and their generalization, allows us to calculate\nimportant characteristics of observable market dynamics. In the second part of\nthe paper this calculus is used in an attempt to build a dynamic equation in\nthe form: future price tend to the value maximizing the number of shares traded\nper unit time. To build a practical automated trading machine P&L dynamics\ninstead of price dynamics is considered. This allows a trading automate\nresilient to catastrophic P&L drains to be built. The results are very\npromising, yet when all the fees and trading commissions are taken into\naccount, are close to breakeven. In the end of the paper important criteria for\nautomated trading systems are presented. We list the system types that can and\ncannot make money on the market. These criteria can be successfully applied not\nonly by automated trading machines, but also by a human trader.\n"
    },
    {
        "paper_id": 1510.05561,
        "authors": "Zachary Feinstein, Birgit Rudloff",
        "title": "A Supermartingale Relation for Multivariate Risk Measures",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The equivalence between multiportfolio time consistency of a dynamic\nmultivariate risk measure and a supermartingale property is proven.\nFurthermore, the dual variables under which this set-valued supermartingale is\na martingale are characterized as the worst-case dual variables in the dual\nrepresentation of the risk measure. Examples of multivariate risk measures\nsatisfying the supermartingale property are given. Crucial for obtaining the\nresults are dual representations of scalarizations of set-valued dynamic risk\nmeasures, which are of independent interest in the fast growing literature on\nmultivariate risks.\n"
    },
    {
        "paper_id": 1510.05698,
        "authors": "Oleksandr Vashkiv",
        "title": "Basic industrial funds of cargo motor transport enterprises: problems of\n  effective use",
        "comments": "e.g.: 172 pages,2 figures, 37 tables, ISBN 966-7411-44-9",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work investigates the structure of basic industrial funds of cargo motor\ntransport enterprises and peculiarities of the processes of their reproduction\nin the conditions of social and economic relations transformation. On the basis\nof statistic data of cargo-motor transport enterprises of Ivano-Frankivsk, Lviv\nand Ternopil regions the author investigates the effect of production factors\non the level of capital productivity of the basic funds, he determines reserves\nof its increase. The author motivates the necessity of adaptive qualitative\nchanges in the management and realization of industrial potential and\ninnovations activization in the sphere of cargo motor transportations,\nscientiffically grounded recommendations for efficiency increase of the usage\nof basic industrial funds of cargo motor transportation enterprises in modern\neconomic conditions are provided in this work.\n"
    },
    {
        "paper_id": 1510.0579,
        "authors": "Michael R. Metel, Traian A. Pirvu, Julian Wong",
        "title": "Risk management under Omega measure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the Omega measure, which considers all moments when assessing\nportfolio performance, is equivalent to the widely used Sharpe ratio under\njointly elliptic distributions of returns. Portfolio optimization of the Sharpe\nratio is then explored, with an active-set algorithm presented for markets\nprohibiting short sales. When asymmetric returns are considered we show that\nthe Omega measure and Sharpe ratio lead to different optimal portfolios.\n"
    },
    {
        "paper_id": 1510.05854,
        "authors": "Lisa MH Hall, Alastair Buckley, Jose Mawyin",
        "title": "Estimating the Impact of Wind Generation in the UK",
        "comments": "7 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the impact of wind generation on market prices and system\ncosts in the UK between 2013 and 2014. The wider effects and implications of\nwind generation is of direct relevance and importance to policy makers, as well\nas the grid operator and market traders. We compare electricity generation from\nCoal, Gas and wind, on both the wholesale and imbalance market. We calculate\nthe system cost of wind generation (government subsidies and curtailment costs)\nand the total energy costs. For the first time in the UK, we calculate the\nMerit Order Effect on spot price due to the wind component and show a $1.67\\%$\nprice decrease for every percentage point of wind generation (compared to the\n``zero-wind'' price). The net result of total costs and price savings is\nroughly zero (slight positive gain). We also consider the effect of not having\neither an onshore or an offshore wind component. We show that the Merit-Order\nEffect savings are heavily reduced, leading to an outgoing cost of wind\ngeneration in both cases. It is therefore important to have a significant total\npercentage of wind generation, from both onshore and offshore farms.\n"
    },
    {
        "paper_id": 1510.05858,
        "authors": "Tahir Choulli, Catherine Daveloose, Mich\\`ele Vanmaele",
        "title": "A martingale representation theorem and valuation of defaultable\n  securities",
        "comments": "completely revised",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market model where there are two levels of information. The\npublic information generated by the financial assets, and a larger flow of\ninformation that contains additional knowledge about a random time. This random\ntime can represent many economic and financial settings, such as the default\ntime of a firm for credit risk, and the death time of an insured for life\ninsurance. By using the expansion of filtration, the random time uncertainty\nand its entailed risk are fully considered without any mathematical\nrestriction. In this context with no model's specification for the random time,\nthe main challenge lies in finding the dynamics and the structures for the\nvalue processes of defaultable or mortality and/or longevity securities which\nare vital for the insurance securitization. To overcome this obstacle, we\nelaborate our optional martingale representation results, which state that any\nmartingale in the large filtration stopped at the random time can be decomposed\ninto precise and unique orthogonal local martingales (i.e. local martingales\nwhose product remains a local martingale). This constitutes our first and\nprobably the principal contribution. Even though the driving motivation for\nthis representation resides in credit risk theory, our results are applicable\nto several other financial and economics contexts, such as life insurance and\nfinancial markets with random horizon. Thanks to this optional representation,\nwe decompose any defaultable or mortality and/or longevity liability into the\nsum of \"non-correlated\" risks using a risk basis. This constitutes our second\ncontribution.\n"
    },
    {
        "paper_id": 1510.05875,
        "authors": "Nikolaos Halidias",
        "title": "An elementary approach to the option pricing problem",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our goal here is to discuss the pricing problem of European and American\noptions in discrete time using elementary calculus so as to be an easy\nreference for first year undergraduate students. Using the binomial model we\ncompute the fair price of European and American options. We explain the notion\nof Arbitrage and the notion of the fair price of an option using common sense.\nWe give a criterion that the holder can use to decide when it is appropriate to\nexercise the option. We prove the put-call parity formulas for both European\nand American options and we discuss the relation between American and European\noptions. We give also the bounds for European and American options. We also\ndiscuss the portfolio's optimization problem and the fair value in the case\nwhere the holder can not produce the opposite portfolio.\n"
    },
    {
        "paper_id": 1510.06337,
        "authors": "Ron W Nielsen",
        "title": "Mathematics of Predicting Growth",
        "comments": "17 pages, 5 figures, 1 table, 5382 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical methods of analysis of data and of predicting growth are\ndiscussed. The starting point is the analysis of the growth rates, which can be\nexpressed as a function of time or as a function of the size of the growing\nentity. Application of these methods is illustrated using the world economic\ngrowth but they can be applied to any other type of growth.\n"
    },
    {
        "paper_id": 1510.06809,
        "authors": "Jian Yang",
        "title": "A Link between Sequential Semi-anonymous Nonatomic Games and their Large\n  Finite Counterparts",
        "comments": "in International Journal of Game Theory, 2016, forthcoming",
        "journal-ref": null,
        "doi": "10.1007/s00182-016-0539-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that equilibria of a sequential semi-anonymous nonatomic game (SSNG)\ncan be adopted by players in corresponding large but finite dynamic games to\nachieve near-equilibrium payoffs. Such equilibria in the form of random\nstate-to-action rules are parsimonious in form and easy to execute, as they are\nboth oblivious of past history and blind to other players' present states. Our\ntransient results can be extended to a stationary case, where the finite\ncounterparts are special discounted stochastic games. The kind of equilibria we\nadopt for SSNG are similar to distributional equilibria that are well\nunderstood in literature, and they themselves are shown to exist.\n"
    },
    {
        "paper_id": 1510.06812,
        "authors": "Jian Yang",
        "title": "Game-theoretic Modeling of Players' Ambiguities on External Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a game-theoretic framework that incorporates both incomplete\ninformation and general ambiguity attitudes on factors external to all players.\nOur starting point is players' preferences on payoff-distribution vectors,\nessentially mappings from states of the world to distributions of payoffs to be\nreceived by players. There are two ways in which equilibria for this preference\ngame can be defined. When the preferences possess ever more features, we can\ngradually add ever more structures to the game. These include real-valued\nutility-like functions over payoff-distribution vectors, sets of probabilistic\npriors over states of the world, and eventually the traditional\nexpected-utility framework involving one single prior. We establish equilibrium\nexistence results, show the upper hemi-continuity of equilibrium sets over\nchanging ambiguity attitudes, and uncover relations between the two versions of\nequilibria. Some attention is paid to the enterprising game, in which players\nexhibit ambiguity seeking attitudes while betting optimistically on the\nfavorable resolution of ambiguities. The two solution concepts are unified at\nthis game's pure equilibria, whose existence is guaranteed when strategic\ncomplementarities are present. The current framework can be applied to settings\nlike auctions involving ambiguity on competitors' assessments of item worths.\n"
    },
    {
        "paper_id": 1510.06813,
        "authors": "Jian Yang",
        "title": "Analysis of Markovian Competitive Situations using Nonatomic Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For dynamic situations where the evolution of a player's state is influenced\nby his own action as well as other players' states and actions, we show that\nequilibria derived for nonatomic games (NGs) can be used by their large finite\ncounterparts to achieve near-equilibrium performances. We focus on the case\nwith quite general spaces but also with independently generated shocks driving\nrandom actions and state transitions. The NG equilibria we consider are random\nstate-to-action maps that pay no attention to players' external environments.\nThey are adoptable by a variety of real situations where awareness of other\nplayers' states can be anywhere between full and non-existent. Transient\nresults here also form the basis of a link between an NG's stationary\nequilibrium (SE) and good stationary profiles for large finite games.\n"
    },
    {
        "paper_id": 1510.06946,
        "authors": "Jozef Barun\\'ik and Tobias Kley",
        "title": "Quantile Coherency: A General Measure for Dependence between Cyclical\n  Economic Variables",
        "comments": "paper (49 pages) and online supplement (31 pages), R codes to\n  replicate the figures in the paper are available at\n  https://github.com/tobiaskley/quantile_coherency_replication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce quantile coherency to measure general dependence\nstructures emerging in the joint distribution in the frequency domain and argue\nthat this type of dependence is natural for economic time series but remains\ninvisible when only the traditional analysis is employed. We define estimators\nwhich capture the general dependence structure, provide a detailed analysis of\ntheir asymptotic properties and discuss how to conduct inference for a general\nclass of possibly nonlinear processes. In an empirical illustration we examine\nthe dependence of bivariate stock market returns and shed new light on\nmeasurement of tail risk in financial markets. We also provide a modelling\nexercise to illustrate how applied researchers can benefit from using quantile\ncoherency when assessing time series models.\n"
    },
    {
        "paper_id": 1510.0703,
        "authors": "Daniel Lacker",
        "title": "Law invariant risk measures and information divergences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A one-to-one correspondence is drawn between law invariant risk measures and\ndivergences, which we define as functionals of pairs of probability measures on\narbitrary standard Borel spaces satisfying a few natural properties.\nDivergences include many classical information divergence measures, such as\nrelative entropy and $f$-divergences. Several properties of divergence and\ntheir duality with law invariant risk measures are developed, most notably\nrelating their chain rules or additivity properties with certain notions of\ntime consistency for dynamic law invariant risk measures known as acceptance\nand rejection consistency. These properties are linked also to a peculiar\nproperty of the acceptance sets on the level of distributions, analogous to\nresults of Weber on weak acceptance and rejection consistency. Finally, the\nexamples of shortfall risk measures and optimized certainty equivalents are\ndiscussed in some detail, and it is shown that the relative entropy is\nessentially the only divergence satisfying the chain rule.\n"
    },
    {
        "paper_id": 1510.07033,
        "authors": "Daniel Lacker",
        "title": "Liquidity, risk measures, and concentration of measure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expanding on techniques of concentration of measure, we develop a\nquantitative framework for modeling liquidity risk using convex risk measures.\nThe fundamental objects of study are curves of the form $(\\rho(\\lambda\nX))_{\\lambda \\ge 0}$, where $\\rho$ is a convex risk measure and $X$ a random\nvariable, and we call such a curve a \\emph{liquidity risk profile}. The shape\nof a liquidity risk profile is intimately linked with the tail behavior of the\nunderlying $X$ for some notable classes of risk measures, namely shortfall risk\nmeasures. We exploit this link to systematically bound liquidity risk profiles\nfrom above by other real functions $\\gamma$, deriving tractable necessary and\nsufficient conditions for \\emph{concentration inequalities} of the form\n$\\rho(\\lambda X) \\le \\gamma(\\lambda)$, for all $\\lambda \\ge 0$. These\nconcentration inequalities admit useful dual representations related to\ntransport inequalities, and this leads to efficient uniform bounds for\nliquidity risk profiles for large classes of $X$. On the other hand, some\nmodest new mathematical results emerge from this analysis, including a new\ncharacterization of some classical transport-entropy inequalities. Lastly, the\nanalysis is deepened by means of a surprising connection between time\nconsistency properties of law invariant risk measures and the tensorization of\nconcentration inequalities.\n"
    },
    {
        "paper_id": 1510.07111,
        "authors": "Jak\\v{s}a Cvitani\\'c and Dylan Possama\\\"i and Nizar Touzi",
        "title": "Dynamic programming approach to principal-agent problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general formulation of the Principal-Agent problem with a\nlump-sum payment on a finite horizon, providing a systematic method for solving\nsuch problems. Our approach is the following: we first find the contract that\nis optimal among those for which the agent's value process allows a dynamic\nprogramming representation, for which the agent's optimal effort is\nstraightforward to find. We then show that the optimization over the restricted\nfamily of contracts represents no loss of generality. As a consequence, we have\nreduced this non-zero sum stochastic differential game to a stochastic control\nproblem which may be addressed by the standard tools of control theory. Our\nproofs rely on the backward stochastic differential equations approach to\nnon-Markovian stochastic control, and more specifically, on the recent\nextensions to the second order case.\n"
    },
    {
        "paper_id": 1510.07199,
        "authors": "Wujiang Lou",
        "title": "Coherent CVA and FVA with Liability Side Pricing of Derivatives",
        "comments": "21 pages, 1 figure. The 2nd Fixed Income and Operational Risk\n  Conference USA, May 2015",
        "journal-ref": "Risk, August 2015, pp.48-53",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents FVA and CVA of a bilateral derivative in a coherent\nmanner, based on recent developments in fair value accounting and ISDA\nstandards. We argue that a derivative liability, after primary risk factors\nbeing hedged, resembles in economics an issued variable funding note, and\nshould be priced at the market rate of the issuer's debt. For the purpose of\ndetermining the fair value, the party on the liability side is economically\nneutral to make a deposit to the other party, which earns his current debt rate\nand effectively provides funding and hedging for the party holding the\nderivative asset. The newly derived partial differential equation for an option\ndiscounts the derivative's receivable part with counterparty's curve and\npayable part with own financing curve. The price difference from the\ncounterparty risk free price, or total counterparty risk adjustment, is\nprecisely defined by discounting the product of the risk free price and the\ncredit spread at the local liability curve. Subsequently the adjustment can be\nbroken into a default risk component -- CVA and a funding component -- FVA,\nconsistent with a simple note's fair value treatment and in accordance with the\nusual understanding of a bond's credit spread consisting of a CDS spread and a\nbasis. As for FVA, we define a cost -- credit funding adjustment (CFA) and a\nbenefit -- debit funding adjustment (DFA), in parallel to CVA and DVA and\nattributed to counterparty's and own funding basis. This resolves a number of\noutstanding FVA debate issues, such as double counting, violation of the law of\none price, misuse of cash flow discounting, and controversial hedging of own\ndefault risk. It also allows an integrated implementation strategy and reuse of\nexisting CVA infrastructure.\n"
    },
    {
        "paper_id": 1510.07221,
        "authors": "Alexander Kushpel",
        "title": "Pricing of high-dimensional options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing of high-dimensional options is one of the most important problems in\nMathematical Finance. The objective of this manuscript is to present an\noriginal self-contained treatment of the multidimensional pricing. During the\npast decades the Black-Scholes this model, which essentially is based on the\nlog-normal assumption, has been increasingly criticised. In particular, it was\nnoticed by Mandelbrot that empirical log-returns distributions are more\nconcentrated around the origin and have considerably heavier tails. This\nsuggests to adjust the Black-Scholes model by the introduction of the Levy\nprocesses instead of Brownian ones. This approach has been extensively studied\nin a univariate setup since the nineties. In the multivariate settings the\ntheory is not so advanced. We present a general method of high-dimensional\noption pricing based on a wide range of jump-diffusion models. Namely, we\nconstruct approximation formulas for the price of spread options. It is\nimportant to get an efficient approximation for the respective density\nfunction, since the reward function has usually a simple structure. Instead of\na commonly used tabulation approach, we use the respective m-widths to compare\na wide range of numerical methods. We give an algorithm of almost optimal, in\nthe sense of the respective m-widths, reconstruction of density functions. To\ndemonstrate the power of our approach we consider in details a concrete class\nof Levy driven processes and present the respective rates of convergence of\napproximation formulas. The interrelationship between the theory and tools\nreflects the richness and deep connections in Financial Mathematics, Stochastic\nProcesses, Theory of Martingales, Functional Analysis, Topology and Harmonic\nAnalysis.\n"
    },
    {
        "paper_id": 1510.0728,
        "authors": "Paulo Rocha, Frank Raischel, Jo\\~ao P. Boto, Pedro G. Lind",
        "title": "Uncovering the evolution of non-stationary stochastic variables: the\n  example of asset volume-price fluctuations",
        "comments": "9 pages, 14 figures",
        "journal-ref": "Phys. Rev. E 93, 052122 (2016)",
        "doi": "10.1103/PhysRevE.93.052122",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a framework for describing the evolution of stochastic observables\nhaving a non-stationary distribution of values. The framework is applied to\nempirical volume-prices from assets traded at the New York stock exchange.\nUsing Kullback-Leibler divergence we evaluate the best model out from four\nbiparametric models standardly used in the context of financial data analysis.\nIn our present data sets we conclude that the inverse $\\Gamma$-distribution is\na good model, particularly for the distribution tail of the largest\nvolume-price fluctuations. Extracting the time-series of the corresponding\nparameter values we show that they evolve in time as stochastic variables\nthemselves. For the particular case of the parameter controlling the\nvolume-price distribution tail we are able to extract an Ornstein-Uhlenbeck\nequation which describes the fluctuations of the largest volume-prices observed\nin the data. Finally, we discuss how to bridge from the stochastic evolution of\nthe distribution parameters to the stochastic evolution of the (non-stationary)\nobservable and put our conclusions into perspective for other applications in\ngeophysics and biology.\n"
    },
    {
        "paper_id": 1510.07599,
        "authors": "Semei Coronado, Rebeca Jim\\'enez-Rodr\\'iguez, Omar Rojas",
        "title": "An empirical analysis of the relationships between crude oil, gold and\n  stock markets",
        "comments": "submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the direction of the causality between crude oil, gold\nand stock markets for the largest economy in the world with respect to such\nmarkets, the US. To do so, we apply non-linear Granger causality tests. We find\na nonlinear causal relationship among the three markets considered, with the\ncausality going in all directions, when the full sample and different\nsubsamples are considered. However, we find a unidirectional nonlinear causal\nrelationship between the crude oil and gold market (with the causality only\ngoing from oil price changes to gold price changes) when the subsample runs\nfrom the first date of any year between the mid-1990s and 2001 to last\navailable data (February 5, 2015). The latter result may explain the lack of\nconsensus existing in the literature about the direction of the causal link\nbetween the crude oil and gold markets.\n"
    },
    {
        "paper_id": 1510.07608,
        "authors": "Alexander Lipton",
        "title": "Modern Monetary Circuit Theory, Stability of Interconnected Banking\n  Network, and Balance Sheet Optimization for Individual Banks",
        "comments": "67 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A modern version of Monetary Circuit Theory with a particular emphasis on\nstochastic underpinning mechanisms is developed. It is explained how money is\ncreated by the banking system as a whole and by individual banks. The role of\ncentral banks as system stabilizers and liquidity providers is elucidated. It\nis shown how in the process of money creation banks become naturally\ninterconnected. A novel Extended Structural Default Model describing the\nstability of the Interconnected Banking Network is proposed. The purpose of\nbanks' capital and liquidity is explained. Multi-period constrained\noptimization problem for banks's balance sheet is formulated and solved in a\nsimple case. Both theoretical and practical aspects are covered.\n"
    },
    {
        "paper_id": 1510.07888,
        "authors": "J. V. Howard",
        "title": "Exchanging Goods Using Valuable Money",
        "comments": "26 pages, 10 figures, revised twice",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A group of people wishes to use money to exchange goods efficiently over\nseveral time periods. However, there are disadvantages to using any of the\ngoods as money, and in addition fiat money issued in the form of notes or coins\nwill be valueless in the final time period, and hence in all earlier periods.\nAlso, Walrasian market prices are determined only up to an arbitrary rescaling.\nNevertheless we show that it is possible to devise a system which uses money to\nexchange goods and in which money has a determinate positive value. In this\nsystem, tokens are initially supplied to all traders by a central authority and\nrecovered by a purchase tax. All trades must be made using tokens or promissory\nnotes for tokens. This mechanism controls the flow rather than the stock of\nmoney: it introduces some trading frictions, some redistribution of wealth, and\nsome distortion of prices, but these effects can all be made small.\n"
    },
    {
        "paper_id": 1510.07927,
        "authors": "Aleksandra Aloric, Peter Sollich, Peter McBurney, Tobias Galla",
        "title": "Emergence of Cooperative Long-term Market Loyalty in Double Auction\n  Markets",
        "comments": "33 pages, 11 figures; referee remarks included, published: April 27,\n  2016",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0154606",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Loyal buyer-seller relationships can arise by design, e.g. when a seller\ntailors a product to a specific market niche to accomplish the best possible\nreturns, and buyers respond to the dedicated efforts the seller makes to meet\ntheir needs. We ask whether it is possible, instead, for loyalty to arise\nspontaneously, and in particular as a consequence of repeated interaction and\nco-adaptation among the agents in a market. We devise a stylized model of\ndouble auction markets and adaptive traders that incorporates these features.\nTraders choose where to trade (which market) and how to trade (to buy or to\nsell) based on their previous experience. We find that when the typical scale\nof market returns (or, at fixed scale of returns, the intensity of choice)\nbecome higher than some threshold, the preferred state of the system is\nsegregated: both buyers and sellers are segmented into subgroups that are\npersistently loyal to one market over another. We characterize the segregated\nstate analytically in the limit of large markets: it is stabilized by some\nagents acting cooperatively to enable trade, and provides higher rewards than\nits unsegregated counterpart both for individual traders and the population as\na whole.\n"
    },
    {
        "paper_id": 1510.07928,
        "authors": "Ron W Nielsen",
        "title": "The Insecure Future of the World Economic Growth",
        "comments": "14 pages, 9 figures, 2 tables, 4234 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Growth rate of the world Growth Domestic Product (GDP) is analysed to\ndetermine possible pathways of the future economic growth. The analysis is\nbased on using the latest data of the World Bank and it reveals that the growth\nrate between 1960 and 2014 was following a trajectory approaching\nasymptotically a constant value. The most likely prediction is that the world\neconomic growth will continue to increase exponentially and that it will become\nunsustainable possibly even during the current century. A more optimistic but\nless realistic prediction is based on the assumption that the growth rate will\nstart to decrease linearly. In this case, the world economic growth is\npredicted to reach a maximum, if the growth rate is going to decrease linearly\nwith time, or to follow a logistic trajectory, if the growth rate is going to\ndecrease linearly with the size of the world GDP.\n"
    },
    {
        "paper_id": 1510.08103,
        "authors": "Mihaela van der Schaar, Simpson Zhang",
        "title": "From Acquaintances to Friends: Homophily and Learning in Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the evolution of a network in a discrete time,\nstochastic setting in which agents learn about each other through repeated\ninteractions and maintain/break links on the basis of what they learn from\nthese interactions. Agents have homophilous preferences and limited capacity,\nso they maintain links with others who are learned to be similar to themselves\nand cut links to others who are learned to be dissimilar to themselves. Thus\nlearning influences the evolution of the network, but learning is imperfect so\nthe evolution is stochastic. Homophily matters. Higher levels of homophily\ndecrease the (average) number of links that agents form. However, the effect of\nhomophily is anomalous: mutually beneficial links may be dropped before\nlearning is completed, thereby resulting in sparser networks and less\nclustering than under complete information. There may be big differences\nbetween the networks that emerge under complete and incomplete information.\nHomophily matters here as well: initially, greater levels of homophily increase\nthe difference between the complete and incomplete information networks, but\nsufficiently high levels of homophily eventually decrease the difference.\nComplete and incomplete information networks differ the most when the degree of\nhomophily is intermediate. With multiple stages of life, the effects of\nincomplete information are large initially but fade somewhat over time.\n"
    },
    {
        "paper_id": 1510.08161,
        "authors": "Adriana Ocejo",
        "title": "Asian option as a fixed-point",
        "comments": "Accepted in Journal of Fixed Point Theory and Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize the price of an Asian option, a financial contract, as a\nfixed-point of a non-linear operator. In recent years, there has been interest\nin incorporating changes of regime into the parameters describing the evolution\nof the underlying asset price, namely the interest rate and the volatility, to\nmodel sudden exogenous events in the economy. Asian options are particularly\ninteresting because the payoff depends on the integrated asset price. We study\nthe case of both floating- and fixed-strike Asian call options with arithmetic\naveraging when the asset follows a regime-switching geometric Brownian motion\nwith coefficients that depend on a Markov chain. The typical approach to\nfinding the value of a financial option is to solve an associated system of\ncoupled partial differential equations. Alternatively, we propose an iterative\nprocedure that converges to the value of this contract with geometric rate\nusing a classical fixed-point theorem.\n"
    },
    {
        "paper_id": 1510.08162,
        "authors": "Li Lin, Didier Sornette",
        "title": "\"Speculative Influence Network\" during financial bubbles: application to\n  Chinese Stock Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the Speculative Influence Network (SIN) to decipher the causal\nrelationships between sectors (and/or firms) during financial bubbles. The SIN\nis constructed in two steps. First, we develop a Hidden Markov Model (HMM) of\nregime-switching between a normal market phase represented by a geometric\nBrownian motion (GBM) and a bubble regime represented by the stochastic\nsuper-exponential Sornette-Andersen (2002) bubble model. The calibration of the\nHMM provides the probability at each time for a given security to be in the\nbubble regime. Conditional on two assets being qualified in the bubble regime,\nwe then use the transfer entropy to quantify the influence of the returns of\none asset $i$ onto another asset $j$, from which we introduce the adjacency\nmatrix of the SIN among securities. We apply our technology to the Chinese\nstock market during the period 2005-2008, during which a normal phase was\nfollowed by a spectacular bubble ending in a massive correction. We introduce\nthe Net Speculative Influence Intensity (NSII) variable as the difference\nbetween the transfer entropies from $i$ to $j$ and from $j$ to $i$, which is\nused in a series of rank ordered regressions to predict the maximum loss\n(\\%{MaxLoss}) endured during the crash. The sectors that influenced other\nsectors the most are found to have the largest losses. There is a clear\nprediction skill obtained by using the transfer entropy involving industrial\nsectors to explain the \\%{MaxLoss} of financial institutions but not vice\nversa. We also show that the bubble state variable calibrated on the Chinese\nmarket data corresponds well to the regimes when the market exhibits a strong\nprice acceleration followed by clear change of price regimes. Our results\nsuggest that SIN may contribute significant skill to the development of general\nlinkage-based systemic risks measures and early warning metrics.\n"
    },
    {
        "paper_id": 1510.08285,
        "authors": "Jochen L. Leidner",
        "title": "Computer-Suported Risk Identification for the Holistic Management of\n  Risks",
        "comments": "19 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk is part of the fabric of every business; surprisingly, there is little\nwork on establishing best practices for systematic, repeatable risk\nidentification, arguably the first step of any risk management process. In this\npaper, we present a proposal that constitutes a more holistic risk management\napproach, a methodology for computer-supported risk identification is proposed\nthat may lead to more consistent (objective, repeatable) risk analysis.\n"
    },
    {
        "paper_id": 1510.08335,
        "authors": "Victor M. Zavala and Kibaek Kim and Mihai Anitescu and John Birge",
        "title": "A Stochastic Electricity Market Clearing Formulation with Consistent\n  Pricing Properties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue that deterministic market clearing formulations introduce arbitrary\ndistortions between day-ahead and expected real-time prices that bias economic\nincentives and block diversification. We extend and analyze the stochastic\nclearing formulation proposed by Pritchard et al. (2010) in which the social\nsurplus function induces penalties between day-ahead and real-time quantities.\nWe prove that the formulation yields price distortions that are bounded by the\nbid prices, and we show that adding a similar penalty term to transmission\nflows and phase angles ensures boundedness throughout the network. We prove\nthat when the price distortions are zero, day-ahead quantities converge to the\nquantile of real-time counterparts. The undesired effects of price distortions\nsuggest that stochastic settings provide significant benefits over\ndeterministic ones that go beyond social surplus improvements. We propose\nadditional metrics to evaluate these benefits.\n"
    },
    {
        "paper_id": 1510.08439,
        "authors": "Dylan Possama\\\"i and Xiaolu Tan and Chao Zhou",
        "title": "Stochastic control for a class of nonlinear kernels and applications",
        "comments": "48 pages. Minor modifications compared to the published version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic control problem for a class of nonlinear kernels.\nMore precisely, our problem of interest consists in the optimisation, over a\nset of possibly non-dominated probability measures, of solutions of backward\nstochastic differential equations (BSDEs). Since BSDEs are nonlinear\ngeneralisations of the traditional (linear) expectations, this problem can be\nunderstood as stochastic control of a family of nonlinear expectations, or\nequivalently of nonlinear kernels. Our first main contribution is to prove a\ndynamic programming principle for this control problem in an abstract setting,\nwhich we then use to provide a semi-martingale characterisation of the value\nfunction. We next explore several applications of our results. We first obtain\na wellposedness result for second order BSDEs (as introduced in [86]) which\ndoes not require any regularity assumption on the terminal condition and the\ngenerator. Then we prove a nonlinear optional decomposition in a robust\nsetting, extending recent results of [71], which we then use to obtain a\nsuper-hedging duality in uncertain, incomplete and nonlinear financial markets.\nFinally, we relate, under additional regularity assumptions, the value function\nto a viscosity solution of an appropriate path-dependent partial differential\nequation (PPDE).\n"
    },
    {
        "paper_id": 1510.08615,
        "authors": "Ladislav Kristoufek and Miloslav Vosvrda",
        "title": "Gold, currencies and market efficiency",
        "comments": "15 pages, 1 figure",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 449,\n  1 May 2016, Pages 27-34",
        "doi": "10.1016/j.physa.2015.12.075",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Gold and currency markets form a unique pair with specific interactions and\ndynamics. We focus on the efficiency ranking of gold markets with respect to\nthe currency of purchase. By utilizing the Efficiency Index (EI) based on\nfractal dimension, approximate entropy and long-term memory on a wide portfolio\nof 142 gold price series for different currencies, we construct the efficiency\nranking based on the extended EI methodology we provide. Rather unexpected\nresults are uncovered as the gold prices in major currencies lay among the\nleast efficient ones whereas very minor currencies are among the most efficient\nones. We argue that such counterintuitive results can be partly attributed to a\nunique period of examination (2011-2014) characteristic by quantitative easing\nand rather unorthodox monetary policies together with the investigated illegal\ncollusion of major foreign exchange market participants, as well as some other\nfactors discussed in some detail.\n"
    },
    {
        "paper_id": 1510.0911,
        "authors": "Jia-Wen Gu and Mogens Steffensen",
        "title": "Optimal Portfolio Liquidation and Dynamic Mean-variance Criterion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the optimal portfolio liquidation problem under\nthe dynamic mean-variance criterion and derive time-consistent solutions in\nthree important models. We give adapted optimal strategies under a reconsidered\nmean-variance subject at any point in time. We get explicit trading strategies\nin the basic model and when random pricing signals are incorporated. When we\nconsider stochastic liquidity and volatility, we construct a generalized HJB\nequation under general assumptions for the parameters. We obtain an explicit\nsolution in stochastic volatility model with a given structure supported by\nempirical studies.\n"
    },
    {
        "paper_id": 1511.00026,
        "authors": "Alexander Schied and Iryna Voloshchenko",
        "title": "Pathwise no-arbitrage in a class of Delta hedging strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a strictly pathwise setting for Delta hedging exotic options,\nbased on F\\\"ollmer's pathwise It\\=o calculus. Price trajectories are\n$d$-dimensional continuous functions whose pathwise quadratic variations and\ncovariations are determined by a given local volatility matrix. The existence\nof Delta hedging strategies in this pathwise setting is established via\nexistence results for recursive schemes of parabolic Cauchy problems and via\nthe existence of functional Cauchy problems on path space. Our main results\nestablish the nonexistence of pathwise arbitrage opportunities in classes of\nstrategies containing these Delta hedging strategies and under relatively mild\nconditions on the local volatility matrix.\n"
    },
    {
        "paper_id": 1511.00065,
        "authors": "Ivar Ekeland, Yiming Long, Qinglong Zhou",
        "title": "A New Class of Problems in the Calculus of Variations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1134/S1560354713060026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates an infinite-horizon problems in the one-dimensional\ncalculus of variations, arising from the Ramsey model of endogeneous economic\ngrowth. Following Chichilnisky, we introduce an additional term, which models\nconcern for the well-being of future generations. We show that there are no\noptimal solutions, but that there are equilibrium strateges, i.e. Nash\nequilibria of the leader-follower game between successive generations. To solve\nthe problem, we approximate the Chichilnisky criterion by a biexponential\ncriterion, we characterize its equilibria by a pair of coupled differential\nequations of HJB type, and we go to the limit. We find all the equilibrium\nstrategies for the Chichilnisky criterion. The mathematical analysis is\ndifficult because one has to solve an implicit differential equation in the\nsense of Thom. Our analysis extends earlier work by Ekeland and Lazrak. It is\nshown that optimal solutions a class of problems raising from time\ninconsistency problems in the framework of the neoclassical one-sector model of\neconomic growth, and contains new results in environment economics. Without\nexogenous commitment mechanism, a notion of the equilibrium strategies instead\nof the optimal strategies is introduced. We characterized the equilibrium\nstrategies by an integro-differential equation system. For two special\ncriteria, the bi-exponential criteria and the Chichilnisky criteria, we\nestablished the existence of the equilibrium strategies.\n"
    },
    {
        "paper_id": 1511.0014,
        "authors": "Jakob Kisiala",
        "title": "Conditional Value-at-Risk: Theory and Applications",
        "comments": "62 pages (without bibliography and appendix), 27 figures,\n  Dissertation presented for the degree of MSc in Operational Research,\n  University of Edinburgh",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis presents the Conditional Value-at-Risk concept and combines an\nanalysis that covers its application as a risk measure and as a vector norm.\nFor both areas of application the theory is revised in detail and examples are\ngiven to show how to apply the concept in practice.\n  In the first part, CVaR as a risk measure is introduced and the analysis\ncovers the mathematical definition of CVaR and different methods to calculate\nit. Then, CVaR optimization is analysed in the context of portfolio selection\nand how to apply CVaR optimization for hedging a portfolio consisting of\noptions. The original contributions in this part are an alternative proof of\nAcerbi's Integral Formula in the continuous case and an explicit programme\nformulation for portfolio hedging.\n  The second part first analyses the Scaled and Non-Scaled CVaR norm as new\nfamily of norms in $\\mathbb{R}^n$ and compares this new norm family to the more\nwidely known $L_p$ norms. Then, model (or signal) recovery problems are\ndiscussed and it is described how appropriate norms can be used to recover a\nsignal with less observations than the dimension of the signal. The last\nchapter of this dissertation then shows how the Non-Scaled CVaR norm can be\nused in this model recovery context. The original contributions in this part\nare an alternative proof of the equivalence of two different characterizations\nof the Scaled CVaR norm, a new proposition that the Scaled CVaR norm is\npiecewise convex, and the entire \\autoref{chapter:Recovery_using_CVaR}. Since\nthe CVaR norm is a rather novel concept, its applications in a model recovery\ncontext have not been researched yet. Therefore, the final chapter of this\nthesis might lay the basis for further research in this area.\n"
    },
    {
        "paper_id": 1511.00468,
        "authors": "Vadim Arkin and Alexander Slastnikov",
        "title": "Real Options and Threshold Strategies",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper considers an investment timing problem appearing in real options\ntheory. Present values from an investment project are modeled by general\ndiffusion process. We prove necessary and sufficient conditions under which an\noptimal investment time is induced by threshold strategy. We study also the\nconditions of optimality of threshold strategy (over all threshold strategies)\nand discuss the connection between solutions to investment timing problem and\nto free-boundary problem.\n"
    },
    {
        "paper_id": 1511.00483,
        "authors": "Richard Pin\\v{c}\\'ak, Erik Barto\\v{s}",
        "title": "With string model to time series forecasting",
        "comments": "13 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:physics/0205053 by other authors",
        "journal-ref": "Physica A436 (2015) 135-146",
        "doi": "10.1016/j.physa.2015.05.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Overwhelming majority of econometric models applied on a long term basis in\nthe financial forex market do not work sufficiently well. The reason is that\ntransaction costs and arbitrage opportunity are not included, as this does not\nsimulate the real financial markets. Analyses are not conducted on the non\nequidistant date but rather on the aggregate date, which is also not a real\nfinancial case. In this paper, we would like to show a new way how to analyze\nand, moreover, forecast financial market. We utilize the projections of the\nreal exchange rate dynamics onto the string-like topology in the OANDA market.\nThe latter approach allows us to build the stable prediction models in trading\nin the financial forex market. The real application of the multi-string\nstructures is provided to demonstrate our ideas for the solution of the problem\nof the robust portfolio selection. The comparison with the trend following\nstrategies was performed, the stability of the algorithm on the transaction\ncosts for long trade periods was confirmed.\n"
    },
    {
        "paper_id": 1511.0074,
        "authors": "Enrique Mart\\'inez-Miranda and Peter McBurney and Matthew J. Howard",
        "title": "Learning Unfair Trading: a Market Manipulation Analysis From the\n  Reinforcement Learning Perspective",
        "comments": "7 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market manipulation is a strategy used by traders to alter the price of\nfinancial securities. One type of manipulation is based on the process of\nbuying or selling assets by using several trading strategies, among them\nspoofing is a popular strategy and is considered illegal by market regulators.\nSome promising tools have been developed to detect manipulation, but cases can\nstill be found in the markets. In this paper we model spoofing and pinging\ntrading, two strategies that differ in the legal background but share the same\nelemental concept of market manipulation. We use a reinforcement learning\nframework within the full and partial observability of Markov decision\nprocesses and analyse the underlying behaviour of the manipulators by finding\nthe causes of what encourages the traders to perform fraudulent activities.\nThis reveals procedures to counter the problem that may be helpful to market\nregulators as our model predicts the activity of spoofers.\n"
    },
    {
        "paper_id": 1511.00848,
        "authors": "Giacomo Bormetti, Giorgia Callegaro, Giulia Livieri, Andrea\n  Pallavicini",
        "title": "A backward Monte Carlo approach to exotic option pricing",
        "comments": "37 pages, 13 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel algorithm which allows to sample paths from an underlying\nprice process in a local volatility model and to achieve a substantial variance\nreduction when pricing exotic options. The new algorithm relies on the\nconstruction of a discrete multinomial tree. The crucial feature of our\napproach is that -- in a similar spirit to the Brownian Bridge -- each random\npath runs backward from a terminal fixed point to the initial spot price. We\ncharacterize the tree in two alternative ways: in terms of the optimal grids\noriginating from the Recursive Marginal Quantization algorithm and following an\napproach inspired by the finite difference approximation of the diffusion's\ninfinitesimal generator. We assess the reliability of the new methodology\ncomparing the performance of both approaches and benchmarking them with\ncompetitor Monte Carlo methods.\n"
    },
    {
        "paper_id": 1511.00884,
        "authors": "Maximilian Ga{\\ss}, Kathrin Glau, Maximilian Mair",
        "title": "Magic points in finance: Empirical integration for parametric option\n  pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an offline-online procedure for Fourier transform based option\npricing. The method supports the acceleration of such essential tasks of\nmathematical finance as model calibration, real-time pricing, and, more\ngenerally, risk assessment and parameter risk estimation. We adapt the\nempirical magic point interpolation method of Barrault, Nguyen, Maday and\nPatera (2004) to parametric Fourier pricing. In the offline phase, a quadrature\nrule is tailored to the family of integrands of the parametric pricing problem.\nIn the online phase, the quadrature rule then yields fast and accurate\napproximations of the option prices. Under analyticity assumptions the pricing\nerror decays exponentially. Numerical experiments in one dimension confirm our\ntheoretical findings and show a significant gain in efficiency, even for\nexamples beyond the scope of the theoretical results.\n"
    },
    {
        "paper_id": 1511.01207,
        "authors": "Ivan Degano, Sebastian Ferrando and Alfredo Gonzalez",
        "title": "Trajectory based models. Evaluation of minmax pricing bounds",
        "comments": "45 pages, 15 figures. This is the second version of the paper. There\n  is a somewhat different example that connects to known probabilistic models.\n  Some results along this direction are presented. We also discuss the\n  literature pertaining to the computation of fair price bounds",
        "journal-ref": "Dynamics of Continuous, Discrete and Impulsive Systems Series B:\n  Applications and Algorithms, 2018, Vol 25, 97-128",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies sub and super-replication price bounds for contingent\nclaims defined on general trajectory based market models. No prior\nprobabilistic or topological assumptions are placed on the trajectory space,\ntrading is assumed to take place at a finite number of occasions but not\nbounded in number nor necessarily equally spaced in time. For a given option,\nthere exists an interval bounding the set of possible fair prices; such\ninterval exists under more general conditions than the usual no-arbitrage\nrequirement. The paper develops a backward recursive method to evaluate the\noption bounds; the global minmax optimization, defining the price interval, is\nreduced to a local minmax optimization via dynamic programming. Trajectory sets\nare introduced for which existing non-probabilistic markets models are nested\nas a particular case. Several examples are presented, the effect of the\npresence of arbitrage on the price bounds is illustrated.\n"
    },
    {
        "paper_id": 1511.01395,
        "authors": "Zura Kakushadze",
        "title": "On Origins of Alpha",
        "comments": "8 pages",
        "journal-ref": "The Hedge Fund Journal 108 (2015) 47-50",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue that an important contributing factor into market inefficiency is\nthe lack of a robust mechanism for the stock price to rise if a company has\ngood earnings, e.g., via buybacks/dividends. Instead, the stock price is prone\nto volatility due to rather random perception/interpretation of earnings\nannouncements (among other data) by market participants. We present empirical\nevidence indicating that dividend paying stocks on average are less volatile,\neven factoring out market cap. We further ponder possible ways of increasing\nmarket efficiency via 1) instituting such a mechanism, 2) a taxation scheme\nthat would depend on holding periods, and 3) a universal crossing\nengine/exchange for mutual and pension funds (and similar long holding horizon\nvehicles) with no dark pools, 100% transparency, and no advantage for timing\norders.\n"
    },
    {
        "paper_id": 1511.0146,
        "authors": "Andrey Itkin",
        "title": "LSV models with stochastic interest rates and correlated jumps",
        "comments": "33 pages, 9 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing and hedging exotic options using local stochastic volatility models\ndrew a serious attention within the last decade, and nowadays became almost a\nstandard approach to this problem. In this paper we show how this framework\ncould be extended by adding to the model stochastic interest rates and\ncorrelated jumps in all three components. We also propose a new fully implicit\nmodification of the popular Hundsdorfer and Verwer and Modified Craig-Sneyd\nfinite-difference schemes which provides second order approximation in space\nand time, is unconditionally stable and preserves positivity of the solution,\nwhile still has a linear complexity in the number of grid nodes.\n"
    },
    {
        "paper_id": 1511.01529,
        "authors": "Oleg Malafeyev and Achal Awasthi",
        "title": "A Dynamic Model of Functioning of a Bank",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze dynamic programming as a novel approach to solve\nthe problem of maximizing the profits of a bank. The mathematical model of the\nproblem and the description of a bank's work is described in this paper. The\nproblem is then approached using the method of dynamic programming. Dynamic\nprogramming makes sure that the solutions obtained are globally optimal and\nnumerically stable. The optimization process is set up as a discrete\nmulti-stage decision process and solved with the help of dynamic programming.\n"
    },
    {
        "paper_id": 1511.01564,
        "authors": "Song-Ping Zhu, Nhat-Tan Le, Wen-Ting Chen and Xiaoping Lu",
        "title": "Pricing Parisian down-and-in options",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.aml.2014.10.019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we price American-style Parisian down-and-in call options\nunder the Black-Scholes framework. Usually, pricing an American-style option is\nmuch more difficult than pricing its European-style counterpart because of the\nappearance of the optimal exercise boundary in the former. Fortunately, the\noptimal exercise boundary associated with an American-style Parisian knock-in\noption only appears implicitly in its pricing partial differential equation\n(PDE) systems, instead of explicitly as in the case of an American-style\nParisian knock-out option. We also recognize that the \"moving window\" technique\ndeveloped for pricing European-style Parisian up-and-out options can be adopted\nto price American-style Parisian knock-in options as well. In particular, we\nobtain a simple analytical solution for American-style Parisian down-and-in\ncall options and our new formula is written in terms of four double integrals,\nwhich can be easily computed numerically.\n"
    },
    {
        "paper_id": 1511.01707,
        "authors": "Johan Dahlin and Thomas B. Sch\\\"on",
        "title": "Getting Started with Particle Metropolis-Hastings for Inference in\n  Nonlinear Dynamical Models",
        "comments": "41 pages, 7 figures. In press for Journal of Statistical Software.\n  Source code for R, Python and MATLAB available at:\n  https://github.com/compops/pmh-tutorial",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This tutorial provides a gentle introduction to the particle\nMetropolis-Hastings (PMH) algorithm for parameter inference in nonlinear\nstate-space models together with a software implementation in the statistical\nprogramming language R. We employ a step-by-step approach to develop an\nimplementation of the PMH algorithm (and the particle filter within) together\nwith the reader. This final implementation is also available as the package\npmhtutorial in the CRAN repository. Throughout the tutorial, we provide some\nintuition as to how the algorithm operates and discuss some solutions to\nproblems that might occur in practice. To illustrate the use of PMH, we\nconsider parameter inference in a linear Gaussian state-space model with\nsynthetic data and a nonlinear stochastic volatility model with real-world\ndata.\n"
    },
    {
        "paper_id": 1511.01763,
        "authors": "Harri Nyrhinen",
        "title": "On real growth and run-off companies in insurance ruin theory",
        "comments": "To appear in Advances in Applied Probability (September 2016)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study solvency of insurers in a comprehensive model where various economic\nfactors affect the capital developments of the companies. The main interest is\nin the impact of real growth to ruin probabilities. The volume of the business\nis allowed to increase or decrease. In the latter case, the study is focused on\nrun-off companies. Our main results give sharp asymptotic estimates for\ninfinite time ruin probabilities.\n"
    },
    {
        "paper_id": 1511.01824,
        "authors": "Liang Wu, Jingyi Luo, Yingkai Tang and Gregory Bardes",
        "title": "Positive skewness, anti-leverage, reverse volatility asymmetry, and\n  short sale constraints: Evidence from the Chinese markets",
        "comments": "26 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are some statistical anomalies in the Chinese stock market, i.e.,\npositive return skewness, anti-leverage effect (positive returns induce higher\nvolatility than negative returns); and reverse volatility asymmetry\n(contemporaneous return-volatility correlation is positive). In this paper, we\nfirst confirm the existence of these anomalies using daily firm-level stock\nreturn data on the raw returns, excess returns and normalized excess returns.\nWe empirically show that the asymmetry response of investors to news is one\ncause of the statistical anomalies if short sales are constrained. Then in the\ncontext of slow adoption of security lending policy, we conduct panel analysis\nand empirically verify that the lifting of short sale constraints leads to\nsignificantly less skewness, less anti-leverage effect and less reverse\nvolatility asymmetry. Positive skewness is a feature of lottery. Investors are\nencouraged to bet on the upside lottery like potentials in the Chinese markets\nwhere the stocks skew more to the upside when short sales are constrained.\n"
    },
    {
        "paper_id": 1511.01965,
        "authors": "Vikram Krishnamurthy and Sujay Bhatt",
        "title": "Sequential Detection of Market shocks using Risk-averse Agent Based\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers a statistical signal processing problem involving agent\nbased models of financial markets which at a micro-level are driven by socially\naware and risk- averse trading agents. These agents trade (buy or sell) stocks\nby exploiting information about the decisions of previous agents (social\nlearning) via an order book in addition to a private (noisy) signal they\nreceive on the value of the stock. We are interested in the following: (1)\nModelling the dynamics of these risk averse agents, (2) Sequential detection of\na market shock based on the behaviour of these agents. Structural results which\ncharacterize social learning under a risk measure, CVaR (Conditional\nValue-at-risk), are presented and formulation of the Bayesian change point\ndetection problem is provided. The structural results exhibit two interesting\nprop- erties: (i) Risk averse agents herd more often than risk neutral agents\n(ii) The stopping set in the sequential detection problem is non-convex. The\nframework is validated on data from the Yahoo! Tech Buzz game dataset.\n"
    },
    {
        "paper_id": 1511.02046,
        "authors": "Kuang-Ting Chen",
        "title": "Modeling Market Inefficiencies within a Single Instrument",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a minimal model beyond geometric Brownian motion\nthat aims to describe price actions with market inefficiency. From simple\nfinancial theory considerations, we arrive at a simple two-variable hidden\nMarkovian time series model, with one of the variable entirely unobserved.\nThen, we analyze the simplest version of the model, using path integral and\nGreen's function techniques from physics. We show that in this model, the\ninefficient market price is trend-following when the standard deviation of the\nlog reasonable price ($\\sigma$) is larger than that of the log market price\n($\\sigma'$), and mean-reversing when it is smaller. The risk premium is\nproportional to the difference between the current market price and the\nexponential moving average (EMA) of the past prices. This model thus provides a\ntheoretical explanation how the EMA of the past price can directly affect\nfuture prices, i.e., the so-called ``Bollinger bands\" in technical analyses. We\nthen carry out a maximum likelihood estimate for the model parameters from the\nobserved market price, by integrating out the reasonable price in Fourier\nspace. Finally we analyze recent S\\&P500 index data and see to what extent the\nreal world data can be described by this simple model.\n"
    },
    {
        "paper_id": 1511.02229,
        "authors": "Hela Jeddi, Dhafer Malouche",
        "title": "Wage gap between men and women in Tunisia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on estimating wage differences between males and females\nin Tunisia by using the Oaxaca-Blinder decomposition, a technical that isolates\nwage gap due to characteristics, from wage gap due to discrimination against\nwomen. The data used in the analysis is obtained from the Tunisian Population\nand Employment Survey 2005. It is estimated that, the gender wage gap is about\n19% and the results ascertain that the gender wage gap is mostly attributed to\ndiscrimination, especially to underestimation of females'caracteristics on the\nlabor market.\n"
    },
    {
        "paper_id": 1511.02716,
        "authors": "Samuel N. Cohen and Victor Fedyashov",
        "title": "Nash equilibria for non zero-sum ergodic stochastic differential games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider non zero-sum games where multiple players control\nthe drift of a process, and their payoffs depend on its ergodic behaviour. We\nestablish their connection with systems of Ergodic BSDEs, and prove the\nexistence of a Nash equilibrium under the generalised Isaac's conditions. We\nalso study the case of interacting players of different type.\n"
    },
    {
        "paper_id": 1511.02934,
        "authors": "Ivan Granito and Paolo De Angelis",
        "title": "Capital allocation and risk appetite under Solvency II framework",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.1.1136.8404",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to introduce a method for computing the allocated\nSolvency II Capital Requirement (SCR) of each Risk which the company is exposed\nto, taking in account for the diversification effect among different risks. The\nmethod suggested is based on the Euler principle. We show that it has very\nsuitable properties like coherence in the sense of Denault (2001) and RORAC\ncompatibility, and practical implications for the companies that use the\nstandard formula. Further, we show how this approach can be used to evaluate\nthe underwriting and reinsurance policies and to define a measure of the\nCompany's risk appetite, based on the capital at risk return.\n"
    },
    {
        "paper_id": 1511.03159,
        "authors": "Niushan Gao, Foivos Xanthos",
        "title": "On the C-property and $w^*$-representations of risk measures",
        "comments": "minor revisions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We identify a large class of Orlicz spaces $X$ for which the topology\n$\\sigma(X,X_n^\\sim)$ fails the C-property introduced in [7]. We also establish\na variant of the C-property and use it to prove a $w^*$-representation theorem\nfor proper convex increasing functionals on dual Banach lattices that satisfy a\nsuitable version of Delbaen's Fatou property. Our results apply, in particular,\nto risk measures on all Orlicz spaces over $[0,1]$ which is not $L_1[0,1]$.\n"
    },
    {
        "paper_id": 1511.03616,
        "authors": "Thibaut Mastrolia and Dylan Possama\\\"i",
        "title": "Moral hazard under ambiguity",
        "comments": "52 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the Holmstro\\\"om and Milgrom problem [47] by adding\nuncertainty about the volatility of the output for both the Agent and the\nPrincipal. We study more precisely the impact of the \"Nature\" playing against\nthe Agent and the Principal by choosing the worst possible volatility of the\noutput. We solve the first--best and the second--best problems associated with\nthis framework and we show that optimal contracts are in a class of contracts\nsimilar to [14, 15], linear with respect to the output and its quadratic\nvariation. We compare our results with the classical problem in [47].\n"
    },
    {
        "paper_id": 1511.03704,
        "authors": "Phillip G. Bradford",
        "title": "Foundations for Wash Sales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider an ephemeral sale-and-repurchase of a security resulting in the same\nposition before the sale and after the repurchase. A sale-and-repurchase is a\nwash sale if these transactions result in a loss within $\\pm 30$ calendar days.\nSince a portfolio is essentially the same after a wash sale, any tax advantage\nfrom such a loss is not allowed. That is, after a wash sale a portfolio is\nunchanged so any loss captured by the wash sale is deemed to be solely for tax\nadvantage and not investment purposes.\n  This paper starts by exploring variations of the birthday problem to model\nwash sales. The birthday problem is: Determine the number of independent and\nidentically distributed random variables required so there is a probability of\nat least 1/2 that two or more of these random variables share the same outcome.\nThis paper gives necessary conditions for wash sales based on variations on the\nbirthday problem. This allows us to answer questions such as: What is the\nlikelihood of a wash sale in an unmanaged portfolio where purchases and sales\nare independent, uniform, and random? This paper ends by exploring the\nLittlewood-Offord problem as it relates capital gains and losses with wash\nsales.\n"
    },
    {
        "paper_id": 1511.03732,
        "authors": "Felix Patzelt",
        "title": "Instability and Information",
        "comments": "Doctoral Dissertation. Department of Neurophysics, Institute for\n  Theoretical Physics, University of Bremen. First referee and supervisor:\n  Prof. Dr. Klaus Pawelzik. Second referee: Prof. Dr. Stefan Bornholdt.\n  Defence: June 4, 2014. Defence referees: Prof. Dr. Klaus Pawelzik, Prof. Dr.\n  Stefan Bornholdt, Prof. Dr. Monika Fritz, Prof. Dr. Dr. Ulrich Krause",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many complex systems exhibit extreme events far more often than expected for\na normal distribution. This work examines how self-similar bursts of activity\nacross several orders of magnitude can emerge from first principles in systems\nthat adapt to information. Surprising connections are found between two\napparently unrelated research topics: hand-eye coordination in balancing tasks\nand speculative trading in financial markets. Seemingly paradoxically, locally\nminimising fluctuations can increase a dynamical system's sensitivity to\nunpredictable perturbations and thereby facilitate global catastrophes. This\ngeneral principle is studied in several domain-specific models and in\nbehavioural experiments. It explains many findings in both fields and resolves\nan apparent antinomy: the coexistence of stabilising control or market\nefficiency and perpetual instabilities resembling critical phenomena in\nphysical systems.\n"
    },
    {
        "paper_id": 1511.03744,
        "authors": "Hyungbin Park",
        "title": "Sensitivity Analysis of Long-Term Cash Flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, a sensitivity analysis of long-term cash flows with respect\nto perturbations in the underlying process is presented. For this purpose, we\nemploy the martingale extraction through which a pricing operator is\ntransformed into what is easier to address. The method of Fournie et al. will\nbe combined with the martingale extraction. We prove that the sensitivity of\nlong-term cash flows can be represented in a simple form.\n"
    },
    {
        "paper_id": 1511.03777,
        "authors": "Liang Wu, Lei Zhang and Zhiming Fu",
        "title": "Deleveraging, short sale constraints and market crash",
        "comments": "19 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a theory of market crashes resulting from a\ndeleveraging shock. We consider two representative investors in a market\nholding different opinions about the public available information. The\ndeleveraging shock forces the high confidence investors to liquidate their\nrisky assets to pay back their margin loans. When short sales are constrained,\nthe deleveraging shock creates a liquidity vacuum in which no trades can occur\nbetween the two representative investors until the price drop to a threshold\nbelow which low confidence investors take over the reduced demands. There are\ntwo roles short sellers could play to stabilize the market. First, short\nsellers provide extra supply in a bullish market so that the price of the asset\nis settled lower than otherwise. Second, short sellers catch the falling price\nearlier in the deleveraging process if they are previously allowed to hold a\nlarger short position. We apply our model to explain the recent deleveraging\ncrisis of the Chinese market with great success.\n"
    },
    {
        "paper_id": 1511.03863,
        "authors": "Jan-Henrik Steg",
        "title": "Preemptive Investment under Uncertainty",
        "comments": "38 pages, 3 figures",
        "journal-ref": "Games and Economic Behavior, 110C (2018), pp. 90-119",
        "doi": "10.1016/j.geb.2018.03.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a general characterization of subgame perfect equilibria\nfor strategic timing problems, where two firms have the (real) option to make\nan irreversible investment. Profit streams are uncertain and depend on the\nmarket structure. The analysis is based directly on the inherent economic\nstructure of the model. In particular, the determination of equilibria with\npreemptive investment is reduced to solving a single class of constrained\noptimal stopping problems. The general results are applied to typical\nstate-space models, completing commonly insufficient equilibrium arguments,\nshowing when uncertainty leads to qualitatively different behavior, and\nestablishing additional equilibria that are Pareto improvements.\n"
    },
    {
        "paper_id": 1511.03876,
        "authors": "V\\'ictor Blanco and Jos\\'e M. P\\'erez-S\\'anchez",
        "title": "On the aggregation of experts' information in Bonus-Malus systems",
        "comments": "8 Tables; 3 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present in this paper a new premium computation principle based on the use\nof prior information from multiple sources for computing the premium charged to\na policyholder. Under this framework, based on the use of Ordered Weighted\nAveraging (OWA) operators, we propose alternative collective and Bayes premiums\nand describe some approaches to compute them. Several examples illustrates the\nnew framework for premium computation.\n"
    },
    {
        "paper_id": 1511.04096,
        "authors": "Xin Liu, Qi Gong, Vidyadhar G. Kulkarni",
        "title": "A Stochastic Model of Order Book Dynamics using Bouncing Geometric\n  Brownian Motions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a limit order book, where buyers and sellers register to trade a\nsecurity at specific prices. The largest price buyers on the book are willing\nto offer is called the market bid price, and the smallest price sellers on the\nbook are willing to accept is called the market ask price. Market ask price is\nalways greater than market bid price, and these prices move upwards and\ndownwards due to new arrivals, market trades, and cancellations. We model these\ntwo price processes as \"bouncing geometric Brownian motions (GBMs)\", which are\ndefined as exponentials of two mutually reflected Brownian motions. We then\nmodify these bouncing GBMs to construct a discrete time stochastic process of\ntrading times and trading prices, which is parameterized by a positive\nparameter $\\delta$. Under this model, it is shown that the inter-trading times\nare inverse Gaussian distributed, and the logarithmic returns between\nconsecutive trading times follow a normal inverse Gaussian distribution. Our\nmain results show that the logarithmic trading price process is a renewal\nreward process, and under a suitable scaling, this process converges to a\nstandard Brownian motion as $\\delta\\to 0$. We also prove that the modified ask\nand bid processes approach the original bouncing GBMs as $\\delta\\to0$. Finally,\nwe derive a simple and effective prediction formula for trading prices, and\nillustrate the effectiveness of the prediction formula with an example using\nreal stock price data.\n"
    },
    {
        "paper_id": 1511.04116,
        "authors": "Julius Bonart and Martin Gould",
        "title": "Latency and liquidity provision in a limit order book",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a recent, high-quality data set from Nasdaq to perform an empirical\nanalysis of order flow in a limit order book (LOB) before and after the arrival\nof a market order. For each of the stocks that we study, we identify a sequence\nof distinct phases across which the net flow of orders differs considerably. We\nnote some of our results are consist with the widely reported phenomenon of\nstimulated refill, but that others are not. We therefore propose alternative\nmechanical and strategic motivations for the behaviour that we observe. Based\non our findings, we argue that strategic liquidity providers consider both\nadverse selection and expected waiting costs when deciding how to act.\n"
    },
    {
        "paper_id": 1511.04218,
        "authors": "Jana Bielagk, Arnaud Lionnet and Goncalo Dos Reis",
        "title": "Equilibrium pricing under relative performance concerns",
        "comments": "46 pages, 11 pictures, currently submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the effects of the social interactions of a finite set of\nagents on an equilibrium pricing mechanism. A derivative written on\nnon-tradable underlyings is introduced to the market and priced in an\nequilibrium framework by agents who assess risk using convex dynamic risk\nmeasures expressed by Backward Stochastic Differential Equations (BSDE). Each\nagent is not only exposed to financial and non-financial risk factors, but she\nalso faces performance concerns with respect to the other agents.\n  Within our proposed model we prove the existence and uniqueness of an\nequilibrium whose analysis involves systems of fully coupled multi-dimensional\nquadratic BSDEs. We extend the theory of the representative agent by showing\nthat a non-standard aggregation of risk measures is possible via\nweighted-dilated infimal convolution. We analyze the impact of the problem's\nparameters on the pricing mechanism, in particular how the agents' performance\nconcern rates affect prices and risk perceptions. In extreme situations, we\nfind that the concern rates destroy the equilibrium while the risk measures\nthemselves remain stable.\n"
    },
    {
        "paper_id": 1511.04314,
        "authors": "Travis Fisher, Sergio Pulido, Johannes Ruf",
        "title": "Financial Models with Defaultable Num\\'eraires",
        "comments": "18 pages. Forthcoming in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial models are studied where each asset may potentially lose value\nrelative to any other. Conditioning on non-devaluation, each asset can serve as\nproper num\\'eraire and classical valuation rules can be formulated. It is shown\nwhen and how these local valuation rules can be aggregated to obtain global\narbitrage-free valuation formulas.\n"
    },
    {
        "paper_id": 1511.04764,
        "authors": "Zura Kakushadze",
        "title": "Shrinkage = Factor Model",
        "comments": "5 pages; a trivial typo corrected; to appear as an Invited Editorial\n  in Journal of Asset Management",
        "journal-ref": "Journal of Asset Management 17(2) (2016) 69-72, Invited Editorial",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Shrunk sample covariance matrix is a factor model of a special form combining\nsome (typically, style) risk factor(s) and principal components with a\n(block-)diagonal factor covariance matrix. As such, shrinkage, which\nessentially inherits out-of-sample instabilities of the sample covariance\nmatrix, is not an alternative to multifactor risk models but one out of myriad\npossible regularization schemes. We give an example of a scheme designed to be\nless prone to said instabilities. We contextualize this within multifactor\nmodels.\n"
    },
    {
        "paper_id": 1511.04768,
        "authors": "Bin Zou and Rudi Zagst",
        "title": "Optimal Investment with Transaction Costs under Cumulative Prospect\n  Theory in Discrete Time",
        "comments": "34 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal investment problems under the framework of cumulative\nprospect theory (CPT). A CPT investor makes investment decisions in a\nsingle-period financial market with transaction costs. The objective is to seek\nthe optimal investment strategy that maximizes the prospect value of the\ninvestor's final wealth. We obtain the optimal investment strategy explicitly\nin two examples. An economic analysis is conducted to investigate the impact of\nthe transaction costs and risk aversion on the optimal investment strategy.\n"
    },
    {
        "paper_id": 1511.04863,
        "authors": "Gechun Liang, Thaleia Zariphopoulou",
        "title": "Representation of homothetic forward performance processes in stochastic\n  factor models via ergodic and infinite horizon BSDE",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an incomplete market, with incompleteness stemming from stochastic factors\nimperfectly correlated with the underlying stocks, we derive representations of\nhomothetic (power, exponential and logarithmic) forward performance processes\nin factor-form using ergodic BSDE. We also develop a connection between the\nforward processes and infinite horizon BSDE, and, moreover, with risk-sensitive\noptimization. In addition, we develop a connection, for large time horizons,\nwith a family of classical homothetic value function processes with random\nendowments.\n"
    },
    {
        "paper_id": 1511.04935,
        "authors": "Jamie Fairbrother, Amanda Turner, Stein Wallace",
        "title": "Scenario generation for single-period portfolio selection problems with\n  tail risk measures: coping with high dimensions and integer variables",
        "comments": null,
        "journal-ref": "Informs Journal on Computing 30(3):472-491, 2018",
        "doi": "10.1287/ijoc.2017.0790",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a problem-driven scenario generation approach to the\nsingle-period portfolio selection problem which use tail risk measures such as\nconditional value-at-risk. Tail risk measures are useful for quantifying\npotential losses in worst cases. However, for scenario-based problems these are\nproblematic: because the value of a tail risk measure only depends on a small\nsubset of the support of the distribution of asset returns, traditional\nscenario based methods, which spread scenarios evenly across the whole support\nof the distribution, yield very unstable solutions unless we use a very large\nnumber of scenarios. The proposed approach works by prioritizing the\nconstruction of scenarios in the areas of a probability distribution which\ncorrespond to the tail losses of feasible portfolios.\n  The proposed approach can be applied to difficult instances of the portfolio\nselection problem characterized by high-dimensions, non-elliptical\ndistributions of asset returns, and the presence of integer variables. It is\nalso observed that the methodology works better as the feasible set of\nportfolios becomes more constrained. Based on this fact, a heuristic algorithm\nbased on the sample average approximation method is proposed. This algorithm\nworks by adding artificial constraints to the problem which are gradually\ntightened, allowing one to telescope onto high quality solutions.\n"
    },
    {
        "paper_id": 1511.0495,
        "authors": "Xun Li, Ping Lin, Xue-Cheng Tai and Jinghui Zhou",
        "title": "Pricing Two-asset Options under Exponential L\\'evy Model Using a Finite\n  Element Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a finite element method (FEM) for a partial\nintegro-differential equation (PIDE) to price two-asset options with underlying\nprice processes modeled by an exponential Levy process. We provide a\nvariational formulation in a weighted Sobolev space, and establish existence\nand uniqueness of the FEM-based solution. Then we discuss the localization of\nthe infinite domain problem to a finite domain and analyze its error. We tackle\nthe localized problem by an explicit-implicit time-discretization of the PIDE,\nwhere the space-discretization is done through a standard continuous finite\nelement method. Error estimates are given for the fully discretized localized\nproblem where two assets are assumed to have uncorrelated jumps. Numerical\nexperiments for the polynomial option and a few other two-asset options shed\nlight on good performance of our proposed method.\n"
    },
    {
        "paper_id": 1511.05303,
        "authors": "Hans Colonius",
        "title": "An invitation to coupling and copulas: with applications to multisensory\n  modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an introduction to the stochastic concepts of\n\\emph{coupling} and \\emph{copula}. Coupling means the construction of a joint\ndistribution of two or more random variables that need not be defined on one\nand the same probability space, whereas a copula is a function that joins a\nmultivariate distribution to its one-dimensional margins. Their role in\nstochastic modeling is illustrated by examples from multisensory perception.\nPointers to more advanced and recent treatments are provided.\n"
    },
    {
        "paper_id": 1511.05404,
        "authors": "Alexandre Vidmer, An Zeng, Mat\\'u\\v{s} Medo and Yi-Cheng Zhang",
        "title": "Prediction in complex systems: the case of the international trade\n  network",
        "comments": null,
        "journal-ref": "Vidmer, A., Zeng, A., Medo, M., & Zhang, Y. C. (2015). Prediction\n  in complex systems: The case of the international trade network. Physica A:\n  Statistical Mechanics and its Applications, 436, 188-199",
        "doi": "10.1016/j.physa.2015.05.057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the future evolution of complex systems is one of the main\nchallenges in complexity science. Based on a current snapshot of a network,\nlink prediction algorithms aim to predict its future evolution. We apply here\nlink prediction algorithms to data on the international trade between\ncountries. This data can be represented as a complex network where links\nconnect countries with the products that they export. Link prediction\ntechniques based on heat and mass diffusion processes are employed to obtain\npredictions for products exported in the future. These baseline predictions are\nimproved using a recent metric of country fitness and product similarity. The\noverall best results are achieved with a newly developed metric of product\nsimilarity which takes advantage of causality in the network evolution.\n"
    },
    {
        "paper_id": 1511.05465,
        "authors": "Claudia Ceci, Katia Colaneri and Alessandra Cretarola",
        "title": "The F\\\"ollmer-Schweizer decomposition under incomplete information",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": "10.1080/17442508.2017.1290094",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the F\\\"ollmer-Schweizer decomposition of a square\nintegrable random variable $\\xi$ with respect to a given semimartingale $S$\nunder restricted information. Thanks to the relationship between this\ndecomposition and that of the projection of $\\xi$ with respect to the given\ninformation flow, we characterize the integrand appearing in the\nF\\\"ollmer-Schweizer decomposition under partial information in the general case\nwhere $\\xi$ is not necessarily adapted to the available information level. For\npartially observable Markovian models where the dynamics of $S$ depends on an\nunobservable stochastic factor $X$, we show how to compute the decomposition by\nmeans of filtering problems involving functions defined on an\ninfinite-dimensional space. Moreover, in the case of a partially observed\njump-diffusion model where $X$ is described by a pure jump process taking\nvalues in a finite dimensional space, we compute explicitly the integrand in\nthe F\\\"ollmer-Schweizer decomposition by working with finite dimensional\nfilters.\n"
    },
    {
        "paper_id": 1511.05661,
        "authors": "Karol Duris, Shih-Hau Tan, Choi-Hong Lai, Daniel Sevcovic",
        "title": "Comparison of the analytical approximation formula and Newton's method\n  for solving a class of nonlinear Black-Scholes parabolic equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market illiquidity, feedback effects, presence of transaction costs, risk\nfrom unprotected portfolio and other nonlinear effects in PDE based option\npricing models can be described by solutions to the generalized Black-Scholes\nparabolic equation with a diffusion term nonlinearly depending on the option\nprice itself. Different linearization techniques such as Newton's method and\nanalytic asymptotic approximation formula are adopted and compared for a wide\nclass of nonlinear Black-Scholes equations including, in particular, the market\nilliquidity model and the risk-adjusted pricing model. Accuracy and time\ncomplexity of both numerical methods are compared. Furthermore, market quotes\ndata was used to calibrate model parameters.\n"
    },
    {
        "paper_id": 1511.05948,
        "authors": "Matyas Barczy, Balazs Nyul, Gyula Pap",
        "title": "Least squares estimation for the subcritical Heston model based on\n  continuous time observations",
        "comments": "22 pages. arXiv admin note: text overlap with arXiv:1310.4783",
        "journal-ref": "Journal of Statistical Theory and Practice 3(1), (2019), 13:18",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove strong consistency and asymptotic normality of least squares\nestimators for the subcritical Heston model based on continuous time\nobservations. We also present some numerical illustrations of our results.\n"
    },
    {
        "paper_id": 1511.06032,
        "authors": "Renjie Wang, Cody Hyndman and Anastasis Kratsios",
        "title": "The Entropic Measure Transform",
        "comments": "32 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the entropic measure transform (EMT) problem for a general\nprocess and prove the existence of a unique optimal measure characterizing the\nsolution. The density process of the optimal measure is characterized using a\nsemimartingale BSDE under general conditions. The EMT is used to reinterpret\nthe conditional entropic risk-measure and to obtain a convenient formula for\nthe conditional expectation of a process which admits an affine representation\nunder a related measure. The entropic measure transform is then used provide a\nnew characterization of defaultable bond prices, forward prices, and futures\nprices when the asset is driven by a jump diffusion. The characterization of\nthese pricing problems in terms of the EMT provides economic interpretations as\na maximization of returns subject to a penalty for removing financial risk as\nexpressed through the aggregate relative entropy. The EMT is shown to extend\nthe optimal stochastic control characterization of default-free bond prices of\nGombani and Runggaldier (Math. Financ. 23(4):659-686, 2013). These methods are\nillustrated numerically with an example in the defaultable bond setting.\n"
    },
    {
        "paper_id": 1511.0632,
        "authors": "Andreas Haier, Ilya Molchanov and Michael Schmutz",
        "title": "Intragroup transfers, intragroup diversification and their risk\n  assessment",
        "comments": "20 pages, 3 figures. Revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When assessing group solvency, an important question is to what extent\nintragroup transfers may be considered, as this determines to which extent\ndiversification can be achieved. We suggest a framework to describe the\nfamilies of admissible transfers that range from the free movement of capital\nto excluding any transactions. The constraints on admissible transactions are\ndescribed as random closed sets. The paper focuses on the corresponding\nsolvency tests that amount to the existence of acceptable selections of the\nrandom sets of admissible transactions.\n"
    },
    {
        "paper_id": 1511.06454,
        "authors": "Nina Anchugina",
        "title": "A simple framework for the axiomatization of exponential and\n  quasi-hyperbolic discounting",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": "10.1007/s11238-016-9566-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main goal of this paper is to investigate which normative requirements,\nor axioms, lead to exponential and quasi-hyperbolic forms of discounting.\nExponential discounting has a well-established axiomatic foundation originally\ndeveloped by Koopmans (1960, 1972) and Koopmans et al. (1964) with subsequent\ncontributions by several other authors, including Bleichrodt et al. (2008). The\npapers by Hayashi (2003) and Olea and Strzalecki (2014) axiomatize\nquasi-hyperbolic discounting. The main contribution of this paper is to provide\nan alternative foundation for exponential and quasi-hyperbolic discounting,\nwith simple, transparent axioms and relatively straightforward proofs. Using\ntechniques by Fishburn (1982) and Harvey (1986), we show that Anscombe and\nAumann's (1963) version of Subjective Expected Utility theory can be readily\nadapted to axiomatize the aforementioned types of discounting, in both finite\nand infinite horizon settings.\n"
    },
    {
        "paper_id": 1511.06734,
        "authors": "Emmanuel Haven and Sandro Sozzo",
        "title": "A Generalized Probability Framework to Model Economic Agents' Decisions\n  Under Uncertainty",
        "comments": "13 pages, standard Latex. arXiv admin note: text overlap with\n  arXiv:1302.3850",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The applications of techniques from statistical (and classical) mechanics to\nmodel interesting problems in economics and finance has produced valuable\nresults. The principal movement which has steered this research direction is\nknown under the name of `econophysics'. In this paper, we illustrate and\nadvance some of the findings that have been obtained by applying the\nmathematical formalism of quantum mechanics to model human decision making\nunder `uncertainty' in behavioral economics and finance. Starting from\nEllsberg's seminal article, decision making situations have been experimentally\nverified where the application of Kolmogorovian probability in the formulation\nof expected utility is problematic. Those probability measures which by\nnecessity must situate themselves in Hilbert space (such as `quantum\nprobability') enable a faithful representation of experimental data. We thus\nprovide an explanation for the effectiveness of the mathematical framework of\nquantum mechanics in the modeling of human decision making. We want to be\nexplicit though that we are not claiming that decision making has microscopic\nquantum mechanical features.\n"
    },
    {
        "paper_id": 1511.0687,
        "authors": "Luca Marotta, Salvatore Miccich\\`e, Yoshi Fujiwara, Hiroshi Iyetomi,\n  Hideaki Aoyama, Mauro Gallegati, Rosario N. Mantegna",
        "title": "Backbone of credit relationships in the Japanese credit market",
        "comments": "14 pages, 8 figures",
        "journal-ref": "EPJ Data Science, 5 (10), 1-14, (2016)",
        "doi": "10.1140/epjds/s13688-016-0071-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We detect the backbone of the weighted bipartite network of the Japanese\ncredit market relationships. The backbone is detected by adapting a general\nmethod used in the investigation of weighted networks. With this approach we\ndetect a backbone that is statistically validated against a null hypothesis of\nuniform diversification of loans for banks and firms. Our investigation is done\nyear by year and it covers more than thirty years during the period from 1980\nto 2011. We relate some of our findings with economic events that have\ncharacterized the Japanese credit market during the last years. The study of\nthe time evolution of the backbone allows us to detect changes occurred in\nnetwork size, fraction of credit explained, and attributes characterizing the\nbanks and the firms present in the backbone.\n"
    },
    {
        "paper_id": 1511.06873,
        "authors": "Federico Musciotto, Luca Marotta, Salvatore Miccich\\`e, Jyrki Piilo,\n  Rosario N. Mantegna",
        "title": "Patterns of trading profiles at the Nordic Stock Exchange. A\n  correlation-based approach",
        "comments": "25 pages, 8 figures",
        "journal-ref": "Chaos Solitons and Fractal, 88, 267-278, (2016)",
        "doi": "10.1016/j.chaos.2016.02.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the trading behavior of Finnish individual investors trading\nthe stocks selected to compute the OMXH25 index in 2003 by tracking the\nindividual daily investment decisions. We verify that the set of investors is a\nhighly heterogeneous system under many aspects. We introduce a correlation\nbased method that is able to detect a hierarchical structure of the trading\nprofiles of heterogeneous individual investors. We verify that the detected\nhierarchical structure is highly overlapping with the cluster structure\nobtained with the approach of statistically validated networks when an\nappropriate threshold of the hierarchical trees is used. We also show that the\ncombination of the correlation based method and of the statistically validated\nmethod provides a way to expand the information about the clusters of investors\nwith similar trading profiles in a robust and reliable way.\n"
    },
    {
        "paper_id": 1511.06943,
        "authors": "Marcelo Brutti Righi",
        "title": "A composition between risk and deviation measures",
        "comments": null,
        "journal-ref": "Annals of Operations Research (2019) 282, 299-313",
        "doi": "10.1007/s10479-018-2913-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The intuition of risk is based on two main concepts: loss and variability. In\nthis paper, we present a composition of risk and deviation measures, which\ncontemplate these two concepts. Based on the proposed Limitedness axiom, we\nprove that this resulting composition, based on properties of the two\ncomponents, is a coherent risk measure. Similar results for the cases of convex\nand co-monotone risk measures are exposed. We also provide examples of known\nand new risk measures constructed under this framework in order to highlight\nthe importance of our approach, especially the role of the Limitedness axiom.\n"
    },
    {
        "paper_id": 1511.06992,
        "authors": "Ron W Nielsen",
        "title": "Early Warning Signs of the Economic Crisis in Greece: A Warning for\n  Other Countries and Regions",
        "comments": "7 pages, 4 figures, 2162 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Warning signs about the developing economic crisis in Greece were present in\nthe growth rate of the Gross Domestic Product (GDP) and in the growth of the\nGDP well before the economic collapse. The growth rate was strongly unstable.\nOn average, in less than 50 years, it decreased 10-folds but after reaching a\nlow minimum it quickly increased 6-folds only to crash before completing the\nfull cycle. The decreasing growth rate was leading to an asymptotic maximum of\nthe GDP but it was soon replaced by a fast-increasing growth rate propelling\nthe GDP along a pseudo-hyperbolic trajectory, which if continued would have\nescaped to infinity in 2017. Such a growth could not have been possibly\nsupported. Under these conditions, the economic collapse in Greece was\ninevitable.\n"
    },
    {
        "paper_id": 1511.07101,
        "authors": "Linh Nghiem",
        "title": "Risk-return relationship: An empirical study of different statistical\n  methods for estimating the Capital Asset Pricing Models (CAPM) and the\n  Fama-French model for large cap stocks",
        "comments": "Undergraduate thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Capital Asset Pricing Model (CAPM) is one of the original models in\nexplaining risk-return relationship in the financial market. However, when\napplying the CAPM into reality, it demonstrates a lot of shortcomings. While\nimproving the performance of the model, many studies, on one hand, have\nattempted to apply different statistical methods to estimate the model, on the\nother hand, have added more predictors to the model. First, the thesis focuses\non reviewing the CAPM and comparing popular statistical methods used to\nestimate it, and then, the thesis compares predictive power of the CAPM and the\nFama-French model, which is an important extension of the CAPM. Through an\nempirical study on the data set of large cap stocks, we have demonstrated that\nthere is no statistical method that would recover the expected relationship\nbetween systematic risk (represented by beta) and return from the CAPM, and\nthat the Fama-French model does not have a better predictive performance than\nthe CAPM on individual stocks. Therefore, the thesis provides more evidence to\nsupport the incorrectness of the CAPM and the limitation of the Fama-French\nmodel in explaining risk-return relationship.\n"
    },
    {
        "paper_id": 1511.07203,
        "authors": "Jan A. Audestad",
        "title": "Some Dynamic Market Models",
        "comments": "48 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this text, we study the temporal behavior of markets using models\nexpressible as ordinary differential equations. The markets studied are those\nwhere each customer buys only one copy of the good, for example, subscription\nof smartphone service, journals and newspapers, and goods such as books, music\nand games. The underlying model is the diffusion model of Frank Bass. Evolution\nof markets with no competitors and markets with several competitors are\nanalyzed where, in particulat, the effects of churning upon the market\nevolution is investigated. Analytic solutions are given for the temporal\nevolution of several types of interactive games.\n"
    },
    {
        "paper_id": 1511.0723,
        "authors": "Julien Claisse, Gaoyue Guo, Pierre Henry-Labordere",
        "title": "Some Results on Skorokhod Embedding and Robust Hedging with Local Time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide some results on Skorokhod embedding with local time\nand its applications to the robust hedging problem in finance. First we\ninvestigate the robust hedging of options depending on the local time by using\nthe recently introduced stochastic control approach, in order to identify the\noptimal hedging strategies, as well as the market models that realize the\nextremal no-arbitrage prices. As a by-product, the optimality of Vallois'\nSkorokhod embeddings is recovered. In addition, under appropriate conditions,\nwe derive a new solution to the two-marginal Skorokhod embedding as a\ngeneralization of the Vallois solution. It turns out from our analysis that one\nneeds to relax the monotonicity assumption on the embedding functions in order\nto embed a larger class of marginal distributions. Finally, in a full-marginal\nsetting where the stopping times given by Vallois are well-ordered, we\nconstruct a remarkable Markov martingale which provides a new example of fake\nBrownian motion.\n"
    },
    {
        "paper_id": 1511.07359,
        "authors": "A. Rej, R. Benichou, J. de Lataillade, G. Z\\'erah, J.-Ph. Bouchaud",
        "title": "Optimal Trading with Linear and (small) Non-Linear Costs",
        "comments": "16 pages, 1 figure, paragraph added to explain the relation of our\n  results with arXiv:1402.5306",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reconsider the problem of optimal trading in the presence of linear and\nquadratic costs, for arbitrary linear costs but in the limit where quadratic\ncosts are small. Using matched asymptotic expansion techniques, we find that\nthe trading speed vanishes inside a band that is narrower than in the absence\nof quadratic costs, by an amount that scales as the one-third power of\nquadratic costs. Outside of the band, we find three regimes: a small boundary\nlayer where the velocity vanishes linearly with the distance to the band, an\nintermediate region where the velocity behaves as a square-root of that\ndistance, and a far region where it becomes linear. Our solution is consistent\nwith available numerical results. We determine the conditions in which our\nexpansion is useful in practical applications, and generalize our solution to\nother forms of non-linear costs.\n"
    },
    {
        "paper_id": 1511.07419,
        "authors": "Rabi Bhattacharya, Hyeonju Kim, Mukul Majumdar",
        "title": "Sustainability in the Stochastic Ramsey Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a self-contained exposition of the problem of\nsustaining a constant consumption level in a Ramsey model. Our focus is on the\ncase in which the output capital-ratio is random. After a brief review of the\nknown results on the probabilities of sustaining a target consumption from an\ninitial stock, we present some new results on estimating the probabilities by\nusing Chebyshev inequalities. Some numerical calculations for these estimates\nare also provided.\n"
    },
    {
        "paper_id": 1511.07773,
        "authors": "Jean-David Fermanian, Olivier Gu\\'eant, Jiang Pu",
        "title": "The behavior of dealers and clients on the European corporate bond\n  market: the case of Multi-Dealer-to-Client platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the last two decades, most financial markets have undergone an evolution\ntoward electronification. The market for corporate bonds is one of the last\nmajor financial markets to follow this unavoidable path. Traditionally\nquote-driven i.e., dealer-driven) rather than order-driven, the market for\ncorporate bonds is still mainly dominated by voice trading, but a lot of\nelectronic platforms have emerged. These electronic platforms make it possible\nfor buy-side agents to simultaneously request several dealers for quotes, or\neven directly trade with other buy-siders. The research presented in this\narticle is based on a large proprietary database of requests for quotes (RFQ)\nsent, through the multi-dealer-to-client (MD2C) platform operated by Bloomberg\nFixed Income Trading, to one of the major liquidity providers in European\ncorporate bonds. Our goal is (i) to model the RFQ process on these platforms\nand the resulting competition between dealers, and (ii) to use our model in\norder to implicit from the RFQ database the behavior of both dealers and\nclients on MD2C platforms.\n"
    },
    {
        "paper_id": 1511.07821,
        "authors": "Ting Ting Chen, Tetsuya Takaishi",
        "title": "Box-Cox transformation of firm size data in statistical analysis",
        "comments": "4 pages, 9 figures",
        "journal-ref": "Journal of Physics: Conference Series 490 (2014) 012182",
        "doi": "10.1088/1742-6596/490/1/012182",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Firm size data usually do not show the normality that is often assumed in\nstatistical analysis such as regression analysis. In this study we focus on two\nfirm size data: the number of employees and sale. Those data deviate\nconsiderably from a normal distribution. To improve the normality of those data\nwe transform them by the Box-Cox transformation with appropriate parameters.\nThe Box-Cox transformation parameters are determined so that the transformed\ndata best show the kurtosis of a normal distribution. It is found that the two\nfirm size data transformed by the Box-Cox transformation show strong linearity.\nThis indicates that the number of employees and sale have the similar property\nas a firm size indicator. The Box-Cox parameters obtained for the firm size\ndata are found to be very close to zero. In this case the Box-Cox\ntransformations are approximately a log-transformation. This suggests that the\nfirm size data we used are approximately log-normal distributions.\n"
    },
    {
        "paper_id": 1511.07945,
        "authors": "Hannah Cheng Juan Zhan, William Rea, and Alethea Rea",
        "title": "An Application of Correlation Clustering to Portfolio Diversification",
        "comments": "33 pages, 11 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel application of a clustering algorithm developed\nfor constructing a phylogenetic network to the correlation matrix for 126\nstocks listed on the Shanghai A Stock Market. We show that by visualizing the\ncorrelation matrix using a Neighbor-Net network and using the circular ordering\nproduced during the construction of the network we can reduce the risk of a\ndiversified portfolio compared with random or industry group based selection\nmethods in times of market increase.\n"
    },
    {
        "paper_id": 1511.08068,
        "authors": "Paolo Barucca and Fabrizio Lillo",
        "title": "The organization of the interbank network and how ECB unconventional\n  measures affected the e-MID overnight market",
        "comments": "33 pages, 5 figures",
        "journal-ref": "Comput Manag Sci (2017)",
        "doi": "10.1007/s10287-017-0293-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The topological properties of interbank networks have been discussed widely\nin the literature mainly because of their relevance for systemic risk. Here we\npropose to use the Stochastic Block Model to investigate and perform a model\nselection among several possible two block organizations of the network: these\ninclude bipartite, core-periphery, and modular structures. We apply our method\nto the e-MID interbank market in the period 2010-2014 and we show that in\nnormal conditions the most likely network organization is a bipartite\nstructure. In exceptional conditions, such as after LTRO, one of the most\nimportant unconventional measures by ECB at the beginning of 2012, the most\nlikely structure becomes a random one and only in 2014 the e-MID market went\nback to a normal bipartite organization. By investigating the strategy of\nindividual banks, we explore possible explanations and we show that the\ndisappearance of many lending banks and the strategy switch of a very small set\nof banks from borrower to lender is likely at the origin of this structural\nchange.\n"
    },
    {
        "paper_id": 1511.08194,
        "authors": "Rafa{\\l} M. {\\L}ochowski",
        "title": "Integration with respect to model-free price paths with jumps",
        "comments": "This paper has been withdrawn by the author since the result\n  contained in the paper is now presented in more comprehensive paper\n  arxiv:1609.02349",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For every adapted, c\\`agl\\`ad process (strategy) $G$ and typical c\\`adl\\`ag\nprice paths whose jumps satisfy some mild growth condition we define integral\n$G\\cdot S$ as a limit of simple integrals.\n"
    },
    {
        "paper_id": 1511.08349,
        "authors": "Jacopo Mancin and Wolfgang J. Runggaldier",
        "title": "On the Existence of Martingale Measures in Jump Diffusion Market Models",
        "comments": "A version has appeared in \"Arbitrage, Credit and Informational\n  Risks\", Peking University Series in Mathematics Vol.5, World Scientific 2014.\n  Arbitrage, Credit and Informational Risks, (C. Hillairet, M. Jeanblanc, Y.\n  Jiao, eds.). Peking University Series in Mathematics, Vol.5, World Scientific\n  Publishing Co. Pte. Ltd., 2014, pp.29-51",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of jump-diffusion market models we construct examples that\nsatisfy the weaker no-arbitrage condition of NA1 (NUPBR), but not NFLVR. We\nshow that in these examples the only candidate for the density process of an\nequivalent local martingale measure is a supermartingale that is not a\nmartingale, not even a local martingale. This candidate is given by the\nsupermartingale deflator resulting from the inverse of the discounted growth\noptimal portfolio. In particular, we con- sider an example with constraints on\nthe portfolio that go beyond the standard ones for admissibility.\n"
    },
    {
        "paper_id": 1511.08409,
        "authors": "Joaquin Fernandez-Tapia, Olivier Gu\\'eant, Jean-Michel Lasry",
        "title": "Optimal Real-Time Bidding Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ad-trading desks of media-buying agencies are increasingly relying on\ncomplex algorithms for purchasing advertising inventory. In particular,\nReal-Time Bidding (RTB) algorithms respond to many auctions -- usually Vickrey\nauctions -- throughout the day for buying ad-inventory with the aim of\nmaximizing one or several key performance indicators (KPI). The optimization\nproblems faced by companies building bidding strategies are new and interesting\nfor the community of applied mathematicians. In this article, we introduce a\nstochastic optimal control model that addresses the question of the optimal\nbidding strategy in various realistic contexts: the maximization of the\ninventory bought with a given amount of cash in the framework of audience\nstrategies, the maximization of the number of conversions/acquisitions with a\ngiven amount of cash, etc. In our model, the sequence of auctions is modeled by\na Poisson process and the \\textit{price to beat} for each auction is modeled by\na random variable following almost any probability distribution. We show that\nthe optimal bids are characterized by a Hamilton-Jacobi-Bellman equation, and\nthat almost-closed form solutions can be found by using a fluid limit.\nNumerical examples are also carried out.\n"
    },
    {
        "paper_id": 1511.08449,
        "authors": "Poulomi Ganguli, Devashish Kumar, and Auroop R. Ganguly",
        "title": "Water Stress on U.S. Power Production at Decadal Time Horizons",
        "comments": "Tech Report prepared for Advanced Research Projects Agency Energy,\n  United States Department of Energy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thermoelectric power production at risk, owing to current and projected water\nscarcity and rising stream temperatures, is assessed for the contiguous United\nStates at decadal scales. Regional water scarcity is driven by climate\nvariability and change, as well as by multi-sector water demand. While a\nplanning horizon of zero to about thirty years is occasionally prescribed by\nstakeholders, the challenges to risk assessment at these scales include the\ndifficulty in delineating decadal climate trends from intrinsic natural or\nmultiple model variability. Current generation global climate or earth system\nmodels are not credible at the spatial resolutions of power plants, especially\nfor surface water quantity and stream temperatures, which further exacerbates\nthe assessment challenge. Population changes, which are difficult to project,\ncannot serve as adequate proxies for changes in the water demand across\nsectors. The hypothesis that robust assessments of power production at risk are\npossible, despite the uncertainties, has been examined as a proof of concept.\nAn approach is presented for delineating water scarcity and temperature from\nclimate models, observations and population storylines, as well as for\nassessing power production at risk by examining geospatial correlations of\npower plant locations within regions where the usable water supply for energy\nproduction happens to be scarcer and warmer. Our analyses showed that in the\nnear term, more than 200 counties are likely to be exposed to water scarcity in\nthe next three decades. Further, we noticed that stream gauges in more than\nfive counties in the 2030s and ten counties in the 2040s showed a significant\nincrease in water temperature, which exceeded the power plant effluent\ntemperature threshold set by the EPA. Power plants in South Carolina,\nLouisiana, and Texas are likely to be vulnerable owing to climate-driven water\nstresses.\n"
    },
    {
        "paper_id": 1511.08466,
        "authors": "Zorana Grbac, David Krief and Peter Tankov",
        "title": "Approximate Option Pricing in the L\\'evy Libor Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the pricing of options on interest rates such as\ncaplets and swaptions in the L\\'evy Libor model developed by Eberlein and\n\\\"Ozkan (2005). This model is an extension to L\\'evy driving processes of the\nclassical log-normal Libor market model (LMM) driven by a Brownian motion.\nOption pricing is significantly less tractable in this model than in the LMM\ndue to the appearance of stochastic terms in the jump part of the driving\nprocess when performing the measure changes which are standard in pricing of\ninterest rate derivatives. To obtain explicit approximation for option prices,\nwe propose to treat a given L\\'evy Libor model as a suitable perturbation of\nthe log-normal LMM. The method is inspired by recent works by Cern\\'y, Denkl\nand Kallsen (2013) and M\\'enass\\'e and Tankov (2015). The approximate option\nprices in the L\\'evy Libor model are given as the corresponding LMM prices plus\ncorrection terms which depend on the characteristics of the underlying L\\'evy\nprocess and some additional terms obtained from the LMM model.\n"
    }
]