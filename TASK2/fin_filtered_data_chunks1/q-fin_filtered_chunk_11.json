[
    {
        "paper_id": 2005.12473,
        "authors": "Roba Bairakdar, Lu Cao, Melina Mailhot",
        "title": "Range Value-at-Risk: Multivariate and Extreme Values",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of univariate Range Value-at-Risk, presented by Cont et al.\n(2010), is extended in the multidimensional setting. Traditional risk measures\nare not well suited when dealing with heavy-tail distributions and infinite\ntail expectations. The multivariate definitions of robust truncated tail\nexpectations are provided to overcome this problem. Robustness and other\nproperties as well as empirical estimators are derived. Closed-form expressions\nand special cases in the extreme value framework are also discussed. Numerical\nand graphical examples are provided to examine the accuracy of the empirical\nestimators.\n"
    },
    {
        "paper_id": 2005.12483,
        "authors": "Xin Man and Ernest Chan",
        "title": "The best way to select features?",
        "comments": "8 pages, submitted to ACM International Conference on AI in Finance\n  (ICAIF-2020)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Feature selection in machine learning is subject to the intrinsic randomness\nof the feature selection algorithms (for example, random permutations during\nMDA). Stability of selected features with respect to such randomness is\nessential to the human interpretability of a machine learning algorithm. We\nproposes a rank based stability metric called instability index to compare the\nstabilities of three feature selection algorithms MDA, LIME, and SHAP as\napplied to random forests. Typically, features are selected by averaging many\nrandom iterations of a selection algorithm. Though we find that the variability\nof the selected features does decrease as the number of iterations increases,\nit does not go to zero, and the features selected by the three algorithms do\nnot necessarily converge to the same set. We find LIME and SHAP to be more\nstable than MDA, and LIME is at least as stable as SHAP for the top ranked\nfeatures. Hence overall LIME is best suited for human interpretability.\nHowever, the selected set of features from all three algorithms significantly\nimproves various predictive metrics out of sample, and their predictive\nperformances do not differ significantly. Experiments were conducted on\nsynthetic datasets, two public benchmark datasets, and on proprietary data from\nan active investment strategy.\n"
    },
    {
        "paper_id": 2005.12572,
        "authors": "Alessandro Doldi and Marco Frittelli",
        "title": "Entropy Martingale Optimal Transport and Nonlinear Pricing-Hedging\n  Duality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this paper is to develop a duality between a novel Entropy\nMartingale Optimal Transport problem (A) and an associated optimization problem\n(B). In (A) we follow the approach taken in the Entropy Optimal Transport (EOT)\nprimal problem by Liero et al. \"Optimal entropy-transport problems and a new\nHellinger-Kantorovic distance between positive measures\", Invent. math. 2018,\nbut we add the constraint, typical of Martingale Optimal Transport (MOT)\ntheory, that the infimum of the cost functional is taken over martingale\nprobability measures, instead of finite positive measures, as in Liero et al..\nThe Problem (A) differs from the corresponding problem in Liero et al. not only\nby the martingale constraint, but also because we admit less restrictive\npenalization terms $\\mathcal{D}_{U}$, which may not have a divergence\nformulation. In Problem (B) the objective functional, associated via Fenchel\nconjugacy to the terms $\\mathcal{D}_{U}$, is not any more linear, as in OT or\nin MOT. This leads to a novel optimization problem which also has a clear\nfinancial interpretation as a nonlinear subhedging value. Our theory allows us\nto establish a nonlinear robust pricing-hedging duality, which covers a wide\nrange of known robust results. We also focus on Wasserstein-induced\npenalizations and we study how the duality is affected by variations in the\npenalty terms, with a special focus on the convergence of EMOT to the extreme\ncase of MOT.\n"
    },
    {
        "paper_id": 2005.12593,
        "authors": "Bruno Bouchard (CEREMADE), Adil Reghai, Benjamin Virrion (CEREMADE)",
        "title": "Computation of Expected Shortfall by fast detection of worst scenarios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a multi-step algorithm for the computation of the historical\nexpected shortfall such as defined by the Basel Minimum Capital Requirements\nfor Market Risk. At each step of the algorithm, we use Monte Carlo simulations\nto reduce the number of historical scenarios that potentially belong to the set\nof worst scenarios. The number of simulations increases as the number of\ncandidate scenarios is reduced and the distance between them diminishes. For\nthe most naive scheme, we show that the L p-error of the estimator of the\nExpected Shortfall is bounded by a linear combination of the probabilities of\ninversion of favorable and unfavorable scenarios at each step, and of the last\nstep Monte Carlo error associated to each scenario. By using concentration\ninequalities, we then show that, for sub-gamma pricing errors, the\nprobabilities of inversion converge at an exponential rate in the number of\nsimulated paths. We then propose an adaptative version in which the algorithm\nimproves step by step its knowledge on the unknown parameters of interest: mean\nand variance of the Monte Carlo estimators of the different scenarios. Both\nschemes can be optimized by using dynamic programming algorithms that can be\nsolved off-line. To our knowledge, these are the first non-asymptotic bounds\nfor such estimators. Our hypotheses are weak enough to allow for the use of\nestimators for the different scenarios and steps based on the same random\nvariables, which, in practice, reduces considerably the computational effort.\nFirst numerical tests are performed.\n"
    },
    {
        "paper_id": 2005.126,
        "authors": "Hiroya Taniguchi and Ken Yamada",
        "title": "The Race between Technology and Woman: Changes in Gender and Skill\n  Premia in OECD Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The era of technological change entails complex patterns of changes in wages\nand employment. We develop a unified framework to measure the contribution of\ntechnological change embodied in new capital to changes in the relative wages\nand income shares of different types of labor. We obtain the aggregate\nelasticities of substitution by estimating and aggregating sectoral production\nfunction parameters with cross-country and cross-industry panel data from OECD\ncountries. We show that advances in information, communication, and computation\ntechnologies contribute significantly to narrowing the gender wage gap,\nwidening the skill wage gap, and declining labor shares.\n"
    },
    {
        "paper_id": 2005.12619,
        "authors": "Riccardo Doyle",
        "title": "Using Network Interbank Contagion in Bank Default Prediction",
        "comments": "10 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interbank contagion can theoretically exacerbate losses in a financial system\nand lead to additional cascade defaults during downturn. In this paper we\nproduce default analysis using both regression and neural network models to\nverify whether interbank contagion offers any predictive explanatory power on\ndefault events. We predict defaults of U.S. domiciled commercial banks in the\nfirst quarter of 2010 using data from the preceding four quarters. A number of\nestablished predictors (such as Tier 1 Capital Ratio and Return on Equity) are\nincluded alongside contagion to gauge if the latter adds significance. Based on\nthis methodology, we conclude that interbank contagion is extremely explanatory\nin default prediction, often outperforming more established metrics, in both\nregression and neural network models. These findings have sizeable implications\nfor the future use of interbank contagion as a variable of interest for stress\ntesting, bank issued bond valuation and wider bank default prediction.\n"
    },
    {
        "paper_id": 2005.12638,
        "authors": "Dainis Zegners, Uwe Sunde, Anthony Strittmatter",
        "title": "Decisions and Performance Under Bounded Rationality: A Computational\n  Benchmarking Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a novel approach to analyze human decision-making that\ninvolves comparing the behavior of professional chess players relative to a\ncomputational benchmark of cognitively bounded rationality. This benchmark is\nconstructed using algorithms of modern chess engines and allows investigating\nbehavior at the level of individual move-by-move observations, thus\nrepresenting a natural benchmark for computationally bounded optimization. The\nanalysis delivers novel insights by isolating deviations from this benchmark of\nbounded rationality as well as their causes and consequences for performance.\nThe findings document the existence of several distinct dimensions of\nbehavioral deviations, which are related to asymmetric positional evaluation in\nterms of losses and gains, time pressure, fatigue, and complexity. The results\nalso document that deviations from the benchmark do not necessarily entail\nworse performance. Faster decisions are associated with more frequent\ndeviations from the benchmark, yet they are also associated with better\nperformance. The findings are consistent with an important influence of\nintuition and experience, thereby shedding new light on the recent debate about\ncomputational rationality in cognitive processes.\n"
    },
    {
        "paper_id": 2005.12774,
        "authors": "Ka Wai Tsang, Zhaoyi He",
        "title": "Mean-Variance Portfolio Management with Functional Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0219024920500557",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new functional optimization approach to portfolio\noptimization problems by treating the unknown weight vector as a function of\npast values instead of treating them as fixed unknown coefficients in the\nmajority of studies. We first show that the optimal solution, in general, is\nnot a constant function. We give the optimal conditions for a vector function\nto be the solution, and hence give the conditions for a plug-in solution\n(replacing the unknown mean and variance by certain estimates based on past\nvalues) to be optimal. After showing that the plug-in solutions are sub-optimal\nin general, we propose gradient-ascent algorithms to solve the functional\noptimization for mean-variance portfolio management with theorems for\nconvergence provided. Simulations and empirical studies show that our approach\ncan perform significantly better than the plug-in approach.\n"
    },
    {
        "paper_id": 2005.12949,
        "authors": "Alexander Lipton, Aetienne Sardon, Fabian Sch\\\"ar, Christian\n  Sch\\\"upbach",
        "title": "From Tether to Libra: Stablecoins, Digital Currency and the Future of\n  Money",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides an overview on stablecoins and introduces a novel\nterminology to help better identify stablecoins with truly disruptive\npotential. It provides a compact definition for stablecoins, identifying the\nunique features that make them distinct from previously known payment systems.\nFurthermore, it surveys the different use cases for stablecoins as well as the\nunderlying economic incentives for creating them. Finally, it outlines critical\nregulatory considerations that constrain stablecoins and summarizes key factors\nthat are driving their rapid development.\n"
    },
    {
        "paper_id": 2005.13005,
        "authors": "Roberto Baviera and Giuseppe Messuti",
        "title": "Daily Middle-Term Probabilistic Forecasting of Power Consumption in\n  North-East England",
        "comments": "26 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2006.16388",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Probabilistic forecasting of power consumption in a middle-term horizon\n(months to a year) is a main challenge in the energy sector. It plays a key\nrole in planning future generation plants and transmission grid. We propose a\nnew model that incorporates trend and seasonality features as in traditional\ntime-series analysis and weather conditions as explicative variables in a\nparsimonious machine learning approach, known as Gaussian Process. Applying to\na daily power consumption dataset in North East England provided by one of the\nlargest energy suppliers, we obtain promising results in Out-of-Sample density\nforecasts up to one year, even using a small dataset, with only a two-year\nIn-Sample data. In order to verify the quality of the achieved power\nconsumption probabilistic forecast we consider measures that are common in the\nenergy sector as pinball loss and Winkler score and backtesting conditional and\nunconditional tests, standard in the banking sector after the introduction of\nBasel II Accords.\n"
    },
    {
        "paper_id": 2005.13008,
        "authors": "Eiji Yamamura., Yoshiro Tsutsui",
        "title": "Impact of the State of Emergency Declaration for COVID-19 on Preventive\n  Behaviors and Mental Conditions in Japan: Difference in Difference Analysis\n  using Panel Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the COVID-19 epidemic in Japan between March and April 2020, Internet\nsurveys were conducted to construct panel data to investigate changes at the\nindividual level regarding preventive behaviors and mental conditions by\nsurveying the same respondents at different times. Specifically, the\ndifference-in-difference (DID) method was used to explore the impact of the\nCOVID-19 state of emergency declared by the government. Key findings were: (1)\nthe declaration led people to stay home, while also generating anger, fear, and\nanxiety. (2) The effect of the declaration on the promotion of preventive\nbehaviors was larger than the detrimental effect on mental conditions. (3)\nOverall, the effect on women was larger than that on men.\n"
    },
    {
        "paper_id": 2005.13033,
        "authors": "Dar\\'io Alatorre, Carlos Gershenson, and Jos\\'e L. Mateos",
        "title": "Stocks and Cryptocurrencies: Anti-fragile or Robust?",
        "comments": "22 pages, 16 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0280487",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In contrast with robust systems that resist noise or fragile systems that\nbreak with noise, antifragility is defined as a property of complex systems\nthat benefit from noise or disorder. Here we define and test a simple measure\nof antifragility for complex dynamical systems. In this work we use our\nantifragility measure to analyze real data from return prices in the stock and\ncryptocurrency markets. Our definition of antifragility is the product of the\nreturn price and a perturbation. We explore different types of perturbations\nthat typically arise from within the system. Our results suggest that for both\nthe stock market and the cryptocurrency market, the tendency among the 'top\nperformers' is to be robust rather than antifragile. It would be important to\nexplore other possible definitions of antifragility to understand its role in\nfinancial markets and in complex dynamical systems in general.\n"
    },
    {
        "paper_id": 2005.13138,
        "authors": "Kavita Surana, Anuraag Singh and Ambuj D Sagar",
        "title": "Strengthening science, technology, and innovation-based incubators to\n  help achieve Sustainable Development Goals: Lessons from India",
        "comments": null,
        "journal-ref": "Technological Forecasting and Social Change 157 (August 2020):\n  120057",
        "doi": "10.1016/j.techfore.2020.120057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Policymakers in developing countries increasingly see science, technology,\nand innovation (STI) as an avenue for meeting sustainable development goals\n(SDGs), with STI-based startups as a key part of these efforts. Market failures\ncall for government interventions in supporting STI for SDGs and\npublicly-funded incubators can potentially fulfil this role. Using the specific\ncase of India, we examine how publicly-funded incubators could contribute to\nstrengthening STI-based entrepreneurship. India's STI policy and its links to\nsocietal goals span multiple decades -- but since 2015 these goals became\nformally organized around the SDGs. We examine why STI-based incubators were\ncreated under different policy priorities before 2015, the role of public\nagencies in implementing these policies, and how some incubators were\nparticularly effective in addressing the societal challenges that can now be\nmapped to SDGs. We find that effective incubation for supporting STI-based\nentrepreneurship to meet societal goals extended beyond traditional incubation\nactivities. For STI-based incubators to be effective, policymakers must\nstrengthen the 'incubation system'. This involves incorporating targeted SDGs\nin specific incubator goals, promoting coordination between existing incubator\nprograms, developing a performance monitoring system, and finally, extending\nextensive capacity building at multiple levels including for incubator managers\nand for broader STI in the country.\n"
    },
    {
        "paper_id": 2005.13248,
        "authors": "Fabien Le Floc'h",
        "title": "More Robust Pricing of European Options Based on Fourier Cosine Series\n  Expansions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present an alternative formula to price European options through cosine\nseries expansions, under models with a known characteristic function such as\nthe Heston stochastic volatility model. It is more robust across strikes and as\nfast as the original COS method.\n"
    },
    {
        "paper_id": 2005.13252,
        "authors": "Fabien Le Floc'h",
        "title": "Notes on the SWIFT method based on Shannon Wavelets for Option Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This note shows that the cosine expansion based on the Vieta formula is\nequivalent to a discretization of the Parseval identity. We then evaluate the\nuse of simple direct algorithms to compute the Shannon coefficients for the\npayoff. Finally, we explore the efficiency of a Filon quadrature instead of the\nVieta formula for the coefficients related to the probability density function.\n"
    },
    {
        "paper_id": 2005.13416,
        "authors": "L\\'aszl\\'o Csat\\'o and D\\'ora Gr\\'eta Petr\\'oczy",
        "title": "Bibliometric indices as a measure of performance and competitive balance\n  in the knockout stage of the UEFA Champions League",
        "comments": "25 pages, 9 figures, 7 tables",
        "journal-ref": "Central European Journal of Operations Research , 2024,\n  forthcoming",
        "doi": "10.1007/s10100-023-00896-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue for the application of bibliometric indices to quantify the\nlong-term uncertainty of outcome in sports. The Euclidean index is proposed to\nreward quality over quantity, while the rectangle index can be an appropriate\nmeasure of core performance. Their differences are highlighted through an\naxiomatic analysis and several examples. Our approach also requires a weighting\nscheme to compare different achievements. The methodology is illustrated by\nstudying the knockout stage of the UEFA Champions League in the 20 seasons\nplayed between 2003 and 2023: club and country performances as well as three\ntypes of competitive balance are considered. Measuring competition at the level\nof national associations is a novelty. All results are remarkably robust\nconcerning the bibliometric index and the assigned weights. Since the\nperformances of national associations are more stable than the results of\nindividual clubs, it would be better to build the seeding in the UEFA Champions\nLeague group stage upon association coefficients adjusted for league finishing\npositions rather than club coefficients.\n"
    },
    {
        "paper_id": 2005.13417,
        "authors": "Tim Janke and Florian Steinke",
        "title": "Probabilistic multivariate electricity price forecasting using implicit\n  generative ensemble post-processing",
        "comments": "To be presented at the 16th International Conference on Probabilistic\n  Methods Applied to Power Systems 2020 (PMAPS 2020)",
        "journal-ref": null,
        "doi": "10.1109/PMAPS47429.2020.9183687",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The reliable estimation of forecast uncertainties is crucial for\nrisk-sensitive optimal decision making. In this paper, we propose implicit\ngenerative ensemble post-processing, a novel framework for multivariate\nprobabilistic electricity price forecasting. We use a likelihood-free implicit\ngenerative model based on an ensemble of point forecasting models to generate\nmultivariate electricity price scenarios with a coherent dependency structure\nas a representation of the joint predictive distribution. Our ensemble\npost-processing method outperforms well-established model combination\nbenchmarks. This is demonstrated on a data set from the German day-ahead\nmarket. As our method works on top of an ensemble of domain-specific expert\nmodels, it can readily be deployed to other forecasting tasks.\n"
    },
    {
        "paper_id": 2005.13621,
        "authors": "Christopher D. Clack, Elias Court, Dmitrijs Zaparanuks",
        "title": "Dynamic Coupling and Market Instability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine dynamic coupling and feedback effects between High Frequency\nTraders (HFTs) and how they can destabilize markets. We develop a general\nframework for modelling dynamic interaction based on recurrence relations, and\nuse this to show how unexpected latency and feedback can trigger oscillatory\ninstability between HFT market makers with inventory constraints. Our analysis\nsuggests that the modelled instability is an unintentional emergent behaviour\nof the market that does not depend on the complexity of HFT strategies - even\napparently stable strategies are vulnerable. Feedback instability can lead to\nsubstantial movements in market prices such as price spikes and crashes.\n"
    },
    {
        "paper_id": 2005.13665,
        "authors": "Zihao Zhang, Stefan Zohren, Stephen Roberts",
        "title": "Deep Learning for Portfolio Optimization",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.3905/jfds.2020.1.042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We adopt deep learning models to directly optimise the portfolio Sharpe\nratio. The framework we present circumvents the requirements for forecasting\nexpected returns and allows us to directly optimise portfolio weights by\nupdating model parameters. Instead of selecting individual assets, we trade\nExchange-Traded Funds (ETFs) of market indices to form a portfolio. Indices of\ndifferent asset classes show robust correlations and trading them substantially\nreduces the spectrum of available assets to choose from. We compare our method\nwith a wide range of algorithms with results showing that our model obtains the\nbest performance over the testing period, from 2011 to the end of April 2020,\nincluding the financial instabilities of the first quarter of 2020. A\nsensitivity analysis is included to understand the relevance of input features\nand we further study the performance of our approach under different cost rates\nand different risk levels via volatility scaling.\n"
    },
    {
        "paper_id": 2005.13722,
        "authors": "Ian M. Trotter, Lu\\'is A. C. Schmidt, Bruno C. M. Pinto, Andrezza L.\n  Batista, J\\'essica Pellenz, Maritza Isidro, Aline Rodrigues, Attawan G. S.\n  Suela, Loredany Rodrigues",
        "title": "COVID-19 and Global Economic Growth: Policy Simulations with a\n  Pandemic-Enabled Neoclassical Growth Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the COVID-19 pandemic of 2019/2020, authorities have used temporary\nad-hoc policy measures, such as lockdowns and mass quarantines, to slow its\ntransmission. However, the consequences of widespread use of these\nunprecedented measures are poorly understood. To contribute to the\nunderstanding of the economic and human consequences of such policy measures,\nwe therefore construct a mathematical model of an economy under the impact of a\npandemic, select parameter values to represent the global economy under the\nimpact of COVID-19, and perform numerical experiments by simulating a large\nnumber of possible policy responses. By varying the starting date of the policy\nintervention in the simulated scenarios, we find that the most effective policy\nintervention occurs around the time when the number of active infections is\ngrowing at its highest rate -- that is, the results suggest that the most\nsevere measures should only be implemented when the disease is sufficiently\nspread. The intensity of the intervention, above a certain threshold, does not\nappear to have a great impact on the outcomes in our simulations, due to the\nstrongly concave relationship that we identify between production shortfall and\ninfection rate reductions. Our experiments further suggest that the\nintervention should last until after the peak established by the reduced\ninfection rate, which implies that stricter policies should last longer. The\nmodel and its implementation, along with the general insights from our policy\nexperiments, may help policymakers design effective emergency policy responses\nin the face of a serious pandemic, and contribute to our understanding of the\nrelationship between the economic growth and the spread of infectious diseases.\n"
    },
    {
        "paper_id": 2005.13741,
        "authors": "Weidong Tian, Zimu Zhu",
        "title": "A Portfolio Choice Problem Under Risk Capacity Constraint",
        "comments": null,
        "journal-ref": "Annals of Finance (2022)",
        "doi": "10.1007/s10436-021-00404-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal investing problem for a retiree facing\nlongevity risk and living standard risk. We formulate the investing problem as\na portfolio choice problem under a time-varying risk capacity constraint. We\nderive the optimal investment strategy under the specific condition on model\nparameters in terms of second-order ordinary differential equations. We\ndemonstrate an endogenous number that measures the expected value to sustain\nthe spending post-retirement. The optimal portfolio is nearly neutral to the\nstock market movement if the portfolio's value is higher than this number; but,\nif the portfolio is not worth enough to sustain the retirement spending, the\nretiree actively invests in the stock market for the higher expected return.\nBesides, we solve an optimal portfolio choice problem under a leverage\nconstraint and show that the optimal portfolio would lose significantly in\nstressed markets. This paper shows that the time-varying risk capacity\nconstraint has important implications for asset allocation in retirement.\n"
    },
    {
        "paper_id": 2005.13831,
        "authors": "Christian Dehm, Thai Nguyen, Mitja Stadje",
        "title": "Non-concave expected utility optimization with uncertain time horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an expected utility maximization problem where the utility\nfunction is not necessarily concave and the time horizon is uncertain. We\nestablish a necessary and sufficient condition for the optimality for general\nnon-concave utility function in a complete financial market. We show that the\ngeneral concavification approach of the utility function to deal with\nnon-concavity, while being still applicable when the time horizon is a stopping\ntime with respect to the financial market filtration, leads to sub-optimality\nwhen the time horizon is independent of the financial risk, and hence can not\nbe directly applied. For the latter case, we suggest a recursive procedure\nwhich is based on the dynamic programming principle. We illustrate our findings\nby carrying out a multi-period numerical analysis for optimal investment\nproblem under a convex option compensation scheme with random time horizon. We\nobserve that the distribution of the non-concave portfolio in both certain and\nuncertain random time horizon is right-skewed with a long right tail,\nindicating that the investor expects frequent small losses and a few large\ngains from the investment. While the (certain) average time horizon portfolio\nat a premature stopping date is unimodal, the random time horizon portfolio is\nmultimodal distributed which provides the investor a certain flexibility of\nswitching between the local maximizers, depending on the market performance.\nThe multimodal structure with multiple peaks of different heights can be\nexplained by the concavification procedure, whereas the distribution of the\ntime horizon has significant impact on the amplitude between the modes.\n"
    },
    {
        "paper_id": 2005.1389,
        "authors": "Jherek Healy",
        "title": "Equivalence between forward rate interpolations and discount factor\n  interpolations for the yield curve construction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The traditional way of building a yield curve is to choose an interpolation\non discount factors, implied by the market tradable instruments. Since then,\nconstructions based on specific interpolations of the forward rates have become\nthe trend. We show here that some popular interpolation methods on the forward\nrates correspond exactly to classical interpolation methods on discount\nfactors. This paper also aims at clarifying the differences between\ninterpolations in terms of discount factors, instantaneous forward rates,\ndiscrete forward rates, and constant period forward rates.\n"
    },
    {
        "paper_id": 2005.13974,
        "authors": "Can Yang, Junjie Zhai, Helong Li",
        "title": "On the Bound of Cumulative Return in Trading Series and the Verification\n  Using Technical Trading Rules",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although there is a wide use of technical trading rules in stock markets, the\nprofitability of them still remains controversial. This paper first presents\nand proves the upper bound of cumulative return, and then introduces many of\nconventional technical trading rules. Furthermore, with the help of bootstrap\nmethodology, we investigate the profitability of technical trading rules on\ndifferent international stock markets, including developed markets and emerging\nmarkets. At last, the results show that the technical trading rules are hard to\nbeat the market, and even less profitable than the random trading strategy.\n"
    },
    {
        "paper_id": 2005.13995,
        "authors": "Xinyue Cui, Zhaoyu Xu, Yue Zhou",
        "title": "Using Machine Learning to Forecast Future Earnings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this essay, we have comprehensively evaluated the feasibility and\nsuitability of adopting the Machine Learning Models on the forecast of\ncorporation fundamentals (i.e. the earnings), where the prediction results of\nour method have been thoroughly compared with both analysts' consensus\nestimation and traditional statistical models. As a result, our model has\nalready been proved to be capable of serving as a favorable auxiliary tool for\nanalysts to conduct better predictions on company fundamentals. Compared with\nprevious traditional statistical models being widely adopted in the industry\nlike Logistic Regression, our method has already achieved satisfactory\nadvancement on both the prediction accuracy and speed. Meanwhile, we are also\nconfident enough that there are still vast potentialities for this model to\nevolve, where we do hope that in the near future, the machine learning model\ncould generate even better performances compared with professional analysts.\n"
    },
    {
        "paper_id": 2005.14063,
        "authors": "Fabio Antonelli, Alessandro Ramponi, Sergio Scarlatti",
        "title": "A moment matching method for option pricing under stochastic interest\n  rates",
        "comments": "19 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a simple, but new, approximation methodology for\npricing a call option in a Black \\& Scholes market characterized by stochastic\ninterest rates. The method, based on a straightforward Gaussian moment matching\ntechnique applied to a conditional Black \\& Scholes formula, is quite general\nand it applies to various models, whether affine or not. To check its accuracy\nand computational time, we implement it for the CIR interest rate model\ncorrelated with the underlying, using the Monte Carlo simulations as a\nbenchmark. The method's performance turns out to be quite remarkable, even when\ncompared with analogous results obtained by the affine approximation technique\npresented in Grzelak and Oosterlee (2011) and by the expansion formula\nintroduced in Kim and Kunimoto (1999), as we show in the last section.\n"
    },
    {
        "paper_id": 2005.14126,
        "authors": "Bastien Baldacci, Philippe Bergault, Joffrey Derchu, Mathieu Rosenbaum",
        "title": "On bid and ask side-specific tick sizes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The tick size, which is the smallest increment between two consecutive prices\nfor a given asset, is a key parameter of market microstructure. In particular,\nthe behavior of high frequency market makers is highly related to its value. We\ntake the point of view of an exchange and investigate the relevance of having\ndifferent tick sizes on the bid and ask sides of the order book. Using an\napproach based on the model with uncertainty zones, we show that when\nside-specific tick sizes are suitably chosen, it enables the exchange to\nimprove the quality of liquidity provision.\n"
    },
    {
        "paper_id": 2005.1435,
        "authors": "Pablo Olivares",
        "title": "Pricing Temperature Derivatives under a Time-Changed Levy Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of the paper is to price weather contracts using temperature as\nthe underlying process when the later follows a mean-reverting dynamics driven\nby a time-changed Brownian motion coupled to a Gamma Levy subordinator and\ntime-dependent deterministic volatility. This type of model captures the\ncomplexity of the temperature dynamic providing a more accurate valuation of\ntheir associate weather contracts. An approximated price is obtained by a\nFourier expansion of its characteristic function combined with a selection of\nthe equivalent martingale measure following the Esscher transform proposed in\nGerber and Shiu (1994).\n"
    },
    {
        "paper_id": 2005.14361,
        "authors": "Konrad Gajewski and Sebastian Ferrando and Pablo Olivares",
        "title": "Pricing Energy Contracts under Regime Switching Time-Changed models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The shortcomings of the popular Black-Scholes-Merton (BSM) model have led to\nmodels which could more accurately model the behavior of the underlying assets\nin energy markets, particularly in electricity and future oil prices. In this\npaper we consider a class of regime switching time-changed Levy processes,\nwhich builds upon the BSM model by incorporating jumps through a random clock,\nas well as randomly varying parameters according to a two-state continuous-time\nMarkov chain. We implement pricing methods based on expansions of the\ncharacteristic function as in \\cite{Fourier}. Finally, we estimate the\nparameters of the model by incorporating historic energy data and option quotes\nusing a variety of methods.\n"
    },
    {
        "paper_id": 2005.14442,
        "authors": "Lijun Pan, Yongjin Wang",
        "title": "Competition among Large and Heterogeneous Small Firms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the model of Parenti (2018) on large and small firms by introducing\ncost heterogeneity among small firms. We propose a novel necessary and\nsufficient condition for the existence of such a mixed market structure.\nFurthermore, in contrast to Parenti (2018), we show that in the presence of\ncost heterogeneity among small firms, trade liberalization may raise or reduce\nthe mass of small firms in operation.\n"
    },
    {
        "paper_id": 2005.14631,
        "authors": "Gal Shahaf, Ehud Shapiro and Nimrod Talmon",
        "title": "Egalitarian and Just Digital Currency Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Cryptocurrencies are a digital medium of exchange with decentralized control\nthat renders the community operating the cryptocurrency its sovereign. Leading\ncryptocurrencies use proof-of-work or proof-of-stake to reach consensus, thus\nare inherently plutocratic. This plutocracy is reflected not only in control\nover execution, but also in the distribution of new wealth, giving rise to\n``rich get richer'' phenomena. Here, we explore the possibility of an\nalternative digital currency that is egalitarian in control and just in the\ndistribution of created wealth. Such currencies can form and grow in grassroots\nand sybil-resilient way. A single currency community can achieve distributive\njustice by egalitarian coin minting, whereby each member mints one coin at\nevery time step. Egalitarian minting results, in the limit, in the dilution of\nany inherited assets and in each member having an equal share of the minted\ncurrency, adjusted by the relative productivity of the members. Our main\ntheorem shows that a currency network, where agents can be members of more than\none currency community, can achieve distributive justice globally across the\nnetwork by joint egalitarian minting, whereby each agent mints one coin in only\none community at each timestep. Specifically, we show that a sufficiently large\nintersection between two communities -- relative to the gap in their\nproductivity -- will cause the exchange rates between their currencies to\nconverge to 1:1, resulting in global distributive justice.\n"
    },
    {
        "paper_id": 2005.14658,
        "authors": "Luisa Roa, Alejandro Correa-Bahnsen, Gabriel Suarez, Fernando\n  Cort\\'es-Tejada, Mar\\'ia A. Luque and Cristi\\'an Bravo",
        "title": "Super-App Behavioral Patterns in Credit Risk Models: Financial,\n  Statistical and Regulatory Implications",
        "comments": "Accepted - v2. 25 pages",
        "journal-ref": "Expert Systems with Applications: 114486 (2020)",
        "doi": "10.1016/j.eswa.2020.114486",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we present the impact of alternative data that originates from\nan app-based marketplace, in contrast to traditional bureau data, upon credit\nscoring models. These alternative data sources have shown themselves to be\nimmensely powerful in predicting borrower behavior in segments traditionally\nunderserved by banks and financial institutions. Our results, validated across\ntwo countries, show that these new sources of data are particularly useful for\npredicting financial behavior in low-wealth and young individuals, who are also\nthe most likely to engage with alternative lenders. Furthermore, using the\nTreeSHAP method for Stochastic Gradient Boosting interpretation, our results\nalso revealed interesting non-linear trends in the variables originating from\nthe app, which would not normally be available to traditional banks. Our\nresults represent an opportunity for technology companies to disrupt\ntraditional banking by correctly identifying alternative data sources and\nhandling this new information properly. At the same time alternative data must\nbe carefully validated to overcome regulatory hurdles across diverse\njurisdictions.\n"
    },
    {
        "paper_id": 2005.14659,
        "authors": "Tega Anighoro",
        "title": "Value relevance of the components of oil and gas reserve quantity change\n  disclosures of upstream oil and gas companies in the london stock exchange",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The high level of risk and uncertainty in harnessing oil and gas reserves\nposes an accounting dilemma in the reporting of reserves quantity information;\ninformation which is critical and relied on by investors for decision making.\nDifferent studies have indicated that reserves disclosure information is\nfundamental to understanding the value of the firm. This study attempts to\ncontribute to the growing value relevance literature on reserves disclosures by\nexamining the value relevance of the components of oil and gas reserve quantity\nchange disclosures of upstream oil and gas companies in the London Stock\nExchange. Particularly, it investigates the relationship between average\nhistorical share returns and changes in reserves from explorations,\nacquisitions, production, revisions and sale. It also examines the value\nrelevance of the quality of these disclosures. Using archival data from LSE,\ndatabases and annual reports, and applying a multifactor framework, the\nempirical results suggested that changes in reserves as well as the components\nof these changes where associated with share returns though insignificantly due\nto the significant impact of oil price and longitudinal effect posed by\napplying the measurement approach with utilizes historical returns. However,\nthe quality of reserves disclosures has a positively significant relationship\nwith share returns. The volatility and decline in oil price is also reflected\nin both low average share returns at -0.4% and low average growth in reserves\nat 8.94% for the last 8 years in the sector.\n"
    },
    {
        "paper_id": 2005.1467,
        "authors": "Florian Ziel",
        "title": "The energy distance for ensemble and scenario reduction",
        "comments": "Accepted for publication in Philosophical Transactions A",
        "journal-ref": null,
        "doi": "10.1098/rsta.2019.0431",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scenario reduction techniques are widely applied for solving sophisticated\ndynamic and stochastic programs, especially in energy and power systems, but\nalso used in probabilistic forecasting, clustering and estimating generative\nadversarial networks (GANs). We propose a new method for ensemble and scenario\nreduction based on the energy distance which is a special case of the maximum\nmean discrepancy (MMD). We discuss the choice of energy distance in detail,\nespecially in comparison to the popular Wasserstein distance which is\ndominating the scenario reduction literature. The energy distance is a metric\nbetween probability measures that allows for powerful tests for equality of\narbitrary multivariate distributions or independence. Thanks to the latter, it\nis a suitable candidate for ensemble and scenario reduction problems. The\ntheoretical properties and considered examples indicate clearly that the\nreduced scenario sets tend to exhibit better statistical properties for the\nenergy distance than a corresponding reduction with respect to the Wasserstein\ndistance. We show applications to a Bernoulli random walk and two real data\nbased examples for electricity demand profiles and day-ahead electricity\nprices.\n"
    },
    {
        "paper_id": 2006.00085,
        "authors": "Andrea Aspri, Elena Beretta, Alberto Gandolfi, Etienne Wasmer",
        "title": "Mortality containment vs. economics opening: optimal policies in a\n  SEIARD model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We adapt a SEIRD differential model with asymptomatic population and Covid\ndeaths, which we call SEAIRD, to simulate the evolution of COVID-19, and add a\ncontrol function affecting both the diffusion of the virus and GDP, featuring\nall direct and indirect containment policies; to model feasibility, the control\nis assumed to be a piece-wise linear function satisfying additional\nconstraints. We describe the joint dynamics of infection and the economy and\ndiscuss the trade-off between production and fatalities. In particular, we\ncarefully study the conditions for the existence of the optimal policy response\nand its uniqueness. Uniqueness crucially depends on the marginal rate of\nsubstitution between the statistical value of a human life and GDP; we show an\nexample with a phase transition: above a certain threshold, there is a unique\noptimal containment policy; below the threshold, it is optimal to abstain from\nany containment; and at the threshold itself there are two optimal policies. We\nthen explore and evaluate various profiles of various control policies\ndependent on a small number of parameters.\n"
    },
    {
        "paper_id": 2006.00123,
        "authors": "Dhagash Mehta, Dhruv Desai, Jithin Pradeep",
        "title": "Machine Learning Fund Categorizations",
        "comments": "8 pages, 2-column format, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given the surge in popularity of mutual funds (including exchange-traded\nfunds (ETFs)) as a diversified financial investment, a vast variety of mutual\nfunds from various investment management firms and diversification strategies\nhave become available in the market. Identifying similar mutual funds among\nsuch a wide landscape of mutual funds has become more important than ever\nbecause of many applications ranging from sales and marketing to portfolio\nreplication, portfolio diversification and tax loss harvesting. The current\nbest method is data-vendor provided categorization which usually relies on\ncuration by human experts with the help of available data. In this work, we\nestablish that an industry wide well-regarded categorization system is\nlearnable using machine learning and largely reproducible, and in turn\nconstructing a truly data-driven categorization. We discuss the intellectual\nchallenges in learning this man-made system, our results and their\nimplications.\n"
    },
    {
        "paper_id": 2006.00158,
        "authors": "Daiki Maki and Yasushi Ota",
        "title": "The impacts of asymmetry on modeling and forecasting realized volatility\n  in Japanese stock markets",
        "comments": "27 pages, 4 figures and 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the impacts of asymmetry on the modeling and\nforecasting of realized volatility in the Japanese futures and spot stock\nmarkets. We employ heterogeneous autoregressive (HAR) models allowing for three\ntypes of asymmetry: positive and negative realized semivariance (RSV),\nasymmetric jumps, and leverage effects. The estimation results show that\nleverage effects clearly influence the modeling of realized volatility models.\nLeverage effects exist for both the spot and futures markets in the Nikkei 225.\nAlthough realized semivariance aids better modeling, the estimations of RSV\nmodels depend on whether these models have leverage effects. Asymmetric jump\ncomponents do not have a clear influence on realized volatility models. While\nleverage effects and realized semivariance also improve the out-of-sample\nforecast performance of volatility models, asymmetric jumps are not useful for\npredictive ability. The empirical results of this study indicate that\nasymmetric information, in particular, leverage effects and realized\nsemivariance, yield better modeling and more accurate forecast performance.\nAccordingly, asymmetric information should be included when we model and\nforecast the realized volatility of Japanese stock markets.\n"
    },
    {
        "paper_id": 2006.00218,
        "authors": "Imanol Perez Arribas, Cristopher Salvi, Lukasz Szpruch",
        "title": "Sig-SDEs model for quantitative finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical models, calibrated to data, have become ubiquitous to make key\ndecision processes in modern quantitative finance. In this work, we propose a\nnovel framework for data-driven model selection by integrating a classical\nquantitative setup with a generative modelling approach. Leveraging the\nproperties of the signature, a well-known path-transform from stochastic\nanalysis that recently emerged as leading machine learning technology for\nlearning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new\nperspective on neural SDEs and can be calibrated to exotic financial products\nthat depend, in a non-linear way, on the whole trajectory of asset prices.\nFurthermore, we our approach enables to consistently calibrate under the\npricing measure $\\mathbb Q$ and real-world measure $\\mathbb P$. Finally, we\ndemonstrate the ability of Sig-SDE to simulate future possible market scenarios\nneeded for computing risk profiles or hedging strategies. Importantly, this new\nmodel is underpinned by rigorous mathematical analysis, that under appropriate\nconditions provides theoretical guarantees for convergence of the presented\nalgorithms.\n"
    },
    {
        "paper_id": 2006.00268,
        "authors": "Yujie Hu, Joni Downs",
        "title": "Measuring and Visualizing Place-Based Space-Time Job Accessibility",
        "comments": null,
        "journal-ref": "Journal of Transport Geography, 74, 278-288 (2019)",
        "doi": "10.1016/j.jtrangeo.2018.12.002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Place-based accessibility measures, such as the gravity-based model, are\nwidely applied to study the spatial accessibility of workers to job\nopportunities in cities. However, gravity-based measures often suffer from\nthree main limitations: (1) they are sensitive to the spatial configuration and\nscale of the units of analysis, which are not specifically designed for\ncapturing job accessibility patterns and are often too coarse; (2) they omit\nthe temporal dynamics of job opportunities and workers in the calculation,\ninstead assuming that they remain stable over time; and (3) they do not lend\nthemselves to dynamic geovisualization techniques. In this paper, a new\nmethodological framework for measuring and visualizing place-based job\naccessibility in space and time is presented that overcomes these three\nlimitations. First, discretization and dasymetric mapping approaches are used\nto disaggregate counts of jobs and workers over specific time intervals to a\nfine-scale grid. Second, Shen (1998) gravity-based accessibility measure is\nmodified to account for temporal fluctuations in the spatial distributions of\nthe supply of jobs and the demand of workers and is used to estimate hourly job\naccessibility at each cell. Third, a four-dimensional volumetric rendering\napproach is employed to integrate the hourly job access estimates into a\nspace-time cube environment, which enables the users to interactively visualize\nthe space-time job accessibility patterns. The integrated framework is\ndemonstrated in the context of a case study of the Tampa Bay region of Florida.\nThe findings demonstrate the value of the proposed methodology in job\naccessibility analysis and the policy-making process.\n"
    },
    {
        "paper_id": 2006.00279,
        "authors": "Rebecca Mitchell, Roger Maull, Simon Pearson, Steve Brewer and Martin\n  Collison",
        "title": "The impact of COVID-19 on the UK fresh food supply chain",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The resilience of the food supply chain is a matter of critical importance,\nboth for national security and broader societal well bring. COVID19 has\npresented a test to the current system, as well as means by which to explore\nwhether the UK's food supply chain will be resilient to future disruptions. In\nthe face of a growing need to ensure that food supply is more environmentally\nsustainable and socially just, COVOD19 also represents an opportunity to\nconsider the ability of the system to innovative, and its capacity for change.\nThe purpose of this case based study is to explore the response and resilience\nof the UK fruit and vegetable food supply chain to COVID19, and to assess this\nempirical evidence in the context of a resilience framework based on the\nadaptive cycle. To achieve this we reviewed secondary data associated with\nchanges to retail demand, conducted interviews with 23 organisations associated\nwith supply to this market, and conducted four video workshops with 80\norganisations representing half of the UK fresh produce community. The results\nhighlight that, despite significant disruption, the retail dominated fresh food\nsupply chain has demonstrated a high degree of resilience. In the context of\nthe adaptive cycle, the system has shown signs of being stuck in a rigidity\ntrap, as yet unable to exploit more radical innovations that may also assist in\naddressing other drivers for change. This has highlighted the significant role\nthat innovation and R&D communities will need to play in enabling the supply\nchain to imagine and implement alternative future states post COVID.\n"
    },
    {
        "paper_id": 2006.00282,
        "authors": "Neofytos Rodosthenous and Hongzhong Zhang",
        "title": "When to sell an asset amid anxiety about drawdowns",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/mafi.12278",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider risk averse investors with different levels of anxiety about\nasset price drawdowns. The latter is defined as the distance of the current\nprice away from its best performance since inception. These drawdowns can\nincrease either continuously or by jumps, and will contribute towards the\ninvestor's overall impatience when breaching the investor's private tolerance\nlevel. We investigate the unusual reactions of investors when aiming to sell an\nasset under such adverse market conditions. Mathematically, we study the\noptimal stopping of the utility of an asset sale with a random discounting that\ncaptures the investor's overall impatience. The random discounting is given by\nthe cumulative amount of time spent by the drawdowns in an undesirable high\nregion, fine tuned by the investor's personal tolerance and anxiety about\ndrawdowns. We prove that in addition to the traditional take-profit sales, the\nreal-life employed stop-loss orders and trailing stops may become part of the\noptimal selling strategy, depending on different personal characteristics. This\npaper thus provides insights on the effect of anxiety and its distinction with\ntraditional risk aversion on decision making.\n"
    },
    {
        "paper_id": 2006.00596,
        "authors": "Vygintas Gontis",
        "title": "Long-range memory test by the burst and inter-burst duration\n  distribution",
        "comments": "18 pages, 8 figures",
        "journal-ref": "J. Stat. Mech. (2020) 093406",
        "doi": "10.1088/1742-5468/abb4db",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is empirically established that order flow in the financial markets is\npositively auto-correlated and can serve as an example of a social system with\nlong-range memory. Nevertheless, widely used long-range memory estimators give\nvarying values of the Hurst exponent. We propose the burst and inter-burst\nduration statistical analysis as one more test of long-range memory and\nimplement it with the limit order book data comparing it with other widely used\nestimators. This method gives a more reliable evaluation of the Hurst exponent\nindependent of the stock in consideration or time definition used. Results\nstrengthen the expectation that burst and inter-burst duration analysis can\nserve as a better method to investigate the property of long-range memory.\n"
    },
    {
        "paper_id": 2006.00717,
        "authors": "Benjamin Avanzi and Hayden Lau and Bernard Wong",
        "title": "On the optimality of joint periodic and extraordinary dividend\n  strategies",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2021.04.033",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we model the cash surplus (or equity) of a risky business with\na Brownian motion. Owners can take cash out of the surplus in the form of\n\"dividends\", subject to transaction costs. However, if the surplus hits 0 then\nruin occurs and the business cannot operate any more.\n  We consider two types of dividend distributions: (i) periodic, regular ones\n(that is, dividends can be paid only at countable many points in time,\naccording to a specific arrival process); and (ii) extraordinary dividend\npayments that can be made immediately at any time (that is, the dividend\ndecision time space is continuous and matches that of the surplus process).\nBoth types of dividends attract proportional transaction costs, and\nextraordinary distributions also attracts fixed transaction costs, a realistic\nfeature. A dividend strategy that involves both types of distributions\n(periodic and extraordinary) is qualified as \"hybrid\".\n  We determine which strategies (either periodic, immediate, or hybrid) are\noptimal, that is, we show which are the strategies that maximise the expected\npresent value of dividends paid until ruin, net of transaction costs.\nSometimes, a liquidation strategy (which pays out all monies and stops the\nprocess) is optimal. Which strategy is optimal depends on the profitability of\nthe business, and the level of (proportional and fixed) transaction costs.\nResults are illustrated.\n"
    },
    {
        "paper_id": 2006.00739,
        "authors": "Plamen Nikolov, Nusrat Jimi",
        "title": "The Importance of Cognitive Domains and the Returns to Schooling in\n  South Africa: Evidence from Two Labor Surveys",
        "comments": null,
        "journal-ref": "Labour Economics, May, 101849 (2020)",
        "doi": "10.1016/j.labeco.2020.101849",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Numerous studies have considered the important role of cognition in\nestimating the returns to schooling. How cognitive abilities affect schooling\nmay have important policy implications, especially in developing countries\nduring periods of increasing educational attainment. Using two longitudinal\nlabor surveys that collect direct proxy measures of cognitive skills, we study\nthe importance of specific cognitive domains for the returns to schooling in\ntwo samples. We instrument for schooling levels and we find that each\nadditional year of schooling leads to an increase in earnings by approximately\n18-20 percent. The estimated effect sizes-based on the two-stage least squares\nestimates-are above the corresponding ordinary least squares estimates.\nFurthermore, we estimate and demonstrate the importance of specific cognitive\ndomains in the classical Mincer equation. We find that executive functioning\nskills (i.e., memory and orientation) are important drivers of earnings in the\nrural sample, whereas higher-order cognitive skills (i.e., numeracy) are more\nimportant for determining earnings in the urban sample. Although numeracy is\ntested in both samples, it is only a statistically significant predictor of\nearnings in the urban sample.\n"
    },
    {
        "paper_id": 2006.00754,
        "authors": "Yu-Jui Huang, Zhenhua Wang",
        "title": "Optimal Equilibria for Multi-dimensional Time-inconsistent Stopping\n  Problems",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization, Vol. 59 (2021), No. 2,\n  pp 1705-1729",
        "doi": "10.1137/20M1343774",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal stopping problem under non-exponential discounting, where\nthe state process is a multi-dimensional continuous strong Markov process. The\ndiscount function is taken to be log sub-additive, capturing decreasing\nimpatience in behavioral economics. On strength of probabilistic potential\ntheory, we establish the existence of an optimal equilibrium among a\nsufficiently large collection of equilibria, consisting of finely closed\nequilibria satisfying a boundary condition. This generalizes the existence of\noptimal equilibria for one-dimensional stopping problems in prior literature.\n"
    },
    {
        "paper_id": 2006.00775,
        "authors": "Sudhanshu Pani",
        "title": "A Theory of 'Auction as a Search' in speculative markets",
        "comments": null,
        "journal-ref": "International Journal of Financial Markets and Derivatives 2020\n  Vol.7 No.4",
        "doi": "10.1504/IJFMD.2020.111887",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The tatonnement process in high frequency order driven markets is modeled as\na search by buyers for sellers and vice-versa. We propose a total order book\nmodel, comprising limit orders and latent orders, in the absence of a market\nmaker. A zero intelligence approach of agents is employed using a\ndiffusion-drift-reaction model, to explain the trading through continuous\nauctions (price and volume). The search (levy or brownian) for transaction\nprice is the primary diffusion mechanism with other behavioural dynamics in the\nmodel inspired from foraging, chemotaxis and robotic search. Analytic and\nasymptotic analysis is provided for several scenarios and examples. Numerical\nsimulation of the model extends our understanding of the relative performance\nbetween brownian, superdiffusive and ballistic search in the model.\n"
    },
    {
        "paper_id": 2006.00916,
        "authors": "Nayara Aguiar, Indraneel Chakraborty, Vijay Gupta",
        "title": "Renewable Power Trades and Network Congestion Externalities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Integrating renewable energy production into the electricity grid is an\nimportant policy goal to address climate change. However, such an integration\nfaces economic and technological challenges. As power generation by renewable\nsources increases, power transmission patterns over the electric grid change.\nDue to physical laws, these new transmission patterns lead to non-intuitive\ngrid congestion externalities. We derive the conditions under which negative\nnetwork externalities due to power trades occur. Calibration using a stylized\nframework and data from Europe shows that each additional unit of power traded\nbetween northern and western Europe reduces transmission capacity for the\nsouthern and eastern regions by 27% per unit traded. Such externalities suggest\nthat new investments in the electric grid infrastructure cannot be made\npiecemeal. In our example, power infrastructure investment in northern and\nwestern Europe needs an accompanying investment in southern and eastern Europe\nas well. An economic challenge is regions facing externalities do not always\nhave the financial ability to invest in infrastructure. Power transit fares can\nhelp finance power infrastructure investment in regions facing network\ncongestion externalities. The resulting investment in the overall electricity\ngrid facilitates integration of renewable energy production.\n"
    },
    {
        "paper_id": 2006.00949,
        "authors": "Joseph W. Goetz, Lance Palmer, Lini Zhang, and Swarn Chatterjee",
        "title": "Changes in Household Net Financial Assets After the Great Recession: Did\n  Financial Planners Make a Difference?",
        "comments": "Forthcoming in the Journal of Personal Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study utilized the 2007-2009 Survey of Consumer Finances (SCF) panel\ndataset to examine the impact of financial planner use on household net\nfinancial asset level during the Great recession. Data included 3,862\nrespondents who completed the SCF survey and a follow up interview. The results\nindicated that starting to use a financial planner during the Great Recession\nhad a positive impact on preserving and increasing the value of households' net\nfinancial assets, while curtailing the use of a financial planner during this\ntime had a negative impact on preserving the value of households' financial\nassets. Thus, study findings indicated that the benefit of using a financial\nplanner maybe particularly high during a major financial downturn.\n"
    },
    {
        "paper_id": 2006.01037,
        "authors": "Zachary Feinstein and T. R. Hurd",
        "title": "Contingent Convertible Obligations and Financial Stability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates whether a financial system can be made more stable if\nfinancial institutions share risk by exchanging contingent convertible (CoCo)\ndebt obligations. The question is framed in a financial network model of debt\nand equity interlinkages with the addition of a variant of the CoCo that\nconverts continuously when a bank's equity-debt ratio drops to a trigger level.\nThe main theoretical result is a complete characterization of the clearing\nproblem for the interbank debt and equity at the maturity of the obligations.\nWe then introduce stylized networks to study when introducing contingent\nconvertible bonds improves financial stability, as well as specific networks\nfor which contingent convertible bonds do not provide uniformly improved system\nperformance. To return to the main question, we examine the EU financial\nnetwork at the time of the 2011 EBA stress test to do comparative statics to\nstudy the implications of CoCo debt on financial stability. It is found that by\nreplacing all unsecured interbank debt by standardized CoCo interbank debt\nsecurities, systemic risk in the EU will decrease and bank shareholder value\nwill increase.\n"
    },
    {
        "paper_id": 2006.01185,
        "authors": "Plamen Nikolov, Alan Adelman",
        "title": "Do Private Household Transfers to the Elderly Respond to Public Pension\n  Benefits? Evidence from Rural China",
        "comments": null,
        "journal-ref": "The Journal of the Economics of Ageing 14 (Fall): 100204 (2019)",
        "doi": "10.1016/j.jeoa.2019.100204",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ageing populations in developing countries have spurred the introduction of\npublic pension programs to preserve the standard of living for the elderly. The\noften-overlooked mechanism of intergenerational transfers, however, can dampen\nthese intended policy effects as adult children who make income contributions\nto their parents could adjust their behavior to changes in their parents'\nincome. Exploiting a unique policy intervention in China, we examine using a\ndifference-in-difference-in-differences (DDD) approach how a new pension\nprogram impacts inter vivos transfers. We show that pension benefits lower the\npropensity of receiving transfers from adult children in the context of a large\nmiddle-income country and we also estimate a small crowd-out effect. Taken\ntogether, these estimates fit the pattern of previous research in high-income\ncountries, although our estimates of the crowd-out effect are significantly\nsmaller than previous studies in both high-income and middle-income countries.\n"
    },
    {
        "paper_id": 2006.0129,
        "authors": "Habtamu Tilahun Kassahun, Jette Bredahl Jacobsen, Charles F. Nicholson",
        "title": "Revisiting money and labor for valuing environmental goods and services\n  in developing countries",
        "comments": "There is no difference between the previous version and the current\n  version. This version is to link with the published version DOI",
        "journal-ref": "Ecological Economics 177 (2020) 106771",
        "doi": "10.1016/j.ecolecon.2020.106771",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many Stated Preference studies conducted in developing countries provide a\nlow willingness to pay (WTP) for a wide range of goods and services. However,\nrecent studies in these countries indicate that this may partly be a result of\nthe choice of payment vehicle, not the preference for the good. Thus, low WTP\nmay not indicate a low welfare effect for public projects in developing\ncountries. We argue that in a setting where 1) there is imperfect\nsubstitutability between money and other measures of wealth (e.g. labor), and\n2) institutions are perceived to be corrupt, including payment vehicles that\nare currently available to the individual and less pron to corruption may be\nneeded to obtain valid welfare estimates. Otherwise, we risk underestimating\nthe welfare benefit of projects. We demonstrate this through a rural household\ncontingent valuation (CV) survey designed to elicit the value of access to\nreliable irrigation water in Ethiopia. Of the total average annual WTP for\naccess to reliable irrigation service, cash contribution comprises only 24.41\n%. The implication is that socially desirable projects might be rejected based\non cost-benefit analysis as a result of welfare gain underestimation due to\nmismatch of payment vehicles choice in valuation study.\n"
    },
    {
        "paper_id": 2006.01542,
        "authors": "Kaustav Das and Nicolas Langren\\'e",
        "title": "Explicit approximations of option prices via Malliavin calculus in a\n  general stochastic volatility framework",
        "comments": "arXiv admin note: text overlap with arXiv:1812.07803",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain an explicit approximation formula for European put option prices\nwithin a general stochastic volatility model with time-dependent parameters.\nOur methodology involves writing the put option price as an expectation of a\nBlack-Scholes formula, reparameterising the volatility process and then\nperforming a number of expansions. The bulk of the work is due to computing a\nnumber of expectations induced by the expansion procedure explicitly, which we\nachieve by appealing to techniques from Malliavin calculus. We obtain the\nexplicit representation of the form of the error generated by the expansion\nprocedure, and we provide sufficient ingredients in order to obtain a\nmeaningful bound. Under the assumption of piecewise-constant parameters, our\napproximation formulas become closed-form, and moreover we are able to\nestablish a fast calibration scheme. Furthermore, we perform a numerical\nsensitivity analysis to investigate the quality of our approximation formula in\nthe so-called Stochastic Verhulst model, and show that the errors are well\nwithin the acceptable range for application purposes.\n"
    },
    {
        "paper_id": 2006.01572,
        "authors": "Eckhard Platen and Stefan Tappe",
        "title": "Existence of equivalent local martingale deflators in semimartingale\n  market models",
        "comments": "41 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper offers a systematic investigation on the existence of equivalent\nlocal martingale deflators, which are multiplicative special semimartingales,\nin financial markets given by positive semimartingales. In particular, it shows\nthat the existence of such deflators can be characterized by means of the\nmodified semimartingale characteristics. Several examples illustrate our\nresults. Furthermore, we provide interpretations of the deflators from an\neconomic point of view.\n"
    },
    {
        "paper_id": 2006.01802,
        "authors": "Roger J. A. Laeven, John G. M. Schoenmakers, Nikolaus F. F. Schweizer,\n  Mitja Stadje",
        "title": "Robust Multiple Stopping -- A Pathwise Duality Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a method to solve, theoretically and numerically, general optimal\nstopping problems. Our general setting allows for multiple exercise rights,\ni.e., optimal multiple stopping, for a robust evaluation that accounts for\nmodel uncertainty, and for general reward processes driven by multi-dimensional\njump-diffusions. Our approach relies on first establishing robust martingale\ndual representation results for the multiple stopping problem that satisfy\nappealing pathwise optimality (i.e., almost sure) properties. Next, we exploit\nthese theoretical results to develop upper and lower bounds that, as we\nformally show, not only converge to the true solution asymptotically, but also\nconstitute genuine pre-limiting upper and lower bounds. We illustrate the\napplicability of our approach in a few examples and analyze the impact of model\nuncertainty on optimal multiple stopping strategies.\n"
    },
    {
        "paper_id": 2006.01911,
        "authors": "Fred Espen Benth, Nils Detering and Silvia Lavagnini",
        "title": "Accuracy of Deep Learning in Calibrating HJM Forward Curves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We price European-style options written on forward contracts in a commodity\nmarket, which we model with an infinite-dimensional Heath-Jarrow-Morton (HJM)\napproach. For this purpose we introduce a new class of state-dependent\nvolatility operators that map the square integrable noise into the\nFilipovi\\'{c} space of forward curves. For calibration, we specify a fully\nparametrized version of our model and train a neural network to approximate the\ntrue option price as a function of the model parameters. This neural network\ncan then be used to calibrate the HJM parameters based on observed option\nprices. We conduct a numerical case study based on artificially generated\noption prices in a deterministic volatility setting. In this setting we derive\nclosed pricing formulas, allowing us to benchmark the neural network based\ncalibration approach. We also study calibration in illiquid markets with a\nlarge bid-ask spread. The experiments reveal a high degree of accuracy in\nrecovering the prices after calibration, even if the original meaning of the\nmodel parameters is partly lost in the approximation step.\n"
    },
    {
        "paper_id": 2006.01979,
        "authors": "Ying Hu, Hanqing Jin and Xun Yu Zhou",
        "title": "Consistent Investment of Sophisticated Rank-Dependent Utility Agents in\n  Continuous Time",
        "comments": "44 pages, submitted already",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study portfolio selection in a complete continuous-time market where the\npreference is dictated by the rank-dependent utility. As such a model is\ninherently time inconsistent due to the underlying probability weighting, we\nstudy the investment behavior of sophisticated consistent planners who seek\n(subgame perfect) intra-personal equilibrium strategies. We provide sufficient\nconditions under which an equilibrium strategy is a replicating portfolio of a\nfinal wealth. We derive this final wealth profile explicitly, which turns out\nto be in the same form as in the classical Merton model with the market price\nof risk process properly scaled by a deterministic function in time. We present\nthis scaling function explicitly through the solution to a highly nonlinear and\nsingular ordinary differential equation, whose existence of solutions is\nestablished. Finally, we give a necessary and sufficient condition for the\nscaling function to be smaller than 1 corresponding to an effective reduction\nin risk premium due to probability weighting.\n"
    },
    {
        "paper_id": 2006.02077,
        "authors": "Nicklas Werge (LPSM), Olivier Wintenberger (LPSM)",
        "title": "AdaVol: An Adaptive Recursive Volatility Prediction Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quasi-Maximum Likelihood (QML) procedures are theoretically appealing and\nwidely used for statistical inference. While there are extensive references on\nQML estimation in batch settings, it has attracted little attention in\nstreaming settings until recently. An investigation of the convergence\nproperties of the QML procedure in a general conditionally heteroscedastic time\nseries model is conducted, and the classical batch optimization routines\nextended to the framework of streaming and large-scale problems. An adaptive\nrecursive estimation routine for GARCH models named AdaVol is presented. The\nAdaVol procedure relies on stochastic approximations combined with the\ntechnique of Variance Targeting Estimation (VTE). This recursive method has\ncomputationally efficient properties, while VTE alleviates some convergence\ndifficulties encountered by the usual QML estimation due to a lack of\nconvexity. Empirical results demonstrate a favorable trade-off between AdaVol's\nstability and the ability to adapt to time-varying estimates for real-life\ndata.\n"
    },
    {
        "paper_id": 2006.02143,
        "authors": "Plamen Nikolov",
        "title": "Time Delay and Investment Decisions: Evidence from an Experiment in\n  Tanzania",
        "comments": "Peer-reviewed article published at:\n  http://www.accessecon.com/Pubs/EB/2018/Volume38/EB-18-V38-I2-P109.pdf",
        "journal-ref": "Economics Bulletin 38 (2): 1-15 (2018)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Attitudes toward risk underlie virtually every important economic decision an\nindividual makes. In this experimental study, I examine how introducing a time\ndelay into the execution of an investment plan influences individuals' risk\npreferences. The field experiment proceeded in three stages: a decision stage,\nan execution stage and a payout stage. At the outset, in the Decision Stage\n(Stage 1), each subject was asked to make an investment plan by splitting a\nmonetary investment amount between a risky asset and a safe asset. Subjects\nwere informed that the investment plans they made in the Decision Stage are\nbinding and will be executed during the Execution Stage (Stage 2). The Payout\nStage (Stage 3) was the payout date. The timing of the Decision Stage and\nPayout Stage was the same for each subject, but the timing of the Execution\nStage varied experimentally. I find that individuals who were assigned to\nexecute their investment plans later (i.e., for whom there was a greater delay\nprior to the Execution Stage) invested a greater amount in the risky asset\nduring the Decision Stage.\n"
    },
    {
        "paper_id": 2006.02173,
        "authors": "Jun Sekine and Akihiro Tanaka",
        "title": "Notes on Backward Stochastic Differential Equations for Computing XVA",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The X-valuation adjustment (XVA) problem, which is a recent topic in\nmathematical finance, is considered and analyzed. First, the basic properties\nof backward stochastic differential equations (BSDEs) with a random horizon in\na progressively enlarged filtration are reviewed. Next, the pricing/hedging\nproblem for defaultable over-the-counter (OTC) derivative securities is\ndescribed using such BSDEs. An explicit sufficient condition is given to ensure\nthe non-existence of an arbitrage opportunity for both the seller and buyer of\nthe derivative securities. Furthermore, an explicit pricing formula is\npresented in which XVA is interpreted as approximated correction terms of the\ntheoretical fair price.\n"
    },
    {
        "paper_id": 2006.0246,
        "authors": "Sobin Joseph, Lekhapriya Dheeraj Kashyap, Shashi Jain",
        "title": "Shallow Neural Hawkes: Non-parametric kernel estimation for Hawkes\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-dimensional Hawkes process (MHP) is a class of self and mutually\nexciting point processes that find wide range of applications -- from\nprediction of earthquakes to modelling of order books in high frequency\ntrading. This paper makes two major contributions, we first find an unbiased\nestimator for the log-likelihood estimator of the Hawkes process to enable\nefficient use of the stochastic gradient descent method for maximum likelihood\nestimation. The second contribution is, we propose a specific single hidden\nlayered neural network for the non-parametric estimation of the underlying\nkernels of the MHP. We evaluate the proposed model on both synthetic and real\ndatasets, and find the method has comparable or better performance than\nexisting estimation methods. The use of shallow neural network ensures that we\ndo not compromise on the interpretability of the Hawkes model, while at the\nsame time have the flexibility to estimate any non-standard Hawkes excitation\nkernel.\n"
    },
    {
        "paper_id": 2006.02467,
        "authors": "Javad Shaabani and Ali Akbar Jafari",
        "title": "A New Look to Three-Factor Fama-French Regression Model using Sample\n  Innovations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Fama-French model is widely used in assessing the portfolio's performance\ncompared to market returns. In Fama-French models, all factors are time-series\ndata. The cross-sectional data are slightly different from the time series\ndata. A distinct problem with time-series regressions is that R-squared in time\nseries regressions is usually very high, especially compared with typical\nR-squared for cross-sectional data. The high value of R-squared may cause\nmisinterpretation that the regression model fits the observed data well, and\nthe variance in the dependent variable is explained well by the independent\nvariables. Thus, to do regression analysis, and overcome with the serial\ndependence and volatility clustering, we use standard econometrics time series\nmodels to derive sample innovations. In this study, we revisit and validate the\nFama-French models in two different ways: using the factors and asset returns\nin the Fama-French model and considering the sample innovations in the\nFama-French model instead of studying the factors. Comparing the two methods\nconsidered in this study, we suggest the Fama-French model should be considered\nwith heavy tail distributions as the tail behavior is relevant in Fama-French\nmodels, including financial data, and the QQ plot does not validate that the\nchoice of the normal distribution as the theoretical distribution for the noise\nin the model.\n"
    },
    {
        "paper_id": 2006.02596,
        "authors": "Yuan Hu, Abootaleb Shirvani, Stoyan Stoyanov, Young Shin Kim, Frank J.\n  Fabozzi, and Svetlozar T. Rachev",
        "title": "Option Pricing in Markets with Informed Traders",
        "comments": "This paper is forthcoming in the Journal of Theoretical and Applied\n  Finance and has corrections to the preprint posted on the journal's web site",
        "journal-ref": null,
        "doi": "10.1142/S0219024920500375",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this paper is to introduce the theory of option pricing for\nmarkets with informed traders within the framework of dynamic asset pricing\ntheory. We introduce new models for option pricing for informed traders in\ncomplete markets where we consider traders with information on the stock price\ndirection and stock return mean. The Black-Scholes-Merton option pricing theory\nis extended for markets with informed traders, where price processes are\nfollowing continuous-diffusions. By doing so, the discontinuity puzzle in\noption pricing is resolved. Using market option data, we estimate the implied\nsurface of the probability for a stock upturn, the implied mean stock return\nsurface, and implied trader information intensity surface.\n"
    },
    {
        "paper_id": 2006.02857,
        "authors": "Qianqian Zhou, Junyi Guo",
        "title": "Optimal Control of Investment for an Insurer in Two Currency Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the optimal investment problem of an insurer whose\nsurplus process follows the diffusion approximation of the classical\nCramer-Lundberg model. Investment in the foreign market is allowed, and\ntherefore, the foreign exchange rate model is considered and incorporated. It\nis assumed that the instantaneous mean growth rate of foreign exchange rate\nprice follows an Ornstein-Uhlenbeck process. Dynamic programming method is\nemployed to study the problem of maximizing the expected exponential utility of\nterminal wealth. By soloving the correspoding Hamilton-Jacobi-Bellman\nequations, the optimal investment strategies and the value functions are\nobtained. Finally, numerical analysis is presented.\n"
    },
    {
        "paper_id": 2006.029,
        "authors": "Plamen Nikolov, Alan Adelman",
        "title": "Short-Run Health Consequences of Retirement and Pension Benefits:\n  Evidence from China",
        "comments": "arXiv admin note: text overlap with arXiv:2006.01185",
        "journal-ref": "Forum for Health Economics & Policy 21 (2): 1-27 (2019)",
        "doi": "10.1515/fhep-2017-0031",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the impact of the New Rural Pension Scheme (NRPS) in\nChina. Exploiting the staggered implementation of an NRPS policy expansion that\nbegan in 2009, we use a difference-in-difference approach to study the effects\nof the introduction of pension benefits on the health status, health behaviors,\nand healthcare utilization of rural Chinese adults age 60 and above. The\nresults point to three main conclusions. First, in addition to improvements in\nself-reported health, older adults with access to the pension program\nexperienced significant improvements in several important measures of health,\nincluding mobility, self-care, usual activities, and vision. Second, regarding\nthe functional domains of mobility and self-care, we found that the females in\nthe study group led in improvements over their male counterparts. Third, in our\nsearch for the mechanisms that drive positive retirement program results, we\nfind evidence that changes in individual health behaviors, such as a reduction\nin drinking and smoking, and improved sleep habits, play an important role. Our\nfindings point to the potential benefits of retirement programs resulting from\nsocial spillover effects. In addition, these programs may lessen the morbidity\nburden among the retired population.\n"
    },
    {
        "paper_id": 2006.02977,
        "authors": "Amine Ouazad",
        "title": "Coastal Flood Risk in the Mortgage Market: Storm Surge Models'\n  Predictions vs. Flood Insurance Maps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prior literature has argued that flood insurance maps may not capture the\nextent of flood risk. This paper performs a granular assessment of coastal\nflood risk in the mortgage market by using physical simulations of hurricane\nstorm surge heights instead of using FEMA's flood insurance maps. Matching\nneighborhood-level predicted storm surge heights with mortgage files suggests\nthat coastal flood risk may be large: originations and securitizations in storm\nsurge areas have been rising sharply since 2012, while they remain stable when\nusing flood insurance maps. Every year, more than 50 billion dollars of\noriginations occur in storm surge areas outside of insurance floodplains. The\nshare of agency mortgages increases in storm surge areas, yet remains stable in\nthe flood insurance 100-year floodplain. Mortgages in storm surge areas are\nmore likely to be complex: non-fully amortizing features such as interest-only\nor adjustable rates. Households may also be more vulnerable in storm surge\nareas: median household income is lower, the share of African Americans and\nHispanics is substantially higher, the share of individuals with health\ncoverage is lower. Price-to-rent ratios are declining in storm surge areas\nwhile they are increasing in flood insurance areas. This paper suggests that\nuncovering future financial flood risk requires scientific models that are\nindependent of the flood insurance mapping process.\n"
    },
    {
        "paper_id": 2006.03014,
        "authors": "Ioannis Anagnostou, Tiziano Squartini, Drona Kandhai, Diego\n  Garlaschelli",
        "title": "Uncovering the mesoscale structure of the credit default swap market to\n  improve portfolio risk modelling",
        "comments": "Quantitative Finance (2021)",
        "journal-ref": "Quantitative Finance, 21:9, 1501-1518 (2021)",
        "doi": "10.1080/14697688.2021.1890807",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the most challenging aspects in the analysis and modelling of\nfinancial markets, including Credit Default Swap (CDS) markets, is the presence\nof an emergent, intermediate level of structure standing in between the\nmicroscopic dynamics of individual financial entities and the macroscopic\ndynamics of the market as a whole. This elusive, mesoscopic level of\norganisation is often sought for via factor models that ultimately decompose\nthe market according to geographic regions and economic industries. However, at\na more general level the presence of mesoscopic structure might be revealed in\nan entirely data-driven approach, looking for a modular and possibly\nhierarchical organisation of the empirical correlation matrix between financial\ntime series. The crucial ingredient in such an approach is the definition of an\nappropriate null model for the correlation matrix. Recent research showed that\ncommunity detection techniques developed for networks become intrinsically\nbiased when applied to correlation matrices. For this reason, a method based on\nRandom Matrix Theory has been developed, which identifies the optimal\nhierarchical decomposition of the system into internally correlated and\nmutually anti-correlated communities. Building upon this technique, here we\nresolve the mesoscopic structure of the CDS market and identify groups of\nissuers that cannot be traced back to standard industry/region taxonomies,\nthereby being inaccessible to standard factor models. We use this decomposition\nto introduce a novel default risk model that is shown to outperform more\ntraditional alternatives.\n"
    },
    {
        "paper_id": 2006.03023,
        "authors": "Geoffrey Goodell, Hazem Danny Al-Nakib, Paolo Tasca",
        "title": "Digital Currency and Economic Crises: Helping States Respond",
        "comments": "32 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The current crisis, at the time of writing, has had a profound impact on the\nfinancial world, introducing the need for creative approaches to revitalising\nthe economy at the micro level as well as the macro level. In this informal\nanalysis and design proposal, we describe how infrastructure for digital assets\ncan serve as a useful monetary and fiscal policy tool and an enabler of\nexisting tools in the future, particularly during crises, while aligning the\ntrajectory of financial technology innovation toward a brighter future. We\npropose an approach to digital currency that would allow people without banking\nrelationships to transact electronically and privately, including both internet\npurchases and point-of-sale purchases that are required to be cashless. We also\npropose an approach to digital currency that would allow for more efficient and\ntransparent clearing and settlement, implementation of monetary and fiscal\npolicy, and management of systemic risk. The digital currency could be\nimplemented as central bank digital currency (CBDC), or it could be issued by\nthe government and collateralised by public funds or Treasury assets. Our\nproposed architecture allows both manifestations and would be operated by banks\nand other money services businesses, operating within a framework overseen by\ngovernment regulators. We argue that now is the time for action to undertake\ndevelopment of such a system, not only because of the current crisis but also\nin anticipation of future crises resulting from geopolitical risks, the\ncontinued globalisation of the digital economy, and the changing value and\nrisks that technology brings.\n"
    },
    {
        "paper_id": 2006.03132,
        "authors": "Lars Elend, Sebastian A. Tideman, Kerstin Lopatta, Oliver Kramer",
        "title": "Earnings Prediction with Deep Learning",
        "comments": "7 pages, 4 figures, 2 tables",
        "journal-ref": "LNCS 12325 (2020) 267-274",
        "doi": "10.1007/978-3-030-58285-2_22",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the financial sector, a reliable forecast the future financial performance\nof a company is of great importance for investors' investment decisions. In\nthis paper we compare long-term short-term memory (LSTM) networks to temporal\nconvolution network (TCNs) in the prediction of future earnings per share\n(EPS). The experimental analysis is based on quarterly financial reporting data\nand daily stock market returns. For a broad sample of US firms, we find that\nboth LSTMs outperform the naive persistent model with up to 30.0% more accurate\npredictions, while TCNs achieve and an improvement of 30.8%. Both types of\nnetworks are at least as accurate as analysts and exceed them by up to 12.2%\n(LSTM) and 13.2% (TCN).\n"
    },
    {
        "paper_id": 2006.03301,
        "authors": "Olli Palm\\'en",
        "title": "Inflation Dynamics of Financial Shocks",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the effects of financial shocks on the United States economy by\nusing a Bayesian structural vector autoregressive (SVAR) model that exploits\nthe non-normalities in the data. We use this method to uniquely identify the\nmodel and employ inequality constraints to single out financial shocks. The\nresults point to the existence of two distinct financial shocks that have\nopposing effects on inflation, which supports the idea that financial shocks\nare transmitted to the real economy through both demand and supply side\nchannels.\n"
    },
    {
        "paper_id": 2006.03441,
        "authors": "Tjeerd de Vries, Alexis Akira Toda",
        "title": "Capital and Labor Income Pareto Exponents across Time and Space",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/roiw.12556",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We estimate capital and labor income Pareto exponents across 475 country-year\nobservations that span 52 countries over half a century (1967-2018). We\ndocument two stylized facts: (i) capital income is more unequally distributed\nthan labor income in the tail; namely, the capital exponent (1-3, median 1.46)\nis smaller than labor (2-5, median 3.35), and (ii) capital and labor exponents\nare nearly uncorrelated. To explain these findings, we build an incomplete\nmarket model with job ladders and capital income risk that gives rise to a\ncapital income Pareto exponent smaller than but nearly unrelated to the labor\nexponent. Our results suggest the importance of distinguishing income and\nwealth inequality.\n"
    },
    {
        "paper_id": 2006.03458,
        "authors": "Alessandra Amendola, Vincenzo Candila, Fabrizio Cipollini, Giampiero\n  M. Gallo",
        "title": "Doubly Multiplicative Error Models with Long- and Short-run Components",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest the Doubly Multiplicative Error class of models (DMEM) for\nmodeling and forecasting realized volatility, which combines two components\naccommodating low-, respectively, high-frequency features in the data. We\nderive the theoretical properties of the Maximum Likelihood and Generalized\nMethod of Moments estimators. Two such models are then proposed, the\nComponent-MEM, which uses daily data for both components, and the MEM-MIDAS,\nwhich exploits the logic of MIxed-DAta Sampling (MIDAS). The empirical\napplication involves the S&P 500, NASDAQ, FTSE 100 and Hang Seng indices:\nirrespective of the market, both DMEM's outperform the HAR and other relevant\nGARCH-type models.\n"
    },
    {
        "paper_id": 2006.03498,
        "authors": "Yujie Hu, Fahui Wang, Chester Wilmot",
        "title": "Commuting Variability by Wage Groups in Baton Rouge 1990-2010",
        "comments": null,
        "journal-ref": "Papers in Applied Geography, 3(1), 14-29 (2017)",
        "doi": "10.1080/23754931.2016.1248577",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Residential segregation recently has shifted to more class or income-based in\nthe United States, and neighborhoods are undergoing significant changes such as\ncommuting patterns over time. To better understand the commuting inequality\nacross neighborhoods of different income levels, this research analyzes\ncommuting variability (in both distance and time) across wage groups as well as\nstability over time using the CTPP data 1990-2010 in Baton Rouge. In comparison\nto previous work, commuting distance is estimated more accurately by Monte\nCarlo simulation of individual trips to mitigate aggregation error and scale\neffect. The results based on neighborhoods mean wage rate indicate that\ncommuting behaviors vary across areas of different wage rates and such\nvariability is captured by a convex shape. Affluent neighborhoods tended to\ncommute more but highest-wage neighborhoods retreated for less commuting. This\ntrend remains relatively stable over time despite an overall transportation\nimprovement in general. A complementary analysis based on the distribution of\nwage groups is conducted to gain more detailed insights and uncovers the\nlasting poor mobility (e.g., fewer location and transport options) of the\nlowest-wage workers in 1990-2010.\n"
    },
    {
        "paper_id": 2006.03592,
        "authors": "Olli Palm\\'en",
        "title": "Sovereign Default Risk and Credit Supply: Evidence from the Euro Area",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": "10.1016/j.jimonfin.2020.102257",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Did sovereign default risk affect macroeconomic activity through firms'\naccess to credit during the European sovereign debt crisis? We investigate this\nquestion by a estimating a structural panel vector autoregressive model for\nItaly, Spain, Portugal, and Ireland, where the sovereign risk shock is\nidentified using sign restrictions. The results suggest that decline in the\ncreditworthiness of the sovereign contributed to a fall in private lending and\neconomic activity in several euro-area countries by reducing the value of\nbanks' assets and crowding out private lending.\n"
    },
    {
        "paper_id": 2006.03618,
        "authors": "Mariola Ndrio, Subhonmesh Bose, Lang Tong and Ye Guo",
        "title": "Coordinated Transaction Scheduling in Multi-Area Electricity Markets:\n  Equilibrium and Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tie-line scheduling in multi-area power systems in the US largely proceeds\nthrough a market-based mechanism called Coordinated Transaction Scheduling\n(CTS). We analyze this market mechanism through a game-theoretic lens. Our\nanalysis characterizes the effect of market liquidity, market participants'\nforecasts about inter-area price spreads, transactions fees and coupling of CTS\nmarkets with up-to-congestion virtual transactions. Using real data, we\nempirically verify that CTS bidders can employ simple learning algorithms to\ndiscover Nash equilibria that support the conclusions drawn from equilibrium\nanalysis.\n"
    },
    {
        "paper_id": 2006.0365,
        "authors": "Nusrat Abedin Jimi, Plamen Nikolov, Mohammad Abdul Malek, Subal\n  Kumbhakar",
        "title": "The Effects of Access to Credit on Productivity Among Microenterprises:\n  Separating Technological Changes from Changes in Technical Efficiency",
        "comments": null,
        "journal-ref": "Journal of Productivity Analysis, 52 (2019): 37-55 (2019)",
        "doi": "10.1007/s11123-019-00555-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Improving productivity among farm microenterprises is important, especially\nin low-income countries where market imperfections are pervasive and resources\nare scarce. Relaxing credit constraints can increase the productivity of\nfarmers. Using a field experiment involving microenterprises in Bangladesh, we\nestimate the impact of access to credit on the overall productivity of rice\nfarmers, and disentangle the total effect into technological change (frontier\nshift) and technical efficiency changes. We find that relative to the baseline\nrice output per decimal, access to credit results in, on average, approximately\na 14 percent increase in yield, holding all other inputs constant. After\ndecomposing the total effect into the frontier shift and efficiency\nimprovement, we find that, on average, around 11 percent of the increase in\noutput comes from changes in technology, or frontier shift, while the remaining\n3 percent is attributed to improvements in technical efficiency. The efficiency\ngain is higher for modern hybrid rice varieties, and almost zero for\ntraditional rice varieties. Within the treatment group, the effect is greater\namong pure tenant and mixed-tenant farm households compared with farmers that\nonly cultivate their own land.\n"
    },
    {
        "paper_id": 2006.03686,
        "authors": "Jun-Hao Chen and Samuel Yen-Chi Chen and Yun-Cheng Tsai and\n  Chih-Shiang Shur",
        "title": "Adversarial Robustness of Deep Convolutional Candlestick Learner",
        "comments": "arXiv admin note: text overlap with arXiv:2005.06731",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep learning (DL) has been applied extensively in a wide range of fields.\nHowever, it has been shown that DL models are susceptible to a certain kinds of\nperturbations called \\emph{adversarial attacks}. To fully unlock the power of\nDL in critical fields such as financial trading, it is necessary to address\nsuch issues. In this paper, we present a method of constructing perturbed\nexamples and use these examples to boost the robustness of the model. Our\nalgorithm increases the stability of DL models for candlestick classification\nwith respect to perturbations in the input data.\n"
    },
    {
        "paper_id": 2006.03718,
        "authors": "Timothy J. Garrett, Matheus R. Grasselli, Stephen Keen",
        "title": "Past production constrains current energy demands: persistent scaling in\n  global energy consumption and implications for climate change mitigation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0237672",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate change has become intertwined with the global economy. Here, we\ndescribe the importance of inertia to continued growth in energy consumption.\nDrawing from thermodynamic arguments, and using 38 years of available\nstatistics between 1980 to 2017, we find a persistent time-independent scaling\nbetween the historical time integral $W$ of world inflation-adjusted economic\nproduction $Y$, or $W\\left(t\\right) = \\int_0^t Y\\left(t'\\right)dt'$, and\ncurrent rates of world primary energy consumption $\\mathcal E$, such that\n$\\lambda = \\mathcal{E}/W = 5.9\\pm0.1$ Gigawatts per trillion 2010 US dollars.\nThis empirical result implies that population expansion is a symptom rather\nthan a cause of the current exponential rise in $\\mathcal E$ and carbon dioxide\nemissions $C$, and that it is past innovation of economic production efficiency\n$Y/\\mathcal{E}$ that has been the primary driver of growth, at predicted rates\nthat agree well with data. Options for stabilizing $C$ are then limited to\nrapid decarbonization of $\\mathcal E$ through sustained implementation of over\none Gigawatt of renewable or nuclear power capacity per day. Alternatively,\nassuming continued reliance on fossil fuels, civilization could shift to a\nsteady-state economy that devotes economic production exclusively to\nmaintenance rather than expansion. If this were instituted immediately,\ncontinual energy consumption would still be required, so atmospheric carbon\ndioxide concentrations would not balance natural sinks until concentrations\nexceeded 500 ppmv, and double pre-industrial levels if the steady-state was\nattained by 2030.\n"
    },
    {
        "paper_id": 2006.03723,
        "authors": "Plamen Nikolov, Nusrat Jimi",
        "title": "What Factors Drive Individual Misperceptions of the Returns to Schooling\n  in Tanzania? Some Lessons for Education Policy",
        "comments": null,
        "journal-ref": "Applied Economics: Vol 50, No 44. Applied Economics 50 (44):\n  4705-23",
        "doi": "10.1080/00036846.2018.1466991",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Evidence on educational returns and the factors that determine the demand for\nschooling in developing countries is extremely scarce. Building on previous\nstudies that show individuals underestimating the returns to schooling, we use\ntwo surveys from Tanzania to estimate both the actual and perceived schooling\nreturns and subsequently examine what factors drive individual misperceptions\nregarding actual returns. Using ordinary least squares and instrumental\nvariable methods, we find that each additional year of schooling in Tanzania\nincreases earnings, on average, by 9 to 11 percent. We find that on average\nindividuals underestimate returns to schooling by 74 to 79 percent and three\nfactors are associated with these misperceptions: income, asset poverty and\neducational attainment. Shedding light on what factors relate to individual\nbeliefs about educational returns can inform policy on how to structure\neffective interventions in order to correct individual misperceptions.\n"
    },
    {
        "paper_id": 2006.04212,
        "authors": "Junyi Li, Xitong Wang, Yaoyang Lin, Arunesh Sinha, Micheal P. Wellman",
        "title": "Generating Realistic Stock Market Order Streams",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an approach to generate realistic and high-fidelity stock market\ndata based on generative adversarial networks (GANs). Our Stock-GAN model\nemploys a conditional Wasserstein GAN to capture history dependence of orders.\nThe generator design includes specially crafted aspects including components\nthat approximate the market's auction mechanism, augmenting the order history\nwith order-book constructions to improve the generation task. We perform an\nablation study to verify the usefulness of aspects of our network structure. We\nprovide a mathematical characterization of distribution learned by the\ngenerator. We also propose statistics to measure the quality of generated\norders. We test our approach with synthetic and actual market data, compare to\nmany baseline generative models, and find the generated data to be close to\nreal data.\n"
    },
    {
        "paper_id": 2006.04382,
        "authors": "Ren\\'e A\\\"id, Luciano Campi, Liangchen Li, Mike Ludkovski",
        "title": "An Impulse-Regime Switching Game Model of Vertical Competition",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a new kind of non-zero-sum stochastic differential game with mixed\nimpulse/switching controls, motivated by strategic competition in commodity\nmarkets. A representative upstream firm produces a commodity that is used by a\nrepresentative downstream firm to produce a final consumption good. Both firms\ncan influence the price of the commodity. By shutting down or increasing\ngeneration capacities, the upstream firm influences the price with impulses. By\nswitching (or not) to a substitute, the downstream firm influences the drift of\nthe commodity price process. We study the resulting impulse--regime switching\ngame between the two firms, focusing on explicit threshold-type equilibria.\nRemarkably, this class of games naturally gives rise to multiple Nash\nequilibria, which we obtain via a verification based approach. We exhibit three\ntypes of equilibria depending on the ultimate number of switches by the\ndownstream firm (zero, one or an infinite number of switches). We illustrate\nthe diversification effect provided by vertical integration in the specific\ncase of the crude oil market. Our analysis shows that the diversification gains\nstrongly depend on the pass-through from the crude price to the gasoline price.\n"
    },
    {
        "paper_id": 2006.04624,
        "authors": "Alje van Dam and Koen Frenken",
        "title": "Vertical vs. Horizontal Policy in a Capabilities Model of Economic\n  Development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Against the background of renewed interest in vertical support policies\ntargeting specific industries or technologies, we investigate the effects of\nvertical vs. horizontal policies in a combinatorial model of economic\ndevelopment. In the framework we propose, an economy develops by acquiring new\ncapabilities allowing for the production of an ever greater variety of products\nwith an increasing complexity. Innovation policy can aim to expand the number\nof capabilities (vertical policy) or the ability to combine capabilities\n(horizontal policy). The model shows that for low-income countries, the two\npolicies are complementary. For high-income countries that are specialised in\nthe most complex products, focusing on horizontal policy only yields the\nhighest returns. We reflect on the model results in the light of the\ncontemporary debate on vertical policy.\n"
    },
    {
        "paper_id": 2006.04639,
        "authors": "Jozef Barunik and Michael Ellington",
        "title": "Dynamic Network Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the pricing of short-term and long-term dynamic network\nrisk in the cross-section of stock returns. Stocks with high sensitivities to\ndynamic network risk earn lower returns. We rationalize our finding with\neconomic theory that allows the stochastic discount factor to load on network\nrisk through the precautionary savings channel. A one-standard deviation\nincrease in long-term (short-term) network risk loadings associate with a 7.66%\n(6.71%) drop in annualized expected returns.\n"
    },
    {
        "paper_id": 2006.04659,
        "authors": "Jean-Philippe Aguilar",
        "title": "Explicit option valuation in the exponential NIG model",
        "comments": "V3: practical details and comparisons with FFT added, 42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide closed-form pricing formulas for a wide variety of\npath-independent options, in the exponential L\\'evy model driven by the Normal\ninverse Gaussian process. The results are obtained in both the symmetric and\nasymmetric model, and take the form of simple and quickly convergent series,\nunder some condition involving the log-forward moneyness and the maturity of\ninstruments. Proofs are based on a factorized representation in the Mellin\nspace for the price of an arbitrary path-independent payoff, and on tools from\ncomplex analysis. The validity of the results is assessed thanks to several\ncomparisons with standard numerical methods (Fourier and Fast Fourier\ntransforms, Monte-Carlo simulations) for realistic sets of parameters. Precise\nbounds for the convergence speed and the truncation error are also provided.\n"
    },
    {
        "paper_id": 2006.04687,
        "authors": "Michael Monoyios",
        "title": "Duality for optimal consumption under no unbounded profit with bounded\n  risk",
        "comments": "Final publication version, some minor corrections added. arXiv admin\n  note: substantial text overlap with arXiv:2009.00972, arXiv:2011.00732",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a definitive treatment of duality for optimal consumption over the\ninfinite horizon, in a semimartingale incomplete market satisfying no unbounded\nprofit with bounded risk (NUPBR). Rather than base the dual domain on (local)\nmartingale deflators, we use a class of supermartingale deflators such that\ndeflated wealth plus cumulative deflated consumption is a supermartingale for\nall admissible consumption plans. This yields a strong duality, because the\nenlarged dual domain of processes dominated by deflators is naturally closed,\nwithout invoking its closure. In this way we automatically reach the bipolar of\nthe set of deflators. We complete this picture by proving that the set of\nprocesses dominated by local martingale deflators is dense in our dual domain,\nconfirming that we have identified the natural dual space. In addition to the\noptimal consumption and deflator, we characterise the optimal wealth process.\nAt the optimum, deflated wealth is a supermartingale and a potential, while\ndeflated wealth plus cumulative deflated consumption is a uniformly integrable\nmartingale. This is the natural generalisation of the corresponding feature in\nthe terminal wealth problem, where deflated wealth at the optimum is a\nuniformly integrable martingale. We use no constructions involving equivalent\nlocal martingale measures. This is natural, given that such measures typically\ndo not exist over the infinite horizon and that we are working under NUPBR,\nwhich does not require their existence. The structure of the duality proof\nreveals an interesting feature compared with the terminal wealth problem.\nThere, the dual domain is $L^{1}$-bounded, but here the primal domain has this\nproperty, and hence many steps in the duality proof show a marked reversal of\nroles for the primal and dual domains, compared with the proofs of Kramkov and\nSchachermayer.\n"
    },
    {
        "paper_id": 2006.04727,
        "authors": "Calypso Herrera, Florian Krach, Josef Teichmann",
        "title": "Neural Jump Ordinary Differential Equations: Consistent Continuous-Time\n  Prediction and Filtering",
        "comments": null,
        "journal-ref": "International Conference on Learning Representations (2021)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Combinations of neural ODEs with recurrent neural networks (RNN), like\nGRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time\nseries. While those models outperform existing discrete-time approaches, no\ntheoretical guarantees for their predictive capabilities are available.\nAssuming that the irregularly-sampled time series data originates from a\ncontinuous stochastic process, the $L^2$-optimal online prediction is the\nconditional expectation given the currently available information. We introduce\nthe Neural Jump ODE (NJ-ODE) that provides a data-driven approach to learn,\ncontinuously in time, the conditional expectation of a stochastic process. Our\napproach models the conditional expectation between two observations with a\nneural ODE and jumps whenever a new observation is made. We define a novel\ntraining framework, which allows us to prove theoretical guarantees for the\nfirst time. In particular, we show that the output of our model converges to\nthe $L^2$-optimal prediction. This can be interpreted as solution to a special\nfiltering problem. We provide experiments showing that the theoretical results\nalso hold empirically. Moreover, we experimentally show that our model\noutperforms the baselines in more complex learning tasks and give comparisons\non real-world datasets.\n"
    },
    {
        "paper_id": 2006.04968,
        "authors": "Afrouz Azadikhah Jahromi, Brantly Callaway",
        "title": "Heterogeneous Effects of Job Displacement on Earnings",
        "comments": "40 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers how the effect of job displacement varies across\ndifferent individuals. In particular, our interest centers on features of the\ndistribution of the individual-level effect of job displacement. Identifying\nfeatures of this distribution is particularly challenging -- e.g., even if we\ncould randomly assign workers to be displaced or not, many of the parameters\nthat we consider would not be point identified. We exploit our access to panel\ndata, and our approach relies on comparing outcomes of displaced workers to\noutcomes the same workers would have experienced if they had not been displaced\nand if they maintained the same rank in the distribution of earnings as they\nhad before they were displaced. Using data from the Displaced Workers Survey,\nwe find that displaced workers earn about $157 per week less, on average, than\nthey would have earned if they had not been displaced. We also find that there\nis substantial heterogeneity. We estimate that 42% of workers have higher\nearnings than they would have had if they had not been displaced and that a\nlarge fraction of workers have experienced substantially more negative effects\nthan the average effect of displacement. Finally, we also document major\ndifferences in the distribution of the effect of job displacement across\neducation levels, sex, age, and counterfactual earnings levels. Throughout the\npaper, we rely heavily on quantile regression. First, we use quantile\nregression as a flexible (yet feasible) first step estimator of conditional\ndistributions and quantile functions that our main results build on. We also\nuse quantile regression to study how covariates affect the distribution of the\nindividual-level effect of job displacement.\n"
    },
    {
        "paper_id": 2006.04992,
        "authors": "Akash Doshi, Alexander Issa, Puneet Sachdeva, Sina Rafati, Somnath\n  Rakshit",
        "title": "Deep Stock Predictions",
        "comments": null,
        "journal-ref": "arXiv preprint arXiv:2006.04992",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasting stock prices can be interpreted as a time series prediction\nproblem, for which Long Short Term Memory (LSTM) neural networks are often used\ndue to their architecture specifically built to solve such problems. In this\npaper, we consider the design of a trading strategy that performs portfolio\noptimization using the LSTM stock price prediction for four different\ncompanies. We then customize the loss function used to train the LSTM to\nincrease the profit earned. Moreover, we propose a data driven approach for\noptimal selection of window length and multi-step prediction length, and\nconsider the addition of analyst calls as technical indicators to a multi-stack\nBidirectional LSTM strengthened by the addition of Attention units. We find the\nLSTM model with the customized loss function to have an improved performance in\nthe training bot over a regressive baseline such as ARIMA, while the addition\nof analyst call does improve the performance for certain datasets.\n"
    },
    {
        "paper_id": 2006.05204,
        "authors": "Dmitry B. Rokhlin",
        "title": "Relative utility bounds for empirically optimal portfolios",
        "comments": "20 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a single-period portfolio selection problem for an investor,\nmaximizing the expected ratio of the portfolio utility and the utility of a\nbest asset taken in hindsight. The decision rules are based on the history of\nstock returns with unknown distribution. Assuming that the utility function is\nLipschitz or H\\\"{o}lder continuous (the concavity is not required), we obtain\nhigh probability utility bounds under the sole assumption that the returns are\nindependent and identically distributed. These bounds depend only on the\nutility function, the number of assets and the number of observations. For\nconcave utilities similar bounds are obtained for the portfolios produced by\nthe exponentiated gradient method. Also we use statistical experiments to study\nrisk and generalization properties of empirically optimal portfolios. Herein we\nconsider a model with one risky asset and a dataset, containing the stock\nprices from NYSE.\n"
    },
    {
        "paper_id": 2006.0526,
        "authors": "Martin Herdegen, David Hobson, Joseph Jerome",
        "title": "An elementary approach to the Merton problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider the infinite-horizon Merton\ninvestment-consumption problem in a constant-parameter Black - Scholes - Merton\nmarket for an agent with constant relative risk aversion R. The classical\nprimal approach is to write down a candidate value function and to use a\nverification argument to prove that this is the solution to the problem.\nHowever, features of the problem take it outside the standard settings of\nstochastic control, and the existing primal verification proofs rely on\nparameter restrictions (especially, but not only, R<1), restrictions on the\nspace of admissible strategies, or intricate approximation arguments.\n  The purpose of this paper is to show that these complications can be overcome\nusing a simple and elegant argument involving a stochastic perturbation of the\nutility function.\n"
    },
    {
        "paper_id": 2006.05515,
        "authors": "Micha\\\"el Karpe",
        "title": "An overall view of key problems in algorithmic trading and recent\n  progress",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We summarize the fundamental issues at stake in algorithmic trading, and the\nprogress made in this field over the last twenty years. We first present the\nkey problems of algorithmic trading, describing the concepts of optimal\nexecution, optimal placement, and price impact. We then discuss the most recent\nadvances in algorithmic trading through the use of Machine Learning, discussing\nthe use of Deep Learning, Reinforcement Learning, and Generative Adversarial\nNetworks.\n"
    },
    {
        "paper_id": 2006.05574,
        "authors": "Micha\\\"el Karpe, Jin Fang, Zhongyao Ma, Chen Wang",
        "title": "Multi-Agent Reinforcement Learning in a Realistic Limit Order Book\n  Market Simulation",
        "comments": "7 pages, accepted for inclusion in the 2020 ACM International\n  Conference on AI in Finance (ICAIF 2020)",
        "journal-ref": null,
        "doi": "10.1145/3383455.3422570",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal order execution is widely studied by industry practitioners and\nacademic researchers because it determines the profitability of investment\ndecisions and high-level trading strategies, particularly those involving large\nvolumes of orders. However, complex and unknown market dynamics pose\nsignificant challenges for the development and validation of optimal execution\nstrategies. In this paper, we propose a model-free approach by training\nReinforcement Learning (RL) agents in a realistic market simulation environment\nwith multiple agents. First, we configure a multi-agent historical order book\nsimulation environment for execution tasks built on an Agent-Based Interactive\nDiscrete Event Simulation (ABIDES) [arXiv:1904.12066]. Second, we formulate the\nproblem of optimal execution in an RL setting where an intelligent agent can\nmake order execution and placement decisions based on market microstructure\ntrading signals in High Frequency Trading (HFT). Third, we develop and train an\nRL execution agent using the Double Deep Q-Learning (DDQL) algorithm in the\nABIDES environment. In some scenarios, our RL agent converges towards a\nTime-Weighted Average Price (TWAP) strategy. Finally, we evaluate the\nsimulation with our RL agent by comparing it with a market replay simulation\nusing real market Limit Order Book (LOB) data.\n"
    },
    {
        "paper_id": 2006.05632,
        "authors": "Zura Kakushadze",
        "title": "Quant Bust 2020",
        "comments": "29 pages; to appear in the Apr-Jun 2020 issue of The World Economics\n  Journal",
        "journal-ref": "World Economics 21(2) (2020) 183-217",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explain in a nontechnical fashion why dollar-neutral quant trading\nstrategies, such as equities Statistical Arbitrage, suffered substantial losses\n(drawdowns) during the COVID-19 market selloff. We discuss: (i) why these\nstrategies work during \"normal\" times; (ii) the market regimes when they work\nbest; and (iii) their limitations and the reasons for why they \"break\" during\nextreme market events. An accompanying appendix (with a link to freely\naccessible source code) includes backtests for various strategies, which put\nflesh on and illustrate the discussion in the main text.\n"
    },
    {
        "paper_id": 2006.0564,
        "authors": "Yeon-Koo Che, Chongwoo Choe, Keeyoung Rhee",
        "title": "Bailout Stigma",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model of bailout stigma where accepting a bailout signals a\nfirm's balance-sheet weakness and worsens its funding prospect. To avoid\nstigma, high-quality firms either withdraw from subsequent financing after\nreceiving bailouts or refuse bailouts altogether to send a favorable signal.\nThe former leads to a short-lived stimulation with a subsequent market freeze\neven worse than if there were no bailouts. The latter revives the funding\nmarket, albeit with delay, to the level achievable without any stigma, and\nimplements a constrained optimal outcome. A menu of multiple bailout programs\nalso compounds bailout stigma and worsens market freeze.\n"
    },
    {
        "paper_id": 2006.05678,
        "authors": "Mateusz Iwo Dubaniowski, Hans R. Heinimann",
        "title": "A framework for modeling interdependencies among households, businesses,\n  and infrastructure systems; and their response to disruptions",
        "comments": "34 pages, 10 figures, 5 tables, Accepted in Reliability Engineering &\n  System Safety",
        "journal-ref": null,
        "doi": "10.1016/j.ress.2020.107063",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Urban systems, composed of households, businesses, and infrastructures, are\ncontinuously evolving and expanding. This has several implications because the\nimpacts of disruptions, and the complexity and interdependence of systems, are\nrapidly increasing. Hence, we face a challenge in how to improve our\nunderstanding about the interdependencies among those entities, as well as\ntheir responses to disruptions. The aims of this study were to (1) create an\nagent that mimics the metabolism of a business or household that obtains\nsupplies from and provides output to infrastructure systems; (2) implement a\nnetwork of agents that exchange resources, as coordinated with a price\nmechanism; and (3) test the responses of this prototype model to disruptions.\nOur investigation resulted in the development of a business/household agent and\na dynamically self-organizing mechanism of network coordination under\ndisruption based on costs for production and transportation. Simulation\nexperiments confirmed the feasibility of this new model for analyzing responses\nto disruptions. Among the nine disruption scenarios considered, in line with\nour expectations, the one combining the failures of infrastructure links and\nproduction processes had the most negative impact. We also identified areas for\nfuture research that focus on network topologies, mechanisms for resource\nallocation, and disruption generation.\n"
    },
    {
        "paper_id": 2006.0575,
        "authors": "Christoph Berninger, Almond St\\\"ocker, David R\\\"ugamer",
        "title": "A Bayesian Time-Varying Autoregressive Model for Improved Short- and\n  Long-Term Prediction",
        "comments": "Revised Introduction, results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the application to German interest rates, we propose a\ntimevarying autoregressive model for short and long term prediction of time\nseries that exhibit a temporary non-stationary behavior but are assumed to mean\nrevert in the long run. We use a Bayesian formulation to incorporate prior\nassumptions on the mean reverting process in the model and thereby regularize\npredictions in the far future. We use MCMC-based inference by deriving relevant\nfull conditional distributions and employ a Metropolis-Hastings within Gibbs\nSampler approach to sample from the posterior (predictive) distribution. In\ncombining data-driven short term predictions with long term distribution\nassumptions our model is competitive to the existing methods in the short\nhorizon while yielding reasonable predictions in the long run. We apply our\nmodel to interest rate data and contrast the forecasting performance to the one\nof a 2-Additive-Factor Gaussian model as well as to the predictions of a\ndynamic Nelson-Siegel model.\n"
    },
    {
        "paper_id": 2006.05784,
        "authors": "Thibaut Th\\'eate, S\\'ebastien Mathieu and Damien Ernst",
        "title": "An Artificial Intelligence Solution for Electricity Procurement in\n  Forward Markets",
        "comments": "Scientific article accepted for publication in the Energies journal\n  edited by MDPI",
        "journal-ref": "Energies 2020, 13(23), 6435",
        "doi": "10.3390/en13236435",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Retailers and major consumers of electricity generally purchase an important\npercentage of their estimated electricity needs years ahead in the forward\nmarket. This long-term electricity procurement task consists of determining\nwhen to buy electricity so that the resulting energy cost is minimised, and the\nforecast consumption is covered. In this scientific article, the focus is set\non a yearly base load product from the Belgian forward market, named calendar\n(CAL), which is tradable up to three years ahead of the delivery period. This\nresearch paper introduces a novel algorithm providing recommendations to either\nbuy electricity now or wait for a future opportunity based on the history of\nCAL prices. This algorithm relies on deep learning forecasting techniques and\non an indicator quantifying the deviation from a perfectly uniform reference\nprocurement policy. On average, the proposed approach surpasses the benchmark\nprocurement policies considered and achieves a reduction in costs of 1.65% with\nrespect to the perfectly uniform reference procurement policy achieving the\nmean electricity price. Moreover, in addition to automating the complex\nelectricity procurement task, this algorithm demonstrates more consistent\nresults throughout the years. Eventually, the generality of the solution\npresented makes it well suited for solving other commodity procurement\nproblems.\n"
    },
    {
        "paper_id": 2006.0584,
        "authors": "Selene Perazzini, Giorgio Stefano Gnecco, Fabio Pammolli",
        "title": "A Public-Private Insurance Model for Natural Risk Management: an\n  Application to Seismic and Flood Risks on Residential Buildings in Italy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a public-private insurance scheme for earthquakes and\nfloods in Italy in which property-owners, the insurer and the government\nco-operate in risk financing. Our model departs from the existing literature by\ndescribing a public-private insurance intended to relieve the financial burden\nthat natural events place on governments, while at the same time assisting\nindividuals and protecting the insurance business. Hence, the business is\naiming at maximizing social welfare rather than profits. Given the limited\namount of data available on natural risks, expected losses per individual have\nbeen estimated through risk-modeling. In order to evaluate the insurer's loss\nprofile, spatial correlation among insured assets has been evaluated by means\nof the Hoeffding bound for r-dependent random variables. Though earthquakes\ngenerate expected losses that are almost six times greater than floods, we\nfound that the amount of public funds needed to manage the two perils is almost\nthe same. We argue that this result is determined by a combination of the risk\naversion of individuals and the shape of the loss distribution. Lastly, since\nearthquakes and floods are uncorrelated, we tested whether jointly managing the\ntwo perils can counteract the negative impact of spatial correlation. Some\nbenefit from risk diversification emerged, though the probability of the\ngovernment having to inject further capital might be considerable. Our findings\nsuggest that, when not supported by the government, private insurance might\neither financially over-expose the insurer or set premiums so high that\nindividuals would fail to purchase policies.\n"
    },
    {
        "paper_id": 2006.05843,
        "authors": "Julia Ackermann, Thomas Kruse, Mikhail Urusov",
        "title": "Optimal trade execution in an order book model with stochastic liquidity\n  parameters",
        "comments": "39 pages; to appear in SIAM J. Financial Math",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze an optimal trade execution problem in a financial market with\nstochastic liquidity. To this end we set up a limit order book model in which\nboth order book depth and resilience evolve randomly in time. Trading is\nallowed in both directions and at discrete points in time. We derive an\nexplicit recursion that, under certain structural assumptions, characterizes\nminimal execution costs. We also discuss several qualitative aspects of optimal\nstrategies, such as existence of profitable round trips or closing the position\nin one go, and compare our findings with the literature.\n"
    },
    {
        "paper_id": 2006.05845,
        "authors": "Selene Perazzini",
        "title": "Public-Private Partnership in the Management of Natural Disasters: A\n  Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Natural hazards can considerably impact the overall society of a country. As\nsome degree of public sector involvement is always necessary to deal with the\nconsequences of natural disasters, central governments have increasingly\ninvested in proactive risk management planning. In order to empower and involve\nthe whole society, some countries have established public-private partnerships,\nmainly with the insurance industry, with satisfactorily outcomes. Although they\nhave proven necessary and most often effective, the public-private initiatives\nhave often incurred high debts or have failed to achieved the desired risk\nreduction objectives. We review the role of these partnerships in the\nmanagement of natural risks, with particular attention to the insurance sector.\nAmong other country-specific issues, poor risk knowledge and weak governance\nhave widely challenged the initiatives during the recent years, while the\nfuture is threatened by the uncertainty of climate change and unsustainable\ndevelopment. In order to strengthen the country's resilience, a greater\ninvolvement of all segments of the community, especially the weakest layers, is\nneeded and the management of natural risks should be included in a sustainable\ndevelopment plan.\n"
    },
    {
        "paper_id": 2006.05859,
        "authors": "Anindya Ghose, Beibei Li, Meghanath Macha, Chenshuo Sun, Natasha Ying\n  Zhang Foutz",
        "title": "Trading Privacy for the Greater Social Good: How Did America React\n  During COVID-19?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital contact tracing and analysis of social distancing from smartphone\nlocation data are two prime examples of non-therapeutic interventions used in\nmany countries to mitigate the impact of the COVID-19 pandemic. While many\nunderstand the importance of trading personal privacy for the public good,\nothers have been alarmed at the potential for surveillance via measures enabled\nthrough location tracking on smartphones. In our research, we analyzed massive\nyet atomic individual-level location data containing over 22 billion records\nfrom ten Blue (Democratic) and ten Red (Republican) cities in the U.S., based\non which we present, herein, some of the first evidence of how Americans\nresponded to the increasing concerns that government authorities, the private\nsector, and public health experts might use individual-level location data to\ntrack the COVID-19 spread. First, we found a significant decreasing trend of\nmobile-app location-sharing opt-out. Whereas areas with more Democrats were\nmore privacy-concerned than areas with more Republicans before the advent of\nthe COVID-19 pandemic, there was a significant decrease in the overall opt-out\nrates after COVID-19, and this effect was more salient among Democratic than\nRepublican cities. Second, people who practiced social distancing (i.e., those\nwho traveled less and interacted with fewer close contacts during the pandemic)\nwere also less likely to opt-out, whereas the converse was true for people who\npracticed less social-distancing. This relationship also was more salient among\nDemocratic than Republican cities. Third, high-income populations and males,\ncompared with low-income populations and females, were more\nprivacy-conscientious and more likely to opt-out of location tracking.\n"
    },
    {
        "paper_id": 2006.05863,
        "authors": "Julia Ackermann, Thomas Kruse, Mikhail Urusov",
        "title": "C\\`adl\\`ag semimartingale strategies for optimal trade execution in\n  stochastic order book models",
        "comments": "53 pages; to appear in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze an optimal trade execution problem in a financial market with\nstochastic liquidity. To this end we set up a limit order book model in\ncontinuous time. Both order book depth and resilience are allowed to evolve\nrandomly in time. We allow for trading in both directions and for c\\`adl\\`ag\nsemimartingales as execution strategies. We derive a quadratic BSDE that under\nappropriate assumptions characterizes minimal execution costs and identify\nconditions under which an optimal execution strategy exists. We also\ninvestigate qualitative aspects of optimal strategies such as, e.g., appearance\nof strategies with infinite variation or existence of block trades and discuss\nconnections with the discrete-time formulation of the problem. Our findings are\nillustrated in several examples.\n"
    },
    {
        "paper_id": 2006.06035,
        "authors": "Philip Protter and Alejandra Quintos",
        "title": "Optimal Group Size in Microlending",
        "comments": "15 pages. Ann Finance (2021)",
        "journal-ref": null,
        "doi": "10.1007/s10436-020-00382-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Microlending, where a bank lends to a small group of people without credit\nhistories, began with the Grameen Bank in Bangladesh, and is widely seen as the\ncreation of Muhammad Yunus, who received the Nobel Peace Prize in recognition\nof his largely successful efforts. Since that time the modeling of microlending\nhas received a fair amount of academic attention. One of the issues not yet\naddressed in full detail, however, is the issue of the size of the group. Some\nattention has nevertheless been paid using an experimental and game theory\napproach. We, instead, take a mathematical approach to the issue of an optimal\ngroup size, where the goal is to minimize the probability of default of the\ngroup. To do this, one has to create a model with interacting forces, and to\nmake precise the hypotheses of the model. We show that the original choice of\nMuhammad Yunus, of a group size of five people, is, under the right, and, we\nbelieve, reasonable hypotheses, either close to optimal, or even at times\nexactly optimal, i.e., the optimal group size is indeed five people.\n"
    },
    {
        "paper_id": 2006.06076,
        "authors": "Richard J. Martin and Aldous Birchall",
        "title": "Black to Negative: Embedded optionalities in commodities markets",
        "comments": "Extended section on Levy models and given explicit formulae and\n  numerical example. Corrected typo in put/call formulae (eq.5,6 in this vsn)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the modelling of commodities that are supposed to have positive\nprice but, on account of a possible failure in the physical delivery mechanism,\nmay turn out not to. This is done by explicitly incorporating a `delivery\nliability' option into the contract. As such it is a simple generalisation of\nthe established Black model.\n"
    },
    {
        "paper_id": 2006.06078,
        "authors": "Ho Fai Chan, Ahmed Skali, David Savage, David Stadelmann, Benno\n  Torgler",
        "title": "Risk Attitudes and Human Mobility during the COVID-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41598-020-76763-2",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Behavioral responses to pandemics are less shaped by actual mortality or\nhospitalization risks than they are by risk attitudes. We explore human\nmobility patterns as a measure of behavioral responses during the COVID-19\npandemic. Our results indicate that risk-taking attitude is a critical factor\nin predicting reduction in human mobility and increase social confinement\naround the globe. We find that the sharp decline in movement after the WHO\n(World Health Organization) declared COVID-19 to be a pandemic can be\nattributed to risk attitudes. Our results suggest that regions with risk-averse\nattitudes are more likely to adjust their behavioral activity in response to\nthe declaration of a pandemic even before most official government lockdowns.\nFurther understanding of the basis of responses to epidemics, e.g.,\nprecautionary behavior, will help improve the containment of the spread of the\nvirus.\n"
    },
    {
        "paper_id": 2006.06123,
        "authors": "Matata Ponyo Mapon and Jean-Paul K. Tsasa",
        "title": "IMF Programs and Economic Growth in the DRC",
        "comments": "in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  At the end of 2012 the International Monetary Fund (IMF) has suspended its\nfinancial assistance to the Democratic Republic of the Congo (DRC). Due to\ninflationary pressures which occurred in the last quarter of 2016, several\ndecision-makers called for a reopening of a formal cooperation with the IMF.\nThis process was formally completed in December 2019. The restart of IMF\nprograms was greeted with satisfaction by politicians and widely commented in\nthe media. However, recent history shows that the DRC managed to achieve\nexceptional economic performance, between 2012 and 2016, without being in a\nformal cooperation with the IMF. Some people wonder whether IMF assistance is a\ncurse for recipient countries? We argue that the underlying problem has nothing\nto do with accepting or not the IMF assistance, but rather in the ability of\npolicy makers to establish effective leadership and good governance for the\ndevelopment and implementation supporting structural reforms.\n"
    },
    {
        "paper_id": 2006.06217,
        "authors": "Abhishek Gupta (1 and 2), Camylle Lanteigne (1 and 3), and Sara\n  Kingsley (4) ((1) Montreal AI Ethics Institute, (2) Microsoft, (3) McGill\n  University, (4) Carnegie Mellon University)",
        "title": "SECure: A Social and Environmental Certificate for AI Systems",
        "comments": "Accepted for presentation at the Canadian Society for Ecological\n  Economics 2020 Research Symposium, Tracing the Veins 2020, ICML 2020\n  Deploying and Monitoring Machine Learning Systems workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In a world increasingly dominated by AI applications, an understudied aspect\nis the carbon and social footprint of these power-hungry algorithms that\nrequire copious computation and a trove of data for training and prediction.\nWhile profitable in the short-term, these practices are unsustainable and\nsocially extractive from both a data-use and energy-use perspective. This work\nproposes an ESG-inspired framework combining socio-technical measures to build\neco-socially responsible AI systems. The framework has four pillars:\ncompute-efficient machine learning, federated learning, data sovereignty, and a\nLEEDesque certificate.\n  Compute-efficient machine learning is the use of compressed network\narchitectures that show marginal decreases in accuracy. Federated learning\naugments the first pillar's impact through the use of techniques that\ndistribute computational loads across idle capacity on devices. This is paired\nwith the third pillar of data sovereignty to ensure the privacy of user data\nvia techniques like use-based privacy and differential privacy. The final\npillar ties all these factors together and certifies products and services in a\nstandardized manner on their environmental and social impacts, allowing\nconsumers to align their purchase with their values.\n"
    },
    {
        "paper_id": 2006.06237,
        "authors": "Tim Schmitz and Ingo Hoffmann",
        "title": "Re-evaluating cryptocurrencies' contribution to portfolio\n  diversification -- A portfolio analysis with special focus on German\n  investors",
        "comments": "59 pages, 20 figures. JEL Classification: C12, C13, C32, E22, G11",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate whether mixing cryptocurrencies to a German\ninvestor portfolio improves portfolio diversification. We analyse this research\nquestion by applying a (mean variance) portfolio analysis using a toolbox\nconsisting of (i) the comparison of descriptive statistics, (ii) graphical\nmethods and (iii) econometric spanning tests. In contrast to most of the former\nstudies we use a (broad) customized, Equally-Weighted Cryptocurrency Index\n(EWCI) to capture the average development of a whole ex ante defined\ncryptocurrency universe and to mitigate possible survivorship biases in the\ndata. According to Glas/Poddig (2018), this bias could have led to misleading\nresults in some already existing studies. We find that cryptocurrencies can\nimprove portfolio diversification in a few of the analyzed windows from our\ndataset (consisting of weekly observations from 2014-01-01 to 2019-05-31).\nHowever, we cannot confirm this pattern as the normal case. By including\ncryptocurrencies in their portfolios, investors predominantly cannot reach a\nsignificantly higher efficient frontier. These results also hold, if the\nnon-normality of cryptocurrency returns is considered. Moreover, we control for\nchanges of the results, if transaction costs/illiquidities on the\ncryptocurrency market are additionally considered.\n"
    },
    {
        "paper_id": 2006.06395,
        "authors": "Jos\\'e M. Corcuera and Giulia Di Nunno",
        "title": "Path-dependent Kyle equilibrium model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an auction type equilibrium model with an insider in line with\nthe one originally introduced by Kyle in 1985 and then extended to the\ncontinuous time setting by Back in 1992. The novelty introduced with this paper\nis that we deal with a general price functional depending on the whole past of\nthe aggregate demand, i.e. we work with path-dependency. By using the\nfunctional It\\^o calculus, we provide necessary and sufficient conditions for\nthe existence of an equilibrium. Furthermore, we consider both the cases of a\nrisk-neutral and a risk-averse insider.\n"
    },
    {
        "paper_id": 2006.06548,
        "authors": "Jobst Heitzig and Forest W. Simmons",
        "title": "Efficient democratic decisions via nondeterministic proportional\n  consensus",
        "comments": "10 pages main text plus 61 pages supplement",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Are there voting methods which (i) give everyone, including minorities, an\nequal share of effective power even if voters act strategically, (ii) promote\nconsensus rather than polarization and inequality, and (iii) do not favour the\nstatus quo or rely too much on chance?\n  We show the answer is yes by describing two nondeterministic voting methods,\none based on automatic bargaining over lotteries, the other on conditional\ncommitments to approve compromise options. Our theoretical analysis and\nagent-based simulation experiments suggest that with these, majorities cannot\nconsistently suppress minorities as with deterministic methods, proponents of\nthe status quo cannot block decisions as in consensus-based approaches, the\nresulting aggregate welfare is comparable to existing methods, and average\nrandomness is lower than for other nondeterministic methods.\n"
    },
    {
        "paper_id": 2006.06595,
        "authors": "Guglielmo D'Amico, Riccardo De Blasis, Philippe Regnault",
        "title": "Confidence sets for dynamic poverty indexes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we extend the research on the dynamic poverty indexes, namely\nthe dynamic Headcount ratio, the dynamic income-gap ratio, the dynamic Gini and\nthe dynamic Sen, proposed in D'Amico and Regnault (2018). The contribution is\ntwofold. First, we extend the computation of the dynamic Gini index, thus the\nSen index accordingly, with the inclusion of the inequality within each class\nof poverty where people are classified according to their income. Second, for\neach poverty index, we establish a central limit theorem that gives us the\npossibility to determine the confidence sets. An application to the Italian\nincome data from 1998 to 2012 confirms the effectiveness of the considered\napproach and the possibility to determine the evolution of poverty and\ninequality in real economies.\n"
    },
    {
        "paper_id": 2006.06642,
        "authors": "I.S. Gandzha, O.V. Kliushnichenko, S.P. Lukyanets",
        "title": "The Epidemic-Driven Collapse in a System with Limited Economic Resource",
        "comments": "5 pages, 3 figures; references added, figures revised, added and\n  rearranged",
        "journal-ref": null,
        "doi": "10.1140/epjb/s10051-021-00099-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a possibility of socioeconomic collapse caused by the spread of\nepidemic in a basic dynamical model with negative feedback between the infected\npopulation size and a formal collective economic resource. The\nepidemic-resource coupling is supposed to be of activation type, with the\nrecovery rate governed by the Arrhenius-like law and resource playing the role\nof temperature. Such a coupling can result in the collapsing effect opposite to\nthermal explosion because of the limited resource. In this case, the system can\nno longer stabilize and return to the stable pre- or post-epidemic states. We\ndemonstrate that such a collapse can partially be mitigated by means of a\nnegative resource or debt.\n"
    },
    {
        "paper_id": 2006.07167,
        "authors": "Shantanu Awasthi and Indranil SenGupta",
        "title": "First exit-time analysis for an approximate Barndorff-Nielsen and\n  Shephard model with stationary self-decomposable variance process",
        "comments": "27 pages, 7 figures",
        "journal-ref": "Journal of Stochastic Analysis (formerly, Communications on\n  Stochastic Analysis), 2021",
        "doi": "10.31390/josa.2.1.05",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, an approximate version of the Barndorff-Nielsen and Shephard\nmodel, driven by a Brownian motion and a L\\'evy subordinator, is formulated.\nThe first-exit time of the log-return process for this model is analyzed. It is\nshown that with certain probability, the first-exit time process of the\nlog-return is decomposable into the sum of the first exit time of the Brownian\nmotion with drift, and the first exit time of a L\\'evy subordinator with drift.\nSubsequently, the probability density functions of the first exit time of some\nspecific L\\'evy subordinators, connected to stationary, self-decomposable\nvariance processes, are studied. Analytical expressions of the probability\ndensity function of the first-exit time of three such L\\'evy subordinators are\nobtained in terms of various special functions. The results are implemented to\nempirical S&P 500 dataset.\n"
    },
    {
        "paper_id": 2006.07204,
        "authors": "Manav Raj, Arun Sundararajan, Calum You",
        "title": "COVID-19 and Digital Resilience: Evidence from Uber Eats",
        "comments": "26 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using order-level data from Uber Technologies, we study how the COVID-19\npandemic and the ensuing shutdown of businesses in the United States in 2020\naffected small business restaurant supply and demand on the Uber Eats platform.\nWe find evidence that small restaurants experience significant increases in\nactivity on the platform following the closure of the dine-in channel. We\ndocument how locality- and restaurant-specific characteristics moderate the\nsize of the increase in activity through the digital channel and explain how\nthese increases may be due to both demand- and supply-side shock. We observe an\nincrease in the intensity of competitive effects following the economic shock\nand show that growth in the number of providers on a platform induces both\nmarket expansion and heightened inter-provider competition. Higher platform\nactivity in response to the shock does not only have short-run implications:\nrestaurants with larger demand shocks had a higher on-platform survival rate\none year after the lockdown, suggesting that the platform channel contributes\ntowards long-run resilience following a crisis. Our findings document the\nheterogeneous effects of platforms during the pandemic, underscore the critical\nrole that digital technologies play in enabling business resilience in the\neconomy, and provide insight into how platforms can manage competing incentives\nwhen balancing market expansion and growth goals with the competitive interests\nof their incumbent providers.\n"
    },
    {
        "paper_id": 2006.07223,
        "authors": "Shuoqing Deng, Xun Li, Huyen Pham, Xiang Yu",
        "title": "Optimal Consumption with Reference to Past Spending Maximum",
        "comments": "Final version, forthcoming in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the infinite-horizon optimal consumption with a\npath-dependent reference under exponential utility. The performance is measured\nby the difference between the nonnegative consumption rate and a fraction of\nthe historical consumption maximum. The consumption running maximum process is\nchosen as an auxiliary state process, and hence the value function depends on\ntwo state variables. The Hamilton-Jacobi-Bellman (HJB) equation can be\nheuristically expressed in a piecewise manner across different regions to take\ninto account all constraints. By employing the dual transform and smooth-fit\nprinciple, some thresholds of the wealth variable are derived such that a\nclassical solution to the HJB equation and the feedback optimal investment and\nconsumption strategies can be obtained in closed form in each region. A\ncomplete proof of the verification theorem is provided, and numerical examples\nare presented to illustrate some financial implications.\n"
    },
    {
        "paper_id": 2006.07311,
        "authors": "Edward J. Oughton and Jatin Mathur",
        "title": "Predicting cell phone adoption metrics using satellite imagery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Approximately half of the global population does not have access to the\ninternet, even though digital connectivity can reduce poverty by\nrevolutionizing economic development opportunities. Due to a lack of data,\nMobile Network Operators and governments struggle to effectively determine if\ninfrastructure investments are viable, especially in greenfield areas where\ndemand is unknown. This leads to a lack of investment in network\ninfrastructure, resulting in a phenomenon commonly referred to as the `digital\ndivide`. In this paper we present a machine learning method that uses publicly\navailable satellite imagery to predict telecoms demand metrics, including cell\nphone adoption and spending on mobile services, and apply the method to Malawi\nand Ethiopia. Our predictive machine learning approach consistently outperforms\nbaseline models which use population density or nightlight luminosity, with an\nimprovement in data variance prediction of at least 40%. The method is a\nstarting point for developing more sophisticated predictive models of\ninfrastructure demand using machine learning and publicly available satellite\nimagery. The evidence produced can help to better inform infrastructure\ninvestment and policy decisions.\n"
    },
    {
        "paper_id": 2006.07329,
        "authors": "Siyu Huang, Wensha Gou, Hongbo Cai, Xiaomeng Li and Qinghua Chen",
        "title": "Effects of Regional Trade Agreement to Local and Global Trade Purity\n  Relationships",
        "comments": "23 pages,18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In contrast to the rapid integration of the world economy, many regional\ntrade agreements (RTAs) have also emerged since the early 1990s. This seeming\ncontradiction has encouraged scholars and policy makers to explore the true\neffects of RTAs, including both regional and global trade relationships. This\npaper defines synthesized trade resistance and decomposes it into natural and\nartificial factors. Here, we separate the influence of geographical distance,\neconomic volume, overall increases in transportation and labor costs and use\nthe expectation maximization algorithm to optimize the parameters and quantify\nthe trade purity indicator, which describes the true global trade environment\nand relationships among countries. This indicates that although global and most\nregional trade relations gradually deteriorated during the period 2007-2017,\nRTAs generate trade relations among members, especially contributing to the\nrelative prosperity of EU and NAFTA countries. In addition, we apply the\nnetwork to reflect the purity of the trade relations among countries. The\neffects of RTAs can be analyzed by comparing typical trade unions and trade\ncommunities, which are presented using an empirical network structure. This\nanalysis shows that the community structure is quite consistent with some trade\nunions, and the representative RTAs constitute the core structure of\ninternational trade network. However, the role of trade unions has weakened,\nand multilateral trade liberalization has accelerated in the past decade. This\nmeans that more countries have recently tended to expand their trading partners\noutside of these unions rather than limit their trading activities to RTAs.\n"
    },
    {
        "paper_id": 2006.07341,
        "authors": "Peter Cotton",
        "title": "Addressing the Herd Immunity Paradox Using Symmetry, Convexity\n  Adjustments and Bond Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In constant parameter compartmental models an early onset of herd immunity is\nat odds with estimates of R values from early stage growth. This paper utilizes\na result from the theory of interest rate modeling, namely a bond pricing\nformula of Vasicek, and an approach inspired by a foundational result in\nstatistics, de Finetti's Theorem, to show how the modeling discrepancy can be\nexplained. Moreover the difference between predictions of classic constant\nparameter epidemiological models and those with variation and stochastic\nevolution can be reduced to simple \"convexity\" formulas. A novel feature of\nthis approach is that we do not attempt to locate a true model but only a model\nthat is equivalent after permutations. Convexity adjustments can also be used\nfor cross sectional comparisons and we derive easy to use rules of thumb for\nestimating threshold infection level in one region given knowledge of threshold\ninfection in another.\n"
    },
    {
        "paper_id": 2006.07456,
        "authors": "Alessandro Micheli and Eyal Neuman",
        "title": "Evidence of Crowding on Russell 3000 Reconstitution Events",
        "comments": "35 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a methodology which replicates in great accuracy the FTSE Russell\nindexes reconstitutions, including the quarterly rebalancings due to new\ninitial public offerings (IPOs). While using only data available in the CRSP US\nStock database for our index reconstruction, we demonstrate the accuracy of\nthis methodology by comparing it to the original Russell US indexes for the\ntime period between 1989 to 2019. A python package that generates the\nreplicated indexes is also provided.\n  As an application, we use our index reconstruction protocol to compute the\npermanent and temporary price impact on the Russell 3000 annual additions and\ndeletions, and on the quarterly additions of new IPOs . We find that the index\nportfolios following the Russell 3000 index and rebalanced on an annual basis\nare overall more crowded than those following the index on a quarterly basis.\nThis phenomenon implies that transaction costs of indexing strategies could be\nsignificantly reduced by buying new IPOs additions in proximity to quarterly\nrebalance dates.\n"
    },
    {
        "paper_id": 2006.07635,
        "authors": "Yajie Yu, Bernhard Hientzsch, Narayan Ganesan",
        "title": "Backward Deep BSDE Methods and Applications to Nonlinear Problems",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a backward deep BSDE method applied to Forward\nBackward Stochastic Differential Equations (FBSDE) with given terminal\ncondition at maturity that time-steps the BSDE backwards. We present an\napplication of this method to a nonlinear pricing problem - the differential\nrates problem. To time-step the BSDE backward, one needs to solve a nonlinear\nproblem. For the differential rates problem, we derive an exact solution of\nthis time-step problem and a Taylor-based approximation. Previously backward\ndeep BSDE methods only treated zero or linear generators. While a Taylor\napproach for nonlinear generators was previously mentioned, it had not been\nimplemented or applied, while we apply our method to nonlinear generators and\nderive details and present results. Likewise, previously backward deep BSDE\nmethods were presented for fixed initial risk factor values $X_0$ only, while\nwe present a version with random $X_0$ and a version that learns portfolio\nvalues at intermediate times as well. The method is able to solve nonlinear\nFBSDE problems in high dimensions.\n"
    },
    {
        "paper_id": 2006.07669,
        "authors": "Young Shin Kim, Kum-Hwan Roh, and Raphael Douady",
        "title": "Tempered Stable Processes with Time Varying Exponential Tails",
        "comments": "Kum-Hwan Roh gratefully acknowledges the support of Basic Science\n  Research Program through the National Research Foundation of Korea (NRF)\n  grant funded by the Korea government [Grant No. NRF-2017R1D1A3B03036548]",
        "journal-ref": null,
        "doi": "10.1080/14697688.2021.1962958",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a new time series model having a stochastic\nexponential tail. This model is constructed based on the Normal Tempered Stable\ndistribution with a time-varying parameter. The model captures the stochastic\nexponential tail, which generates the volatility smile effect and volatility\nterm structure in option pricing. Moreover, the model describes the\ntime-varying volatility of volatility. We empirically show the stochastic\nskewness and stochastic kurtosis by applying the model to analyze S&P 500 index\nreturn data. We present the Monte-Carlo simulation technique for the parameter\ncalibration of the model for the S&P 500 option prices. We can see that the\nstochastic exponential tail makes the model better to analyze the market option\nprices by the calibration.\n"
    },
    {
        "paper_id": 2006.07684,
        "authors": "Guanxing Fu and Xizhi Su and Chao Zhou",
        "title": "Mean Field Exponential Utility Game: A Probabilistic Approach",
        "comments": "35 pages; more references are cited and examples are added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an $N$-player and a mean field exponential utility game. Each player\nmanages two stocks; one is driven by an individual shock and the other is\ndriven by a common shock. Moreover, each player is concerned not only with her\nown terminal wealth but also with the relative performance of her competitors.\nWe use the probabilistic approach to study these two games. We show the unique\nequilibrium of the $N$-player game and the mean field game can be characterized\nby a novel multi-dimensional FBSDE with quadratic growth and a novel mean-field\nFBSDEs, respectively. The well-posedness result and the convergence result are\nestablished.\n"
    },
    {
        "paper_id": 2006.0769,
        "authors": "Zhu Liu, Philippe Ciais, Zhu Deng, Steven J. Davis, Bo Zheng, Yilong\n  Wang, Duo Cui, Biqing Zhu, Xinyu Dou, Piyu Ke, Taochun Sun, Rui Guo, Olivier\n  Boucher, Francois-Marie Breon, Chenxi Lu, Runtao Guo, Eulalie Boucher,\n  Frederic Chevallier",
        "title": "Carbon Monitor: a near-real-time daily dataset of global CO2 emission\n  from fossil fuel and cement production",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41597-020-00708-7",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We constructed a near-real-time daily CO2 emission dataset, namely the Carbon\nMonitor, to monitor the variations of CO2 emissions from fossil fuel combustion\nand cement production since January 1st 2019 at national level with near-global\ncoverage on a daily basis, with the potential to be frequently updated. Daily\nCO2 emissions are estimated from a diverse range of activity data, including:\nhourly to daily electrical power generation data of 29 countries, monthly\nproduction data and production indices of industry processes of 62\ncountries/regions, daily mobility data and mobility indices of road\ntransportation of 416 cities worldwide. Individual flight location data and\nmonthly data were utilised for aviation and maritime transportation sectors\nestimates. In addition, monthly fuel consumption data that corrected for daily\nair temperature of 206 countries were used for estimating the emissions from\ncommercial and residential buildings. This Carbon Monitor dataset manifests the\ndynamic nature of CO2 emissions through daily, weekly and seasonal variations\nas influenced by workdays and holidays, as well as the unfolding impacts of the\nCOVID-19 pandemic. The Carbon Monitor near-real-time CO2 emission dataset shows\na 7.8% decline of CO2 emission globally from Jan 1st to Apr 30th in 2020 when\ncompared with the same period in 2019, and detects a re-growth of CO2 emissions\nby late April which are mainly attributed to the recovery of economy activities\nin China and partial easing of lockdowns in other countries. Further, this\ndaily updated CO2 emission dataset could offer a range of opportunities for\nrelated scientific research and policy making.\n"
    },
    {
        "paper_id": 2006.07771,
        "authors": "Kevin S. Zhang and Traian A. Pirvu",
        "title": "Numerical Simulation of Exchange Option with Finite Liquidity:\n  Controlled Variate Model",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop numerical pricing methodologies for European style\nExchange Options written on a pair of correlated assets, in a market with\nfinite liquidity. In contrast to the standard multi-asset Black-Scholes\nframework, trading in our market model has a direct impact on the asset's\nprice. The price impact is incorporated into the dynamics of the first asset\nthrough a specific trading strategy, as in large trader liquidity model.\nTwo-dimensional Milstein scheme is implemented to simulate the pair of assets\nprices. The option value is numerically estimated by Monte Carlo with the\nMargrabe option as controlled variate. Time complexity of these numerical\nschemes are included. Finally, we provide a deep learning framework to\nimplement this model effectively in a production environment.\n"
    },
    {
        "paper_id": 2006.07847,
        "authors": "Christof Schmidhuber",
        "title": "Trends, Reversion, and Critical Phenomena in Financial Markets",
        "comments": "28 pages, 6 figures, minor corrections and additional references in\n  revised version",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.125642",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets across all asset classes are known to exhibit trends. These\ntrends have been exploited by traders for decades. Here, we empirically measure\nwhen trends revert, based on 30 years of daily futures prices for equity\nindices, interest rates, currencies and commodities. We find that trends tend\nto revert once they reach a critical level of statistical significance. Based\non polynomial regression, we carefully measure this critical level. We find\nthat it is universal across asset classes and has a universal scaling behavior,\nas the trend's time horizon runs from a few days to several years. The\ncorresponding regression coefficients are small, but statistically highly\nsignificant, as confirmed by bootstrapping and out-of-sample testing. Our\nresults signal to investors when to exit a trend. They also reveal how markets\nhave become more efficient over the decades. Moreover, they point towards a\npotential deep analogy between financial markets and critical phenomena: our\nanalysis supports the conjecture that financial markets can be modeled as\nstatistical mechanical ensembles of Buy/Sell orders near critical points. In\nthis analogy, the trend strength plays the role of an order parameter, whose\ndynamcis is described by a Langevin equation with a quartic potential.\n"
    },
    {
        "paper_id": 2006.07911,
        "authors": "Sajjad Taghiyeh, David C Lengacher and Robert B Handfield",
        "title": "Loss Rate Forecasting Framework Based on Macroeconomic Changes:\n  Application to US Credit Card Industry",
        "comments": "45 pages, 16 figures, 16 tables, submitted to Expert Systems with\n  Applications Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major part of the balance sheets of the largest US banks consists of credit\ncard portfolios. Hence, managing the charge-off rates is a vital task for the\nprofitability of the credit card industry. Different macroeconomic conditions\naffect individuals' behavior in paying down their debts. In this paper, we\npropose an expert system for loss forecasting in the credit card industry using\nmacroeconomic indicators. We select the indicators based on a thorough review\nof the literature and experts' opinions covering all aspects of the economy,\nconsumer, business, and government sectors. The state of the art machine\nlearning models are used to develop the proposed expert system framework. We\ndevelop two versions of the forecasting expert system, which utilize different\napproaches to select between the lags added to each indicator. Among 19\nmacroeconomic indicators that were used as the input, six were used in the\nmodel with optimal lags, and seven indicators were selected by the model using\nall lags. The features that were selected by each of these models covered all\nthree sectors of the economy. Using the charge-off data for the top 100 US\nbanks ranked by assets from the first quarter of 1985 to the second quarter of\n2019, we achieve mean squared error values of 1.15E-03 and 1.04E-03 using the\nmodel with optimal lags and the model with all lags, respectively. The proposed\nexpert system gives a holistic view of the economy to the practitioners in the\ncredit card industry and helps them to see the impact of different\nmacroeconomic conditions on their future loss.\n"
    },
    {
        "paper_id": 2006.07938,
        "authors": "Boris M. Dolgonosov",
        "title": "The energy representation of world GDP",
        "comments": "7 pages, 1 figure, 1 table. Short communication. A novel energy\n  representation of world GDP is proposed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dependence of world GDP on current energy consumption and total energy\nproduced over the previous period and materialized in the form of production\ninfrastructure is studied. The dependence describes empirical data with high\naccuracy over the entire observation interval 1965-2018.\n"
    },
    {
        "paper_id": 2006.08004,
        "authors": "Christoph Berninger, Julian Pfeiffer",
        "title": "The Gauss2++ Model -- A Comparison of Different Measure Change\n  Specifications for a Consistent Risk Neutral and Real World Calibration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Especially in the insurance industry interest rate models play a crucial role\ne.g. to calculate the insurance company's liabilities, performance scenarios or\nrisk measures. A prominant candidate is the 2-Additive-Factor Gaussian Model\n(Gauss2++) - in a different representation also known as the 2-Factor\nHull-White model. In this paper, we propose a framework to estimate the model\nsuch that it can be applied under the risk neutral and the real world measure\nin a consistent manner. We first show that any progressive and\nsquare-integrable function can be used to specify the change of measure without\nloosing the analytic tractability of e.g. zero-coupon bond prices in both\nworlds. We further propose two time dependent candidates, which are easy to\ncalibrate: a step and a linear function. They represent two variants of our\nframework and distinguish between a short and a long term risk premium, which\nallows to regularize the interest rates in the long horizon. We apply both\nvariants to historical data and show that they indeed produce realistic and\nmuch more stable long term interest rate forecast than the usage of a constant\nfunction. This stability over time would translate to performance scenarios of\ne.g. interest rate sensitive fonds and risk measures.\n"
    },
    {
        "paper_id": 2006.08009,
        "authors": "Sebastian Wehrle and Johannes Schmidt and Christian Mikovits",
        "title": "The Cost of Undisturbed Landscapes",
        "comments": "Changelog: * scaled intermittent generation profiles to match energy\n  balances, * updated results accordingly, * added figures and tables",
        "journal-ref": null,
        "doi": "10.1016/j.enpol.2021.112617",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By 2030 Austria aims to meet 100% of its electricity demand from domestic\nrenewable sources, with most of the additional generation coming from wind and\nsolar energy. Apart from the benefit of reducing CO2 emissions and,\npotentially, system cost, wind power is associated with negative impacts at the\nlocal level, such as interference with landscape aesthetics. Some of these\nimpacts might be avoided by using alternative renewable energy technologies.\nThus, we quantify the opportunity cost of wind power versus its best feasible\nalternative solar photovoltaics, using the power system model medea. Our\nfindings suggest that the cost of undisturbed landscapes is considerable,\nparticularly when solar PV is mainly realized on roof-tops. Under a wide range\nof assumptions, the opportunity cost of wind power is high enough to allow for\nsignificant compensation of the ones affected by local, negative wind turbine\nexternalities.\n"
    },
    {
        "paper_id": 2006.0811,
        "authors": "Nils Detering, Thilo Meyer-Brandis, Konstantinos Panagiotou, Daniel\n  Ritter",
        "title": "Suffocating Fire Sales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fire sales are among the major drivers of market instability in modern\nfinancial systems. Due to iterated distressed selling and the associated price\nimpact, initial shocks to some institutions can be amplified dramatically\nthrough the network induced by portfolio overlaps. In this paper, we develop a\nmathematical framework that allows us to investigate central characteristics\nthat drive or hinder the propagation of distress. We investigate single systems\nas well as ensembles of systems that are alike, where similarity is measured in\nterms of the empirical distribution of all defining properties of a system.\nThis asymptotic approach ensures a great deal of robustness to statistical\nuncertainty and temporal fluctuations. A characterization of those systems that\nare resilient to small shocks emerges, and we provide criteria that regulators\nmight exploit in order to assess the stability of a financial system.\n  We illustrate the application of these criteria for some exemplary\nconfigurations in the context of capital requirements and test the\napplicability of our results for systems of moderate size by Monte Carlo\nsimulations.\n"
    },
    {
        "paper_id": 2006.08307,
        "authors": "Hugh Christensen, Simon Godsill, Richard E Turner",
        "title": "Hidden Markov Models Applied To Intraday Momentum Trading With Side\n  Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Hidden Markov Model for intraday momentum trading is presented which\nspecifies a latent momentum state responsible for generating the observed\nsecurities' noisy returns. Existing momentum trading models suffer from\ntime-lagging caused by the delayed frequency response of digital filters.\nTime-lagging results in a momentum signal of the wrong sign, when the market\nchanges trend direction. A key feature of this state space formulation, is no\nsuch lagging occurs, allowing for accurate shifts in signal sign at market\nchange points. The number of latent states in the model is estimated using\nthree techniques, cross validation, penalized likelihood criteria and\nsimulation-based model selection for the marginal likelihood. All three\ntechniques suggest either 2 or 3 hidden states. Model parameters are then found\nusing Baum-Welch and Markov Chain Monte Carlo, whilst assuming a single\n(discretized) univariate Gaussian distribution for the emission matrix. Often a\nmomentum trader will want to condition their trading signals on additional\ninformation. To reflect this, learning is also carried out in the presence of\nside information. Two sets of side information are considered, namely a ratio\nof realized volatilities and intraday seasonality. It is shown that splines can\nbe used to capture statistically significant relationships from this\ninformation, allowing returns to be predicted. An Input Output Hidden Markov\nModel is used to incorporate these univariate predictive signals into the\ntransition matrix, presenting a possible solution for dealing with the signal\ncombination problem. Bayesian inference is then carried out to predict the\nsecurities $t+1$ return using the forward algorithm. Simple modifications to\nthe current framework allow for a fully non-parametric model with asynchronous\nprediction.\n"
    },
    {
        "paper_id": 2006.08375,
        "authors": "S.P. Lukyanets, I.S. Gandzha, and O.V. Kliushnichenko",
        "title": "Modeling and Controlling the Spread of Epidemic with Various Social and\n  Economic Scenarios",
        "comments": "14 pages, 6 figures, 5 tables",
        "journal-ref": "Chaos Soliton Frac. (2021), v. 148, 111046",
        "doi": "10.1016/j.chaos.2021.111046",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a dynamical model for describing the spread of epidemics. This\nmodel is an extension of the SIQR (susceptible-infected-quarantined-recovered)\nand SIRP (susceptible-infected-recovered-pathogen) models used earlier to\ndescribe various scenarios of epidemic spreading. As compared to the basic SIR\nmodel, our model takes into account two possible routes of contagion\ntransmission: direct from the infected compartment to the susceptible\ncompartment and indirect via some intermediate medium or fomites. Transmission\nrates are estimated in terms of average distances between the individuals in\nselected social environments and characteristic time spans for which the\nindividuals stay in each of these environments. We also introduce a collective\neconomic resource associated with the average amount of money or income per\nindividual to describe the socioeconomic interplay between the spreading\nprocess and the resource available to infected individuals. The\nepidemic-resource coupling is supposed to be of activation type, with the\nrecovery rate governed by the Arrhenius-like law. Our model brings an advantage\nof building various control strategies to mitigate the effect of epidemic and\ncan be applied, in particular, to modeling the spread of COVID-19.\n"
    },
    {
        "paper_id": 2006.08446,
        "authors": "Olivier Cabrignac, Arthur Charpentier, Ewen Gallic",
        "title": "Modeling Joint Lives within Families",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Family history is usually seen as a significant factor insurance companies\nlook at when applying for a life insurance policy. Where it is used, family\nhistory of cardiovascular diseases, death by cancer, or family history of high\nblood pressure and diabetes could result in higher premiums or no coverage at\nall. In this article, we use massive (historical) data to study dependencies\nbetween life length within families. If joint life contracts (between a husband\nand a wife) have been long studied in actuarial literature, little is known\nabout child and parents dependencies. We illustrate those dependencies using\n19th century family trees in France, and quantify implications in annuities\ncomputations. For parents and children, we observe a modest but significant\npositive association between life lengths. It yields different estimates for\nremaining life expectancy, present values of annuities, or whole life insurance\nguarantee, given information about the parents (such as the number of parents\nalive). A similar but weaker pattern is observed when using information on\ngrandparents.\n"
    },
    {
        "paper_id": 2006.08469,
        "authors": "Dhruv Sharma, Jean-Philippe Bouchaud, Stanislao Gualdi, Marco Tarzia,\n  Francesco Zamponi",
        "title": "V-, U-, L-, or W-shaped economic recovery after COVID: Insights from an\n  Agent Based Model",
        "comments": "20 Pages. Model description within the main text. Discussion on\n  multiple lockdowns added. See code for paper at\n  https://gitlab.com/sharma.dhruv/markovid",
        "journal-ref": "PLoS ONE 16, e0247823 (2021)",
        "doi": "10.1371/journal.pone.0247823",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the impact of a Covid-19--like shock on a simple model economy,\ndescribed by the previously developed Mark-0 Agent-Based Model. We consider a\nmixed supply and demand shock, and show that depending on the shock parameters\n(amplitude and duration), our model economy can display V-shaped, U-shaped or\nW-shaped recoveries, and even an L-shaped output curve with permanent output\nloss. This is due to the economy getting trapped in a self-sustained \"bad\"\nstate. We then discuss two policies that attempt to moderate the impact of the\nshock: giving easy credit to firms, and the so-called helicopter money, i.e.\ninjecting new money into the households savings. We find that both policies are\neffective if strong enough. We highlight the potential danger of terminating\nthese policies too early, although inflation is substantially increased by lax\naccess to credit. Finally, we consider the impact of a second lockdown. While\nwe only discuss a limited number of scenarios, our model is flexible and\nversatile enough to accommodate a wide variety of situations, thus serving as a\nuseful exploratory tool for a qualitative, scenario-based understanding of\npost-Covid recovery. The corresponding code is available on-line.\n"
    },
    {
        "paper_id": 2006.08682,
        "authors": "David Byrd, Sruthi Palaparthi, Maria Hybinette, Tucker Hybinette Balch",
        "title": "The Importance of Low Latency to Order Book Imbalance Trading Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a pervasive assumption that low latency access to an exchange is a\nkey factor in the profitability of many high-frequency trading strategies. This\nbelief is evidenced by the \"arms race\" undertaken by certain financial firms to\nco-locate with exchange servers. To the best of our knowledge, our study is the\nfirst to validate and quantify this assumption in a continuous double auction\nmarket with a single exchange similar to the New York Stock Exchange. It is not\nfeasible to conduct this exploration with historical data in which trader\nidentity and location are not reported. Accordingly, we investigate the\nrelationship between latency of access to order book information and\nprofitability of trading strategies exploiting that information with an\nagent-based interactive discrete event simulation in which thousands of agents\npursue archetypal trading strategies. We introduce experimental traders\npursuing a low-latency order book imbalance (OBI) strategy in a controlled\nmanner across thousands of simulated trading days, and analyze OBI trader\nprofit while varying distance (latency) from the exchange. Our experiments\nsupport that latency is inversely related to profit for the OBI traders, but\nmore interestingly show that latency rank, rather than absolute magnitude, is\nthe key factor in allocating returns among agents pursuing a similar strategy.\n"
    },
    {
        "paper_id": 2006.08806,
        "authors": "Alex Evans",
        "title": "Liquidity Provider Returns in Geometric Mean Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric mean market makers (G3Ms), such as Uniswap and Balancer, comprise a\npopular class of automated market makers (AMMs) defined by the following rule:\nthe reserves of the AMM before and after each trade must have the same\n(weighted) geometric mean. This paper extends several results known for\nconstant-weight G3Ms to the general case of G3Ms with time-varying and\npotentially stochastic weights. These results include the returns and\nno-arbitrage prices of liquidity pool (LP) shares that investors receive for\nsupplying liquidity to G3Ms. Using these expressions, we show how to create\nG3Ms whose LP shares replicate the payoffs of financial derivatives. The\nresulting hedges are model-independent and exact for derivative contracts whose\npayoff functions satisfy an elasticity constraint. These strategies allow LP\nshares to replicate various trading strategies and financial contracts,\nincluding standard options. G3Ms are thus shown to be capable of recreating a\nvariety of active trading strategies through passive positions in LP shares.\n"
    },
    {
        "paper_id": 2006.08976,
        "authors": "Matteo Zampieri, Andrea Toreti, Andrej Ceglar, Pierluca De Palma,\n  Thomas Chatzopoulos",
        "title": "Analysing the resilience of the European commodity production system\n  with PyResPro, the Python Production Resilience package",
        "comments": "17 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a Python object-oriented software and code to compute the\nannual production resilience indicator. The annual production resilience\nindicator can be applied to different anthropic and natural systems such as\nagricultural production, natural vegetation and water resources. Here, we show\nan example of resilience analysis of the economic values of the agricultural\nproduction in Europe. The analysis is conducted for individual time-series in\norder to estimate the resilience of a single commodity and to groups of\ntime-series in order to estimate the overall resilience of diversified\nproduction systems composed of different crops and/or different countries. The\nproposed software is powerful and easy to use with publicly available datasets\nsuch as the one used in this study.\n"
    },
    {
        "paper_id": 2006.09154,
        "authors": "Bao-Gen Li, Dian-Yi Ling, Zu-Guo Yu",
        "title": "Multifractal temporally weighted detrended partial cross-correlation\n  analysis to quantify intrinsic power-law cross-correlation of two\n  non-stationary time series affected by common external factors",
        "comments": "15 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.125920",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When common factors strongly influence two cross-correlated time series\nrecorded in complex natural and social systems, the results will be biased if\nwe use multifractal detrended cross-correlation analysis (MF-DXA) without\nconsidering these common factors. Based on multifractal temporally weighted\ndetrended cross-correlation analysis (MF-TWXDFA) proposed by our group and\nmultifractal partial cross-correlation analysis (MF-DPXA) proposed by Qian et\nal., we propose a new method---multifractal temporally weighted detrended\npartial cross-correlation analysis (MF-TWDPCCA) to quantify intrinsic power-law\ncross-correlation of two non-stationary time series affected by common external\nfactors in this paper. We use MF-TWDPCCA to characterize the intrinsic\ncross-correlations between the two simultaneously recorded time series by\nremoving the effects of other potential time series. To test the performance of\nMF-TWDPCCA, we apply it, MF-TWXDFA and MF-DPXA on simulated series. Numerical\ntests on artificially simulated series demonstrate that MF-TWDPCCA can\naccurately detect the intrinsic cross-correlations for two simultaneously\nrecorded series. To further show the utility of MF-TWDPCCA, we apply it on time\nseries from stock markets and find that there exists significantly multifractal\npower-law cross-correlation between stock returns. A new partial\ncross-correlation coefficient is defined to quantify the level of intrinsic\ncross-correlation between two time series.\n"
    },
    {
        "paper_id": 2006.09247,
        "authors": "Jie Fang and Jianwu Lin",
        "title": "Prior knowledge distillation based on financial time series",
        "comments": "Published on IEEE-INDIN-2020, 6 pages",
        "journal-ref": "IEEE INDIN 2020 Conference",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the major characteristics of financial time series is that they\ncontain a large amount of non-stationary noise, which is challenging for deep\nneural networks. People normally use various features to address this problem.\nHowever, the performance of these features depends on the choice of\nhyper-parameters. In this paper, we propose to use neural networks to represent\nthese indicators and train a large network constructed of smaller networks as\nfeature layers to fine-tune the prior knowledge represented by the indicators.\nDuring back propagation, prior knowledge is transferred from human logic to\nmachine logic via gradient descent. Prior knowledge is the deep belief of\nneural network and teaches the network to not be affected by non-stationary\nnoise. Moreover, co-distillation is applied to distill the structure into a\nmuch smaller size to reduce redundant features and the risk of overfitting. In\naddition, the decisions of the smaller networks in terms of gradient descent\nare more robust and cautious than those of large networks. In numerical\nexperiments, we find that our algorithm is faster and more accurate than\ntraditional methods on real financial datasets. We also conduct experiments to\nverify and comprehend the method.\n"
    },
    {
        "paper_id": 2006.09455,
        "authors": "Matteo Gambara and Josef Teichmann",
        "title": "Consistent Recalibration Models and Deep Calibration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consistent Recalibration models (CRC) have been introduced to capture in\nnecessary generality the dynamic features of term structures of derivatives'\nprices. Several approaches have been suggested to tackle this problem, but all\nof them, including CRC models, suffered from numerical intractabilities mainly\ndue to the presence of complicated drift terms or consistency conditions. We\novercome this problem by machine learning techniques, which allow to store the\ncrucial drift term's information in neural network type functions. This yields\nfirst time dynamic term structure models which can be efficiently simulated.\n"
    },
    {
        "paper_id": 2006.09474,
        "authors": "Amarin Siripanich and Taha Rashidi",
        "title": "A demographic microsimulation model with an integrated household\n  alignment method",
        "comments": "This paper has been submitted to a journal to be reviewed and\n  considered for publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many dynamic microsimulation models have shown their ability to reasonably\nproject detailed population and households using non-data based household\nformation and dissolution rules. Although, those rules allow modellers to\nsimplify changes in the household construction, they typically fall short in\nreplicating household projections or if applied retrospectively the observed\nhousehold numbers. Consequently, such models with biased estimation for\nhousehold size and other household related attributes lose their usefulness in\napplications that are sensitive to household size, such as in travel demand and\nhousing demand modelling. Nonetheless, these demographic microsimulation models\nwith their associated shortcomings have been commonly used to assess various\nplanning policies which can result in misleading judgements. In this paper, we\ncontribute to the literature of population microsimulation by introducing a\nfully integrated system of models for different life event where a household\nalignment method adjusts household size distribution to closely align with any\ngiven target distribution. Furthermore, some demographic events that are\ngenerally difficult to model, such as incorporating immigrant families into a\npopulation, can be included. We illustrated an example of the household\nalignment method and put it to test in a dynamic microsimulation model that we\ndeveloped using dymiumCore, a general-purpose microsimulation toolkit in R, to\nshow potential improvements and weaknesses of the method. The implementation of\nthis model has been made publicly available on GitHub.\n"
    },
    {
        "paper_id": 2006.09493,
        "authors": "Martin Larsson, Marvin S. Mueller and Josef Teichmann",
        "title": "Stopper-Controller Games embedded in Single-Player Control Problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 2002, Benjamin Jourdain and Claude Martini discovered that for a class of\npayoff functions, the pricing problem for American options can be reduced to\npricing of European options for an appropriately associated payoff, all within\na Black-Scholes framework. This discovery has been investigated in great detail\nby S\\\"oren Christensen, Jan Kallsen and Matthias Lenga in a recent work in\n2020. In the present work we prove that this phenomenon can be observed in a\nwider context, and even holds true in a setup of non-linear stochastic\nprocesses. We analyse this problem from both probabilistic and analytic\nviewpoints. In the classical situation, Jourdain and Martini used this method\nto approximate prices of American put options. The broader applicability now\npotentially covers non-linear frameworks such as model uncertainty and\ncontroller-and-stopper-games.\n"
    },
    {
        "paper_id": 2006.09518,
        "authors": "Kerry Back, Francois Cocquemas, Ibrahim Ekren, and Abraham Lioui",
        "title": "Optimal Transport and Risk Aversion in Kyle's Model of Informed Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish connections between optimal transport theory and the dynamic\nversion of the Kyle model, including new characterizations of informed trading\nprofits via conjugate duality and Monge-Kantorovich duality. We use these\nconnections to extend the model to multiple assets, general distributions, and\nrisk-averse market makers. With risk-averse market makers, liquidity is lower,\nassets exhibit short-term reversals, and risk premia depend on market maker\ninventories, which are mean reverting. We illustrate the model by showing that\nimplied volatilities predict stock returns when there is informed trading in\nstocks and options and market makers are risk averse.\n"
    },
    {
        "paper_id": 2006.09542,
        "authors": "Zhibin Niu, Runlin Li, Junqi Wu, Dawei Cheng, Jiawan Zhang",
        "title": "iConViz: Interactive Visual Exploration of the Default Contagion Risk of\n  Networked-Guarantee Loans",
        "comments": "2020 IEEE Conference on Visual Analytics Science and Technology\n  (VAST)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Groups of enterprises can serve as guarantees for one another and form\ncomplex networks when obtaining loans from commercial banks. During economic\nslowdowns, corporate default may spread like a virus and lead to large-scale\ndefaults or even systemic financial crises. To help financial regulatory\nauthorities and banks manage the risk associated with networked loans, we\nidentified the default contagion risk, a pivotal issue in developing preventive\nmeasures, and established iConVis, an interactive visual analysis tool that\nfacilitates the closed-loop analysis process. A novel financial metric, the\ncontagion effect, was formulated to quantify the infectious consequences of\nguarantee chains in this type of network. Based on this metric, we designed and\nimplement a series of novel and coordinated views that address the analysis of\nfinancial problems. Experts evaluated the system using real-world financial\ndata. The proposed approach grants practitioners the ability to avoid previous\nad hoc analysis methodologies and extend coverage of the conventional Capital\nAccord to the banking industry.\n"
    },
    {
        "paper_id": 2006.09611,
        "authors": "Laura Leal, Mathieu Lauri\\`ere, Charles-Albert Lehalle",
        "title": "Learning a functional control for high-frequency finance",
        "comments": "24 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We use a deep neural network to generate controllers for optimal trading on\nhigh frequency data. For the first time, a neural network learns the mapping\nbetween the preferences of the trader, i.e. risk aversion parameters, and the\noptimal controls. An important challenge in learning this mapping is that in\nintraday trading, trader's actions influence price dynamics in closed loop via\nthe market impact. The exploration--exploitation tradeoff generated by the\nefficient execution is addressed by tuning the trader's preferences to ensure\nlong enough trajectories are produced during the learning phase. The issue of\nscarcity of financial data is solved by transfer learning: the neural network\nis first trained on trajectories generated thanks to a Monte-Carlo scheme,\nleading to a good initialization before training on historical trajectories.\nMoreover, to answer to genuine requests of financial regulators on the\nexplainability of machine learning generated controls, we project the obtained\n\"blackbox controls\" on the space usually spanned by the closed-form solution of\nthe stylized optimal trading problem, leading to a transparent structure. For\nmore realistic loss functions that have no closed-form solution, we show that\nthe average distance between the generated controls and their explainable\nversion remains small. This opens the door to the acceptance of ML-generated\ncontrols by financial regulators.\n"
    },
    {
        "paper_id": 2006.09723,
        "authors": "Karolina Sowinska and Pranava Madhyastha",
        "title": "A Tweet-based Dataset for Company-Level Stock Return Prediction",
        "comments": "Dataset available here:\n  https://github.com/ImperialNLP/stockreturnpred",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Public opinion influences events, especially related to stock market\nmovement, in which a subtle hint can influence the local outcome of the market.\nIn this paper, we present a dataset that allows for company-level analysis of\ntweet based impact on one-, two-, three-, and seven-day stock returns. Our\ndataset consists of 862, 231 labelled instances from twitter in English, we\nalso release a cleaned subset of 85, 176 labelled instances to the community.\nWe also provide baselines using standard machine learning algorithms and a\nmulti-view learning based approach that makes use of different types of\nfeatures. Our dataset, scripts and models are publicly available at:\nhttps://github.com/ImperialNLP/stockreturnpred.\n"
    },
    {
        "paper_id": 2006.09955,
        "authors": "Pietro Rossi, Flavio Cocco, and Giacomo Bormetti",
        "title": "Deep learning Profit & Loss",
        "comments": "19 pages, 3 figures, and 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Building the future profit and loss (P&L) distribution of a portfolio\nholding, among other assets, highly non-linear and path-dependent derivatives\nis a challenging task. We provide a simple machinery where more and more assets\ncould be accounted for in a simple and semi-automatic fashion. We resort to a\nvariation of the Least Square Monte Carlo algorithm where interpolation of the\ncontinuation value of the portfolio is done with a feed forward neural network.\nThis approach has several appealing features not all of them will be fully\ndiscussed in the paper. Neural networks are extremely flexible regressors. We\ndo not need to worry about the fact that for multi assets payoff, the exercise\nsurface could be non connected. Neither we have to search for smart regressors.\nThe idea is to use, regardless of the complexity of the payoff, only the\nunderlying processes. Neural networks with many outputs can interpolate every\nsingle assets in the portfolio generated by a single Monte Carlo simulation.\nThis is an essential feature to account for the P&L distribution of the whole\nportfolio when the dependence structure between the different assets is very\nstrong like the case where one has contingent claims written on the same\nunderlying.\n"
    },
    {
        "paper_id": 2006.10194,
        "authors": "Ruomeng Cui (1), Hao Ding (1), Feng Zhu (2) ((1) Goizueta Business\n  School, Emory University, (2) Harvard Business School, Harvard University)",
        "title": "Gender Inequality in Research Productivity During the COVID-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the disproportionate impact of the lockdown as a result of the\nCOVID-19 outbreak on female and male academics' research productivity in social\nscience. The lockdown has caused substantial disruptions to academic\nactivities, requiring people to work from home. How this disruption affects\nproductivity and the related gender equity is an important operations and\nsocietal question. We collect data from the largest open-access preprint\nrepository for social science on 41,858 research preprints in 18 disciplines\nproduced by 76,832 authors across 25 countries over a span of two years. We use\na difference-in-differences approach leveraging the exogenous pandemic shock.\nOur results indicate that, in the 10 weeks after the lockdown in the United\nStates, although the total research productivity increased by 35%, female\nacademics' productivity dropped by 13.9% relative to that of male academics. We\nalso show that several disciplines drive such gender inequality. Finally, we\nfind that this intensified productivity gap is more pronounced for academics in\ntop-ranked universities, and the effect exists in six other countries. Our work\npoints out the fairness issue in productivity caused by the lockdown, a finding\nthat universities will find helpful when evaluating faculty productivity. It\nalso helps organizations realize the potential unintended consequences that can\narise from telecommuting.\n"
    },
    {
        "paper_id": 2006.10245,
        "authors": "Veronika Czellar, David T. Frazier and Eric Renault",
        "title": "Approximate Maximum Likelihood for Complex Structural Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Indirect Inference (I-I) is a popular technique for estimating complex\nparametric models whose likelihood function is intractable, however, the\nstatistical efficiency of I-I estimation is questionable. While the efficient\nmethod of moments, Gallant and Tauchen (1996), promises efficiency, the price\nto pay for this efficiency is a loss of parsimony and thereby a potential lack\nof robustness to model misspecification. This stands in contrast to simpler I-I\nestimation strategies, which are known to display less sensitivity to model\nmisspecification precisely due to their focus on specific elements of the\nunderlying structural model. In this research, we propose a new\nsimulation-based approach that maintains the parsimony of I-I estimation, which\nis often critical in empirical applications, but can also deliver estimators\nthat are nearly as efficient as maximum likelihood. This new approach is based\non using a constrained approximation to the structural model, which ensures\nidentification and can deliver estimators that are nearly efficient. We\ndemonstrate this approach through several examples, and show that this approach\ncan deliver estimators that are nearly as efficient as maximum likelihood, when\nfeasible, but can be employed in many situations where maximum likelihood is\ninfeasible.\n"
    },
    {
        "paper_id": 2006.10505,
        "authors": "Jozef Barunik and Zdenek Drabek and Matej Nevrla",
        "title": "Investment Disputes and Abnormal Volatility of Stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dramatic growth of investment disputes between foreign investors and host\nstates rises serious questions about the impact of those disputes on investors.\nThis paper is the first to explain increased uncertainty of investors about the\noutcome of arbitration, which may or may not lead to compensation for damages\nclaimed by the investor. We find robust evidence that investment disputes lead\nto abnormal share fluctuations of companies involved in disputes with host\ncountries. Importantly, while a positive outcome for an investor decreases\nuncertainty back to original levels, we document strong increase in the\nvolatility of companies with negative outcome for the investor. We find that\nseveral variables including size of the award, political instability, location\nof arbitration, country of origin of investor or public policy considerations\nin host country explain large portion of the investor's uncertainty.\n"
    },
    {
        "paper_id": 2006.10946,
        "authors": "Toshifumi Nakamura",
        "title": "A simple model of interbank trading with tiered remuneration",
        "comments": "8 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A negative interest rate policy is often accompanied by tiered remuneration,\nwhich allows for exemption from negative rates. This study proposes a basic\nmodel of interest rates formed in the interbank market with a tiering system.\nThe results predicted by the model largely mirror actual market developments in\nlate 2019, when the European Central Bank introduced, and the Switzerland\nNational Bank modified, the tiering system.\n"
    },
    {
        "paper_id": 2006.11088,
        "authors": "Martin Bladt and Alexander J. McNeil",
        "title": "Time series copula models using d-vines and v-transforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  An approach to modelling volatile financial return series using stationary\nd-vine copula processes combined with Lebesgue-measure-preserving\ntransformations known as v-transforms is proposed. By developing a method of\nstochastically inverting v-transforms, models are constructed that can describe\nboth stochastic volatility in the magnitude of price movements and serial\ncorrelation in their directions. In combination with parametric marginal\ndistributions it is shown that these models can rival and sometimes outperform\nwell-known models in the extended GARCH family.\n"
    },
    {
        "paper_id": 2006.11119,
        "authors": "Chenkai Xu, Hongwei Lin, Xuansu Fang",
        "title": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification",
        "comments": "18 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a novel stock index model, namely the manifold\nfeature(MF) index, to reflect the overall price activity of the entire stock\nmarket. Based on the theory of manifold learning, the researched stock dataset\nis assumed to be a low-dimensional manifold embedded in a higher-dimensional\nEuclidean space. After data preprocessing, its manifold structure and discrete\nLaplace-Beltrami operator(LBO) matrix are constructed. We propose a\nhigh-dimensional data feature detection method to detect feature points on the\neigenvectors of LBO, and the stocks corresponding to these feature points are\nconsidered as the constituent stocks of the MF index. Finally, the MF index is\ngenerated by a weighted formula using the price and market capitalization of\nthese constituents. The stock market studied in this research is the Shanghai\nStock Exchange(SSE). We propose four metrics to compare the MF index series and\nthe SSE index series (SSE 50, SSE 100, SSE 150, SSE 180 and SSE 380). From the\nperspective of data approximation, the results demonstrate that our indexes are\ncloser to the stock market than the SSE index series. From the perspective of\nrisk premium, MF indexes have higher stability and lower risk.\n"
    },
    {
        "paper_id": 2006.11146,
        "authors": "Richard J. Martin",
        "title": "Credit migration: Generating generators",
        "comments": "Minor corrections from V1",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Markovian credit migration models are a reasonably standard tool nowadays,\nbut there are fundamental difficulties with calibrating them. We show how these\nare resolved using a simplified form of matrix generator and explain why\nrisk-neutral calibration cannot be done without volatility information. We also\nshow how to use elementary ideas from differential geometry to make general\ninferences about calibration stability. This the longer version of an article\npublished by RISK (Feb 2021).\n"
    },
    {
        "paper_id": 2006.11156,
        "authors": "Tarun Chitra, Alex Evans",
        "title": "Why Stake When You Can Borrow?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As smart contract platforms autonomously manage billions of dollars of\ncapital, quantifying the portfolio risk that investors engender in these\nsystems is increasingly important. Recent work illustrates that Proof of Stake\n(PoS) is vulnerable to financial attacks arising from on-chain lending and has\nworse capital efficiency than Proof of Work (PoW) \\cite{fanti_pos_econ}.\nNumerous methods for improving capital efficiency have been proposed that allow\nstakers to create fungible derivative claims on their staked assets. In this\npaper, we construct a unifying model for studying the security risks of these\nproposals. This model combines birth-death P\\'olya processes and risk models\nadapted from the credit derivatives literature to assess token inequality and\nreturn profiles. We find that there is a sharp transition between 'safe' and\n'unsafe' derivative usage. Surprisingly, we find that contrary to\n\\cite{fanti2019compounding} there exist conditions where derivatives can\n\\emph{reduce} concentration of wealth in these networks. This model also\napplies to Decentralized Finance (DeFi) protocols where staked assets are used\nas insurance. Our theoretical results are validated using agent-based\nsimulation.\n"
    },
    {
        "paper_id": 2006.11222,
        "authors": "Sanjay Mansabdar and Hussain C Yaganti",
        "title": "Valuing the quality option in agricultural commodity futures: a Monte\n  Carlo simulation based approach",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agricultural commodity futures are often settled by delivery. Quality options\nthat allow the futures short to deliver one of several underlying assets are\ncommonly used in such contracts to prevent manipulation. Inclusion of these\noptions reduces the price of the futures contract and leads to degraded\ncontract hedging performance. Valuation of these options is a first step in\nassessing the impact of the quality options embedded into a futures contract.\nThis paper demonstrates a Monte Carlo simulation based approach to estimate the\nvalue of a quality option. In order to improve simulation efficiency, the\ntechnique of antithetic variables is used. This approach can help in the\nassessment of the impact of embedded quality options.\n"
    },
    {
        "paper_id": 2006.11279,
        "authors": "Derek Singh, Shuzhong Zhang",
        "title": "Distributionally Robust Profit Opportunities",
        "comments": "arXiv admin note: text overlap with arXiv:2004.09432",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper expands the notion of robust profit opportunities in financial\nmarkets to incorporate distributional uncertainty using Wasserstein distance as\nthe ambiguity measure. Financial markets with risky and risk-free assets are\nconsidered. The infinite dimensional primal problems are formulated, leading to\ntheir simpler finite dimensional dual problems. A principal motivating question\nis how does distributional uncertainty help or hurt the robustness of the\nprofit opportunity. Towards answering this question, some theory is developed\nand computational experiments are conducted. Finally some open questions and\nsuggestions for future research are discussed.\n"
    },
    {
        "paper_id": 2006.11426,
        "authors": "Bastien Baldacci, Jerome Benveniste",
        "title": "A note on Almgren-Chriss optimal execution problem with geometric\n  Brownian motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve explicitly the Almgren-Chriss optimal liquidation problem where the\nstock price process follows a geometric Brownian motion. Our technique is to\nwork in terms of cash and to use functional analysis tools. We show that this\nframework extends readily to the case of a stochastic drift for the price\nprocess and the liquidation of a portfolio.\n"
    },
    {
        "paper_id": 2006.11749,
        "authors": "Asahi Noguchi",
        "title": "Shifting Policy Strategy in Keynesianism",
        "comments": null,
        "journal-ref": "Review of Keynesian Studies (The Keynes Society Japan), 2019,\n  Vol.1, pp.61-91",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the evolution of Keynesianism making use of concepts\noffered by Imre Lakatos. The Keynesian \"hard core\" lies in its views regarding\nthe instability of the market economy, its \"protective belt\" in the policy\nstrategy for macroeconomic stabilization using fiscal policy and monetary\npolicy. Keynesianism developed as a policy program to counter classical\nliberalism, which attributes priority to the autonomy of the market economy and\ntries to limit the role of government. In general, the core of every policy\nprogram consists in an unfalsifiable worldview and a value judgment that remain\nunchanged. On the other hand, a policy strategy with a protective belt\ninevitably evolves owing to changes in reality and advances in scientific\nknowledge. This is why the Keynesian policy strategy has shifted from being\nfiscal-led to one that is monetary-led because of the influence of monetarism;\nfurther, the Great Recession has even led to their integration.\n"
    },
    {
        "paper_id": 2006.1175,
        "authors": "Asahi Noguchi",
        "title": "The Economic Costs of Containing a Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The coronavirus disease (COVID-19) has caused one of the most serious social\nand economic losses to countries around the world since the Spanish influenza\npandemic of 1918 (during World War I). It has resulted in enormous economic as\nwell as social costs, such as increased deaths from the spread of infection in\na region. This is because public regulations imposed by national and local\ngovernments to deter the spread of infection inevitably involves a deliberate\nsuppression of the level of economic activity. Given this trade-off between\neconomic activity and epidemic prevention, governments should execute public\ninterventions to minimize social and economic losses from the pandemic. A major\nproblem regarding the resultant economic losses is that it unequally impacts\ncertain strata of the society. This raises an important question on how such\neconomic losses should be shared equally across the society. At the same time,\nthere is some antipathy towards economic compensation by means of public debt,\nwhich is likely to increase economic burden in the future. However, as Paul\nSamuelson once argued, much of the burden, whether due to public debt or\notherwise, can only be borne by the present generation, and not by future\ngenerations.\n"
    },
    {
        "paper_id": 2006.11888,
        "authors": "A. Hilario-Caballero, A. Garcia-Bernabeu, J. V. Salcedo, M. Vercher",
        "title": "Tri-criterion model for constructing low-carbon mutual fund portfolios:\n  a preference-based multi-objective genetic algorithm approach",
        "comments": "14 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sustainable finance, which integrates environmental, social and governance\n(ESG) criteria on financial decisions rests on the fact that money should be\nused for good purposes. Thus, the financial sector is also expected to play a\nmore important role to decarbonise the global economy. To align financial flows\nwith a pathway towards a low-carbon economy, investors should be able to\nintegrate in their financial decisions additional criteria beyond return and\nrisk to manage climate risk. We propose a tri-criterion portfolio selection\nmodel to extend the classical Markowitz mean-variance approach in order to\ninclude investors preferences on the portfolio carbon risk exposure as an\nadditional criterion. To approximate the 3D Pareto front we apply an efficient\nmulti-objective genetic algorithm called ev-MOGA which is based on the concept\nof e-dominance. Furthermore, we introduce an a posteriori approach to\nincorporate the investor's preferences into the solution process regarding\ntheir sustainability preferences measured by the carbon risk exposure and\nhis/her loss-adverse attitude. We test the performance of the proposed\nalgorithm in a cross section of European SRI open-end funds to assess the\nextent to which climate related risk could be embedded in the portfolio\naccording to the investor's preferences.\n"
    },
    {
        "paper_id": 2006.11914,
        "authors": "Ale\\v{s} \\v{C}ern\\'y and Johannes Ruf",
        "title": "Simplified stochastic calculus via semimartingale representations",
        "comments": "32 pages; updated references; fixed a small typo in proof of T3.17",
        "journal-ref": "Electronic Journal of Probability, 27, article no. 3, 1-32, 2022",
        "doi": "10.1214/21-EJP729",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a stochastic calculus that makes it easy to capture a variety of\npredictable transformations of semimartingales such as changes of variables,\nstochastic integrals, and their compositions. The framework offers a unified\ntreatment of real-valued and complex-valued semimartingales. The proposed\ncalculus is a blueprint for the derivation of new relationships among\nstochastic processes with specific examples provided below.\n"
    },
    {
        "paper_id": 2006.11976,
        "authors": "A. Itkin, A. Lipton, D. Muravey",
        "title": "From the Black-Karasinski to the Verhulst model to accommodate the\n  unconventional Fed's policy",
        "comments": "21 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we argue that some of the most popular short-term interest\nmodels have to be revisited and modified to reflect current market conditions\nbetter. In particular, we propose a modification of the popular\nBlack-Karasinski model, which is widely used by practitioners for modeling\ninterest rates, credit, and commodities. Our adjustment gives rise to the\nstochastic Verhulst model, which is well-known in the population dynamics and\nepidemiology as a logistic model. We demonstrate that the Verhulst model's\ndynamics are well suited to the current economic environment and the Fed's\nactions. Besides, we derive new integral equations for the zero-coupon bond\nprices for both the BK and Verhulst models. For the BK model for small\nmaturities up to 2 years, we solve the corresponding integral equation by using\nthe reduced differential transform method. For the Verhulst integral equation,\nunder some mild assumptions, we find the closed-form solution. Numerical\nexamples show that computationally our approach is significantly more efficient\nthan the standard finite difference method.\n"
    },
    {
        "paper_id": 2006.12022,
        "authors": "Daniel Bartl, Samuel Drapeau, Jan Obloj, Johannes Wiesel",
        "title": "Sensitivity analysis of Wasserstein distributionally robust optimization\n  problems",
        "comments": "Forthcoming in \"Proceedings of the Royal Society A\"",
        "journal-ref": null,
        "doi": "10.1098/rspa.2021.0176",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider sensitivity of a generic stochastic optimization problem to model\nuncertainty. We take a non-parametric approach and capture model uncertainty\nusing Wasserstein balls around the postulated model. We provide explicit\nformulae for the first order correction to both the value function and the\noptimizer and further extend our results to optimization under linear\nconstraints. We present applications to statistics, machine learning,\nmathematical finance and uncertainty quantification. In particular, we provide\nexplicit first-order approximation for square-root LASSO regression\ncoefficients and deduce coefficient shrinkage compared to the ordinary least\nsquares regression. We consider robustness of call option pricing and deduce a\nnew Black-Scholes sensitivity, a non-parametric version of the so-called Vega.\nWe also compute sensitivities of optimized certainty equivalents in finance and\npropose measures to quantify robustness of neural networks to adversarial\nexamples.\n"
    },
    {
        "paper_id": 2006.12251,
        "authors": "Mudit Kapoor and Shamika Ravi",
        "title": "Impact of national lockdown on COVID-19 deaths in select European\n  countries and the US using a Changes-in-Changes model",
        "comments": "13 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we estimate the impact of national lockdown on COVID-19\nrelated total and daily deaths, per million people, in select European\ncountries. In particular, we compare countries that imposed a nationwide\nlockdown (Treatment group); Belgium, Denmark, France, Germany, Italy, Norway,\nSpain, United Kingdom (UK), and the US, to Sweden (Control group) that did not\nimpose national lockdown using a changes-in-changes (CIC) estimation model. The\nkey advantage of the CIC model as compared to the standard\ndifference-in-difference model is that CIC allows for mean and variance of the\noutcomes to change over time in the absence of any policy intervention, and CIC\naccounts for endogeneity in the choice of policy intervention. Our results\nindicate that in contrast to Sweden, which did not impose a national lockdown,\nGermany, and to some extent, the US were the two countries where nationwide\nlockdown had a significant impact on the reduction in COVID-19 related total\nand daily deaths per million people. In Norway and Denmark, there was no\nsignificant impact on total and daily deaths per million people relative to\nSweden. Whereas in other countries; Belgium, France, Italy, Spain, and the UK,\nthe effect of the lockdown was in the opposite direction, that is, they\nexperienced significantly higher COVID-19 related total and daily deaths per\nmillion people, post the lockdown as compared to Sweden. Our results suggest\nthat the impact of nationwide lockdown on COVID-19 related total and daily\ndeaths per million people varied from one country to another.\n"
    },
    {
        "paper_id": 2006.12388,
        "authors": "Ariah Klages-Mundt, Dominik Harz, Lewis Gudgeon, Jun-You Liu, Andreea\n  Minca",
        "title": "Stablecoins 2.0: Economic Foundations and Risk-based Models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3419614.3423261",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stablecoins are one of the most widely capitalized type of cryptocurrency.\nHowever, their risks vary significantly according to their design and are often\npoorly understood. We seek to provide a sound foundation for stablecoin theory,\nwith a risk-based functional characterization of the economic structure of\nstablecoins. First, we match existing economic models to the disparate set of\ncustodial systems. Next, we characterize the unique risks that emerge in\nnon-custodial stablecoins and develop a model framework that unifies existing\nmodels from economics and computer science. We further discuss how this\nmodeling framework is applicable to a wide array of cryptoeconomic systems,\nincluding cross-chain protocols, collateralized lending, and decentralized\nexchanges. These unique risks yield unanswered research questions that will\nform the crux of research in decentralized finance going forward.\n"
    },
    {
        "paper_id": 2006.12426,
        "authors": "Jonathan Readshaw and Stefano Giani",
        "title": "Using Company Specific Headlines and Convolutional Neural Networks to\n  Predict Stock Fluctuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents a Convolutional Neural Network (CNN) for the prediction of\nnext-day stock fluctuations using company-specific news headlines. Experiments\nto evaluate model performance using various configurations of word-embeddings\nand convolutional filter widths are reported. The total number of convolutional\nfilters used is far fewer than is common, reducing the dimensionality of the\ntask without loss of accuracy. Furthermore, multiple hidden layers with\ndecreasing dimensionality are employed. A classification accuracy of 61.7\\% is\nachieved using pre-learned embeddings, that are fine-tuned during training to\nrepresent the specific context of this task. Multiple filter widths are also\nimplemented to detect different length phrases that are key for classification.\nTrading simulations are conducted using the presented classification results.\nInitial investments are more than tripled over a 838 day testing period using\nthe optimal classification configuration and a simple trading strategy. Two\nnovel methods are presented to reduce the risk of the trading simulations.\nAdjustment of the sigmoid class threshold and re-labelling headlines using\nmultiple classes form the basis of these methods. A combination of these\napproaches is found to more than double the Average Trade Profit (ATP) achieved\nduring baseline simulations.\n"
    },
    {
        "paper_id": 2006.12623,
        "authors": "Francesca Greselin, Simone Pellegrino, Achille Vernizzi",
        "title": "The Social Welfare Implications of the Zenga Index",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the social welfare implications of the Zenga index, a recently\nproposed index of inequality. Our proposal is derived by following the seminal\nbook by Son (2011) and the recent working paper by Kakwani and Son (2019). We\ncompare the Zenga based approach with the classical one, based on the Lorenz\ncurve and the Gini coefficient, as well as the Bonferroni index. We show that\nthe social welfare specification based on the Zenga uniformity curve presents\nsome peculiarities that distinguish it from the other considered indexes. The\nsocial welfare specification presented here provides a deeper understanding of\nhow the Zenga index evaluates the inequality in a distribution.\n"
    },
    {
        "paper_id": 2006.12686,
        "authors": "Nelson Vadori and Sumitra Ganesh and Prashant Reddy and Manuela Veloso",
        "title": "Risk-Sensitive Reinforcement Learning: a Martingale Approach to Reward\n  Uncertainty",
        "comments": "Published at ICAIF 2020: ACM International Conference on AI in\n  Finance",
        "journal-ref": null,
        "doi": "10.1145/3383455.3422519",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel framework to account for sensitivity to rewards\nuncertainty in sequential decision-making problems. While risk-sensitive\nformulations for Markov decision processes studied so far focus on the\ndistribution of the cumulative reward as a whole, we aim at learning policies\nsensitive to the uncertain/stochastic nature of the rewards, which has the\nadvantage of being conceptually more meaningful in some cases. To this end, we\npresent a new decomposition of the randomness contained in the cumulative\nreward based on the Doob decomposition of a stochastic process, and introduce a\nnew conceptual tool - the \\textit{chaotic variation} - which can rigorously be\ninterpreted as the risk measure of the martingale component associated to the\ncumulative reward process. We innovate on the reinforcement learning side by\nincorporating this new risk-sensitive approach into model-free algorithms, both\npolicy gradient and value function based, and illustrate its relevance on grid\nworld and portfolio optimization problems.\n"
    },
    {
        "paper_id": 2006.12765,
        "authors": "Ale\\v{s} \\v{C}ern\\'y and Johannes Ruf",
        "title": "Simplified calculus for semimartingales: Multiplicative compensators and\n  changes of measure",
        "comments": "30 pages",
        "journal-ref": "Stochastic Processes and Their Applications 161, 572-602, 2023",
        "doi": "10.1016/j.spa.2023.04.010",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper develops multiplicative compensation for complex-valued\nsemimartingales and studies some of its consequences. It is shown that the\nstochastic exponential of any complex-valued semimartingale with independent\nincrements becomes a true martingale after multiplicative compensation when\nsuch compensation is meaningful. This generalization of the L\\'evy--Khintchin\nformula fills an existing gap in the literature. It allows, for example, the\ncomputation of the Mellin transform of a signed stochastic exponential, which\nin turn has practical applications in mean--variance portfolio theory.\nGirsanov-type results based on multiplicatively compensated semimartingales\nsimplify treatment of absolutely continuous measure changes. As an example, we\nobtain the characteristic function of log returns for a popular class of\nminimax measures in a L\\'evy setting.\n"
    },
    {
        "paper_id": 2006.12966,
        "authors": "Guido Ascari and Sophocles Mavroeidis",
        "title": "The unbearable lightness of equilibria in a low interest rate\n  environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Structural models with no solution are incoherent, and those with multiple\nsolutions are incomplete. We show that models with occasionally binding\nconstraints are not generically coherent. Coherency requires restrictions on\nthe parameters or on the support of the distribution of the shocks. In presence\nof multiple shocks, the support restrictions cannot be independent from each\nother, so the assumption of orthogonality of structural shocks is incompatible\nwith coherency. Models whose coherency is based on support restrictions are\ngenerically incomplete, admitting a very large number of minimum state variable\nsolutions.\n"
    },
    {
        "paper_id": 2006.12989,
        "authors": "George Bouzianis and Lane P. Hughston",
        "title": "Optimal Hedging in Incomplete Markets",
        "comments": "22 pages, 4 figures, to appear in Applied Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal hedging in an incomplete market with an\nestablished pricing kernel. In such a market, prices are uniquely determined,\nbut perfect hedges are usually not available. We work in the rather general\nsetting of a L\\'evy-Ito market, where assets are driven jointly by an\n$n$-dimensional Brownian motion and an independent Poisson random measure on an\n$n$-dimensional state space. Given a position in need of hedging and the\ninstruments available as hedges, we demonstrate the existence of an optimal\nhedge portfolio, where optimality is defined by use of an expected least\nsquared-error criterion over a specified time frame, and where the numeraire\nwith respect to which the hedge is optimized is taken to be the benchmark\nprocess associated with the designated pricing kernel.\n"
    },
    {
        "paper_id": 2006.13036,
        "authors": "S. Chakravarty, M. Lundberg, P. Nikolov, J. Zenker",
        "title": "Vocational Training Programs and Youth Labor Market Outcomes: Evidence\n  from Nepal",
        "comments": null,
        "journal-ref": "Journal of Development Economics 136, 71-110 (2019)",
        "doi": "10.1016/j.jdeveco.2018.09.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lack of skills is arguably one of the most important determinants of high\nlevels of unemployment and poverty. In response, policymakers often initiate\nvocational training programs in effort to enhance skill formation among the\nyouth. Using a regression-discontinuity design, we examine a large youth\ntraining intervention in Nepal. We find, twelve months after the start of the\ntraining program, that the intervention generated an increase in non-farm\nemployment of 10 percentage points (ITT estimates) and up to 31 percentage\npoints for program compliers (LATE estimates). We also detect sizeable gains in\nmonthly earnings. Women who start self-employment activities inside their homes\nlargely drive these impacts. We argue that low baseline educational levels and\nnon-farm employment levels and Nepal's social and cultural norms towards women\ndrive our large program impacts. Our results suggest that the program enables\notherwise underemployed women to earn an income while staying at home - close\nto household errands and in line with the socio-cultural norms that prevent\nthem from taking up employment outside the house.\n"
    },
    {
        "paper_id": 2006.13181,
        "authors": "Josef Dan\\v{e}k and J. Posp\\'i\\v{s}il",
        "title": "Numerical aspects of integration in semi-closed option pricing formulas\n  for stochastic volatility jump diffusion models",
        "comments": null,
        "journal-ref": "Int. J. Comput. Math. 97(6), 1268-1292, 2020",
        "doi": "10.1080/00207160.2019.1614174",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In mathematical finance, a process of calibrating stochastic volatility (SV)\noption pricing models to real market data involves a numerical calculation of\nintegrals that depend on several model parameters. This optimization task\nconsists of large number of integral evaluations with high precision and low\ncomputational time requirements. However, for some model parameters, many\nnumerical quadrature algorithms fail to meet these requirements. We can observe\nan enormous increase in function evaluations, serious precision problems and a\nsignificant increase of computational time. In this paper we numerically\nanalyse these problems and show that they are especially caused by inaccurately\nevaluated integrands. We propose a fast regime switching algorithm that tells\nif it is sufficient to evaluate the integrand in standard double arithmetic or\nif a higher precision arithmetic has to be used. We compare and recommend\nnumerical quadratures for typical SV models and different parameter values,\nespecially for problematic cases.\n"
    },
    {
        "paper_id": 2006.13209,
        "authors": "Thilo Klein and Robert Aue and Josue Ortega",
        "title": "School choice with independent versus consolidated districts",
        "comments": null,
        "journal-ref": "Volume 147, September 2024, Pages 170-205",
        "doi": "10.1016/j.geb.2024.07.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the welfare effects of school district consolidation.\nUsing incomplete rank-ordered lists (ROLs) submitted for admission to the\nHungarian secondary school system, we estimate complete ROLs assuming that\nparents do not use dominated strategies and that the matching outcome is\nstable. These estimates aid in constructing a counterfactual district-based\nassignment and discerning the factors driving parents' preferences over\nschools. We find that district consolidation leads to large welfare gains in\nBudapest, equivalent to students attending a school five kilometres closer to\ntheir residences. These gains offset the additional travel distances incurred\nin the consolidated assignment. 73\\% of matched students benefit from district\nconsolidation, while fewer than 3\\% are assigned to a less preferred school.\nStudents from smaller and less under-demanded districts benefit relatively\nmore, as well as those with high academic ability. Using reported preferences\ninstead of estimated ones also yields large gains from district consolidation.\n"
    },
    {
        "paper_id": 2006.13368,
        "authors": "Ding Wang, Brian Yueshuai He, Jingqin Gao, Joseph Y. J. Chow, Kaan\n  Ozbay, Shri Iyer",
        "title": "Impact of COVID-19 behavioral inertia on reopening strategies for New\n  York City Transit",
        "comments": null,
        "journal-ref": "International Journal of Transportation Science & Technology 10(2)\n  197-211 (2021)",
        "doi": "10.1016/j.ijtst.2021.01.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has affected travel behaviors and transportation system\noperations, and cities are grappling with what policies can be effective for a\nphased reopening shaped by social distancing. A baseline model was previously\ndeveloped and calibrated for pre-COVID conditions as MATSim-NYC. A new COVID\nmodel is calibrated that represents travel behavior during the COVID-19\npandemic by recalibrating the population agendas to include work-from-home and\nre-estimating the mode choice model for MATSim-NYC to fit observed traffic and\ntransit ridership data. Assuming the change in behavior exhibits inertia during\nreopening, we analyze the increase in car traffic due to the phased reopen plan\nguided by the state government of New York. Four reopening phases and two\nreopening scenarios (with and without transit capacity restrictions) are\nanalyzed. A Phase 4 reopening with 100% transit capacity may only see as much\nas 73% of pre-COVID ridership and an increase in the number of car trips by as\nmuch as 142% of pre-pandemic levels. Limiting transit capacity to 50% would\ndecrease transit ridership further from 73% to 64% while increasing car trips\nto as much as 143% of pre-pandemic levels. While the increase appears small,\nthe impact on consumer surplus is disproportionately large due to already\nincreased traffic congestion. Many of the trips also get shifted to other modes\nlike micromobility. The findings imply that a transit capacity restriction\npolicy during reopening needs to be accompanied by (1) support for\nmicromobility modes, particularly in non-Manhattan boroughs, and (2) congestion\nalleviation policies that focus on reducing traffic in Manhattan, such as\ncordon-based pricing.\n"
    },
    {
        "paper_id": 2006.13521,
        "authors": "Herv\\'e Andres, Pierre-Edouard Arrouy, Paul Bonnefoy, Alexandre\n  Boumezoued, Sophian Mehalla",
        "title": "Fast calibration of the LIBOR Market Model with Stochastic Volatility\n  based on analytical gradient",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to take advantage of the common knowledge of the characteristic\nfunction of the swap rate process as modelled in the LIBOR Market Model with\nStochastic Volatility and Displaced Diffusion (DDSVLMM) to derive analytical\nexpressions of the gradient of swaptions prices with respect to the model\nparameters. We use this result to derive an efficient calibration method for\nthe DDSVLMM using gradient-based optimization algorithms. Our study relies on\nand extends the work by (Cui et al., 2017) that developed the analytical\ngradient for fast calibration of the Heston model, based on an alternative\nformulation of the Heston moment generating function proposed by (del Ba{\\~n}o\net al., 2010). Our main conclusion is that the analytical gradient-based\ncalibration is highly competitive for the DDSVLMM, as it significantly limits\nthe number of steps in the optimization algorithm while improving its accuracy.\nThe efficiency of this novel approach is compared to classical standard\noptimization procedures.\n"
    },
    {
        "paper_id": 2006.13539,
        "authors": "Eduardo Abi Jaber (CES), Enzo Miller (LPSM (UMR\\_8001)), Huy\\^en Pham\n  (LPSM (UMR\\_8001))",
        "title": "Markowitz portfolio selection for multivariate affine and quadratic\n  Volterra models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns portfolio selection with multiple assets under rough\ncovariance matrix. We investigate the continuous-time Markowitz mean-variance\nproblem for a multivariate class of affine and quadratic Volterra models. In\nthis incomplete non-Markovian and non-semimartingale market framework with\nunbounded random coefficients, the optimal portfolio strategy is expressed by\nmeans of a Riccati backward stochastic differential equation (BSDE). In the\ncase of affine Volterra models, we derive explicit solutions to this BSDE in\nterms of multi-dimensional Riccati-Volterra equations. This framework includes\nmultivariate rough Heston models and extends the results of \\cite{han2019mean}.\nIn the quadratic case, we obtain new analytic formulae for the the Riccati BSDE\nand we establish their link with infinite dimensional Riccati equations. This\ncovers rough Stein-Stein and Wishart type covariance models. Numerical results\non a two dimensional rough Stein-Stein model illustrate the impact of rough\nvolatilities and stochastic correlations on the optimal Markowitz strategy. In\nparticular for positively correlated assets, we find that the optimal strategy\nin our model is a `buy rough sell smooth' one.\n"
    },
    {
        "paper_id": 2006.13585,
        "authors": "Ryan Donnelly and Matthew Lorig",
        "title": "Optimal Trading with Differing Trade Signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of maximizing portfolio value when an agent has a\nsubjective view on asset value which differs from the traded market price. The\nagent's trades will have a price impact which affect the price at which the\nasset is traded. In addition to the agent's trades affecting the market price,\nthe agent may change his view on the asset's value if its difference from the\nmarket price persists. We also consider a situation of several agents\ninteracting and trading simultaneously when they have a subjective view on the\nasset value. Two cases of the subjective views of agents are considered, one in\nwhich they all share the same information, and one in which they all have an\nindividual signal correlated with price innovations. To study the large agent\nproblem we take a mean-field game approach which remains tractable. After\nclassifying the mean-field equilibrium we compute the cross-sectional\ndistribution of agents' inventories and the dependence of price distribution on\nthe amount of shared information among the agents.\n"
    },
    {
        "paper_id": 2006.13661,
        "authors": "Lijun Bo, Huafu Liao, Xiang Yu",
        "title": "Optimal Tracking Portfolio with A Ratcheting Capital Benchmark",
        "comments": "Final version, forthcoming in SIAM Journal on Control and\n  Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the finite horizon portfolio management by optimally\ntracking a ratcheting capital benchmark process. It is assumed that the fund\nmanager can dynamically inject capital into the portfolio account such that the\ntotal capital dominates a non-decreasing benchmark floor process at each\nintermediate time. The tracking problem is formulated to minimize the cost of\naccumulated capital injection. We first transform the original problem with\nfloor constraints into an unconstrained control problem, however, under a\nrunning maximum cost. By identifying a controlled state process with\nreflection, the problem is further shown to be equivalent to an auxiliary\nproblem, which leads to a nonlinear Hamilton-Jacobi-Bellman (HJB) equation with\na Neumann boundary condition. By employing the dual transform, the\nprobabilistic representation and some stochastic flow analysis, the existence\nof the unique classical solution to the HJB equation is established. The\nverification theorem is carefully proved, which gives the complete\ncharacterization of the feedback optimal portfolio. The application to market\nindex tracking is also discussed when the index process is modeled by a\ngeometric Brownian motion.\n"
    },
    {
        "paper_id": 2006.1385,
        "authors": "Matteo Fontana, Massimo Tavoni, Simone Vantini",
        "title": "Global Sensitivity and Domain-Selective Testing for Functional-Valued\n  Responses: An Application to Climate Economy Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the dynamics and evolution of climate change and associated\nuncertainties is key for designing robust policy actions. Computer models are\nkey tools in this scientific effort, which have now reached a high level of\nsophistication and complexity. Model auditing is needed in order to better\nunderstand their results, and to deal with the fact that such models are\nincreasingly opaque with respect to their inner workings. Current techniques\nsuch as Global Sensitivity Analysis (GSA) are limited to dealing either with\nmultivariate outputs, stochastic ones, or finite-change inputs. This limits\ntheir applicability to time-varying variables such as future pathways of\ngreenhouse gases. To provide additional semantics in the analysis of a model\nensemble, we provide an extension of GSA methodologies tackling the case of\nstochastic functional outputs with finite change inputs. To deal with finite\nchange inputs and functional outputs, we propose an extension of currently\navailable GSA methodologies while we deal with the stochastic part by\nintroducing a novel, domain-selective inferential technique for sensitivity\nindices. Our method is explored via a simulation study that shows its\nrobustness and efficacy in detecting sensitivity patterns. We apply it to real\nworld data, where its capabilities can provide to practitioners and\npolicymakers additional information about the time dynamics of sensitivity\npatterns, as well as information about robustness.\n"
    },
    {
        "paper_id": 2006.13889,
        "authors": "Paul Friedrich and Josef Teichmann",
        "title": "Deep Investing in Kyle's Single Period Model",
        "comments": "12 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Kyle model describes how an equilibrium of order sizes and security\nprices naturally arises between a trader with insider information and the price\nproviding market maker as they interact through a series of auctions. Ever\nsince being introduced by Albert S. Kyle in 1985, the model has become\nimportant in the study of market microstructure models with asymmetric\ninformation. As it is well understood, it serves as an excellent opportunity to\nstudy how modern deep learning technology can be used to replicate and better\nunderstand equilibria that occur in certain market learning problems.\n  We model the agents in Kyle's single period setting using deep neural\nnetworks. The networks are trained by interacting following the rules and\nobjectives as defined by Kyle. We show how the right network architectures and\ntraining methods lead to the agents' behaviour converging to the theoretical\nequilibrium that is predicted by Kyle's model.\n"
    },
    {
        "paper_id": 2006.13921,
        "authors": "Revathi Bhuvaneswari, Antonio Segalini",
        "title": "Determining Secondary Attributes for Credit Evaluation in P2P Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There has been an increased need for secondary means of credit evaluation by\nboth traditional banking organizations as well as peer-to-peer lending\nentities. This is especially important in the present technological era where\nsticking with strict primary credit histories doesn't help distinguish between\na 'good' and a 'bad' borrower, and ends up hurting both the individual borrower\nas well as the investor as a whole. We utilized machine learning classification\nand clustering algorithms to accurately predict a borrower's creditworthiness\nwhile identifying specific secondary attributes that contribute to this score.\nWhile extensive research has been done in predicting when a loan would be fully\npaid, the area of feature selection for lending is relatively new. We achieved\n65% F1 and 73% AUC on the LendingClub data while identifying key secondary\nattributes.\n"
    },
    {
        "paper_id": 2006.13922,
        "authors": "Lewis Gudgeon and Sam M. Werner and Daniel Perez and William J.\n  Knottenbelt",
        "title": "DeFi Protocols for Loanable Funds: Interest Rates, Liquidity and Market\n  Efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We coin the term *Protocols for Loanable Funds (PLFs)* to refer to protocols\nwhich establish distributed ledger-based markets for loanable funds. PLFs are\nemerging as one of the main applications within Decentralized Finance (DeFi),\nand use smart contract code to facilitate the intermediation of loanable funds.\nIn doing so, these protocols allow agents to borrow and save programmatically.\nWithin these protocols, interest rate mechanisms seek to equilibrate the supply\nand demand for funds. In this paper, we review the methodologies used to set\ninterest rates on three prominent DeFi PLFs, namely Compound, Aave and dYdX. We\nprovide an empirical examination of how these interest rate rules have behaved\nsince their inception in response to differing degrees of liquidity. We then\ninvestigate the market efficiency and inter-connectedness between multiple\nprotocols, examining first whether Uncovered Interest Parity holds within a\nparticular protocol and second whether the interest rates for a particular\ntoken market show dependence across protocols, developing a Vector Error\nCorrection Model for the dynamics.\n"
    },
    {
        "paper_id": 2006.13934,
        "authors": "Domonkos F. Vamossy",
        "title": "Investor Emotions and Earnings Announcements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Armed with a decade of social media data, I explore the impact of investor\nemotions on earnings announcements. In particular, I test whether the emotional\ncontent of firm-specific messages posted on social media just prior to a firm's\nearnings announcement predicts its earnings and announcement returns. I find\nthat investors are typically excited about firms that end up exceeding\nexpectations, yet their enthusiasm results in lower announcement returns.\nSpecifically, a standard deviation increase in excitement is associated with an\n7.8 basis points lower announcement return, which translates into an\napproximately -5.8% annualized loss. My findings confirm that emotions and\nmarket dynamics are closely related and highlight the importance of considering\ninvestor emotions when assessing a firm's short-term value.\n"
    },
    {
        "paper_id": 2006.14047,
        "authors": "Mario Alloza, Jesus Gonzalo and Carlos Sanz",
        "title": "Dynamic Effects of Persistent Shocks",
        "comments": "37 pages, 2 Tables, 5 Figures and 34 pages of Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide evidence that many narrative shocks used by prominent literature\nare persistent. We show that the two leading methods to estimate impulse\nresponses to an independently identified shock (local projections and\ndistributed lag models) treat persistence differently, hence identifying\ndifferent objects. We propose corrections to re-establish the equivalence\nbetween local projections and distributed lag models, providing applied\nresearchers with methods and guidance to estimate their desired object of\ninterest. We apply these methods to well-known empirical work and find that how\npersistence is treated has a sizable impact on the estimates of dynamic\neffects.\n"
    },
    {
        "paper_id": 2006.14121,
        "authors": "Zura Kakushadze",
        "title": "Option Pricing: Channels, Target Zones and Sideways Markets",
        "comments": "11 pages",
        "journal-ref": "Bulletin of Applied Economics 7(2) (2020) 25-33",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After a market downturn, especially in an uncertain economic environment such\nas the current state, there can be a relatively long period with a sideways\nmarket, where indexes, stocks, etc., move in channels with support and\nresistance levels. We discuss option pricing in such scenarios, in both cases\nof unattainable as well as attainable boundaries, and obtain closed-form option\npricing formulas. Our results also apply to FX rates in target zones without\ninterest rate pegging (USD/HKD, digital currencies, etc.).\n"
    },
    {
        "paper_id": 2006.14272,
        "authors": "Max Nendel, Frank Riedel, Maren Diane Schmeck",
        "title": "A decomposition of general premium principles into risk and deviation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an axiomatic approach to general premium principles in a\nprobability-free setting that allows for Knightian uncertainty. Every premium\nprinciple is the sum of a risk measure, as a generalization of the expected\nvalue, and a deviation measure, as a generalization of the variance. One can\nuniquely identify a maximal risk measure and a minimal deviation measure in\nsuch decompositions. We show how previous axiomatizations of premium principles\ncan be embedded into our more general framework. We discuss dual\nrepresentations of convex premium principles, and study the consistency of\npremium principles with a financial market in which insurance contracts are\ntraded.\n"
    },
    {
        "paper_id": 2006.14288,
        "authors": "Ariel Neufeld, Antonis Papapantoleon, Qikun Xiang",
        "title": "Model-free bounds for multi-asset options using option-implied\n  information and their exact computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider derivatives written on multiple underlyings in a one-period\nfinancial market, and we are interested in the computation of model-free upper\nand lower bounds for their arbitrage-free prices. We work in a completely\nrealistic setting, in that we only assume the knowledge of traded prices for\nother single- and multi-asset derivatives, and even allow for the presence of\nbid-ask spread in these prices. We provide a fundamental theorem of asset\npricing for this market model, as well as a superhedging duality result, that\nallows to transform the abstract maximization problem over probability measures\ninto a more tractable minimization problem over vectors, subject to certain\nconstraints. Then, we recast this problem into a linear semi-infinite\noptimization problem, and provide two algorithms for its solution. These\nalgorithms provide upper and lower bounds for the prices that are\n$\\varepsilon$-optimal, as well as a characterization of the optimal pricing\nmeasures. These algorithms are efficient and allow the computation of bounds in\nhigh-dimensional scenarios (e.g. when $d=60$). Moreover, these algorithms can\nbe used to detect arbitrage opportunities and identify the corresponding\narbitrage strategies. Numerical experiments using both synthetic and real\nmarket data showcase the efficiency of these algorithms, while they also allow\nto understand the reduction of model risk by including additional information,\nin the form of known derivative prices.\n"
    },
    {
        "paper_id": 2006.14307,
        "authors": "Francesca Biagini, Katharina Oberpriller",
        "title": "Reduced-form setting under model uncertainty with non-linear affine\n  processes",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend the reduced-form setting under model uncertainty\nintroduced in [5] to include intensities following an affine process under\nparameter uncertainty, as defined in [15]. This framework allows to introduce a\nlongevity bond under model uncertainty in a consistent way with the classical\ncase under one prior, and to compute its valuation numerically. Moreover, we\nare able to price a contingent claim with the sublinear conditional operator\nsuch that the extended market is still arbitrage-free in the sense of \"No\nArbitrage of the first kind\" as in [6].\n"
    },
    {
        "paper_id": 2006.14313,
        "authors": "Th\\'eophile Carniel and Jean-Michel Dalle",
        "title": "Towards Entrepreneurial Ecosystem Indicators : Speed and Acceleration",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest the use of indicators to analyze entrepreneurial ecosystems, in a\nway similar to ecological indicators: simple, measurable, and actionable\ncharacteristics, used to convey relevant information to stakeholders and\npolicymakers. We define 3 possible such indicators: Fundraising Speed,\nAcceleration and nth-year speed, all related to the ability of startups to\ndevelop more or less rapidly in a given ecosystem. Results based on these 3\nindicators for 6 prominent ecosystems (Berlin, Israel, London, New York, Paris,\nSilicon Valley) exhibit markedly different situations and trajectories.\nAltogether, they contribute to confirm that such indicators can help shed new\nand interesting light on entrepreneurial ecosystems, to the benefit of\npotentially more grounded policy decisions, and all the more so in otherwise\nblurred and somewhat cacophonic environments.\n"
    },
    {
        "paper_id": 2006.14402,
        "authors": "Sang Il Lee",
        "title": "Deeply Equal-Weighted Subset Portfolios",
        "comments": "arXiv admin note: text overlap with arXiv:2001.10278",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The high sensitivity of optimized portfolios to estimation errors has\nprevented their practical application. To mitigate this sensitivity, we propose\na new portfolio model called a Deeply Equal-Weighted Subset Portfolio (DEWSP).\nDEWSP is a subset of top-N ranked assets in an asset universe, the members of\nwhich are selected based on the predicted returns from deep learning algorithms\nand are equally weighted. Herein, we evaluate the performance of DEWSPs of\ndifferent sizes N in comparison with the performance of other types of\nportfolios such as optimized portfolios and historically equal-weighed subset\nportfolios (HEWSPs), which are subsets of top-N ranked assets based on the\nhistorical mean returns. We found the following advantages of DEWSPs: First,\nDEWSPs provides an improvement rate of 0.24% to 5.15% in terms of monthly\nSharpe ratio compared to the benchmark, HEWSPs. In addition, DEWSPs are built\nusing a purely data-driven approach rather than relying on the efforts of\nexperts. DEWSPs can also target the relative risk and return to the baseline of\nthe EWP of an asset universe by adjusting the size N. Finally, the DEWSP\nallocation mechanism is transparent and intuitive. These advantages make DEWSP\ncompetitive in practice.\n"
    },
    {
        "paper_id": 2006.14473,
        "authors": "S M Raju and Ali Mohammad Tarif",
        "title": "Real-Time Prediction of BITCOIN Price using Machine Learning Techniques\n  and Public Sentiment Analysis",
        "comments": "14 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bitcoin is the first digital decentralized cryptocurrency that has shown a\nsignificant increase in market capitalization in recent years. The objective of\nthis paper is to determine the predictable price direction of Bitcoin in USD by\nmachine learning techniques and sentiment analysis. Twitter and Reddit have\nattracted a great deal of attention from researchers to study public sentiment.\nWe have applied sentiment analysis and supervised machine learning principles\nto the extracted tweets from Twitter and Reddit posts, and we analyze the\ncorrelation between bitcoin price movements and sentiments in tweets. We\nexplored several algorithms of machine learning using supervised learning to\ndevelop a prediction model and provide informative analysis of future market\nprices. Due to the difficulty of evaluating the exact nature of a Time\nSeries(ARIMA) model, it is often very difficult to produce appropriate\nforecasts. Then we continue to implement Recurrent Neural Networks (RNN) with\nlong short-term memory cells (LSTM). Thus, we analyzed the time series model\nprediction of bitcoin prices with greater efficiency using long short-term\nmemory (LSTM) techniques and compared the predictability of bitcoin price and\nsentiment analysis of bitcoin tweets to the standard method (ARIMA). The RMSE\n(Root-mean-square error) of LSTM are 198.448 (single feature) and 197.515\n(multi-feature) whereas the ARIMA model RMSE is 209.263 which shows that LSTM\nwith multi feature shows the more accurate result.\n"
    },
    {
        "paper_id": 2006.14498,
        "authors": "Hans B\\\"uhler, Blanka Horvath, Terry Lyons, Imanol Perez Arribas, and\n  Ben Wood",
        "title": "A Data-driven Market Simulator for Small Data Environments",
        "comments": "27 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neural network based data-driven market simulation unveils a new and flexible\nway of modelling financial time series without imposing assumptions on the\nunderlying stochastic dynamics. Though in this sense generative market\nsimulation is model-free, the concrete modelling choices are nevertheless\ndecisive for the features of the simulated paths. We give a brief overview of\ncurrently used generative modelling approaches and performance evaluation\nmetrics for financial time series, and address some of the challenges to\nachieve good results in the latter. We also contrast some classical approaches\nof market simulation with simulation based on generative modelling and\nhighlight some advantages and pitfalls of the new approach. While most\ngenerative models tend to rely on large amounts of training data, we present\nhere a generative model that works reliably in environments where the amount of\navailable training data is notoriously small. Furthermore, we show how a rough\npaths perspective combined with a parsimonious Variational Autoencoder\nframework provides a powerful way for encoding and evaluating financial time\nseries in such environments where available training data is scarce. Finally,\nwe also propose a suitable performance evaluation metric for financial time\nseries and discuss some connections of our Market Generator to deep hedging.\n"
    },
    {
        "paper_id": 2006.14499,
        "authors": "Indrajit Banerjee, Atul Kumar, Rupam Bhattacharyya",
        "title": "Examining the Effect of COVID-19 on Foreign Exchange Rate and Stock\n  Market -- An Applied Insight into the Variable Effects of Lockdown on Indian\n  Economy",
        "comments": "8 Figures and Supplementary Document included - Total 72 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since March 25, 2020, India had been under a nation-wide lockdown announced\nas a response to the spread of SARS-CoV-2 and COVID-19 and has resorted to a\nprocess of 'unlocking' the lockdown over the past couple of months. This work\nattempts to examine the effect of novel coronavirus 2019 (COVID-19) and its\nresulting disease, the COVID-19, on the foreign exchange rates and stock market\nperformances of India using secondary data over a span of 112 days spanning\nbetween March 11 and June 30, 2020. The study explores whether the causal\nrelationships and directions among the growth rate of confirmed cases\n(GROWTHC), exchange rate (GEX) and SENSEX value (GSENSEX) are remaining the\nsame across different pre and post-lockdown phases, attempting to capture any\npotential changes over time via the vector autoregressive (VAR) models. A\npositive correlation is found between the growth rate of confirmed cases and\nthe growth rate of exchange rate, and a negative correlation between the growth\nrate of confirmed cases and the growth rate of SENSEX value. However, on\napplying a vector autoregressive (VAR) model, it is observed that an increase\nin the confirmed COVID-19 cases causes no significant change in the values of\nthe exchange rate and SENSEX index. The result varies if the analysis is split\nacross different time periods - before lockdown, the four phases of lockdown,\nand the first phase of unlock. Nuanced and sensible interpretations of the\nnumeric results indicate significant variability across time in terms of the\nrelation between the variables of interest. The detailed knowledge about the\nvarying patterns of dependence could potentially help the policy makers and\ninvestors of India in order to develop their strategies to cope up with the\nsituation.\n"
    },
    {
        "paper_id": 2006.1451,
        "authors": "Daniel J. Egger, Claudio Gambella, Jakub Marecek, Scott McFaddin,\n  Martin Mevissen, Rudy Raymond, Andrea Simonetto, Stefan Woerner, Elena\n  Yndurain",
        "title": "Quantum Computing for Finance: State of the Art and Future Prospects",
        "comments": "24 pages",
        "journal-ref": "IEEE Transactions on Quantum Engineering, vol. 1, pp. 1-24, 2020,\n  Art no. 3101724",
        "doi": "10.1109/TQE.2020.3030314",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This article outlines our point of view regarding the applicability,\nstate-of-the-art, and potential of quantum computing for problems in finance.\nWe provide an introduction to quantum computing as well as a survey on problem\nclasses in finance that are computationally challenging classically and for\nwhich quantum computing algorithms are promising. In the main part, we describe\nin detail quantum algorithms for specific applications arising in financial\nservices, such as those involving simulation, optimization, and machine\nlearning problems. In addition, we include demonstrations of quantum algorithms\non IBM Quantum back-ends and discuss the potential benefits of quantum\nalgorithms for problems in financial services. We conclude with a summary of\ntechnical challenges and future prospects.\n"
    },
    {
        "paper_id": 2006.14814,
        "authors": "Markus Hess",
        "title": "A pure-jump mean-reverting short rate model",
        "comments": "Published at https://doi.org/10.15559/20-VMSTA152 in the Modern\n  Stochastics: Theory and Applications (https://vmsta.org/) by VTeX\n  (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2020, Vol. 7, No. 2,\n  113-134",
        "doi": "10.15559/20-VMSTA152",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new multi-factor short rate model is presented which is bounded from below\nby a real-valued function of time. The mean-reverting short rate process is\nmodeled by a sum of pure-jump Ornstein--Uhlenbeck processes such that the\nrelated bond prices possess affine representations. Also the dynamics of the\nassociated instantaneous forward rate is provided and a condition is derived\nunder which the model can be market-consistently calibrated. The analytical\ntractability of this model is illustrated by the derivation of an explicit\nplain vanilla option price formula. With view on practical applications,\nsuitable probability distributions are proposed for the driving jump processes.\nThe paper is concluded by presenting a post-crisis extension of the proposed\nshort and forward rate model.\n"
    },
    {
        "paper_id": 2006.14833,
        "authors": "David R. Ba\\~nos, Marc Lagunas-Merino, Salvador Ortiz-Latorre",
        "title": "Variance and interest rate risk in unit-linked insurance policies",
        "comments": "21 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the risks derived from selling long term policies that any insurance\ncompany has, arises from interest rates. In this paper we consider a general\nclass of stochastic volatility models written in forward variance form. We also\ndeal with stochastic interest rates to obtain the risk-free price for\nunit-linked life insurance contracts, as well as providing a perfect hedging\nstrategy by completing the market. We conclude with a simulation experiment,\nwhere we price unit-linked policies using Norwegian mortality rates. In\naddition we compare prices for the classical Black-Scholes model against the\nHeston stochastic volatility model with a Vasicek interest rate model.\n"
    },
    {
        "paper_id": 2006.14842,
        "authors": "Jean-Bernard Chatelain (PJSE), Kirsten Ralf",
        "title": "The Welfare of Ramsey Optimal Policy Facing Auto-Regressive Shocks",
        "comments": null,
        "journal-ref": "Economics Bulletin, Economics Bulletin, 2020, 40 ((2)),\n  pp.1797-1803",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With non-controllable auto-regressive shocks, the welfare of Ramsey optimal\npolicy is the solution of a single Riccati equation of a linear quadratic\nregulator. The existing theory by Hansen and Sargent (2007) refers to an\nadditional Sylvester equation but miss another equation for computing the block\nmatrix weighting the square of non-controllable variables in the welfare\nfunction. There is no need to simulate impulse response functions over a long\nperiod, to compute period loss functions and to sum their discounted value over\nthis long period, as currently done so far. Welfare is computed for the case of\nthe new-Keynesian Phillips curve with an auto-regressive cost-push shock. JEL\nclassification numbers: C61, C62, C73, E47, E52, E61, E63.\n"
    },
    {
        "paper_id": 2006.14868,
        "authors": "Miguel Costa-Gomes and Georgios Gerasimou",
        "title": "Status Quo Bias and the Decoy Effect: A Comparative Analysis in Choice\n  under Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inertia and context-dependent choice effects are well-studied classes of\nbehavioural phenomena. While much is known about these effects in isolation,\nlittle is known about whether one of them \"dominates\" the other when both can\npotentially be present. Knowledge of any such dominance is relevant for\neffective choice architecture and descriptive modelling. We initiate this\nempirical investigation with a between-subjects lab experiment in which each\nsubject made a single decision over two or three money lotteries. Our\nexperiment was designed to test for dominance between *status quo bias* and the\n*decoy effect*. We find strong evidence for status quo bias and no evidence for\nthe decoy effect. We also find that status quo bias can be powerful enough so\nthat, at the aggregate level, a fraction of subjects switch from being\nrisk-averse to being risk-seeking. Survey evidence suggests that this is due to\nsubjects focusing on the maximum possible amount when the risky lottery is the\ndefault and on the highest probability of winning the biggest possible reward\nwhen there is no default. The observed reversal in risk attitudes is\nexplainable by a large class of Koszegi-Rabin (2006) reference-dependent\npreferences.\n"
    },
    {
        "paper_id": 2006.15008,
        "authors": "Sam L. Polk and Bruce M. Boghosian",
        "title": "The Nonuniversality of Wealth Distribution Tails Near Wealth\n  Condensation Criticality",
        "comments": "20 pages, 2 figures",
        "journal-ref": "SIAM Journal on Applied Mathematics 81, no. 4 (2021): 1717-1741",
        "doi": "10.1137/19M1306051",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we modify the affine wealth model of wealth distributions to\nexamine the effects of nonconstant redistribution on the very wealthy. Previous\nstudies of this model, restricted to flat redistribution schemes, have\ndemonstrated the presence of a phase transition to a partially wealth-condensed\nstate, or \"partial oligarchy,\" at the critical value of an order parameter.\nThese studies have also indicated the presence of an exponential tail in wealth\ndistribution precisely at criticality. Away from criticality, the tail was\nobserved to be Gaussian. In this work, we generalize the flat redistribution\nwithin the affine wealth model to allow for an essentially arbitrary\nredistribution policy. We show that the exponential tail observed near\ncriticality in prior work is, in fact, a special case of a much broader class\nof critical, slower-than-Gaussian decays that depend sensitively on the\ncorresponding asymptotic behavior of the progressive redistribution model used.\nWe thereby demonstrate that the functional form of the tail of the wealth\ndistribution in a near-critical society is not universal in nature but rather\nentirely determined by the specifics of public policy decisions. This is\nsignificant because most major economies today are observed to be\nnear-critical.\n"
    },
    {
        "paper_id": 2006.15012,
        "authors": "Ali Hirsa and Weilong Fu",
        "title": "An unsupervised deep learning approach in solving partial\n  integro-differential equations",
        "comments": "22 pages, 4 figures",
        "journal-ref": "Quantitative Finance, 2022",
        "doi": "10.1080/14697688.2022.2057870",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate solving partial integro-differential equations (PIDEs) using\nunsupervised deep learning in this paper. To price options, assuming underlying\nprocesses follow Levy processes, we require to solve PIDEs. In supervised deep\nlearning, pre-calculated labels are used to train neural networks to fit the\nsolution of the PIDE. In an unsupervised deep learning, neural networks are\nemployed as the solution, and the derivatives and the integrals in the PIDE are\ncalculated based on the neural network. By matching the PIDE and its boundary\nconditions, the neural network gives an accurate solution of the PIDE. Once\ntrained, it would be fast for calculating options values as well as option\nGreeks.\n"
    },
    {
        "paper_id": 2006.15054,
        "authors": "Michael C. Fu, Bingqing Li, Rongwen Wu, Tianqi Zhang",
        "title": "Option Pricing Under a Discrete-Time Markov Switching Stochastic\n  Volatility with Co-Jump Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider option pricing using a discrete-time Markov switching stochastic\nvolatility with co-jump model, which can model volatility clustering and\nvarying mean-reversion speeds of volatility. For pricing European options, we\ndevelop a computationally efficient method for obtaining the probability\ndistribution of average integrated variance (AIV), which is key to option\npricing under stochastic-volatility-type models. Building upon the efficiency\nof the European option pricing approach, we are able to price an American-style\noption, by converting its pricing into the pricing of a portfolio of European\noptions. Our work also provides constructive guidance for analyzing derivatives\nbased on variance, e.g., the variance swap. Numerical results indicate our\nmethods can be implemented very efficiently and accurately.\n"
    },
    {
        "paper_id": 2006.15158,
        "authors": "Tomoyuki Ichiba, Nicole Tianjiao Yang",
        "title": "Relative Arbitrage Opportunities with Interactions among $N$ Investors",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relative arbitrage portfolio outperforms a benchmark portfolio over a\ngiven time-horizon with probability one. With market price of risk processes\ndepending on the market portfolio and investors, this paper analyzes the\nmulti-agent optimization of relative arbitrage opportunities in the coupled\nsystem of market and wealth dynamics. We construct a well-posed market\ndynamical system of McKean-Vlasov type under an empirical measure of investors,\nwhere each investor seeks for relative arbitrage with respect to a benchmark\ndependent on market and all the agents. We show the conditions to guarantee\nrelative arbitrage opportunities among competitive investors through the\nFichera drift. Under mild conditions, we derive the optimal strategies for\ninvestors and the unique Nash equilibrium that depends on the smallest\nnonnegative solution of a Cauchy problem.\n"
    },
    {
        "paper_id": 2006.15183,
        "authors": "Francis X. Diebold",
        "title": "Real-Time Real Economic Activity: Entering and Exiting the Pandemic\n  Recession of 2020",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Entering and exiting the Pandemic Recession, I study the high-frequency\nreal-activity signals provided by a leading nowcast, the ADS Index of Business\nConditions produced and released in real time by the Federal Reserve Bank of\nPhiladelphia. I track the evolution of real-time vintage beliefs and compare\nthem to a later-vintage chronology. Real-time ADS plunges and then swings as\nits underlying economic indicators swing, but the ADS paths quickly converge to\nindicate a return to brisk positive growth by mid-May. I show, moreover, that\nthe daily real activity path was highly correlated with the daily COVID-19\ncases. Finally, I provide a comparative assessment of the real-time ADS signals\nprovided when exiting the Great Recession.\n"
    },
    {
        "paper_id": 2006.15214,
        "authors": "Zhongjun Wang, Mengye Sun, A. M. Elsawah",
        "title": "Improving MF-DFA model with applications in precious metals market",
        "comments": "23 pages, 17 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the aggravation of the global economic crisis and inflation, the\nprecious metals with safe-haven function have become more popular. An improved\nMF-DFA method is proposed to analyze price fluctuations of the precious metals\nmarket. Based on the widely used multifractal detrended fluctuation analysis\nmethod (MF-DFA), we compare these two methods and find that the Bi-OSW-MF-DFA\nmethod possesses better efficiency. This article analyzes the degree of\nmultifractality between spot gold market and spot silver market as well as\ntheir risks. From the numerical results and figures, it is found that two\nelements constitute the contributions in the formation of multifractality in\ntime series and the risk of the spot silver market is higher than that of the\nspot gold market. This attempt could lead to a better understanding of\ncomplicated precious metals market.\n"
    },
    {
        "paper_id": 2006.15312,
        "authors": "Sanjay K. Nawalkha, Xiaoyang Zhuo",
        "title": "A Theory of Equivalent Expectation Measures for Contingent Claim Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a dynamic change of measure approach for computing the\nanalytical solutions of expected future prices (and therefore, expected\nreturns) of contingent claims over a finite horizon. The new approach\nconstructs hybrid probability measures called the \"equivalent expectation\nmeasures\"(EEMs), which provide the physical expectation of the claim's future\nprice until before the horizon date, and serve as pricing measures on or after\nthe horizon date. The EEM theory can be used for empirical investigations of\nboth the cross-section and the term structure of returns of contingent claims,\nsuch as Treasury bonds, corporate bonds, and financial derivatives.\n"
    },
    {
        "paper_id": 2006.15384,
        "authors": "Chendi Ni, Yuying Li, Peter Forsyth, Ray Carroll",
        "title": "Optimal Asset Allocation For Outperforming A Stochastic Benchmark Target",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a data-driven Neural Network (NN) optimization framework to\ndetermine the optimal multi-period dynamic asset allocation strategy for\noutperforming a general stochastic target. We formulate the problem as an\noptimal stochastic control with an asymmetric, distribution shaping, objective\nfunction. The proposed framework is illustrated with the asset allocation\nproblem in the accumulation phase of a defined contribution pension plan, with\nthe goal of achieving a higher terminal wealth than a stochastic benchmark. We\ndemonstrate that the data-driven approach is capable of learning an adaptive\nasset allocation strategy directly from historical market returns, without\nassuming any parametric model of the financial market dynamics. Following the\noptimal adaptive strategy, investors can make allocation decisions simply\ndepending on the current state of the portfolio. The optimal adaptive strategy\noutperforms the benchmark constant proportion strategy, achieving a higher\nterminal wealth with a 90% probability, a 46% higher median terminal wealth,\nand a significantly more right-skewed terminal wealth distribution. We further\ndemonstrate the robustness of the optimal adaptive strategy by testing the\nperformance of the strategy on bootstrap resampled market data, which has\ndifferent distributions compared to the training data.\n"
    },
    {
        "paper_id": 2006.15431,
        "authors": "Archil Gulisashvili",
        "title": "Large deviation principles for stochastic volatility models with\n  reflection and three faces of the Stein and Stein model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce stochastic volatility models, in which the volatility is\ndescribed by a time-dependent nonnegative function of a reflecting diffusion.\nThe idea to use reflecting diffusions as building blocks of the volatility came\ninto being because of a certain volatility misspecification in the classical\nStein and Stein model. A version of this model that uses the reflecting\nOrnstein-Uhlenbeck process as the volatility process is a special example of a\nstochastic volatility model with reflection. The main results obtained in the\npresent paper are sample path and small-noise large deviation principles for\nthe log-price process in a stochastic volatility model with reflection under\nrather mild restrictions. We use these results to study the asymptotic behavior\nof binary barrier options and call prices in the small-noise regime.\n"
    },
    {
        "paper_id": 2006.15483,
        "authors": "Wenlong Hu",
        "title": "Risk management of guaranteed minimum maturity benefits under stochastic\n  mortality and regime-switching by Fourier space time-stepping framework",
        "comments": "An algorithm mistake in hedging strategies section",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we adopted a net liability model which assesses both market\nrisk on the liability side and revenue risk on the asset side for a Guaranteed\nMinimum Maturity Benefit (GMMB) embedded in variable annuity (VA) contracts.\nNumeric solutions for net liabilities, fair rate of fees and Greeks of GMMB are\nobtained by a more accurate and fast Fourier Space time-stepping (FST)\nalgorithm. Monte Carlo results are provided for comparative purpose. The\nunhedged and three statically hedged portfolios are introduced, and their\nperformances are assessed by comparing the short term and long term portfolio's\nvolatility. Recently, we noticed FST algorithm can only be used to hedge the\ngross liability of GMMB and it leads to an incorrect result when applying FST\nalgorithm to the net liability model. We have modified the method we used in\nthe hedging part and obtained the reliable result now. They will be reported in\nnear future.\n"
    },
    {
        "paper_id": 2006.15491,
        "authors": "Wei Wang, Huifu Xu and Tiejun Ma",
        "title": "Quantitative Statistical Robustness for Tail-Dependent Law Invariant\n  Risk Measures",
        "comments": "28 pages; 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When estimating the risk of a financial position with empirical data or Monte\nCarlo simulations via a tail-dependent law invariant risk measure such as the\nConditional Value-at-Risk (CVaR), it is important to ensure the robustness of\nthe statistical estimator particularly when the data contain noise. Kr\u007fatscher\net al. [1] propose a new framework to examine the qualitative robustness of\nestimators for tail-dependent law invariant risk measures on Orlicz spaces,\nwhich is a step further from earlier work for studying the robustness of risk\nmeasurement procedures by Cont et al. [2]. In this paper, we follow the stream\nof research to propose a quantitative approach for verifying the statistical\nrobustness of tail-dependent law invariant risk measures. A distinct feature of\nour approach is that we use the Fortet-Mourier metric to quantify the variation\nof the true underlying probability measure in the analysis of the discrepancy\nbetween the laws of the plug-in estimators of law invariant risk measure based\non the true data and perturbed data, which enables us to derive an explicit\nerror bound for the discrepancy when the risk functional is Lipschitz\ncontinuous with respect to a class of admissible laws. Moreover, the newly\nintroduced notion of Lipschitz continuity allows us to examine the degree of\nrobustness for tail-dependent risk measures. Finally, we apply our quantitative\napproach to some well-known risk measures to illustrate our theory.\n"
    },
    {
        "paper_id": 2006.15563,
        "authors": "Claudio Fontana and Wolfgang J. Runggaldier",
        "title": "Arbitrage concepts under trading restrictions in discrete-time financial\n  markets",
        "comments": "29 pages, 1 figure",
        "journal-ref": "Journal of Mathematical Economics, 2021, 92: 66-80",
        "doi": "10.1016/j.jmateco.2020.10.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a discrete-time setting, we study arbitrage concepts in the presence of\nconvex trading constraints. We show that solvability of portfolio optimization\nproblems is equivalent to absence of arbitrage of the first kind, a condition\nweaker than classical absence of arbitrage opportunities. We center our\nanalysis on this characterization of market viability and derive versions of\nthe fundamental theorems of asset pricing based on portfolio optimization\narguments. By considering specifically a discrete-time setup, we simplify\nexisting results and proofs that rely on semimartingale theory, thus allowing\nfor a clear understanding of the foundational economic concepts involved. We\nexemplify these concepts, as well as some unexpected situations, in the context\nof one-period factor models with arbitrage opportunities under borrowing\nconstraints.\n"
    },
    {
        "paper_id": 2006.15823,
        "authors": "Ralph Rudd, Thomas A. McWalter, Joerg Kienitz and Eckhard Platen",
        "title": "Robust Product Markovian Quantization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recursive marginal quantization (RMQ) allows the construction of optimal\ndiscrete grids for approximating solutions to stochastic differential equations\nin d-dimensions. Product Markovian quantization (PMQ) reduces this problem to d\none-dimensional quantization problems by recursively constructing product\nquantizers, as opposed to a truly optimal quantizer. However, the standard\nNewton-Raphson method used in the PMQ algorithm suffers from numerical\ninstabilities, inhibiting widespread adoption, especially for use in\ncalibration. By directly specifying the random variable to be quantized at each\ntime step, we show that PMQ, and RMQ in one dimension, can be expressed as\nstandard vector quantization. This reformulation allows the application of the\naccelerated Lloyd's algorithm in an adaptive and robust procedure. Furthermore,\nin the case of stochastic volatility models, we extend the PMQ algorithm by\nusing higher-order updates for the volatility or variance process. We\nillustrate the technique for European options, using the Heston model, and more\nexotic products, using the SABR model.\n"
    },
    {
        "paper_id": 2006.15988,
        "authors": "Chika O. Okafor",
        "title": "Social Networks as a Mechanism for Discrimination",
        "comments": "4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study labor markets in which firms hire via referrals. I develop an\nemployment model showing that--despite initial equality in ability, employment,\nwages, and network structure--minorities receive fewer jobs through referral\nand lower expected wages, simply because their social group is smaller. This\ndisparity, termed \"social network discrimination,\" falls outside the dominant\neconomics discrimination models--taste-based and statistical. Social network\ndiscrimination can be mitigated by minorities having more social ties or a\n\"stronger-knit\" network. I calibrate the model using a\nnationally-representative U.S. sample and estimate the lower-bound welfare gap\ncaused by social network discrimination at over four percent, disadvantaging\nblack workers.\n"
    },
    {
        "paper_id": 2006.16099,
        "authors": "Emma Duchini, Stefania Simion, Arthur Turrell, Jack Blundell",
        "title": "Pay Transparency and Gender Equality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since 2018 UK firms with at least 250 employees have been mandated to\npublicly disclose gender equality indicators. Exploiting variations in this\nmandate across firm size and time we show that pay transparency closes 18\npercent of the gender pay gap by reducing men's wage growth. The public\navailability of the equality indicators seems to influence employers' response\nas worse performing firms and industries more exposed to public scrutiny reduce\ntheir gender pay gap the most. Employers are also 9 percent more likely to post\nwages in job vacancies, potentially in an effort to improve gender equality at\nentry level.\n"
    },
    {
        "paper_id": 2006.16383,
        "authors": "E. Ramos-P\\'erez, P.J. Alonso-Gonz\\'alez, J.J. N\\'u\\~nez-Vel\\'azquez",
        "title": "Forecasting volatility with a stacked model based on a hybridized\n  Artificial Neural Network",
        "comments": "22 pages, 7 tables, 1 Figure. Published in Expert Systems with\n  Applications, Volume 129, 1 September 2019, Pages 1-9",
        "journal-ref": "Expert Systems with Applications, Volume 129, 1 September 2019,\n  Pages 1-9",
        "doi": "10.1016/j.eswa.2019.03.046",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An appropriate calibration and forecasting of volatility and market risk are\nsome of the main challenges faced by companies that have to manage the\nuncertainty inherent to their investments or funding operations such as banks,\npension funds or insurance companies. This has become even more evident after\nthe 2007-2008 Financial Crisis, when the forecasting models assessing the\nmarket risk and volatility failed. Since then, a significant number of\ntheoretical developments and methodologies have appeared to improve the\naccuracy of the volatility forecasts and market risk assessments. Following\nthis line of thinking, this paper introduces a model based on using a set of\nMachine Learning techniques, such as Gradient Descent Boosting, Random Forest,\nSupport Vector Machine and Artificial Neural Network, where those algorithms\nare stacked to predict S&P500 volatility. The results suggest that our\nconstruction outperforms other habitual models on the ability to forecast the\nlevel of volatility, leading to a more accurate assessment of the market risk.\n"
    },
    {
        "paper_id": 2006.16407,
        "authors": "Fathi Abid, Wafa Abdelmalek and Sana Ben Hamida",
        "title": "Dynamic Hedging using Generated Genetic Programming Implied Volatility\n  Models",
        "comments": "32 pages,13 figures, Intech Open Science",
        "journal-ref": "INTECH 2012 - Genetic Programming - New Approaches and Successful\n  Applications",
        "doi": "10.5772/48148",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this paper is to improve the accuracy of dynamic hedging using\nimplied volatilities generated by genetic programming. Using real data from\nS&P500 index options, the genetic programming's ability to forecast Black and\nScholes implied volatility is compared between static and dynamic\ntraining-subset selection methods. The performance of the best generated GP\nimplied volatilities is tested in dynamic hedging and compared with\nBlack-Scholes model. Based on MSE total, the dynamic training of GP yields\nbetter results than those obtained from static training with fixed samples.\nAccording to hedging errors, the GP model is more accurate almost in all\nhedging strategies than the BS model, particularly for in-the-money call\noptions and at-the-money put options.\n"
    },
    {
        "paper_id": 2006.16703,
        "authors": "Paul McCloud",
        "title": "Expectation and Price in Incomplete Markets",
        "comments": "31 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk-neutral pricing dictates that the discounted derivative price is a\nmartingale in a measure equivalent to the economic measure. The residual\nambiguity for incomplete markets is here resolved by minimising the entropy of\nthe price measure from the economic measure, subject to mark-to-market\nconstraints, following arguments based on the optimisation of portfolio risk.\nThe approach accounts for market and funding convexities and incorporates\navailable price information, interpolating between methodologies based on\nexpectation and replication.\n"
    },
    {
        "paper_id": 2006.16911,
        "authors": "Kwadwo Osei Bonsu, Jie Song",
        "title": "Turbulence on the Global Economy influenced by Artificial Intelligence\n  and Foreign Policy Inefficiencies",
        "comments": "This is the pre-print version",
        "journal-ref": null,
        "doi": "10.47305/JLIA2020113ob",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is said that Data and Information are the new oil. One, who handles the\ndata, handles the emerging future of the global economy. Complex algorithms and\nintelligence-based filter programs are utilized to manage, store, handle and\nmaneuver vast amounts of data for the fulfillment of specific purposes. This\npaper seeks to find the bridge between artificial intelligence and its impact\non the international policy implementation in the light of geopolitical\ninfluence, global economy and the future of labor markets. We hypothesize that\nthe distortion in the labor markets caused by artificial intelligence can be\nmitigated by a collaborative international foreign policy on the deployment of\nAI in the industrial circles. We, in this paper, then proceed to propose a\ndisposition for the essentials of AI-based foreign policy and implementation,\nwhile asking questions such as 'could AI become the real Invisible Hand\ndiscussed by economists?'.\n"
    },
    {
        "paper_id": 2007.00017,
        "authors": "Samuel Mugel, Carlos Kuchkovsky, Escolastico Sanchez, Samuel\n  Fernandez-Lorenzo, Jorge Luis-Hita, Enrique Lizaso, Roman Orus",
        "title": "Dynamic Portfolio Optimization with Real Datasets Using Quantum\n  Processors and Quantum-Inspired Tensor Networks",
        "comments": "13 pages, 5 figures, 5 tables, revised version, to appear in Physical\n  Review Research",
        "journal-ref": "Phys. Rev. Research 4, 013006 (2022)",
        "doi": "10.1103/PhysRevResearch.4.013006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we tackle the problem of dynamic portfolio optimization, i.e.,\ndetermining the optimal trading trajectory for an investment portfolio of\nassets over a period of time, taking into account transaction costs and other\npossible constraints. This problem is central to quantitative finance. After a\ndetailed introduction to the problem, we implement a number of quantum and\nquantum-inspired algorithms on different hardware platforms to solve its\ndiscrete formulation using real data from daily prices over 8 years of 52\nassets, and do a detailed comparison of the obtained Sharpe ratios, profits and\ncomputing times. In particular, we implement classical solvers (Gekko,\nexhaustive), D-Wave Hybrid quantum annealing, two different approaches based on\nVariational Quantum Eigensolvers on IBM-Q (one of them brand-new and tailored\nto the problem), and for the first time in this context also a quantum-inspired\noptimizer based on Tensor Networks. In order to fit the data into each specific\nhardware platform, we also consider doing a preprocessing based on clustering\nof assets. From our comparison, we conclude that D-Wave Hybrid and Tensor\nNetworks are able to handle the largest systems, where we do calculations up to\n1272 fully-connected qubits for demonstrative purposes. Finally, we also\ndiscuss how to mathematically implement other possible real-life constraints,\nas well as several ideas to further improve the performance of the studied\nmethods.\n"
    },
    {
        "paper_id": 2007.00254,
        "authors": "Shankhyajyoti De, Arabin Kumar Dey, and Deepak Gauda",
        "title": "Construction of confidence interval for a univariate stock price signal\n  predicted through Long Short Term Memory Network",
        "comments": "14 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we show an innovative way to construct bootstrap confidence\ninterval of a signal estimated based on a univariate LSTM model. We take three\ndifferent types of bootstrap methods for dependent set up. We prescribe some\nuseful suggestions to select the optimal block length while performing the\nbootstrapping of the sample. We also propose a benchmark to compare the\nconfidence interval measured through different bootstrap strategies. We\nillustrate the experimental results through some stock price data set.\n"
    },
    {
        "paper_id": 2007.00449,
        "authors": "Mostapha Kalami Heris and Shahryar Rahnamayan",
        "title": "Multi-objective Optimal Control of Dynamic Integrated Model of Climate\n  and Economy: Evolution in Action",
        "comments": "8 pages, 6 figures, conference paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One of the widely used models for studying economics of climate change is the\nDynamic Integrated model of Climate and Economy (DICE), which has been\ndeveloped by Professor William Nordhaus, one of the laureates of the 2018 Nobel\nMemorial Prize in Economic Sciences. Originally a single-objective optimal\ncontrol problem has been defined on DICE dynamics, which is aimed to maximize\nthe social welfare. In this paper, a bi-objective optimal control problem\ndefined on DICE model, objectives of which are maximizing social welfare and\nminimizing the temperature deviation of atmosphere. This multi-objective\noptimal control problem solved using Non-Dominated Sorting Genetic Algorithm II\n(NSGA-II) also it is compared to previous works on single-objective version of\nthe problem. The resulting Pareto front rediscovers the previous results and\ngeneralizes to a wide range of non-dominant solutions to minimize the global\ntemperature deviation while optimizing the economic welfare. The previously\nused single-objective approach is unable to create such a variety of\npossibilities, hence, its offered solution is limited in vision and reachable\nperformance. Beside this, resulting Pareto-optimal set reveals the fact that\ntemperature deviation cannot go below a certain lower limit, unless we have\nsignificant technology advancement or positive change in global conditions.\n"
    },
    {
        "paper_id": 2007.00486,
        "authors": "Yu Hu, David Soler Soneira, Mar\\'ia Jes\\'us S\\'anchez",
        "title": "Barriers to grid-connected battery systems: Evidence from the Spanish\n  electricity market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.est.2021.102262",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Electrical energy storage is considered essential for the future energy\nsystems. Among all the energy storage technologies, battery systems may provide\nflexibility to the power grid in a more distributed and decentralized way. In\ncountries with deregulated electricity markets, grid-connected battery systems\nshould be operated under the specific market design of the country. In this\nwork, using the Spanish electricity market as an example, the barriers to\ngrid-connected battery systems are investigated using utilization analysis. The\nconcept of \"potentially profitable utilization time\" is proposed and introduced\nto identify and evaluate future potential grid applications for battery\nsystems. The numerical and empirical analysis suggests that the high cycle cost\nfor battery systems is still the main barrier for grid-connected battery\nsystems. In Spain, for energy arbitrage within the day-ahead market, it is\nrequired that the battery wear cost decreases to 15 Euro/MWh to make the\npotentially profitable utilization rate higher than 20%. Nevertheless, the\npotentially profitable utilization of batteries is much higher in the\napplications when higher flexibility is demanded. The minimum required battery\nwear cost corresponding to 20% potentially profitable utilization time\nincreases to 35 Euro/MWh for energy arbitrage within the day-ahead market and\nancillary services, and 50 Euro/MWh for upward secondary reserve. The results\nof this study contribute to the awareness of battery storage technology and its\nflexibility in grid applications. The findings also have significant\nimplications for policy makers and market operators interested in promoting\ngrid-connected battery storage under a deregulated power market.\n"
    },
    {
        "paper_id": 2007.00705,
        "authors": "Grzegorz Krzy\\.zanowski, Andr\\'es Sosa",
        "title": "Performance analysis of Zero Black-Derman-Toy interest rate model in\n  catastrophic events: COVID-19 case study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we continue the research of our recent interest rate tree model\ncalled Zero Black-Derman-Toy (ZBDT) model, which includes the possibility of a\njump at each step to a practically zero interest rate. This approach allows to\nbetter match to risk of financial slowdown caused by catastrophic events. We\npresent how to valuate a wide range of financial derivatives for such a model.\nThe classical Black-Derman-Toy (BDT) model and novel ZBDT model are described\nand analogies in their calibration methodology are established. Finally two\ncases of applications of the novel ZBDT model are introduced. The first of them\nis the hypothetical case of an S-shape term structure and decreasing volatility\nof yields. The second case is an application of the ZBDT model in the structure\nof United States sovereign bonds in the current $2020$ economic slowdown caused\nby the Coronavirus pandemic. The objective of this study is to understand the\ndifferences presented by the valuation in both models for different\nderivatives.\n"
    },
    {
        "paper_id": 2007.00933,
        "authors": "Magnus Lundgren, Mark Klamberg, Karin Sundstr\\\"om, and Julia Dahlqvist",
        "title": "Emergency Powers in Response to COVID-19: Policy diffusion, Democracy,\n  and Preparedness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine COVID-19-related states of emergency (SOEs) using data on 180\ncountries in the period January 1 through June 12, 2020. The results suggest\nthat states' declaration of SOEs is driven by both external and internal\nfactors. A permissive regional environment, characterized by many and\nsimultaneously declared SOEs, may have diminished reputational and political\ncosts, making employment of emergency powers more palatable for a wider range\nof governments. At the same time, internal characteristics, specifically\ndemocratic institutions and pandemic preparedness, shaped governments'\ndecisions. Weak democracies with poor pandemic preparedness were considerably\nmore likely to opt for SOEs than dictatorships and robust democracies with\nhigher preparedness. We find no significant association between pandemic\nimpact, measured as national COVID-19-related deaths, and SOEs, suggesting that\nmany states adopted SOEs proactively before the disease spread locally.\n"
    },
    {
        "paper_id": 2007.01194,
        "authors": "Qingyin Ge, Yunuo Ma, Yuezhi Liao, Rongyu Li, Tianle Zhu",
        "title": "Risk Management and Return Prediction",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  With the good development in the financial industry, the market starts to\ncatch people's eyes, not only by the diversified investing choices ranging from\nbonds and stocks to futures and options but also by the general \"high-risk,\nhigh-reward\" mindset prompting people to put money in the financial market.\nPeople are interested in reducing risk at a given level of return since there\nis no way of having both high returns and low risk. Many researchers have been\nstudying this issue, and the most pioneering one is Harry Markowitz's Modern\nPortfolio Theory developed in 1952, which is the cornerstone of investment\nportfolio management and aims at \"maximum the return at the given risk\". In\ncontrast to that, fifty years later, E. Robert Fernholz's Stochastic Portfolio\nTheory, as opposed to the normative assumption served as the basis of earlier\nmodern portfolio theory, is consistent with the observable characteristics of\nactual portfolios and markets. In this paper, after introducing some basic\ntheories of Markowitz's MPT and Fernholz's SPT, then we step across to the\napplication side, trying to figure out under four basic models based on\nMarkowitz Efficient Frontier, including Markowitz Model, Constant Correlation\nModel, Single Index Model, and Multi-Factor Model, which portfolios will be\nselected and how do these portfolios perform in the real world. Here we also\ninvolve universal Portfolio Algorithmby Thomas M. Cover to select portfolios as\na comparison. Besides, each portfolio value at Risk, Expected Shortfall, and\ncorresponding bootstrap confidence interval for risk management will be\nevaluated. Finally, by utilizing factor analysis and time series models, we\ncould predict the future performance of our four models.\n"
    },
    {
        "paper_id": 2007.01414,
        "authors": "Marlon Moresco, Marcelo Righi and Eduardo Horta",
        "title": "Minkowski gauges and deviation measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to derive deviation measures through the Minkowski gauge of a\ngiven set of acceptable positions. We show that, given a suitable acceptance\nset, any positive homogeneous deviation measure can be accommodated in our\nframework. In doing so, we provide a new interpretation for such measures,\nnamely, that they quantify how much one must shrink or deleverage a position\nfor it to become acceptable. In particular, the Minkowski Deviation of a set\nwhich is convex, stable under scalar addition, and radially bounded at\nnon-constants, is a generalized deviation measure. Furthermore, we explore the\nrelations existing between mathematical and financial properties attributable\nto an acceptance set, and the corresponding properties of the induced measure.\nHence, we fill the gap that is the lack of an acceptance set for deviation\nmeasures. Dual characterizations in terms of polar sets and support functionals\nare provided.\n"
    },
    {
        "paper_id": 2007.01426,
        "authors": "Aili Zhang, Ping Chen, Shuanming Li, Wenyuan Wang",
        "title": "Risk Modelling on Liquidations with L\\'{e}vy Processes",
        "comments": "4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been decades since the academic world of ruin theory defined the\ninsolvency of an insurance company as the time when its surplus falls below\nzero. This simplification, however, needs careful adaptions to imitate the\nreal-world liquidation process. Inspired by Broadie et al. (2007) and Li et al.\n(2020), this paper uses a three-barrier model to describe the financial stress\ntowards bankruptcy of an insurance company. The financial status of the insurer\nis divided into solvent, insolvent and liquidated three states, where the\ninsurer's surplus process at the state of solvent and insolvent is modelled by\ntwo spectrally negative L\\'{e}vy processes, which have been taken as good\ncandidates to model insurance risks. We provide a rigorous definition of the\ntime of liquidation ruin in this three-barrier model. By adopting the\ntechniques of excursions in the fluctuation theory, we study the joint\ndistribution of the time of liquidation, the surplus at liquidation and the\nhistorical high of the surplus until liquidation, which generalizes the known\nresults on the classical expected discounted penalty function in Gerber and\nShiu (1998). The results have semi-explicit expressions in terms of the scale\nfunctions and the L\\'{e}vy triplets associated with the two underlying L\\'{e}vy\nprocesses. The special case when the two underlying L\\'{e}vy processes coincide\nwith each other is also studied, where our results are expressed compactly via\nonly the scale functions. The corresponding results have good consistency with\nthe existing literatures on Parisian ruin with (or without) a lower barrier in\nLandriault et al. (2014), Baurdoux et al. (2016) and Frostig and Keren-Pinhasik\n(2019). Besides, numerical examples are provided to illustrate the underlying\nfeatures of liquidation ruin.\n"
    },
    {
        "paper_id": 2007.0143,
        "authors": "Jeffrey Cohen, Alex Khan, Clark Alexander",
        "title": "Portfolio Optimization of 40 Stocks Using the DWave Quantum Annealer",
        "comments": "15 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the use of quantum computers for building a portfolio out of a\nuniverse of U.S. listed, liquid equities that contains an optimal set of\nstocks. Starting from historical market data, we look at various problem\nformulations on the D-Wave Systems Inc. D-Wave 2000Q(TM) System (hereafter\ncalled DWave) to find the optimal risk vs return portfolio; an optimized\nportfolio based on the Markowitz formulation and the Sharpe ratio, a simplified\nChicago Quantum Ratio (CQR), then a new Chicago Quantum Net Score (CQNS). We\napproach this first classically, then by our new method on DWave. Our results\nshow that practitioners can use a DWave to select attractive portfolios out of\n40 U.S. liquid equities.\n"
    },
    {
        "paper_id": 2007.01448,
        "authors": "Runjing Lu and Yanying Sheng",
        "title": "From Fear to Hate: How the Covid-19 Pandemic Sparks Racial Animus in the\n  United States",
        "comments": "34 pages, 12 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We estimate the effect of the Coronavirus (Covid-19) pandemic on racial\nanimus, as measured by Google searches and Twitter posts including a commonly\nused anti-Asian racial slur. Our empirical strategy exploits the plausibly\nexogenous variation in the timing of the first Covid-19 diagnosis across\nregions in the United States. We find that the first local diagnosis leads to\nan immediate increase in racist Google searches and Twitter posts, with the\nlatter mainly coming from existing Twitter users posting the slur for the first\ntime. This increase could indicate a rise in future hate crimes, as we document\na strong correlation between the use of the slur and anti-Asian hate crimes\nusing historic data. Moreover, we find that the rise in the animosity is\ndirected at Asians rather than other minority groups and is stronger on days\nwhen the connection between the disease and Asians is more salient, as proxied\nby President Trump's tweets mentioning China and Covid-19 at the same time. In\ncontrast, the negative economic impact of the pandemic plays little role in the\ninitial increase in racial animus. Our results suggest that de-emphasizing the\nconnection between the disease and a particular racial group can be effective\nin curbing current and future racial animus.\n"
    },
    {
        "paper_id": 2007.01467,
        "authors": "Kazuya Kaneko, Koichi Miyamoto, Naoyuki Takeda, Kazuyoshi Yoshino",
        "title": "Quantum Pricing with a Smile: Implementation of Local Volatility Model\n  on Quantum Computer",
        "comments": "29 pages, 13 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Applications of the quantum algorithm for Monte Carlo simulation to pricing\nof financial derivatives have been discussed in previous papers. However, up to\nnow, the pricing model discussed in such papers is Black-Scholes model, which\nis important but simple. Therefore, it is motivating to consider how to\nimplement more complex models used in practice in financial institutions. In\nthis paper, we then consider the local volatility (LV) model, in which the\nvolatility of the underlying asset price depends on the price and time. We\npresent two types of implementation. One is the register-per-RN way, which is\nadopted in most of previous papers. In this way, each of random numbers (RNs)\nrequired to generate a path of the asset price is generated on a separated\nregister, so the required qubit number increases in proportion to the number of\nRNs. The other is the PRN-on-a-register way, which is proposed in the author's\nprevious work. In this way, a sequence of pseudo-random numbers (PRNs)\ngenerated on a register is used to generate paths of the asset price, so the\nrequired qubit number is reduced with a trade-off against circuit depth. We\npresent circuit diagrams for these two implementations in detail and estimate\nrequired resources: qubit number and T-count.\n"
    },
    {
        "paper_id": 2007.01511,
        "authors": "Hyong Chol O, Tae Song Kim",
        "title": "Analysis on the Pricing model for a Discrete Coupon Bond with Early\n  redemption provision by the Structural Approach",
        "comments": "30 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, using the structural approach is derived a mathematical model\nof the discrete coupon bond with the provision that allow the holder to demand\nearly redemption at any coupon dates prior to the maturity and based on this\nmodel is provided some analysis including min-max and gradient estimates of the\nbond price. Using these estimates the existence and uniqueness of the default\nboundaries and some relationships between the design parameters of the discrete\ncoupon bond with early redemption provision are described. Then under some\nassumptions the existence and uniqueness of the early redemption boundaries is\nproved and the analytic formula of the bond price is provided using higher\nbinary options. Finally for our bond is provided the analysis on the duration\nand credit spread, which are used widely in financial reality. Our works\nprovide a design guide of the discrete coupon bond with the early redemption\nprovision\n"
    },
    {
        "paper_id": 2007.01623,
        "authors": "Loris Cannelli, Giuseppe Nuti, Marzio Sala, Oleg Szehr",
        "title": "Hedging using reinforcement learning: Contextual $k$-Armed Bandit versus\n  $Q$-learning",
        "comments": "30 pages, 11 figures",
        "journal-ref": "The Journal of Finance and Data Science Volume 9, November 2023,\n  100101",
        "doi": "10.1016/j.jfds.2023.100101",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The construction of replication strategies for contingent claims in the\npresence of risk and market friction is a key problem of financial engineering.\nIn real markets, continuous replication, such as in the model of Black, Scholes\nand Merton (BSM), is not only unrealistic but it is also undesirable due to\nhigh transaction costs. A variety of methods have been proposed to balance\nbetween effective replication and losses in the incomplete market setting. With\nthe rise of Artificial Intelligence (AI), AI-based hedgers have attracted\nconsiderable interest, where particular attention was given to Recurrent Neural\nNetwork systems and variations of the $Q$-learning algorithm. From a practical\npoint of view, sufficient samples for training such an AI can only be obtained\nfrom a simulator of the market environment. Yet if an agent was trained solely\non simulated data, the run-time performance will primarily reflect the accuracy\nof the simulation, which leads to the classical problem of model choice and\ncalibration. In this article, the hedging problem is viewed as an instance of a\nrisk-averse contextual $k$-armed bandit problem, which is motivated by the\nsimplicity and sample-efficiency of the architecture. This allows for realistic\nonline model updates from real-world data. We find that the $k$-armed bandit\nmodel naturally fits to the Profit and Loss formulation of hedging, providing\nfor a more accurate and sample efficient approach than $Q$-learning and\nreducing to the Black-Scholes model in the absence of transaction costs and\nrisks.\n"
    },
    {
        "paper_id": 2007.01672,
        "authors": "Sotirios Sabanis, Ying Zhang",
        "title": "A fully data-driven approach to minimizing CVaR for portfolio of assets\n  via SGLD with discontinuous updating",
        "comments": "arXiv admin note: text overlap with arXiv:1910.02008",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new approach in stochastic optimization via the use of stochastic gradient\nLangevin dynamics (SGLD) algorithms, which is a variant of stochastic gradient\ndecent (SGD) methods, allows us to efficiently approximate global minimizers of\npossibly complicated, high-dimensional landscapes. With this in mind, we extend\nhere the non-asymptotic analysis of SGLD to the case of discontinuous\nstochastic gradients. We are thus able to provide theoretical guarantees for\nthe algorithm's convergence in (standard) Wasserstein distances for both convex\nand non-convex objective functions. We also provide explicit upper estimates of\nthe expected excess risk associated with the approximation of global minimizers\nof these objective functions. All these findings allow us to devise and present\na fully data-driven approach for the optimal allocation of weights for the\nminimization of CVaR of portfolio of assets with complete theoretical\nguarantees for its performance. Numerical results illustrate our main findings.\n"
    },
    {
        "paper_id": 2007.02076,
        "authors": "Zbigniew Palmowski and Tomasz Serafin",
        "title": "Note on simulation pricing of $\\pi$-options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we adapt a Monte Carlo algorithm introduced by Broadie and\nGlasserman (1997) to price a $\\pi$-option. This method is based on the\nsimulated price tree that comes from discretization and replication of possible\ntrajectories of the underlying asset's price. As a result this algorithm\nproduces the lower and the upper bounds that converge to the true price with\nthe increasing depth of the tree. Under specific parametrization, this\n$\\pi$-option is related to relative maximum drawdown and can be used in the\nreal-market environment to protect a portfolio against volatile and unexpected\nprice drops. We also provide some numerical analysis.\n"
    },
    {
        "paper_id": 2007.02109,
        "authors": "Jiachen Ye, Peng Ji, Marc Barthelemy",
        "title": "Scenarios for a post-COVID-19 world airline network",
        "comments": "12 pages, 7 main figures (+5 figures in appendix)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The airline industry was severely hit by the COVID-19 crisis with an average\ndemand decrease of about $64\\%$ (IATA, April 2020) which triggered already\nseveral bankruptcies of airline companies all over the world. While the\nrobustness of the world airline network (WAN) was mostly studied as an\nhomogeneous network, we introduce a new tool for analyzing the impact of a\ncompany failure: the `airline company network' where two airlines are connected\nif they share at least one route segment. Using this tool, we observe that the\nfailure of companies well connected with others has the largest impact on the\nconnectivity of the WAN. We then explore how the global demand reduction\naffects airlines differently, and provide an analysis of different scenarios if\nits stays low and does not come back to its pre-crisis level. Using traffic\ndata from the Official Aviation Guide (OAG) and simple assumptions about\ncustomer's airline choice strategies, we find that the local effective demand\ncan be much lower than the average one, especially for companies that are not\nmonopolistic and share their segments with larger companies. Even if the\naverage demand comes back to $60\\%$ of the total capacity, we find that between\n$46\\%$ and $59\\%$ of the companies could experience a reduction of more than\n$50\\%$ of their traffic, depending on the type of competitive advantage that\ndrives customer's airline choice. These results highlight how the complex\ncompetitive structure of the WAN weakens its robustness when facing such a\nlarge crisis.\n"
    },
    {
        "paper_id": 2007.02113,
        "authors": "Qinwen Zhu, Gr\\'egoire Loeper, Wen Chen and Nicolas Langren\\'e",
        "title": "Markovian approximation of the rough Bergomi model for Monte Carlo\n  option pricing",
        "comments": "20 pages, 3 figures",
        "journal-ref": "Mathematics 9(5) 528 (2021)",
        "doi": "10.3390/math9050528",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recently developed rough Bergomi (rBergomi) model is a rough fractional\nstochastic volatility (RFSV) model which can generate more realistic term\nstructure of at-the-money volatility skews compared with other RFSV models.\nHowever, its non-Markovianity brings mathematical and computational challenges\nfor model calibration and simulation. To overcome these difficulties, we show\nthat the rBergomi model can be approximated by the Bergomi model, which has the\nMarkovian property. Our main theoretical result is to establish and describe\nthe affine structure of the rBergomi model. We demonstrate the efficiency and\naccuracy of our method by implementing a Markovian approximation algorithm\nbased on a hybrid scheme.\n"
    },
    {
        "paper_id": 2007.02316,
        "authors": "Carlos Escudero and Sandra Ranilla-Cortina",
        "title": "Optimal portfolios for different anticipating integrals under insider\n  information",
        "comments": null,
        "journal-ref": "Mathematics 9, 75 (2021)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the non-adapted version of a simple problem of portfolio\noptimization in a financial market that results from the presence of insider\ninformation. We analyze it via anticipating stochastic calculus and compare the\nresults obtained by means of the Russo-Vallois forward, the Ayed-Kuo, and the\nHitsuda-Skorokhod integrals. We compute the optimal portfolio for each of these\ncases with the aim of establishing a comparison between these integrals in\norder to clarify their potential use in this type of problem. Our results give\na partial indication that, while the forward integral yields a portfolio that\nis financially meaningful, the Ayed-Kuo and the Hitsuda-Skorokhod integrals do\nnot provide an appropriate investment strategy for this problem.\n"
    },
    {
        "paper_id": 2007.02323,
        "authors": "Benjamin Gottesman Berdah",
        "title": "Recombining tree approximations for Game Options in Local Volatility\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a numerical method for optimal stopping in the\nframework of one dimensional diffusion. We use the Skorokhod embedding in order\nto construct recombining tree approximations for diffusions with general\ncoefficients. This technique allows us to determine convergence rates and\nconstruct nearly optimal stopping times which are optimal at the same rate.\nFinally, we demonstrate the efficiency of our scheme with several examples of\ngame options.\n"
    },
    {
        "paper_id": 2007.02553,
        "authors": "Huy N. Chau",
        "title": "On robust fundamental theorems of asset pricing in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to a study of robust fundamental theorems of asset\npricing in discrete time and finite horizon settings. Uncertainty is modelled\nby a (possibly uncountable) family of price processes on the same probability\nspace. Our technical assumption is the continuity of the price processes with\nrespect to uncertain parameters. In this setting, we introduce a new\ntopological framework which allows us to use the classical arguments in\narbitrage pricing theory involving $L^p$ spaces, the Hahn-Banach separation\ntheorem and other tools from functional analysis. The first result is the\nequivalence of a ``no robust arbitrage\" condition and the existence of a new\n``robust pricing system\". The second result shows superhedging dualities and\nthe existence of superhedging strategies without restrictive conditions on\npayoff functions, unlike other related studies. The third result discusses\ncompleteness in the present robust setting. When other options are available\nfor static trading, we could reduce the set of robust pricing systems and hence\nthe superhedging prices.\n"
    },
    {
        "paper_id": 2007.02567,
        "authors": "Pierre Cohort, Jacopo Corbetta and Ismail Laachir",
        "title": "Analytical scores for stress scenarios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, inspired by the Archer-Mouy-Selmi approach, we present two\nmethodologies for scoring the stress test scenarios used by CCPs for sizing\ntheir Default Funds. These methodologies can be used by risk managers to\ncompare different sets of scenarios and could be particularly useful when\nevaluating the relevance of adding new scenarios to a pre-existing set.\n"
    },
    {
        "paper_id": 2007.02673,
        "authors": "Daniel \\v{S}tifani\\'c, Jelena Musulin, Adrijana Mio\\v{c}evi\\'c, Sandi\n  Baressi \\v{S}egota, Roman \\v{S}ubi\\'c, Zlatan Car",
        "title": "Impact of COVID-19 on Forecasting Stock Prices: An Integration of\n  Stationary Wavelet Transform and Bidirectional Long Short-Term Memory",
        "comments": "26 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  COVID-19 is an infectious disease that mostly affects the respiratory system.\nAt the time of this research being performed, there were more than 1.4 million\ncases of COVID-19, and one of the biggest anxieties is not just our health, but\nour livelihoods, too. In this research, authors investigate the impact of\nCOVID-19 on the global economy, more specifically, the impact of COVID-19 on\nfinancial movement of Crude Oil price and three U.S. stock indexes: DJI, S&P\n500 and NASDAQ Composite. The proposed system for predicting commodity and\nstock prices integrates the Stationary Wavelet Transform (SWT) and\nBidirectional Long Short-Term Memory (BDLSTM) networks. Firstly, SWT is used to\ndecompose the data into approximation and detail coefficients. After\ndecomposition, data of Crude Oil price and stock market indexes along with\nCOVID-19 confirmed cases were used as input variables for future price movement\nforecasting. As a result, the proposed system BDLSTM+WT-ADA achieved\nsatisfactory results in terms of five-day Crude Oil price forecast.\n"
    },
    {
        "paper_id": 2007.02692,
        "authors": "Benjamin Virrion (CEREMADE)",
        "title": "Deep Importance Sampling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a generic path-dependent importance sampling algorithm where the\nGirsanov induced change of probability on the path space is represented by a\nsequence of neural networks taking the past of the trajectory as an input. At\neach learning step, the neural networks' parameters are trained so as to reduce\nthe variance of the Monte Carlo estimator induced by this change of measure.\nThis allows for a generic path dependent change of measure which can be used to\nreduce the variance of any path-dependent financial payoff. We show in our\nnumerical experiments that for payoffs consisting of either a call, an\nasymmetric combination of calls and puts, a symmetric combination of calls and\nputs, a multi coupon autocall or a single coupon autocall, we are able to\nreduce the variance of the Monte Carlo estimators by factors between 2 and 9.\nThe numerical experiments also show that the method is very robust to changes\nin the parameter values, which means that in practice, the training can be done\noffline and only updated on a weekly basis.\n"
    },
    {
        "paper_id": 2007.02934,
        "authors": "Wolfgang Banzhaf",
        "title": "The Effects of Taxes on Wealth Inequality in Artificial Chemistry Models\n  of Economic Activity",
        "comments": "13 pages, 18 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0255719",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a number of Artificial Chemistry models for economic activity and\nwhat consequences they have for the formation of economic inequality. We are\nparticularly interested in what tax measures are effective in dampening\neconomic inequality. By starting from well-known kinetic exchange models, we\nexamine different scenarios for reducing the tendency of economic activity\nmodels to form unequal wealth distribution in equilibrium.\n"
    },
    {
        "paper_id": 2007.02935,
        "authors": "Jos\\'e Nilmar Alves de Oliveira, Jaime Orrillo, Franklin Gamboa",
        "title": "The Home Office in Times of COVID-19 Pandemic and its impact in the\n  Labor Supply",
        "comments": "13 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We lightly modify Eriksson's (1996) model to accommodate the home office in a\nsimple model of endogenous growth. By home office we mean any working activity\ncarried out away from the workplace which is assumed to be fixed. Due to the\nstrong mobility restrictions imposed on citizens during the COVID-19 pandemic,\nwe allow the home office to be located at home. At the home office, however, in\nconsequence of the fear and anxiety workers feel because of COVID-19, they\nbecome distracted and spend less time working. We show that in the long run,\nthe intertemporal elasticity of substitution of the home-office labor is\nsufficiently small only if the intertemporal elasticity of substitution of the\ntime spent on distracting activities is small enough also.\n"
    },
    {
        "paper_id": 2007.03015,
        "authors": "Paulina Concha Larrauri and Upmanu Lall",
        "title": "Big Data links from Climate to Commodity Production Forecasts and Risk\n  Management",
        "comments": "15 pages, 4 figures, and 7 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Frozen concentrated orange juice (FCOJ) is a commodity traded in the\nInternational Commodity Exchange. The FCOJ future price volatility is high\nbecause the world's orange production is concentrated in a few places, which\nresults in extreme sensitivity to weather and disease. Most of the oranges\nproduced in the United States are from Florida. The United States Department of\nAgriculture (USDA) issues orange production forecasts on the second week of\neach month from October to July. The October forecast in particular seems to\naffect FCOJ price volatility. We assess how a prediction of the directionality\nand magnitude of the error of the USDA October forecast could affect the\ndecision making process of multiple FCOJ market participants, and if the\n\"production uncertainty\" of the forecast could be reduced by incorporating\nother climate variables. The models developed open up the opportunity to assess\nthe application of the resulting probabilistic forecasts of the USDA production\nforecast error on the trading decisions of the different FCOJ stakeholders, and\nto perhaps consider the inclusion of climate predictors in the USDA forecast.\n"
    },
    {
        "paper_id": 2007.03453,
        "authors": "Patrick Chang",
        "title": "Fourier instantaneous estimators and the Epps effect",
        "comments": "17 pages, 10 figures, 2 tables. Link to the supporting Julia code:\n  https://github.com/CHNPAT005/PC-FIE",
        "journal-ref": "PLoS ONE 15(9): e0239415, 2020",
        "doi": "10.1371/journal.pone.0239415",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare the Malliavin-Mancino and Cuchiero-Teichmann Fourier instantaneous\nestimators to investigate the impact of the Epps effect arising from asynchrony\nin the instantaneous estimates. We demonstrate the instantaneous Epps effect\nunder a simulation setting and provide a simple method to ameliorate the\neffect. We find that using the previous tick interpolation in the\nCuchiero-Teichmann estimator results in unstable estimates when dealing with\nasynchrony, while the ability to bypass the time domain with the\nMalliavin-Mancino estimator allows it to produce stable estimates and is\ntherefore better suited for ultra-high frequency finance. An empirical analysis\nusing Trade and Quote data from the Johannesburg Stock Exchange illustrates the\ninstantaneous Epps effect and how the intraday correlation dynamics can vary\nbetween days for the same equity pair.\n"
    },
    {
        "paper_id": 2007.03477,
        "authors": "Carlo Fezzi, Valeria Fanghella",
        "title": "Real-time estimation of the short-run impact of COVID-19 on economic\n  activity using electricity market data",
        "comments": "Accepted for publication in Environmental and Resource Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has caused more than 8 million confirmed cases and\n500,000 death to date. In response to this emergency, many countries have\nintroduced a series of social-distancing measures including lockdowns and\nbusinesses' temporary shutdowns, in an attempt to curb the spread of the\ninfection. Accordingly, the pandemic has been generating unprecedent disruption\non practically every aspect of society. This paper demonstrates that\nhigh-frequency electricity market data can be used to estimate the causal,\nshort-run impact of COVID-19 on the economy. In the current uncertain economic\nconditions, timeliness is essential. Unlike official statistics, which are\npublished with a delay of a few months, with our approach one can monitor\nvirtually every day the impact of the containment policies, the extent of the\nrecession and measure whether the monetary and fiscal stimuli introduced to\naddress the crisis are being effective. We illustrate our methodology on daily\ndata for the Italian day-ahead power market. Not surprisingly, we find that the\ncontainment measures caused a significant reduction in economic activities and\nthat the GDP at the end of in May 2020 is still about 11% lower that what it\nwould have been without the outbreak.\n"
    },
    {
        "paper_id": 2007.03494,
        "authors": "Dirk Roeder, Georgi Dimitroff",
        "title": "Volatility model calibration with neural networks a comparison between\n  direct and indirect methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a recent paper \"Deep Learning Volatility\" a fast 2-step deep calibration\nalgorithm for rough volatility models was proposed: in the first step the time\nconsuming mapping from the model parameter to the implied volatilities is\nlearned by a neural network and in the second step standard solver techniques\nare used to find the best model parameter.\n  In our paper we compare these results with an alternative direct approach\nwhere the the mapping from market implied volatilities to model parameters is\napproximated by the neural network, without the need for an extra solver step.\nUsing a whitening procedure and a projection of the target parameter to [0,1],\nin order to be able to use a sigmoid type output function we found that the\ndirect approach outperforms the two-step one for the data sets and methods\npublished in \"Deep Learning Volatility\".\n  For our implementation we use the open source tensorflow 2 library. The paper\nshould be understood as a technical comparison of neural network techniques and\nnot as an methodically new Ansatz.\n"
    },
    {
        "paper_id": 2007.03573,
        "authors": "Zhibin Niu, Runlin Li, Junqi Wu, Yaqi Xue, Jiawan Zhang",
        "title": "regvis.net -- A Visual Bibliography of Regulatory Visualization",
        "comments": "2 pages. Refer to http://regvis.net",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information visualization and visual analytics technology has attracted\nsignificant attention from the financial regulation community. In this\nresearch, we present regvis.net, a visual survey of regulatory visualization\nthat allows researchers from both the computing and financial communities to\nreview their literature of interest. We have collected and manually tagged more\nthan 80 regulation visualization related publications. To the best of our\nknowledge, this is the first publication set tailored for regulatory\nvisualization. We have provided a webpage (http://regvis.net) for interactive\nsearches and filtering. Each publication is represented by a thumbnail of the\nrepresentative system interface or key visualization chart, and users can\nconduct multi-condition screening explorations and fixed text searches.\n"
    },
    {
        "paper_id": 2007.03585,
        "authors": "Stefano De Marco",
        "title": "On the harmonic mean representation of the implied volatility",
        "comments": "20 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well know that, in the short maturity limit, the implied volatility\napproaches the integral harmonic mean of the local volatility with respect to\nlog-strike, see [Berestycki et al., Asymptotics and calibration of local\nvolatility models, Quantitative Finance, 2, 2002]. This paper is dedicated to a\ncomplementary model-free result: an arbitrage-free implied volatility in fact\nis the harmonic mean of a positive function for any fixed maturity. We\ninvestigate the latter function, which is tightly linked to Fukasawa's\ninvertible map $f_{1/2}$ [Fukasawa, The normalizing transformation of the\nimplied volatility smile, Mathematical Finance, 22, 2012], and its relation\nwith the local volatility surface. It turns out that the log-strike\ntransformation $z = f_{1/2}(k)$ defines a new coordinate system in which the\nshort-dated implied volatility approaches the arithmetic (as opposed to\nharmonic) mean of the local volatility. As an illustration, we consider the\ncase of the SSVI parameterization: in this setting, we obtain an explicit\nformula for the volatility swap from options on realized variance.\n"
    },
    {
        "paper_id": 2007.03654,
        "authors": "Andreas M. Hein",
        "title": "The Cathedral and the Starship: Learning from the Middle Ages for Future\n  Long-Duration Projects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A popular analogue used in the space domain is that of historical building\nprojects, notably cathedrals that took decades and in some cases centuries to\ncomplete. Cathedrals are often taken as archetypes for long-term projects. In\nthis article, I will explore the cathedral from the point of view of project\nmanagement and systems architecting and draw implications for long-term\nprojects in the space domain, notably developing a starship. I will show that\nthe popular image of a cathedral as a continuous long-term project is in\ncontradiction to the current state of research. More specifically, I will show\nthat for the following propositions: The cathedrals were built based on an\ninitial detailed master plan; Building was a continuous process that adhered to\nthe master plan; Investments were continuously provided for the building\nprocess. Although initial plans might have existed, the construction process\ntook often place in multiple campaigns, sometimes separated by decades. Such\ninterruptions made knowledge-preservation very challenging. The reason for the\nlong stretches of inactivity was mostly due to a lack of funding. Hence, the\navailability of funding coincided with construction activity. These findings\npaint a much more relevant picture of cathedral building for long-duration\nprojects today: How can a project be completed despite a range of uncertainties\nregarding loss in skills, shortage in funding, and interruptions? It is\nconcluded that long-term projects such as an interstellar exploration program\ncan take inspiration from cathedrals by developing a modular architecture,\nallowing for extensibility and flexibility, thinking about value delivery at an\nearly point, and establishing mechanisms and an organization for stable\nfunding.\n"
    },
    {
        "paper_id": 2007.0398,
        "authors": "Matthias Raddant and Hiroshi Takahashi",
        "title": "Interdependencies of female board member appointments",
        "comments": null,
        "journal-ref": "International Review of Financial Analysis Volume 81, May 2022,\n  102080",
        "doi": "10.1016/j.irfa.2022.102080",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the networks of Japanese corporate boards and its influence on\nthe appointments of female board members. We find that corporate boards with\nwomen show homophily with respect to gender. The corresponding firms often have\nabove average profitability. We also find that new appointments of women are\nmore likely at boards which observe female board members at other firms to\nwhich they are tied by either ownership relations or corporate board\ninterlocks.\n"
    },
    {
        "paper_id": 2007.04051,
        "authors": "Jamaal Ahmad, Kristian Buchardt, and Christian Furrer",
        "title": "Computation of bonus in multi-state life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider computation of market values of bonus payments in multi-state\nwith-profit life insurance. The bonus scheme consists of additional benefits\nbought according to a dividend strategy that depends on the past realization of\nfinancial risk, the current individual insurance risk, the number of additional\nbenefits currently held, and so-called portfolio-wide means describing the\nshape of the insurance business. We formulate numerical procedures that\nefficiently combine simulation of financial risk with classic methods for the\noutstanding insurance risk. Special attention is given to the case where the\nnumber of additional benefits bought only depends on the financial risk.\nMethods and results are illustrated via a numerical example.\n"
    },
    {
        "paper_id": 2007.04082,
        "authors": "Lakshay Chauhan, John Alberg, Zachary C. Lipton",
        "title": "Uncertainty-Aware Lookahead Factor Models for Quantitative Investing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On a periodic basis, publicly traded companies report fundamentals, financial\ndata including revenue, earnings, debt, among others. Quantitative finance\nresearch has identified several factors, functions of the reported data that\nhistorically correlate with stock market performance. In this paper, we first\nshow through simulation that if we could select stocks via factors calculated\non future fundamentals (via oracle), that our portfolios would far outperform\nstandard factor models. Motivated by this insight, we train deep nets to\nforecast future fundamentals from a trailing 5-year history. We propose\nlookahead factor models which plug these predicted future fundamentals into\ntraditional factors. Finally, we incorporate uncertainty estimates from both\nneural heteroscedastic regression and a dropout-based heuristic, improving\nperformance by adjusting our portfolios to avert risk. In retrospective\nanalysis, we leverage an industry-grade portfolio simulator (backtester) to\nshow simultaneous improvement in annualized return and Sharpe ratio.\nSpecifically, the simulated annualized return for the uncertainty-aware model\nis 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84\n(vs 0.52).\n"
    },
    {
        "paper_id": 2007.04154,
        "authors": "Patryk Gierjatowicz and Marc Sabate-Vidales and David \\v{S}i\\v{s}ka\n  and Lukasz Szpruch and \\v{Z}an \\v{Z}uri\\v{c}",
        "title": "Robust pricing and hedging via neural SDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical modelling is ubiquitous in the financial industry and drives key\ndecision processes. Any given model provides only a crude approximation to\nreality and the risk of using an inadequate model is hard to detect and\nquantify. By contrast, modern data science techniques are opening the door to\nmore robust and data-driven model selection mechanisms. However, most machine\nlearning models are \"black-boxes\" as individual parameters do not have\nmeaningful interpretation. The aim of this paper is to combine the above\napproaches achieving the best of both worlds. Combining neural networks with\nrisk models based on classical stochastic differential equations (SDEs), we\nfind robust bounds for prices of derivatives and the corresponding hedging\nstrategies while incorporating relevant market data. The resulting model called\nneural SDE is an instantiation of generative models and is closely linked with\nthe theory of causal optimal transport. Neural SDEs allow consistent\ncalibration under both the risk-neutral and the real-world measures. Thus the\nmodel can be used to simulate market scenarios needed for assessing risk\nprofiles and hedging strategies. We develop and analyse novel algorithms needed\nfor efficient use of neural SDEs. We validate our approach with numerical\nexperiments using both local and stochastic volatility models.\n"
    },
    {
        "paper_id": 2007.04203,
        "authors": "Thomas Spooner and Rahul Savani",
        "title": "A Natural Actor-Critic Algorithm with Downside Risk Constraints",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existing work on risk-sensitive reinforcement learning - both for symmetric\nand downside risk measures - has typically used direct Monte-Carlo estimation\nof policy gradients. While this approach yields unbiased gradient estimates, it\nalso suffers from high variance and decreased sample efficiency compared to\ntemporal-difference methods. In this paper, we study prediction and control\nwith aversion to downside risk which we gauge by the lower partial moment of\nthe return. We introduce a new Bellman equation that upper bounds the lower\npartial moment, circumventing its non-linearity. We prove that this proxy for\nthe lower partial moment is a contraction, and provide intuition into the\nstability of the algorithm by variance decomposition. This allows\nsample-efficient, on-line estimation of partial moments. For risk-sensitive\ncontrol, we instantiate Reward Constrained Policy Optimization, a recent\nactor-critic method for finding constrained policies, with our proxy for the\nlower partial moment. We extend the method to use natural policy gradients and\ndemonstrate the effectiveness of our approach on three benchmark problems for\nrisk-sensitive reinforcement learning.\n"
    },
    {
        "paper_id": 2007.04408,
        "authors": "Chinonso Nwankwo and Weizhong Dai",
        "title": "An Adaptive and Explicit Fourth Order Runge-Kutta-Fehlberg Method\n  Coupled with Compact Finite Differencing for Pricing American Put Options",
        "comments": "This is a preprint of an article published in Japan Journal of\n  Industrial and Applied Mathematics (JJIAM). The final authenticated version\n  is available online at: https://doi.org/10.1007/s13160-021-00470-2",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an adaptive and explicit fourth-order Runge-Kutta-Fehlberg method\ncoupled with a fourth-order compact scheme to solve the American put options\nproblem. First, the free boundary problem is converted into a system of partial\ndifferential equations with a fixed domain by using logarithm transformation\nand taking additional derivatives. With the addition of an intermediate\nfunction with a fixed free boundary, a quadratic formula is derived to compute\nthe velocity of the optimal exercise boundary analytically. Furthermore, we\nimplement an extrapolation method to ensure that at least, a third-order\naccuracy in space is maintained at the boundary point when computing the\noptimal exercise boundary from its derivative. As such, it enables us to employ\nfourth-order spatial and temporal discretization with Dirichlet boundary\nconditions for obtaining the numerical solution of the asset option, option\nGreeks, and the optimal exercise boundary. The advantage of the\nRunge-Kutta-Fehlberg method is based on error control and the adjustment of the\ntime step to maintain the error at a certain threshold. By comparing with some\nexisting methods in the numerical experiment, it shows that the present method\nhas a better performance in terms of computational speed and provides a more\naccurate solution.\n"
    },
    {
        "paper_id": 2007.04435,
        "authors": "Doron Klunover",
        "title": "Nice guys don't always finish last: succeeding in hierarchical\n  organizations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What are the chances of an ethical individual rising through the ranks of a\npolitical party or a corporation in the presence of unethical peers? To answer\nthis question, I consider a four-player two-stage elimination tournament, in\nwhich players are partitioned into those willing to be involved in sabotage\nbehavior and those who are not. I show that, under certain conditions, the\nlatter are more likely to win the tournament.\n"
    },
    {
        "paper_id": 2007.04758,
        "authors": "Jiwook Jang and Rosy Oh",
        "title": "A Bivariate Compound Dynamic Contagion Process for Cyber Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As corporates and governments become more digital, they become vulnerable to\nvarious forms of cyber attack. Cyber insurance products have been used as risk\nmanagement tools, yet their pricing does not reflect actual risk, including\nthat of multiple, catastrophic and contagious losses. For the modelling of\naggregate losses from cyber events, in this paper we introduce a bivariate\ncompound dynamic contagion process, where the bivariate dynamic contagion\nprocess is a point process that includes both externally excited joint jumps,\nwhich are distributed according to a shot noise Cox process and two separate\nself-excited jumps, which are distributed according to the branching structure\nof a Hawkes process with an exponential fertility rate, respectively. We\nanalyse the theoretical distributional properties for these processes\nsystematically, based on the piecewise deterministic Markov process developed\nby Davis (1984) and the univariate dynamic contagion process theory developed\nby Dassios and Zhao (2011). The analytic expression of the Laplace transform of\nthe compound process and its moments are presented, which have the potential to\nbe applicable to a variety of problems in credit, insurance, market and other\noperational risks. As an application of this process, we provide insurance\npremium calculations based on its moments. Numerical examples show that this\ncompound process can be used for the modelling of aggregate losses from cyber\nevents. We also provide the simulation algorithm for statistical analysis,\nfurther business applications and research.\n"
    },
    {
        "paper_id": 2007.04829,
        "authors": "Tam\\'as S. Bir\\'o and Zolt\\'an N\\'eda",
        "title": "Gintropy: Gini index based generalization of Entropy",
        "comments": "13 pages, 3 Figures",
        "journal-ref": null,
        "doi": "10.3390/e22080879",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Entropy is being used in physics, mathematics, informatics and in related\nareas to describe equilibration, dissipation, maximal probability states and\noptimal compression of information. The Gini index on the other hand is an\nestablished measure for social and economical inequalities in a society. In\nthis paper we explore the mathematical similarities and connections in these\ntwo quantities and introduce a new measure that is capable to connect these two\nat an interesting analogy level. This supports the idea that a generalization\nof the Gibbs--Boltzmann--Shannon entropy, based on a transformation of the\nLorenz curve, can properly serve in quantifying different aspects of complexity\nin socio- and econo-physics.\n"
    },
    {
        "paper_id": 2007.04838,
        "authors": "Edmond Lezmi, Jules Roche, Thierry Roncalli, Jiali Xu",
        "title": "Improving the Robustness of Trading Strategy Backtesting with Boltzmann\n  Machines and Generative Adversarial Networks",
        "comments": "72 pages, 30 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article explores the use of machine learning models to build a market\ngenerator. The underlying idea is to simulate artificial multi-dimensional\nfinancial time series, whose statistical properties are the same as those\nobserved in the financial markets. In particular, these synthetic data must\npreserve the probability distribution of asset returns, the stochastic\ndependence between the different assets and the autocorrelation across time.\nThe article proposes then a new approach for estimating the probability\ndistribution of backtest statistics. The final objective is to develop a\nframework for improving the risk management of quantitative investment\nstrategies, in particular in the space of smart beta, factor investing and\nalternative risk premia.\n"
    },
    {
        "paper_id": 2007.04909,
        "authors": "Mikhail Zhitlukhin",
        "title": "Asymptotic minimization of expected time to reach a large wealth level\n  in an asset market game",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic game-theoretic model of a discrete-time asset market\nwith short-lived assets and endogenous asset prices. We prove that the strategy\nwhich invests in the assets proportionally to their expected relative payoffs\nasymptotically minimizes the expected time needed to reach a large wealth\nlevel. The result is obtained under the assumption that the relative asset\npayoffs and the growth rate of the total payoff during each time period are\nindependent and identically distributed.\n"
    },
    {
        "paper_id": 2007.04962,
        "authors": "Giovane Cerezuela Policeno, Mario Edson Passerino Fischer da Silva and\n  Vitor Pestana Ostrensky",
        "title": "An\\'alise dos fatores determinantes para o decreto de pris\\~ao\n  preventiva em casos envolvendo acusa\\c{c}\\~oes por roubo, tr\\'afico de drogas\n  e furto: Um estudo no \\^ambito das cidades mais populosas do Paran\\'a",
        "comments": "23 pages, in Portuguese",
        "journal-ref": "RJLB - REVISTA JUR\\'IDICA LUSO-BRASILEIRA, v. 1, p. 919-944, 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the theoretical assumptions of the Labeling Approach, Critical\nCriminology and Behavioral Economics, taking into account that almost half of\nthe Brazilian prison population is composed of individuals who are serving\npre-trial detention, it was sought to assess the characteristics of the\nflagranteated were presented as determinants to influence the subjectivity of\nthe judges in the decision to determine, or not, the custody of these. The\nresearch initially adopted a deductive methodology, based on the principle that\nexternal objective factors encourage magistrates to decide in a certain sense.\nIt was then focused on the identification of which characteristics of the\nflagranteated and allegedly committed crimes would be relevant to guide such\ndecisions. Subsequently, after deduction of such factors, an inductive\nmethodology was adopted, analyzing the data from the theoretical assumptions\npointed out. During the research, 277 decisions were analyzed, considering\ndecision as individual decision and not custody hearing. This sample embarked\ndecisions of six judges among the largest cities of the State of Paran\\'a and\nconcerning the crimes of theft, robbery and drug trafficking. It was then\nconcluded that the age, gender, social class and type of accusation that the\nflagranteated suffered are decisive for the decree of his provisional arrest,\nbeing that, depending on the judge competent in the case, the chances of the\ndecree can increase in Up to 700%, taking into account that the circumstantial\nand causal variables are constant. Given the small sample size, as far as the\nnumber of judges is concerned, more extensive research is needed so that\nconclusions of national validity can be developed.\n"
    },
    {
        "paper_id": 2007.05289,
        "authors": "Spyridon M. Tzaninis and Nikolaos D. Macheras",
        "title": "A characterization of progressively equivalent probability measures\n  preserving the structure of a compound mixed renewal process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generalizing earlier works of Delbaen & Haezendonck [5] as well as of [18]\nand [16] for given compound mixed renewal process S under a probability measure\nP, we characterize all those probability measures Q on the domain of P such\nthat Q and P are progressively equivalent and S remains a compound mixed\nrenewal process under Q with improved properties. As a consequence, we prove\nthat any compound mixed renewal process can be converted into a compound mixed\nPoisson process through a change of measures. Applications related to the ruin\nproblem and to the computation of premium calculation principles in an\ninsurance market without arbitrage opportunities are discussed in [26] and\n[27], respectively.\n"
    },
    {
        "paper_id": 2007.05884,
        "authors": "Plamen Nikolov, Md Shahadath Hossain",
        "title": "Do Pension Benefits Accelerate Cognitive Decline in Late Adulthood?\n  Evidence from Rural China",
        "comments": "62 pages; 1 figure; 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economists have mainly focused on human capital accumulation rather than on\nthe causes and consequences of human capital depreciation in late adulthood. To\ninvestigate how human capital depreciates over the life cycle, we examine how a\nnewly introduced pension program, the National Rural Pension Scheme, affects\ncognitive performance in rural China. We find significant adverse effects of\naccess to pension benefits on cognitive functioning among the elderly. We\ndetect the most substantial impact of the program on delayed recall, a\ncognition measure linked to the onset of dementia. In terms of mechanisms,\ncognitive deterioration in late adulthood is mediated by a substantial\nreduction in social engagement, volunteering, and activities fostering mental\nacuity.\n"
    },
    {
        "paper_id": 2007.05933,
        "authors": "Mirco Rubin, Dario Ruzzi",
        "title": "Equity Tail Risk in the Treasury Bond Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper quantifies the effects of equity tail risk on the US government\nbond market. We estimate equity tail risk with option-implied stock market\nvolatility that stems from large negative price jumps, and we assess its value\nin reduced-form predictive regressions for Treasury returns and a term\nstructure model for interest rates. We find that the left tail volatility of\nthe stock market significantly predicts one-month excess returns on Treasuries\nboth in- and out-of-sample. The incremental value of employing equity tail risk\nas a return forecasting factor can be of economic importance for a\nmean-variance investor trading bonds. The estimated term structure model shows\nthat equity tail risk is priced in the US government bond market and,\nconsistent with the theory of flight-to-safety, Treasury prices increase when\nthe perception of tail risk is higher. Our results concerning the predictive\npower and pricing of equity tail risk extend to major government bond markets\nin Europe.\n"
    },
    {
        "paper_id": 2007.06262,
        "authors": "Guglielmo D'Amico and Filippo Petroni",
        "title": "A micro-to-macro approach to returns, volumes and waiting times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fundamental variables in financial market are not only price and return but a\nvery important role is also played by trading volumes. Here we propose a new\nmultivariate model that takes into account price returns, logarithmic variation\nof trading volumes and also waiting times, the latter to be intended as the\ntime interval between changes in trades, price, and volume of stocks. Our\napproach is based on a generalization of semi-Markov chains where an endogenous\nindex process is introduced. We also take into account the dependence structure\nbetween the above mentioned variables by means of copulae. The proposed model\nis motivated by empirical evidences which are known in financial literature and\nthat are also confirmed in this work by analysing real data from Italian stock\nmarket in the period August 2015 - August 2017. By using Monte Carlo\nsimulations, we show that the model reproduces all these empirical evidences.\n"
    },
    {
        "paper_id": 2007.0633,
        "authors": "Jean-Fran\\c{c}ois Renaud and Clarence Simard",
        "title": "A stochastic control problem with linearly bounded control rates in a\n  Brownian model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aiming for more realistic optimal dividend policies, we consider a stochastic\ncontrol problem with linearly bounded control rates using a performance\nfunction given by the expected present value of dividend payments made up to\nruin. In a Brownian model, we prove the optimality of a member of a new family\nof control strategies called delayed linear control strategies, for which the\ncontrolled process is a refracted diffusion process. For some parameters\nspecifications, we retrieve the strategy initially proposed by Avanzi & Wong\n(2012) to regularize dividend payments, which is more consistent with actual\npractice.\n"
    },
    {
        "paper_id": 2007.06451,
        "authors": "Micha{\\l} Chorowski, Ryszard Kutner ((1) College of Inter Faculty\n  Individual Studies in Mathematics and Natural Sciences University of Warsaw,\n  (2) Faculty of Physics University of Warsaw)",
        "title": "Government intervention modeling in microeconomic company market\n  evolution",
        "comments": "submitted to Acta Physica Polonica A, Proceedings of the 10th Polish\n  Symposium of Physics in Economy and Social Sciences FENS, 3-5 July 2019,\n  Otwock - Swierk",
        "journal-ref": null,
        "doi": "10.12693/APhysPolA.138.74",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern technology and innovations are becoming more crucial than ever for the\nsurvival of companies in the market. Therefore, it is significant both from\ntheoretical and practical points of view to understand how governments can\ninfluence technology growth and innovation diffusion (TGID) processes. We\npropose a simple but essential extension of Ausloos-Clippe-P\\c{e}kalski and\nrelated Cichy numerical models of the TGID in the market. Both models are\ninspired by the nonlinear non-equilibrium statistical physics. Our extension\ninvolves a parameter describing the probability of government intervention in\nthe TGID process in the company market. We show, using Monte Carlo simulations,\nthe effects interventionism can have on the companies' market, depending on the\nsegment of firms that are supported. The high intervention probability can\nresult, paradoxically, in the destabilization of the market development. It\nlowers the market's technology level in the long-time limit compared to markets\nwith a lower intervention parameter. We found that the intervention in the\ntechnologically weak and strong segments of the company market does not\nsubstantially influence the market dynamics, compared to the intervention\nhelping the middle-level companies. However, this is still a simple model which\ncan be extended further and made more realistic by including other factors.\nNamely, the cost and risk of innovation or limited government resources and\ncapabilities to support companies.\n"
    },
    {
        "paper_id": 2007.0646,
        "authors": "Tarek Nassar, Sandro Ephrem",
        "title": "Optimal allocation using the Sortino ratio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present an asset allocation strategy based on the\nmaximization of the Sortino ratio. Unlike the Sharpe ratio, the Sortino ratio\npenalizes negative return variances only. The resulting allocation is valid for\nany time horizon unlike. The returns of a strategy based on such an allocation\nare empirically illustrated using historical Dow Jones data and display a\nsignificant upgrade on more traditional allocation strategies such as the Kelly\ncriterion.\n"
    },
    {
        "paper_id": 2007.06465,
        "authors": "Christian P. Fries",
        "title": "Non-Linear Discounting and Default Compensation: Valuation of\n  Non-Replicable Value and Damage: When the Social Discount Rate may become\n  Negative",
        "comments": "40 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a model that adds a non-linearity to discounting:\nthe discounting factor may depend on the notional (i.e., discounted values are\nno longer linear in the notional). In the first part of the paper, we provide a\ndiscounting when discount factors cannot be derived from market products. That\nis, a risk-neutralising trading strategy cannot be performed. This is the case\nwhen one needs a risk-free (default-free) discounting, but default protection\non funding providers is not traded. For this case, we derive a default\ncompensation factor that describes the present value of a strategy to\ncompensate for default (like buying default protection would do). In a second\npart of the paper, we introduce a model where the survival probability, and\nhence the discount factor, depends on the notional. This model introduces an\neffect not present in the classical modelling of a time-dependent survival\nprobability. Our model allows that large liquidity requirements are more likely\nto default instantly than small ones. Combined, the two models build a\nframework where discounting (and hence valuation) is non-linear: discount\nfactors depend on the amount to be discounted.\n  Our approach builds on top of the classical theory of discounting (which may\neither be given as market-implied or be derived from a model of utility,\nconsumption and production). In that sense, it is rather a generalisation than\nan alternative. The modelling approach has specific relevance for climate\nmodels, where discounting is an important aspect in assessing the severity of\nfuture events. Our model may result in non-decaying discount factors (negative\ndiscount rates) for certain scenarios.\n"
    },
    {
        "paper_id": 2007.06468,
        "authors": "Lucas B\\\"ottcher and Hans Gersbach",
        "title": "Incentivizing Narrow-Spectrum Antibiotic Development with Refunding",
        "comments": "21 pages and 10 figures",
        "journal-ref": "Bull Math Biol 84, 59 (2022)",
        "doi": "10.1007/s11538-022-01013-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rapid rise of antibiotic resistance is a serious threat to global public\nhealth. Without further incentives, pharmaceutical companies have little\ninterest in developing antibiotics, since the success probability is low and\ndevelopment costs are huge. The situation is exacerbated by the \"antibiotics\ndilemma\": Developing narrow-spectrum antibiotics against resistant bacteria is\nmost beneficial for society, but least attractive for companies since their\nusage is more limited than for broad-spectrum drugs and thus sales are low.\nStarting from a general mathematical framework for the study of\nantibiotic-resistance dynamics with an arbitrary number of antibiotics, we\nidentify efficient treatment protocols and introduce a market-based refunding\nscheme that incentivizes pharmaceutical companies to develop narrow-spectrum\nantibiotics: Successful companies can claim a refund from a newly established\nantibiotics fund that partially covers their development costs. The proposed\nrefund involves a fixed and variable part. The latter (i) increases with the\nuse of the new antibiotic for currently resistant strains in comparison with\nother newly developed antibiotics for this purpose---the resistance\npremium---and (ii) decreases with the use of this antibiotic for non-resistant\nbacteria. We outline how such a refunding scheme can solve the antibiotics\ndilemma and cope with various sources of uncertainty inherent in antibiotic\nR\\&D. Finally, connecting our refunding approach to the recently established\nantimicrobial resistance (AMR) action fund, we discuss how the antibiotics fund\ncan be financed.\n"
    },
    {
        "paper_id": 2007.0651,
        "authors": "Ben-Zhang Yang, Xin-Jiang He and Song-Ping Zhu",
        "title": "Mean-variance-utility portfolio selection with time and state dependent\n  risk aversion",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2005.06782",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under mean-variance-utility framework, we propose a new portfolio selection\nmodel, which allows wealth and time both have influences on risk aversion in\nthe process of investment. We solved the model under a game theoretic framework\nand analytically derived the equilibrium investment (consumption) policy. The\nresults conform with the facts that optimal investment strategy heavily depends\non the investor's wealth and future income-consumption balance as well as the\ncontinuous optimally consumption process is highly dependent on the consumption\npreference of the investor.\n"
    },
    {
        "paper_id": 2007.0653,
        "authors": "Ivan Kitov",
        "title": "Race and gender income inequality in the USA: black women vs. white men",
        "comments": "19 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:1510.02752",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Income inequality between different races in the U.S. is especially large.\nThis difference is even larger when gender is involved. In a complementary\nstudy, we have developed a dynamic microeconomic model accurately describing\nthe evolution of male and female incomes since 1930. Here, we extend our\nanalysis and model the disparity between black and white population in the\nU.S., separately for males and females. Unfortunately, income microdata\nprovided by the U.S. Census Bureau for other races and ethnic groups are not\ntime compatible or too short for modelling purposes. We are forced to constrain\nour analysis to black and white population, but all principal results can be\nextrapolated to other races and ethnicities. Our analysis shows that black\nfemales and white males are two poles of the overall income inequality. The\nprediction of income distribution for two extreme cases with one model is the\nmain challenge of this study.\n"
    },
    {
        "paper_id": 2007.06535,
        "authors": "Shahzada Aamir Mushtaq, Wang Yuhui",
        "title": "The New Digital Platforms: Merger Control in Pakistan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Pakistan competition policy, as in many other countries, was originally\ndesigned to regulate business conduct in traditional markets and for tangible\ngoods and services. However, the development and proliferation of the internet\nhas led to the emergence of digital companies which have disrupted many sectors\nof the economy. These platforms provide digital infrastructure for a range of\nservices including search engines, marketplaces, and social networking sites.\nThe digital economy poses a myriad of challenges for competition authorities\nworldwide, especially with regard to digital mergers and acquisitions (M&As).\nWhile some jurisdictions such as the European Union and the United States have\ntaken significant strides in regulating technological M&As, there is an\nincreasing need for developing countries such as Pakistan to rethink their\ncompetition policy tools. This paper investigates whether merger reviews in the\nPakistan digital market are informed by the same explanatory variables as in\nthe traditional market, by performing an empirical comparative analysis of the\nCompetition Commission of Pakistan's (CCP's) M&A decisions between 2014 and\n2019. The findings indicate the CCP applies the same decision factors in\nreviewing both traditional and digital M&As. As such, this paper establishes a\nbasis for igniting the policy and economic debate of regulating the digital\nplatform industry in Pakistan.\n"
    },
    {
        "paper_id": 2007.06617,
        "authors": "Parisa Golbayani, Ionu\\c{t} Florescu, Rupak Chatterjee",
        "title": "A comparative study of forecasting Corporate Credit Ratings using Neural\n  Networks, Support Vector Machines, and Decision Trees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit ratings are one of the primary keys that reflect the level of\nriskiness and reliability of corporations to meet their financial obligations.\nRating agencies tend to take extended periods of time to provide new ratings\nand update older ones. Therefore, credit scoring assessments using artificial\nintelligence has gained a lot of interest in recent years. Successful machine\nlearning methods can provide rapid analysis of credit scores while updating\nolder ones on a daily time scale. Related studies have shown that neural\nnetworks and support vector machines outperform other techniques by providing\nbetter prediction accuracy. The purpose of this paper is two fold. First, we\nprovide a survey and a comparative analysis of results from literature applying\nmachine learning techniques to predict credit rating. Second, we apply\nourselves four machine learning techniques deemed useful from previous studies\n(Bagged Decision Trees, Random Forest, Support Vector Machine and Multilayer\nPerceptron) to the same datasets. We evaluate the results using a 10-fold cross\nvalidation technique. The results of the experiment for the datasets chosen\nshow superior performance for decision tree based models. In addition to the\nconventional accuracy measure of classifiers, we introduce a measure of\naccuracy based on notches called \"Notch Distance\" to analyze the performance of\nthe above classifiers in the specific context of credit rating. This measure\ntells us how far the predictions are from the true ratings. We further compare\nthe performance of three major rating agencies, Standard $\\&$ Poors, Moody's\nand Fitch where we show that the difference in their ratings is comparable with\nthe decision tree prediction versus the actual rating on the test dataset.\n"
    },
    {
        "paper_id": 2007.06848,
        "authors": "Jungsik Hwang",
        "title": "Modeling Financial Time Series using LSTM with Trainable Initial Hidden\n  States",
        "comments": "5 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Extracting previously unknown patterns and information in time series is\ncentral to many real-world applications. In this study, we introduce a novel\napproach to modeling financial time series using a deep learning model. We use\na Long Short-Term Memory (LSTM) network equipped with the trainable initial\nhidden states. By learning to reconstruct time series, the proposed model can\nrepresent high-dimensional time series data with its parameters. An experiment\nwith the Korean stock market data showed that the model was able to capture the\nrelative similarity between a large number of stock prices in its latent space.\nBesides, the model was also able to predict the future stock trends from the\nlatent space. The proposed method can help to identify relationships among many\ntime series, and it could be applied to financial applications, such as\noptimizing the investment portfolios.\n"
    },
    {
        "paper_id": 2007.06994,
        "authors": "Manini Ojha and Mohammad Arshad Rahman",
        "title": "Do Online Courses Provide an Equal Educational Value Compared to\n  In-Person Classroom Teaching? Evidence from US Survey Data using Quantile\n  Regression",
        "comments": "20 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Education has traditionally been classroom-oriented with a gradual growth of\nonline courses in recent times. However, the outbreak of the COVID-19 pandemic\nhas dramatically accelerated the shift to online classes. Associated with this\nlearning format is the question: what do people think about the educational\nvalue of an online course compared to a course taken in-person in a classroom?\nThis paper addresses the question and presents a Bayesian quantile analysis of\npublic opinion using a nationally representative survey data from the United\nStates. Our findings show that previous participation in online courses and\nfull-time employment status favor the educational value of online courses. We\nalso find that the older demographic and females have a greater propensity for\nonline education. In contrast, highly educated individuals have a lower\nwillingness towards online education vis-\\`a-vis traditional classes. Besides,\ncovariate effects show heterogeneity across quantiles which cannot be captured\nusing probit or logit models.\n"
    },
    {
        "paper_id": 2007.07068,
        "authors": "Carlos Andr\\'es Araiza Iturria, Fr\\'ed\\'eric Godin and M\\'elina\n  Mailhot",
        "title": "Modeling and measuring incurred claims risk liabilities for a multi-line\n  property and casualty insurer",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s13385-021-00267-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a stochastic model allowing property and casualty insurers with\nmultiple business lines to measure their liabilities for incurred claims risk\nand calculate associated capital requirements. Our model includes many\ndesirable features which enable reproducing empirical properties of loss ratio\ndynamics. For instance, our model integrates a double generalized linear model\nrelying on accident semester and development lag effects to represent both the\nmean and dispersion of loss ratio distributions, an autocorrelation structure\nbetween loss ratios of the various development lags, and a hierarchical copula\nmodel driving the dependence across the various business lines. The model\nallows for a joint simulation of loss triangles and the quantification of the\noverall portfolio risk through risk measures. Consequently, a diversification\nbenefit associated to the economic capital requirements can be measured, in\naccordance with IFRS 17 standards which allow for the recognition of such\nbenefit. The allocation of capital across business lines based on the Euler\nallocation principle is then illustrated. The implementation of our model is\nperformed by estimating its parameters based on a car insurance data obtained\nfrom the General Insurance Statistical Agency (GISA), and by conducting\nnumerical simulations whose results are then presented.\n"
    },
    {
        "paper_id": 2007.07166,
        "authors": "Fabio Ciulla and Rosario N. Mantegna",
        "title": "Dynamics of fintech terms in news and blogs and specialization of\n  companies of the fintech industry",
        "comments": "11 pages, 4 figures, 4 tables",
        "journal-ref": null,
        "doi": "10.1063/5.0004487",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a large scale analysis of a list of fintech terms in (i) news and\nblogs in English language and (ii) professional descriptions of companies\noperating in many countries. The occurrence and co-occurrence of fintech terms\nand locutions shows a progressive evolution of the list of fintech terms in a\ncompact and coherent set of terms used worldwide to describe fintech business\nactivities. By using methods of complex networks that are specifically designed\nto deal with heterogeneous systems, our analysis of a large set of professional\ndescriptions of companies shows that companies having fintech terms in their\ndescription present over-expressions of specific attributes of country,\nmunicipality, and economic sector. By using the approach of statistically\nvalidated networks, we detect geographical and economic over-expressions of a\nset of companies related to the multi-industry, geographically and economically\ndistributed fintech movement.\n"
    },
    {
        "paper_id": 2007.07207,
        "authors": "Sana Ben Hamida and Wafa Abdelmalek and Fathi Abid",
        "title": "Applying Dynamic Training-Subset Selection Methods Using Genetic\n  Programming for Forecasting Implied Volatility",
        "comments": "20 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2006.16407",
        "journal-ref": "Wiley Computational Intelligence December 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility is a key variable in option pricing, trading and hedging\nstrategies. The purpose of this paper is to improve the accuracy of forecasting\nimplied volatility using an extension of genetic programming (GP) by means of\ndynamic training-subset selection methods. These methods manipulate the\ntraining data in order to improve the out of sample patterns fitting. When\napplied with the static subset selection method using a single training data\nsample, GP could generate forecasting models which are not adapted to some out\nof sample fitness cases. In order to improve the predictive accuracy of\ngenerated GP patterns, dynamic subset selection methods are introduced to the\nGP algorithm allowing a regular change of the training sample during evolution.\nFour dynamic training-subset selection methods are proposed based on random,\nsequential or adaptive subset selection. The latest approach uses an adaptive\nsubset weight measuring the sample difficulty according to the fitness cases\nerrors. Using real data from SP500 index options, these techniques are compared\nto the static subset selection method. Based on MSE total and percentage of non\nfitted observations, results show that the dynamic approach improves the\nforecasting performance of the generated GP models, specially those obtained\nfrom the adaptive random training subset selection method applied to the whole\nset of training samples.\n"
    },
    {
        "paper_id": 2007.07319,
        "authors": "Antonio Briola, Jeremy Turiel, Tomaso Aste",
        "title": "Deep Learning modeling of Limit Order Book: a comparative perspective",
        "comments": "16 pages, 4 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The present work addresses theoretical and practical questions in the domain\nof Deep Learning for High Frequency Trading. State-of-the-art models such as\nRandom models, Logistic Regressions, LSTMs, LSTMs equipped with an Attention\nmask, CNN-LSTMs and MLPs are reviewed and compared on the same tasks, feature\nspace and dataset, and then clustered according to pairwise similarity and\nperformance metrics. The underlying dimensions of the modeling techniques are\nhence investigated to understand whether these are intrinsic to the Limit Order\nBook's dynamics. We observe that the Multilayer Perceptron performs comparably\nto or better than state-of-the-art CNN-LSTM architectures indicating that\ndynamic spatial and temporal dimensions are a good approximation of the LOB's\ndynamics, but not necessarily the true underlying dimensions.\n"
    },
    {
        "paper_id": 2007.07353,
        "authors": "Ignacio Escanuela Romana",
        "title": "Keynesian models of depression. Supply shocks and the COVID-19 Crisis",
        "comments": "The last sentence has been deleted (\"a possible critical\n  quasi-experiment\")",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this work is twofold: to expand the depression models\nproposed by Tobin and analyse a supply shock, such as the Covid-19 pandemic, in\nthis Keynesian conceptual environment. The expansion allows us to propose the\nevolution of all endogenous macroeconomic variables. The result obtained is\nrelevant due to its theoretical and practical implications. A quantity or\nKeynesian adjustment to the shock produces a depression through the effect on\naggregate demand. This depression worsens in the medium/long-term. It is\naccompanied by increases in inflation, inflation expectations and the real\ninterest rate. A stimulus tax policy is also recommended, as well as an active\nmonetary policy to reduce real interest rates. On the other hand, the pricing\nor Marshallian adjustment foresees a more severe and rapid depression in the\nshort-term. There would be a reduction in inflation and inflation expectations,\nand an increase in the real interest rates. The tax or monetary stimulus\nmeasures would only impact inflation. This result makes it possible to clarify\nand assess the resulting depression, as well as propose policies. Finally, it\noffers conflicting predictions that allow one of the two models to be\nfalsified.\n"
    },
    {
        "paper_id": 2007.07487,
        "authors": "Fu Qiao, Yan Yan",
        "title": "How does stock market reflect the change in economic demand? A study on\n  the industry-specific volatility spillover networks of China's stock market\n  during the outbreak of COVID-19",
        "comments": "38 pages, 10 tables and 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the carefully selected industry classification standard, we divide 102\nindustry securities indices in China's stock market into four demand-oriented\nsector groups and identify demand-oriented industry-specific volatility\nspillover networks. The \"deman-oriented\" is a new idea of reconstructing the\nstructure of the networks considering the relationship between industry sectors\nand the economic demand their outputs meeting. Networks with the new structure\nhelp us improve the understanding of the economic demand change, especially\nwhen the macroeconomic is dramatically influenced by exogenous shocks like the\noutbreak of COVID-19. At the beginning of the outbreak of COVID-19, in China's\nstock market, spillover effects from industry indices of sectors meeting the\ninvestment demand to those meeting the consumption demands rose significantly.\nHowever, these spillover effects fell after the outbreak containment in China\nappeared to be effective. Besides, some services sectors including utility,\ntransportation and information services have played increasingly important\nroles in the networks of industry-specific volatility spillovers as of the\nCOVID-19 out broke. By implication, firstly, being led by Chinese government,\nthe COVID-19 is successfully contained and the work resumption is organized\nwith a high efficiency in China. The risk of the investment demand therefore\nwas controlled and eliminated relatively fast. Secondly, the intensive using of\nnon-pharmaceutical interventions (NPIs) led to supply restriction in services\nin China. It will still be a potential threat for the Chinese economic recovery\nin the next stage.\n"
    },
    {
        "paper_id": 2007.07701,
        "authors": "Fabio Antonelli, Alessandro Ramponi, Sergio Scarlatti",
        "title": "Approximate XVA for European claims",
        "comments": "24 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of computing the Value Adjustment of European\ncontingent claims when default of either party is considered, possibly\nincluding also funding and collateralization requirements. As shown in Brigo et\nal. (\\cite{BLPS}, \\cite{BFP}), this leads to a more articulate variety of Value\nAdjustments ({XVA}) that introduce some nonlinear features. When exploiting a\nreduced-form approach for the default times, the adjusted price can be\ncharacterized as the solution to a possibly nonlinear Backward Stochastic\nDifferential Equation (BSDE). The expectation representing the solution of the\nBSDE is usually quite hard to compute even in a Markovian setting, and one\nmight resort either to the discretization of the Partial Differential Equation\ncharacterizing it or to Monte Carlo Simulations. Both choices are\ncomputationally very expensive and in this paper we suggest an approximation\nmethod based on an appropriate change of numeraire and on a Taylor's polynomial\nexpansion when intensities are represented by means of affine processes\ncorrelated with the asset's price. The numerical discussion at the end of this\nwork shows that, at least in the case of the CIR intensity model, even the\nsimple first-order approximation has a remarkable computational efficiency.\n"
    },
    {
        "paper_id": 2007.07839,
        "authors": "Ugur Korkut Pata",
        "title": "COVID-19 Induced Economic Uncertainty: A Comparison between the United\n  Kingdom and the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this study is to investigate the effects of the COVID-19\npandemic on economic policy uncertainty in the US and the UK. The impact of the\nincrease in COVID-19 cases and deaths in the country, and the increase in the\nnumber of cases and deaths outside the country may vary. To examine this, the\nstudy employs bootstrap ARDL cointegration approach from March 8, 2020 to May\n24, 2020. According to the bootstrap ARDL results, a long-run equilibrium\nrelationship is confirmed for five out of the 10 models. The long-term\ncoefficients obtained from the ARDL models suggest that an increase in COVID-19\ncases and deaths outside of the UK and the US has a significant effect on\neconomic policy uncertainty. The US is more affected by the increase in the\nnumber of COVID-19 cases. The UK, on the other hand, is more negatively\naffected by the increase in the number of COVID-19 deaths outside the country\nthan the increase in the number of cases. Moreover, another important finding\nfrom the study demonstrates that COVID-19 is a factor of great uncertainty for\nboth countries in the short-term.\n"
    },
    {
        "paper_id": 2007.07963,
        "authors": "Keisuke Kokubun, Yoshiaki Ino, Kazuyoshi Ishimura",
        "title": "Social capital and resilience make an employee cooperate for coronavirus\n  measures and lower his/her turnover intention",
        "comments": "amendment in Table 1",
        "journal-ref": "International Journal of Workplace Health Management, (2022)",
        "doi": "10.1108/IJWHM-07-2021-0142",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important theme is how to maximize the cooperation of employees when\ndealing with crisis measures taken by the company. Therefore, to find out what\nkind of employees have cooperated with the company's measures in the current\ncorona (COVID-19) crisis, and what effect the cooperation has had to these\nemployees/companies to get hints for preparing for the next crisis, the pass\nanalysis was carried out using awareness data obtained from a questionnaire\nsurvey conducted on 2,973 employees of Japanese companies in China. The results\nshowed that employees with higher social capital and resilience were more\nsupportive of the company's measures against corona and that employees who were\nmore supportive of corona measures were less likely to leave their jobs.\nHowever, regarding fatigue and anxiety about the corona felt by employees, it\nwas shown that it not only works to support cooperation in corona\ncountermeasures but also enhances the turnover intention. This means that just\nby raising the anxiety of employees, even if a company achieves the short-term\ngoal of having them cooperate with the company's countermeasures against\ncorona, it may not reach the longer-term goal by making them increase their\nintention to leave. It is important for employees to be aware of the crisis and\nto fear it properly. But more than that, it should be possible for the company\nto help employees stay resilient, build good relationships with them, and\nincrease their social capital to make them support crisis measurement of the\ncompany most effectively while keeping their turnover intention low.\n"
    },
    {
        "paper_id": 2007.07998,
        "authors": "David Marcos",
        "title": "Transaction Costs in Execution Trading",
        "comments": "MSc Thesis. Uploaded by request",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In the present work we develop a formalism to tackle the problem of optimal\nexecution when trading market securities. More precisely, we introduce a\nutility function that balances market impact and timing risk, with this last\nbeing modelled as the very negative transaction costs incurred by our order\nexecution. The framework is built upon existing theory on optimal trading\nstrategies, but incorporates characteristics that enable distinctive execution\nstrategies. The formalism is complemented by an analysis of various impact\nmodels and different distributional properties of market returns.\n"
    },
    {
        "paper_id": 2007.08115,
        "authors": "Vassilis Polimenis",
        "title": "Uncovering a factor-based expected return conditioning structure with\n  Regression Trees jointly for many stocks",
        "comments": "11 pages, 5 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given the success and almost universal acceptance of the simple linear\nregression three-factor model, it is interesting to analyze the informational\ncontent of the three factors in explaining stock returns when the analysis is\nallowed to consider non-linear dependencies between factors and stock returns.\nIn order to better understand factor-based conditioning information with\nrespect to expected stock returns within a regression tree setting, the\nanalysis of stock returns is demonstrated using daily stock return data for 5\nmajor US corporations. The first finding is that in all cases (solo and joint)\nthe most informative factor is always the market excess return factor. Further,\nthree major issues are discussed: a) the balance of a depth=1 tree as it\nrelates to properties of the stock return distribution, b) the mechanism behind\ndepth=1 tree balance in a joint regression tree and c) the dominant stock in a\njoint regression tree. It is shown that high skew values alone cannot explain\nthe imbalance of the resulting tree split as stocks with pronounced skew may\nproduce balanced tree splits.\n"
    },
    {
        "paper_id": 2007.08376,
        "authors": "Daniel Bartl, Michael Kupper, Ariel Neufeld",
        "title": "Duality Theory for Robust Utility Maximisation",
        "comments": null,
        "journal-ref": "Finance and Stochastics, 2021+",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a duality theory for the robust utility maximisation\nproblem in continuous time for utility functions defined on the positive real\naxis. Our results are inspired by -- and can be seen as the robust analogues of\n-- the seminal work of Kramkov & Schachermayer [18]. Namely, we show that if\nthe set of attainable trading outcomes and the set of pricing measures satisfy\na bipolar relation, then the utility maximisation problem is in duality with a\nconjugate problem. We further discuss the existence of optimal trading\nstrategies. In particular, our general results include the case of logarithmic\nand power utility, and they apply to drift and volatility uncertainty.\n"
    },
    {
        "paper_id": 2007.08475,
        "authors": "J{\\o}rgen Vitting Andersen and Andrzej Nowak",
        "title": "Symmetry and financial Markets",
        "comments": "27 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is hard to overstate the importance that the concept of symmetry has had\nin every field of physics, a fact alluded to by the Nobel Prize winner P.W.\nAnderson, who once wrote that physics is the study of symmetry. Whereas the\nidea of symmetry is widely used in science in general, very few (if not almost\nno) applications has found its way into the field of finance. Still, the\nphenomenon appears relevant in terms of for example the symmetry of strategies\nthat can happen in the decision making to buy or sell financial shares. Game\ntheory is therefore one obvious avenue where to look for symmetry, but as will\nbe shown, also technical analysis and long term economic growth could be\nphenomena which show the hallmark of a symmetry\n"
    },
    {
        "paper_id": 2007.08523,
        "authors": "Matteo Bizzarri, Fabrizio Panebianco, Paolo Pin",
        "title": "Epidemic dynamics with homophily, vaccination choices, and pseudoscience\n  attitudes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We interpret attitudes towards science and pseudosciences as cultural traits\nthat diffuse in society through communication efforts exerted by agents. We\npresent a tractable model that allows us to study the interaction among the\ndiffusion of an epidemic, vaccination choices, and the dynamics of cultural\ntraits. We apply it to study the impact of homophily between pro-vaxxers and\nanti-vaxxers on the total number of cases (the cumulative infection). We show\nthat, during the outbreak of a disease, homophily has the direct effect of\ndecreasing the speed of recovery. Hence, it may increase the number of cases\nand make the disease endemic. The dynamics of the shares of the two cultural\ntraits in the population is crucial in determining the sign of the total effect\non the cumulative infection: more homophily is beneficial if agents are not too\nflexible in changing their cultural trait, is detrimental otherwise.\n"
    },
    {
        "paper_id": 2007.08804,
        "authors": "Karim Barigou (SAF), Lukasz Delong",
        "title": "Pricing equity-linked life insurance contracts with multiple risk\n  factors by neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the pricing of equity-linked life insurance contracts\nwith death and survival benefits in a general model with multiple stochastic\nrisk factors: interest rate, equity, volatility, unsystematic and systematic\nmortality. We price the equity-linked contracts by assuming that the insurer\nhedges the risks to reduce the local variance of the net asset value process\nand requires a compensation for the non-hedgeable part of the liability in the\nform of an instantaneous standard deviation risk margin. The price can then be\nexpressed as the solution of a system of non-linear partial differential\nequations. We reformulate the problem as a backward stochastic differential\nequation with jumps and solve it numerically by the use of efficient neural\nnetworks. Sensitivity analysis is performed with respect to initial parameters\nand an analysis of the accuracy of the approximation of the true price with our\nneural networks is provided.\n"
    },
    {
        "paper_id": 2007.08815,
        "authors": "Daniel Bartl, Stephan Eckstein, Michael Kupper",
        "title": "Limits of random walks with distributionally robust transition\n  probabilities",
        "comments": "14 pages, forthcoming in ECP",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a nonlinear random walk which, in each time step, is free to\nchoose its own transition probability within a neighborhood (w.r.t. Wasserstein\ndistance) of the transition probability of a fixed L\\'evy process. In analogy\nto the classical framework we show that, when passing from discrete to\ncontinuous time via a scaling limit, this nonlinear random walk gives rise to a\nnonlinear semigroup. We explicitly compute the generator of this semigroup and\ncorresponding PDE as a perturbation of the generator of the initial L\\'evy\nprocess.\n"
    },
    {
        "paper_id": 2007.08829,
        "authors": "Matteo Burzoni, Cosimo Munari, Ruodu Wang",
        "title": "Adjusted Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and study the main properties of a class of convex risk measures\nthat refine Expected Shortfall by simultaneously controlling the expected\nlosses associated with different portions of the tail distribution. The\ncorresponding adjusted Expected Shortfalls quantify risk as the minimum amount\nof capital that has to be raised and injected into a financial position $X$ to\nensure that Expected Shortfall $ES_p(X)$ does not exceed a pre-specified\nthreshold $g(p)$ for every probability level $p\\in[0,1]$. Through the choice of\nthe benchmark risk profile $g$ one can tailor the risk assessment to the\nspecific application of interest. We devote special attention to the study of\nrisk profiles defined by the Expected Shortfall of a benchmark random loss, in\nwhich case our risk measures are intimately linked to second-order stochastic\ndominance.\n"
    },
    {
        "paper_id": 2007.09043,
        "authors": "Matthieu Garcin, Jules Klein, Sana Laaribi",
        "title": "Estimation of time-varying kernel densities and chronology of the impact\n  of COVID-19 on financial markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The time-varying kernel density estimation relies on two free parameters: the\nbandwidth and the discount factor. We propose to select these parameters so as\nto minimize a criterion consistent with the traditional requirements of the\nvalidation of a probability density forecast. These requirements are both the\nuniformity and the independence of the so-called probability integral\ntransforms, which are the forecast time-varying cumulated distributions applied\nto the observations. We thus build a new numerical criterion incorporating both\nthe uniformity and independence properties by the mean of an adapted\nKolmogorov-Smirnov statistic. We apply this method to financial markets during\nthe COVID-19 crisis. We determine the time-varying density of daily price\nreturns of several stock indices and, using various divergence statistics, we\nare able to describe the chronology of the crisis as well as regional\ndisparities. For instance, we observe a more limited impact of COVID-19 on\nfinancial markets in China, a strong impact in the US, and a slow recovery in\nEurope.\n"
    },
    {
        "paper_id": 2007.09201,
        "authors": "Matthew Lorig",
        "title": "Bond indifference prices and indifference yield curves",
        "comments": "11 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a market with stochastic interest rates, we consider an investor who can\neither (i) invest all if his money in a savings account or (ii) purchase\nzero-coupon bonds and invest the remainder of his wealth in a savings account.\nThe indifference price of the bond is the price for which the investor could\nachieve the same expected utility under both scenarios. In an affine term\nstructure setting, under the assumption that an investor has a utility function\nin either exponential or power form, we show that the indifference price of a\nzero-coupon bond is the root of an integral expression. As an example, we\ncompute bond indifference prices and the corresponding indifference yield\ncurves in the Vasicek setting and interpret the results.\n"
    },
    {
        "paper_id": 2007.0923,
        "authors": "Daniel L. Mendoza (1 and 2), Cheryl S. Pirozzi (1), Erik T. Crosman\n  (3), Theodore G. Liou (1 and 4), Yue Zhang (5), Jessica J. Cleeves (6),\n  Stephen C. Bannister (7), William R.L. Anderegg (8), Robert Paine III (1)\n  ((1) Division of Respiratory, Critical Care and Occupational Pulmonary\n  Medicine, School of Medicine, University of Utah, (2) Department of\n  Atmospheric Sciences, University of Utah, (3) Department of Life, Earth, and\n  Environmental Sciences, West Texas A&M University, (4) Center for\n  Quantitative Biology, University of Utah, (5) Division of Epidemiology,\n  Department of Internal Medicine, University of Utah School of Medicine, (6)\n  Center for Science and Mathematics Education, University of Utah, (7)\n  Department of Economics, University of Utah, (8) School of Biological\n  Sciences, University of Utah)",
        "title": "Absentee and Economic Impact of Low-Level Fine Particulate Matter and\n  Ozone Exposure in K-12 Students",
        "comments": "31 pages, 11 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.12720.17925",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High air pollution levels are associated with school absences. However, low\nlevel pollution impact on individual school absences are under-studied. We\nmodelled PM2.5 and ozone concentrations at 36 schools from July 2015 to June\n2018 using data from a dense, research grade regulatory sensor network. We\ndetermined exposures and daily absences at each school. We used generalized\nestimating equations model to retrospectively estimate rate ratios for\nassociation between outdoor pollutant concentrations and school absences. We\nestimated lost school revenue, productivity, and family economic burden. PM2.5\nand ozone concentrations and absence rates vary across the School District.\nPollution exposure were associated with as high a rate ratio of 1.02 absences\nper ug/m$^3$ and 1.01 per ppb increase for PM2.5 and ozone, respectively.\nSignificantly, even PM2.5 and ozone exposure below regulatory standards (<12.1\nug/m$^3$ and <55 ppb) was associated with positive rate ratios of absences:\n1.04 per ug/m$^3$ and 1.01 per ppb increase, respectively. Granular local\nmeasurements enabled demonstration of air pollution impacts that varied between\nschools undetectable with averaged pollution levels. Reducing pollution by 50%\nwould save $452,000 per year districtwide. Pollution reduction benefits would\nbe greatest in schools located in socioeconomically disadvantaged areas.\nExposures to air pollution, even at low levels, are associated with increased\nschool absences. Heterogeneity in exposure, disproportionately affecting\nsocioeconomically disadvantaged schools, points to the need for fine resolution\nexposure estimation. The economic cost of absences associated with air\npollution is substantial even excluding indirect costs such as hospital visits\nand medication. These findings may help inform decisions about recess during\nsevere pollution events and regulatory considerations for localized pollution\nsources.\n"
    },
    {
        "paper_id": 2007.0932,
        "authors": "Jose Blanchet, Henry Lam, Yang Liu, Ruodu Wang",
        "title": "Convolution Bounds on Quantile Aggregation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantile aggregation with dependence uncertainty has a long history in\nprobability theory with wide applications in finance, risk management,\nstatistics, and operations research. Using a recent result on inf-convolution\nof quantile-based risk measures, we establish new analytical bounds for\nquantile aggregation which we call convolution bounds. Convolution bounds both\nunify every analytical result available in quantile aggregation and enlighten\nour understanding of these methods. These bounds are the best available in\ngeneral. Moreover, convolution bounds are easy to compute, and we show that\nthey are sharp in many relevant cases. They also allow for interpretability on\nthe extremal dependence structure. The results directly lead to bounds on the\ndistribution of the sum of random variables with arbitrary dependence. We\ndiscuss relevant applications in risk management and economics.\n"
    },
    {
        "paper_id": 2007.09349,
        "authors": "Baishuai Zuo, Chuancun Yin, Narayanaswamy Balakrishnan",
        "title": "Explicit expressions for joint moments of $n$-dimensional elliptical\n  distributions",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by Stein's lemma, we derive two expressions for the joint moments of\nelliptical distributions. We use two different methods to derive\n$E[X_{1}^{2}f(\\mathbf{X})]$ for any measurable function $f$ satisfying some\nregularity conditions. Then, by applying this result, we obtain new formulae\nfor expectations of product of normally distributed random variables, and also\npresent simplified expressions of $E[X_{1}^{2}f(\\mathbf{X})]$ for multivariate\nStudent-$t$, logistic and Laplace distributions.\n"
    },
    {
        "paper_id": 2007.0935,
        "authors": "Baishuai Zuo, Chuancun Yin",
        "title": "Conditional tail risk expectations for location-scale mixture of\n  elliptical distributions",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present general results on the univariate tail conditional expectation\n(TCE) and multivariate tail conditional expectation for location-scale mixture\nof elliptical distributions. Examples include the location-scale mixture of\nnormal distributions, location-scale mixture of Student-$t$ distributions,\nlocation-scale mixture of Logistic distributions and location-scale mixture of\nLaplace distributions. We also consider portfolio risk decomposition with TCE\nfor location-scale mixture of elliptical distributions.\n"
    },
    {
        "paper_id": 2007.09419,
        "authors": "Jonas Al-Hadad and Zbigniew Palmowski",
        "title": "Perpetual American options with asset-dependent discounting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the following optimal stopping problem\n$$V^{\\omega}_{\\rm A}(s) = \\sup_{\\tau\\in\\mathcal{T}}\n\\mathbb{E}_{s}[e^{-\\int_0^\\tau \\omega(S_w) dw} g(S_\\tau)],$$ where the process\n$S_t$ is a jump-diffusion process, $\\mathcal{T}$ is a family of stopping times\nwhile $g$ and $\\omega$ are fixed payoff function and discount function,\nrespectively. In a financial market context, if $g(s)=(K-s)^+$ or\n$g(s)=(s-K)^+$ and $\\mathbb{E}$ is the expectation taken with respect to a\nmartingale measure, $V^{\\omega}_{\\rm A}(s)$ describes the price of a perpetual\nAmerican option with a discount rate depending on the value of the asset\nprocess $S_t$. If $\\omega$ is a constant, the above problem produces the\nstandard case of pricing perpetual American options. In the first part of this\npaper we find sufficient conditions for the convexity of the value function\n$V^{\\omega}_{\\rm A}(s)$. This allows us to determine the stopping region as a\ncertain interval and hence we are able to identify the form of $V^{\\omega}_{\\rm\nA}(s)$. We also prove a put-call symmetry for American options with\nasset-dependent discounting. In the case when $S_t$ is a geometric L\\'evy\nprocess we give exact expressions using the so-called omega scale functions\nintroduced in Li and Palmowski (2018). We prove that the analysed value\nfunction satisfies the HJB equation and we give sufficient conditions for the\nsmooth fit property as well. Finally, we present a few examples for which we\nobtain the analytical form of the value function $V^{\\omega}_{\\rm A}(s)$.\n"
    },
    {
        "paper_id": 2007.09566,
        "authors": "Mudit Kapoor, Anup Malani, Shamika Ravi, Arnav Agrawal",
        "title": "Authoritarian Governments Appear to Manipulate COVID Data",
        "comments": "23 pages (including 5 figures and 3 tables)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Because SARS-Cov-2 (COVID-19) statistics affect economic policies and\npolitical outcomes, governments have an incentive to control them. Manipulation\nmay be less likely in democracies, which have checks to ensure transparency. We\nshow that data on disease burden bear indicia of data modification by\nauthoritarian governments relative to democratic governments. First, data on\nCOVID-19 cases and deaths from authoritarian governments show significantly\nless variation from a 7 day moving average. Because governments have no reason\nto add noise to data, lower deviation is evidence that data may be massaged.\nSecond, data on COVID-19 deaths from authoritarian governments do not follow\nBenford's law, which describes the distribution of leading digits of numbers.\nDeviations from this law are used to test for accounting fraud. Smoothing and\nadjustments to COVID-19 data may indicate other alterations to these data and a\nneed to account for such alterations when tracking the disease.\n"
    },
    {
        "paper_id": 2007.09911,
        "authors": "Wen Chen, Nicolas Langren\\'e",
        "title": "Deep neural network for optimal retirement consumption in defined\n  contribution pension system",
        "comments": "19 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a deep neural network approach to solve a lifetime\nexpected mortality-weighted utility-based model for optimal consumption in the\ndecumulation phase of a defined contribution pension system. We formulate this\nproblem as a multi-period finite-horizon stochastic control problem and train a\ndeep neural network policy representing consumption decisions. The optimal\nconsumption policy is determined by personal information about the retiree such\nas age, wealth, risk aversion and bequest motive, as well as a series of\neconomic and financial variables including inflation rates and asset returns\njointly simulated from a proposed seven-factor economic scenario generator\ncalibrated from market data. We use the Australian pension system as an\nexample, with consideration of the government-funded means-tested Age Pension\nand other practical aspects such as fund management fees. The key findings from\nour numerical tests are as follows. First, our deep neural network optimal\nconsumption policy, which adapts to changes in market conditions, outperforms\ndeterministic drawdown rules proposed in the literature. Moreover, the\nout-of-sample outperformance ratios increase as the number of training\niterations increases, eventually reaching outperformance on all testing\nscenarios after less than 10 minutes of training. Second, a sensitivity\nanalysis is performed to reveal how risk aversion and bequest motives change\nthe consumption over a retiree's lifetime under this utility framework. Third,\nwe provide the optimal consumption rate with different starting wealth\nbalances. We observe that optimal consumption rates are not proportional to\ninitial wealth due to the Age Pension payment. Forth, with the same initial\nwealth balance and utility parameter settings, the optimal consumption level is\ndifferent between males and females due to gender differences in mortality.\n"
    },
    {
        "paper_id": 2007.09939,
        "authors": "Keisuke Kokubun",
        "title": "Social capital may mediate the relationship between social distance and\n  COVID-19 prevalence",
        "comments": "amendments in references",
        "journal-ref": "INQUIRY, Vol. 58 (2021) 1-11",
        "doi": "10.1177/00469580211005189",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The threat of the new coronavirus (COVID-19) is increasing. Regarding the\ndifference in the infection rate observed in each region, in addition to\nstudies seeking the cause due to differences in the social distance (population\ndensity), there is an increasing trend toward studies seeking the cause due to\ndifferences in social capital. However, studies have not yet been conducted on\nwhether social capital could influence the infection rate even if it controls\nthe effect of population density. Therefore, in this paper, we analyzed the\nrelationship between infection rate, population density, and social capital\nusing statistical data for each prefecture. Statistical analysis showed that\nsocial capital not only correlates with infection rates and population\ndensities but still has a negative correlation with infection rates controlling\nfor the effects of population density. Besides, controlling the relationship\nbetween variables by mean age showed that social capital had a greater\ncorrelation with infection rate than population density. In other words, social\ncapital mediates the correlation between population density and infection\nrates. This means that social distance alone is not enough to deter coronavirus\ninfection, and social capital needs to be recharged.\n"
    },
    {
        "paper_id": 2007.10268,
        "authors": "David Roodman",
        "title": "The impacts of incarceration on crime",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper reviews the research on the impacts of incarceration on crime.\nWhere data availability permits, reviewed studies are replicated and\nreanalyzed. Among three dozen studies I reviewed, I obtained or reconstructed\nthe data and code for eight. Replication and reanalysis revealed significant\nmethodological concerns in seven and led to major reinterpretations of four. I\nestimate that, at typical policy margins in the United States today,\ndecarceration has zero net impact on crime outside of prison. That estimate is\nuncertain, but at least as much evidence suggests that decarceration reduces\ncrime as increases it. The crux of the matter is that tougher sentences hardly\ndeter crime, and that while imprisoning people temporarily stops them from\ncommitting crime outside prison walls, it also tends to increase their\ncriminality after release. As a result, \"tough-on-crime\" initiatives can reduce\ncrime in the short run but cause offsetting harm in the long run. A\ncost-benefit analysis finds that even under a devil's advocate reading of this\nevidence, in which incarceration does reduce crime in U.S., it is unlikely to\nincrease aggregate welfare.\n"
    },
    {
        "paper_id": 2007.10269,
        "authors": "David Roodman",
        "title": "The domestic economic impacts of immigration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper critically reviews the research on the impact of immigration on\nemployment and wages of natives in wealthy countries--where \"natives\" includes\nprevious immigrants and their descendants. While written for a non-technical\naudience, the paper engages with technical issues and probes one particularly\nintense scholarly debate in an appendix. While the available evidence is not\ndefinitive, it paints a consistent picture. Industrial economies can generally\nabsorb migrants quickly, in part because capital is mobile and flexible, in\npart because immigrants are consumers as well as producers. Thus, long-term\naverage impacts are probably zero at worst. And the long-term may come quickly,\nespecially if migration inflows are predictable to investors. Possibly, skilled\nimmigration boosts productivity and wages for many others. Around the averages,\nthere are distributional effects. Among low-income \"native\" workers, the ones\nwho stand to lose the most are those who most closely resemble new arrivals, in\nbeing immigrants themselves, being low-skill, being less assimilated, and,\npotentially, undocumented. But native workers and earlier immigrants tend to\nbenefit from the arrival of workers different from them, who complement more\nthan compete with them in production. Thus skilled immigration can offset the\neffects of low-skill immigration on natives and earlier immigrants.\n"
    },
    {
        "paper_id": 2007.1027,
        "authors": "David Roodman",
        "title": "The impacts of alcohol taxes: A replication review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper reviews the research on the impacts of alcohol taxation outcomes\nsuch as heavy drinking and mortality. Where data availability permits, reviewed\nstudies are replicated and reanalyzed. Despite weaknesses in the majority of\nstudies, and despite seeming disagreements among the more credible one--ones\nbased on natural experiments--we can be reasonably confident that taxing\nalcohol reduces drinking in general and problem drinking in particular. The\nlarger and cleaner the underlying natural experiment, the more apt a study is\nto detect impacts on drinking. Estimates from the highest-powered study\nsettings, such as in Alaska in 2002 and Finland in 2004, suggest an elasticity\nof mortality with respect to price of -1 to -3. A 10% price increase in the US\nwould, according to this estimate, save 2,000-6,000 lives and 48,000-130,000\nyears of life each year.\n"
    },
    {
        "paper_id": 2007.10415,
        "authors": "Ariel Ortiz-Bobea, Toby R. Ault, Carlos M. Carrillo, Robert G.\n  Chambers and David B. Lobell",
        "title": "The Historical Impact of Anthropogenic Climate Change on Global\n  Agricultural Productivity",
        "comments": "paper (14 pages; 5 figures) and supplemental materials (24 pages; 14\n  figures and 4 tables)",
        "journal-ref": "Nat. Clim. Chang. 11, 306-312 (2021)",
        "doi": "10.1038/s41558-021-01000-1",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Agricultural research has fostered productivity growth, but the historical\ninfluence of anthropogenic climate change on that growth has not been\nquantified. We develop a robust econometric model of weather effects on global\nagricultural total factor productivity (TFP) and combine this model with\ncounterfactual climate scenarios to evaluate impacts of past climate trends on\nTFP. Our baseline model indicates that anthropogenic climate change has reduced\nglobal agricultural TFP by about 21% since 1961, a slowdown that is equivalent\nto losing the last 9 years of productivity growth. The effect is substantially\nmore severe (a reduction of ~30-33%) in warmer regions such as Africa and Latin\nAmerica and the Caribbean. We also find that global agriculture has grown more\nvulnerable to ongoing climate change.\n"
    },
    {
        "paper_id": 2007.10462,
        "authors": "Marc Chataigner, St\\'ephane Cr\\'epey and Matthew Dixon",
        "title": "Deep Local Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep learning for option pricing has emerged as a novel methodology for fast\ncomputations with applications in calibration and computation of Greeks.\nHowever, many of these approaches do not enforce any no-arbitrage conditions,\nand the subsequent local volatility surface is never considered. In this\narticle, we develop a deep learning approach for interpolation of European\nvanilla option prices which jointly yields the full surface of local\nvolatilities. We demonstrate the modification of the loss function or the feed\nforward network architecture to enforce (hard constraints approach) or favor\n(soft constraints approach) the no-arbitrage conditions and we specify the\nexperimental design parameters that are needed for adequate performance. A\nnovel component is the use of the Dupire formula to enforce bounds on the local\nvolatility associated with option prices, during the network fitting. Our\nmethodology is benchmarked numerically on real datasets of DAX vanilla options.\n"
    },
    {
        "paper_id": 2007.10564,
        "authors": "Peng-Fei Dai, Xiong Xiong, Toan Luu Duc Huynh, Jiqiang Wang",
        "title": "The impact of economic policy uncertainties on the volatility of\n  European carbon market",
        "comments": "26 pages, 1 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European Union Emission Trading Scheme is a carbon emission allowance\ntrading system designed by Europe to achieve emission reduction targets. The\namount of carbon emission caused by production activities is closely related to\nthe socio-economic environment. Therefore, from the perspective of economic\npolicy uncertainty, this article constructs the GARCH-MIDAS-EUEPU and\nGARCH-MIDAS-GEPU models for investigating the impact of European and global\neconomic policy uncertainty on carbon price fluctuations. The results show that\nboth European and global economic policy uncertainty will exacerbate the\nlong-term volatility of European carbon spot return, with the latter having a\nstronger impact when the change is the same. Moreover, the volatility of the\nEuropean carbon spot return can be forecasted better by the predictor, global\neconomic policy uncertainty. This research can provide some implications for\nmarket managers in grasping carbon market trends and helping participants\ncontrol the risk of fluctuations in carbon allowances.\n"
    },
    {
        "paper_id": 2007.10643,
        "authors": "Tiziano De Angelis, Nikita Merkulov, Jan Palczewski",
        "title": "On the value of non-Markovian Dynkin games with partial and asymmetric\n  information",
        "comments": "Changes include the proof of existence of a saddle point for the game\n  and inclusion of a separate payoff when players stop at the same time. There\n  are also some editorial changes and improvements in presentation",
        "journal-ref": null,
        "doi": "10.1214/21-AAP1721",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that zero-sum Dynkin games in continuous time with partial and\nasymmetric information admit a value in randomised stopping times when the\nstopping payoffs of the players are general \\cadlag measurable processes. As a\nby-product of our method of proof we also obtain existence of optimal\nstrategies for both players. The main novelties are that we do not assume a\nMarkovian nature of the game nor a particular structure of the information\navailable to the players. This allows us to go beyond the variational methods\n(based on PDEs) developed in the literature on Dynkin games in continuous time\nwith partial/asymmetric information. Instead, we focus on a probabilistic and\nfunctional analytic approach based on the general theory of stochastic\nprocesses and Sion's min-max theorem (M. Sion, Pacific J. Math., 8, 1958, pp.\n171-176). Our framework encompasses examples found in the literature on\ncontinuous time Dynkin games with asymmetric information and we provide\ncounterexamples to show that our assumptions cannot be further relaxed.\n"
    },
    {
        "paper_id": 2007.10727,
        "authors": "Ayoub Ammy-Driss and Matthieu Garcin",
        "title": "Efficiency of the financial markets during the COVID-19 crisis:\n  time-varying parameters of fractional stable dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the impact of COVID-19 on financial markets. It\nfocuses on the evolution of the market efficiency, using two efficiency\nindicators: the Hurst exponent and the memory parameter of a fractional\nL\\'evy-stable motion. The second approach combines, in the same model of\ndynamic, an alpha-stable distribution and a dependence structure between price\nreturns. We provide a dynamic estimation method for the two efficiency\nindicators. This method introduces a free parameter, the discount factor, which\nwe select so as to get the best alpha-stable density forecasts for observed\nprice returns. The application to stock indices during the COVID-19 crisis\nshows a strong loss of efficiency for US indices. On the opposite, Asian and\nAustralian indices seem less affected and the inefficiency of these markets\nduring the COVID-19 crisis is even questionable.\n"
    },
    {
        "paper_id": 2007.10805,
        "authors": "Suneel Sarswat and Abhishek Kr Singh",
        "title": "Formally Verified Trades in Financial Markets",
        "comments": "Aceepted in ICFEM 2020. arXiv admin note: substantial text overlap\n  with arXiv:1907.07885",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a formal framework for analyzing trades in financial markets.\nThese days, all big exchanges use computer algorithms to match buy and sell\nrequests and these algorithms must abide by certain regulatory guidelines. For\nexample, market regulators enforce that a matching produced by exchanges should\nbe fair, uniform and individual rational. To verify these properties of trades,\nwe first formally define these notions in a theorem prover and then develop\nmany important results about matching demand and supply. Finally, we use this\nframework to verify properties of two important classes of double sided auction\nmechanisms. All the definitions and results presented in this paper are\ncompletely formalized in the Coq proof assistant without adding any additional\naxioms to it.\n"
    },
    {
        "paper_id": 2007.11098,
        "authors": "Omid Safarzadeh",
        "title": "Generating Trading Signals by ML algorithms or time series ones?",
        "comments": "20 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research investigates efficiency on-line learning Algorithms to generate\ntrading signals.I employed technical indicators based on high frequency stock\nprices and generated trading signals through ensemble of Random Forests.\nSimilarly, Kalman Filter was used for signaling trading positions. Comparing\nTime Series methods with Machine Learning methods, results spurious of Kalman\nFilter to Random Forests in case of on-line learning predictions of stock\nprices\n"
    },
    {
        "paper_id": 2007.11201,
        "authors": "Vishal Keswani, Sakshi Singh, Ashutosh Modi",
        "title": "IITK at the FinSim Task: Hypernym Detection in Financial Domain via\n  Context-Free and Contextualized Word Embeddings",
        "comments": "6 pages, 1 figure, 4 tables. Accepted at the Second Workshop on\n  Financial Technology and Natural Language Processing (FinNLP-2020)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present our approaches for the FinSim 2020 shared task on\n\"Learning Semantic Representations for the Financial Domain\". The goal of this\ntask is to classify financial terms into the most relevant hypernym (or\ntop-level) concept in an external ontology. We leverage both context-dependent\nand context-independent word embeddings in our analysis. Our systems deploy\nWord2vec embeddings trained from scratch on the corpus (Financial Prospectus in\nEnglish) along with pre-trained BERT embeddings. We divide the test dataset\ninto two subsets based on a domain rule. For one subset, we use unsupervised\ndistance measures to classify the term. For the second subset, we use simple\nsupervised classifiers like Naive Bayes, on top of the embeddings, to arrive at\na final prediction. Finally, we combine both the results. Our system ranks 1st\nbased on both the metrics, i.e., mean rank and accuracy.\n"
    },
    {
        "paper_id": 2007.11388,
        "authors": "David Roodman",
        "title": "The impact of life-saving interventions on fertility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Many interventions in global health save lives. One criticism sometimes\nlobbed at these interventions invokes the spirit of Malthus. The good done, the\ncharge goes, is offset by the harm of spreading the earth's limited resources\nmore thinly: more people, and more misery per person. To the extent this holds,\nthe net benefit of savings lives is lower than it appears at first. On the\nother hand, if lower mortality, especially in childhood, leads families to have\nfewer children, life-saving interventions could reduce population. This\ndocument critically reviews the evidence. It finds that the impact of\nlife-saving interventions on fertility and population growth varies by context,\nand is rarely greater than 1:1. In places where lifetime births/woman has been\nconverging to 2 or lower, saving one child's life should lead parents to avert\na birth they would otherwise have. The impact of mortality drops on fertility\nwill be nearly 1:1, so population growth will hardly change. In the\nincreasingly exceptional locales where couples appear not to limit fertility\nmuch, such as Niger and Mali, the impact of saving a life on total births will\nbe smaller, and may come about mainly through the biological channel of\nlactational amenorrhea. Here, mortality-drop-fertility-drop ratios of 1:0.5 and\n1:0.33 appear more plausible. But in the long-term, it would be surprising if\nthese few countries do not join the rest of the world in the transition to\nlower and more intentionally controlled fertility.\n"
    },
    {
        "paper_id": 2007.11407,
        "authors": "Ionut Jianu",
        "title": "Examining the drivers of business cycle divergence between Euro Area and\n  Romania",
        "comments": "14 pages",
        "journal-ref": "Theoretical and Applied Economics, Vol. 27 (2020), No. 2",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research aims to provide an explanatory analyses of the business cycles\ndivergence between Euro Area and Romania, respectively its drivers, since the\nsynchronisation of output-gaps is one of the most important topic in the\ncontext of a potential EMU accession. According to the estimates, output-gaps\nsynchronisation entered on a downward path in the subperiod 2010-2017, compared\nto 2002-2009. The paper demonstrates there is a negative relationship between\nbusiness cycles divergence and three factors (economic structure convergence,\nwage structure convergence and economic openness), but also a positive\nrelationship between it and its autoregressive term, respectively the GDP per\ncapita convergence.\n"
    },
    {
        "paper_id": 2007.11408,
        "authors": "Ionut Jianu",
        "title": "The impact of private sector credit on income inequalities in European\n  Union (15 member states)",
        "comments": "14 pages",
        "journal-ref": "Theoretical and Applied Economics, Vol. 24 (2017), No. 2",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to provide a comprehensive analysis on the income\ninequalities recorded in the EU-15 in the 1995-2014 period and to estimate the\nimpact of private sector credit on income disparities. In order to estimate the\nimpact, I used the panel data technique with 15 cross-sections for the first 15\nMember States of the European Union, applying generalized error correction\nmodel.\n"
    },
    {
        "paper_id": 2007.11409,
        "authors": "Ionut Jianu",
        "title": "The impact of government health and education expenditure on income\n  inequality in European Union",
        "comments": "15 pages",
        "journal-ref": "Theoretical and Applied Economics, Special Issue, 2018",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research aims to provide an overview of the existing inequalities and\ntheir drivers in the member states of the European Union as well as their\ndevelopements in the 2002-2008 and 2009- 2015 sub-periods. It also analyses the\nimpact of health and education government spending on income inequality in the\nEuropean Union over the 2002-2015 period. In this context, I applied the\nEstimated Generalized Least Squares method using panel data for the 28-member\nstates of the European Union.\n"
    },
    {
        "paper_id": 2007.11435,
        "authors": "Ionut Jianu",
        "title": "The Effect of Young People Not In Employment, Education or Training, On\n  Poverty Rate in European Union",
        "comments": "14 pages",
        "journal-ref": "Journal of Eastern Europe Research in Business and Economics, Vol.\n  2019, Article ID 955941",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to estimate the effect of young people who are not in\nemployment, education or training (neets rate) on the people at risk of poverty\nrate in the European Union. Statistical data covering the 2010-2016 period for\nall EU-28 Member States have been used. Regarding the methodology, the study\nwas performed by using Panel Estimated Generalized Least Squares method,\nweighted by Period SUR option. The effect of neets rate on poverty rate proved\nto be positive and statistically significant in European Union, since this\nindicator includes two main areas which are extremely relevant for poverty\ndimension. Firstly, young unemployment rate was one of the main channels\nthrough which the financial crisis has affected the population income.\nSecondly, it accounts for the educational system coverage and its skills\ndeficiencies.\n"
    },
    {
        "paper_id": 2007.11436,
        "authors": "Ionut Jianu, Ion Dobre, Dumitru Alexandru Bodislav, Carmen Valentina\n  Radulescu, Sorin Burlacu",
        "title": "The implications of institutional specificities on the income\n  inequalities drivers in European Union",
        "comments": "18 pages",
        "journal-ref": "Economic Computation and Economic Cybernetics Studies and\n  Research, Vol. 53, No. 2 / 2019",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to review the different impacts of income inequality drivers\non the Gini coefficient, depending on institutional specificities. In this\ncontext, we divided the European Union member states in two clusters (the\ncluster of member states with inclusive institutions / extractive institutions)\nusing the institutional pillar as a clustering criterion. In both cases, we\nassesed the impact of income inequality drivers on Gini coefficient by using a\nfixed effects model in order to examine the role and importance of the\ninstitutions in the dynamics of income disparities.The models were estimated by\napplying the Panel Estimated Generalized Least Squares (EGLS) method, this\nbeing weighted by Cross-section weights option. The separate assessment of the\nincome inequality reactivity to the change in its determinants according to the\ninstitutional criterion represents a new approach in this field of research and\nthe results show that the impact of moderating income inequality strategies is\nlimitedin the case of member states with extractive institutions.\n"
    },
    {
        "paper_id": 2007.11439,
        "authors": "Ionut Jianu",
        "title": "A comprehensive view of the manifestations of aggregate demand and\n  aggregate supply shocks in Greece, Ireland, Italy and Portugal",
        "comments": "18 pages",
        "journal-ref": "Theoretical and Applied Economics, Vol. 23 (2016), No. 2",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main goal of the paper is to extract the aggregate demand and aggregate\nsupply shocks in Greece, Ireland, Italy and Portugal, as well as to examine the\ncorrelation among the two types of shocks. The decomposition of the shocks was\nachieved by using a structural vector autoregression that analyses the\nrelationship between the evolution of the gross domestic product and inflation\nin the period 1997-2015. The goal of the paper is to confirm the aggregate\ndemand - aggregate supply model in the above-mentioned economies.\n"
    },
    {
        "paper_id": 2007.11546,
        "authors": "Jiawei Du",
        "title": "A Research on Cross-sectional Return Dispersion and Volatility of US\n  Stock Market during COVID-19",
        "comments": "1. The code for the caculation of CSAD and CSSD has some mistakes. 2.\n  There are some vague points in the description of herding effect. 3.I update\n  all the mistakes, typo and the inaccurate description and enrich the data and\n  models to contribute to a journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We studied the volatility and cross-sectional return dispersion effect of S&P\nHealth Care Sector under the covid-19 epidemic. We innovatively used the Google\nindex to proxy the impact of the epidemic and modeled the volatility. We also\nstudied the influencing factors of the log-return of S&P Energy Sector and S&P\nHealth Care Sector. We found that volatility is significantly affected by both\nthe epidemic and cross-sectional return dispersion, and the coefficients in\nfront of them are all positive, which means that the herding behaviour did not\nexist and as the cross-sectional return dispersion increases and the epidemic\nbecomes more severe, the volatility of stock returns is also increasing. We\nalso found that the epidemic has a significant negative impact on the return of\nthe energy sector, and finally we provided our suggestions to investors.\n"
    },
    {
        "paper_id": 2007.1158,
        "authors": "Thanasis Ziogas, Dimitris Ballas, Sierdjan Koster, Arjen Edzes",
        "title": "How happy are my neighbours? Modelling spatial spillover effects of\n  well-being",
        "comments": "45 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article uses data of subjective Life Satisfaction aggregated to the\ncommunity level in Canada and examines the spatial interdependencies and\nspatial spillovers of community happiness. A theoretical model of utility is\npresented. Using spatial econometric techniques, we find that the utility of\ncommunity, proxied by subjective measures of life satisfaction, is affected\nboth by the utility of neighbouring communities as well as by the latter's\naverage household income and unemployment rate. Shared cultural traits and\ninstitutions may justify such spillovers. The results are robust to the\ndifferent binary contiguity spatial weights matrices used and to the various\neconometric models. Clusters of both high-high and low-low in Life Satisfaction\ncommunities are also found based on the Moran's I test\n"
    },
    {
        "paper_id": 2007.11618,
        "authors": "Reason Lesego Machete",
        "title": "Towards a Sustainable Agricultural Credit Guarantee Scheme",
        "comments": "13 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since 1986, Government of Botswana has been running an Agricultural Credit\nGuarantee Scheme for dry-land arable farming. The scheme purports to assist\ndry-land crop farmers who have taken loans with participating banks or lending\ninstitutions to help them meet their debt obligations in case of crop failure\ndue to drought, floods, frost or hailstorm. Nonetheless, to date, the scheme\nhas focused solely on drought. The scheme has placed an unsustainable financial\nburden on Government because it is not based on sound actuarial principles.\nThis paper argues that the level of Government subsidies should take into\naccount the gains made by farmers during non-drought years. It is an attempt to\ncircumvent the challenges of correlated climate risks and recommends a quasi\nself-financing mechanism, assuming that the major driver of crop yield failure\nis drought. Moreover, it provides a novel subsidy and premium rate setting\nmethod.\n"
    },
    {
        "paper_id": 2007.11781,
        "authors": "Chao Deng, Xizhi Su and Chao Zhou",
        "title": "Relative wealth concerns with partial information and heterogeneous\n  priors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a Nash equilibrium in a market with $ N $ agents with the\nperformance criteria of relative wealth level when the market return is\nunobservable. Each investor has a random prior belief on the return rate of the\nrisky asset. The investors can be heterogeneous in both the mean and variance\nof the prior. By a separation result and a martingale argument, we show that\nthe optimal investment strategy under a stochastic return rate model can be\ncharacterized by a fully-coupled linear FBSDE. Two sets of deep neural networks\nare used for the numerical computation to first find each investor's estimate\nof the mean return rate and then solve the FBSDEs. We establish the existence\nand uniqueness result for the class of FBSDEs with stochastic coefficients and\nsolve the utility game under partial information using deep neural network\nfunction approximators. We demonstrate the efficiency and accuracy by a\nbase-case comparison with the solution from the finite difference scheme in the\nlinear case and apply the algorithm to the general case of nonlinear hidden\nvariable process. Simulations of investment strategies show a herd effect that\ninvestors trade more aggressively under relativeness concerns. Statistical\nproperties of the investment strategies and the portfolio performance,\nincluding the Sharpe ratios and the Variance Risk ratios (VRRs) are examed. We\nobserve that the agent with the most accurate prior estimate is likely to lead\nthe herd, and the effect of competition on heterogeneous agents varies more\nwith market characteristics compared to the homogeneous case.\n"
    },
    {
        "paper_id": 2007.11789,
        "authors": "M. Keith Chen, Judith A. Chevalier, and Elisa F. Long",
        "title": "Nursing Home Staff Networks and COVID-19",
        "comments": "22 pages, 2 figures. This working paper updated July 22nd, 2020. For\n  the most recent version, see\n  https://www.anderson.ucla.edu/faculty_pages/keith.chen/papers/WP_Nursing_Home_Networks_and_COVID19.pdf",
        "journal-ref": "published version in: PNAS January 5, 2021 118 (1)",
        "doi": "10.1073/pnas.2015455118",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Nursing homes and other long term-care facilities account for a\ndisproportionate share of COVID-19 cases and fatalities worldwide. Outbreaks in\nU.S. nursing homes have persisted despite nationwide visitor restrictions\nbeginning in mid-March. An early report issued by the Centers for Disease\nControl and Prevention identified staff members working in multiple nursing\nhomes as a likely source of spread from the Life Care Center in Kirkland,\nWashington to other skilled nursing facilities. The full extent of staff\nconnections between nursing homes---and the crucial role these connections\nserve in spreading a highly contagious respiratory infection---is currently\nunknown given the lack of centralized data on cross-facility nursing home\nemployment. In this paper, we perform the first large-scale analysis of nursing\nhome connections via shared staff using device-level geolocation data from 30\nmillion smartphones, and find that 7 percent of smartphones appearing in a\nnursing home also appeared in at least one other facility---even after visitor\nrestrictions were imposed. We construct network measures of nursing home\nconnectedness and estimate that nursing homes have, on average, connections\nwith 15 other facilities. Controlling for demographic and other factors, a\nhome's staff-network connections and its centrality within the greater network\nstrongly predict COVID-19 cases. Traditional federal regulatory metrics of\nnursing home quality are unimportant in predicting outbreaks, consistent with\nrecent research. Results suggest that eliminating staff linkages between\nnursing homes could reduce COVID-19 infections in nursing homes by 44 percent.\n"
    },
    {
        "paper_id": 2007.11877,
        "authors": "Thomas Ankenbrand, Denis Bieri, Roland Cortivo, Johannes Hoehener,\n  Thomas Hardjono",
        "title": "Proposal for a Comprehensive (Crypto) Asset Taxonomy",
        "comments": null,
        "journal-ref": "2020 Crypto Valley Conference on Blockchain Technology (CVCBT)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Developments in the distributed ledger technology have led to new types of\nassets with a broad range of purposes. Although some classification frameworks\nfor common instruments from traditional finance and some for these new, so\ncalled cryptographic assets already exist and are used, a holistic approach to\nintegrate both worlds is missing. The present paper fills this research gap by\nidentifying 14 attributes, each of which is assigned different characteristics,\nthat can be used to classify all types of assets in a structured manner. Our\nproposed taxonomy which is an extension of existing classification frameworks,\nsummarises these findings in a morphological box and is tested for\npracticability by classifying exemplary assets like cash and bitcoin. The final\nclassification framework can help to ensure that the various stakeholders, such\nas investors or supervisors, have a consistent view of the different types of\nassets, and in particular of their characteristics, and also helps to establish\nstandardised terminology.\n"
    },
    {
        "paper_id": 2007.11941,
        "authors": "Marco Grassia, Giuseppe Mangioni, Stefano Schiavo, Silvio Traverso",
        "title": "(Unintended) Consequences of export restrictions on medical goods during\n  the Covid-19 pandemic",
        "comments": "9 pages, 1 figure, 4 tables",
        "journal-ref": "Journal of Complex Networks, Volume 10, Issue 1, February 2022",
        "doi": "10.1093/comnet/cnab045",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the first half of 2020, several countries have responded to the challenges\nposed by the Covid-19 pandemic by restricting their export of medical supplies.\nSuch measures are meant to increase the domestic availability of critical\ngoods, and are commonly used in times of crisis. Yet, not much is known about\ntheir impact, especially on countries imposing them. Here we show that export\nbans are, by and large, counterproductive. Using a model of shock diffusion\nthrough the network of international trade, we simulate the impact of\nrestrictions under different scenarios. We observe that while they would be\nbeneficial to a country implementing them in isolation, their generalized use\nmakes most countries worse off relative to a no-ban scenario. As a corollary,\nwe estimate that prices increase in many countries imposing the restrictions.\nWe also find that the cost of restraining from export bans is small, even when\nothers continue to implement them. Finally, we document a change in countries'\nposition within the international trade network, suggesting that export bans\nhave geopolitical implications.\n"
    },
    {
        "paper_id": 2007.11973,
        "authors": "Michele Loi, Eleonora Vigan\\`o, Lonneke van der Plas",
        "title": "The societal and ethical relevance of computational creativity",
        "comments": "4 pages, 1 figure, Eleventh International Conference on Computational\n  Creativity, ICCC'20",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide a philosophical account of the value of creative\nsystems for individuals and society. We characterize creativity in very broad\nphilosophical terms, encompassing natural, existential, and social creative\nprocesses, such as natural evolution and entrepreneurship, and explain why\ncreativity understood in this way is instrumental for advancing human\nwell-being in the long term. We then explain why current mainstream AI tends to\nbe anti-creative, which means that there are moral costs of employing this type\nof AI in human endeavors, although computational systems that involve\ncreativity are on the rise. In conclusion, there is an argument for ethics to\nbe more hospitable to creativity-enabling AI, which can also be in a trade-off\nwith other values promoted in AI ethics, such as its explainability and\naccuracy.\n"
    },
    {
        "paper_id": 2007.12007,
        "authors": "Ionut Jianu",
        "title": "The Relationship between the Economic and Financial Crises and\n  Unemployment Rate in the European Union -- How Institutions Affected Their\n  Linkage",
        "comments": null,
        "journal-ref": "Journal of Eastern Europe Research in Business and Economics, Vol.\n  2019, Article ID 403548",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to estimate the impact of economic and financial crises on\nthe unemployment rate in the European Union, taking also into consideration the\ninstitutional specificities, since unemployment was the main channel through\nwhich the economic and financial crisis influenced the social developments.. In\nthis context, I performed two institutional clusters depending on their\ninclusive or extractive institutional features and, in each cases, I computed\nthe crisis effect on unemployment rate over the 2003-2017 period. Both models\nwere estimated by using Panel Estimated Generalized Least Squares method, and\nare weighted by Period SUR option in order to remove, in advance the possible\ninconveniences of the models. The institutions proved to be a relevant\ncriterion that drives the impact of economic and financial crises on the\nunemployment rate, highlighting that countries with inclusive institutions are\nless vulnerable to economic shocks and are more resilient than countries with\nextractive institutions. The quality of institutions was also found to have a\nsignificant effect on the response of unemployment rate to the dynamic of its\ndrivers.\n"
    },
    {
        "paper_id": 2007.12177,
        "authors": "Michael Bailey, Drew Johnston, Theresa Kuchler, Dominic Russel, Bogdan\n  State, Johannes Stroebel",
        "title": "Online Appendix & Additional Results for The Determinants of Social\n  Connectedness in Europe",
        "comments": "21 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this online appendix we provide additional information and analyses to\nsupport \"The Determinants of Social Connectedness in Europe.\" We include a\nnumber of case studies illustrating how language, history, and other factors\nhave shaped European social networks. We also look at the effects of social\nconnectedness. Our results provide empirical support for theoretical models\nthat suggest social networks play an important role in individuals' travel\ndecisions. We study variation in the degree of connectedness of regions to\nother European countries, finding a negative correlation between Euroscepticism\nand greater levels of international connection.\n"
    },
    {
        "paper_id": 2007.12226,
        "authors": "Stephan Leitner, Bartosz Gula, Dietmar Jannach, Ulrike Krieg-Holz,\n  Friederike Wall",
        "title": "Understanding the dynamics emerging from infodemics: A call to action\n  for interdisciplinary research",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research on infodemics, i.e., the rapid spread of (mis)information related to\na hazardous event, such as the COVID-19 pandemic, requires the integration of a\nmultiplicity of scientific disciplines. The dynamics emerging from infodemics\nhave the potential to generate complex behavioral patterns. In order to react\nappropriately, it is of ultimate importance for the fields of Business and\nEconomics to understand the dynamics emerging from it. In the short run,\ndynamics might lead to an adaptation in household spending or to a shift in\nbuying behavior towards online providers. In the long run, changes in\ninvestments, consumer behavior, and markets are to be expected. We argue that\nthe dynamics emerge from complex interactions among multiple factors, such as\ninformation and misinformation accessible for individuals and the formation and\nrevision of beliefs. (Mis)information accessible to individuals is, amongst\nothers, affected by algorithms specifically designed to provide personalized\ninformation, while automated fact-checking algorithms can help reduce the\namount of circulating misinformation. The formation and revision of individual\n(and probably false) beliefs and individual fact-checking and interpretation of\ninformation are heavily affected by linguistic patterns inherent to information\nduring pandemics and infodemics and further factors, such as affect, intuition\nand motives. We argue that, in order to get a deep(er) understanding of the\ndynamics emerging from infodemics, the fields of Business and Economics should\nintegrate the perspectives of Computer Science and Information Systems,\n(Computational) Linguistics, and Cognitive Science into the wider context of\neconomic systems (e.g., organizations, markets or industries) and propose a way\nto do so.\n"
    },
    {
        "paper_id": 2007.12228,
        "authors": "Foad Shokrollahi and Marcin Marcin Magdziarz",
        "title": "Equity warrant pricing under subdiffusive fractional Brownian motion of\n  the short rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose an extension of the Merton model. We apply the\nsubdiffusive mechanism to analyze equity warrant in a fractional Brownian\nmotion environment, when the short rate follows the subdiffusive fractional\nBlack-Scholes model. We obtain the pricing formula for zero-coupon bond in the\nintroduced model and derive the partial differential equation with appropriate\nboundary conditions for the valuation of equity warrant. Finally, the pricing\nformula for equity warrant is provided under subdiffusive fractional Brownian\nmotion model of the short rate.\n"
    },
    {
        "paper_id": 2007.12255,
        "authors": "Carlos Denner dos Santos, Jessica Alves",
        "title": "Home Advantage in the Brazilian Elite Football: Verifying managers'\n  capacity to outperform their disadvantage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Home advantage (HA) in football, soccer is well documented in the literature;\nhowever, the explanation for such phenomenon is yet to be completed, as this\npaper demonstrates that it is possible to overcome such disadvantage through\nmanagerial planning and intervention (tactics), an effect so far absent in the\nliterature. To accomplish that, this study develops an integrative theoretical\nmodel of team performance to explain HA based on prior literature, pushing its\nlimits to unfold the manager role in such a persistent pattern of performance\nin soccer. Data on one decade of the national tournament of Brazil was obtained\nfrom public sources, including information about matches and coaches of all 12\nteams who played these competitions. Our conceptual modeling allows an\nempirical analysis of HA and performance that covers the effects of tactics,\npresence of supporters in matches and team fatigue via logistic regression. The\nresults confirm the HA in the elite division of Brazilian soccer across all\nlevels of comparative technical quality, a new variable introduced to control\nfor a potential technical gap between teams (something that would turn the\nmanagerial influence null). Further analysis provides evidence that highlights\nmanagerial capacity to block the HA effect above and beyond the influences of\nfatigue (distance traveled) and density of people in the matches. This is the\ncase of coaches Abel Braga, Marcelo Fernandes and Roger Machado, who were\ncapable to reverse HA when playing against teams of similar quality. Overall,\nthe home advantage diminishes as the comparative quality increases but\ndisappears only when the two teams are of extremely different technical\nquality.\n"
    },
    {
        "paper_id": 2007.12338,
        "authors": "Yuyu Chen, Peng Liu, Yang Liu, Ruodu Wang",
        "title": "Ordering and Inequalities for Mixtures on Risk Aggregation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aggregation sets, which represent model uncertainty due to unknown\ndependence, are an important object in the study of robust risk aggregation. In\nthis paper, we investigate ordering relations between two aggregation sets for\nwhich the sets of marginals are related by two simple operations: distribution\nmixtures and quantile mixtures. Intuitively, these operations ``homogenize\"\nmarginal distributions by making them similar. As a general conclusion from our\nresults, more ``homogeneous\" marginals lead to a larger aggregation set, and\nthus more severe model uncertainty, although the situation for quantile\nmixtures is much more complicated than that for distribution mixtures. We\nproceed to study inequalities on the worst-case values of risk measures in risk\naggregation, which represent conservative calculation of regulatory capital.\nAmong other results, we obtain an order relation on VaR under quantile mixture\nfor marginal distributions with monotone densities. Numerical results are\npresented to visualize the theoretical results and further inspire some\nconjectures. Finally, we provide applications on portfolio diversification\nunder dependence uncertainty and merging p-values in multiple hypothesis\ntesting, and discuss the connection of our results to joint mixability.\n"
    },
    {
        "paper_id": 2007.12431,
        "authors": "Gilles Zumbach",
        "title": "Tile test for back-testing risk evaluation",
        "comments": "22 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new test for measuring the accuracy of financial market risk estimations is\nintroduced. It is based on the probability integral transform (PIT) of the ex\npost realized returns using the ex ante probability distributions underlying\nthe risk estimation. If the forecast is correct, the result of the PIT, that we\ncalled probtile, should be an iid random variable with a uniform distribution.\nThe new test measures the variance of the number of probtiles in a tiling over\nthe whole sample. Using different tilings allow to check the dynamic and the\ndistributional aspect of risk methodologies. The new test is very powerful, and\nnew benchmarks need to be introduced to take into account subtle mean reversion\neffects induced by some risk estimations. The test is applied on 2 data sets\nfor risk horizons of 1 and 10 days. The results show unambiguously the\nimportance of capturing correctly the dynamic of the financial market, and\nexclude some broadly used risk methodologies.\n"
    },
    {
        "paper_id": 2007.12433,
        "authors": "Akhmad Muhammadin, Rashila Ramli, Syamsul Ridjal, Muhlis Kanto,\n  Syamsul Alam, Hamzah Idris",
        "title": "Effects of dynamic capability and marketing strategy on the\n  organizational performance of the banking sector in Makassar, Indonesia",
        "comments": "16 pages, 1 figure, 14 tables",
        "journal-ref": "Revista ESPACIOS. 41/24(2020)26-41",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The dynamic capability and marketing strategy are challenges to the banking\nsector in Indonesia. This study uses a survey method solving 39 banks in\nMakassar. Data collection was conducted of questionnaires. The results show\nthat, the dynamic capability has a positive yet insignificant impact on the\norganizational performance, the marketing strategy has a positive and\nsignificant effect on organizational performance and, dynamic capability and\nmarketing strategy have a positive and significant effect on the organization's\nperformance in the banking sector in Makassar. Keywords : dynamic capability,\nmarketing strategy, organizational performance, banking\n"
    },
    {
        "paper_id": 2007.1262,
        "authors": "Yang Li and Yi Pan",
        "title": "A Novel Ensemble Deep Learning Model for Stock Prediction Based on Stock\n  Prices and News",
        "comments": "15 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, machine learning and deep learning have become popular\nmethods for financial data analysis, including financial textual data,\nnumerical data, and graphical data. This paper proposes to use sentiment\nanalysis to extract useful information from multiple textual data sources and a\nblending ensemble deep learning model to predict future stock movement. The\nblending ensemble model contains two levels. The first level contains two\nRecurrent Neural Networks (RNNs), one Long-Short Term Memory network (LSTM) and\none Gated Recurrent Units network (GRU), followed by a fully connected neural\nnetwork as the second level model. The RNNs, LSTM, and GRU models can\neffectively capture the time-series events in the input data, and the fully\nconnected neural network is used to ensemble several individual prediction\nresults to further improve the prediction accuracy. The purpose of this work is\nto explain our design philosophy and show that ensemble deep learning\ntechnologies can truly predict future stock price trends more effectively and\ncan better assist investors in making the right investment decision than other\ntraditional methods.\n"
    },
    {
        "paper_id": 2007.12681,
        "authors": "Longbing Cao, Qiang Yang and Philip S. Yu",
        "title": "Data science and AI in FinTech: An overview",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial technology (FinTech) has been playing an increasingly critical role\nin driving modern economies, society, technology, and many other areas. Smart\nFinTech is the new-generation FinTech, largely inspired and empowered by data\nscience and new-generation AI and (DSAI) techniques. Smart FinTech synthesizes\nbroad DSAI and transforms finance and economies to drive intelligent,\nautomated, whole-of-business and personalized economic and financial\nbusinesses, services and systems. The research on data science and AI in\nFinTech involves many latest progress made in smart FinTech for BankingTech,\nTradeTech, LendTech, InsurTech, WealthTech, PayTech, RiskTech,\ncryptocurrencies, and blockchain, and the DSAI techniques including complex\nsystem methods, quantitative methods, intelligent interactions, recognition and\nresponses, data analytics, deep learning, federated learning,\nprivacy-preserving processing, augmentation, optimization, and system\nintelligence enhancement. Here, we present a highly dense research overview of\nsmart financial businesses and their challenges, the smart FinTech ecosystem,\nthe DSAI techniques to enable smart FinTech, and some research directions of\nsmart FinTech futures to the DSAI communities.\n"
    },
    {
        "paper_id": 2007.12838,
        "authors": "Peng-Fei Dai (TJU), Xiong Xiong (TJU), Wei-Xing Zhou (ECUST)",
        "title": "The role of global economic policy uncertainty in predicting crude oil\n  futures volatility: Evidence from a two-factor GARCH-MIDAS model",
        "comments": "19 pages, 16 figures",
        "journal-ref": "Resources Policy 78, 102849 (2022)",
        "doi": "10.1016/j.resourpol.2022.102849",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to examine whether the global economic policy uncertainty\n(GEPU) and uncertainty changes have different impacts on crude oil futures\nvolatility. We establish single-factor and two-factor models under the\nGARCH-MIDAS framework to investigate the predictive power of GEPU and GEPU\nchanges excluding and including realized volatility. The findings show that the\nmodels with rolling-window specification perform better than those with\nfixed-span specification. For single-factor models, the GEPU index and its\nchanges, as well as realized volatility, are consistent effective factors in\npredicting the volatility of crude oil futures. Specially, GEPU changes have\nstronger predictive power than the GEPU index. For two-factor models, GEPU is\nnot an effective forecast factor for the volatility of WTI crude oil futures or\nBrent crude oil futures. The two-factor model with GEPU changes contains more\ninformation and exhibits stronger forecasting ability for crude oil futures\nmarket volatility than the single-factor models. The GEPU changes are indeed\nthe main source of long-term volatility of the crude oil futures.\n"
    },
    {
        "paper_id": 2007.1288,
        "authors": "Peng-Fei Dai (TJU), Xiong Xiong (TJU), Wei-Xing Zhou (ECUST)",
        "title": "Visibility graph analysis of economy policy uncertainty indices",
        "comments": "9 pages, 13 figures",
        "journal-ref": "Physica A 0378-4371(2019)",
        "doi": "10.1016/j.physa.2019.121748",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Uncertainty plays an important role in the global economy. In this paper, the\neconomic policy uncertainty (EPU) indices of the United States and China are\nselected as the proxy variable corresponding to the uncertainty of national\neconomic policy. By adopting the visibility graph algorithm, the four economic\npolicy uncertainty indices of the United States and China are mapped into\ncomplex networks, and the topological properties of the corresponding networks\nare studied. The Hurst exponents of all the four indices are within\n$\\left[0.5,1\\right]$, which implies that the economic policy uncertainty is\npersistent. The degree distributions of the EPU networks have power-law tails\nand are thus scale-free. The average clustering coefficients of the four EPU\nnetworks are high and close to each other, while these networks exhibit weak\nassortative mixing. We also find that the EPU network in United States based on\ndaily data shows the small-world feature since the average shortest path length\nincreases logarithmically with the network size such that\n$L\\left(N\\right)=0.626\\ln N+0.405$. Our research highlights the possibility to\nstudy the EPU from the view angle of complex networks.\n"
    },
    {
        "paper_id": 2007.13103,
        "authors": "Nicole B\\\"auerle and Alexander Glauner",
        "title": "Distributionally Robust Markov Decision Processes and their Connection\n  to Risk Measures",
        "comments": null,
        "journal-ref": "Mathematics of Operations Research (47), 1757-1780, 2021",
        "doi": "10.1287/moor.2021.1187",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider robust Markov Decision Processes with Borel state and action\nspaces, unbounded cost and finite time horizon. Our formulation leads to a\nStackelberg game against nature. Under integrability, continuity and\ncompactness assumptions we derive a robust cost iteration for a fixed policy of\nthe decision maker and a value iteration for the robust optimization problem.\nMoreover, we show the existence of deterministic optimal policies for both\nplayers. This is in contrast to classical zero-sum games. In case the state\nspace is the real line we show under some convexity assumptions that the\ninterchange of supremum and infimum is possible with the help of Sion's minimax\nTheorem. Further, we consider the problem with special ambiguity sets. In\nparticular we are able to derive some cases where the robust optimization\nproblem coincides with the minimization of a coherent risk measure. In the\nfinal section we discuss two applications: A robust LQ problem and a robust\nproblem for managing regenerative energy.\n"
    },
    {
        "paper_id": 2007.13238,
        "authors": "Vadim A. Karatayev, V\\'itor V. Vasconcelos, Anne-Sophie Lafuite, Simon\n  A. Levin, Chris T. Bauch, Madhur Anand",
        "title": "A well-timed switch from local to global agreements accelerates climate\n  change mitigation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41467-021-23056-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent attempts at cooperating on climate change mitigation highlight the\nlimited efficacy of large-scale agreements, when commitment to mitigation is\ncostly and initially rare. Bottom-up approaches using region-specific\nmitigation agreements promise greater success, at the cost of slowing global\nadoption. Here, we show that a well-timed switch from regional to global\nnegotiations dramatically accelerates climate mitigation compared to using only\nlocal, only global, or both agreement types simultaneously. This highlights the\nscale-specific roles of mitigation incentives: local incentives capitalize on\nregional differences (e.g., where recent disasters incentivize mitigation) by\ncommitting early-adopting regions, after which global agreements draw in\nlate-adopting regions. We conclude that global agreements are key to overcoming\nthe expenses of mitigation and economic rivalry among regions but should be\nattempted once regional agreements are common. Gradually up-scaling efforts\ncould likewise accelerate mitigation at smaller scales, for instance when\ncostly ecosystem restoration initially faces limited public and legislative\nsupport.\n"
    },
    {
        "paper_id": 2007.13549,
        "authors": "Plamen Nikolov, Hongjian Wang, Kevin Acker",
        "title": "The Wage Premium of Communist Party Membership: Evidence from China",
        "comments": "arXiv admin note: text overlap with arXiv:1010.5586 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social status and political connections could confer large economic benefits\nto an individual. Previous studies focused on China examine the relationship\nbetween Communist party membership and earnings and find a positive\ncorrelation. However, this correlation may be partly or totally spurious,\nthereby generating upwards-biased estimates of the importance of political\nparty membership. Using data from three surveys spanning more than three\ndecades, we estimate the causal effect of Chinese party membership on monthly\nearnings in in China. We find that, on average, membership in the Communist\nparty of China increases monthly earnings and we find evidence that the wage\npremium has grown in recent years. We explore for potential mechanisms and we\nfind suggestive evidence that improvements in one's social network, acquisition\nof job-related qualifications and improvement in one's social rank and life\nsatisfaction likely play an important role. (JEL D31, J31, P2)\n"
    },
    {
        "paper_id": 2007.13566,
        "authors": "Claudia Foroni and Francesco Ravazzolo and Luca Rossini",
        "title": "Are low frequency macroeconomic variables important for high frequency\n  electricity prices?",
        "comments": "This paper has previously circulated with the title: \"Forecasting\n  daily electricity prices with monthly macroeconomic variables\" (ECB Working\n  paper Series No. 2250)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent research finds that forecasting electricity prices is very relevant.\nIn many applications, it might be interesting to predict daily electricity\nprices by using their own lags or renewable energy sources. However, the recent\nturmoil of energy prices and the Russian-Ukrainian war increased attention in\nevaluating the relevance of industrial production and the Purchasing Managers'\nIndex output survey in forecasting the daily electricity prices. We develop a\nBayesian reverse unrestricted MIDAS model which accounts for the mismatch in\nfrequency between the daily prices and the monthly macro variables in Germany\nand Italy. We find that the inclusion of macroeconomic low frequency variables\nis more important for short than medium term horizons by means of point and\ndensity measures. In particular, accuracy increases by combining hard and soft\ninformation, while using only surveys gives less accurate forecasts than using\nonly industrial production data.\n"
    },
    {
        "paper_id": 2007.13823,
        "authors": "Kristoffer Persson",
        "title": "Economic Reality, Economic Media and Individuals' Expectations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the relationship between economic media sentiment and\nindividuals' expetations and perceptions about economic conditions. We test if\neconomic media sentiment Granger-causes individuals' expectations and opinions\nconcerning economic conditions, controlling for macroeconomic variables. We\ndevelop a measure of economic media sentiment using a supervised machine\nlearning method on a data set of Swedish economic media during the period\n1993-2017. We classify the sentiment of 179,846 media items, stemming from\n1,071 unique media outlets, and use the number of news items with positive and\nnegative sentiment to construct a time series index of economic media\nsentiment. Our results show that this index Granger-causes individuals'\nperception of macroeconomic conditions. This indicates that the way the\neconomic media selects and frames macroeconomic news matters for individuals'\naggregate perception of macroeconomic reality.\n"
    },
    {
        "paper_id": 2007.13879,
        "authors": "Jaros{\\l}aw Gruszka, Janusz Szwabi\\'nski",
        "title": "Advanced Strategies of Portfolio Management in the Heston Market Model",
        "comments": "22 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.125978",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a great number of factors to take into account when building and\nmanaging an investment portfolio. It is widely believed that a proper set-up of\nthe portfolio combined with a good, robust management strategy is the key to\nsuccessful investment. In this paper, we aim at an analysis of two aspects that\nmay have an impact on investment performance: diversity of assets and inclusion\nof cash in the portfolio. We also propose two new management strategies based\non the MACD and RSI factors known from technical analysis. Monte Carlo\nsimulations within the Heston model of a market are used to perform numerical\nexperiments.\n"
    },
    {
        "paper_id": 2007.13902,
        "authors": "Jeremy Ferwerda, Nicholas Adams-Cohen, Kirk Bansak, Jennifer Fei,\n  Duncan Lawrence, Jeremy M. Weinstein, Jens Hainmueller",
        "title": "Leveraging the Power of Place: A Data-Driven Decision Helper to Improve\n  the Location Decisions of Economic Immigrants",
        "comments": "51 pages (including appendix), 13 figures. Immigration Policy Lab\n  (IPL) Working Paper Series, Working Paper No. 20-06",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A growing number of countries have established programs to attract immigrants\nwho can contribute to their economy. Research suggests that an immigrant's\ninitial arrival location plays a key role in shaping their economic success.\nYet immigrants currently lack access to personalized information that would\nhelp them identify optimal destinations. Instead, they often rely on\navailability heuristics, which can lead to the selection of sub-optimal landing\nlocations, lower earnings, elevated outmigration rates, and concentration in\nthe most well-known locations. To address this issue and counteract the effects\nof cognitive biases and limited information, we propose a data-driven decision\nhelper that draws on behavioral insights, administrative data, and machine\nlearning methods to inform immigrants' location decisions. The decision helper\nprovides personalized location recommendations that reflect immigrants'\npreferences as well as data-driven predictions of the locations where they\nmaximize their expected earnings given their profile. We illustrate the\npotential impact of our approach using backtests conducted with administrative\ndata that links landing data of recent economic immigrants from Canada's\nExpress Entry system with their earnings retrieved from tax records.\nSimulations across various scenarios suggest that providing location\nrecommendations to incoming economic immigrants can increase their initial\nearnings and lead to a mild shift away from the most populous landing\ndestinations. Our approach can be implemented within existing institutional\nstructures at minimal cost, and offers governments an opportunity to harness\ntheir administrative data to improve outcomes for economic immigrants.\n"
    },
    {
        "paper_id": 2007.13972,
        "authors": "Young Shin Kim",
        "title": "Portfolio Optimization on the Dispersion Risk and the Asymmetric Tail\n  Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a market model with returns assumed to follow a\nmultivariate normal tempered stable distribution defined by a mixture of the\nmultivariate normal distribution and the tempered stable subordinator. This\ndistribution is able to capture two stylized facts: fat-tails and asymmetry,\nthat have been empirically observed for asset return distributions. On the new\nmarket model, we discuss a new portfolio optimization method, which is an\nextension of Markowitz's mean-variance optimization. The new optimization\nmethod considers not only reward and dispersion but also asymmetry. The\nefficient frontier is also extended to a curved surface on three-dimensional\nspace of reward, dispersion, and asymmetry. We also propose a new performance\nmeasure which is an extension of the Sharpe Ratio. Moreover, we derive\nclosed-form solutions for two important measures used by portfolio managers in\nportfolio construction: the marginal Value-at-Risk (VaR) and the marginal\nConditional VaR (CVaR). We illustrate the proposed model using stocks\ncomprising the Dow Jones Industrial Average. First, perform the new portfolio\noptimization and then demonstrating how the marginal VaR and marginal CVaR can\nbe used for portfolio optimization under the model. Based on the empirical\nevidence presented in this paper, our framework offers realistic portfolio\noptimization and tractable methods for portfolio risk management.\n"
    },
    {
        "paper_id": 2007.14022,
        "authors": "Andreas Tryphonides",
        "title": "Heterogeneity and the Dynamic Effects of Aggregate Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a semi-structural approach, the paper identifies how heterogeneity and\nfinancial frictions affect the transmission of aggregate shocks. Approximating\na heterogeneous agent model around the representative agent allocation can\nsuccessfully trace the aggregate and distributional dynamics and can be\nconsistent with alternative mechanisms. Employing Spanish macroeconomic data as\nwell as firm and household survey data, the paper finds that frictions on both\nconsumption and investment have rich interactions with aggregate shocks. The\nresponse of heterogeneity amplifies or dampens these effects depending on the\ntype of the shock. Both dispersion in consumption shares and the marginal\nrevenue product of firms, as well as the proportion of investment constrained\nfirms are key determinants of the fiscal multiplier.\n"
    },
    {
        "paper_id": 2007.14069,
        "authors": "Miklos Rasonyi and Kinga Tikosi",
        "title": "Convergence of the Kiefer-Wolfowitz algorithm in the presence of\n  discontinuities",
        "comments": "revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we estimate the expected error of a stochastic approximation\nalgorithm where the maximum of a function is found using finite differences of\na stochastic representation of that function. An error estimate of\n$O(n^{-1/5})$ for the $n$th iteration is achieved using suitable parameters.\nThe novelty with respect to previous studies is that we allow the stochastic\nrepresentation to be discontinuous and to consist of possibly dependent random\nvariables (satisfying a mixing condition).\n"
    },
    {
        "paper_id": 2007.14162,
        "authors": "Weston Barger, Ryan Donnelly",
        "title": "Insider Trading with Temporary Price Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model an informed agent with information about the future value of an\nasset trying to maximize profits when subjected to a transaction cost as well\nas a market maker tasked with setting fair transaction prices. In a single\nauction model, equilibrium is characterized by the unique root of a particular\npolynomial. Analysis of this polynomial with small levels of risk-aversion and\ntransaction costs reveal a dimensionless parameter which captures several\norders of asymptotic accuracy of the equilibrium behaviour. In a continuous\ntime analogue of the single auction model, incorporation of a transaction costs\nallows the informed agent's optimal trading strategy to be obtained in feedback\nform. Linear equilibrium is characterized by the unique solution to a system of\ntwo ordinary differential equations, of which one is forward in time and one is\nbackward. When transaction costs are in effect, the price set by the market\nmaker in equilibrium is not fully revealing of the informed agent's private\nsignal, leaving an information gap at the end of the trading interval. When\nconsidering vanishing transaction costs, the equilibrium trading strategy and\npricing rules converge to their frictionless counterparts.\n"
    },
    {
        "paper_id": 2007.14328,
        "authors": "Marc Lagunas-Merino, Salvador Ortiz-Latorre",
        "title": "A decomposition formula for fractional Heston jump diffusion models",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an option pricing formula for European options in a stochastic\nvolatility model. In particular, the volatility process is defined using a\nfractional integral of a diffusion process and both the stock price and the\nvolatility processes have jumps in order to capture the market effect known as\nleverage effect. We show how to compute a martingale representation for the\nvolatility process. Finally, using It\\^o calculus for processes with\ndiscontinuous trajectories, we develop a first order approximation formula for\noption prices. There are two main advantages in the usage of such approximating\nformulas to traditional pricing methods. First, to improve computational\neffciency, and second, to have a deeper understanding of the option price\nchanges in terms of changes in the model parameters.\n"
    },
    {
        "paper_id": 2007.14447,
        "authors": "Ali Namaki, Jamshid Ardalankia, Reza Raei, Leila Hedayatifar, Ali\n  Hosseiny, Emmanuel Haven, G.Reza Jafari",
        "title": "Analysis of the Global Banking Network by Random Matrix Theory",
        "comments": "5 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since 2008, the network analysis of financial systems is one of the most\nimportant subjects in economics. In this paper, we have used the complexity\napproach and Random Matrix Theory (RMT) for analyzing the global banking\nnetwork. By applying this method on a cross border lending network, it is shown\nthat the network has been denser and the connectivity between peripheral nodes\nand the central section has risen. Also, by considering the collective behavior\nof the system and comparing it with the shuffled one, we can see that this\nnetwork obtains a specific structure. By using the inverse participation ratio\nconcept, we can see that after 2000, the participation of different modes to\nthe network has increased and tends to the market mode of the system. Although\nno important change in the total market share of trading occurs, through the\npassage of time, the contribution of some countries in the network structure\nhas increased. The technique proposed in the paper can be useful for analyzing\ndifferent types of interaction networks between countries.\n"
    },
    {
        "paper_id": 2007.1462,
        "authors": "Fabio Vanni, David Lambert, and Luigi Palatella",
        "title": "Epidemic response to physical distancing policies and their impact on\n  the outbreak risk",
        "comments": "45 pages, 20 figures, Corrected bibliography typos",
        "journal-ref": "Scientific Reports 2021",
        "doi": "10.1038/s41598-021-02760-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a theoretical framework that highlights the impact of physical\ndistancing variables such as human mobility and physical proximity on the\nevolution of epidemics and, crucially, on the reproduction number. In\nparticular, in response to the coronavirus disease (CoViD-19) pandemic,\ncountries have introduced various levels of 'lockdown' to reduce the number of\nnew infections. Specifically we use a collisional approach to an infection-age\nstructured model described by a renewal equation for the time homogeneous\nevolution of epidemics. As a result, we show how various contributions of the\nlockdown policies, namely physical proximity and human mobility, reduce the\nimpact of SARS-CoV-2 and mitigate the risk of disease resurgence. We check our\ntheoretical framework using real-world data on physical distancing with two\ndifferent data repositories, obtaining consistent results. Finally, we propose\nan equation for the effective reproduction number which takes into account\ntypes of interactions among people, which may help policy makers to improve\nremote-working organizational structure.\n"
    },
    {
        "paper_id": 2007.1463,
        "authors": "Yoshi Fujiwara, Hiroyasu Inoue, Takayuki Yamaguchi, Hideaki Aoyama,\n  and Takuma Tanaka",
        "title": "Money flow network among firms' accounts in a regional bank of Japan",
        "comments": "22 pages with 13 figures and 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we investigate the flow of money among bank accounts possessed\nby firms in a region by employing an exhaustive list of all the bank transfers\nin a regional bank in Japan, to clarify how the network of money flow is\nrelated to the economic activities of the firms. The network statistics and\nstructures are examined and shown to be similar to those of a nationwide\nproduction network. Specifically, the bowtie analysis indicates what we refer\nto as a \"walnut\" structure with core and upstream/downstream components. To\nquantify the location of an individual account in the network, we used the\nHodge decomposition method and found that the Hodge potential of the account\nhas a significant correlation to its position in the bowtie structure as well\nas to its net flow of incoming and outgoing money and links, namely the net\ndemand/supply of individual accounts. In addition, we used non-negative matrix\nfactorization to identify important factors underlying the entire flow of\nmoney; it can be interpreted that these factors are associated with regional\neconomic activities.One factor has a feature whereby the remittance source is\nlocalized to the largest city in the region, while the destination is\nscattered. The other factors correspond to the economic activities specific to\ndifferent local places.This study serves as a basis for further investigation\non the relationship between money flow and economic activities of firms.\n"
    },
    {
        "paper_id": 2007.14702,
        "authors": "Wolfgang Karl H\\\"ardle, Campbell R. Harvey, Raphael C. G. Reule",
        "title": "Editorial: Understanding Cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3360304",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrency refers to a type of digital asset that uses distributed\nledger, or blockchain, technology to enable a secure transaction. Although the\ntechnology is widely misunderstood, many central banks are considering\nlaunching their own national cryptocurrency. In contrast to most data in\nfinancial economics, detailed data on the history of every transaction in the\ncryptocurrency complex are freely available. Furthermore, empirically-oriented\nresearch is only now beginning, presenting an extraordinary research\nopportunity for academia. We provide some insights into the mechanics of\ncryptocurrencies, describing summary statistics and focusing on potential\nfuture research avenues in financial economics.\n"
    },
    {
        "paper_id": 2007.14841,
        "authors": "Vadim S. Balashov, Yuxing Yan, Xiaodi Zhu",
        "title": "Who Manipulates Data During Pandemics? Evidence from Newcomb-Benford Law",
        "comments": "37 pages, 11 tables, 2 Appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the Newcomb-Benford law to test if countries have manipulated reported\ndata during the COVID-19 pandemic. We find that democratic countries, countries\nwith the higher gross domestic product (GDP) per capita, higher healthcare\nexpenditures, and better universal healthcare coverage are less likely to\ndeviate from the Newcomb-Benford law. The relationship holds for the cumulative\nnumber of reported deaths and total cases but is more pronounced for the death\ntoll. The findings are robust for second-digit tests, for a sub-sample of\ncountries with regional data, and in relation to the previous swine flu (H1N1)\n2009-2010 pandemic. The paper further highlights the importance of independent\nsurveillance data verification projects.\n"
    },
    {
        "paper_id": 2007.14874,
        "authors": "Lennart Oelschl\\\"ager and Timo Adam",
        "title": "Detecting bearish and bullish markets in financial time series using\n  hierarchical hidden Markov models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets exhibit alternating periods of rising and falling prices.\nStock traders seeking to make profitable investment decisions have to account\nfor those trends, where the goal is to accurately predict switches from bullish\ntowards bearish markets and vice versa. Popular tools for modeling financial\ntime series are hidden Markov models, where a latent state process is used to\nexplicitly model switches among different market regimes. In their basic form,\nhowever, hidden Markov models are not capable of capturing both short- and\nlong-term trends, which can lead to a misinterpretation of short-term price\nfluctuations as changes in the long-term trend. In this paper, we demonstrate\nhow hierarchical hidden Markov models can be used to draw a comprehensive\npicture of financial markets, which can contribute to the development of more\nsophisticated trading strategies. The feasibility of the suggested approach is\nillustrated in two real-data applications, where we model data from two major\nstock indices, the Deutscher Aktienindex and the Standard & Poor's 500.\n"
    },
    {
        "paper_id": 2007.15041,
        "authors": "Umut Cetin and Kasper Larsen",
        "title": "Uniqueness in Cauchy problems for diffusive real-valued strict local\n  martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a real-valued one dimensional diffusive strict local martingale,, we\nprovide a set of smooth functions in which the Cauchy problem has a unique\nclassical solution under a local H\\\"older condition. Under the weaker\nEngelbert-Schmidt conditions, we provide a set in which the Cauchy problem has\na unique weak solution. We exemplify our results using quadratic normal\nvolatility models and the two dimensional Bessel process.\n"
    },
    {
        "paper_id": 2007.15128,
        "authors": "Alexandre Carbonneau",
        "title": "Deep Hedging of Long-Term Financial Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents a deep reinforcement learning approach for global hedging\nof long-term financial derivatives. A similar setup as in Coleman et al. (2007)\nis considered with the risk management of lookback options embedded in\nguarantees of variable annuities with ratchet features. The deep hedging\nalgorithm of Buehler et al. (2019a) is applied to optimize neural networks\nrepresenting global hedging policies with both quadratic and non-quadratic\npenalties. To the best of the author's knowledge, this is the first paper that\npresents an extensive benchmarking of global policies for long-term contingent\nclaims with the use of various hedging instruments (e.g. underlying and\nstandard options) and with the presence of jump risk for equity. Monte Carlo\nexperiments demonstrate the vast superiority of non-quadratic global hedging as\nit results simultaneously in downside risk metrics two to three times smaller\nthan best benchmarks and in significant hedging gains. Analyses show that the\nneural networks are able to effectively adapt their hedging decisions to\ndifferent penalties and stylized facts of risky asset dynamics only by\nexperiencing simulations of the financial market exhibiting these features.\nNumerical results also indicate that non-quadratic global policies are\nsignificantly more geared towards being long equity risk which entails earning\nthe equity risk premium.\n"
    },
    {
        "paper_id": 2007.15265,
        "authors": "Xiaojun Chen, Yun Shi, Xiaozhou Wang",
        "title": "Equilibrium Oil Market Share under the COVID-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Equilibrium models for energy markets under uncertain demand and supply have\nattracted considerable attentions. This paper focuses on modelling crude oil\nmarket share under the COVID-19 pandemic using two-stage stochastic\nequilibrium. We describe the uncertainties in the demand and supply by random\nvariables and provide two types of production decisions (here-and-now and\nwait-and-see). The here-and-now decision in the first stage does not depend on\nthe outcome of random events to be revealed in the future and the wait-and-see\ndecision in the second stage is allowed to depend on the random events in the\nfuture and adjust the feasibility of the here-and-now decision in rare\nunexpected scenarios such as those observed during the COVID-19 pandemic. We\ndevelop a fast algorithm to find a solution of the two-stage stochastic\nequilibrium. We show the robustness of the two-stage stochastic equilibrium\nmodel for forecasting the oil market share using the real market data from\nJanuary 2019 to May 2020.\n"
    },
    {
        "paper_id": 2007.15475,
        "authors": "Roland R. Ramsahai",
        "title": "Connecting actuarial judgment to probabilistic learning techniques with\n  graph theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Graphical models have been widely used in applications ranging from medical\nexpert systems to natural language processing. Their popularity partly arises\nsince they are intuitive representations of complex inter-dependencies among\nvariables with efficient algorithms for performing computationally intensive\ninference in high-dimensional models. It is argued that the formalism is very\nuseful for applications in the modelling of non-life insurance claims data. It\nis also shown that actuarial models in current practice can be expressed\ngraphically to exploit the advantages of the approach. More general models are\nproposed within the framework to demonstrate the potential use of graphical\nmodels for probabilistic learning with telematics and other dynamic actuarial\ndata. The discussion also demonstrates throughout that the intuitive nature of\nthe models allows the inclusion of qualitative knowledge or actuarial judgment\nin analyses.\n"
    },
    {
        "paper_id": 2007.15545,
        "authors": "Carlo Campajola and Domenico Di Gangi and Fabrizio Lillo and Daniele\n  Tantari",
        "title": "Modelling time-varying interactions in complex systems: the Score Driven\n  Kinetic Ising Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A common issue when analyzing real-world complex systems is that the\ninteractions between the elements often change over time: this makes it\ndifficult to find optimal models that describe this evolution and that can be\nestimated from data, particularly when the driving mechanisms are not known.\nHere we offer a new perspective on the development of models for time-varying\ninteractions introducing a generalization of the well-known Kinetic Ising Model\n(KIM), a minimalistic pairwise constant interactions model which has found\napplications in multiple scientific disciplines. Keeping arbitrary choices of\ndynamics to a minimum and seeking information theoretical optimality, the\nScore-Driven methodology lets us significantly increase the knowledge that can\nbe extracted from data using the simple KIM. In particular, we first identify a\nparameter whose value at a given time can be directly associated with the local\npredictability of the dynamics. Then we introduce a method to dynamically learn\nthe value of such parameter from the data, without the need of specifying\nparametrically its dynamics. Finally, we extend our framework to disentangle\ndifferent sources (e.g. endogenous vs exogenous) of predictability in real\ntime.\n  We apply our methodology to several complex systems including financial\nmarkets, temporal (social) networks, and neuronal populations. Our results show\nthat the Score-Driven KIM produces insightful descriptions of the systems,\nallowing to predict forecasting accuracy in real time as well as to separate\ndifferent components of the dynamics. This provides a significant\nmethodological improvement for data analysis in a wide range of disciplines.\n"
    },
    {
        "paper_id": 2007.1555,
        "authors": "Adel Daoud, Anders Herlitz, and SV Subramanian",
        "title": "Combining distributive ethics and causal Inference to make trade-offs\n  between austerity and population health",
        "comments": "Working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The International Monetary Fund (IMF) provides financial assistance to its\nmember-countries in economic turmoil, but requires at the same time that these\ncountries reform their public policies. In several contexts, these reforms are\nat odds with population health. While researchers have empirically analyzed the\nconsequences of these reforms on health, no analysis exist on identifying fair\ntradeoffs between consequences on population health and economic outcomes. Our\narticle analyzes and identifies the principles governing these tradeoffs.\nFirst, this article reviews existing policy-evaluation studies, which show, on\nbalance, that IMF policies frequently cause adverse effects on child health and\nmaterial standards in the pursuit of macroeconmic improvement. Second, this\narticle discusses four theories in distributive ethics (maximization,\negalitarianianism, prioritarianiasm, and sufficientarianism) to identify which\nis the most compatible with the core mission of the IMF, that is, improved\nmacroeconomics (Articles of Agreement) while at the same time balancing\nconsequences on health. Using a distributive-ethics analyses of IMF polices, we\nargue that sufficientarianism is the most compatible theory. Third, this\narticle offer a qualitative rearticulation of the Articles of Agreement, and\nformalize sufficientarian principles in the language of causal inference. We\nalso offer a framework on how to empirically measure, from observational data,\nthe extent that IMF policies trade off fairly between population health and\neconomic outcomes. We conclude with policy recommendations and suggestions for\nfuture research.\n"
    },
    {
        "paper_id": 2007.15704,
        "authors": "Tymofii Brik and Maksym Obrizan",
        "title": "Job market effects of COVID-19 on urban Ukrainian households",
        "comments": "12 pages, 0 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The employment status of billions of people has been affected by the COVID\nepidemic around the Globe. New evidence is needed on how to mitigate the job\nmarket crisis, but there exists only a handful of studies mostly focusing on\ndeveloped countries. We fill in this gap in the literature by using novel data\nfrom Ukraine, a transition country in Eastern Europe, which enacted strict\nquarantine policies early on. We model four binary outcomes to identify\nrespondents (i) who are not working during quarantine, (ii) those who are more\nlikely to work from home, (iii) respondents who are afraid of losing a job,\nand, finally, (iv) survey participants who have savings for 1 month or less if\nquarantine is further extended. Our findings suggest that respondents employed\nin public administration, programming and IT, as well as highly qualified\nspecialists, were more likely to secure their jobs during the quarantine.\nFemales, better educated respondents, and those who lived in Kyiv were more\nlikely to work remotely. Working in the public sector also made people more\nconfident about their future employment perspectives. Although our findings are\nlimited to urban households only, they provide important early evidence on the\ncorrelates of job market outcomes, expectations, and financial security,\nindicating potential deterioration of socio-economic inequalities.\n"
    },
    {
        "paper_id": 2007.1598,
        "authors": "Ale\\v{s} \\v{C}ern\\'y",
        "title": "The Hansen ratio in mean--variance portfolio theory",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is shown that the ratio between the mean and the $L^2$-norm leads to a\nparticularly parsimonious description of the mean-variance efficient frontier\nand the dual pricing kernel restrictions known as the Hansen-Jagannathan (HJ)\nbounds. Because this ratio has not appeared in economic theory previously, it\nseems appropriate to name it the Hansen ratio. The initial treatment of the\nmean-variance theory via the Hansen ratio is extended in two directions, to\nmonotone mean-variance preferences and to arbitrary Hilbert space setting. A\nmultiperiod example with IID returns is also discussed.\n"
    },
    {
        "paper_id": 2007.15982,
        "authors": "Trent Spears, Stefan Zohren and Stephen Roberts",
        "title": "Investment sizing with deep learning prediction uncertainties for\n  high-frequency Eurodollar futures trading",
        "comments": "15 pages, 6 Figures, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we show that prediction uncertainty estimates gleaned from deep\nlearning models can be useful inputs for influencing the relative allocation of\nrisk capital across trades. In this way, consideration of uncertainty is\nimportant because it permits the scaling of investment size across trade\nopportunities in a principled and data-driven way. We showcase this insight\nwith a prediction model and find clear outperformance based on a Sharpe ratio\nmetric, relative to trading strategies that either do not take uncertainty into\naccount, or that utilize an alternative market-based statistic as a proxy for\nuncertainty. Of added novelty is our modelling of high-frequency data at the\ntop level of the Eurodollar Futures limit order book for each trading day of\n2018, whereby we predict interest rate curve changes on small time horizons. We\nare motivated to study the market for these popularly-traded interest rate\nderivatives since it is deep and liquid, and contributes to the efficient\nfunctioning of global finance -- though there is relatively little by way of\nits modelling contained in the academic literature. Hence, we verify the\nutility of prediction models and uncertainty estimates for trading applications\nin this complex and multi-dimensional asset price space.\n"
    },
    {
        "paper_id": 2007.16096,
        "authors": "Nassim Nicholas Taleb, Yaneer Bar-Yam, and Pasquale Cirillo",
        "title": "On Single Point Forecasts for Fat-Tailed Variables",
        "comments": "Accepted, International Journal of Forecasting",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss common errors and fallacies when using naive \"evidence based\"\nempiricism and point forecasts for fat-tailed variables, as well as the\ninsufficiency of using naive first-order scientific methods for tail risk\nmanagement. We use the COVID-19 pandemic as the background for the discussion\nand as an example of a phenomenon characterized by a multiplicative nature, and\nwhat mitigating policies must result from the statistical properties and\nassociated risks. In doing so, we also respond to the points raised by\nIoannidis et al. (2020).\n"
    },
    {
        "paper_id": 2008.00124,
        "authors": "Qi Guo, Bruno Remillard, Anatoliy Swishchuk",
        "title": "Multivariate General Compound Point Processes in Limit Order Books",
        "comments": "16 pages and 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we focus on a new generalization of multivariate general\ncompound Hawkes process (MGCHP), which we referred to as the multivariate\ngeneral compound point process (MGCPP). Namely, we applied a multivariate point\nprocess to model the order flow instead of the Hawkes process. Law of large\nnumbers (LLN) and two functional central limit theorems (FCLTs) for the MGCPP\nwere proved in this work. Applications of the MGCPP in the limit order market\nwere also considered. We provided numerical simulations and comparisons for the\nMGCPP and MGCHP by applying Google, Apple, Microsoft, Amazon, and Intel trading\ndata.\n"
    },
    {
        "paper_id": 2008.00253,
        "authors": "Kevin L. McKinney and John M. Abowd",
        "title": "Male Earnings Volatility in LEHD before, during, and after the Great\n  Recession",
        "comments": "Revision submitted to JBES with figures included in the text and\n  Appendix added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is part of a coordinated collection of papers on prime-age male\nearnings volatility. Each paper produces a similar set of statistics for the\nsame reference population using a different primary data source. Our primary\ndata source is the Census Bureau's Longitudinal Employer-Household Dynamics\n(LEHD) infrastructure files. Using LEHD data from 1998 to 2016, we create a\nwell-defined population frame to facilitate accurate estimation of temporal\nchanges comparable to designed longitudinal samples of people. We show that\nearnings volatility, excluding increases during recessions, has declined over\nthe analysis period, a finding robust to various sensitivity analyses.\n"
    },
    {
        "paper_id": 2008.00391,
        "authors": "Chonghu Guan, Zuo Quan Xu, Rui Zhou",
        "title": "Dynamic optimal reinsurance and dividend-payout in finite time horizon",
        "comments": "7 figures",
        "journal-ref": null,
        "doi": "10.1287/moor.2022.1276",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a dynamic optimal reinsurance and dividend-payout problem\nfor an insurance company in a finite time horizon. The goal of the company is\nto maximize the expected cumulative discounted dividend payouts until\nbankruptcy or maturity which comes earlier. The company is allowed to buy\nreinsurance contracts dynamically over the whole time horizon to cede its risk\nexposure with other reinsurance companies. This is a mixed singular-classical\ncontrol problem and the corresponding Hamilton-Jacobi-Bellman equation is a\nvariational inequality with a fully nonlinear operator and subject to a\ngradient constraint. We obtain the $C^{2,1}$ smoothness of the value function\nand a comparison principle for its gradient function by the penalty\napproximation method so that one can establish an efficient numerical scheme to\ncompute the value function. We find that the surplus-time space can be divided\ninto three non-overlapping regions by a risk-magnitude and time-dependent\nreinsurance barrier and a time-dependent dividend-payout barrier. The insurance\ncompany should be exposed to a higher risk as its surplus increases; be exposed\nto the entire risk once its surplus upward crosses the reinsurance barrier; and\npay out all its reserves exceeding the dividend-payout barrier. The estimated\nlocalities of these regions are also provided.\n"
    },
    {
        "paper_id": 2008.00392,
        "authors": "Hyun Jin Jang, Zuo Quan Xu, Harry Zheng",
        "title": "Optimal Investment, Heterogeneous Consumption and Best Time for\n  Retirement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal investment and consumption problem with\nheterogeneous consumption of basic and luxury goods, together with the choice\nof time for retirement. The utility for luxury goods is not necessarily a\nconcave function. The optimal heterogeneous consumption strategies for a class\nof non-homothetic utility maximizer are shown to consume only basic goods when\nthe wealth is small, to consume basic goods and make savings when the wealth is\nintermediate, and to consume almost all in luxury goods when the wealth is\nlarge. The optimal retirement policy is shown to be both universal, in the\nsense that all individuals should retire at the same level of marginal utility\nthat is determined only by income, labor cost, discount factor as well as\nmarket parameters, and not universal, in the sense that all individuals can\nachieve the same marginal utility with different utility and wealth. It is also\nshown that individuals prefer to retire as time goes by if the marginal labor\ncost increases faster than that of income. The main tools used in analyzing the\nproblem are from PDE and stochastic control theory including variational\ninequality and dual transformation. We finally conduct the simulation analysis\nfor the featured model parameters to investigate practical and economic\nimplications by providing their figures.\n"
    },
    {
        "paper_id": 2008.00462,
        "authors": "Anindya Goswami and Sharan Rajani and Atharva Tanksale",
        "title": "Data-Driven Option Pricing using Single and Multi-Asset Supervised\n  Learning",
        "comments": "18 figures, 11 tables, 25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose three different data-driven approaches for pricing European-style\ncall options using supervised machine-learning algorithms. These approaches\nyield models that give a range of fair prices instead of a single price point.\nThe performance of the models are tested on two stock market indices: NIFTY$50$\nand BANKNIFTY from the Indian equity market. Although neither historical nor\nimplied volatility is used as an input, the results show that the trained\nmodels have been able to capture the option pricing mechanism better than or\nsimilar to the Black-Scholes formula for all the experiments. Our choice of\nscale free I/O allows us to train models using combined data of multiple\ndifferent assets from a financial market. This not only allows the models to\nachieve far better generalization and predictive capability, but also solves\nthe problem of paucity of data, the primary limitation of using machine\nlearning techniques. We also illustrate the performance of the trained models\nin the period leading up to the 2020 Stock Market Crash (Jan 2019 to April\n2020).\n"
    },
    {
        "paper_id": 2008.0047,
        "authors": "Eyal Neuman, Alexander Schied, Chengguo Weng, Xiaole Xue",
        "title": "A central bank strategy for defending a currency peg",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a central bank strategy for maintaining a two-sided currency\ntarget zone, in which an exchange rate of two currencies is forced to stay\nbetween two thresholds. To keep the exchange rate from breaking the prescribed\nbarriers, the central bank is generating permanent price impact and thereby\naccumulating inventory in the foreign currency. Historical examples of failed\ntarget zones illustrate that this inventory can become problematic, in\nparticular when there is an adverse macroeconomic trend in the market. We model\nthis situation through a continuous-time market impact model of\nAlmgren--Chriss-type with drift, in which the exchange rate is a diffusion\nprocess controlled by the price impact of the central bank's intervention\nstrategy. The objective of the central bank is to enforce the target zone\nthrough a strategy that minimizes the accumulated inventory. We formulate this\nobjective as a stochastic control problem with random time horizon. It is\nsolved by reduction to a singular boundary value problem that was solved by\nLasry and Lions (1989). Finally, we provide numerical simulations of optimally\ncontrolled exchange rate processes and the corresponding evolution of the\ncentral bank inventory.\n"
    },
    {
        "paper_id": 2008.0086,
        "authors": "Kenichi Hirayama, Akihiko Noda",
        "title": "Evaluating the Financial Market Function in Prewar Japan using a\n  Time-Varying Parameter Model",
        "comments": "25 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores when the financial market lost the price formation\nfunction in prewar Japan in the sense of Fama's (1970) semi-strong form market\nefficiency using a new dataset. We particularly focus on the relationship\nbetween the prewar Japanese financial market and several government policy\ninterventions to explore whether the semi-strong form market efficiency evolves\nover time. To capture the long-run impact of government policy interventions\nagainst the markets, we measure the time-varying joint degree of market\nefficiency and the time-varying impulse responses based on Ito et al.'s (2014;\n2017) generalized least squares-based time-varying vector autoregressive model.\nThe empirical results reveal that (1) the joint degree of market efficiency in\nthe prewar Japanese financial market fluctuated over time because of external\nevents such as policy changes and wars, (2) the semi-strong form EMH is almost\nsupported in the prewar Japanese financial market, (3) Lo's (2004) adaptive\nmarket hypothesis is supported in the prewar Japanese financial market even if\nwe consider that the public information affects the financial markets, and (4)\nthe prewar Japanese financial markets lost the price formation function in 1932\nand that was a turning point in the market.\n"
    },
    {
        "paper_id": 2008.00863,
        "authors": "Rui Zhou, Daniel P. Palomar",
        "title": "Solving High-Order Portfolios via Successive Convex Approximation\n  Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The first moment and second central moments of the portfolio return, a.k.a.\nmean and variance, have been widely employed to assess the expected profit and\nrisk of the portfolio. Investors pursue higher mean and lower variance when\ndesigning the portfolios. The two moments can well describe the distribution of\nthe portfolio return when it follows the Gaussian distribution. However, the\nreal world distribution of assets return is usually asymmetric and\nheavy-tailed, which is far from being a Gaussian distribution. The asymmetry\nand the heavy-tailedness are characterized by the third and fourth central\nmoments, i.e., skewness and kurtosis, respectively. Higher skewness and lower\nkurtosis are preferred to reduce the probability of extreme losses. However,\nincorporating high-order moments in the portfolio design is very difficult due\nto their non-convexity and rapidly increasing computational cost with the\ndimension. In this paper, we propose a very efficient and convergence-provable\nalgorithm framework based on the successive convex approximation (SCA)\nalgorithm to solve high-order portfolios. The efficiency of the proposed\nalgorithm framework is demonstrated by the numerical experiments.\n"
    },
    {
        "paper_id": 2008.00908,
        "authors": "Eunjung Noh",
        "title": "Equilibrium under TWAP trading with quadratic transaction costs",
        "comments": "I find serious fundamental mistakes, so I would like to withdraw the\n  paper. I will work again and upload later",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how transaction cost affects to the equilibrium return and optimal\nstock holdings in equilibrium. To this end, we develop a continuous-time\nrisk-sharing model where heterogenous agents trade toward terminal target\nholdings subject to a quadratic transaction cost. The equilibrium stock\nholdings and trading rate under transaction cost are characterized by a unique\nsolution to a forward-backward stochastic differential equation (FBSDE). The\nequilibrium return is also characterized as the unique solution of a system of\ncoupled but linear FBSDEs.\n"
    },
    {
        "paper_id": 2008.00925,
        "authors": "Chinonso Nwankwo and Weizhong Dai",
        "title": "Multigrid Iterative Algorithm based on Compact Finite Difference Schemes\n  and Hermite interpolation for Solving Regime Switching American Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a multigrid iterative algorithm for solving a system of coupled\nfree boundary problems for pricing American put options with regime-switching.\nThe algorithm is based on our recently developed compact finite difference\nscheme coupled with Hermite interpolation for solving the coupled partial\ndifferential equations consisting of the asset option and the delta, gamma, and\nspeed sensitivities. In the algorithm, we first use the Gauss-Seidel method as\na smoother and then implement a multigrid strategy based on modified cycle\n(M-cycle) for solving our discretized equations. Hermite interpolation with\nNewton interpolatory divided difference (as the basis) is used in estimating\nthe coupled asset, delta, gamma, and speed options in the set of equations. A\nnumerical experiment is performed with the two- and four- regime examples and\ncompared with other existing methods to validate the optimal strategy. Results\nshow that this algorithm provides a fast and efficient tool for pricing\nAmerican put options with regime-switching.\n"
    },
    {
        "paper_id": 2008.01241,
        "authors": "Christian Bayer, Jinniao Qiu, Yao Yao",
        "title": "Pricing Options Under Rough Volatility with Backward SPDEs",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the option pricing problems for rough volatility\nmodels. As the framework is non-Markovian, the value function for a European\noption is not deterministic; rather, it is random and satisfies a backward\nstochastic partial differential equation (BSPDE). The existence and uniqueness\nof weak solution is proved for general nonlinear BSPDEs with unbounded random\nleading coefficients whose connections with certain forward-backward stochastic\ndifferential equations are derived as well. These BSPDEs are then used to\napproximate American option prices. A deep leaning-based method is also\ninvestigated for the numerical approximations to such BSPDEs and associated\nnon-Markovian pricing problems. Finally, the examples of rough Bergomi type are\nnumerically computed for both European and American options.\n"
    },
    {
        "paper_id": 2008.01277,
        "authors": "Hong Shaopeng",
        "title": "Generalized Autoregressive Score asymmetric Laplace Distribution and\n  Extreme Downward Risk Prediction",
        "comments": "13 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to the skessed distribution, high peak and thick tail and asymmetry of\nfinancial return data, it is difficult to describe the traditional\ndistribution. In recent years, generalized autoregressive score (GAS) has been\nused in many fields and achieved good results. In this paper, under the\nframework of generalized autoregressive score (GAS), the asymmetric Laplace\ndistribution (ALD) is improved, and the GAS-ALD model is proposed, which has\nthe characteristics of time-varying parameters, can describe the peak thick\ntail, biased and asymmetric distribution. The model is used to study the\nShanghai index, Shenzhen index and SME board index. It is found that: 1) the\ndistribution parameters and moments of the three indexes have obvious\ntime-varying characteristics and aggregation characteristics. 2) Compared with\nthe commonly used models for calculating VaR and ES, the GAS-ALD model has a\nhigh prediction effect.\n"
    },
    {
        "paper_id": 2008.01385,
        "authors": "Paul Hager and Eyal Neuman",
        "title": "The Multiplicative Chaos of $H=0$ Fractional Brownian Fields",
        "comments": "48 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a family of fractional Brownian fields $\\{B^{H}\\}_{H\\in (0,1)}$\non $\\mathbb{R}^{d}$, where $H$ denotes their Hurst parameter. We first define a\nrich class of normalizing kernels $\\psi$ such that the covariance of $$\nX^{H}(x) = \\Gamma(H)^{\\frac{1}{2}} \\left( B^{H}(x) - \\int_{\\mathbb{R}^{d}}\nB^{H}(u) \\psi(u, x)du\\right), $$ converges to the covariance of a\nlog-correlated Gaussian field when $H \\downarrow 0$.\n  We then use Berestycki's ``good points'' approach in order to derive the\nlimiting measure of the so-called multiplicative chaos of the fractional\nBrownian field $$ M^{H}_\\gamma(dx) = e^{\\gamma X^{H}(x) - \\frac{\\gamma^{2}}{2}\nE[X^{H}(x)^{2}] }dx, $$ as $H\\downarrow 0$ for all $\\gamma \\in\n(0,\\gamma^{*}(d)]$, where $\\gamma^{*}(d)>\\sqrt{\\frac{7}{4}d}$. As a corollary\nwe establish the $L^{2}$ convergence of $M^{H}_\\gamma$ over the sets of ``good\npoints'', where the field $X^H$ has a typical behaviour. As a by-product of the\nconvergence result, we prove that for log-normal rough volatility models with\nsmall Hurst parameter, the volatility process is supported on the sets of\n``good points'' with probability close to $1$. Moreover, on these sets the\nvolatility converges in $L^2$ to the volatility of multifractal random walks.\n"
    },
    {
        "paper_id": 2008.01463,
        "authors": "Teemu Pennanen and Udomsak Rakwongwan",
        "title": "Optimal semi-static hedging in illiquid markets",
        "comments": "25 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study indifference pricing of exotic derivatives by using hedging\nstrategies that take static positions in quoted derivatives but trade the\nunderlying and cash dynamically over time. We use real quotes that come with\nbid-ask spreads and finite quantities. Galerkin method and integration\nquadratures are used to approximate the hedging problem by a finite dimensional\nconvex optimization problem which is solved by an interior point method. The\ntechniques are extended also to situations where the underlying is subject to\nbid-ask spreads. As an illustration, we compute indifference prices of\npath-dependent options written on S&P500 index. Semi-static hedging improves\nconsiderably on the purely static options strategy as well as dynamic trading\nwithout options. The indifference prices make good economic sense even in the\npresence of arbitrage opportunities that are found when the underlying is\nassumed perfectly liquid. When transaction costs are introduced, the arbitrage\nopportunities vanish but the indifference prices remain almost unchanged.\n"
    },
    {
        "paper_id": 2008.01535,
        "authors": "Kwadwo Osei Bonsu",
        "title": "Weighted Accuracy Algorithmic Approach In Counteracting Fake News And\n  Disinformation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2478/ers-2021-0007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the world is becoming more dependent on the internet for information\nexchange, some overzealous journalists, hackers, bloggers, individuals and\norganizations tend to abuse the gift of free information environment by\npolluting it with fake news, disinformation and pretentious content for their\nown agenda. Hence, there is the need to address the issue of fake news and\ndisinformation with utmost seriousness. This paper proposes a methodology for\nfake news detection and reporting through a constraint mechanism that utilizes\nthe combined weighted accuracies of four machine learning algorithms.\n"
    },
    {
        "paper_id": 2008.01649,
        "authors": "Roy Cerqueti and Valerio Ficcadenti",
        "title": "Anxiety for the pandemic and trust in financial markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has generated disruptive changes in many fields. Here\nwe focus on the relationship between the anxiety felt by people during the\npandemic and the trust in the future performance of financial markets.\nPrecisely, we move from the idea that the volume of Google searches about\n\"coronavirus\" can be considered as a proxy of the anxiety and, jointly with the\nstock index prices, can be used to produce mood indicators -- in terms of\npessimism and optimism -- at country level. We analyse the \"very high human\ndeveloped countries\" according to the Human Development Index plus China and\ntheir respective main stock market indexes. Namely, we propose both a temporal\nand a global measure of pessimism and optimism and provide accordingly a\nclassification of indexes and countries. The results show the existence of\ndifferent clusters of countries and markets in terms of pessimism and optimism.\nMoreover, specific regimes along the time emerge, with an increasing optimism\nspreading during the mid of June 2020. Furthermore, countries with different\ngovernment responses to the pandemic have experienced different levels of mood\nindicators, so that countries with less strict lockdown had a higher level of\noptimism.\n"
    },
    {
        "paper_id": 2008.0167,
        "authors": "Zhongfang Zhuang, Chin-Chia Michael Yeh, Liang Wang, Wei Zhang,\n  Junpeng Wang",
        "title": "Multi-stream RNN for Merchant Transaction Prediction",
        "comments": "Accepted by KDD 2020 Workshop on Machine Learning in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, digital payment systems have significantly changed people's\nlifestyles. New challenges have surfaced in monitoring and guaranteeing the\nintegrity of payment processing systems. One important task is to predict the\nfuture transaction statistics of each merchant. These predictions can thus be\nused to steer other tasks, ranging from fraud detection to recommendation. This\nproblem is challenging as we need to predict not only multivariate time series\nbut also multi-steps into the future. In this work, we propose a multi-stream\nRNN model for multi-step merchant transaction predictions tailored to these\nrequirements. The proposed multi-stream RNN summarizes transaction data in\ndifferent granularity and makes predictions for multiple steps in the future.\nOur extensive experimental results have demonstrated that the proposed model is\ncapable of outperforming existing state-of-the-art methods.\n"
    },
    {
        "paper_id": 2008.01687,
        "authors": "A. R. Provenzano, D. Trifir\\`o, A. Datteo, L. Giada, N. Jean, A.\n  Riciputi, G. Le Pera, M. Spadaccino, L. Massaron and C. Nordio",
        "title": "Machine Learning approach for Credit Scoring",
        "comments": "28 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we build a stack of machine learning models aimed at composing a\nstate-of-the-art credit rating and default prediction system, obtaining\nexcellent out-of-sample performances. Our approach is an excursion through the\nmost recent ML / AI concepts, starting from natural language processes (NLP)\napplied to economic sectors' (textual) descriptions using embedding and\nautoencoders (AE), going through the classification of defaultable firms on the\nbase of a wide range of economic features using gradient boosting machines\n(GBM) and calibrating their probabilities paying due attention to the treatment\nof unbalanced samples. Finally we assign credit ratings through genetic\nalgorithms (differential evolution, DE). Model interpretability is achieved by\nimplementing recent techniques such as SHAP and LIME, which explain predictions\nlocally in features' space.\n"
    },
    {
        "paper_id": 2008.01828,
        "authors": "Daniel L. Mendoza (1 and 2), Tabitha M. Benney (3), Rajive Ganguli\n  (4), Rambabu Pothina (4), Benjamin Krick (3), Cheryl S. Pirozzi (5), Erik T.\n  Crosman (6), Yue Zhang (7) ((1) Department of Atmospheric Sciences,\n  University of Utah, Salt Lake City, Utah USA, (2) Department of City &\n  Metropolitan Planning, University of Utah, Salt Lake City, Utah USA, (3)\n  Department of Political Science, University of Utah, Salt Lake City, Utah\n  USA, (4) Department of Mining Engineering, University of Utah, Salt Lake\n  City, Utah USA, (5) Division of Pulmonary and Critical Care Medicine,\n  Department of Internal Medicine, School of Medicine, University of Utah, Salt\n  Lake City, Utah USA, (6) Department of Life, Earth, and Environmental\n  Sciences, West Texas A&M University, Canyon, Texas USA, (7) Division of\n  Epidemiology, Department of Internal Medicine, School of Medicine, University\n  of Utah, Salt Lake City, Utah USA)",
        "title": "Understanding the Relationship between Social Distancing Policies,\n  Traffic Volume, Air Quality, and the Prevalence of COVID-19 Outcomes in Urban\n  Neighborhoods",
        "comments": "34 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.18592.20487",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In response to the COVID-19 pandemic, governments have implemented policies\nto curb the spread of the novel virus. Little is known about how these policies\nimpact various groups in society. This paper explores the relationship between\nsocial distancing policies, traffic volumes and air quality and how they impact\nvarious socioeconomic groups. This study aims to understand how disparate\ncommunities respond to Stay-at-Home Orders and other social distancing policies\nto understand how human behavior in response to policy may play a part in the\nprevalence of COVID-19 positive cases. We collected data on traffic density,\nair quality, socio-economic status, and positive cases rates of COVID-19 for\neach zip code of Salt Lake County, Utah (USA) between February 17 and June 12,\n2020. We studied the impact of social distancing policies across three periods\nof policy implementation. We found that wealthier and whiter zip codes\nexperienced a greater reduction in traffic and air pollution during the\nStay-at-Home period. However, air quality did not necessarily follow traffic\nvolumes in every case due to the complexity of interactions between emissions\nand meteorology. We also found a strong relationship between lower\nsocioeconomic status and positive COVID-19 rates. This study provides initial\nevidence for social distancing's effectiveness in limiting the spread of\nCOVID-19, while providing insight into how socioeconomic status has compounded\nvulnerability during this crisis. Behavior restrictions disproportionately\nbenefit whiter and wealthier communities both through protection from spread of\nCOVID-19 and reduction in air pollution. Such findings may be further\ncompounded by the impacts of air pollution, which likely exacerbate COVID-19\ntransmission and mortality rates. Policy makers need to consider adapting\nsocial distancing policies to maximize equity in health protection.\n"
    },
    {
        "paper_id": 2008.02166,
        "authors": "Ionut Jianu, Laura-Madalina Pirscoveanu, Maria-Daniela Tudorache",
        "title": "The impact of financial risks on economic growth in EU-15",
        "comments": "22 pg",
        "journal-ref": "Theoretical and Applied Economics, Vol. 24 (2017), No. 1",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the impact of financial risks on economic growth in the\nfirst 15 Member States of the European Union, considering 1995-2014 period and\naims to lay down a new explanatory model of economic growth, based mainly on\nthe behavioral reactivity of the financial disruptions mentioned above. The\nmodel was estimated through the panel estimated generalized least squares\nmethod and included additional control variables in order to strengthen the\nresearch conducted. Our goal consists in the examination of the financial risks\nin the European Union and in the estimation of their impact on economic growth.\n"
    },
    {
        "paper_id": 2008.0223,
        "authors": "Vasil Yasenov and David Hausman and Michael Hotard and Duncan Lawrence\n  and Alexandra Siegel and Jessica S. Wolff and David D. Laitin and Jens\n  Hainmueller",
        "title": "Identifying Opportunities to Improve the Network of Immigration Legal\n  Services Providers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Immigration legal services providers (ISPs) are a principal source of support\nfor low-income immigrants seeking immigration benefits. Yet there is scant\nquantitative evidence on the prevalence and geographic distribution of ISPs in\nthe United States. To fill this gap, we construct a comprehensive, nationwide\ndatabase of 2,138 geocoded ISP offices that offer low- or no-cost legal\nservices to low-income immigrants. We use spatial optimization methods to\nanalyze the geographic network of ISPs and measure ISPs' proximity to the\nlow-income immigrant population. Because both ISPs and immigrants are highly\nconcentrated in major urban areas, most low-income immigrants live close to an\nISP. However, we also find a sizable fraction of low-income immigrants in\nunderserved areas, which are primarily in midsize cities in the South. This\nreflects both a general skew in non-governmental organization service provision\nand the more recent arrival of immigrants in these largely Southern\ndestinations. Finally, our optimization analysis suggests significant gains\nfrom placing new ISPs in underserved areas to maximize the number of low-income\nimmigrants who live near an ISP. Overall, our results provide vital information\nto immigrants, funders, and policymakers about the current state of the ISP\nnetwork and opportunities to improve it.\n"
    },
    {
        "paper_id": 2008.0242,
        "authors": "Xiangyu Wang, Jianming Xia, Zuo Quan Xu, Zhou Yang",
        "title": "Minimal Quantile Functions Subject to Stochastic Dominance Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a problem of finding an SSD (second-order stochastic\ndominance)-minimal quantile function subject to the mixture of FSD (first-order\nstochastic dominance) and SSD constraints. The SSD-minimal solution is\nexplicitly worked out and has a close relation to the Skorokhod problem. This\nresult is then applied to explicitly solve a risk minimizing problem in\nfinancial economics.\n"
    },
    {
        "paper_id": 2008.02581,
        "authors": "Juan Dominguez-Moran and Rouven Geismar",
        "title": "Teaching Economics with Interactive Browser-Based Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interactive simulation toolkits come in handy when teaching macroeconomic\nmodels by facilitating an easy understanding of underlying economic concepts\nand offering an intuitive approach to the models' comparative statics. Based on\nthe example of the IS-LM model, this paper demonstrates innovative\nbrowser-based features well-suited for the shift in education to online\nplatforms accelerated by COVID-19. The free and open-source code can be found\nalongside the standalone HTML files for the AD-AS and the Solow growth model at\nhttps://gitlab.tu-berlin.de/chair-of-macroeconomics/.\n"
    },
    {
        "paper_id": 2008.02629,
        "authors": "Monica Azqueta-Gavaldon, Gonzalo Azqueta-Gavaldon, Inigo\n  Azqueta-Gavaldon, and Andres Azqueta-Gavaldon",
        "title": "Developing a real estate yield investment deviceusing granular data and\n  machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This project aims at creating an investment device to help investors\ndetermine which real estate units have a higher return to investment in Madrid.\nTo do so, we gather data from Idealista.com, a real estate web-page with\nmillions of real estate units across Spain, Italy and Portugal. In this\npreliminary version, we present the road map on how we gather the data;\ndescriptive statistics of the 8,121 real estate units gathered (rental and\nsale); build a return index based on the difference in prices of rental and\nsale units(per neighbourhood and size) and introduce machine learning\nalgorithms for rental real estate price prediction.\n"
    },
    {
        "paper_id": 2008.02649,
        "authors": "Milena Lopreite, Pietro Panzarasa, Michelangelo Puliga, Massimo\n  Riccaboni",
        "title": "Early warnings of COVID-19 outbreaks across Europe from social media?",
        "comments": "Work submitted to Scientific Reports",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze data from Twitter to uncover early-warning signals of COVID-19\noutbreaks in Europe in the winter season 2019-2020, before the first public\nannouncements of local sources of infection were made. We show evidence that\nunexpected levels of concerns about cases of pneumonia were raised across a\nnumber of European countries. Whistleblowing came primarily from the\ngeographical regions that eventually turned out to be the key breeding grounds\nfor infections. These findings point to the urgency of setting up an integrated\ndigital surveillance system in which social media can help geo-localize chains\nof contagion that would otherwise proliferate almost completely undetected.\n"
    },
    {
        "paper_id": 2008.03123,
        "authors": "Weihong Ni, Corina Constantinescu, Alfredo Eg\\'idio dos Reis,\n  V\\'eronique Maume-Deschamps (ICJ, PSPM)",
        "title": "Pricing foreseeable and unforeseeable risks in insurance portfolios",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1905.07157",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this manuscript we propose a method for pricing insurance products that\ncover not only traditional risks, but also unforeseen ones. By considering the\nPoisson process parameter to be a mixed random variable, we capture the\nheterogeneity of foreseeable and unforeseeable risks. To illustrate, we\nestimate the weights for the two risk streams for a real dataset from a\nPortuguese insurer. To calculate the premium, we set the frequency and severity\nas distributions that belong to the linear exponential family. Under a Bayesian\nsetup , we show that when working with a finite mixture of conjugate priors,\nthe premium can be estimated by a mixture of posterior means, with updated\nparameters, depending on claim histories. We emphasise the riskiness of the\nunforeseeable trend, by choosing heavy-tailed distributions. After estimating\ndistribution parameters involved using the Expectation-Maximization algorithm,\nwe found that Bayesian premiums derived are more reactive to claim trends than\ntraditional ones.\n"
    },
    {
        "paper_id": 2008.03204,
        "authors": "Christian Bayer and Fabian Andsem Harang and Paolo Pigato",
        "title": "Log-modulated rough stochastic volatility models",
        "comments": "28 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new class of rough stochastic volatility models obtained by\nmodulating the power-law kernel defining the fractional Brownian motion (fBm)\nby a logarithmic term, such that the kernel retains square integrability even\nin the limit case of vanishing Hurst index $H$. The so-obtained log-modulated\nfractional Brownian motion (log-fBm) is a continuous Gaussian process even for\n$H = 0$. As a consequence, the resulting super-rough stochastic volatility\nmodels can be analysed over the whole range $0 \\le H < 1/2$ without the need of\nfurther normalization. We obtain skew asymptotics of the form $\\log(1/T)^{-p}\nT^{H-1/2}$ as $T\\to 0$, $H \\ge 0$, so no flattening of the skew occurs as $H\n\\to 0$.\n"
    },
    {
        "paper_id": 2008.03283,
        "authors": "M. Alper \\c{C}enesiz and Lu\\'is Guimar\\~aes",
        "title": "COVID-19: What If Immunity Wanes?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a simple economic model in which social-distancing reduces contagion,\nwe study the implications of waning immunity for the epidemiological dynamics\nand social activity. If immunity wanes, we find that COVID-19 likely becomes\nendemic and that social-distancing is here to stay until the discovery of a\nvaccine or cure. But waning immunity does not necessarily change optimal\nactions on the onset of the pandemic. Decentralized equilibria are virtually\nindependent of waning immunity until close to peak infections. For centralized\nequilibria, the relevance of waning immunity decreases in the probability of\nfinding a vaccine or cure, the costs of infection (e.g., infection-fatality\nrate), and the presence of other NPIs that lower contagion (e.g., quarantining\nand mask use). In simulations calibrated to July 2020, our model suggests that\nwaning immunity is virtually unimportant for centralized equilibria until at\nleast 2021. This provides vital time for individuals and policymakers to learn\nabout immunity against SARS-CoV-2 before it becomes critical.\n"
    },
    {
        "paper_id": 2008.03355,
        "authors": "Samuel J. Ferguson",
        "title": "Obamacare and a Fix for the IRS Iteration",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the quantities appearing in Internal Revenue Service (IRS) tax\nguidance for calculating the health insurance premium tax credit created by the\nPatient Protection and Affordable Care Act, also called Obamacare. We ask the\nquestion of whether there is a procedure, computable by hand, which can\ncalculate the appropriate premium tax credit for any household with\nself-employment income. We motivate current IRS tax guidance, which has had\nself-employed taxpayers use a fixed point iteration to calculate their premium\ntax credits since 2014. Then, we give an example showing that the IRS iteration\ncan lead to a divergent sequence of iterates. As a consequence, IRS guidance\ndoes not calculate appropriate premium tax credits for tax returns in certain\nincome intervals, adversely affecting eligible beneficiaries. A bisection\nprocedure for calculating premium tax credits is proposed. We prove that this\nprocedure calculates appropriate premium tax credits for a model of simple tax\nreturns. This is generalized to the case where premium tax credits are received\nin advance, which is the most common one in applications. We outline the\nproblem of calculating appropriate premium tax credits for models of general\ntax returns. While the bisection procedure will work with the tax code in its\ncurrent configuration, it could fail, eg, in states which have not expanded\nMedicaid, if a new deduction with certain properties were to arise.\n"
    },
    {
        "paper_id": 2008.03443,
        "authors": "Rajiv Kashyap, Mohamed Menisy, Peter Caiazzo, Jim Samuel",
        "title": "Transparency versus Performance in Financial Markets: The Role of CSR\n  Communications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although companies are exhorted to provide more information to the financial\ncommunity, it is evident that they choose different paths based upon their\nstrategic emphasis and competitive environments. Our investigation explores the\nempirical boundary conditions under which firms choose to disclose versus\nwithhold information from investors based upon their strategic emphasis. We\nfound significant differences in terms of voluntary information disclosures\nbetween firms that consistently delivered positive earnings surprises versus\nthose that delivered negative earnings surprises. We investigated this effect\nin a more granular fashion by separately examining differences in\nenvironmental, social, and governance disclosures between the two pools of\nfirms. We found that in essence, the differences remained consistent and\npositive earnings firms were significantly more likely to disclose information\nabout their ESG activities than their counterparts. Interestingly, none of the\nmeasures of financial performance were instrumental in distinguishing between\nthe two pools of firms. However, our measures of reach -- as measured by the\nnumber of -- negative news stories lends credence to our findings. From a fund\nmanager-s perspective, this finding should raise an immediate red flag firms\nthat are likely to underperform are likely to be less transparent than\noverperformers.\n"
    },
    {
        "paper_id": 2008.035,
        "authors": "Luis Escauriaza, Daniel C. Schwarz, Hao Xing",
        "title": "Radner equilibrium and systems of quadratic BSDEs with discontinuous\n  generators",
        "comments": "49 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by an equilibrium problem, we establish the existence of a solution\nfor a family of Markovian backward stochastic differential equations with\nquadratic nonlinearity and discontinuity in $Z$. Using unique continuation and\nbackward uniqueness, we show that the set of discontinuity has measure zero. In\na continuous-time stochastic model of an endowment economy, we prove the\nexistence of an incomplete Radner equilibrium with nondegenerate endogenous\nvolatility.\n"
    },
    {
        "paper_id": 2008.03623,
        "authors": "Igor Halperin",
        "title": "The Inverted Parabola World of Classical Quantitative Finance:\n  Non-Equilibrium and Non-Perturbative Finance Perspective",
        "comments": "14 pages, 4 figures, 11 equations",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical quantitative finance models such as the Geometric Brownian Motion\nor its later extensions such as local or stochastic volatility models do not\nmake sense when seen from a physics-based perspective, as they are all\nequivalent to a negative mass oscillator with a noise. This paper presents an\nalternative formulation based on insights from physics.\n"
    },
    {
        "paper_id": 2008.03672,
        "authors": "Thilini V. Mahanama and Abootaleb Shirvani",
        "title": "A Natural Disasters Index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Natural disasters, such as tornadoes, floods, and wildfire pose risks to life\nand property, requiring the intervention of insurance corporations. One of the\nmost visible consequences of changing climate is an increase in the intensity\nand frequency of extreme weather events. The relative strengths of these\ndisasters are far beyond the habitual seasonal maxima, often resulting in\nsubsequent increases in property losses. Thus, insurance policies should be\nmodified to endure increasingly volatile catastrophic weather events. We\npropose a Natural Disasters Index (NDI) for the property losses caused by\nnatural disasters in the United States based on the \"Storm Data\" published by\nthe National Oceanic and Atmospheric Administration. The proposed NDI is an\nattempt to construct a financial instrument for hedging the intrinsic risk. The\nNDI is intended to forecast the degree of future risk that could forewarn the\ninsurers and corporations allowing them to transfer insurance risk to capital\nmarket investors. This index could also be modified to other regions and\ncountries.\n"
    },
    {
        "paper_id": 2008.04048,
        "authors": "Martin Kyere and Marcel Ausloos",
        "title": "Corporate Governance and Firms Financial Performance in the United\n  Kingdom",
        "comments": "47 pages, 6 tables, 73 references",
        "journal-ref": null,
        "doi": "10.1002/ijfe.1883",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this study is to examine empirically the impact of good\ncorporate governance on financial performance of United Kingdom non-financial\nlisted firms. Agency theory and stewardship theory serve as the bases of a\nconceptual model. Five corporate governance mechanisms are examined on two\nfinancial performance indicators, return on assets (ROA) and Tobin's Q,\nemploying cross-sectional regression methodology. The conclusion drawn from\nempirical test so performed on 252 firms listed on London Stock Exchange for\nthe year 2014 indicates a positive or a negative relationship, but also\nsometimes no effect, of corporate governance mechanisms impact on financial\nperformance. The implications are discussed. Thereby, so distinguishing effects\ndue to causes, we present a proof that, when the right corporate governance\nmechanisms are chosen, the finances of a firm can be improved. The results of\nthis research should have some implication on academia and policy makers\nthoughts.\n"
    },
    {
        "paper_id": 2008.04059,
        "authors": "Linwei Hu, Jie Chen, Joel Vaughan, Hanyu Yang, Kelly Wang, Agus\n  Sudjianto, Vijayan N. Nair",
        "title": "Supervised Machine Learning Techniques: An Overview with Applications to\n  Banking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article provides an overview of Supervised Machine Learning (SML) with a\nfocus on applications to banking. The SML techniques covered include Bagging\n(Random Forest or RF), Boosting (Gradient Boosting Machine or GBM) and Neural\nNetworks (NNs). We begin with an introduction to ML tasks and techniques. This\nis followed by a description of: i) tree-based ensemble algorithms including\nBagging with RF and Boosting with GBMs, ii) Feedforward NNs, iii) a discussion\nof hyper-parameter optimization techniques, and iv) machine learning\ninterpretability. The paper concludes with a comparison of the features of\ndifferent ML algorithms. Examples taken from credit risk modeling in banking\nare used throughout the paper to illustrate the techniques and interpret the\nresults of the algorithms.\n"
    },
    {
        "paper_id": 2008.04068,
        "authors": "Runshan Fu, Yan Huang and Param Vir Singh",
        "title": "Crowd, Lending, Machine, and Bias",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Big data and machine learning (ML) algorithms are key drivers of many fintech\ninnovations. While it may be obvious that replacing humans with machine would\nincrease efficiency, it is not clear whether and where machines can make better\ndecisions than humans. We answer this question in the context of crowd lending,\nwhere decisions are traditionally made by a crowd of investors. Using data from\nProsper.com, we show that a reasonably sophisticated ML algorithm predicts\nlisting default probability more accurately than crowd investors. The dominance\nof the machine over the crowd is more pronounced for highly risky listings. We\nthen use the machine to make investment decisions, and find that the machine\nbenefits not only the lenders but also the borrowers. When machine prediction\nis used to select loans, it leads to a higher rate of return for investors and\nmore funding opportunities for borrowers with few alternative funding options.\nWe also find suggestive evidence that the machine is biased in gender and race\neven when it does not use gender and race information as input. We propose a\ngeneral and effective \"debasing\" method that can be applied to any prediction\nfocused ML applications, and demonstrate its use in our context. We show that\nthe debiased ML algorithm, which suffers from lower prediction accuracy, still\nleads to better investment decisions compared with the crowd. These results\nindicate that ML can help crowd lending platforms better fulfill the promise of\nproviding access to financial resources to otherwise underserved individuals\nand ensure fairness in the allocation of these resources.\n"
    },
    {
        "paper_id": 2008.04069,
        "authors": "Asmar Aliyeva",
        "title": "Insider Ownership and Dividend Payout Policy: The Role of Business Cycle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate how the relationship between managerial stock incentives and\nthe dividend payout policy is impacted by the business cycle by using the data\nof S&P 1500 companies during 2000-2018. We find a strong negative relationship\nbetween managerial stock options and annual dividend payouts of companies for\nthe full sample. Although the direction of the relationship is also negative\nfor the recession period, the coefficient is found to be insignificant. We also\nfind that the mentioned relationship may vary during the recession depending on\nthe size of the company. The impact of stock options on the dividend payout is\nnegative for medium-sized companies and the coefficient is both economically\nand statistically significant. The direction of impact changes for large-cap\ncompanies indicating to deterioration of the CEO voting power in those\ncompanies and less agency problem. We also determine that the percentage of\nshares held by the CEO has a positive impact on annual dividends distributed\nfor large-cap companies, whereas this relationship changes in times of\nrecession.\n"
    },
    {
        "paper_id": 2008.0411,
        "authors": "Hao Tang, Anurag Pal, Lu-Feng Qiao, Tian-Yu Wang, Jun Gao, Xian-Min\n  Jin",
        "title": "Quantum Computation for Pricing the Collateralized Debt Obligations",
        "comments": "19 pages, 18 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Collateralized debt obligation (CDO) has been one of the most commonly used\nstructured financial products and is intensively studied in quantitative\nfinance. By setting the asset pool into different tranches, it effectively\nworks out and redistributes credit risks and returns to meet the risk\npreferences for different tranche investors. The copula models of various kinds\nare normally used for pricing CDOs, and the Monte Carlo simulations are\nrequired to get their numerical solution. Here we implement two typical CDO\nmodels, the single-factor Gaussian copula model and Normal Inverse Gaussian\ncopula model, and by applying the conditional independence approach, we manage\nto load each model of distribution in quantum circuits. We then apply quantum\namplitude estimation as an alternative to Monte Carlo simulation for CDO\npricing. We demonstrate the quantum computation results using IBM Qiskit. Our\nwork addresses a useful task in finance instrument pricing, significantly\nbroadening the application scope for quantum computing in finance.\n"
    },
    {
        "paper_id": 2008.04131,
        "authors": "Keisuke Kokubun",
        "title": "Aggression in the workplace makes social distance difficult",
        "comments": "implication enriched",
        "journal-ref": "Int. J. Environ. Res. Public Health, Vol. 18, No. 10 (2021) 5074",
        "doi": "10.3390/ijerph18105074",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The spread of new coronavirus (COVID-19) infections continues to increase.\nThe practice of social distance attracts attention as a measure to prevent the\nspread of infection, but it is difficult for some occupations. Therefore, in\nprevious studies, the scale of factors that determine social distance has been\ndeveloped. However, it was not clear how to select the items among them, and it\nseemed to be somewhat arbitrary. In response to this trend, this paper\nextracted eight scales by performing exploratory factor analysis based on\ncertain rules while eliminating arbitrariness as much as possible. They were\nAdverse Conditions, Leadership, Information Processing, Response to Aggression,\nMechanical Movement, Autonomy, Communication with the Outside, and Horizontal\nTeamwork. Of these, Adverse Conditions, Response to Aggression, and Horizontal\nTeamwork had a positive correlation with Physical Proximity, and Information\nProcessing, Mechanical Movement, Autonomy, and Communication with the Outside\nhad a negative correlation with Physical Proximity. Furthermore, as a result of\nmultiple regression analysis, it was shown that Response to Aggression, not the\nmere teamwork assumed in previous studies, had the greatest influence on\nPhysical Proximity.\n"
    },
    {
        "paper_id": 2008.04639,
        "authors": "Emiko Inoue, Hiroya Taniguchi, Ken Yamada",
        "title": "Measuring Energy-saving Technological Change: International Trends and\n  Differences",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jeem.2022.102709",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological change is essential to balance economic growth and\nenvironmental sustainability. This study documents energy-saving technological\nchange to understand the trends and differences therein in OECD countries. We\nestimate sector-level production functions with factor-augmenting technologies\nusing cross-country and cross-industry panel data and shift-share instruments,\nthereby measuring energy-saving technological change for each country and\nsector. Our results show how the levels and growth rates of energy-saving\ntechnology vary across countries, sectors, and time. In addition, we evaluate\nthe extent to which factor-augmenting technologies contribute to economic\ngrowth and how this contribution differs across countries and sectors.\n"
    },
    {
        "paper_id": 2008.04782,
        "authors": "Adit Chopra, Abhi Bansal, Aryaman Wadhwa",
        "title": "Evidence of Predicting Early Signs of Corporate Bankruptcy Using\n  Financial Ratios in the Indian Landscape",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corporate bankruptcy impacts the functioning of the economy as it impacts its\nvarious stakeholders: Shareholders, financial and operational lenders, and the\ngovernment. This paper aims to study the impact of a wide array of\nprofitability, leverage and efficiency ratios to predict early signs of\nbankruptcy in public listed companies in India using a logistic regression\nconsidering impacts at two levels: one year and two years before the filing of\nbankruptcy with the NCLT during the year 2019. The study proves that the\naccuracies of the classification model are 81.4% and 85.1% respectively for one\nyear and two years before the bankruptcy.\n"
    },
    {
        "paper_id": 2008.0485,
        "authors": "Adrian Kent (Department of Applied Mathematics and Theoretical\n  Physics, University of Cambridge)",
        "title": "Counting the costs of COVID-19: why future treatment option values\n  matter",
        "comments": "Comments about the value of acquired immunity added to discussion of\n  end states; references on the long-term effects of Covid-19 added (v3). Arxiv\n  abstract updated (v4). Title and abstract changed and comments added to\n  reflect focus on option values of future treatments; references formatted for\n  journal. Accepted version. To appear in Applied Economics and Finance (Nov\n  2020). (v5)",
        "journal-ref": "Applied Economics and Finance, [S.I.], v. 7, n. 6, p. 36-43, Sep.\n  2020",
        "doi": "10.11114/aef.v7i6.5034",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I critique a recent analysis (Miles, Stedman & Heald, 2020) of COVID-19\nlockdown costs and benefits, focussing on the United Kingdom (UK). Miles et al.\n(2020) argue that the March-June UK lockdown was more costly than the benefit\nof lives saved, evaluated using the NICE threshold of {\\pounds}30000 for a\nquality-adjusted life year (QALY) and that the costs of a lockdown for 13 weeks\nfrom mid-June would be vastly greater than any plausible estimate of the\nbenefits, even if easing produced a second infection wave causing over 7000\ndeaths weekly by mid-September.\n  I note here two key problems that significantly affect their estimates and\ncast doubt on their conclusions. Firstly, their calculations arbitrarily cut\noff after 13 weeks, without costing the epidemic end state. That is, they\nassume indifference between mid-September states of 13 or 7500 weekly deaths\nand corresponding infection rates. This seems indefensible unless one assumes\nthat (a) there is little chance of any effective vaccine or improved medical or\nsocial interventions for the foreseeable future, (b) notwithstanding temporary\nlockdowns, COVID-19 will very likely propagate until herd immunity. Even under\nthese assumptions it is very questionable. Secondly, they ignore the costs of\nserious illness, possible long-term lowering of life quality and expectancy for\nsurvivors. These are uncertain, but plausibly at least as large as the costs in\ndeaths.\n  In summary, policy on tackling COVID-19 cannot be rationally made without\nestimating probabilities of future medical interventions and long-term illness\ncosts. More work on modelling these uncertainties is urgently needed.\n"
    },
    {
        "paper_id": 2008.04985,
        "authors": "Nicholas Moehle, Mykel J. Kochenderfer, Stephen Boyd, Andrew Ang",
        "title": "Tax-Aware Portfolio Construction via Convex Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an optimization-based tax-aware portfolio construction method\nthat adds tax liability to standard Markowitz-based portfolio construction. Our\nmethod produces a trade list that specifies the number of shares to buy of each\nasset and the number of shares to sell from each tax lot held. To avoid wash\nsales (in which some realized capital losses are disallowed), we assume that we\ntrade monthly, and cannot simultaneously buy and sell the same asset.\n  The tax-aware portfolio construction problem is not convex, but it becomes\nconvex when we specify, for each asset, whether we buy or sell it. It can be\nsolved using standard mixed-integer convex optimization methods at the cost of\nvery long solve times for some problem instances. We present a custom convex\nrelaxation of the problem that borrows curvature from the risk model. This\nrelaxation can provide a good approximation of the true tax liability, while\ngreatly enhancing computational tractability. This method requires the solution\nof only two convex optimization problems: the first determines whether we buy\nor sell each asset, and the second generates the final trade list. In our\nnumerical experiments, our method almost always solves the nonconvex problem to\noptimality, and when it does not, it produces a trade list very close to\noptimal. Backtests show that the performance of our method is indistinguishable\nfrom that obtained using a globally optimal solution, but with significantly\nreduced computational effort.\n"
    },
    {
        "paper_id": 2008.05147,
        "authors": "Vica Tendenan, Richard Gerlach, and Chao Wang",
        "title": "Tail risk forecasting using Bayesian realized EGARCH models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a Bayesian framework for the realized exponential\ngeneralized autoregressive conditional heteroskedasticity (realized EGARCH)\nmodel, which can incorporate multiple realized volatility measures for the\nmodelling of a return series. The realized EGARCH model is extended by adopting\na standardized Student-t and a standardized skewed Student-t distribution for\nthe return equation. Different types of realized measures, such as sub-sampled\nrealized variance, sub-sampled realized range, and realized kernel, are\nconsidered in the paper. The Bayesian Markov chain Monte Carlo (MCMC)\nestimation employs the robust adaptive Metropolis algorithm (RAM) in the burn\nin period and the standard random walk Metropolis in the sample period. The\nBayesian estimators show more favourable results than maximum likelihood\nestimators in a simulation study. We test the proposed models with several\nindices to forecast one-step-ahead Value at Risk (VaR) and Expected Shortfall\n(ES) over a period of 1000 days. Rigorous tail risk forecast evaluations show\nthat the realized EGARCH models employing the standardized skewed Student-t\ndistribution and incorporating sub-sampled realized range are favored, compared\nto a range of models.\n"
    },
    {
        "paper_id": 2008.05417,
        "authors": "Christian Deutscher, David Winkelmann, Marius \\\"Otting",
        "title": "Bookmakers' mispricing of the disappeared home advantage in the German\n  Bundesliga after the COVID-19 break",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The outbreak of COVID-19 in March 2020 led to a shutdown of economic\nactivities in Europe. This included the sports sector, since public gatherings\nwere prohibited. The German Bundesliga was among the first sport leagues\nrealising a restart without spectators. Several recent studies suggest that the\nhome advantage of teams was eroded for the remaining matches. Our paper\nanalyses the reaction by bookmakers to the disappearance of such home\nadvantage. We show that bookmakers had problems to adjust the betting odds in\naccordance to the disappeared home advantage, opening opportunities for\nprofitable betting strategies.\n"
    },
    {
        "paper_id": 2008.05519,
        "authors": "Jiequn Han, Ruimeng Hu, Jihao Long",
        "title": "Convergence of Deep Fictitious Play for Stochastic Differential Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic differential games have been used extensively to model agents'\ncompetitions in Finance, for instance, in P2P lending platforms from the\nFintech industry, the banking system for systemic risk, and insurance markets.\nThe recently proposed machine learning algorithm, deep fictitious play,\nprovides a novel efficient tool for finding Markovian Nash equilibrium of large\n$N$-player asymmetric stochastic differential games [J. Han and R. Hu,\nMathematical and Scientific Machine Learning Conference, pages 221-245, PMLR,\n2020]. By incorporating the idea of fictitious play, the algorithm decouples\nthe game into $N$ sub-optimization problems, and identifies each player's\noptimal strategy with the deep backward stochastic differential equation (BSDE)\nmethod parallelly and repeatedly. In this paper, we prove the convergence of\ndeep fictitious play (DFP) to the true Nash equilibrium. We can also show that\nthe strategy based on DFP forms an $\\eps$-Nash equilibrium. We generalize the\nalgorithm by proposing a new approach to decouple the games, and present\nnumerical results of large population games showing the empirical convergence\nof the algorithm beyond the technical assumptions in the theorems.\n"
    },
    {
        "paper_id": 2008.05527,
        "authors": "Peter B. Lerner",
        "title": "Transmission of market orders through communication line with\n  relativistic delay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The notion of \"relativistic finance\" became ingrained in public imagination\nand has been asserted in many mass-media reports. Yet, despite an observed\ndrive of the most reputable Wall Street firms to establish their servers ever\ncloser to the trading hubs, there is surprisingly little \"hard\" information\nrelated to relativistic delay of the trading orders. In this paper, the author\nuses modified M/M/G queue theory to describe propagation of the trading signal\nwith finite velocity.\n"
    },
    {
        "paper_id": 2008.05653,
        "authors": "Jonathan Meng and Feng Fu",
        "title": "Understanding Gambling Behavior and Risk Attitudes Using\n  Cryptocurrency-based Casino Blockchain Data",
        "comments": "21 pages. Comments welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The statistical concept of Gambler's Ruin suggests that gambling has a large\namount of risk. Nevertheless, gambling at casinos and gambling on the Internet\nare both hugely popular activities. In recent years, both prospect theory and\nlab-controlled experiments have been used to improve our understanding of risk\nattitudes associated with gambling. Despite theoretical progress, collecting\nreal-life gambling data, which is essential to validate predictions and\nexperimental findings, remains a challenge. To address this issue, we collect\npublicly available betting data from a \\emph{DApp} (decentralized application)\non the Ethereum Blockchain, which instantly publishes the outcome of every\nsingle bet (consisting of each bet's timestamp, wager, probability of winning,\nuserID, and profit). This online casino is a simple dice game that allows\ngamblers to tune their own winning probabilities. Thus the dataset is well\nsuited for studying gambling strategies and the complex dynamic of risk\nattitudes involved in betting decisions. We analyze the dataset through the\nlens of current probability-theoretic models and discover empirical examples of\ngambling systems. Our results shed light on understanding the role of risk\npreferences in human financial behavior and decision-makings beyond gambling.\n"
    },
    {
        "paper_id": 2008.05693,
        "authors": "Benjamin Avanzi, Gregory Clive Taylor, Melantha Wang, Bernard Wong",
        "title": "SynthETIC: an individual insurance claim simulator with feature control",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2021.06.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent years have seen rapid increase in the application of machine learning\nto insurance loss reserving. They yield most value when applied to large data\nsets, such as individual claims, or large claim triangles. In short, they are\nlikely to be useful in the analysis of any data set whose volume is sufficient\nto obscure a naked-eye view of its features. Unfortunately, such large data\nsets are in short supply in the actuarial literature. Accordingly, one needs to\nturn to synthetic data. Although the ultimate objective of these methods is\napplication to real data, the use of synthetic data containing features\ncommonly observed in real data is also to be encouraged.\n  While there are a number of claims simulators in existence, each valuable\nwithin its own context, the inclusion of a number of desirable (but\ncomplicated) data features requires further development. Accordingly, in this\npaper we review those desirable features, and propose a new simulator of\nindividual claim experience called `SynthETIC`.\n  Our simulator is publicly available, open source, and fills a gap in the\nnon-life actuarial toolkit. The simulator specifically allows for desirable\n(but optionally complicated) data features typically occurring in practice,\nsuch as variations in rates of settlements and development patterns; as with\nsuperimposed inflation, and various discontinuities, and also enables various\ndependencies between variables. The user has full control of the mechanics of\nthe evolution of an individual claim. As a result, the complexity of the data\nset generated (meaning the level of difficulty of analysis) may be dialled\nanywhere from extremely simple to extremely complex.\n"
    },
    {
        "paper_id": 2008.05824,
        "authors": "M. Andrea Arias-Serna, Jean-Michel Loubes, Francisco J. Caro-Lopera",
        "title": "Risk Measures Estimation Under Wasserstein Barycenter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Randomness in financial markets requires modern and robust multivariate\nmodels of risk measures. This paper proposes a new approach for modeling\nmultivariate risk measures under Wasserstein barycenters of probability\nmeasures supported on location-scatter families. Simple and advanced copulas\nmultivariate Value at Risk models are compared with the derived technique. The\nperformance of the model is also checked in market indices of United States\ngenerated by the financial crisis due to COVID-19. The introduced model behaves\nsatisfactory in both common and volatile periods of asset prices, providing\nrealistic VaR forecast in this era of social distancing.\n"
    },
    {
        "paper_id": 2008.05883,
        "authors": "Sebastian Kraus and Nicolas Koch",
        "title": "Effect of pop-up bike lanes on cycling in European cities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The bicycle is a low-cost means of transport linked to low risk of COVID-19\ntransmission. Governments have incentivized cycling by redistributing street\nspace as part of their post-lockdown strategies. Here, we evaluate the impact\nof provisional bicycle infrastructure on cycling traffic in European cities. We\nscrape daily bicycle counts spanning over a decade from 736 bicycle counters in\n106 European cities. We combine this with data on announced and completed\npop-up bike lane road work projects. On average 11.5 kilometers of provisional\npop-up bike lanes have been built per city. Each kilometer has increased\ncycling in a city by 0.6%. We calculate that the new infrastructure will\ngenerate $2.3 billion in health benefits per year, if cycling habits are\nsticky.\n"
    },
    {
        "paper_id": 2008.05885,
        "authors": "R. Church, J. C. Duque, D. E. Restrepo",
        "title": "The p-Innovation ecosystems model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a spatially constrained clustering problem\nbelonging to the family of \"p-regions\" problems. Our formulation is motivated\nby the recent developments of economic complexity on the evolution of the\neconomic output through key interactions among industries within economic\nregions. The objective of this model consists in aggregating a set of\ngeographic areas into a prescribed number of regions (so-called innovation\necosystems) such that the resulting regions preserve the most relevant\ninteractions among industries. We formulate the p-Innovation Ecosystems model\nas a mixed-integer programming (MIP) problem and propose a heuristic solution\napproach. We explore a case involving the municipalities of Colombia to\nillustrate how such a model can be applied and used for policy and regional\ndevelopment.\n"
    },
    {
        "paper_id": 2008.06042,
        "authors": "Bairui Du, Delmiro Fernandez-Reyes and Paolo Barucca",
        "title": "Image Processing Tools for Financial Time Series Classification",
        "comments": "12 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The application of deep learning to time series forecasting is one of the\nmajor challenges in present machine learning. We propose a novel methodology\nthat combines machine learning and image processing methods to define and\npredict market states with intraday financial data. A wavelet transform is\napplied to the log-return of stock prices for both image extraction and\ndenoising. A convolutional neural network then extracts patterns from denoised\nwavelet images to classify daily time series, i.e. a market state is associated\nwith the binary prediction of the daily close price movement based on the\nwavelet image constructed from the price changes in the first hours of the day.\nThis method overcomes the low signal-to-noise ratio problem in financial time\nseries and gets a competitive prediction accuracy of the market states 'Up' and\n'Down' of financial data as tested on the S&P 500.\n"
    },
    {
        "paper_id": 2008.06051,
        "authors": "Tatsushi Oka and Wei Wei and Dan Zhu",
        "title": "A Spatial Stochastic SIR Model for Transmission Networks with\n  Application to COVID-19 Epidemic in China",
        "comments": "Typos were fixed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Governments around the world have implemented preventive measures against the\nspread of the coronavirus disease (COVID-19). In this study, we consider a\nmultivariate discrete-time Markov model to analyze the propagation of COVID-19\nacross 33 provincial regions in China. This approach enables us to evaluate the\neffect of mobility restriction policies on the spread of the disease. We use\ndata on daily human mobility across regions and apply the Bayesian framework to\nestimate the proposed model. The results show that the spread of the disease in\nChina was predominately driven by community transmission within regions and the\nlockdown policy introduced by local governments curbed the spread of the\npandemic. Further, we document that Hubei was only the epicenter of the early\nepidemic stage. Secondary epicenters, such as Beijing and Guangdong, had\nalready become established by late January 2020, and the disease spread out to\nconnected regions. The transmission from these epicenters substantially\ndeclined following the introduction of human mobility restrictions across\nregions.\n"
    },
    {
        "paper_id": 2008.0613,
        "authors": "Neil Shephard",
        "title": "An estimator for predictive regression: reliable inference for financial\n  economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating linear regression using least squares and reporting robust\nstandard errors is very common in financial economics, and indeed, much of the\nsocial sciences and elsewhere. For thick tailed predictors under\nheteroskedasticity this recipe for inference performs poorly, sometimes\ndramatically so. Here, we develop an alternative approach which delivers an\nunbiased, consistent and asymptotically normal estimator so long as the means\nof the outcome and predictors are finite. The new method has standard errors\nunder heteroskedasticity which are easy to reliably estimate and tests which\nare close to their nominal size. The procedure works well in simulations and in\nan empirical exercise. An extension is given to quantile regression.\n"
    },
    {
        "paper_id": 2008.06184,
        "authors": "I.L. Degano, S.E. Ferrando and A.L. Gonzalez",
        "title": "No-Arbitrage Symmetries",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The no-arbitrage property is widely accepted to be a centerpiece of modern\nfinancial mathematics and could be considered to be a financial law applicable\nto a large class of (idealized) markets. The paper addresses the following\nbasic question: can one characterize the class of transformations that leave\nthe law of no-arbitrage invariant? We provide a geometric formalization of this\nquestion in a non probabilistic setting of discrete time, the so-called\ntrajectorial models. The paper then characterizes, in a local sense, the\nno-arbitrage symmetries and illustrates their meaning in a detailed example.\nOur context makes the result available to the stochastic setting as a special\ncase\n"
    },
    {
        "paper_id": 2008.06225,
        "authors": "Jie Fang, Jianwu Lin, Shutao Xia, Yong Jiang, Zhikang Xia, Xiang Liu",
        "title": "Neural Network-based Automatic Factor Construction",
        "comments": "Accepted by the Journal, Quantitative Finance. 21 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Instead of conducting manual factor construction based on traditional and\nbehavioural finance analysis, academic researchers and quantitative investment\nmanagers have leveraged Genetic Programming (GP) as an automatic feature\nconstruction tool in recent years, which builds reverse polish mathematical\nexpressions from trading data into new factors. However, with the development\nof deep learning, more powerful feature extraction tools are available. This\npaper proposes Neural Network-based Automatic Factor Construction (NNAFC), a\ntailored neural network framework that can automatically construct diversified\nfinancial factors based on financial domain knowledge and a variety of neural\nnetwork structures. The experiment results show that NNAFC can construct more\ninformative and diversified factors than GP, to effectively enrich the current\nfactor pool. For the current market, both fully connected and recurrent neural\nnetwork structures are better at extracting information from financial time\nseries than convolution neural network structures. Moreover, new factors\nconstructed by NNAFC can always improve the return, Sharpe ratio, and the max\ndraw-down of a multi-factor quantitative investment strategy due to their\nintroducing more information and diversification to the existing factor pool.\n"
    },
    {
        "paper_id": 2008.06377,
        "authors": "Shreya Bose and Ibrahim Ekren",
        "title": "Kyle-Back Models with risk aversion and non-Gaussian Beliefs",
        "comments": "to appear in Annals of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the problem of existence of equilibrium in Kyle's continuous\ntime insider trading model can be tackled by considering a forward-backward\nsystem coupled via an optimal transport type constraint at maturity. The\nforward component is a stochastic differential equation representing an\nendogenously determined state variable and the backward component is a\nquasilinear parabolic equation representing the pricing function. By obtaining\na stochastic representation for the solution of such a system, we show the\nwell-posedness of solutions and study the properties of the equilibrium\nobtained for small enough risk aversion parameter. In our model, the insider\nhas exponential type utility and the belief of the market maker on the\ndistribution of the price at final time can be non-Gaussian.\n"
    },
    {
        "paper_id": 2008.0645,
        "authors": "Muhammad Rehan, Jahanzaib Alvi, Suleyman Serdar Karaca",
        "title": "Short Term Stress of Covid-19 On World Major Stock Indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main objective of this study is to check short term stress of COVID-19 on\nthe American, European, Asian, and Pacific stock market indices, furthermore,\nthe correlation between all the stock markets during the pandemic. Secondary\ndata of 41 stock exchange from 32 countries have been collected from\ninvesting.com website from 1st July 2019 to 14th May 2020 for the stock market\nand the COVID-19 data has been collected according to the first cases reported\nin the country, stocks market are classified either developed or emerging\neconomy, further divided according to the subcontinent i.e. America, Europe,\nand Pacific/Asia, the main focus in the data is the report of first COVID-19\ncases. The study reveals that there is volatility in the all the 41 stock\nmarket (American, Europe, Asia, and Pacific) after reporting of the first case\nand volatility increase with the increase of COVID-19 cases, moreover, there is\na significant negative relationship between the number of COVID-19 cases and 41\nmajor stock indices of American, Europe, Asia and Pacific, European\nsubcontinent market found more effected from the COVID-19 than another\nsubcontinent, there is Clustering effect of COVID-19 on all the stock market\nexcept American's stock market due to smart capital investing.\n"
    },
    {
        "paper_id": 2008.06598,
        "authors": "Peter A. Forsyth",
        "title": "A Stochastic Control Approach to Defined Contribution Plan Decumulation:\n  \"The Nastiest, Hardest Problem in Finance\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We pose the decumulation strategy for a Defined Contribution (DC) pension\nplan as a problem in optimal stochastic control. The controls are the\nwithdrawal amounts and the asset allocation strategy. We impose maximum and\nminimum constraints on the withdrawal amounts, and impose no-shorting\nno-leverage constraints on the asset allocation strategy. Our objective\nfunction measures reward as the expected total withdrawals over the\ndecumulation horizon, and risk is measured by Expected Shortfall (ES) at the\nend of the decumulation period. We solve the stochastic control problem\nnumerically, based on a parametric model of market stochastic processes. We\nfind that, compared to a fixed constant withdrawal strategy, with minimum\nwithdrawal set to the constant withdrawal amount, the optimal strategy has a\nsignificantly higher expected average withdrawal, at the cost of a very small\nincrease in ES risk. Tests on bootstrapped resampled historical market data\nindicate that this strategy is robust to parametric model misspecification.\n"
    },
    {
        "paper_id": 2008.0666,
        "authors": "Max Luke, Priyanshi Somani, Turner Cotterman, Dhruv Suri, Stephen J.\n  Lee",
        "title": "No COVID-19 Climate Silver Lining in the US Power Sector",
        "comments": "13 pages, 6 figures, preprint",
        "journal-ref": null,
        "doi": "10.1038/s41467-021-24959-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent studies conclude that the global coronavirus (COVID-19) pandemic\ndecreased power sector CO$_2$ emissions globally and in the United States. In\nthis paper, we analyze the statistical significance of CO2 emissions reductions\nin the U.S. power sector from March through December 2020. We use Gaussian\nprocess (GP) regression to assess whether CO2 emissions reductions would have\noccurred with reasonable probability in the absence of COVID-19 considering\nuncertainty due to factors unrelated to the pandemic and adjusting for weather,\nseasonality, and recent emissions trends. We find that monthly CO2 emissions\nreductions are only statistically significant in April and May 2020 considering\nhypothesis tests at 5% significance levels. Separately, we consider the\npotential impact of COVID-19 on coal-fired power plant retirements through\n2022. We find that only a small percentage of U.S. coal power plants are at\nrisk of retirement due to a possible COVID-19-related sustained reduction in\nelectricity demand and prices. We observe and anticipate a return to\npre-COVID-19 CO2 emissions in the U.S. power sector.\n"
    },
    {
        "paper_id": 2008.07082,
        "authors": "Chonghu Guan, Jing Peng, Zuo Quan Xu",
        "title": "A free boundary problem arising from a multi-state regime-switching\n  stock trading model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a free boundary problem, which arises from an optimal\ntrading problem of a stock that is driven by a uncertain market status process.\nThe free boundary problem is a variational inequality system of three functions\nwith a degenerate operator. The main contribution of this paper is that we not\nonly prove all the four switching free boundaries are no-overlapping, monotonic\nand $C^{\\infty}$-smooth, but also completely determine their relative\nlocalities and provide the optimal trading strategies for the stock trading\nproblem.\n"
    },
    {
        "paper_id": 2008.07103,
        "authors": "Yichun Chi, Xun Yu Zhou and Sheng Chao Zhuang",
        "title": "Variance Contracts",
        "comments": "42 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the design of an optimal insurance contract in which the insured\nmaximizes her expected utility and the insurer limits the variance of his risk\nexposure while maintaining the principle of indemnity and charging the premium\naccording to the expected value principle. We derive the optimal policy\nsemi-analytically, which is coinsurance above a deductible when the variance\nbound is binding. This policy automatically satisfies the incentive-compatible\ncondition, which is crucial to rule out ex post moral hazard. We also find that\nthe deductible is absent if and only if the contract pricing is actuarially\nfair. Focusing on the actuarially fair case, we carry out comparative statics\non the effects of the insured's initial wealth and the variance bound on\ninsurance demand. Our results indicate that the expected coverage is always\nlarger for a wealthier insured, implying that the underlying insurance is a\nnormal good, which supports certain recent empirical findings. Moreover, as the\nvariance constraint tightens, the insured who is prudent cedes less losses,\nwhile the insurer is exposed to less tail risk.\n"
    },
    {
        "paper_id": 2008.07221,
        "authors": "Iegor Riepin, Thomas M\\\"obius, Felix M\\\"usgens",
        "title": "Modelling uncertainty in coupled electricity and gas systems -- is it\n  worth the effort?",
        "comments": "35 pages, 14 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The interdependence of electricity and natural gas markets is becoming a\nmajor topic in energy research. Integrated energy models are used to assist\ndecision-making for businesses and policymakers addressing challenges of energy\ntransition and climate change. The analysis of complex energy systems requires\nlarge-scale models, which are based on extensive databases, intertemporal\ndynamics and a multitude of decision variables. Integrating such energy system\nmodels results in increased system complexity. This complexity poses a\nchallenge for energy modellers to address multiple uncertainties that affect\nboth markets. Stochastic optimisation approaches enable an adequate\nconsideration of uncertainties in investment and operation planning; however,\nstochastic modelling of integrated large-scale energy systems further scales\nthe level of complexity. In this paper, we combine integrated and stochastic\noptimisation problems and parametrise our model for European electricity and\ngas markets. We analyse and compare the impact of uncertain input parameters,\nsuch as gas and electricity demand, renewable energy capacities and fuel and\nCO2 prices, on the quality of the solution obtained in the integrated\noptimisation problem. Our results quantify the value of encoding uncertainty as\na part of a model. While the methodological contribution should be of interest\nfor energy modellers, our findings are relevant for industry experts and\nstakeholders with an empirical interest in the European energy system.\n"
    },
    {
        "paper_id": 2008.07564,
        "authors": "Eduardo Ramos-P\\'erez, Pablo J. Alonso-Gonz\\'alez, Jos\\'e Javier\n  N\\'u\\~nez-Vel\\'azquez",
        "title": "Stochastic reserving with a stacked model based on a hybridized\n  Artificial Neural Network",
        "comments": null,
        "journal-ref": "Expert Systems with Applications, Volume 163, January 2021",
        "doi": "10.1016/j.eswa.2020.113782",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Currently, legal requirements demand that insurance companies increase their\nemphasis on monitoring the risks linked to the underwriting and asset\nmanagement activities. Regarding underwriting risks, the main uncertainties\nthat insurers must manage are related to the premium sufficiency to cover\nfuture claims and the adequacy of the current reserves to pay outstanding\nclaims. Both risks are calibrated using stochastic models due to their nature.\nThis paper introduces a reserving model based on a set of machine learning\ntechniques such as Gradient Boosting, Random Forest and Artificial Neural\nNetworks. These algorithms and other widely used reserving models are stacked\nto predict the shape of the runoff. To compute the deviation around a former\nprediction, a log-normal approach is combined with the suggested model. The\nempirical results demonstrate that the proposed methodology can be used to\nimprove the performance of the traditional reserving techniques based on\nBayesian statistics and a Chain Ladder, leading to a more accurate assessment\nof the reserving risk.\n"
    },
    {
        "paper_id": 2008.07798,
        "authors": "Marc Mukendi Mpanda, Safari Mukeru, Mmboniseni Mulaudzi",
        "title": "Generalisation of Fractional-Cox-Ingersoll-Ross Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we define a generalised fractional Cox-Ingersoll-Ross process\nas a square of singular stochastic differential equation with respect to\nfractional Brownian motion with Hurst parameter H in (0,1) and continuous drift\nfunction. Firstly, we show that this differential equation has a unique\nsolution which is continuous and positive up to the time of the first visit to\nzero. In addition, we prove that it is strictly positive everywhere almost\nsurely for H > 1/2. In the case where H < 1/2, we consider a sequence of\nincreasing functions and we prove that the probability of hitting zero tends to\nzero as n goes to infinity. These results are illustrated with some simulations\nusing the generalisation of the extended Cox-Ingersoll-Ross process.\n"
    },
    {
        "paper_id": 2008.07807,
        "authors": "Bastien Baldacci, Iuliia Manziuk",
        "title": "Adaptive trading strategies across liquidity pools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we provide a flexible framework for optimal trading in an\nasset listed on different venues. We take into account the dependencies between\nthe imbalance and spread of the venues, and allow for partial execution of\nlimit orders at different limits as well as market orders. We present a\nBayesian update of the model parameters to take into account possibly changing\nmarket conditions and propose extensions to include short/long trading signals,\nmarket impact or hidden liquidity. To solve the stochastic control problem of\nthe trader we apply the finite difference method and also develop a deep\nreinforcement learning algorithm allowing to consider more complex settings.\n"
    },
    {
        "paper_id": 2008.07822,
        "authors": "Matthieu Garcin and Martino Grasselli",
        "title": "Long vs Short Time Scales: the Rough Dilemma and Beyond",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a large dataset on major FX rates, we test the robustness of the rough\nfractional volatility model over different time scales, by including smoothing\nand measurement errors into the analysis. Our findings lead to new stylized\nfacts in the log-log plots of the second moments of realized variance\nincrements against lag which exhibit some convexity in addition to the\nroughness and stationarity of the volatility. The very low perceived Hurst\nexponents at small scales is consistent with the rough framework, while the\nhigher perceived Hurst exponents for larger scales leads to a nonlinear\nbehavior of the log-log plot that has not been described by models introduced\nso far.\n"
    },
    {
        "paper_id": 2008.07836,
        "authors": "Tomoshiro Ochiai and Jose C. Nacher",
        "title": "Unveiling the directional network behind the financial statements data\n  using volatility constraint correlation",
        "comments": "14 pages, 6 figures",
        "journal-ref": "Physica A: Volume 600, 127534, 12 pages (2022)",
        "doi": "10.1016/j.physa.2022.127534",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial data, such as financial statements, contain valuable and critical\ninformation that may assist stakeholders and investors in optimizing their\ncapital to maximize overall economic growth. Since there are many variables in\nfinancial statements, it is crucial to determine the causal relationships, that\nis, the directional influence between them in a structural way, as well as to\nunderstand the associated accounting mechanisms. However, the analysis of\nvariable-to-variable relationships in financial information using standard\ncorrelation functions is not sufficient to unveil directionality. Here, we use\nthe volatility constrained correlation (VC correlation) method to predict the\ndirectional relationship between two arbitrary variables. We apply the VC\ncorrelation method to five significant financial information variables\n(revenue, net income, operating income, own capital, and market capitalization)\nof 2321 firms listed on the Tokyo Stock Exchange over 28 years from 1990 to\n2018. This study identifies which accounting variables are influential and\nwhich are susceptible. Our findings show that operating income is the most\ninfluential variable while market capitalization and revenue are the most\nsusceptible variables. Surprisingly, the results differ from the existing\nintuitive understanding suggested by widely used investment strategy\nindicators, the price--earnings ratio and the price-to-book ratio, which report\nthat net income and own capital are the most influential variables affecting\nmarket capitalization. This analysis may assist managers, stakeholders, and\ninvestors to improve financial management performance and optimize firms'\nfinancial strategies in future operations.\n"
    },
    {
        "paper_id": 2008.07871,
        "authors": "Peter Belcak, Jan-Peter Calliess, Stefan Zohren",
        "title": "Fast Agent-Based Simulation Framework with Applications to Reinforcement\n  Learning and the Study of Trading Latency Effects",
        "comments": "Presented at the International Workshop on Multi-Agent Systems and\n  Agent-Based Simulation (MABS@AAMAS) 2021, 12 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new software toolbox for agent-based simulation. Facilitating\nrapid prototyping by offering a user-friendly Python API, its core rests on an\nefficient C++ implementation to support simulation of large-scale multi-agent\nsystems. Our software environment benefits from a versatile message-driven\narchitecture. Originally developed to support research on financial markets, it\noffers the flexibility to simulate a wide-range of different (easily\ncustomisable) market rules and to study the effect of auxiliary factors, such\nas delays, on the market dynamics. As a simple illustration, we employ our\ntoolbox to investigate the role of the order processing delay in normal trading\nand for the scenario of a significant price change. Owing to its general\narchitecture, our toolbox can also be employed as a generic multi-agent system\nsimulator. We provide an example of such a non-financial application by\nsimulating a mechanism for the coordination of no-regret learning agents in a\nmulti-agent network routing scenario previously proposed in the literature.\n"
    },
    {
        "paper_id": 2008.07907,
        "authors": "Victor Olkhov",
        "title": "Volatility Depends on Market Trades and Macro Theory",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the randomness of market trade as the origin of price and return\nstochasticity. We look at time series of trade values and volumes as random\nvariables during the averaging interval {\\Delta} and describe the dependences\nof market-based volatilities of price and return on the volatilities and\ncorrelations of market trade values and volumes. We describe the market-based\norigin of the lower boundaries of the accuracy of macroeconomic variables and\nconsider, as an example, the accuracy of macroeconomic investments. We\nhighlight that current macroeconomic models describe relations between the 1st\norder variables determined by sums of trade values or volumes. To predict\nmarket-based volatilities of price, return, and volatilities of macroeconomic\nvariables, one should develop econometric methodologies, collect data, and\nelaborate macroeconomic theories of the 2nd order that model the mutual\ndependence of the 1st and 2nd order economic variables. The absence of\nmacroeconomic theories of the 2nd order means no economic basis for predictions\nof market-based volatilities of price and return, as well as volatilities of\nany macroeconomic variables. In turn, that limits the accuracy of forecasting\nprobabilities of price, return, and the accuracy of macroeconomic variables in\nthe best case by Gaussian distributions.\n"
    },
    {
        "paper_id": 2008.08004,
        "authors": "Jesus Lago, Grzegorz Marcjasz, Bart De Schutter, Rafa{\\l} Weron",
        "title": "Forecasting day-ahead electricity prices: A review of state-of-the-art\n  algorithms, best practices and an open-access benchmark",
        "comments": null,
        "journal-ref": "Applied Energy (2021) vol. 293, art. num. 116983",
        "doi": "10.1016/j.apenergy.2021.116983",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the field of electricity price forecasting has benefited from plenty of\ncontributions in the last two decades, it arguably lacks a rigorous approach to\nevaluating new predictive algorithms. The latter are often compared using\nunique, not publicly available datasets and across too short and limited to one\nmarket test samples. The proposed new methods are rarely benchmarked against\nwell established and well performing simpler models, the accuracy metrics are\nsometimes inadequate and testing the significance of differences in predictive\nperformance is seldom conducted. Consequently, it is not clear which methods\nperform well nor what are the best practices when forecasting electricity\nprices. In this paper, we tackle these issues by performing a literature survey\nof state-of-the-art models, comparing state-of-the-art statistical and deep\nlearning methods across multiple years and markets, and by putting forward a\nset of best practices. In addition, we make available the considered datasets,\nforecasts of the state-of-the-art models, and a specifically designed python\ntoolbox, so that new algorithms can be rigorously evaluated in future studies.\n"
    },
    {
        "paper_id": 2008.08006,
        "authors": "Grzegorz Marcjasz, Jesus Lago, Rafa{\\l} Weron",
        "title": "Neural networks in day-ahead electricity price forecasting: Single vs.\n  multiple outputs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advancements in the fields of artificial intelligence and machine\nlearning methods resulted in a significant increase of their popularity in the\nliterature, including electricity price forecasting. Said methods cover a very\nbroad spectrum, from decision trees, through random forests to various\nartificial neural network models and hybrid approaches. In electricity price\nforecasting, neural networks are the most popular machine learning method as\nthey provide a non-linear counterpart for well-tested linear regression models.\nTheir application, however, is not straightforward, with multiple\nimplementation factors to consider. One of such factors is the network's\nstructure. This paper provides a comprehensive comparison of two most common\nstructures when using the deep neural networks -- one that focuses on each hour\nof the day separately, and one that reflects the daily auction structure and\nmodels vectors of the prices. The results show a significant accuracy advantage\nof using the latter, confirmed on data from five distinct power exchanges.\n"
    },
    {
        "paper_id": 2008.08511,
        "authors": "Felix Montag, Alina Sagimuldina and Monika Schnitzer",
        "title": "Are temporary value-added tax reductions passed on to consumers?\n  Evidence from Germany's stimulus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides the first estimates of the pass-through rate of the\nongoing temporary value-added tax (VAT) reduction, which is part of the German\nfiscal response to COVID-19. Using a unique dataset containing the universe of\nprice changes at fuel stations in Germany and France in June and July 2020, we\nemploy a difference-in-differences strategy and find that pass-through is fast\nand substantial but remains incomplete for all fuel types. Furthermore, we find\na high degree of heterogeneity between the pass-through estimates for different\nfuel types. Our results are consistent with the interpretation that\npass-through rates are higher for customer groups who are more likely to exert\ncompetitive pressure by shopping for lower prices. Our results have important\nimplications for the effectiveness of the stimulus measure and the\ncost-effective design of unconventional fiscal policy.\n"
    },
    {
        "paper_id": 2008.08576,
        "authors": "Simon J.A. Malham, Jiaqi Shen, Anke Wiese",
        "title": "Series expansions and direct inversion for the Heston model",
        "comments": "87 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Efficient sampling for the conditional time integrated variance process in\nthe Heston stochastic volatility model is key to the simulation of the stock\nprice based on its exact distribution. We construct a new series expansion for\nthis integral in terms of double infinite weighted sums of particular\nindependent random variables through a change of measure and the decomposition\nof squared Bessel bridges. When approximated by series truncations, this\nrepresentation has exponentially decaying truncation errors. We propose\nfeasible strategies to largely reduce the implementation of the new series to\nsimulations of simple random variables that are independent of any model\nparameters. We further develop direct inversion algorithms to generate samples\nfor such random variables based on Chebyshev polynomial approximations for\ntheir inverse distribution functions. These approximations can be used under\nany market conditions. Thus, we establish a strong, efficient and almost exact\nsampling scheme for the Heston model.\n"
    },
    {
        "paper_id": 2008.08669,
        "authors": "Jeffrey Cohen, Alex Khan, Clark Alexander",
        "title": "Portfolio Optimization of 60 Stocks Using Classical and Quantum\n  Algorithms",
        "comments": "19 pages, 15 figures, 21 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We continue to investigate the use of quantum computers for building an\noptimal portfolio out of a universe of 60 U.S. listed, liquid equities.\nStarting from historical market data, we apply our unique problem formulation\non the D-Wave Systems Inc. D-Wave 2000Q (TM) quantum annealing system\n(hereafter called D-Wave) to find the optimal risk vs return portfolio. We\napproach this first classically, then using the D-Wave, to select efficient buy\nand hold portfolios. Our results show that practitioners can use either\nclassical or quantum annealing methods to select attractive portfolios. This\nbuilds upon our prior work on optimization of 40 stocks.\n"
    },
    {
        "paper_id": 2008.08705,
        "authors": "Sudiksha Joshi",
        "title": "Reforming the State-Based Forward Guidance through Wage Growth Rate\n  Threshold: Evidence from FRB/US Simulations",
        "comments": "48 pages, 20 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I have analyzed the practicality of the Evans Rule in the state based forward\nguidance and possible ways to reform it. I examined the biases, measurement\nerrors, and other limitations extant in the unemployment and the inflation rate\nin the Evans Rule. Using time series analysis, I calibrated the thresholds of\nECI wage growth and the employment to population ratio and investigated the\nrelationship between other labor utilization variables. Then I imposed various\nshocks and constructed impulse response functions to contrast the paths of\neight macroeconomic variables under three scenarios. The results suggest that\nunder the wage growth rate scenario, the federal funds rate lift off earlier\nthan under the current Evans Rule.\n"
    },
    {
        "paper_id": 2008.08733,
        "authors": "Hamed Amini and Zachary Feinstein",
        "title": "Optimal Network Compression",
        "comments": "34 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a formulation of the optimal network compression\nproblem for financial systems. This general formulation is presented for\ndifferent levels of network compression or rerouting allowed from the initial\ninterbank network. We prove that this problem is, generically, NP-hard. We\nfocus on objective functions generated by systemic risk measures under shocks\nto the financial network. We use this framework to study the (sub)optimality of\nthe maximally compressed network. We conclude by studying the optimal\ncompression problem for specific networks; this permits us to study, e.g., the\nso-called robust fragility of certain network topologies more generally as well\nas the potential benefits and costs of network compression. In particular,\nunder systematic shocks and heterogeneous financial networks the robust\nfragility results of Acemoglu et al. (2015) no longer hold generally.\n"
    },
    {
        "paper_id": 2008.08759,
        "authors": "Takeshi Kato, Yasuhiro Asa, Misa Owa",
        "title": "Positionality-Weighted Aggregation Methods for Cumulative Voting",
        "comments": "10 pages, 3 figures. Published online at\n  http://www.redfame.com/journal/index.php/ijsss/article/view/5171",
        "journal-ref": "International Journal of Social Science Studies, Vol. 9, No. 2\n  (2021) 79-88",
        "doi": "10.11114/ijsss.v9i2.5171",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Respecting minority opinions is vital in solving social problems. However,\nminority opinions are often ignored in general majority rules. To build\nconsensus on pluralistic values and make social choices that consider minority\nopinions, we propose aggregation methods that give weighting to the minority's\npositionality on cardinal cumulative voting. Based on quadratic and linear\nvoting, we formulated three weighted aggregation methods that differ in the\nratio of votes to cumulative points and the weighting of the minority to all\nmembers, and assuming that the distributions of votes follow normal\ndistributions, we calculated the frequency distributions of the aggregation\nresults. We found that minority opinions are more likely to be reflected\nproportionately to the average of the distribution in two of the above three\nmethods. This implies that Sen and Gotoh's idea of considering the social\nposition of unfortunate people on ordinal ranking in the welfare economics, was\nillustrated by weighting the minority's positionality on cardinal voting. In\naddition, it is possible to visualize the number and positionality of the\nminority from the analysis of the aggregation results. These results will be\nuseful to promote mutual understanding between the majority and minority by\ninteractively visualizing the contents of the proposed aggregation methods in\nthe consensus-building process. With the further development of information\ntechnology, the consensus building based on big data will be necessary. We\nrecommend the use of our proposed aggregation methods to make social choices\nfor pluralistic values such as social, environmental, and economic.\n"
    },
    {
        "paper_id": 2008.08918,
        "authors": "David Cavanagh, Mark Hoey, Andrew Clark, Michael Small, Paul Bailey,\n  and Jon Watson",
        "title": "West Australian Pandemic Response: The Black Swan of Black Swans",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The COVID-19 Pandemic has been described as the global challenge of our time,\nan enormous human tragedy with dramatic economic impacts. This paper describes\nthe response and expected recovery process for Western Australia, where a rapid\nand effective response was implemented. This has enabled an early transition\ninto an expected recovery both in health and economic terms. The positive\nlessons learned from this experience are documented as they emerge in order to\nsupport other states and nations as they address this issue globally in the\nnear-term and consider enduring improvements for the longer term. While the\nauthors have personal experience in the WA context, wider observations across\nAustralia and selected international benchmarks are also included. Key lessons\ninclude the importance of good health advice in Australia's interest; timely,\nsynchronized and aligned action at all levels of government; a program of well\ncommunicated, aligned health and economic measures which support all in society\nallowing a very high level of appropriate community behaviour, ensuring the\nhealth system was not overloaded; innovation in telehealth, testing, pandemic\nmodelling, and integrated operations which also allowed essential industries to\ncontinue; and strong border and travel controls with highly effective isolation\npreventing community spread, ultimately enabling rapid elimination of the\ndisease from the hospital system. In combination, these demonstrate that in the\ncase of Western Australia the result of first eliminating the disease from the\ncommunity, and then reopening the economy progressively at a strong pace, has\nenabled a world leading outcome in both in health and economic terms. The\nlessons from this experience are widely applicable, shareable both as\nsupporting service to other regions and through knowledge transfer.\n"
    },
    {
        "paper_id": 2008.09044,
        "authors": "Chassagneux Jean-Francois and Chotai Hinesh and Crisan Dan",
        "title": "Modelling multi-period carbon markets using singular forward backward\n  SDEs",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model for the evolution of emissions and the price of\nemissions allowances in a carbon market such as the European Union Emissions\nTrading System (EU ETS). The model accounts for multiple trading periods, or\nphases, with multiple times at which compliance can occur. At the end of each\ntrading period, the participating firms must surrender allowances for their\nemissions made during that period, and additional allowances can be used for\ncompliance in the following periods. We show that the multi-period allowance\npricing problem is well-posed for various mechanisms (such as banking,\nborrowing and withdrawal of allowances) linking the trading periods. The\nresults are based on the analysis of a forward-backward stochastic differential\nequation with coupled forward and backward components, a discontinuous terminal\ncondition and a forward component that is degenerate. We also introduce an\ninfinite period model, for a carbon market with a sequence of compliance times\nand with no end date. We show that, under appropriate conditions, the value\nfunction for the multi-period pricing problem converges, as the number of\nperiods increases, to a value function for this infinite period model, and that\nsuch functions are unique.\n"
    },
    {
        "paper_id": 2008.09108,
        "authors": "K.E. Feldman",
        "title": "Analytic Calibration in Andreasen-Huge SABR Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive analytic formulae which link $\\alpha$, $\\nu$ and $\\rho$ parameters\nin Andreasen-Huge style SABR model to the ATM price and option prices at four\nstrikes close to ATM. Based on these formulae we give a characterisation for\nthe SABR parameters in terms of derivatives of the swap rate forward\nprobability density function. We test the analytic result in the application to\nthe interest rate futures option market.\n"
    },
    {
        "paper_id": 2008.09407,
        "authors": "Maciej Ber\\k{e}sewicz, Katarzyna Pawlukiewicz",
        "title": "Estimation of the number of irregular foreigners in Poland using\n  non-linear count regression models",
        "comments": "36 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Population size estimation requires access to unit-level data in order to\ncorrectly apply capture-recapture methods. Unfortunately, for reasons of\nconfidentiality access to such data may be limited. To overcome this issue we\napply and extend the hierarchical Poisson-Gamma model proposed by Zhang (2008),\nwhich initially was used to estimate the number of irregular foreigners in\nNorway.\n  The model is an alternative to the current capture-recapture approach as it\ndoes not require linking multiple sources and is solely based on aggregated\nadministrative data that include (1) the number of apprehended irregular\nforeigners, (2) the number of foreigners who faced criminal charges and (3) the\nnumber of foreigners registered in the central population register. The model\nexplicitly assumes a relationship between the unauthorized and registered\npopulation, which is motivated by the interconnection between these two groups.\nThis makes the estimation conditionally dependent on the size of regular\npopulation, provides interpretation with analogy to registered population and\nmakes the estimated parameter more stable over time.\n  In this paper, we modify the original idea to allow for covariates and\nflexible count distributions in order to estimate the number of irregular\nforeigners in Poland in 2019. We also propose a parametric bootstrap for\nestimating standard errors of estimates. Based on the extended model we\nconclude that in as of 31.03.2019 and 30.09.2019 around 15,000 and 20,000\nforeigners and were residing in Poland without valid permits. This means that\nthose apprehended by the Polish Border Guard account for around 15-20% of the\ntotal.\n"
    },
    {
        "paper_id": 2008.09454,
        "authors": "Samuel N. Cohen, Christoph Reisinger and Sheng Wang",
        "title": "Detecting and repairing arbitrage in traded option prices",
        "comments": "Our implementation of this algorithm in Python is available in the\n  repository https://github.com/vicaws/arbitragerepair",
        "journal-ref": null,
        "doi": "10.1080/1350486X.2020.1846573",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Option price data are used as inputs for model calibration, risk-neutral\ndensity estimation and many other financial applications. The presence of\narbitrage in option price data can lead to poor performance or even failure of\nthese tasks, making pre-processing of the data to eliminate arbitrage\nnecessary. Most attention in the relevant literature has been devoted to\narbitrage-free smoothing and filtering (i.e. removing) of data. In contrast to\nsmoothing, which typically changes nearly all data, or filtering, which\ntruncates data, we propose to repair data by only necessary and minimal\nchanges. We formulate the data repair as a linear programming (LP) problem,\nwhere the no-arbitrage relations are constraints, and the objective is to\nminimise prices' changes within their bid and ask price bounds. Through\nempirical studies, we show that the proposed arbitrage repair method gives\nsparse perturbations on data, and is fast when applied to real world\nlarge-scale problems due to the LP formulation. In addition, we show that\nremoving arbitrage from prices data by our repair method can improve model\ncalibration with enhanced robustness and reduced calibration error.\n"
    },
    {
        "paper_id": 2008.09471,
        "authors": "Zezheng Zhang and Matloob Khushi",
        "title": "GA-MSSR: Genetic Algorithm Maximizing Sharpe and Sterling Ratio Method\n  for RoboTrading",
        "comments": null,
        "journal-ref": "IJCNN 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Foreign exchange is the largest financial market in the world, and it is also\none of the most volatile markets. Technical analysis plays an important role in\nthe forex market and trading algorithms are designed utilizing machine learning\ntechniques. Most literature used historical price information and technical\nindicators for training. However, the noisy nature of the market affects the\nconsistency and profitability of the algorithms. To address this problem, we\ndesigned trading rule features that are derived from technical indicators and\ntrading rules. The parameters of technical indicators are optimized to maximize\ntrading performance. We also proposed a novel cost function that computes the\nrisk-adjusted return, Sharpe and Sterling Ratio (SSR), in an effort to reduce\nthe variance and the magnitude of drawdowns. An automatic robotic trading\n(RoboTrading) strategy is designed with the proposed Genetic Algorithm\nMaximizing Sharpe and Sterling Ratio model (GA-MSSR) model. The experiment was\nconducted on intraday data of 6 major currency pairs from 2018 to 2019. The\nresults consistently showed significant positive returns and the performance of\nthe trading system is superior using the optimized rule-based features. The\nhighest return obtained was 320% annually using 5-minute AUDUSD currency pair.\nBesides, the proposed model achieves the best performance on risk factors,\nincluding maximum drawdowns and variance in return, comparing to benchmark\nmodels. The code can be accessed at\nhttps://github.com/zzzac/rule-based-forextrading-system\n"
    },
    {
        "paper_id": 2008.09481,
        "authors": "Joel da Costa, Tim Gebbie",
        "title": "Learning low-frequency temporal patterns for quantitative trading",
        "comments": "9 pages, 7 figures",
        "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI),\n  Canberra, Australia, 2020, pp. 1091-1099",
        "doi": "10.1109/SSCI47803.2020.9308232",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the viability of a modularised mechanistic online machine\nlearning framework to learn signals in low-frequency financial time series\ndata. The framework is proved on daily sampled closing time-series data from\nJSE equity markets. The input patterns are vectors of pre-processed sequences\nof daily, weekly and monthly or quarterly sampled feature changes. The data\nprocessing is split into a batch processed step where features are learnt using\na stacked autoencoder via unsupervised learning, and then both batch and online\nsupervised learning are carried out using these learnt features, with the\noutput being a point prediction of measured time-series feature fluctuations.\nWeight initializations are implemented with restricted Boltzmann machine\npre-training, and variance based initializations. Historical simulations are\nthen run using an online feedforward neural network initialised with the\nweights from the batch training and validation step. The validity of results\nare considered under a rigorous assessment of backtest overfitting using both\ncombinatorially symmetrical cross validation and probabilistic and deflated\nSharpe ratios. Results are used to develop a view on the phenomenology of\nfinancial markets and the value of complex historical data-analysis for trading\nunder the unstable adaptive dynamics that characterise financial markets.\n"
    },
    {
        "paper_id": 2008.09482,
        "authors": "Pengfei Xi, Shiyang Lai, Xueying Wang, Weiqiang Huang",
        "title": "Using detrended deconvolution foreign exchange network to identify\n  currency status",
        "comments": "9 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article proposed a hybrid detrended deconvolution foreign exchange\nnetwork construction method (DDFEN), which combined the detrended\ncross-correlation analysis coefficient (DCCC) and the network deconvolution\nmethod together. DDFEN is designed to reveal the `true' correlation of\ncurrencies by filtering indirect effects in the foreign exchange networks\n(FXNs). The empirical results show that DDFEN can reflect the change of\ncurrency status in the long term and also perform more stable than traditional\nnetwork construction methods.\n"
    },
    {
        "paper_id": 2008.09667,
        "authors": "Xiao Li and Weili Wu",
        "title": "A Blockchain Transaction Graph based Machine Learning Method for Bitcoin\n  Price Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin, as one of the most popular cryptocurrency, is recently attracting\nmuch attention of investors. Bitcoin price prediction task is consequently a\nrising academic topic for providing valuable insights and suggestions. Existing\nbitcoin prediction works mostly base on trivial feature engineering, that\nmanually designs features or factors from multiple areas, including Bticoin\nBlockchain information, finance and social media sentiments. The feature\nengineering not only requires much human effort, but the effectiveness of the\nintuitively designed features can not be guaranteed. In this paper, we aim to\nmining the abundant patterns encoded in bitcoin transactions, and propose\nk-order transaction graph to reveal patterns under different scope. We propose\nthe transaction graph based feature to automatically encode the patterns. A\nnovel prediction method is proposed to accept the features and make price\nprediction, which can take advantage from particular patterns from different\nhistory period. The results of comparison experiments demonstrate that the\nproposed method outperforms the most recent state-of-art methods.\n"
    },
    {
        "paper_id": 2008.09815,
        "authors": "Yaqian Zhou, Hai Yang, Jintao Ke, Hai Wang, Xinwei Li",
        "title": "Competitive ride-sourcing market with a third-party integrator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, some transportation service providers attempt to integrate the ride\nservices offered by multiple independent ride-sourcing platforms, and\npassengers are able to request ride through such third-party integrators or\nconnectors and receive service from any one of the platforms. This novel\nbusiness model, termed as third-party platform-integration in this paper, has\npotentials to alleviate the cost of market fragmentation due to the demand\nsplitting among multiple platforms. While most existing studies focus on the\noperation strategies for one single monopolist platform, much less is known\nabout the competition and platform-integration as well as the implications on\noperation strategy and system efficiency. In this paper, we propose\nmathematical models to describe the ride-sourcing market with multiple\ncompeting platforms and compare system performance metrics between two market\nscenarios, i.e., with and without platform-integration, at Nash equilibrium as\nwell as social optimum. We find that platform-integration can increase total\nrealized demand and social welfare at both Nash equilibrium and social optimum,\nbut may not necessarily generate a greater profit when vehicle supply is\nsufficiently large or/and market is too fragmented. We show that the market\nwith platform-integration generally achieves greater social welfare. On one\nhand, the integrator in platform-integration is able to generate a thicker\nmarket and reduce matching frictions; on the other hand, multiple platforms are\nstill competing by independently setting their prices, which help to mitigate\nmonopoly mark-up as in the monopoly market.\n"
    },
    {
        "paper_id": 2008.09818,
        "authors": "Anand Deo and Karthyek Murthy",
        "title": "Optimizing tail risks using an importance sampling based extrapolation\n  for heavy-tailed objectives",
        "comments": "20 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the prominence of Conditional Value-at-Risk (CVaR) as a measure\nfor tail risk in settings affected by uncertainty, we develop a new formula for\napproximating CVaR based optimization objectives and their gradients from\nlimited samples. A key difficulty that limits the widespread practical use of\nthese optimization formulations is the large amount of data required by the\nstate-of-the-art sample average approximation schemes to approximate the CVaR\nobjective with high fidelity. Unlike the state-of-the-art sample average\napproximations which require impractically large amounts of data in tail\nprobability regions, the proposed approximation scheme exploits the\nself-similarity of heavy-tailed distributions to extrapolate data from suitable\nlower quantiles. The resulting approximations are shown to be statistically\nconsistent and are amenable for optimization by means of conventional gradient\ndescent. The approximation is guided by means of a systematic\nimportance-sampling scheme whose asymptotic variance reduction properties are\nrigorously examined. Numerical experiments demonstrate the superiority of the\nproposed approximations and the ease of implementation points to the\nversatility of settings to which the approximation scheme can be applied.\n"
    },
    {
        "paper_id": 2008.10145,
        "authors": "Takaaki Hamada",
        "title": "Implications of the Tradeoff between Inside and Outside Social Status in\n  Group Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a group choice problem of agents pursuing social status. We\nassume heterogeneous agents want to signal their private information (ability,\nincome, patience, altruism, etc.) to others, facing tradeoff between \"outside\nstatus\" (desire to be perceived in prestigious group from outside observers)\nand \"inside status\" (desire to be perceived talented from peers inside their\ngroup). To analyze the tradeoff, we develop two stage signaling model in which\neach agent firstly chooses her group and secondly chooses her action in the\ngroup she chose. They face binary choice problems both in group and action\nchoices. Using cutoff strategy, we construct an partially separating\nequilibrium such that there are four populations: (i) choosing high group with\nstrong incentive for action in the group, (ii) high group with weak incentive,\n(iii) low group with strong incentive, and (iv) low group with weak incentive.\nBy comparative statics results, we find some spillover effects from a certain\ngroup to another, on how four populations change, when a policy is taken in\neach group. These results have rich implications for group choice problems like\nschool, firm or residential preference.\n"
    },
    {
        "paper_id": 2008.10184,
        "authors": "Liang Wang, Weixuan Xia",
        "title": "Power-type derivatives for rough volatility with jumps",
        "comments": "Significantly revised; new data sets",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a novel pricing-hedging framework for volatility\nderivatives which simultaneously takes into account rough volatility and\nvolatility jumps. Our model directly targets the instantaneous variance of a\nrisky asset and consists of a generalized fractional Ornstein-Uhlenbeck process\ndriven by a L\\'{e}vy subordinator and an independent sinusoidal-composite\nL\\'{e}vy process. The former component captures short-term dependence in the\ninstantaneous volatility, while the latter is introduced expressly for\nrectifying the activity level of the average forward variance. Such a framework\nensures that the characteristic function of average forward variance is\nobtainable in semi-closed form, without having to invoke any geometric-mean\napproximations. To analyze swaps and European-style options on average forward\nvolatility, we introduce a general class of power-type derivatives on the\naverage forward variance, which also provide flexible nonlinear leverage\nexposure. Pricing-hedging formulae are based on a modified numerical Fourier\ntransform technique. A comparative empirical study is conducted on two\nindependent recent data sets on VIX options, before and during the COVID-19\npandemic, to demonstrate that the proposed framework is highly amenable to\nefficient model calibration under various choices of kernels.\n"
    },
    {
        "paper_id": 2008.10257,
        "authors": "Xue Dong He and Zhaoli Jiang and Steven Kou",
        "title": "Portfolio Selection under Median and Quantile Maximization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although maximizing median and quantiles is intuitively appealing and has an\naxiomatic foundation, it is difficult to study the optimal portfolio strategy\ndue to the discontinuity and time inconsistency in the objective function. We\nuse the intra-personal equilibrium approach to study the problem.\nInterestingly, we find that the only viable outcome is from the median\nmaximization, because for other quantiles either the equilibrium does not exist\nor there is no investment in the risky assets. The median maximization strategy\ngives a simple explanation to why wealthier people invest more percentage of\ntheir wealth in risky assets.\n"
    },
    {
        "paper_id": 2008.10745,
        "authors": "Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Matthew O. Jackson and\n  Samuel Thau",
        "title": "Interacting Regional Policies in Containing a Disease",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.2021520118",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regional quarantine policies, in which a portion of a population surrounding\ninfections are locked down, are an important tool to contain disease. However,\njurisdictional governments -- such as cities, counties, states, and countries\n-- act with minimal coordination across borders. We show that a regional\nquarantine policy's effectiveness depends upon whether (i) the network of\ninteractions satisfies a balanced-growth condition, (ii) infections have a\nshort delay in detection, and (iii) the government has control over and\nknowledge of the necessary parts of the network (no leakage of behaviors). As\nthese conditions generally fail to be satisfied, especially when interactions\ncross borders, we show that substantial improvements are possible if\ngovernments are outward-looking and proactive: triggering quarantines in\nreaction to neighbors' infection rates, in some cases even before infections\nare detected internally. We also show that even a few lax governments -- those\nthat wait for nontrivial internal infection rates before quarantining -- impose\nsubstantial costs on the whole system. Our results illustrate the importance of\nunderstanding contagion across policy borders and offer a starting point in\ndesigning proactive policies for decentralized jurisdictions.\n"
    },
    {
        "paper_id": 2008.10775,
        "authors": "Ruda Zhang and Roger Ghanem",
        "title": "Drivers learn city-scale dynamic equilibrium",
        "comments": "10 pages, 1 table, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding driver behavior in on-demand mobility services is crucial for\ndesigning efficient and sustainable transport models. Drivers' delivery\nstrategy is well understood, but their search strategy and learning process\nstill lack an empirically validated model. Here we provide a game-theoretic\nmodel of driver search strategy and learning dynamics, interpret the collective\noutcome in a thermodynamic framework, and verify its various implications\nempirically. We capture driver search strategies in a multi-market oligopoly\nmodel, which has a unique Nash equilibrium and is globally asymptotically\nstable. The equilibrium can therefore be obtained via heuristic learning rules\nwhere drivers pursue the incentive gradient or simply imitate others. To help\nunderstand city-scale phenomena, we offer a macroscopic view with the laws of\nthermodynamics. With 870 million trips of over 50k drivers in New York City, we\nshow that the equilibrium well explains the spatiotemporal patterns of driver\nsearch behavior, and estimate an empirical constitutive relation. We find that\nnew drivers learn the equilibrium within a year, and those who stay longer\nlearn better. The collective response to new competition is also as predicted.\nAmong empirical studies of driver strategy in on-demand services, our work\nexamines the longest period, the most trips, and is the largest for taxi\nindustry.\n"
    },
    {
        "paper_id": 2008.10885,
        "authors": "Asim Kumer Dey, Toufiqul Haq, Kumer Das, and Irina Panovska",
        "title": "Quantifying the impact of COVID-19 on the US stock market: An analysis\n  from multi-source information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel temporal complex network approach to quantify the US\ncounty level spread dynamics of COVID-19. The objective is to study the effects\nof the local spread dynamics, COVID-19 cases and death, and Google search\nactivities on the US stock market. We use both conventional econometric and\nMachine Learning (ML) models. The results suggest that COVID-19 cases and\ndeaths, its local spread, and Google searches have impacts on abnormal stock\nprices between January 2020 to May 2020. In addition, incorporating information\nabout local spread significantly improves the performance of forecasting models\nof the abnormal stock prices at longer forecasting horizons. On the other hand,\nalthough a few COVID-19 related variables, e.g., US total deaths and US new\ncases exhibit causal relationships on price volatility, COVID-19 cases and\ndeaths, local spread of COVID-19, and Google search activities do not have\nimpacts on price volatility.\n"
    },
    {
        "paper_id": 2008.10926,
        "authors": "Riccardo Ciacci and Dario Sansone",
        "title": "The Impact of Sodomy Law Repeals on Crime",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We exploit variation in the timing of decriminalization of same-sex sexual\nintercourse across U.S. states to estimate the impact of these law changes on\ncrime through difference-in-difference and event-study models. We provide the\nfirst evidence that sodomy law repeals led to a decline in the number of\narrests for disorderly conduct, prostitution, and other sex offenses.\nFurthermore, we show that these repeals led to a reduction in arrests for drug\nand alcohol consumption.\n"
    },
    {
        "paper_id": 2008.1093,
        "authors": "Valentin Courgeau, Almut E.D. Veraart",
        "title": "High-frequency Estimation of the L\\'evy-driven Graph Ornstein-Uhlenbeck\n  process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the Graph Ornstein-Uhlenbeck (GrOU) process observed on a\nnon-uniform discrete time grid and introduce discretised maximum likelihood\nestimators with parameters specific to the whole graph or specific to each\ncomponent, or node. Under a high-frequency sampling scheme, we study the\nasymptotic behaviour of those estimators as the mesh size of the observation\ngrid goes to zero. We prove two stable central limit theorems to the same\ndistribution as in the continuously-observed case under both finite and\ninfinite jump activity for the L\\'evy driving noise. When a graph structure is\nnot explicitly available, the stable convergence allows to consider\npurpose-specific sparse inference procedures, i.e. pruning, on the edges\nthemselves in parallel to the GrOU inference and preserve its asymptotic\nproperties. We apply the new estimators to wind capacity factor measurements,\ni.e. the ratio between the wind power produced locally compared to its rated\npeak power, across fifty locations in Northern Spain and Portugal. We show the\nsuperiority of those estimators compared to the standard least squares\nestimator through a simulation study extending known univariate results across\ngraph configurations, noise types and amplitudes.\n"
    },
    {
        "paper_id": 2008.10952,
        "authors": "Adit Chopra",
        "title": "A Data Envelopment Analysis Approach to Benchmark the Performance of\n  Mutual Funds in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the Indian economy grows digitally and becomes more financially inclusive,\nmore and more investors have started to invest in the Indian capital markets.\nThe number of retail and institutional folios with Indian mutual fund schemes\nhave continued to rise for the 74th consecutive month. This study considers 139\nmutual fund schemes (98 equity schemes) and aims to ascertain the various\nmetrics and parameters, retail and institutional investors continue to rely on\nto make investment recommendations. We compare these with the results from a\ndata envelopment analysis model that generates an efficiency frontier based on\nan optimal risk, cost, and return trade-off. We further put forth an iteration\nof the DEA model, not only considering risk, cost, and return characteristics\nbut also incorporating metrics such as the information ratio which hold\nsignificance for retail and institutional investors. We compare these results\nwith traditional metrics and fund rankings published by established industry\nrating agencies.\n"
    },
    {
        "paper_id": 2008.10967,
        "authors": "Herve Bercegol and Henri Benisty",
        "title": "An energy-based macroeconomic model validated by global historical\n  series since 1820",
        "comments": "Main text: 12 pages, 4 figures, 1 table; Appendices: 18 pages, 8\n  figures; 57 References",
        "journal-ref": "Ecological Economics 192 (2022) 107253",
        "doi": "10.1016/j.ecolecon.2021.107253",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global historical series spanning the last two centuries recently became\navailable for primary energy consumption (PEC) and Gross Domestic Product\n(GDP). Based on a thorough analysis of the data, we propose a new, simple\nmacroeconomic model whereby physical power is fueling economic power. From 1820\nto 1920, the linearity between global PEC and world GDP justifies basic\nequations where, originally, PEC incorporates unskilled human labor that\nconsumes and converts energy from food. In a consistent model, both physical\ncapital and human capital are fed by PEC and represent a form of stored energy.\nIn the following century, from 1920 to 2016, GDP grows quicker than PEC.\nPeriods of quasi-linearity of the two variables are separated by distinct\njumps, which can be interpreted as radical technology shifts. The GDP to PEC\nratio accumulates game-changing innovation, at an average growth rate\nproportional to PEC. These results seed alternative strategies for modeling and\nfor political management of the climate crisis and the energy transition.\n"
    },
    {
        "paper_id": 2008.11275,
        "authors": "Walter H. Bruckman",
        "title": "Formula to Determine the Countries Equilibrium Exchange Rate With the\n  Dollar and Proposal for a Second Bretton Woods Conference",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the way in which can be determined the exchange rates\nthat simultaneously balance the trade balances of all countries that trade with\neach other within a common market. A mathematical synthesis between the theory\nof comparative advantages of Ricardo and Mill and the New Theory on\nInternational Trade of Paul Krugman is also presented in this paper. This\nmathematical synthesis shows that these theories are complementary. Also\npresented in this paper is a proposal to organize a common market of the\nAmerican hemisphere. This economic alliance would allow to establish a\npolitical alliance for the common defense of the entire American hemisphere.\nThe formula we developed in this paper to determine the exchange rates of the\ncountries solves the problem that Mexico, Canada, Europe, Japan and China\ncurrently experience with the United States in relation to the deficits and\nsurpluses in the trade balance of the countries and their consequent impediment\nso that stable growth of international trade can be achieved.\n"
    },
    {
        "paper_id": 2008.11327,
        "authors": "Makoto Mizuno, Hideaki Aoyama, Yoshi Fujiwara",
        "title": "Untangling the complexity of market competition in consumer goods -A\n  complex Hilbert PCA analysis",
        "comments": "27 pages with 9 Figures and 7 Tables, including a 2-page Appendix",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0245531",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Today's consumer goods markets are rapidly evolving with significant growth\nin the number of information media as well as the number of competitive\nproducts. In this environment, obtaining a quantitative grasp of heterogeneous\ninteractions of firms and customers, which have attracted interest of\nmanagement scientists and economists, requires the analysis of extremely\nhigh-dimensional data. Existing approaches in quantitative research could not\nhandle such data without any reliable prior knowledge nor strong assumptions.\nAlternatively, we propose a novel method called complex Hilbert principal\ncomponent analysis (CHPCA) and construct a synchronization network using Hodge\ndecomposition. CHPCA enables us to extract significant comovements with a time\nlead/delay in the data, and Hodge decomposition is useful for identifying the\ntime-structure of correlations. We apply this method to the Japanese beer\nmarket data and reveal comovement of variables related to the consumer choice\nprocess across multiple products. Furthermore, we find remarkable customer\nheterogeneity by calculating the coordinates of each customer in the space\nderived from the results of CHPCA. Lastly, we discuss the policy and managerial\nimplications, limitations, and further development of the proposed method.\n"
    },
    {
        "paper_id": 2008.11334,
        "authors": "Zhaojun Wang, Duy Nong, Amanda M. Countryman, James J. Corbett, and\n  Travis Warziniack",
        "title": "Potential impacts of ballast water regulations on international trade,\n  shipping patterns, and the global economy: An integrated transportation and\n  economic modeling assessment",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jenvman.2020.110892",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global ballast water management regulations aiming to decrease aquatic\nspecies invasion require actions that can increase shipping costs. We employ an\nintegrated shipping cost and global economic modeling approach to investigate\nthe impacts of ballast water regulations on bilateral trade, national\neconomies, and shipping patterns. Given the potential need for more stringent\nregulation at regional hotspots of species invasions, this work considers two\nballast water treatment policy scenarios: implementation of current\ninternational regulations, and a possible stricter regional regulation that\ntargets ships traveling to and from the United States while other vessels\ncontinue to face current standards. We find that ballast water management\ncompliance costs under both scenarios lead to modest negative impacts on\ninternational trade and national economies overall. However, stricter\nregulations applied to U.S. ports are expected to have large negative impacts\non bilateral trade of several specific commodities for a few countries. Trade\ndiversion causes decreased U.S. imports of some products, leading to minor\neconomic welfare losses.\n"
    },
    {
        "paper_id": 2008.11558,
        "authors": "Wonse Kim, Younng-Jin Kim, Gihyun Lee, Woong Kook",
        "title": "Investigation of Flash Crash via Topological Data Analysis",
        "comments": "11 pages, 4 figures, the Third PPICTA",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Topological data analysis has been acknowledged as one of the most successful\nmathematical data analytic methodologies in various fields including medicine,\ngenetics, and image analysis. In this paper, we explore the potential of this\nmethodology in finance by applying persistence landscape and dynamic time\nseries analysis to analyze an extreme event in the stock market, known as Flash\nCrash. We will provide results of our empirical investigation to confirm the\neffectiveness of our new method not only for the characterization of this\nextreme event but also for its prediction purposes.\n"
    },
    {
        "paper_id": 2008.1172,
        "authors": "Paul Kilgarriff and Martin Charlton",
        "title": "A Spatial Analysis of Disposable Income in Ireland: A GWR Approach",
        "comments": "31 pages, 14 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the spatial distribution of income in Ireland. Median\ngross household disposable income data from the CSO, available at the Electoral\nDivision (ED) level, is used to explore the spatial variability in income.\nGeary's C highlights the spatial dependence of income, highlighting that the\ndistribution of income is not random across space and is influenced by\nlocation. Given the presence of spatial autocorrelation, utilising a global OLS\nregression will lead to biased results. Geographically Weighted Regression\n(GWR) is used to examine the spatial heterogeneity of income and the impact of\nlocal demographic drivers on income. GWR results show the demographic drivers\nhave varying levels of influence on income across locations. Lone parent has a\nstronger negative impact in the Cork commuter belt than it does in the Dublin\ncommuter belt. The relationship between household income and the demographic\ncontext of the area is a complicated one. This paper attempts to examine these\nrelationships acknowledging the impact of space.\n"
    },
    {
        "paper_id": 2008.11757,
        "authors": "Ashley Davey, Harry Zheng",
        "title": "Deep Learning for Constrained Utility Maximisation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11009-021-09912-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes two algorithms for solving stochastic control problems\nwith deep learning, with a focus on the utility maximisation problem. The first\nalgorithm solves Markovian problems via the Hamilton Jacobi Bellman (HJB)\nequation. We solve this highly nonlinear partial differential equation (PDE)\nwith a second order backward stochastic differential equation (2BSDE)\nformulation. The convex structure of the problem allows us to describe a dual\nproblem that can either verify the original primal approach or bypass some of\nthe complexity. The second algorithm utilises the full power of the duality\nmethod to solve non-Markovian problems, which are often beyond the scope of\nstochastic control solvers in the existing literature. We solve an adjoint BSDE\nthat satisfies the dual optimality conditions. We apply these algorithms to\nproblems with power, log and non-HARA utilities in the Black-Scholes, the\nHeston stochastic volatility, and path dependent volatility models. Numerical\nexperiments show highly accurate results with low computational cost,\nsupporting our proposed algorithms.\n"
    },
    {
        "paper_id": 2008.11788,
        "authors": "Linyu Zheng and Hongmei He",
        "title": "Share Price Prediction of Aerospace Relevant Companies with Recurrent\n  Neural Networks based on PCA",
        "comments": "38 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The capital market plays a vital role in marketing operations for aerospace\nindustry. However, due to the uncertainty and complexity of the stock market\nand many cyclical factors, the stock prices of listed aerospace companies\nfluctuate significantly. This makes the share price prediction challengeable.\nTo improve the prediction of share price for aerospace industry sector and well\nunderstand the impact of various indicators on stock prices, we provided a\nhybrid prediction model by the combination of Principal Component Analysis\n(PCA) and Recurrent Neural Networks. We investigated two types of aerospace\nindustries (manufacturer and operator). The experimental results show that PCA\ncould improve both accuracy and efficiency of prediction. Various factors could\ninfluence the performance of prediction models, such as finance data, extracted\nfeatures, optimisation algorithms, and parameters of the prediction model. The\nselection of features may depend on the stability of historical data: technical\nfeatures could be the first option when the share price is stable, whereas\nfundamental features could be better when the share price has high fluctuation.\nThe delays of RNN also depend on the stability of historical data for different\ntypes of companies. It would be more accurate through using short-term\nhistorical data for aerospace manufacturers, whereas using long-term historical\ndata for aerospace operating airlines. The developed model could be an\nintelligent agent in an automatic stock prediction system, with which, the\nfinancial industry could make a prompt decision for their economic strategies\nand business activities in terms of predicted future share price, thus\nimproving the return on investment. Currently, COVID-19 severely influences\naerospace industries. The developed approach can be used to predict the share\nprice of aerospace industries at post COVID-19 time.\n"
    },
    {
        "paper_id": 2008.11806,
        "authors": "Shengfeng Mei and Hong Gao",
        "title": "The Time Function of Stock Price",
        "comments": "12 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper tends to define the quantitative relationship between the stock\nprice and time as a time function. Based on the empirical evidence that the\nlog-return of a stock is the series of white noise, a mathematical model of the\nintegral white noise is established to describe the phenomenon of stock price\nmovement. A deductive approach is used to derive the auto-correlation function,\ndisplacement formula and power spectral density of the stock price movement,\nwhich reveals not only the characteristics and rules of the movement but also\nthe predictability of the stock price. The deductive fundamental is provided\nfor the price analysis, prediction and risk management of portfolio investment.\n"
    },
    {
        "paper_id": 2008.1185,
        "authors": "Marco Due\\~nas, Mercedes Campi, Luis Olmos",
        "title": "Changes in mobility and socioeconomic conditions in Bogot\\'a city during\n  the COVID-19 outbreak",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We analyze mobility changes following the implementation of containment\nmeasures aimed at mitigating the spread of COVID-19 in Bogot\\'a, Colombia. We\ncharacterize the mobility network before and during the pandemic and analyze\nits evolution and changes between January and July 2020. We then link the\nobserved mobility changes to socioeconomic conditions, estimating a gravity\nmodel to assess the effect of socioeconomic conditions on mobility flows. We\nobserve an overall reduction in mobility trends, but the overall connectivity\nbetween different areas of the city remains after the lockdown, reflecting the\nmobility network's resilience. We find that the responses to lockdown policies\ndepend on socioeconomic conditions. Before the pandemic, the population with\nbetter socioeconomic conditions shows higher mobility flows. Since the\nlockdown, mobility presents a general decrease, but the population with worse\nsocioeconomic conditions shows lower decreases in mobility flows. We conclude\nderiving policy implications.\n"
    },
    {
        "paper_id": 2008.1205,
        "authors": "Samuel Fern\\'andez-Lorenzo, Diego Porras, Juan Jos\\'e Garc\\'ia-Ripoll",
        "title": "Hybrid quantum-classical optimization for financial index tracking",
        "comments": "24 pages, 12 figures. A few changes in structure implemented in\n  version 2",
        "journal-ref": "Quantum Sci. Technol. 6 034010 (2021)",
        "doi": "10.1088/2058-9565/abf9af",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tracking a financial index boils down to replicating its trajectory of\nreturns for a well-defined time span by investing in a weighted subset of the\nsecurities included in the benchmark. Picking the optimal combination of assets\nbecomes a challenging NP-hard problem even for moderately large indices\nconsisting of dozens or hundreds of assets, thereby requiring heuristic methods\nto find approximate solutions. Hybrid quantum-classical optimization with\nvariational gate-based quantum circuits arises as a plausible method to improve\nperformance of current schemes. In this work we introduce a heuristic pruning\nalgorithm to find weighted combinations of assets subject to cardinality\nconstraints. We further consider different strategies to respect such\nconstraints and compare the performance of relevant quantum ans\\\"{a}tze and\nclassical optimizers through numerical simulations.\n"
    },
    {
        "paper_id": 2008.12108,
        "authors": "Marius-F. Danca",
        "title": "Coexisting Hidden and self-excited attractors in an economic system of\n  integer or fractional order",
        "comments": "Accepted at IJBC (revised)",
        "journal-ref": null,
        "doi": "10.1142/S0218127421500620",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the dynamics of an economic system with foreign financing, of\ninteger or fractional order, are analyzed. The symmetry of the system\ndetermines the existence of two pairs of coexisting attractors. The\ninteger-order version of the system proves to have several combinations of\ncoexisting hidden attractors with self-excited attractors. Because one of the\nsystem variables represents the foreign capital in ow, the presence of hidden\nattractors could be of a real interest in economic models. The fractional-order\nvariant presents another interesting coexistence of attractors in the\nfractional order space.\n"
    },
    {
        "paper_id": 2008.12132,
        "authors": "Christina Uhl, Nadia Abou Nabout, Klaus Miller",
        "title": "How Much Ad Viewability is Enough? The Effect of Display Ad Viewability\n  on Advertising Effectiveness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large share of all online display advertisements (ads) are never seen by a\nhuman. For instance, an ad could appear below the page fold, where a user never\nscrolls. Yet, an ad is essentially ineffective if it is not at least somewhat\nviewable. Ad viewability - which refers to the pixel percentage-in-view and the\nexposure duration of an online display ad - has recently garnered great\ninterest among digital advertisers and publishers. However, we know very little\nabout the impact of ad viewability on advertising effectiveness. We work to\nclose this gap by analyzing a large-scale observational data set with more than\n350,000 ad impressions similar to the data sets that are typically available to\ndigital advertisers and publishers. This analysis reveals that longer exposure\ndurations (>10 seconds) and 100% visible pixels do not appear to be optimal in\ngenerating view-throughs. The highest view-through rates seem to be generated\nwith relatively lower pixel/second-combinations of 50%/1, 50%/5, 75%/1, and\n75%/5. However, this analysis does not account for user behavior that may be\ncorrelated with or even drive ad viewability and may therefore result in\nendogeneity issues. Consequently, we manipulated ad viewability in a randomized\nonline experiment for a major European news website, finding the highest ad\nrecognition rates among relatively higher pixel/second-combinations of 75%/10,\n100%/5 and 100%/10. Everything below 75\\% or 5 seconds performs worse. Yet, we\nfind that it may be sufficient to have either a long exposure duration or high\npixel percentage-in-view to reach high advertising effectiveness. Our results\nprovide guidance to advertisers enabling them to establish target viewability\nrates more appropriately and to publishers who wish to differentiate their\nviewability products.\n"
    },
    {
        "paper_id": 2008.12152,
        "authors": "Aiusha Sangadiev, Rodrigo Rivera-Castro, Kirill Stepanov, Andrey\n  Poddubny, Kirill Bubenchikov, Nikita Bekezin, Polina Pilyugina and Evgeny\n  Burnaev",
        "title": "DeepFolio: Convolutional Neural Networks for Portfolios with Limit Order\n  Book Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work proposes DeepFolio, a new model for deep portfolio management based\non data from limit order books (LOB). DeepFolio solves problems found in the\nstate-of-the-art for LOB data to predict price movements. Our evaluation\nconsists of two scenarios using a large dataset of millions of time series. The\nimprovements deliver superior results both in cases of abundant as well as\nscarce data. The experiments show that DeepFolio outperforms the\nstate-of-the-art on the benchmark FI-2010 LOB. Further, we use DeepFolio for\noptimal portfolio allocation of crypto-assets with rebalancing. For this\npurpose, we use two loss-functions - Sharpe ratio loss and minimum volatility\nrisk. We show that DeepFolio outperforms widely used portfolio allocation\ntechniques in the literature.\n"
    },
    {
        "paper_id": 2008.12275,
        "authors": "Alexey Bakshaev",
        "title": "Market-making with reinforcement-learning (SAC)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper explores the application of a continuous action space soft\nactor-critic (SAC) reinforcement learning model to the area of automated\nmarket-making. The reinforcement learning agent receives a simulated flow of\nclient trades, thus accruing a position in an asset, and learns to offset this\nrisk by either hedging at simulated \"exchange\" spreads or by attracting an\noffsetting client flow by changing offered client spreads (skewing the offered\nprices). The question of learning minimum spreads that compensate for the risk\nof taking the position is being investigated. Finally, the agent is posed with\na problem of learning to hedge a blended client trade flow resulting from\nindependent price processes (a \"portfolio\" position). The position penalty\nmethod is introduced to improve the convergence. An Open-AI gym-compatible\nhedge environment is introduced and the Open AI SAC baseline RL engine is being\nused as a learning baseline.\n"
    },
    {
        "paper_id": 2008.12364,
        "authors": "J\\'anos Kert\\'esz and Johannes Wachs",
        "title": "Complexity science approach to economic crime",
        "comments": "Preprint of published comment in Nat. Rev. Phys",
        "journal-ref": "Nature Review Physics 2020",
        "doi": "10.1038/s42254-020-0238-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this comment we discuss how complexity science and network science are\nparticularly useful for identifying and describing the hidden traces of\neconomic misbehaviour such as fraud and corruption.\n"
    },
    {
        "paper_id": 2008.12427,
        "authors": "John A. Major, Stephen J. Mildenhall",
        "title": "Pricing and Capital Allocation for Multiline Insurance Firms With Finite\n  Assets in an Imperfect Market",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze multiline pricing and capital allocation in equilibrium\nno-arbitrage markets. Existing theories often assume a perfect complete market,\nbut when pricing is linear, there is no diversification benefit from risk\npooling and therefore no role for insurance companies. Instead of a perfect\nmarket, we assume a non-additive distortion pricing functional and the\nprinciple of equal priority of payments in default. Under these assumptions, we\nderive a canonical allocation of premium and margin, with properties that merit\nthe name the natural allocation. The natural allocation gives non-negative\nmargins to all independent lines for default-free insurance but can exhibit\nnegative margins for low-risk lines under limited liability. We introduce novel\nconditional expectation measures of relative risk within a portfolio and use\nthem to derive simple, intuitively appealing expressions for risk margins and\ncapital allocations. We give a unique capital allocation consistent with our\nlaw invariant pricing functional. Such allocations produce returns that vary by\nline, in contrast to many other approaches. Our model provides a bridge between\nthe theoretical perspective that there should be no compensation for bearing\ndiversifiable risk and the empirical observation that more risky lines fetch\nhigher margins relative to subjective expected values.\n"
    },
    {
        "paper_id": 2008.12459,
        "authors": "Nik Dawson, Sacha Molitorisz, Marian-Andrei Rizoiu and Peter Fray",
        "title": "Layoffs, Inequity and COVID-19: A Longitudinal Study of the Journalism\n  Jobs Crisis in Australia from 2012 to 2020",
        "comments": null,
        "journal-ref": "Journalism, p. 146488492199628. 2021",
        "doi": "10.1177/1464884921996286",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In Australia and beyond, journalism is reportedly an industry in crisis, a\ncrisis exacerbated by COVID-19. However, the evidence revealing the crisis is\noften anecdotal or limited in scope. In this unprecedented longitudinal\nresearch, we draw on data from the Australian journalism jobs market from\nJanuary 2012 until March 2020. Using Data Science and Machine Learning\ntechniques, we analyse two distinct data sets: job advertisements (ads) data\ncomprising 3,698 journalist job ads from a corpus of over 8 million Australian\njob ads; and official employment data from the Australian Bureau of Statistics.\nHaving matched and analysed both sources, we address both the demand for and\nsupply of journalists in Australia over this critical period. The data show\nthat the crisis is real, but there are also surprises. Counter-intuitively, the\nnumber of journalism job ads in Australia rose from 2012 until 2016, before\nfalling into decline. Less surprisingly, for the entire period studied the\nfigures reveal extreme volatility, characterised by large and erratic\nfluctuations. The data also clearly show that COVID-19 has significantly\nworsened the crisis. We then tease out more granular findings, including: that\nthere are now more women than men journalists in Australia, but that gender\ninequity is worsening, with women journalists getting younger and worse-paid\njust as men journalists are, on average, getting older and better-paid; that,\ndespite the crisis besetting the industry, the demand for journalism skills has\nincreased; and that, perhaps concerningly, the skills sought by journalism job\nads increasingly include social media and generalist communications.\n"
    },
    {
        "paper_id": 2008.1291,
        "authors": "Danny Turkson and Joy Kafui Ahiabor",
        "title": "Implication of Natal Care and Maternity Leave on Child Morbidity:\n  Evidence from Ghana",
        "comments": "11 pages",
        "journal-ref": "Global Journal of Health Science, 12(9) (2020)",
        "doi": "10.5539/gjhs.v12n9p94",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Failure to receive post-natal care within first week of delivery causes a 3%\nincrease in the possibility of Acute Respiratory Infection in children under\nfive. Mothers with unpaid maternity leave put their children at a risk of 3.9%\nincrease in the possibility of ARI compared to those with paid maternity leave.\n"
    },
    {
        "paper_id": 2008.12953,
        "authors": "Jinxin Wang, Zengde Deng, Taoli Zheng, and Anthony Man-Cho So",
        "title": "Sparse High-Order Portfolios via Proximal DCA and SCA",
        "comments": "ICASSP 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we aim at solving the cardinality constrained high-order\nportfolio optimization, i.e., mean-variance-skewness-kurtosis model with\ncardinality constraint (MVSKC). Optimization for the MVSKC model is of great\ndifficulty in two parts. One is that the objective function is non-convex, the\nother is the combinational nature of the cardinality constraint, leading to\nnon-convexity as well dis-continuity. Based on the observation that cardinality\nconstraint has the difference-of-convex (DC) property, we transform the\ncardinality constraint into a penalty term and then propose three algorithms\nincluding the proximal difference of convex algorithm (pDCA), pDCA with\nextrapolation (pDCAe) and the successive convex approximation (SCA) to handle\nthe resulting penalized MVSK (PMVSK) formulation. Moreover, theoretical\nconvergence results of these algorithms are established respectively. Numerical\nexperiments on the real datasets demonstrate the superiority of our proposed\nmethods in obtaining high utility and sparse solutions as well as efficiency in\nterms of time usage.\n"
    },
    {
        "paper_id": 2008.13082,
        "authors": "Ting He",
        "title": "Nonparametric Predictive Inference for Asian options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Asian option, as one of the path-dependent exotic options, is widely traded\nin the energy market, either for speculation or hedging. However, it is hard to\nprice, especially the one with the arithmetic average price. The traditional\ntrading procedure is either too restrictive by assuming the distribution of the\nunderlying asset or less rigorous by using the approximation. It is attractive\nto infer the Asian option price with few assumptions of the underlying asset\ndistribution and adopt to the historical data with a nonparametric method. In\nthis paper, we present a novel approach to price the Asian option from an\nimprecise statistical aspect. Nonparametric Predictive Inference (NPI) is\napplied to infer the average value of the future underlying asset price, which\nattempts to make the prediction reflecting more uncertainty because of the\nlimited information. A rational pairwise trading criterion is also proposed in\nthis paper for the Asian options comparison, as a risk measure. The NPI method\nfor the Asian option is illustrated in several examples by using the simulation\ntechniques or the empirical data from the energy market.\n"
    },
    {
        "paper_id": 2008.13087,
        "authors": "Mingbin Ben Feng and Eunhye Song",
        "title": "Efficient Nested Simulation Experiment Design via the Likelihood Ratio\n  Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In nested simulation literature, a common assumption is that the experimenter\ncan choose the number of outer scenarios to sample. This paper considers the\ncase when the experimenter is given a fixed set of outer scenarios from an\nexternal entity. We propose a nested simulation experiment design that pools\ninner replications from one scenario to estimate another scenario's conditional\nmean via the likelihood ratio method. Given the outer scenarios, we decide how\nmany inner replications to run at each outer scenario as well as how to pool\nthe inner replications by solving a bi-level optimization problem that\nminimizes the total simulation effort. We provide asymptotic analyses on the\nconvergence rates of the performance measure estimators computed from the\noptimized experiment design. Under some assumptions, the optimized design\nachieves $\\cO(\\Gamma^{-1})$ mean squared error of the estimators given\nsimulation budget $\\Gamma$. Numerical experiments demonstrate that our design\noutperforms a state-of-the-art design that pools replications via regression.\n"
    },
    {
        "paper_id": 2008.13198,
        "authors": "Th\\'eo Roncalli, Th\\'eo Le Guenedal, Fr\\'ed\\'eric Lepetit, Thierry\n  Roncalli, Takaya Sekine",
        "title": "Measuring and Managing Carbon Risk in Investment Portfolios",
        "comments": "58 pages, 32 figures, 11 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article studies the impact of carbon risk on stock pricing. To address\nthis, we consider the seminal approach of G\\\"orgen \\textsl{et al.} (2019), who\nproposed estimating the carbon financial risk of equities by their carbon beta.\nTo achieve this, the primary task is to develop a brown-minus-green (or BMG)\nrisk factor, similar to Fama and French (1992). Secondly, we must estimate the\ncarbon beta using a multi-factor model. While G\\\"orgen \\textsl{et al.} (2019)\nconsidered that the carbon beta is constant, we propose a time-varying\nestimation model to assess the dynamics of the carbon risk. Moreover, we test\nseveral specifications of the BMG factor to understand which climate\nchange-related dimensions are priced in by the stock market. In the second part\nof the article, we focus on the carbon risk management of investment\nportfolios. First, we analyze how carbon risk impacts the construction of a\nminimum variance portfolio. As the goal of this portfolio is to reduce\nunrewarded financial risks of an investment, incorporating the carbon risk into\nthis approach fulfils this objective. Second, we propose a new framework for\nbuilding enhanced index portfolios with a lower exposure to carbon risk than\ncapitalization-weighted stock indices. Finally, we explore how carbon\nsensitivities can improve the robustness of factor investing portfolios.\n"
    },
    {
        "paper_id": 2008.1323,
        "authors": "Mikhail Zhitlukhin",
        "title": "A continuous-time asset market game with short-lived assets",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a continuous-time game-theoretic model of an investment market\nwith short-lived assets and endogenous asset prices. The first goal of the\npaper is to formulate a stochastic equation which determines wealth processes\nof investors and to provide conditions for the existence of its solution. The\nsecond goal is to show that there exists a strategy such that the logarithm of\nthe relative wealth of an investor who uses it is a submartingale regardless of\nthe strategies of the other investors, and the relative wealth of any other\nessentially different strategy vanishes asymptotically. This strategy can be\nconsidered as an optimal growth portfolio in the model.\n"
    },
    {
        "paper_id": 2008.13309,
        "authors": "Jian Wu, William B. Haskell, Wenjie Huang, and Huifu Xu",
        "title": "Preference Robust Optimization with Quasi-Concave Choice Functions for\n  Multi-Attribute Prospects",
        "comments": "59 pages, 6 figures, submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Preference robust choice models concern decision-making problems where the\ndecision maker's (DM) utility/risk preferences are ambiguous and the evaluation\nis based on the worst-case utility function/risk measure from a set of\nplausible utility functions/risk measures. The current preference robust choice\nmodels are mostly built upon von Neumann-Morgenstern expected utility theory,\nthe theory of convex risk measures, or Yaari's dual theory of choice, all of\nwhich assume the DM's preferences satisfy some specified axioms. In this paper,\nwe extend the preference robust approach to a broader class of choice functions\nwhich satisfy monotonicity and quasi-concavity in the space of multi-attribute\nrandom prospects. While our new model is non-parametric and significantly\nextends the coverage of decision-making problems, it also brings new\ncomputational challenges due to the non-convexity of the optimization\nformulations, which arises from the non-concavity of the class of quasi-concave\nchoice functions. To tackle these challenges, we develop a sorting-based\nalgorithm that efficiently evaluates the robust choice function (RCF) by\nsolving a sequence of linear programming problems. Then, we show how to solve\npreference robust optimization (PRO) problems by solving a sequence of convex\noptimization problems. We test our robust choice model and computational scheme\non a single-attribute portfolio optimization problem and a multi-attribute\ncapital allocation problem.\n"
    },
    {
        "paper_id": 2008.13472,
        "authors": "Boubekeur Baba and Guven Sevil",
        "title": "The behavior of stock market prices throughout the episodes of capital\n  inflows",
        "comments": "26 pages, 4 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study aims to investigate the behavior of stock prices throughout the\nepisodes of foreign capital flows using data of daily stock prices and\nquarterly foreign capital flows from 14 EMEs. To this end, the episodes of\ncapital flows are identified using the threshold and the k-means clustering\napproaches. Next, the stock index changepoints are detected using the Pruned\nExact Linear Time (PELT) method. Finally, we combine the results by\ndistributing the detected changepoints over the identified capital flows. The\nresults reveal that the stock indices have been rarely pushed further during\nthe entire surge episodes identified by both approaches, and thus surges of\ncapital flows do not necessarily lead to further appreciation of stock prices.\nIn the meantime, a significant appreciation of stock prices is observed during\nthe normal state of capital flows. On the other hand, it is noticed that the\nstock prices have not often depreciated during the episodes of foreign capital\noutflows in all the selected EMEs, which means that stock prices have been less\nvulnerable to reversals of foreign capital flows\n"
    },
    {
        "paper_id": 2008.13561,
        "authors": "Zhen Zhu, Enzo Weber, Till Strohsal, Duaa Serhan",
        "title": "Sustainable Border Control Policy in the COVID-19 Pandemic: A Math\n  Modeling Study",
        "comments": "10 pages, 3 figures and 1 table. A condensed and modified version.\n  Improved writing, no major results changed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Imported COVID-19 cases, if unchecked, can jeopardize the effort of domestic\ncontainment. We aim to find out what sustainable border control options for\ndifferent entities (e.g., countries, states) exist during the reopening phases,\ngiven their own choice of domestic control measures and new technologies such\nas contact tracing. We propose a SUIHR model, which represents an extension to\nthe discrete time SIR models. The model focuses on studying the spreading of\nvirus predominantly by asymptomatic and pre-symptomatic patients. Imported risk\nand (1-tier) contact tracing are both built into the model. Under plausible\nparameter assumptions, we seek sustainable border control policies, in\ncombination with sufficient internal measures, which allow entities to confine\nthe virus without the need to revert back to more restrictive life styles or to\nrely on herd immunity. When the base reproduction number of COVID-19 exceeds\n2.5, even 100% effective contact tracing alone is not enough to contain the\nspreading. For an entity that has completely eliminated the virus domestically,\nand resumes \"normal\", very strict pre-departure screening and test and\nisolation upon arrival combined with effective contact tracing can only delay\nanother outbreak by 6 months. However, if the total net imported cases are\nnon-increasing, and the entity employs a confining domestic control policy,\nthen the total new cases can be contained even without border control.\n"
    },
    {
        "paper_id": 2008.13726,
        "authors": "Bakhtmina Zia, Dr Muhammad Rafiq PhD Research Scholar, Institute of\n  Management Sciences, Peshawar, Pakistan, Associate Professor, Institute of\n  Management Sciences, Peshawar, Pakistan",
        "title": "Reviewing climate change and agricultural market competitiveness",
        "comments": "Paper from my PhD thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is a collection of knowledge regarding the phenomenon of climate\nchange, competitiveness, and literature linking the two phenomena to\nagricultural market competitiveness. The objective is to investigate the peer\nreviewed and grey literature on the subject to explore the link between climate\nchange and agricultural market competitiveness and also explore an appropriate\ntechnique to validate the presumed relationship empirically. The paper\nconcludes by identifying implications for developing an agricultural\ncompetitiveness index while incorporating the climate change impacts, to\nenhance the potential of agricultural markets for optimizing the agricultural\nsectors competitiveness.\n"
    },
    {
        "paper_id": 2009.00062,
        "authors": "Giovanni Calice, Carlo Sala, Daniele Tantari",
        "title": "Contingent Convertible Bonds in Financial Networks",
        "comments": "16 pages, 5 figures",
        "journal-ref": "Scientific Reports 13.1 (2023): 22337",
        "doi": "10.1038/s41598-023-48228-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the role of contingent convertible bonds (CoCos) in a complex\nnetwork of interconnected banks. By studying the system's phase transitions, we\nreveal that the structure of the interbank network is of fundamental importance\nfor the effectiveness of CoCos as a financial stability enhancing mechanism.\nOur results show that, under some network structures, the presence of CoCos can\nincrease (and not reduce) financial fragility, because of the occurring of\nunneeded triggers and consequential suboptimal conversions that damage CoCos\ninvestors. We also demonstrate that, in the presence of a moderate financial\nshock, lightly interconnected financial networks are more robust than highly\ninterconnected networks. This makes them a potentially optimal choice for both\nCoCos issuers and buyers.\n"
    },
    {
        "paper_id": 2009.0036,
        "authors": "Will Hicks",
        "title": "Pseudo-Hermiticity, Martingale Processes and Non-Arbitrage Pricing",
        "comments": "18 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial models based on the Wick product, and White Noise formalism have\npreviously been suggested in order to incorporate integrals with respect to\nfractional Brownian motion. It has also been pointed out that this leads\nnaturally to a quantum mechanical interpretation of the financial market. In\nthis article we pursue this idea further, and in particular show how the\nframework of quantum probability can be used to construct Martingales, without\nrelying on Brownian integrals. We go on to suggest benefits of doing so, and\navenues for future work.\n"
    },
    {
        "paper_id": 2009.00368,
        "authors": "Claudio Albanese, Stephane Crepey, Rodney Hoskinson, and Bouazza\n  Saadeddine",
        "title": "XVA Analysis From the Balance Sheet",
        "comments": "This is an Accepted Manuscript of an article to be published by\n  Taylor & Francis in Quantitative Finance, which will be available online\n  (once published) at: https://doi.org/10.1080/14697688.2020.1817533",
        "journal-ref": null,
        "doi": "10.1080/14697688.2020.1817533",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  XVAs denote various counterparty risk related valuation adjustments that are\napplied to financial derivatives since the 2007--09 crisis. We root a\ncost-of-capital XVA strategy in a balance sheet perspective which is key in\nidentifying the economic meaning of the XVA terms. Our approach is first\ndetailed in a static setup that is solved explicitly. It is then plugged in the\ndynamic and trade incremental context of a real derivative banking portfolio.\nThe corresponding cost-of-capital XVA strategy ensures to bank shareholders a\nsubmartingale equity process corresponding to a target hurdle rate on their\ncapital at risk, consistently between and throughout deals. Set on a\nforward/backward SDE formulation, this strategy can be solved efficiently using\nGPU computing combined with deep learning regression methods in a whole bank\nbalance sheet context. A numerical case study emphasizes the workability and\nadded value of the ensuing pathwise XVA computations.\n"
    },
    {
        "paper_id": 2009.00544,
        "authors": "Kamwoo Lee and Jeanine Braithwaite",
        "title": "High-Resolution Poverty Maps in Sub-Saharan Africa",
        "comments": "Changed an author's affiliation, updated the narrowing method for DHS\n  clusters leading to slight changes to all validation results",
        "journal-ref": "World Development 159 (2022): 106028",
        "doi": "10.1016/j.worlddev.2022.106028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Up-to-date poverty maps are an important tool for policy makers, but until\nnow, have been prohibitively expensive to produce. We propose a generalizable\nprediction methodology to produce poverty maps at the village level using\ngeospatial data and machine learning algorithms. We tested the proposed method\nfor 25 Sub-Saharan African countries and validated them against survey data.\nThe proposed method can increase the validity of both single country and\ncross-country estimations leading to higher precision in poverty maps of 44\nSub-Saharan African countries than previously available. More importantly, our\ncross-country estimation enables the creation of poverty maps when it is not\npractical or cost-effective to field new national household surveys, as is the\ncase with many low- and middle-income countries.\n"
    },
    {
        "paper_id": 2009.00557,
        "authors": "Fabio Baschetti, Giacomo Bormetti, Silvia Romagnoli, Pietro Rossi",
        "title": "The SINC way: A fast and accurate approach to Fourier pricing",
        "comments": "49 pages, 4 figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to investigate the method outlined by one of us\n(PR) in Cherubini et al. (2009) to compute option prices. We name it the SINC\napproach. While the COS method by Fang and Osterlee (2009) leverages the\nFourier-cosine expansion of truncated densities, the SINC approach builds on\nthe Shannon Sampling Theorem revisited for functions with bounded support. We\nprovide several results which were missing in the early derivation: i) a\nrigorous proof of the convergence of the SINC formula to the correct option\nprice when the support grows and the number of Fourier frequencies increases;\nii) ready to implement formulas for put, Cash-or-Nothing, and Asset-or-Nothing\noptions; iii) a systematic comparison with the COS formula for several\nlog-price models; iv) a numerical challenge against alternative Fast Fourier\nspecifications, such as Carr and Madan (1999) and Lewis (2000); v) an extensive\npricing exercise under the rough Heston model of Jaisson and Rosenbaum (2015);\nvi) formulas to evaluate numerically the moments of a truncated density. The\nadvantages of the SINC approach are numerous. When compared to benchmark\nmethodologies, SINC provides the most accurate and fast pricing computation.\nThe method naturally lends itself to price all options in a smile concurrently\nby means of Fast Fourier techniques, boosting fast calibration. Pricing\nrequires to resort only to odd moments in the Fourier space. A previous version\nof this manuscript circulated with the title `Rough Heston: The SINC way'.\n"
    },
    {
        "paper_id": 2009.00868,
        "authors": "Masahiko Egami, Rusudan Kevkhishvili",
        "title": "Post-Last Exit Time Process and its Application to Loss-Given-Default\n  Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a linear diffusion process after its last exit time from a certain\nregular point. Rather than treating the process as newly born at the last exit\ntime, we view the whole path and separate the original process before and after\nthe last exit time. This enables us not only to identify the transition\nsemigroup, boundary behavior, entrance law, and reverse of the post-last exit\ntime process, but also to establish a financial model for estimating the\nloss-given-default distribution of corporate debt (an all-time important open\nproblem).\n"
    },
    {
        "paper_id": 2009.00907,
        "authors": "Bastien Baldacci, Joffrey Derchu, Iuliia Manziuk",
        "title": "An approximate solution for options market-making in high dimension",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing a book of options on several underlying involves controlling\npositions of several thousands of financial assets. It is one of the most\nchallenging financial problems involving both pricing and microstructural\nmodeling. An options market maker has to manage both long- and short-dated\noptions having very different dynamics. In particular, short-dated options\ninventories cannot be managed as a part of an aggregated inventory, which\nprevents the use of dimensionality reduction techniques such as a factorial\napproach or first-order Greeks approximation. In this paper, we show that a\nsimple analytical approximation of the solution of the market maker's problem\nprovides significantly higher flexibility than the existing algorithms\ndesigning options market making strategies.\n"
    },
    {
        "paper_id": 2009.00972,
        "authors": "Michael Monoyios",
        "title": "Infinite horizon utility maximisation from inter-temporal wealth",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2006.04687",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a duality theory for the problem of maximising expected lifetime\nutility from inter-temporal wealth over an infinite horizon, under the minimal\nno-arbitrage assumption of No Unbounded Profit with Bounded Risk (NUPBR). We\nuse only deflators, with no arguments involving equivalent martingale measures,\nso do not require the stronger condition of No Free Lunch with Vanishing Risk\n(NFLVR). Our formalism also works without alteration for the finite horizon\nversion of the problem. As well as extending work of Bouchard and Pham to any\nhorizon and to a weaker no-arbitrage setting, we obtain a stronger duality\nstatement, because we do not assume by definition that the dual domain is the\npolar set of the primal space. Instead, we adopt a method akin to that used for\ninter-temporal consumption problems, developing a supermartingale property of\nthe deflated wealth and its path that yields an infinite horizon budget\nconstraint and serves to define the correct dual variables. The structure of\nour dual space allows us to show that it is convex, without forcing this\nproperty by assumption. We proceed to enlarge the primal and dual domains to\nconfer solidity to them, and use supermartingale convergence results which\nexploit Fatou convergence, to establish that the enlarged dual domain is the\nbipolar of the original dual space. The resulting duality theorem shows that\nall the classical tenets of convex duality hold. Moreover, at the optimum, the\ndeflated wealth process is a potential converging to zero. We work out\nexamples, including a case with a stock whose market price of risk is a\nthree-dimensional Bessel process, so satisfying NUPBR but not NFLVR.\n"
    },
    {
        "paper_id": 2009.01219,
        "authors": "Christian Bayer and Eric Joseph Hall and Ra\\'ul Tempone",
        "title": "Weak error rates for option pricing under linear rough volatility",
        "comments": "28 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In quantitative finance, modeling the volatility structure of underlying\nassets is vital to pricing options. Rough stochastic volatility models, such as\nthe rough Bergomi model [Bayer, Friz, Gatheral, Quantitative Finance 16(6),\n887-904, 2016], seek to fit observed market data based on the observation that\nthe log-realized variance behaves like a fractional Brownian motion with small\nHurst parameter, $H < 1/2$, over reasonable timescales. Both time series of\nasset prices and option-derived price data indicate that $H$ often takes values\nclose to $0.1$ or less, i.e., rougher than Brownian motion. This change\nimproves the fit to both option prices and time series of underlying asset\nprices while maintaining parsimoniousness. However, the non-Markovian nature of\nthe driving fractional Brownian motion in rough volatility models poses severe\nchallenges for theoretical and numerical analyses and for computational\npractice. While the explicit Euler method is known to converge to the solution\nof the rough Bergomi and similar models, its strong rate of convergence is only\n$H$. We prove rate $H + 1/2$ for the weak convergence of the Euler method for\nthe rough Stein-Stein model, which treats the volatility as a linear function\nof the driving fractional Brownian motion, and, surprisingly, we prove rate one\nfor the case of quadratic payoff functions. Our proof uses Talay-Tubaro\nexpansions and an affine Markovian representation of the underlying and is\nfurther supported by numerical experiments. These convergence results provide a\nfirst step toward deriving weak rates for the rough Bergomi model, which treats\nthe volatility as a nonlinear function of the driving fractional Brownian\nmotion.\n"
    },
    {
        "paper_id": 2009.01276,
        "authors": "Tiziano De Angelis",
        "title": "Stopping spikes, continuation bays and other features of optimal\n  stopping with finite-time horizon",
        "comments": "41 pages, 2 figures; final version accepted for publication in EJP;\n  corrected few typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider optimal stopping problems with finite-time horizon and\nstate-dependent discounting. The underlying process is a one-dimensional linear\ndiffusion and the gain function is time-homogeneous and difference of two\nconvex functions. Under mild technical assumptions with local nature we prove\nfine regularity properties of the optimal stopping boundary including its\ncontinuity and strict monotonicity. The latter was never proven with\nprobabilistic arguments. We also show that atoms in the signed measure\nassociated with the second order spatial derivative of the gain function induce\ngeometric properties of the continuation/stopping set that cannot be observed\nwith smoother gain functions (we call them \\emph{continuation bays} and\n\\emph{stopping spikes}). The value function is continuously differentiable in\ntime without any requirement on the smoothness of the gain function.\n"
    },
    {
        "paper_id": 2009.01317,
        "authors": "Zhiqiang Ma, Grace Bang, Chong Wang, Xiaomo Liu",
        "title": "Towards Earnings Call and Stock Price Movement",
        "comments": "Accepted by KDD 2020 MLF workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Earnings calls are hosted by management of public companies to discuss the\ncompany's financial performance with analysts and investors. Information\ndisclosed during an earnings call is an essential source of data for analysts\nand investors to make investment decisions. Thus, we leverage earnings call\ntranscripts to predict future stock price dynamics. We propose to model the\nlanguage in transcripts using a deep learning framework, where an attention\nmechanism is applied to encode the text data into vectors for the\ndiscriminative network classifier to predict stock price movements. Our\nempirical experiments show that the proposed model is superior to the\ntraditional machine learning baselines and earnings call information can boost\nthe stock price prediction performance.\n"
    },
    {
        "paper_id": 2009.01343,
        "authors": "Abdulnasser Hatemi-J",
        "title": "Bear Markets and Recessions versus Bull Markets and Expansions",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the dynamic interaction between falling and rising\nmarkets for both the real and the financial sectors of the largest economy in\nthe world using asymmetric causality tests. These tests require that each\nunderlying variable in the model be transformed into partial sums of the\npositive and negative components. The positive components represent the rising\nmarkets and the negative components embody the falling markets. The sample\nperiod covers some part of the COVID19 pandemic. Since the data is non normal\nand the volatility is time varying, the bootstrap simulations with leverage\nadjustments are used in order to create reliable critical values when causality\ntests are conducted. The results of the asymmetric causality tests disclose\nthat the bear markets are causing the recessions as well as the bull markets\nare causing the economic expansions. The causal effect of bull markets on\neconomic expansions is higher compared to the causal effect of bear markets on\neconomic recessions. In addition, it is found that economic expansions cause\nbull markets but recessions do not cause bear markets. Thus, the policies that\nremedy the falling financial markets can also help the economy when it is in a\nrecession.\n"
    },
    {
        "paper_id": 2009.0143,
        "authors": "Yonghong An and Pengfei Liu",
        "title": "Eliciting Information from Sensitive Survey Questions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers how to elicit information from sensitive survey\nquestions. First we thoroughly evaluate list experiments (LE), a leading method\nin the experimental literature on sensitive questions. Our empirical results\ndemonstrate that the assumptions required to identify sensitive information in\nLE are violated for the majority of surveys. Next we propose a novel survey\nmethod, called Multiple Response Technique (MRT), for eliciting information\nfrom sensitive questions. We require all of the respondents to answer three\nquestions related to the sensitive information. This technique recovers\nsensitive information at a disaggregated level while still allowing arbitrary\nmisreporting in survey responses. An application of the MRT provides novel\nempirical evidence on sexual orientation and Lesbian, Gay, Bisexual, and\nTransgender (LGBT)-related sentiment.\n"
    },
    {
        "paper_id": 2009.01644,
        "authors": "Stefan Gerhold",
        "title": "A note on large deviations in life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study large and moderate deviations for a life insurance portfolio,\nwithout assuming identically distributed losses. The crucial assumption is that\nlosses are bounded, and that variances are bounded below. From a standard large\ndeviations upper bound, we get an exponential bound for the probability of the\naverage loss exceeding a threshold. A counterexample shows that a full large\ndeviation principle does not follow from our assumptions.\n"
    },
    {
        "paper_id": 2009.01676,
        "authors": "Yongge Wang",
        "title": "Automated Market Makers for Decentralized Finance (DeFi)",
        "comments": "14 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper compares mathematical models for automated market makers including\nlogarithmic market scoring rule (LMSR), liquidity sensitive LMSR (LS-LMSR),\nconstant product/mean/sum, and others. It is shown that though LMSR may not be\na good model for Decentralized Finance (DeFi) applications, LS-LMSR has several\nadvantages over constant product/mean based automated market makers. However,\nLS-LMSR requires complicated computation (i.e., logarithm and exponentiation)\nand the cost function curve is concave. In certain DeFi applications, it is\npreferred to have computationally efficient cost functions with convex curves\nto conform with the principle of supply and demand. This paper proposes and\nanalyzes constant circle/ellipse based cost functions for automated market\nmakers. The proposed cost functions are computationally efficient (only\nrequires multiplication and square root calculation) and have several\nadvantages over widely deployed constant product cost functions. For example,\nthe proposed market makers are more robust against front-runner (slippage)\nattacks.\n"
    },
    {
        "paper_id": 2009.01749,
        "authors": "Craig McIntosh and Andrew Zeitlin",
        "title": "Using Household Grants to Benchmark the Cost Effectiveness of a USAID\n  Workforce Readiness Program",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a randomized experiment to compare a workforce training program to\ncash transfers in Rwanda. Conducted in a sample of poor and underemployed\nyouth, this study measures the impact of the training program not only relative\nto a control group but relative to the counterfactual of simply disbursing the\ncost of the program directly to beneficiaries. While the training program was\nsuccessful in improving a number of core outcomes (productive hours, assets,\nsavings, and subjective well-being), cost-equivalent cash transfers move all\nthese outcomes as well as consumption, income, and wealth. In the head-to-head\ncosting comparison cash proves superior across a number of economic outcomes,\nwhile training outperforms cash only in the production of business knowledge.\nWe find little evidence of complementarity between human and physical capital\ninterventions, and no signs of heterogeneity or spillover effects.\n"
    },
    {
        "paper_id": 2009.02566,
        "authors": "George Hong",
        "title": "Skewing Quanto with Simplicity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple and highly efficient analytical method for solving the\nQuanto Skew problem in Equities under a framework that accommodates both Equity\nand FX volatility skew consistently. Ease of implementation and extremely fast\nperformance of this new approach should benefit a wide spectrum of market\nparticipants.\n"
    },
    {
        "paper_id": 2009.02808,
        "authors": "Mouhamad Drame",
        "title": "Limit Order Book (LOB) shape modeling in presence of heterogeneously\n  informed market participants",
        "comments": "28 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The modeling of the limit order book is directly related to the assumptions\non the behavior of real market participants. This paper is twofold. We first\npresent empirical findings that lay the ground for two improvements to these\nmodels.The first one is concerned with market participants by adding the\nadditional dimension of informed market makers, whereas the second, and maybe\nmore original one, addresses the race in the book between informed traders and\ninformed market makers leading to different shapes of the order book.\n  Namely we build an agent-based model for the order book with four types of\nmarket participants: informed trader, noise trader, informed market makers and\nnoise market makers. We build our model based on the Glosten-Milgrom approach\nand the most recent Huang-Rosenbaum-Saliba approach. We introduce a parameter\ncapturing the race between informed liquidity traders and suppliers after a new\ninformation on the fundamental value of the asset. We then derive the whole\n'static' limit order book and its characteristics -- namely the bid-ask spread\nand volumes available at each level price -- from the interactions between the\nagents and compare it with the pre-existing model. We then discuss the case\nwhere noise traders have an impact on the fundamental value of the asset and\nextend the model to take into account many kinds of informed market makers.\n"
    },
    {
        "paper_id": 2009.02853,
        "authors": "Parag A. Pathak, Harald Schmidt, Adam Solomon, Edwin Song, Tayfun\n  S\\\"onmez, M. Utku \\\"Unver",
        "title": "Do Black and Indigenous Communities Receive their Fair Share of Vaccines\n  Under the 2018 CDC Guidelines",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major focus of debate about rationing guidelines for COVID-19 vaccines is\nwhether and how to prioritize access for minority populations that have been\nparticularly affected by the pandemic, and been the subject of historical and\nstructural disadvantage, particularly Black and Indigenous individuals. We\nsimulate the 2018 CDC Vaccine Allocation guidelines using data from the\nAmerican Community Survey under different assumptions on total vaccine supply.\nBlack and Indigenous individuals combined receive a higher share of vaccines\ncompared to their population share for all assumptions on total vaccine supply.\nHowever, their vaccine share under the 2018 CDC guidelines is considerably\nlower than their share of COVID-19 deaths and age-adjusted deaths. We then\nsimulate one method to incorporate disadvantage in vaccine allocation via a\nreserve system. In a reserve system, units are placed into categories and units\nreserved for a category give preferential treatment to individuals from that\ncategory. Using the Area Deprivation Index (ADI) as a proxy for disadvantage,\nwe show that a 40% high-ADI reserve increases the number of vaccines allocated\nto Black or Indigenous individuals, with a share that approaches their COVID-19\ndeath share when there are about 75 million units. Our findings illustrate that\nwhether an allocation is equitable depends crucially on the benchmark and\nhighlight the importance of considering the expected distribution of outcomes\nfrom implementing vaccine allocation guidelines.\n"
    },
    {
        "paper_id": 2009.02904,
        "authors": "Bony Josaphat and Khreshna Syuhada",
        "title": "Dependent Conditional Value-at-Risk for Aggregate Risk Models",
        "comments": "14 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk measure forecast and model have been developed in order to not only\nprovide better forecast but also preserve its (empirical) property especially\ncoherent property. Whilst the widely used risk measure of Value-at-Risk (VaR)\nhas shown its performance and benefit in many applications, it is in fact not a\ncoherent risk measure. Conditional VaR (CoVaR), defined as mean of losses\nbeyond VaR, is one of alternative risk measures that satisfies coherent\nproperty. There has been several extensions of CoVaR such as Modified CoVaR\n(MCoVaR) and Copula CoVaR (CCoVaR). In this paper, we propose another risk\nmeasure, called Dependent CoVaR (DCoVaR), for a target loss that depends on\nanother random loss, including model parameter treated as random loss. It is\nfound that our DCoVaR outperforms than both MCoVaR and CCoVaR. Numerical\nsimulation is carried out to illustrate the proposed DCoVaR. In addition, we do\nan empirical study of financial returns data to compute the DCoVaR forecast for\nheteroscedastic process.\n"
    },
    {
        "paper_id": 2009.03094,
        "authors": "Zhengxin Joseph Ye and Bjorn W. Schuller",
        "title": "Capturing dynamics of post-earnings-announcement drift using genetic\n  algorithm-optimised supervised learning",
        "comments": "13 pages of main article plus 6 pages of data in appendix. 7 figures\n  and 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While Post-Earnings-Announcement Drift (PEAD) is one of the most studied\nstock market anomalies, the current literature is often limited in explaining\nthis phenomenon by a small number of factors using simpler regression methods.\nIn this paper, we use a machine learning based approach instead, and aim to\ncapture the PEAD dynamics using data from a large group of stocks and a wide\nrange of both fundamental and technical factors. Our model is built around the\nExtreme Gradient Boosting (XGBoost) and uses a long list of engineered input\nfeatures based on quarterly financial announcement data from 1,106 companies in\nthe Russell 1000 index between 1997 and 2018. We perform numerous experiments\non PEAD predictions and analysis and have the following contributions to the\nliterature. First, we show how Post-Earnings-Announcement Drift can be analysed\nusing machine learning methods and demonstrate such methods' prowess in\nproducing credible forecasting on the drift direction. It is the first time\nPEAD dynamics are studied using XGBoost. We show that the drift direction is in\nfact driven by different factors for stocks from different industrial sectors\nand in different quarters and XGBoost is effective in understanding the\nchanging drivers. Second, we show that an XGBoost well optimised by a Genetic\nAlgorithm can help allocate out-of-sample stocks to form portfolios with higher\npositive returns to long and portfolios with lower negative returns to short, a\nfinding that could be adopted in the process of developing market neutral\nstrategies. Third, we show how theoretical event-driven stock strategies have\nto grapple with ever changing market prices in reality, reducing their\neffectiveness. We present a tactic to remedy the difficulty of buying into a\nmoving market when dealing with PEAD signals.\n"
    },
    {
        "paper_id": 2009.0316,
        "authors": "Neeraj Bokde, Bo Tranberg, Gorm Bruun Andresen",
        "title": "A graphical approach to carbon-efficient spot market scheduling for\n  Power-to-X applications",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.enconman.2020.113461",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the Paris agreement of 2015, it was decided to reduce the CO2 emissions of\nthe energy sector to zero by 2050 and to restrict the global mean temperature\nincrease to 1.5 degree Celcius above the pre-industrial level. Such commitments\nare possible only with practically CO2-free power generation based on variable\nrenewable technologies. Historically, the main point of criticism regarding\nrenewable power is the variability driven by weather dependence. Power-to-X\nsystems, which convert excess power to other stores of energy for later use,\ncan play an important role in offsetting the variability of renewable power\nproduction. In order to do so, however, these systems have to be scheduled\nproperly to ensure they are being powered by low-carbon technologies. In this\npaper, we introduce a graphical approach for scheduling power-to-X plants in\nthe day-ahead market by minimizing carbon emissions and electricity costs. This\ngraphical approach is simple to implement and intuitively explain to\nstakeholders. In a simulation study using historical prices and CO2 intensity\nfor four different countries, we find that the price and CO2 intensity tends to\ndecrease with increasing scheduling horizon. The effect diminishes when\nrequiring an increasing amount of full load hours per year. Additionally,\ninvestigating the trade-off between optimizing for price or CO2 intensity shows\nthat it is indeed a trade-off: it is not possible to obtain the lowest price\nand CO2 intensity at the same time.\n"
    },
    {
        "paper_id": 2009.03202,
        "authors": "Shuaiqiang Liu and Lech A. Grzelak and Cornelis W. Oosterlee",
        "title": "The Seven-League Scheme: Deep learning for large time step Monte Carlo\n  simulations of stochastic differential equations",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an accurate data-driven numerical scheme to solve Stochastic\nDifferential Equations (SDEs), by taking large time steps. The SDE\ndiscretization is built up by means of a polynomial chaos expansion method, on\nthe basis of accurately determined stochastic collocation (SC) points. By\nemploying an artificial neural network to learn these SC points, we can perform\nMonte Carlo simulations with large time steps. Error analysis confirms that\nthis data-driven scheme results in accurate SDE solutions in the sense of\nstrong convergence, provided the learning methodology is robust and accurate.\nWith a method variant called the compression-decompression collocation and\ninterpolation technique, we can drastically reduce the number of neural network\nfunctions that have to be learned, so that computational speed is enhanced.\nNumerical experiments confirm a high-quality strong convergence error when\nusing large time steps, and the novel scheme outperforms some classical\nnumerical SDE discretizations. Some applications, here in financial option\nvaluation, are also presented.\n"
    },
    {
        "paper_id": 2009.03239,
        "authors": "Qiao Zhou and Ningning Liu",
        "title": "A Stock Prediction Model Based on DCNN",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The prediction of a stock price has always been a challenging issue, as its\nvolatility can be affected by many factors such as national policies, company\nfinancial reports, industry performance, and investor sentiment etc.. In this\npaper, we present a prediction model based on deep CNN and the candle charts,\nthe continuous time stock information is processed. According to different\ninformation richness, prediction time interval and classification method, the\noriginal data is divided into multiple categories as the training set of CNN.\nIn addition, the convolutional neural network is used to predict the stock\nmarket and analyze the difference in accuracy under different classification\nmethods.\n  The results show that the method has the best performance when the forecast\ntime interval is 20 days. Moreover, the Moving Average Convergence Divergence\nand three kinds of moving average are added as input. This method can\naccurately predict the stock trend of the US NDAQ exchange for 92.2%.\nMeanwhile, this article distinguishes three conventional classification methods\nto provide guidance for future research.\n"
    },
    {
        "paper_id": 2009.03362,
        "authors": "Rodrigo Rivera-Castro, Polina Pilyugina, Evgeny Burnaev",
        "title": "Topological Data Analysis for Portfolio Management of Cryptocurrencies",
        "comments": null,
        "journal-ref": "2019 International Conference on Data Mining Workshops (ICDMW)",
        "doi": "10.1109/ICDMW.2019.00044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management is essential for any investment decision. Yet,\ntraditional methods in the literature are ill-suited for the characteristics\nand dynamics of cryptocurrencies. This work presents a method to build an\ninvestment portfolio consisting of more than 1500 cryptocurrencies covering 6\nyears of market data. It is centred around Topological Data Analysis (TDA), a\nrecent approach to analyze data sets from the perspective of their topological\nstructure. This publication proposes a system combining persistence landscapes\nto identify suitable investment opportunities in cryptocurrencies. Using a\nnovel and comprehensive data set of cryptocurrency prices, this research shows\nthat the proposed system enables analysts to outperform a classic method from\nthe literature without requiring any feature engineering or domain knowledge in\nTDA. This work thus introduces TDA-based portfolio management of\ncryptocurrencies as a viable tool for the practitioner.\n"
    },
    {
        "paper_id": 2009.03379,
        "authors": "Roy Allen and John Rehbeck",
        "title": "Counterfactual and Welfare Analysis with an Approximate Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a conceptual framework for counterfactual and welfare analysis for\napproximate models. Our key assumption is that model approximation error is the\nsame magnitude at new choices as the observed data. Applying the framework to\nquasilinear utility, we obtain bounds on quantities at new prices using an\napproximate law of demand. We then bound utility differences between bundles\nand welfare differences between prices. All bounds are computable as linear\nprograms. We provide detailed analytical results describing how the data map to\nthe bounds including shape restrictions that provide a foundation for plug-in\nestimation. An application to gasoline demand illustrates the methodology.\n"
    },
    {
        "paper_id": 2009.03394,
        "authors": "Mykola Babiak and Jozef Barunik",
        "title": "Deep Learning, Predictability, and Optimal Portfolio Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study dynamic portfolio choice of a long-horizon investor who uses deep\nlearning methods to predict equity returns when forming optimal portfolios. Our\nresults show statistically and economically significant benefits from using\ndeep learning to form optimal portfolios through certainty equivalent returns\nand Sharpe ratios. We demonstrate that a long-short-term-memory recurrent\nneural network, which excels in learning complex time-series dependencies,\ngenerates a superior performance among a variety of networks considered. Return\npredictability via deep learning generates substantially improved portfolio\nperformance across different subsamples, particularly during recessionary\nperiods. These gains are robust to including transaction costs, short-selling\nand borrowing constraints.\n"
    },
    {
        "paper_id": 2009.03436,
        "authors": "Arthur Hu, Xingwei Hu, Hui Tong",
        "title": "Globalization? Trade War? A Counterbalance Perspective",
        "comments": "49 pages, 4 tables, 6 figures, 4 theorems, and 6 proofs",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The embrace of globalization and protectionism among economies has ebbed and\nflowed over the past few decades. These fluctuations call for quantitative\nanalytics to help countries improve their trade policies. Changing attitudes\nabout globalization also imply that the best trade policies may vary over time\nand be country-specific. We argue that the imports and exports of all economies\nconstitute a counterbalanced network where conflict and cooperation are two\nsides of the same coin. Quantitative competitiveness is then formulated for\neach country using a network counterbalance equilibrium. A country could\nimprove its relative strength in the network by embracing globalization,\nprotectionism, trade collaboration, or conflict. This paper presents the\nnecessary conditions for globalization and trade wars, evaluates their side\neffects, derives national bargaining powers, identifies appropriate targets for\nconflict or collaboration, and recommends fair resolutions for trade conflicts.\nData and events from the past twenty years support these conditions.\n"
    },
    {
        "paper_id": 2009.03653,
        "authors": "Sojung Kim and Stefan Weber",
        "title": "Simulation Methods for Robust Risk Assessment and the Distorted Mix\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Uncertainty requires suitable techniques for risk assessment. Combining\nstochastic approximation and stochastic average approximation, we propose an\nefficient algorithm to compute the worst case average value at risk in the face\nof tail uncertainty. Dependence is modelled by the distorted mix method that\nflexibly assigns different copulas to different regions of multivariate\ndistributions. We illustrate the application of our approach in the context of\nfinancial markets and cyber risk.\n"
    },
    {
        "paper_id": 2009.04037,
        "authors": "Jinjing Li, Yogi Vidyattama, Hai Anh La, Riyana Miranti, Denisa M\n  Sologon",
        "title": "The Impact of COVID-19 and Policy Responses on Australian Income\n  Distribution and Poverty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper undertakes a near real-time analysis of the income distribution\neffects of the COVID-19 crisis in Australia to understand the ongoing changes\nin the income distribution as well as the impact of policy responses. By\nsemi-parametrically combining incomplete observed data from three different\nsources, namely, the Monthly Longitudinal Labour Force Survey, the Survey of\nIncome and Housing and the administrative payroll data, we estimate the impact\nof COVID-19 and the associated policy responses on the Australian income\ndistribution between February and June 2020, covering the immediate periods\nbefore and after the initial outbreak. Our results suggest that despite the\ngrowth in unemployment, the Gini of the equalised disposable income inequality\ndropped by nearly 0.03 point since February. The reduction is because of the\nadditional wage subsidies and welfare supports offered as part of the policy\nresponse, offsetting a potential surge in income inequality. Additionally, the\npoverty rate, which could have been doubled in the absence of the government\nresponse, also reduced by 3 to 4 percentage points. The result shows the\neffectiveness of temporary policy measures in maintaining both the living\nstandards and the level of income inequality. However, the heavy reliance on\nthe support measures raises the possibility that the changes in the income\ndistribution may be reversed and even substantially worsened off should the\nmeasures be withdrawn.\n"
    },
    {
        "paper_id": 2009.04113,
        "authors": "Tomomi Kito, Nagi Moriya, Junichi Yamanoi",
        "title": "Inter-organisational patent opposition network: How companies form\n  adversarial relationships",
        "comments": "16 pages, 11 figures, submitted to The Japanese Economic Review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Much of the research on networks using patent data focuses on citations and\nthe collaboration networks of inventors, hence regarding patents as a positive\nsign of invention. However, patenting is, most importantly, a strategic action\nused by companies to compete with each other. This study sheds light on\ninter-organisational adversarial relationships in patenting for the first time.\nWe constructed and analysed the network of companies connected via patent\nopposition relationships that occurred between 1980 and 2018. A majority of the\ncompanies are directly or indirectly connected to each other and hence form the\nlargest connected component. We found that in the network, many companies\ndisapprove patents in various industrial sectors as well as those owned by\nforeign companies. The network exhibits heavy-tailed, power-law-like degree\ndistribution and assortative mixing, making it an unusual type of topology. We\nfurther investigated the dynamics of the formation of this network by\nconducting a temporal network motif analysis, with patent co-ownership among\nthe companies considered. By regarding opposition as a negative relationship\nand patent co-ownership as a positive relationship, we analysed where\ncollaboration may occur in the opposition network and how such positive\nrelationships would interact with negative relationships. The results\nidentified the structurally imbalanced triadic motifs and the temporal patterns\nof the occurrence of triads formed by a mixture of positive and negative\nrelationships. Our findings suggest that the mechanisms of the emergence of the\ninter-organisational adversarial relationships may differ from those of other\ntypes of negative relationships hence necessitating further research.\n"
    },
    {
        "paper_id": 2009.04144,
        "authors": "Fabio Bellini, Pablo Koch-Medina, Cosimo Munari, Gregor Svindland",
        "title": "Law-invariant functionals that collapse to the mean",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss when law-invariant convex functionals \"collapse to the mean\". More\nprecisely, we show that, in a large class of spaces of random variables and\nunder mild semicontinuity assumptions, the expectation functional is, up to an\naffine transformation, the only law-invariant convex functional that is linear\nalong the direction of a nonconstant random variable with nonzero expectation.\nThis extends results obtained in the literature in a bounded setting and under\nadditional assumptions on the functionals. We illustrate the implications of\nour general results for pricing rules and risk measures.\n"
    },
    {
        "paper_id": 2009.04151,
        "authors": "Cosimo Munari",
        "title": "Multi-utility representations of incomplete preferences induced by\n  set-valued risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a variety of numerical representations of preference relations\ninduced by set-valued risk measures. Because of the general incompleteness of\nsuch preferences, we have to deal with multi-utility representations. We look\nfor representations that are both parsimonious (the family of representing\nfunctionals is indexed by a tractable set of parameters) and well behaved (the\nrepresenting functionals satisfy nice regularity properties with respect to the\nstructure of the underlying space of alternatives). The key to our results is a\ngeneral dual representation of set-valued risk measures that unifies the\nexisting dual representations in the literature and highlights their link with\nduality results for scalar risk measures.\n"
    },
    {
        "paper_id": 2009.042,
        "authors": "Alla A. Petukhina, Raphael C. G. Reule, Wolfgang Karl H\\\"ardle",
        "title": "Rise of the Machines? Intraday High-Frequency Trading Patterns of\n  Cryptocurrencies",
        "comments": null,
        "journal-ref": "The European Journal of Finance, Latest Articles, 2020",
        "doi": "10.1080/1351847X.2020.1789684",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research analyses high-frequency data of the cryptocurrency market in\nregards to intraday trading patterns related to algorithmic trading and its\nimpact on the European cryptocurrency market. We study trading quantitatives\nsuch as returns, traded volumes, volatility periodicity, and provide summary\nstatistics of return correlations to CRIX (CRyptocurrency IndeX), as well as\nrespective overall high-frequency based market statistics with respect to\ntemporal aspects. Our results provide mandatory insight into a market, where\nthe grand scale employment of automated trading algorithms and the extremely\nrapid execution of trades might seem to be a standard based on media reports.\nOur findings on intraday momentum of trading patterns lead to a new\nquantitative view on approaching the predictability of economic value in this\nnew digital market.\n"
    },
    {
        "paper_id": 2009.04408,
        "authors": "Delia Coculescu and Freddy Delbaen",
        "title": "Fairness principles for insurance contracts in the presence of default\n  risk",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the theory of cooperative games for the design of fair insurance\ncontracts. An insurance contract needs to specify the premium to be paid and a\npossible participation in the benefit (or surplus) of the company. It results\nfrom the analysis that when a contract is exposed to the default risk of the\ninsurance company, ex-ante equilibrium considerations require a certain\nparticipation in the benefit of the company to be specified in the contracts.\nThe fair benefit participation of agents appears as an outcome of a game\ninvolving the residual risks induced by the default possibility and using fuzzy\ncoalitions.\n"
    },
    {
        "paper_id": 2009.04461,
        "authors": "Alla Petukhina, Simon Trimborn, Wolfgang Karl H\\\"ardle, Hermann\n  Elendner",
        "title": "Investing with Cryptocurrencies -- evaluating their potential for\n  portfolio allocation strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies (CCs) have risen rapidly in market capitalization over the\nlast years. Despite striking price volatility, their high average returns have\ndrawn attention to CCs as alternative investment assets for portfolio and risk\nmanagement. We investigate the utility gains for different types of investors\nwhen they consider cryptocurrencies as an addition to their portfolio of\ntraditional assets. We consider risk-averse, return-seeking as well as\ndiversificationpreferring investors who trade along different allocation\nfrequencies, namely daily, weekly or monthly. Out-of-sample performance and\ndiversification benefits are studied for the most popular\nportfolio-construction rules, including mean-variance optimization,\nrisk-parity, and maximum-diversification strategies, as well as combined\nstrategies. To account for low liquidity in CC markets, we incorporate\nliquidity constraints via the LIBRO method. Our results show that CCs can\nimprove the risk-return profile of portfolios. In particular, a\nmaximum-diversification strategy (maximizing the Portfolio Diversification\nIndex, PDI) draws appreciably on CCs, and spanning tests clearly indicate that\nCC returns are non-redundant additions to the investment universe. Though our\nanalysis also shows that illiquidity of CCs potentially reverses the results.\n"
    },
    {
        "paper_id": 2009.04514,
        "authors": "Alberto Elices",
        "title": "X-Value adjustments: accounting versus economic management perspectives",
        "comments": "48 pages, 2 figures, to be submitted to Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a mathematical framework based on the principle of\ninvariance to classify institutions in two paradigms according to the way in\nwhich credit, debit and funding adjustments are calculated: accounting and\nmanagement perspectives. This conceptual classification helps to answer\nquestions such as: In which paradigm each institution sits (point of\nsituation)? Where is the market consensus and regulation pointing to (target\npoint)? What are the implications, pros and cons of switching perspective to\nalign with future consensus (design of a transition)? An improved solution of\nthe principle of invariance equations is presented to calculate these metrics\navoiding approximations and irrespective of the discounting curve used in Front\nOffice systems. The perspective is changed by appropriate selection of inputs\nalways using the same calculation engine. A description of balance sheet\nfinancing is presented along with the justification of the funding curves used\nfor both perspectives.\n"
    },
    {
        "paper_id": 2009.04536,
        "authors": "Yan Wang, Xuelei Sherry Ni",
        "title": "Improving Investment Suggestions for Peer-to-Peer (P2P) Lending via\n  Integrating Credit Scoring into Profit Scoring",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3374135.3385272",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the peer-to-peer (P2P) lending market, lenders lend the money to the\nborrowers through a virtual platform and earn the possible profit generated by\nthe interest rate. From the perspective of lenders, they want to maximize the\nprofit while minimizing the risk. Therefore, many studies have used machine\nlearning algorithms to help the lenders identify the \"best\" loans for making\ninvestments. The studies have mainly focused on two categories to guide the\nlenders' investments: one aims at minimizing the risk of investment (i.e., the\ncredit scoring perspective) while the other aims at maximizing the profit\n(i.e., the profit scoring perspective). However, they have all focused on one\ncategory only and there is seldom research trying to integrate the two\ncategories together. Motivated by this, we propose a two-stage framework that\nincorporates the credit information into a profit scoring modeling. We\nconducted the empirical experiment on a real-world P2P lending data from the US\nP2P market and used the Light Gradient Boosting Machine (lightGBM) algorithm in\nthe two-stage framework. Results show that the proposed two-stage method could\nidentify more profitable loans and thereby provide better investment guidance\nto the investors compared to the existing one-stage profit scoring alone\napproach. Therefore, the proposed framework serves as an innovative perspective\nfor making investment decisions in P2P lending.\n"
    },
    {
        "paper_id": 2009.04767,
        "authors": "Guilherme Lichand and Julien Christen",
        "title": "Using Nudges to Prevent Student Dropouts in the Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The impacts of COVID-19 reach far beyond the hundreds of lives lost to the\ndisease; in particular, the pre-existing learning crisis is expected to be\nmagnified during school shutdown. Despite efforts to put distance learning\nstrategies in place, the threat of student dropouts, especially among\nadolescents, looms as a major concern. Are interventions to motivate\nadolescents to stay in school effective amidst the pandemic? Here we show that,\nin Brazil, nudges via text messages to high-school students, to motivate them\nto stay engaged with school activities, substantially reduced dropouts during\nschool shutdown, and greatly increased their motivation to go back to school\nwhen classes resume. While such nudges had been shown to decrease dropouts\nduring normal times, it is surprising that those impacts replicate in the\nabsence of regular classes because their effects are typically mediated by\nteachers (whose effort in the classroom changes in response to the nudges).\nResults show that insights from the science of adolescent psychology can be\nleveraged to shift developmental trajectories at a critical juncture. They also\nqualify those insights: effects increase with exposure and gradually fade out\nonce communication stops, providing novel evidence that motivational\ninterventions work by redirecting adolescents' attention.\n"
    },
    {
        "paper_id": 2009.04786,
        "authors": "Olivier F\\'eron, Peter Tankov, Laura Tinsi",
        "title": "Price formation and optimal trading in intraday electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a tractable equilibrium model for price formation in intraday\nelectricity markets in the presence of intermittent renewable generation. Using\nstochastic control theory, we identify the optimal strategies of agents with\nmarket impact and exhibit the Nash equilibrium in closed form for a finite\nnumber of agents as well as in the asymptotic framework of mean field games.\nOur model reproduces the empirical features of intraday market prices, such as\nincreasing price volatility at the approach of the delivery date and the\ncorrelation between price and renewable infeed forecasts, and relates these\nfeatures with market characteristics like liquidity, number of agents, and\nimbalance penalty.\n"
    },
    {
        "paper_id": 2009.04824,
        "authors": "Antoine Falck, Adam Rej, David Thesmar",
        "title": "Is Factor Momentum More than Stock Momentum?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Yes, but only at short lags. In this paper we investigate the relationship\nbetween factor momentum and stock momentum. Using a sample of 72 factors\ndocumented in the literature, we first replicate earlier findings that factor\nmomentum exists and works both directionally and cross-sectionally. We then ask\nif factor momentum is spanned by stock momentum. A simple spanning test reveals\nthat after controlling for stock momentum and factor exposure, statistically\nsignificant Sharpe ratios only belong to implementations which include the last\nmonth of returns. We conclude this study with a simple theoretical model that\ncaptures these forces: (1) there is stock-level mean reversion at short lags\nand momentum at longer lags, (2) there is stock and factor momentum at all lags\nand (3) there is natural comovement between the PNLs of stock and factor\nmomentums at all horizons.\n"
    },
    {
        "paper_id": 2009.04912,
        "authors": "Joop van de Heijning, Stephan Leitner, Alexandra Rausch",
        "title": "On the Effectiveness of Minisum Approval Voting in an Open Strategy\n  Setting: An Agent-Based Approach",
        "comments": "5 pages, 3 figures; ; added literature review section, expanded and\n  strengthened introduction and conclusion, grammar and formatting",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work researches the impact of including a wider range of participants in\nthe strategy-making process on the performance of organizations which operate\nin either moderately or highly complex environments. Agent-based simulation\ndemonstrates that the increased number of ideas generated from larger and\ndiverse crowds and subsequent preference aggregation lead to rapid discovery of\nhigher peaks in the organization's performance landscape. However, this is not\nthe case when the expansion in the number of participants is small. The results\nconfirm the most frequently mentioned benefit in the Open Strategy literature:\nthe discovery of better performing strategies.\n"
    },
    {
        "paper_id": 2009.04917,
        "authors": "Yong Cai and Grant Goehring",
        "title": "The 2020 Sturgis Motorcycle Rally and COVID-19",
        "comments": "8 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Sturgis Motorcycle Rally that took place from August 7-16 was one of the\nlargest public gatherings since the start of the COVID-19 outbreak. Over\n460,000 visitors from across the United States travelled to Sturgis, South\nDakota to attend the ten day event. Using anonymous cell phone tracking data we\nidentify the home counties of visitors to the rally and examine the impact of\nthe rally on the spread of COVID-19. Our baseline estimate suggests a one\nstandard deviation increase in Sturgis attendance increased COVID-19 case\ngrowth by 1.1pp in the weeks after the rally.\n"
    },
    {
        "paper_id": 2009.04975,
        "authors": "A. Fronzetti Colladon, S. Grassi, F. Ravazzolo, F. Violante",
        "title": "Forecasting financial markets with semantic network analysis in the\n  COVID-19 crisis",
        "comments": null,
        "journal-ref": "Journal of Forecasting 42(5), 1187-1204 (2023)",
        "doi": "10.1002/for.2936",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper uses a new textual data index for predicting stock market data.\nThe index is applied to a large set of news to evaluate the importance of one\nor more general economic-related keywords appearing in the text. The index\nassesses the importance of the economic-related keywords, based on their\nfrequency of use and semantic network position. We apply it to the Italian\npress and construct indices to predict Italian stock and bond market returns\nand volatilities in a recent sample period, including the COVID-19 crisis. The\nevidence shows that the index captures the different phases of financial time\nseries well. Moreover, results indicate strong evidence of predictability for\nbond market data, both returns and volatilities, short and long maturities, and\nstock market volatility.\n"
    },
    {
        "paper_id": 2009.05034,
        "authors": "Thomas Krabichler and Josef Teichmann",
        "title": "Deep Replication of a Runoff Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To the best of our knowledge, the application of deep learning in the field\nof quantitative risk management is still a relatively recent phenomenon. This\narticle presents the key notions of Deep Asset Liability Management (Deep~ALM)\nfor a technological transformation in the management of assets and liabilities\nalong a whole term structure. The approach has a profound impact on a wide\nrange of applications such as optimal decision making for treasurers, optimal\nprocurement of commodities or the optimisation of hydroelectric power plants.\nAs a by-product, intriguing aspects of goal-based investing or Asset Liability\nManagement (ALM) in abstract terms concerning urgent challenges of our society\nare expected alongside. We illustrate the potential of the approach in a\nstylised case.\n"
    },
    {
        "paper_id": 2009.05194,
        "authors": "Wenhao Wang, Jing Meng, Duan Chen, and Wei Cong",
        "title": "Scenario Forecast of Cross-border Electric Interconnection towards\n  Renewables in South America",
        "comments": "In order to add more complete contents and revise some typos.\n  Withdraw for further revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cross-border Electric Interconnection towards renewables is a promising\nsolution for electric sector under the UN 2030 sustainable development goals\nwhich is widely promoted in emerging economies. This paper comprehensively\ninvestigates state of art in renewable resources and cross-border electric\ninterconnection in South America. Based on the raw data collected from typical\ncountries, a long-term scenario forecast methodology is applied to estimate key\nindicators of electric sector in target years, comparing the prospects of\nactive promoting cross-border Interconnections Towards Renewables (ITR)\nscenario with Business as Usual (BAU) scenario in South America region. Key\nindicators including peak load, installed capacity, investment, and generation\ncost are forecasted and comparative analyzed by year 2035 and 2050. The\ncomparative data analysis shows that by promoting cross-border interconnection\ntowards renewables in South America, renewable resources can be highly utilized\nfor energy supply, energy matrix can be optimized balanced, economics can be\nobviously driven and generation cost can be greatly reduced.\n"
    },
    {
        "paper_id": 2009.05274,
        "authors": "Mads Kock Pedersen, Carlos Mauricio Casta\\~no D\\'iaz, Qian Janice\n  Wang, Mario Alejandro Alba-Marrugo, Ali Amidi, Rajiv Vaid Basaiawmoit,\n  Carsten Bergenholtz, Morten H. Christiansen, Miroslav Gajdacz, Ralph Hertwig,\n  Byurakn Ishkhanyan, Kim Klyver, Nicolai Ladegaard, Kim Mathiasen, Christine\n  Parsons, Janet Rafner, Anders Ryom Villadsen, Mikkel Wallentin, Blanka Zana,\n  Jacob Friis Sherson",
        "title": "Measuring Cognitive Abilities in the Wild: Validating a Population-Scale\n  Game-Based Cognitive Assessment",
        "comments": "24 pages, 8 figures, and 4 tables. Supplementary Information:\n  osf.io/pnw5z//",
        "journal-ref": "Cognitive Science 47(6), e13308 (2023)",
        "doi": "10.1111/cogs.13308",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rapid individual cognitive phenotyping holds the potential to revolutionize\ndomains as wide-ranging as personalized learning, employment practices, and\nprecision psychiatry. Going beyond limitations imposed by traditional lab-based\nexperiments, new efforts have been underway towards greater ecological validity\nand participant diversity to capture the full range of individual differences\nin cognitive abilities and behaviors across the general population. Building on\nthis, we developed Skill Lab, a novel game-based tool that simultaneously\nassesses a broad suite of cognitive abilities while providing an engaging\nnarrative. Skill Lab consists of six mini-games as well as 14 established\ncognitive ability tasks. Using a popular citizen science platform (N = 10725),\nwe conducted a comprehensive validation in the wild of a game-based cognitive\nassessment suite. Based on the game and validation task data, we constructed\nreliable models to simultaneously predict eight cognitive abilities based on\nthe users' in-game behavior. Follow-'-up validation tests revealed that the\nmodels can discriminate nuances contained within each separate cognitive\nability as well as capture a shared main factor of generalized cognitive\nability. Our game-based measures are five times faster to complete than the\nequivalent task-based measures and replicate previous findings on the decline\nof certain cognitive abilities with age in our large cross-sectional population\nsample (N = 6369). Taken together, our results demonstrate the feasibility of\nrapid in-the-wild systematic assessment of cognitive abilities as a promising\nfirst step towards population-scale benchmarking and individualized mental\nhealth diagnostics.\n"
    },
    {
        "paper_id": 2009.05455,
        "authors": "Klaus Ackermann, Alexey Chernikov, Nandini Anantharama, Miethy Zaman,\n  Paul A Raschky",
        "title": "Object Recognition for Economic Development from Daytime Satellite\n  Imagery",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reliable data about the stock of physical capital and infrastructure in\ndeveloping countries is typically very scarce. This is particular a problem for\ndata at the subnational level where existing data is often outdated, not\nconsistently measured or coverage is incomplete. Traditional data collection\nmethods are time and labor-intensive costly, which often prohibits developing\ncountries from collecting this type of data. This paper proposes a novel method\nto extract infrastructure features from high-resolution satellite images. We\ncollected high-resolution satellite images for 5 million 1km $\\times$ 1km grid\ncells covering 21 African countries. We contribute to the growing body of\nliterature in this area by training our machine learning algorithm on\nground-truth data. We show that our approach strongly improves the predictive\naccuracy. Our methodology can build the foundation to then predict subnational\nindicators of economic development for areas where this data is either missing\nor unreliable.\n"
    },
    {
        "paper_id": 2009.05498,
        "authors": "Martin Herdegen and Nazem Khan",
        "title": "Mean-$\\rho$ portfolio selection and $\\rho$-arbitrage for coherent risk\n  measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit mean-risk portfolio selection in a one-period financial market\nwhere risk is quantified by a positively homogeneous risk measure $\\rho$. We\nfirst show that under mild assumptions, the set of optimal portfolios for a\nfixed return is nonempty and compact. However, unlike in classical\nmean-variance portfolio selection, it can happen that no efficient portfolios\nexist. We call this situation $\\rho$-arbitrage, and prove that it cannot be\nexcluded -- unless $\\rho$ is as conservative as the worst-case risk measure.\n  After providing a primal characterisation of $\\rho$-arbitrage, we focus our\nattention on coherent risk measures that admit a dual representation and give a\nnecessary and sufficient dual characterisation of $\\rho$-arbitrage. We show\nthat the absence of $\\rho$-arbitrage is intimately linked to the interplay\nbetween the set of equivalent martingale measures (EMMs) for the discounted\nrisky assets and the set of absolutely continuous measures in the dual\nrepresentation of $\\rho$. A special case of our result shows that the market\ndoes not admit $\\rho$-arbitrage for Expected Shortfall at level $\\alpha$ if and\nonly if there exists an EMM $\\mathbb{Q} \\approx \\mathbb{P}$ such that $\\Vert\n\\frac{\\text{d}\\mathbb{Q}}{\\text{d}\\mathbb{P}} \\Vert_\\infty < \\frac{1}{\\alpha}$.\n"
    },
    {
        "paper_id": 2009.05507,
        "authors": "Sudiksha Joshi",
        "title": "Forecasting the Leading Indicator of a Recession: The 10-Year minus\n  3-Month Treasury Yield Spread",
        "comments": "43 pages, 27 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research paper, I have applied various econometric time series and\ntwo machine learning models to forecast the daily data on the yield spread.\nFirst, I decomposed the yield curve into its principal components, then\nsimulated various paths of the yield spread using the Vasicek model. After\nconstructing univariate ARIMA models, and multivariate models such as ARIMAX,\nVAR, and Long Short Term Memory, I calibrated the root mean squared error to\nmeasure how far the results deviate from the current values. Through impulse\nresponse functions, I measured the impact of various shocks on the difference\nyield spread. The results indicate that the parsimonious univariate ARIMA model\noutperforms the richly parameterized VAR method, and the complex LSTM with\nmultivariate data performs equally well as the simple ARIMA model.\n"
    },
    {
        "paper_id": 2009.05508,
        "authors": "Bernadett Aradi, G\\'abor Petneh\\'azi, J\\'ozsef G\\'all",
        "title": "Volatility Forecasting with 1-dimensional CNNs via transfer learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility is a natural risk measure in finance as it quantifies the\nvariation of stock prices. A frequently considered problem in mathematical\nfinance is to forecast different estimates of volatility. What makes it\npromising to use deep learning methods for the prediction of volatility is the\nfact, that stock price returns satisfy some common properties, referred to as\n`stylized facts'. Also, the amount of data used can be high, favoring the\napplication of neural networks. We used 10 years of daily prices for hundreds\nof frequently traded stocks, and compared different CNN architectures: some\nnetworks use only the considered stock, but we tried out a construction which,\nfor training, uses much more series, but not the considered stocks.\nEssentially, this is an application of transfer learning, and its performance\nturns out to be much better in terms of prediction error. We also compare our\ndilated causal CNNs to the classical ARIMA method using an automatic model\nselection procedure.\n"
    },
    {
        "paper_id": 2009.05636,
        "authors": "Jason Wittenbach, Brian d'Alessandro, C. Bayan Bruss",
        "title": "Machine Learning for Temporal Data in Finance: Challenges and\n  Opportunities",
        "comments": "KDD '20 ML in Finance Workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Temporal data are ubiquitous in the financial services (FS) industry --\ntraditional data like economic indicators, operational data such as bank\naccount transactions, and modern data sources like website clickstreams -- all\nof these occur as a time-indexed sequence. But machine learning efforts in FS\noften fail to account for the temporal richness of these data, even in cases\nwhere domain knowledge suggests that the precise temporal patterns between\nevents should contain valuable information. At best, such data are often\ntreated as uniform time series, where there is a sequence but no sense of exact\ntiming. At worst, rough aggregate features are computed over a pre-selected\nwindow so that static sample-based approaches can be applied (e.g. number of\nopen lines of credit in the previous year or maximum credit utilization over\nthe previous month). Such approaches are at odds with the deep learning\nparadigm which advocates for building models that act directly on raw or\nlightly processed data and for leveraging modern optimization techniques to\ndiscover optimal feature transformations en route to solving the modeling task\nat hand. Furthermore, a full picture of the entity being modeled (customer,\ncompany, etc.) might only be attainable by examining multiple data streams that\nunfold across potentially vastly different time scales. In this paper, we\nexamine the different types of temporal data found in common FS use cases,\nreview the current machine learning approaches in this area, and finally assess\nchallenges and opportunities for researchers working at the intersection of\nmachine learning for temporal data and applications in FS.\n"
    },
    {
        "paper_id": 2009.05652,
        "authors": "M. Bel\\'en Arouxet, Aurelio F. Bariviera, Ver\\'onica E. Pastor,\n  Victoria Vampa",
        "title": "Covid-19 impact on cryptocurrencies: evidence from a wavelet-based Hurst\n  exponent",
        "comments": "4 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrency history begins in 2008 as a means of payment proposal.\nHowever, cryptocurrencies evolved into complex, high yield speculative assets.\nContrary to traditional financial instruments, they are not (mostly) traded in\norganized, law-abiding venues, but on online platforms, where anonymity reigns.\nThis paper examines the long term memory in return and volatility, using high\nfrequency time series of eleven important coins. Our study covers the\npre-Covid-19 and the subsequent pandemia period. We use a recently developed\nmethod, based on the wavelet transform, which provides more robust estimators\nof the Hurst exponent. We detect that, during the peak of Covid-19 pandemic\n(around March 2020), the long memory of returns was only mildly affected.\nHowever, volatility suffered a temporary impact in its long range correlation\nstructure. Our results could be of interest for both academics and\npractitioners.\n"
    },
    {
        "paper_id": 2009.05771,
        "authors": "Olga G. Lebedinskaya",
        "title": "Application of a system of indicatirs for assessing the socio-economic\n  situation of a subject based on digital shadows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The development of Digital Economy sets its own requirements for the\nformation and development of so-called digital doubles and digital shadows of\nreal objects (subjects/regions). An integral element of their development and\napplication is a multi-level matrix of targets and resource constraints (time,\nfinancial, technological, production, etc.). The volume of statistical\ninformation collected for a digital double must meet several criteria: be\nobjective, characterize the real state of the managed object as accurately as\npossible, contain all the necessary information on all managed parameters, and\nat the same time avoid unnecessary and duplicate indicators (\"information\ngarbage\"). The relevance of forming the profile of the \"digital shadow of the\nregion\" in the context of multitasking and conflict of departmental and Federal\nstatistics predetermined the goal of the work-to form a system of indicators of\nthe socio-economic situation of regions based on the harmonization of\ninformation resources. In this study, an inventory of the composition of\nindicators of statistical forms for their relevance and relevance was carried\nout on the example of assessing the economic health of the subject and the\nlevel of provision of banking services\n"
    },
    {
        "paper_id": 2009.06221,
        "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, Matja\\v{z} Omladi\\v{c}",
        "title": "Spearman's footrule and Gini's gamma: Local bounds for bivariate copulas\n  and the exact region with respect to Blomqvist's beta",
        "comments": "30 pages, 13 figures",
        "journal-ref": null,
        "doi": "10.1016/j.cam.2021.113385",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copulas are becoming an essential tool in analyzing data thus encouraging\ninterest in related questions. In the early stage of exploratory data analysis,\nsay, it is helpful to know local copula bounds with a fixed value of a given\nmeasure of association. These bounds have been computed for Spearman's rho,\nKendall's tau, and Blomqvist's beta. The importance of another two measures of\nassociation, Spearman's footrule and Gini's gamma, has been reconfirmed\nrecently. It is the main purpose of this paper to fill in the gap and present\nthe mentioned local bounds for these two measures as well. It turns out that\nthis is a quite non-trivial endeavor as the bounds are quasi-copulas that are\nnot copulas for certain values of the two measures. We also give relations\nbetween these two measures of association and Blomqvist's beta.\n"
    },
    {
        "paper_id": 2009.0635,
        "authors": "Silvia Bartolucci, Fabio Caccioli, Francesco Caravelli, Pierpaolo Vivo",
        "title": "Upstreamness and downstreamness in input-output analysis from local and\n  aggregate information",
        "comments": "22 pages, 5 figures. Title changed, major restructuring of the paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ranking sectors and countries within global value chains is of paramount\nimportance to estimate risks and forecast growth in large economies. However,\nthis task is often non-trivial due to the lack of complete and accurate\ninformation on the flows of money and goods between sectors and countries,\nwhich are encoded in Input-Output (I-O) tables. In this work, we show that an\naccurate estimation of the role played by sectors and countries in supply chain\nnetworks can be achieved without full knowledge of the I-O tables, but only\nrelying on local and aggregate information, e.g., the total intermediate demand\nper sector. Our method, based on a rank-$1$ approximation to the I-O table,\nshows consistently good performance in reconstructing rankings (i.e.,\nupstreamness and downstreamness measures for countries and sectors) when tested\non empirical data from the World Input-Output Database. Moreover, we connect\nthe accuracy of our approximate framework with the spectral properties of the\nI-O tables, which ordinarily exhibit relatively large spectral gaps. Our\napproach provides a fast and analytical tractable framework to rank\nconstituents of a complex economy without the need of matrix inversions and the\nknowledge of finer intersectorial details.\n"
    },
    {
        "paper_id": 2009.06413,
        "authors": "Falco J. Bargagli-Stoffi, Jan Niederreiter, Massimo Riccaboni",
        "title": "Supervised learning for the prediction of firm dynamics",
        "comments": null,
        "journal-ref": "In: Data Science for Economics and Finance (2021) Springer",
        "doi": "10.1007/978-3-030-66891-4_2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thanks to the increasing availability of granular, yet high-dimensional, firm\nlevel data, machine learning (ML) algorithms have been successfully applied to\naddress multiple research questions related to firm dynamics. Especially\nsupervised learning (SL), the branch of ML dealing with the prediction of\nlabelled outcomes, has been used to better predict firms' performance. In this\ncontribution, we will illustrate a series of SL approaches to be used for\nprediction tasks, relevant at different stages of the company life cycle. The\nstages we will focus on are (i) startup and innovation, (ii) growth and\nperformance of companies, and (iii) firms exit from the market. First, we\nreview SL implementations to predict successful startups and R&D projects.\nNext, we describe how SL tools can be used to analyze company growth and\nperformance. Finally, we review SL applications to better forecast financial\ndistress and company failure. In the concluding Section, we extend the\ndiscussion of SL methods in the light of targeted policies, result\ninterpretability, and causality.\n"
    },
    {
        "paper_id": 2009.0647,
        "authors": "Harry Pei and Bruno Strulovici",
        "title": "Crime Aggregation, Deterrence, and Witness Credibility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a model for the equilibrium frequency of offenses and the\ninformativeness of witness reports when potential offenders can commit multiple\noffenses and witnesses are subject to retaliation risk and idiosyncratic\nreporting preferences. We compare two ways of handling multiple accusations\ndiscussed in legal scholarship: (i) When convictions are based on the\nprobability that the defendant committed at least one, unspecified offense and\nentail a severe punishment, potential offenders induce negative correlation in\nwitnesses' private information, which leads to uninformative reports,\ninformation aggregation failures, and frequent offenses in equilibrium.\nMoreover, lowering the punishment in case of conviction can improve deterrence\nand the informativeness of witnesses' reports. (ii) When accusations are\ntreated separately to adjudicate guilt and conviction entails a severe\npunishment, witness reports are highly informative and offenses are infrequent\nin equilibrium.\n"
    },
    {
        "paper_id": 2009.06521,
        "authors": "Diego Zabaljauregui",
        "title": "Optimal market making under partial information and numerical methods\n  for impulse control games with applications",
        "comments": "PhD Thesis, 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The topics treated in this thesis are inherently two-fold. The first part\nconsiders the problem of a market maker optimally setting bid/ask quotes over a\nfinite time horizon, to maximize her expected utility. The intensities of the\norders she receives depend not only on the spreads she quotes, but also on\nunobservable factors modelled by a hidden Markov chain. This stochastic control\nproblem under partial information is solved by means of stochastic filtering,\ncontrol and PDMPs theory. The value function is characterized as the unique\ncontinuous viscosity solution of its dynamic programming equation and\nnumerically compared with its full information counterpart. The optimal full\ninformation spreads are shown to be biased when the exact market regime is\nunknown, as the market maker needs to adjust for additional regime uncertainty\nin terms of PnL sensitivity and observable order flow volatility.\n  The second part deals with numerically solving nonzero-sum stochastic impulse\ncontrol games. These offer a realistic and far-reaching modelling framework,\nbut the difficulty in solving such problems has hindered their proliferation. A\npolicy-iteration-type solver is proposed to solve an underlying system of\nquasi-variational inequalities, and it is validated numerically with reassuring\nresults.\n  Eventually, the focus is put on games with a symmetric structure and an\nimproved algorithm is put forward. A rigorous convergence analysis is\nundertaken with natural assumptions on the players strategies, which admit\ngraph-theoretic interpretations in the context of weakly chained diagonally\ndominant matrices. The algorithm is used to compute with high precision\nequilibrium payoffs and Nash equilibria of otherwise too challenging problems,\nand even some for which results go beyond the scope of the currently available\ntheory.\n"
    },
    {
        "paper_id": 2009.06874,
        "authors": "Tetsuya Takaishi",
        "title": "Recent scaling properties of Bitcoin price returns",
        "comments": "6 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-6596/1730/1/012124",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While relevant stylized facts are observed for Bitcoin markets, we find a\ndistinct property for the scaling behavior of the cumulative return\ndistribution. For various assets, the tail index $\\mu$ of the cumulative return\ndistribution exhibits $\\mu \\approx 3$, which is referred to as \"the inverse\ncubic law.\" On the other hand, that of the Bitcoin return is claimed to be $\\mu\n\\approx 2$, which is known as \"the inverse square law.\" We investigate the\nscaling properties using recent Bitcoin data and find that the tail index\nchanges to $\\mu \\approx 3$, which is consistent with the inverse cubic law.\nThis suggests that some properties of the Bitcoin market could vary over time.\nWe also investigate the autocorrelation of absolute returns and find that it is\ndescribed by a power-law with two scaling exponents. By analyzing the absolute\nreturns standardized by the realized volatility, we verify that the Bitcoin\nreturn time series is consistent with normal random variables with time-varying\nvolatility.\n"
    },
    {
        "paper_id": 2009.06894,
        "authors": "Hiroyasu Inoue, Yohsuke Murase, Yasuyuki Todo",
        "title": "Do economic effects of the anti-COVID-19 lockdowns in different regions\n  interact through supply chains?",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0255031",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To prevent the spread of COVID-19, many cities, states, and countries have\n`locked down', restricting economic activities in non-essential sectors. Such\nlockdowns have substantially shrunk production in most countries. This study\nexamines how the economic effects of lockdowns in different regions interact\nthrough supply chains, a network of firms for production, simulating an\nagent-based model of production on supply-chain data for 1.6 million firms in\nJapan. We further investigate how the complex network structure affects the\ninteractions of lockdowns, emphasising the role of upstreamness and loops by\ndecomposing supply-chain flows into potential and circular flow components. We\nfind that a region's upstreamness, intensity of loops, and supplier\nsubstitutability in supply chains with other regions largely determine the\neconomic effect of the lockdown in the region. In particular, when a region\nlifts its lockdown, its economic recovery substantially varies depending on\nwhether it lifts lockdown alone or together with another region closely linked\nthrough supply chains. These results propose the need for inter-region policy\ncoordination to reduce the economic loss from lockdowns.\n"
    },
    {
        "paper_id": 2009.06905,
        "authors": "Michael Rollins, Dave Cliff",
        "title": "Which Trading Agent is Best? Using a Threaded Parallel Simulation of a\n  Financial Market Changes the Pecking-Order",
        "comments": "6 pages, 2 tables, 3 figures, to be presented at European Modelling\n  and Simulation Symposium (EMSS2020)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents novel results generated from a new simulation model of a\ncontemporary financial market, that cast serious doubt on the previously widely\naccepted view of the relative performance of various well-known public-domain\nautomated-trading algorithms. Various public-domain trading algorithms have\nbeen proposed over the past 25 years in a kind of arms-race, where each new\ntrading algorithm was compared to the previous best, thereby establishing a\n\"pecking order\", i.e. a partially-ordered dominance hierarchy from best to\nworst of the various trading algorithms. Many of these algorithms were\ndeveloped and tested using simple minimal simulations of financial markets that\nonly weakly approximated the fact that real markets involve many different\ntrading systems operating asynchronously and in parallel. In this paper we use\nBSE, a public-domain market simulator, to run a set of experiments generating\nbenchmark results from several well-known trading algorithms. BSE incorporates\na very simple time-sliced approach to simulating parallelism, which has obvious\nknown weaknesses. We then alter and extend BSE to make it threaded, so that\ndifferent trader algorithms operate asynchronously and in parallel: we call\nthis simulator Threaded-BSE (TBSE). We then re-run the trader experiments on\nTBSE and compare the TBSE results to our earlier benchmark results from BSE.\nOur comparison shows that the dominance hierarchy in our more realistic\nexperiments is different from the one given by the original simple simulator.\nWe conclude that simulated parallelism matters a lot, and that earlier results\nfrom simple simulations comparing different trader algorithms are no longer to\nbe entirely trusted.\n"
    },
    {
        "paper_id": 2009.0691,
        "authors": "Marius Lux, Wolfgang Karl H\\\"ardle, Stefan Lessmann",
        "title": "Data driven value-at-risk forecasting using a SVR-GARCH-KDE hybrid",
        "comments": null,
        "journal-ref": "Computational Statistics (2020)",
        "doi": "10.1007/s00180-019-00934-7",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Appropriate risk management is crucial to ensure the competitiveness of\nfinancial institutions and the stability of the economy. One widely used\nfinancial risk measure is Value-at-Risk (VaR). VaR estimates based on linear\nand parametric models can lead to biased results or even underestimation of\nrisk due to time varying volatility, skewness and leptokurtosis of financial\nreturn series. The paper proposes a nonlinear and nonparametric framework to\nforecast VaR that is motivated by overcoming the disadvantages of parametric\nmodels with a purely data driven approach. Mean and volatility are modeled via\nsupport vector regression (SVR) where the volatility model is motivated by the\nstandard generalized autoregressive conditional heteroscedasticity (GARCH)\nformulation. Based on this, VaR is derived by applying kernel density\nestimation (KDE). This approach allows for flexible tail shapes of the profit\nand loss distribution, adapts for a wide class of tail events and is able to\ncapture complex structures regarding mean and volatility.\n  The SVR-GARCH-KDE hybrid is compared to standard, exponential and threshold\nGARCH models coupled with different error distributions. To examine the\nperformance in different markets, one-day-ahead and ten-days-ahead forecasts\nare produced for different financial indices. Model evaluation using a\nlikelihood ratio based test framework for interval forecasts and a test for\nsuperior predictive ability indicates that the SVR-GARCH-KDE hybrid performs\ncompetitive to benchmark models and reduces potential losses especially for\nten-days-ahead forecasts significantly. Especially models that are coupled with\na normal distribution are systematically outperformed.\n"
    },
    {
        "paper_id": 2009.06914,
        "authors": "Benjamin Patrick Evans, Kirill Glavatskiy, Michael S. Harr\\'e, Mikhail\n  Prokopenko",
        "title": "The impact of social influence in Australian real-estate: market\n  forecasting with a spatial agent-based model",
        "comments": "25 pages + 31 page appendix",
        "journal-ref": null,
        "doi": "10.1007/s11403-021-00324-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Housing markets are inherently spatial, yet many existing models fail to\ncapture this spatial dimension. Here we introduce a new graph-based approach\nfor incorporating a spatial component in a large-scale urban housing\nagent-based model (ABM). The model explicitly captures several social and\neconomic factors that influence the agents' decision-making behaviour (such as\nfear of missing out, their trend following aptitude, and the strength of their\nsubmarket outreach), and interprets these factors in spatial terms. The\nproposed model is calibrated and validated with the housing market data for the\nGreater Sydney region. The ABM simulation results not only include predictions\nfor the overall market, but also produce area-specific forecasting at the level\nof local government areas within Sydney as arising from individual buy and sell\ndecisions. In addition, the simulation results elucidate agent preferences in\nsubmarkets, highlighting differences in agent behaviour, for example, between\nfirst-time home buyers and investors, and between both local and overseas\ninvestors.\n"
    },
    {
        "paper_id": 2009.0696,
        "authors": "Leonardo M. Millefiori, Paolo Braca, Dimitris Zissis, Giannis\n  Spiliopoulos, Stefano Marano, Peter K. Willett, Sandro Carniel",
        "title": "COVID-19 Impact on Global Maritime Mobility",
        "comments": null,
        "journal-ref": "Scientific Reports, vol. 11, 18039, 2021",
        "doi": "10.1038/s41598-021-97461-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To prevent the outbreak of the Coronavirus disease (COVID-19), many countries\naround the world went into lockdown and imposed unprecedented containment\nmeasures. These restrictions progressively produced changes to social behavior\nand global mobility patterns, evidently disrupting social and economic\nactivities. Here, using maritime traffic data collected via a global network of\nAIS receivers, we analyze the effects that the COVID-19 pandemic and\ncontainment measures had on the shipping industry, which accounts alone for\nmore than 80% of the world trade. We rely on multiple data-driven maritime\nmobility indexes to quantitatively assess ship mobility in a given unit of\ntime. The mobility analysis here presented has a worldwide extent and is based\non the computation of: CNM of all ships reporting their position and\nnavigational status via AIS, number of active and idle ships, and fleet average\nspeed. To highlight significant changes in shipping routes and operational\npatterns, we also compute and compare global and local density maps. We compare\n2020 mobility levels to those of previous years assuming that an unchanged\ngrowth rate would have been achieved, if not for COVID-19. Following the\noutbreak, we find an unprecedented drop in maritime mobility, across all\ncategories of commercial shipping. With few exceptions, a generally reduced\nactivity is observable from March to June, when the most severe restrictions\nwere in force. We quantify a variation of mobility between -5.62% and -13.77%\nfor container ships, between +2.28% and -3.32% for dry bulk, between -0.22% and\n-9.27% for wet bulk, and between -19.57% and -42.77% for passenger traffic.\nThis study is unprecedented for the uniqueness and completeness of the employed\ndataset, which comprises a trillion AIS messages broadcast worldwide by 50000\nships, a figure that closely parallels the documented size of the world\nmerchant fleet.\n"
    },
    {
        "paper_id": 2009.07086,
        "authors": "Michael Darlin, Nikolaos Papadis, Leandros Tassiulas",
        "title": "Optimal Bidding Strategy for Maker Auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Maker Protocol is a decentralized finance application that enables\ncollateralized lending. The application uses open-bid, second-price auctions to\ncomplete its loan liquidation process. In this paper, we develop a bidding\nfunction for these auctions, focusing on the costs incurred to participate in\nthe auctions. We then optimize these costs using parameters from historical\nauction data, and compare our optimal bidding prices to the historical auction\nprices. We find that the majority of auctions end at higher prices than our\nrecommended optimal prices, and we propose several theories for these results.\n"
    },
    {
        "paper_id": 2009.07124,
        "authors": "Patrick Reinwald, Stephan Leitner and Friederike Wall",
        "title": "An Agent-Based Model of Delegation Relationships With Hidden-Action: On\n  the Effects of Heterogeneous Memory on Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an agent-based model of delegation relationships between a\nprincipal and an agent, which is based on the standard-hidden action model\nintroduced by Holmstr\\\"om and, by doing so, provide a model which can be used\nto further explore theoretical topics in managerial economics, such as the\nefficiency of incentive mechanisms. We employ the concept of agentization,\ni.e., we systematically transform the standard hidden-action model into an\nagent-based model. Our modeling approach allows for a relaxation of some of the\nrather \"heroic\" assumptions included in the standard hidden-action model,\nwhereby we particularly focus on assumptions related to the (i) availability of\ninformation about the environment and the (ii) principal's and agent's\ncognitive capabilities (with a particular focus on their learning capabilities\nand their memory). Our analysis focuses on how close and how fast the incentive\nscheme, which endogenously emerges from the agent-based model, converges to the\nsolution proposed by the standard hidden-action model. Also, we investigate\nwhether a stable solution can emerge from the agent-based model variant. The\nresults show that in stable environments the emergent result can nearly reach\nthe solution proposed by the standard hidden-action model. Surprisingly, the\nresults indicate that turbulence in the environment leads to stability in\nearlier time periods.\n"
    },
    {
        "paper_id": 2009.07144,
        "authors": "Keisuke Kokubun",
        "title": "What factors have caused Japanese prefectures to attract a larger\n  population influx?",
        "comments": null,
        "journal-ref": "Sustainability, Vol. 14, No. 3 (2022) 1595",
        "doi": "10.3390/su14031595",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regional promotion and centralized correction in Tokyo have long been the\ngoals of the Government of Japan. Furthermore, in the wake of the recent new\ncoronavirus (COVID-19) epidemic, the momentum for rural migration is\nincreasing, to prevent the risk of infection with the help of penetration of\nremote work. However, there is not enough debate about what kind of land will\nattract the population. Therefore, in this paper, we will consider this problem\nby performing correlation analysis and multiple regression analysis with the\ninflow rate and the excess inflow rate of the population as the dependent\nvariables, using recent government statistics for each prefecture. As a result\nof the analysis, in addition to economic factor variables, variables of\nclimatic, amenity, and human factors correlated with the inflow rate, and it\nwas shown that the model has the greatest explanatory power when multiple\nfactors were used in addition to specific factors. Therefore, local prefectures\nare required to take regional promotion measures focusing on not only economic\nfactors but also multifaceted factors to attract the outside population.\n"
    },
    {
        "paper_id": 2009.072,
        "authors": "Eric Benhamou, David Saltiel, Jean-Jacques Ohana, and Jamal Atif",
        "title": "Detecting and adapting to crisis pattern with context based Deep\n  Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep reinforcement learning (DRL) has reached super human levels in complex\ntasks like game solving (Go and autonomous driving). However, it remains an\nopen question whether DRL can reach human level in applications to financial\nproblems and in particular in detecting pattern crisis and consequently\ndis-investing. In this paper, we present an innovative DRL framework consisting\nin two sub-networks fed respectively with portfolio strategies past\nperformances and standard deviations as well as additional contextual features.\nThe second sub network plays an important role as it captures dependencies with\ncommon financial indicators features like risk aversion, economic surprise\nindex and correlations between assets that allows taking into account context\nbased information. We compare different network architectures either using\nlayers of convolutions to reduce network's complexity or LSTM block to capture\ntime dependency and whether previous allocations is important in the modeling.\nWe also use adversarial training to make the final model more robust. Results\non test set show this approach substantially over-performs traditional\nportfolio optimization methods like Markowitz and is able to detect and\nanticipate crisis like the current Covid one.\n"
    },
    {
        "paper_id": 2009.07202,
        "authors": "Joshua Becker, Abdullah Almaatouq, Em\\H{o}ke-\\'Agnes Horv\\'at",
        "title": "Network Structures of Collective Intelligence: The Contingent Benefits\n  of Group Discussion",
        "comments": "27 pages including Appendix preregistration at https://osf.io/9xq2j\n  replication data and code at\n  https://github.com/joshua-a-becker/emergent-network-structure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research on belief formation has produced contradictory findings on whether\nand when communication between group members will improve the accuracy of\nnumeric estimates such as economic forecasts, medical diagnoses, and job\ncandidate assessments. While some evidence suggests that carefully mediated\nprocesses such as the \"Delphi method\" produce more accurate beliefs than\nunstructured discussion, others argue that unstructured discussion outperforms\nmediated processes. Still others argue that independent individuals produce the\nmost accurate beliefs. This paper shows how network theories of belief\nformation can resolve these inconsistencies, even when groups lack apparent\nstructure as in informal conversation. Emergent network structures of influence\ninteract with the pre-discussion belief distribution to moderate the effect of\ncommunication on belief formation. As a result, communication sometimes\nincreases and sometimes decreases the accuracy of the average belief in a\ngroup. The effects differ for mediated processes and unstructured\ncommunication, such that the relative benefit of each communication format\ndepends on both group dynamics as well as the statistical properties of\npre-interaction beliefs. These results resolve contradictions in previous\nresearch and offer practical recommendations for teams and organizations.\n"
    },
    {
        "paper_id": 2009.07341,
        "authors": "Timo Dimitriadis and Xiaochun Liu and Julie Schnaitmann",
        "title": "Encompassing Tests for Value at Risk and Expected Shortfall Multi-Step\n  Forecasts based on Inference on the Boundary",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose forecast encompassing tests for the Expected Shortfall (ES)\njointly with the Value at Risk (VaR) based on flexible link (or combination)\nfunctions. Our setup allows testing encompassing for convex forecast\ncombinations and for link functions which preclude crossings of the combined\nVaR and ES forecasts. As the tests based on these link functions involve\nparameters which are on the boundary of the parameter space under the null\nhypothesis, we derive and base our tests on nonstandard asymptotic theory on\nthe boundary. Our simulation study shows that the encompassing tests based on\nour new link functions outperform tests based on unrestricted linear link\nfunctions for one-step and multi-step forecasts. We further illustrate the\npotential of the proposed tests in a real data analysis for forecasting VaR and\nES of the S&P 500 index.\n"
    },
    {
        "paper_id": 2009.07599,
        "authors": "Philipp Koch",
        "title": "Economic Complexity and Growth: Can value-added exports better explain\n  the link?",
        "comments": "Discussion on potential endogeneity issues with value-added exports\n  extended",
        "journal-ref": null,
        "doi": "10.1016/j.econlet.2020.109682",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In economic literature, economic complexity is typically approximated on the\nbasis of an economy's gross export structure. However, in times of ever\nincreasingly integrated global value chains, gross exports may convey an\ninaccurate image of a country's economic performance since they also\nincorporate foreign value-added and double-counted exports. Thus, I introduce a\nnew empirical approach approximating economic complexity based on a country's\nvalue-added export structure. This approach leads to substantially different\ncomplexity rankings compared to established metrics. Moreover, the explanatory\npower of GDP per capita growth rates for a sample of 40 lower-middle- to\nhigh-income countries is considerably higher, even if controlling for typical\ngrowth regression covariates.\n"
    },
    {
        "paper_id": 2009.07684,
        "authors": "Simone Severini and Luigi Biagini",
        "title": "The direct and indirect effect of CAP support on farm income\n  enhancement:a farm-based econometric analysis",
        "comments": "New paper available",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We assess the correlation between CAP support provided to farmers and their\nincome and use of capital and labour in the first year of the new CAP regime.\nThis is done applying three regression models on the Italian FADN farms\ncontrolling for other farm characteristics. CAP annual payments are positively\ncorrelated with farm income and capital but are negatively correlated with\nlabour use. Farm investment support provided by RDP measures is positively\ncorrelated to the amount of capital. Results suggest that CAP is positively\naffecting farm income directly but also indirectly by supporting the\nsubstitution of labour with capital\n"
    },
    {
        "paper_id": 2009.07727,
        "authors": "Diego Kozlowski, Viktoriya Semeshenko and Andrea Molinari",
        "title": "Latent Dirichlet Allocation Models for World Trade Analysis",
        "comments": null,
        "journal-ref": "PLOS ONE (2021) 16(2): e0245393",
        "doi": "10.1371/journal.pone.0245393",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The international trade is one of the classic areas of study in economics.\nNowadays, given the availability of data, the tools used for the analysis can\nbe complemented and enriched with new methodologies and techniques that go\nbeyond the traditional approach. The present paper shows the application of the\nLatent Dirichlet Allocation Models, a well known technique from the area of\nNatural Language Processing, to search for latent dimensions in the product\nspace of international trade, and their distribution across countries over\ntime. We apply this technique to a dataset of countries' exports of goods from\n1962 to 2016. The findings show the possibility to generate higher level\nclassifications of goods based on the empirical evidence, and also allow to\nstudy the distribution of those classifications within countries. The latter\nshow interesting insights about countries' trade specialisation.\n"
    },
    {
        "paper_id": 2009.07892,
        "authors": "Christopher Kath and Florian Ziel",
        "title": "Optimal Order Execution in Intraday Markets: Minimizing Costs in Trade\n  Trajectories",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Optimal execution, i.e., the determination of the most cost-effective way to\ntrade volumes in continuous trading sessions, has been a topic of interest in\nthe equity trading world for years. Electricity intraday trading slowly follows\nthis trend but is far from being well-researched. The underlying problem is a\nvery complex one. Energy traders, producers, and electricity wholesale\ncompanies receive various position updates from customer businesses, renewable\nenergy production, or plant outages and need to trade these positions in\nintraday markets. They have a variety of options when it comes to position\nsizing or timing. Is it better to trade all amounts at once? Should they split\norders into smaller pieces? Taking the German continuous hourly intraday market\nas an example, this paper derives an appropriate model for electricity trading.\nWe present our results from an out-of-sample study and differentiate between\nsimple benchmark models and our more refined optimization approach that takes\ninto account order book depth, time to delivery, and different trading regimes\nlike XBID (Cross-Border Intraday Project) trading. Our paper is highly relevant\nas it contributes further insight into the academic discussion of algorithmic\nexecution in continuous intraday markets and serves as an orientation for\npractitioners. Our initial results suggest that optimal execution strategies\nhave a considerable monetary impact.\n"
    },
    {
        "paper_id": 2009.07947,
        "authors": "Thomas Dierckx, Jesse Davis and Wim Schoutens",
        "title": "Using Machine Learning and Alternative Data to Predict Movements in\n  Market Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using machine learning and alternative data for the prediction of financial\nmarkets has been a popular topic in recent years. Many financial variables such\nas stock price, historical volatility and trade volume have already been\nthrough extensive investigation. Remarkably, we found no existing research on\nthe prediction of an asset's market implied volatility within this context.\nThis forward-looking measure gauges the sentiment on the future volatility of\nan asset, and is deemed one of the most important parameters in the world of\nderivatives. The ability to predict this statistic may therefore provide a\ncompetitive edge to practitioners of market making and asset management alike.\nConsequently, in this paper we investigate Google News statistics and Wikipedia\nsite traffic as alternative data sources to quantitative market data and\nconsider Logistic Regression, Support Vector Machines and AdaBoost as machine\nlearning models. We show that movements in market implied volatility can indeed\nbe predicted through the help of machine learning techniques. Although the\nemployed alternative data appears to not enhance predictive accuracy, we reveal\npreliminary evidence of non-linear relationships between features obtained from\nWikipedia page traffic and movements in market implied volatility.\n"
    },
    {
        "paper_id": 2009.0803,
        "authors": "Zhifeng Liu, Toan Luu Duc Huynh, Peng-Fei Dai",
        "title": "The impact of COVID-19 on the stock market crash risk in China",
        "comments": "30 pages",
        "journal-ref": "Research in International Business and Finance, 2021, 57(4):101419",
        "doi": "10.1016/j.ribaf.2021.101419",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the impact of the COVID-19 pandemic on the stock\nmarket crash risk in China. For this purpose, we first estimated the\nconditional skewness of the return distribution from a GARCH with skewness\n(GARCH-S) model as the proxy for the equity market crash risk of the Shanghai\nStock Exchange. We then constructed a fear index for COVID-19 using data from\nthe Baidu Index. Based on the findings, conditional skewness reacts negatively\nto daily growth in total confirmed cases, indicating that the pandemic\nincreases stock market crash risk. Moreover, the fear sentiment exacerbates\nsuch risk, especially with regard to the impact of COVID-19. In other words,\nwhen the fear sentiment is high, the stock market crash risk is more strongly\naffected by the pandemic. Our evidence is robust for the number of daily deaths\nand global cases.\n"
    },
    {
        "paper_id": 2009.08214,
        "authors": "William Lefebvre (LPSM), Gregoire Loeper (BNPP CIB GM Lab), Huy\\^en\n  Pham (LPSM)",
        "title": "Mean-variance portfolio selection with tracking error penalization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a variation of the continuous-time mean-variance portfolio\nselection where a tracking-error penalization is added to the mean-variance\ncriterion. The tracking error term penalizes the distance between the\nallocation controls and a reference portfolio with same wealth and fixed\nweights. Such consideration is motivated as follows: (i) On the one hand, it is\na way to robustify the mean-variance allocation in case of misspecified\nparameters, by \"fitting\" it to a reference portfolio that can be agnostic to\nmarket parameters; (ii) On the other hand, it is a procedure to track a\nbenchmark and improve the Sharpe ratio of the resulting portfolio by\nconsidering a mean-variance criterion in the objective function. This problem\nis formulated as a McKean-Vlasov control problem. We provide explicit solutions\nfor the optimal portfolio strategy and asymptotic expansions of the portfolio\nstrategy and efficient frontier for small values of the tracking error\nparameter. Finally, we compare the Sharpe ratios obtained by the standard\nmean-variance allocation and the penalized one for four different reference\nportfolios: equal-weights, minimum-variance, equal risk contributions and\nshrinking portfolio. This comparison is done on a simulated misspecified model,\nand on a backtest performed with historical data. Our results show that in most\ncases, the penalized portfolio outperforms in terms of Sharpe ratio both the\nstandard mean-variance and the reference portfolio.\n"
    },
    {
        "paper_id": 2009.08269,
        "authors": "Robin Hirsch",
        "title": "Marxism, Logic and the Rate of Profit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is argued that Marxism, being based on contradictions, is an illogical\nmethod. More specifically, we present a rejection of Marx's thesis that the\nrate of profit has a long-term tendency to fall.\n"
    },
    {
        "paper_id": 2009.08412,
        "authors": "Kyle Steinhauer, Takahisa Fukadai, Sho Yoshida",
        "title": "Solving the Optimal Trading Trajectory Problem Using Simulated\n  Bifurcation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use an optimization procedure based on simulated bifurcation (SB) to solve\nthe integer portfolio and trading trajectory problem with an unprecedented\ncomputational speed. The underlying algorithm is based on a classical\ndescription of quantum adiabatic evolutions of a network of non-linearly\ninteracting oscillators. This formulation has already proven to beat state of\nthe art computation times for other NP-hard problems and is expected to show\nsimilar performance for certain portfolio optimization problems. Inspired by\nsuch we apply the SB approach to the portfolio integer optimization problem\nwith quantity constraints and trading activities. We show first numerical\nresults for portfolios of up to 1000 assets, which already confirm the power of\nthe SB algorithm for its novel use-case as a portfolio and trading trajectory\noptimizer.\n"
    },
    {
        "paper_id": 2009.08533,
        "authors": "David Itkin and Martin Larsson",
        "title": "Robust Asymptotic Growth in Stochastic Portfolio Theory under Long-Only\n  Constraints",
        "comments": "54 pages",
        "journal-ref": "Mathematical Finance, 2021",
        "doi": "10.1111/mafi.12331",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of maximizing the asymptotic growth rate of an\ninvestor under drift uncertainty in the setting of stochastic portfolio theory\n(SPT). As in the work of Kardaras and Robertson we take as inputs (i) a\nMarkovian volatility matrix $c(x)$ and (ii) an invariant density $p(x)$ for the\nmarket weights, but we additionally impose long-only constraints on the\ninvestor. Our principal contribution is proving a uniqueness and existence\nresult for the class of concave functionally generated portfolios and\ndeveloping a finite dimensional approximation, which can be used to numerically\nfind the optimum. In addition to the general results outlined above, we propose\nthe use of a broad class of models for the volatility matrix $c(x)$, which can\nbe calibrated to data and, under which, we obtain explicit formulas of the\noptimal unconstrained portfolio for any invariant density.\n"
    },
    {
        "paper_id": 2009.08668,
        "authors": "Olivier Accominotti (LSE), Stefano Ugolini (LEREPS)",
        "title": "International Trade Finance from the Origins to the Present: Market\n  Structures, Regulation and Governance",
        "comments": null,
        "journal-ref": "Eric Brousseau, Jean-Michel Glachant, J{\\'e}r{\\^o}me Sgard. The\n  Oxford Handbook of Institutions of International Economic Governance and\n  Market Regulation, Oxford University Press, In press, 9780190900571",
        "doi": "10.1093/oxfordhb/9780190900571.013.1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This chapter presents a history of international trade finance - the oldest\ndomain of international finance - from its emergence in the Middle Ages up to\ntoday. We describe how the structure and governance of the global trade finance\nmarket changed over time and how trade credit instruments evolved. Trade\nfinance products initially consisted of idiosyncratic assets issued by local\nmerchants and bankers. The financing of international trade then became\nincreasingly centralized and credit instruments were standardized through the\ndiffusion of the local standards of consecutive leading trading centres\n(Antwerp, Amsterdam, London). This process of market centralization/product\nstandardization culminated in the nineteenth century when London became the\nglobal centre for international trade finance and the sterling bill of exchange\nemerged as the most widely used trade finance instrument. The structure of the\ntrade finance market then evolved considerably following the First World War\nand disintegrated during the interwar de-globalization and Bretton Woods\nperiod. The reconstruction of global trade finance in the post-1970 period gave\nway to the decentralized market structure that prevails nowadays.\n"
    },
    {
        "paper_id": 2009.08794,
        "authors": "Jeremy D. Turiel, Paolo Barucca and Tomaso Aste",
        "title": "Simplicial persistence of financial markets: filtering, generative\n  processes and portfolio risk",
        "comments": "8 pages, 5 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1910.08628",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce simplicial persistence, a measure of time evolution of network\nmotifs in subsequent temporal layers. We observe long memory in the evolution\nof structures from correlation filtering, with a two regime power law decay in\nthe number of persistent simplicial complexes. Null models of the underlying\ntime series are tested to investigate properties of the generative process and\nits evolutional constraints. Networks are generated with both TMFG filtering\ntechnique and thresholding showing that embedding-based filtering methods\n(TMFG) are able to identify higher order structures throughout the market\nsample, where thresholding methods fail. The decay exponents of these long\nmemory processes are used to characterise financial markets based on their\nstage of development and liquidity. We find that more liquid markets tend to\nhave a slower persistence decay. This is in contrast with the common\nunderstanding that developed markets are more random. We find that they are\nindeed less predictable for what concerns the dynamics of each single variable\nbut they are more predictable for what concerns the collective evolution of the\nvariables. This could imply higher fragility to systemic shocks.\n"
    },
    {
        "paper_id": 2009.08814,
        "authors": "Peter K. Friz, Paul Gassiat, Paolo Pigato",
        "title": "Short dated smile under Rough Volatility: asymptotics and numerics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In [Precise Asymptotics for Robust Stochastic Volatility Models; Ann. Appl.\nProbab. 2021] we introduce a new methodology to analyze large classes of\n(classical and rough) stochastic volatility models, with special regard to\nshort-time and small noise formulae for option prices, using the framework\n[Bayer et al; A regularity structure for rough volatility; Math. Fin. 2020]. We\ninvestigate here the fine structure of this expansion in large deviations and\nmoderate deviations regimes, together with consequences for implied volatility.\nWe discuss computational aspects relevant for the practical application of\nthese formulas. We specialize such expansions to prototypical rough volatility\nexamples and discuss numerical evidence.\n"
    },
    {
        "paper_id": 2009.08821,
        "authors": "Fr\\'ed\\'eric Butin",
        "title": "A bounded operator approach to technical indicators without lag",
        "comments": "10 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the framework of technical analysis for algorithmic trading we use a\nlinear algebra approach in order to define classical technical indicators as\nbounded operators of the space $l^\\infty(\\mathbb{N})$. This more abstract view\nenables us to define in a very simple way the no-lag versions of these tools.\nThen we apply our results to a basic trading system in order to compare the\nclassical Elder's impulse system with its no-lag version and the so-called\nNyquist-Elder's impulse system.\n"
    },
    {
        "paper_id": 2009.08826,
        "authors": "Fr\\'ed\\'eric Butin",
        "title": "Generalized distance to a simplex and a new geometrical method for\n  portfolio optimization",
        "comments": "18 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk aversion plays a significant and central role in investors' decisions in\nthe process of developing a portfolio. In this framework of portfolio\noptimization we determine the portfolio that possesses the minimal risk by\nusing a new geometrical method. For this purpose, we elaborate an algorithm\nthat enables us to compute any generalized Euclidean distance to a standard\nsimplex. With this new approach, we are able to treat the case of portfolio\noptimization without short-selling in its entirety, and we also recover in\ngeometrical terms the well-known results on portfolio optimization with allowed\nshort-selling. Then, we apply our results in order to determine which convex\ncombination of the CAC 40 stocks possesses the lowest risk: not only we get a\nvery low risk compared to the index, but we also get a return rate that is\nalmost three times better than the one of the index.\n"
    },
    {
        "paper_id": 2009.09007,
        "authors": "Felix-Benedikt Liebrich and Max Nendel",
        "title": "Separability vs. robustness of Orlicz spaces: financial and economic\n  perspectives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate robust Orlicz spaces as a generalisation of robust\n$L^p$-spaces. Two constructions of such spaces are distinguished, a top-down\napproach and a bottom-up approach. We show that separability of robust Orlicz\nspaces or their subspaces has very strong implications in terms of the\ndominatedness of the set of priors and the lack of order completeness. Our\nresults have subtle implications for the field of robust finance. For instance,\nnorm closures of bounded continuous functions with respect to the worst-case\n$L^p$-norm, as considered in the $G$-framework, lead to spaces which are\nlattice isomorphic to a sublattice of a classical $L^1$-space lacking, however,\nany form of order completeness. We further show that the topological spanning\npower of options is always limited under nondominated uncertainty.\n"
    },
    {
        "paper_id": 2009.09058,
        "authors": "Iro Ren\\'e Kouarfate, Michael A. Kouritzin, Anne MacKay",
        "title": "Explicit solution simulation method for the 3/2 model",
        "comments": "21 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An explicit weak solution for the 3/2 stochastic volatility model is obtained\nand used to develop a simulation algorithm for option pricing purposes. The 3/2\nmodel is a non-affine stochastic volatility model whose variance process is the\ninverse of a CIR process. This property is exploited here to obtain an explicit\nweak solution, similarly to Kouritzin (2018). A simulation algorithm based on\nthis solution is proposed and tested via numerical examples. The performance of\nthe resulting pricing algorithm is comparable to that of other popular\nsimulation algorithms.\n"
    },
    {
        "paper_id": 2009.09165,
        "authors": "Siddhartha Datta",
        "title": "A study into the impact of anti-extradition bill protests on Bangladeshi\n  immigration into Hong Kong",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consequences from the 2019 anti-extradition protests in Hong Kong have been\nstudied in many facets, but one topic of interest that has not been explored is\nthe impact on the immigration of Bangladeshi immigrants into the city. This\npaper explores the value add of Bangladeshis to the Hong Kong, how the protests\naffected their mentality and consequently their immigration, and potentially\nlonger-term detrimental effects on the city.\n"
    },
    {
        "paper_id": 2009.09198,
        "authors": "Le Dong Hai Nguyen",
        "title": "On the implementation of the Universal Basic Income as a response to\n  technological unemployment",
        "comments": "An early version of this paper was shortlisted by the Royal Economic\n  Society for the Young Economist of the Year award in October 2019",
        "journal-ref": "Int.J.Mgmt.Res.&Eco. 1(3) (2021) 1-6",
        "doi": "10.51483/IJMRE.1.3.2021.1-6",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The effects of automation on our economy and society are more palpable than\never, with nearly half of jobs at risk of being fully executed by machines over\nthe next decade or two. Policymakers and scholars alike have championed the\nUniversal Basic Income (UBI) as a catch-all solution to this problem. This\npaper examines the shortcomings of UBI in addressing the automation-led\nlarge-scale displacement of labor by analyzing empirical data from previous\nUBI-comparable experiments and presenting theoretical projections that\nhighlight disappointing impacts of UBI in the improvement of relevant living\nstandards and employability metrics among pensioners. Finally, a recommendation\nshall be made for the retainment of existing means-tested welfare programs\nwhile bolstering funding and R&D for more up-to-date worker training schemes as\na more effective solution to technological unemployment.\n"
    },
    {
        "paper_id": 2009.09222,
        "authors": "Carlo Fezzi, Valeria Fanghella",
        "title": "Tracking GDP in real-time using electricity market data: insights from\n  the first wave of COVID-19 across Europe",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a methodology for tracking in real time the impact of\nshocks (such as natural disasters, financial crises or pandemics) on gross\ndomestic product (GDP) by analyzing high-frequency electricity market data. As\nan illustration, we estimate the GDP loss caused by COVID-19 in twelve European\ncountries during the first wave of the pandemic. Our results are almost\nindistinguishable from the official statistics of the recession during the\nfirst two quarters of 2020 (correlation coefficient of 0.98) and are validated\nby several robustness tests. However, they are also more chronologically\ndisaggregated and up-to-date than standard macroeconomic indicators and,\ntherefore, can provide crucial and timely information for policy evaluation.\nOur results show that delaying intervention and pursuing 'herd immunity' have\nnot been successful strategies so far, since they increased both economic\ndisruption and mortality. We also find that coordinating policies\ninternationally is fundamental for minimizing spillover effects from NPIs\nacross countries.\n"
    },
    {
        "paper_id": 2009.09329,
        "authors": "Mauricio Contreras G",
        "title": "Endogenous Stochastic Arbitrage Bubbles and the Black--Scholes model",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126323",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a model that incorporates the presence of stochastic\narbitrage explicitly in the Black--Scholes equation. Here, the arbitrage is\ngenerated by a stochastic bubble, which generalizes the deterministic arbitrage\nmodel obtained in the literature. It is considered to be a generic stochastic\ndynamic for the arbitrage bubble, and a generalized Black--Scholes equation is\nthen derived. The resulting equation is similar to that of the stochastic\nvolatility models, but there are no undetermined parameters as the market price\nof risk. The proposed theory has asymptotic behaviors that are associated with\nthe weak and strong arbitrage bubble limits. For the case where the arbitrage\nbubble's volatility is zero (deterministic bubble), the weak limit corresponds\nto the usual Black-Scholes model. The strong limit case also give a\nBlack--Scholes model, but the underlying asset's mean value replaces the\ninterest rate. When the bubble is stochastic, the theory also has weak and\nstrong asymptotic limits that give rise to option price dynamics that are\nsimilar to the Black--Scholes model. Explicit formulas are derived for Gaussian\nand lognormal stochastic bubbles. Consequently, the Black--Scholes model can be\nconsidered to be a \"low energy\" limit of a more general stochastic model.\n"
    },
    {
        "paper_id": 2009.09342,
        "authors": "Andrey Itkin and Dmitry Muravey",
        "title": "Semi-analytic pricing of double barrier options with time-dependent\n  barriers and rebates at hit",
        "comments": "25 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We continue a series of papers devoted to construction of semi-analytic\nsolutions for barrier options. These options are written on underlying\nfollowing some simple one-factor diffusion model, but all the parameters of the\nmodel as well as the barriers are time-dependent. We managed to show that these\nsolutions are systematically more efficient for pricing and calibration than,\neg., the corresponding finite-difference solvers. In this paper we extend this\ntechnique to pricing double barrier options and present two approaches to\nsolving it: the General Integral transform method and the Heat Potential\nmethod. Our results confirm that for double barrier options these semi-analytic\ntechniques are also more efficient than the traditional numerical methods used\nto solve this type of problems.\n"
    },
    {
        "paper_id": 2009.09454,
        "authors": "Maarten P. Scholl, Anisoara Calinescu, J. Doyne Farmer",
        "title": "How Market Ecology Explains Market Malfunction",
        "comments": "9 pages, 5 figures, Conference on Evolutionary Models of Financial\n  Markets, includes responses to reviewers",
        "journal-ref": null,
        "doi": "10.1073/pnas.2015574118",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Standard approaches to the theory of financial markets are based on\nequilibrium and efficiency. Here we develop an alternative based on concepts\nand methods developed by biologists, in which the wealth invested in a\nfinancial strategy is like the abundance of a species. We study a toy model of\na market consisting of value investors, trend followers and noise traders. We\nshow that the average returns of strategies are strongly density dependent,\ni.e. they depend on the wealth invested in each strategy at any given time. In\nthe absence of noise the market would slowly evolve toward an efficient\nequilibrium, but the statistical uncertainty in profitability (which is\nadjusted to match real markets) makes this noisy and uncertain. Even in the\nlong term, the market spends extended periods of time away from perfect\nefficiency. We show how core concepts from ecology, such as the community\nmatrix and food webs, give insight into market behavior. The wealth dynamics of\nthe market ecology explain how market inefficiencies spontaneously occur and\ngives insight into the origins of excess price volatility and deviations of\nprices from fundamental values.\n"
    },
    {
        "paper_id": 2009.09547,
        "authors": "Abraham Londono Pineda, Jose Alejandro Cano, Lissett Pulgarin",
        "title": "Sulfur emission reduction in cargo ship manufacturers and shipping\n  companies based on MARPOL Annex VI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article explores the challenges for the adoption of scrubbers and low\nsulfur fuels on ship manufacturers and shipping companies. Results show that\nship manufacturers, must finance their working capital and operating costs,\nwhich implies an increase in the prices of the ships employing these new\ntechnologies. On the other hand, shipping companies must adopt the most\nappropriate technology according to the areas where ships navigate, the scale\neconomies of trade routes, and the cost-benefit analysis of ship modernization.\n"
    },
    {
        "paper_id": 2009.09572,
        "authors": "Ling Wang (1), Mei Choi Chiu (2), Hoi Ying Wong (1) ((1) The Chinese\n  University of Hong Kong, (2) The Education University of Hong Kong)",
        "title": "Volterra mortality model: Actuarial valuation and risk management with\n  long-range dependence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While abundant empirical studies support the long-range dependence (LRD) of\nmortality rates, the corresponding impact on mortality securities are largely\nunknown due to the lack of appropriate tractable models for valuation and risk\nmanagement purposes. We propose a novel class of Volterra mortality models that\nincorporate LRD into the actuarial valuation, retain tractability, and are\nconsistent with the existing continuous-time affine mortality models. We derive\nthe survival probability in closed-form solution by taking into account of the\nhistorical health records. The flexibility and tractability of the models make\nthem useful in valuing mortality-related products such as death benefits,\nannuities, longevity bonds, and many others, as well as offering optimal\nmean-variance mortality hedging rules. Numerical studies are conducted to\nexamine the effect of incorporating LRD into mortality rates on various\ninsurance products and hedging efficiency.\n"
    },
    {
        "paper_id": 2009.09713,
        "authors": "Sergey Nasekin, Wolfgang Karl H\\\"ardle",
        "title": "Model-driven statistical arbitrage on LETF option markets",
        "comments": null,
        "journal-ref": "Quantitative Finance Quantitative Finance, Volume 19, 2019 - Issue\n  11",
        "doi": "10.1080/14697688.2019.1605186",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study the statistical properties of the moneyness scaling\ntransformation by Leung and Sircar (2015). This transformation adjusts the\nmoneyness coordinate of the implied volatility smile in an attempt to remove\nthe discrepancy between the IV smiles for levered and unlevered ETF options. We\nconstruct bootstrap uniform confidence bands which indicate that the implied\nvolatility smiles are statistically different after moneyness scaling has been\nperformed. An empirical application shows that there are trading opportunities\npossible on the LETF market. A statistical arbitrage type strategy based on a\ndynamic semiparametric factor model is presented. This strategy presents a\nstatistical decision algorithm which generates trade recommendations based on\ncomparison of model and observed LETF implied volatility surface. It is shown\nto generate positive returns with a high probability. Extensive econometric\nanalysis of LETF implied volatility process is performed including\nout-of-sample forecasting based on a semiparametric factor model and uniform\nconfidence bands' study. It provides new insights into the latent dynamics of\nthe implied volatility surface. We also incorporate Heston stochastic\nvolatility into the moneyness scaling method for better tractability of the\nmodel.\n"
    },
    {
        "paper_id": 2009.09739,
        "authors": "Shi Chen, Wolfgang Karl H\\\"ardle, Brenda L\\'opez Cabrera",
        "title": "Regularization Approach for Network Modeling of German Power Derivative\n  Market",
        "comments": null,
        "journal-ref": "Energy Economics, Volume 83, September 2019, Pages 180-196",
        "doi": "10.1016/j.eneco.2019.06.021",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we propose a regularization approach for network modeling of\nGerman power derivative market. To deal with the large portfolio, we combine\nhigh-dimensional variable selection techniques with dynamic network analysis.\nThe estimated sparse interconnectedness of the full German power derivative\nmarket, clearly identify the significant channels of relevant potential risk\nspillovers. Our empirical findings show the importance of interdependence\nbetween different contract types, and identify the main risk contributors. We\nfurther observe strong pairwise interconnections between the neighboring\ncontracts especially for the spot contracts trading in the peak hours, its\nimplications for regulators and investors are also discussed. The network\nanalysis of the full German power derivative market helps us to complement a\nfull picture of system risk, and have a better understanding of the German\npower market functioning and environment.\n"
    },
    {
        "paper_id": 2009.09751,
        "authors": "Friedrich Hubalek and Walter Schachermayer",
        "title": "Convergence of Optimal Expected Utility for a Sequence of Binomial\n  Models",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the convergence of expected utility under the approximation of the\nBlack-Scholes model by binomial models. In a recent paper by D. Kreps and W.\nSchachermayer a surprising and somewhat counter-intuitive example was given:\nsuch a convergence may, in general, fail to hold true. This counterexample is\nbased on a binomial model where the i.i.d. logarithmic one-step increments have\nstrictly positive third moments. This is the case, when the up-tick of the\nlog-price is larger than the down-tick. In the paper by D. Kreps and W.\nSchachermayer it was left as an open question how things behave in the case\nwhen the down-tick is larger than the up-tick and -- most importantly -- in the\ncase of the symmetric binomial model where the up-tick equals the down-tick. Is\nthere a general positive result of convergence of expected utility in this\nsetting? In the present note we provide a positive answer to this question. It\nis based on some rather fine estimates of the convergence arising in the\nCentral Limit Theorem.\n"
    },
    {
        "paper_id": 2009.0977,
        "authors": "Wolfgang Karl H\\\"ardle, Elena Silyakova",
        "title": "Implied Basket Correlation Dynamics",
        "comments": null,
        "journal-ref": "Statistics & Risk Modeling 2016, Volume 33: Issue 1-2",
        "doi": "10.1515/strm-2014-1176",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Equity basket correlation can be estimated both using the physical measure\nfrom stock prices, and also using the risk neutral measure from option prices.\nThe difference between the two estimates motivates a so-called \"dispersion\nstrategy''. We study the performance of this strategy on the German market and\npropose several profitability improvement schemes based on implied correlation\n(IC) forecasts. Modelling IC conceals several challenges. Firstly the number of\ncorrelation coefficients would grow with the size of the basket. Secondly, IC\nis not constant over maturities and strikes. Finally, IC changes over time. We\nreduce the dimensionality of the problem by assuming equicorrelation. The IC\nsurface (ICS) is then approximated from the implied volatilities of stocks and\nthe implied volatility of the basket. To analyze the dynamics of the ICS we\nemploy a dynamic semiparametric factor model.\n"
    },
    {
        "paper_id": 2009.09782,
        "authors": "Simon Trimborn, Wolfgang Karl H\\\"ardle",
        "title": "CRIX an index for cryptocurrencies",
        "comments": null,
        "journal-ref": "Journal of Empirical Finance, Volume 49, December 2018, Pages\n  107-122",
        "doi": "10.1016/j.jempfin.2018.08.004",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The cryptocurrency market is unique on many levels: Very volatile, frequently\nchanging market structure, emerging and vanishing of cryptocurrencies on a\ndaily level. Following its development became a difficult task with the success\nof cryptocurrencies (CCs) other than Bitcoin. For fiat currency markets, the\nIMF offers the index SDR and, prior to the EUR, the ECU existed, which was an\nindex representing the development of European currencies. Index providers\ndecide on a fixed number of index constituents which will represent the market\nsegment. It is a challenge to fix a number and develop rules for the\nconstituents in view of the market changes. In the frequently changing CC\nmarket, this challenge is even more severe. A method relying on the AIC is\nproposed to quickly react to market changes and therefore enable us to create\nan index, referred to as CRIX, for the cryptocurrency market. CRIX is chosen by\nmodel selection such that it represents the market well to enable each\ninterested party studying economic questions in this market and to invest into\nthe market. The diversified nature of the CC market makes the inclusion of\naltcoins in the index product critical to improve tracking performance. We have\nshown that assigning optimal weights to altcoins helps to reduce the tracking\nerrors of a CC portfolio, despite the fact that their market cap is much\nsmaller relative to Bitcoin. The codes used here are available via\nwww.quantlet.de.\n"
    },
    {
        "paper_id": 2009.09799,
        "authors": "Jaehyuk Park, Morgan R. Frank, Lijun Sun, Hyejin Youn",
        "title": "Industrial Topics in Urban Labor System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Categorization is an essential component for us to understand the world for\nourselves and to communicate it collectively. It is therefore important to\nrecognize that classification system are not necessarily static, especially for\neconomic systems, and even more so in urban areas where most innovation takes\nplace and is implemented. Out-of-date classification systems would potentially\nlimit further understanding of the current economy because things constantly\nchange. Here, we develop an occupation-based classification system for the US\nlabor economy, called industrial topics, that satisfy adaptability and\nrepresentability. By leveraging the distributions of occupations across the US\nurban areas, we identify industrial topics - clusters of occupations based on\ntheir co-existence pattern. Industrial topics indicate the mechanisms under the\nsystematic allocation of different occupations. Considering the densely\nconnected occupations as an industrial topic, our approach characterizes\nregional economies by their topical composition. Unlike the existing\nsurvey-based top-down approach, our method provides timely information about\nthe underlying structure of the regional economy, which is critical for\npolicymakers and business leaders, especially in our fast-changing economy.\n"
    },
    {
        "paper_id": 2009.09816,
        "authors": "E. Boguslavskaya and M. Boguslavsky and D.Muravey",
        "title": "Trading multiple mean reversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How should one construct a portfolio from multiple mean-reverting assets?\nShould one add an asset to portfolio even if the asset has zero mean reversion?\nWe consider a position management problem for an agent trading multiple\nmean-reverting assets. We solve an optimal control problem for an agent with\npower utility, and present a semi-explicit solution. The nearly explicit nature\nof the solution allows us to study the effects of parameter mis-specification,\nand derive a number of properties of the optimal solution.\n"
    },
    {
        "paper_id": 2009.09978,
        "authors": "Samsul Alam",
        "title": "A Time Series Data Analysis of Indian Commercial Dynamism",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this report it is analyzed the focuses of the commercial dynamism of\nIndia, covering the fundamentals of growth rate, trade balance, coverage rate,\nopenness rate, share of world indicators and then present each of them in\ndetail.\n"
    },
    {
        "paper_id": 2009.09993,
        "authors": "Artur Sokolovsky and Luca Arnaboldi",
        "title": "A Generic Methodology for the Statistically Uniform & Comparable\n  Evaluation of Automated Trading Platform Components",
        "comments": "Associated processing files are available at:\n  https://doi.org/10.5281/zenodo.4036850",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although machine learning approaches have been widely used in the field of\nfinance, to very successful degrees, these approaches remain bespoke to\nspecific investigations and opaque in terms of explainability, comparability,\nand reproducibility. The primary objective of this research was to shed light\nupon this field by providing a generic methodology that was\ninvestigation-agnostic and interpretable to a financial markets practitioner,\nthus enhancing their efficiency, reducing barriers to entry, and increasing the\nreproducibility of experiments. The proposed methodology is showcased on two\nautomated trading platform components. Namely, price levels, a well-known\ntrading pattern, and a novel 2-step feature extraction method. The methodology\nrelies on hypothesis testing, which is widely applied in other social and\nscientific disciplines to effectively evaluate the concrete results beyond\nsimple classification accuracy. The main hypothesis was formulated to evaluate\nwhether the selected trading pattern is suitable for use in the machine\nlearning setting. Across the experiments we found that the use of the\nconsidered trading pattern in the machine learning setting is only partially\nsupported by statistics, resulting in insignificant effect sizes (Rebound 7 -\n$0.64 \\pm 1.02$, Rebound 11 $0.38 \\pm 0.98$, and rebound 15 - $1.05 \\pm 1.16$),\nbut allowed the rejection of the null hypothesis. We showcased the generic\nmethodology on a US futures market instrument and provided evidence that with\nthis methodology we could easily obtain informative metrics beyond the more\ntraditional performance and profitability metrics. This work is one of the\nfirst in applying this rigorous statistically-backed approach to the field of\nfinancial markets and we hope this may be a springboard for more research.\n"
    },
    {
        "paper_id": 2009.1003,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Jaros{\\l}aw Kwapie\\'n, Pawe{\\l}\n  O\\'swi\\k{e}cimka, Tomasz Stanisz, and Marcin W\\k{a}torek",
        "title": "Complexity in economic and social systems: cryptocurrency market at\n  around COVID-19",
        "comments": null,
        "journal-ref": "Entropy 22, 1043 (2020)",
        "doi": "10.3390/e22091043",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social systems are characterized by an enormous network of connections and\nfactors that can influence the structure and dynamics of these systems. All\nfinancial markets, including the cryptocurrency market, belong to the\neconomical sphere of human activity that seems to be the most interrelated and\ncomplex. The cryptocurrency market complexity can be studied from different\nperspectives. First, the dynamics of the cryptocurrency exchange rates to other\ncryptocurrencies and fiat currencies can be studied and quantified by means of\nmultifractal formalism. Second, coupling and decoupling of the cryptocurrencies\nand the conventional assets can be investigated with the advanced\ncross-correlation analyses based on fractal analysis. Third, an internal\nstructure of the cryptocurrency market can also be a subject of analysis that\nexploits, for example, a network representation of the market. We approach this\nsubject from all three perspectives based on data recorded between January 2019\nand June 2020. This period includes the Covid-19 pandemic and we pay particular\nattention to this event and investigate how strong its impact on the structure\nand dynamics of the market was. Besides, the studied data covers a few other\nsignificant events like double bull and bear phases in 2019. We show that,\nthroughout the considered interval, the exchange rate returns were multifractal\nwith intermittent signatures of bifractality that can be associated with the\nmost volatile periods of the market dynamics like a bull market onset in April\n2019 and the Covid-19 outburst in March 2020. The topology of a minimal\nspanning tree representation of the market also used to alter during these\nevents from a distributed type without any dominant node to a highly\ncentralized type with a dominating hub of USDT. However, the MST topology\nduring the pandemic differs in some details from other volatile periods.\n"
    },
    {
        "paper_id": 2009.10392,
        "authors": "Junni L. Zhang, Wolfgang Karl H\\\"ardle, Cathy Y. Chen, Elisabeth\n  Bommes",
        "title": "Distillation of News Flow into Analysis of Stock Reactions",
        "comments": null,
        "journal-ref": "Journal of Business and Economic Statistics, Volume 34, 2016,\n  Issue 4: Special Issues on Big Data",
        "doi": "10.1080/07350015.2015.1110525",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The gargantuan plethora of opinions, facts and tweets on financial business\noffers the opportunity to test and analyze the influence of such text sources\non future directions of stocks. It also creates though the necessity to distill\nvia statistical technology the informative elements of this prodigious and\nindeed colossal data source. Using mixed text sources from professional\nplatforms, blog fora and stock message boards we distill via different lexica\nsentiment variables. These are employed for an analysis of stock reactions:\nvolatility, volume and returns. An increased sentiment, especially for those\nwith negative prospection, will influence volatility as well as volume. This\ninfluence is contingent on the lexical projection and different across Global\nIndustry Classification Standard (GICS) sectors. Based on review articles on\n100 S&P 500 constituents for the period of October 20, 2009, to October 13,\n2014, we project into BL, MPQA, LM lexica and use the distilled sentiment\nvariables to forecast individual stock indicators in a panel context.\nExploiting different lexical projections to test different stock reaction\nindicators we aim at answering the following research questions: (i) Are the\nlexica consistent in their analytic ability? (ii) To which degree is there an\nasymmetric response given the sentiment scales (positive v.s. negative)? (iii)\nAre the news of high attention firms diffusing faster and result in more timely\nand efficient stock reaction? (iv) Is there a sector-specific reaction from the\ndistilled sentiment measures? We find there is significant incremental\ninformation in the distilled news flow and the sentiment effect is\ncharacterized as an asymmetric, attention-specific and sector-specific response\nof stock reactions.\n"
    },
    {
        "paper_id": 2009.10451,
        "authors": "Alexander Adamou, Yonatan Berman and Ole Peters",
        "title": "The Two Growth Rates of the Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic growth is measured as the rate of relative change in gross domestic\nproduct (GDP) per capita. Yet, when incomes follow random multiplicative\ngrowth, the ensemble-average (GDP per capita) growth rate is higher than the\ntime-average growth rate achieved by each individual in the long run. This\nmathematical fact is the starting point of ergodicity economics. Using the\natypically high ensemble-average growth rate as the principal growth measure\ncreates an incomplete picture. Policymaking would be better informed by\nreporting both ensemble-average and time-average growth rates. We analyse\nrigorously these growth rates and describe their evolution in the United States\nand France over the last fifty years. The difference between the two growth\nrates gives rise to a natural measure of income inequality, equal to the mean\nlogarithmic deviation. Despite being estimated as the average of individual\nincome growth rates, the time-average growth rate is independent of income\nmobility.\n"
    },
    {
        "paper_id": 2009.10764,
        "authors": "Michele Leonardo Bianchi and Giovanni De Luca and Giorgia Rivieccio",
        "title": "CoVaR with volatility clustering, heavy tails and non-linear dependence",
        "comments": "21 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we estimate the conditional value-at-risk by fitting different\nmultivariate parametric models capturing some stylized facts about multivariate\nfinancial time series of equity returns: heavy tails, negative skew, asymmetric\ndependence, and volatility clustering. While the volatility clustering effect\nis got by AR-GARCH dynamics of the GJR type, the other stylized facts are\ncaptured through non-Gaussian multivariate models and copula functions. The\nCoVaR$^{\\leq}$ is computed on the basis on the multivariate normal model, the\nmultivariate normal tempered stable (MNTS) model, the multivariate generalized\nhyperbolic model (MGH) and four possible copula functions. These risk measure\nestimates are compared to the CoVaR$^{=}$ based on the multivariate normal\nGARCH model. The comparison is conducted by backtesting the competitor models\nover the time span from January 2007 to March 2020. In the empirical study we\nconsider a sample of listed banks of the euro area belonging to the main or to\nthe additional global systemically important banks (GSIBs) assessment sample.\n"
    },
    {
        "paper_id": 2009.10819,
        "authors": "Sidra Mehtab, Jaydip Sen, Abhishek Dutta",
        "title": "Stock Price Prediction Using Machine Learning and LSTM-Based Deep\n  Learning Models",
        "comments": "The paper has been accepted for publication in the Proceedings of the\n  Second Symposium on Machine Learning and Metaheuristic Algorithms and\n  Applications (SOMMA'20): http://www.acn-conference.org/2020/somma2020/. The\n  paper is 18 pages long, and it contains 8 Figures and 12 Tables",
        "journal-ref": null,
        "doi": "10.1007/978-981-16-0419-5_8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of stock prices has been an important area of research for a long\ntime. While supporters of the efficient market hypothesis believe that it is\nimpossible to predict stock prices accurately, there are formal propositions\ndemonstrating that accurate modeling and designing of appropriate variables may\nlead to models using which stock prices and stock price movement patterns can\nbe very accurately predicted. In this work, we propose an approach of hybrid\nmodeling for stock price prediction building different machine learning and\ndeep learning-based models. For the purpose of our study, we have used NIFTY 50\nindex values of the National Stock Exchange (NSE) of India, during the period\nDecember 29, 2014 till July 31, 2020. We have built eight regression models\nusing the training data that consisted of NIFTY 50 index records during\nDecember 29, 2014 till December 28, 2018. Using these regression models, we\npredicted the open values of NIFTY 50 for the period December 31, 2018 till\nJuly 31, 2020. We, then, augment the predictive power of our forecasting\nframework by building four deep learning-based regression models using long-and\nshort-term memory (LSTM) networks with a novel approach of walk-forward\nvalidation. We exploit the power of LSTM regression models in forecasting the\nfuture NIFTY 50 open values using four different models that differ in their\narchitecture and in the structure of their input data. Extensive results are\npresented on various metrics for the all the regression models. The results\nclearly indicate that the LSTM-based univariate model that uses one-week prior\ndata as input for predicting the next week open value of the NIFTY 50 time\nseries is the most accurate model.\n"
    },
    {
        "paper_id": 2009.10823,
        "authors": "Martin Jaraiz",
        "title": "Ants, robots, humans: a self-organizing, complex systems modeling\n  approach",
        "comments": "Includes Main tex, Supplementary text and Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most of the grand challenges of humanity today involve complex agent-based\nsystems, such as epidemiology, economics or ecology. However, remains as a\npending task the challenge of identifying the general principles underlying\ntheir self-organizing capabilities. This article presents a novel modeling\napproach, capable to self-deploy both the system structure and the activities\nfor goal-driven agents that can take appropriate actions to achieve their\ngoals. Humans, robots, and animals are all endowed with this type of behavior.\nSelf-organization is shown to emerge from the decisions of a common rational\nactivity algorithm, based on the information of a system-specific goals\ndependency network. The unique self-deployment feature of this approach, that\ncan also be applied to non-goal-driven agents, can boost considerably the range\nand depth of application of agent-based modeling.\n"
    },
    {
        "paper_id": 2009.10834,
        "authors": "Abraham Londono Pineda, Tatiana Arias Naranjo, Jose Alejandro Cano\n  Arenas",
        "title": "Analysis of the main factors for the configuration of green ports in\n  Colombia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study analyzes the factors affecting the configuration and consolidation\nof green ports in Colombia. For this purpose a case stady of maritime cargo\nports of Cartagena, Barranquilla and Santa Marta is performed addressing\nsemiestructured interviews to identify the factors contributing to the\nconsolidation of green ports and the factors guiding the sustainability\nmanagement in the ports that have not yet been certified as green ports. The\nresults show that environmental regulations are atarting point not the key\nfactor to consolidate asgreen ports. As a conclusions, the conversion of\nColombian to green ports should not be limited to the attaiment of\ncertifications, such as Ecoport certification, but should ensure the\ncontribution to sustainable development through economic, social and\nenvironmental dimensions and the achievement of the SDGs\n"
    },
    {
        "paper_id": 2009.10852,
        "authors": "Keith A. Lewis",
        "title": "Efficient Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given two random realized returns on an investment, which is to be preferred?\nThis is a fundamental problem in finance that has no definitive solution except\nin the case one investment always returns more than the other. In 1952\nMarkowitz and Roy introduced the following criterion for risk vs. return in\nportfolio selection: if two portfolios have the same expected realized return\nthen prefer the one with smaller variance. An efficient portfolio has the least\nvariance among all portfolios having the same expected realized return.\n  The primary contribution of this short note is observation that the CAPM\nformula holds for realized returns as random variables, not just their\nexpectations. This follows directly from writing down a mathematical model for\none period investments.\n"
    },
    {
        "paper_id": 2009.10972,
        "authors": "Eduardo Abi Jaber (CES, UP1 UFR27)",
        "title": "The characteristic function of Gaussian stochastic volatility models: an\n  analytic expression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic volatility models based on Gaussian processes, like fractional\nBrownian motion, are able to reproduce important stylized facts of financial\nmarkets such as rich autocorrelation structures, persistence and roughness of\nsample paths. This is made possible by virtue of the flexibility introduced in\nthe choice of the covariance function of the Gaussian process. The price to pay\nis that, in general, such models are no longer Markovian nor semimartingales,\nwhich limits their practical use. We derive, in two different ways, an explicit\nanalytic expression for the joint characteristic function of the log-price and\nits integrated variance in general Gaussian stochastic volatility models. Such\nanalytic expression can be approximated by closed form matrix expressions. This\nopens the door to fast approximation of the joint density and pricing of\nderivatives on both the stock and its realized variance using Fourier inversion\ntechniques. In the context of rough volatility modeling, our results apply to\nthe (rough) fractional Stein--Stein model and provide the first analytic\nformulae for option pricing known to date, generalizing that of Stein--Stein,\nSch{\\\"o}bel-Zhu and a special case of Heston.\n"
    },
    {
        "paper_id": 2009.11007,
        "authors": "Ai Jun Hou, Weining Wang, Cathy Y. H. Chen, Wolfgang Karl H\\\"ardle",
        "title": "Pricing Cryptocurrency Options",
        "comments": null,
        "journal-ref": "Journal of Financial Econometrics, Volume 18, Issue 2, Spring\n  2020, Pages 250 to 279",
        "doi": "10.1093/jjfinec/nbaa006",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies, especially Bitcoin (BTC), which comprise a new digital\nasset class, have drawn extraordinary worldwide attention. The characteristics\nof the cryptocurrency/BTC include a high level of speculation, extreme\nvolatility and price discontinuity. We propose a pricing mechanism based on a\nstochastic volatility with a correlated jump (SVCJ) model and compare it to a\nflexible co-jump model by Bandi and Ren\\`o (2016). The estimation results of\nboth models confirm the impact of jumps and co-jumps on options obtained via\nsimulation and an analysis of the implied volatility curve. We show that a\nsizeable proportion of price jumps are significantly and contemporaneously\nanti-correlated with jumps in volatility. Our study comprises pioneering\nresearch on pricing BTC options. We show how the proposed pricing mechanism\nunderlines the importance of jumps in cryptocurrency markets.\n"
    },
    {
        "paper_id": 2009.11064,
        "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers",
        "title": "Simulation-based optimisation of the timing of loan recovery across\n  different portfolios",
        "comments": "Accepted by the journal \"Expert Systems with Applications\". 25 pages\n  (including appendix), 9 figures. arXiv admin note: text overlap with older\n  arXiv:1907.12615",
        "journal-ref": "Expert Systems with Applications (2021)",
        "doi": "10.1016/j.eswa.2021.114878",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A novel procedure is presented for the objective comparison and evaluation of\na bank's decision rules in optimising the timing of loan recovery. This\nprocedure is based on finding a delinquency threshold at which the financial\nloss of a loan portfolio (or segment therein) is minimised. Our procedure is an\nexpert system that incorporates the time value of money, costs, and the\nfundamental trade-off between accumulating arrears versus forsaking future\ninterest revenue. Moreover, the procedure can be used with different\ndelinquency measures (other than payments in arrears), thereby allowing an\nindirect comparison of these measures. We demonstrate the system across a range\nof credit risk scenarios and portfolio compositions. The computational results\nshow that threshold optima can exist across all reasonable values of both the\npayment probability (default risk) and the loss rate (loan collateral). In\naddition, the procedure reacts positively to portfolios afflicted by either\nsystematic defaults (such as during an economic downturn) or episodic\ndelinquency (i.e., cycles of curing and re-defaulting). In optimising a\nportfolio's recovery decision, our procedure can better inform the quantitative\naspects of a bank's collection policy than relying on arbitrary discretion\nalone.\n"
    },
    {
        "paper_id": 2009.11075,
        "authors": "Anastasios Petropoulos, Vassilis Siakoulis, Konstantinos P. Panousis,\n  Loukas Papadoulas, and Sotirios Chatzis",
        "title": "A Deep Learning Approach for Dynamic Balance Sheet Stress Testing",
        "comments": "3rd ACM International Conference on AI in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the aftermath of the financial crisis, supervisory authorities have\nconsiderably altered the mode of operation of financial stress testing. Despite\nthese efforts, significant concerns and extensive criticism have been raised by\nmarket participants regarding the considered unrealistic methodological\nassumptions and simplifications. Current stress testing methodologies attempt\nto simulate the risks underlying a financial institution's balance sheet by\nusing several satellite models. This renders their integration a really\nchallenging task, leading to significant estimation errors. Moreover, advanced\nstatistical techniques that could potentially capture the non-linear nature of\nadverse shocks are still ignored. This work aims to address these criticisms\nand shortcomings by proposing a novel approach based on recent advances in Deep\nLearning towards a principled method for Dynamic Balance Sheet Stress Testing.\nExperimental results on a newly collected financial/supervisory dataset,\nprovide strong empirical evidence that our paradigm significantly outperforms\ntraditional approaches; thus, it is capable of more accurately and efficiently\nsimulating real world scenarios.\n"
    },
    {
        "paper_id": 2009.11189,
        "authors": "Xiao Yang, Weiqing Liu, Dong Zhou, Jiang Bian and Tie-Yan Liu",
        "title": "Qlib: An AI-oriented Quantitative Investment Platform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantitative investment aims to maximize the return and minimize the risk in\na sequential trading period over a set of financial instruments. Recently,\ninspired by rapid development and great potential of AI technologies in\ngenerating remarkable innovation in quantitative investment, there has been\nincreasing adoption of AI-driven workflow for quantitative research and\npractical investment. In the meantime of enriching the quantitative investment\nmethodology, AI technologies have raised new challenges to the quantitative\ninvestment system. Particularly, the new learning paradigms for quantitative\ninvestment call for an infrastructure upgrade to accommodate the renovated\nworkflow; moreover, the data-driven nature of AI technologies indeed indicates\na requirement of the infrastructure with more powerful performance;\nadditionally, there exist some unique challenges for applying AI technologies\nto solve different tasks in the financial scenarios. To address these\nchallenges and bridge the gap between AI technologies and quantitative\ninvestment, we design and develop Qlib that aims to realize the potential,\nempower the research, and create the value of AI technologies in quantitative\ninvestment.\n"
    },
    {
        "paper_id": 2009.11367,
        "authors": "Cheng Peng, Young Shin Kim, Stefan Mittnik",
        "title": "Portfolio Optimization on Multivariate Regime Switching GARCH Model with\n  Normal Tempered Stable Innovation",
        "comments": null,
        "journal-ref": "J. Risk Financial Manag. 2022, 15(5), 230",
        "doi": "10.3390/jrfm15050230",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses simulation-based portfolio optimization to mitigate the left\ntail risk of the portfolio. The contribution is twofold. (i) We propose the\nMarkov regime-switching GARCH model with multivariate normal tempered stable\ninnovation (MRS-MNTS-GARCH) to accommodate fat tails, volatility clustering and\nregime switch. The volatility of each asset independently follows the\nregime-switch GARCH model, while the correlation of joint innovation of the\nGARCH models follows the Hidden Markov Model. (ii) We use tail risk measures,\nnamely conditional value-at-risk (CVaR) and conditional drawdown-at-risk\n(CDaR), in the portfolio optimization. The optimization is performed with the\nsample paths simulated by the MRS-MNTS-GARCH model. We conduct an empirical\nstudy on the performance of optimal portfolios. Out-of-sample tests show that\nthe optimal portfolios with tail measures outperform the optimal portfolio with\nstandard deviation measure and the equally weighted portfolio in various\nperformance measures. The out-of-sample performance of the optimal portfolios\nis also more robust to suboptimality on the efficient frontier.\n"
    },
    {
        "paper_id": 2009.11557,
        "authors": "Ben Zhe Wang, Jeffrey Sheen, Stefan Tr\\\"uck, Shih-Kang Chao, Wolfgang\n  Karl H\\\"ardle",
        "title": "A note on the impact of news on US household inflation expectations",
        "comments": null,
        "journal-ref": "Macroeconomic Dynamics, Volume 24, Issue 4 June 2020, pp. 995 to\n  1015",
        "doi": "10.1017/S1365100518000482",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Monthly disaggregated US data from 1978 to 2016 reveals that exposure to news\non inflation and monetary policy helps to explain inflation expectations. This\nremains true when controlling for household personal characteristics,\nperceptions of government policy effectiveness, future interest rates and\nunemployment expectations, and sentiment. We find an asymmetric impact of news\non inflation and monetary policy after 1983, with news on rising inflation and\neasier monetary policy having a stronger effect in comparison to news on\nlowering inflation and tightening monetary policy. Our results indicate the\nimpact on inflation expectations of monetary policy news manifested through\nconsumer sentiment during the lower bound period.\n"
    },
    {
        "paper_id": 2009.1166,
        "authors": "Nick James and Max Menzies",
        "title": "Association between COVID-19 cases and international equity indices",
        "comments": "Accepted manuscript. Minor revisions compared to v1. Equal\n  contribution",
        "journal-ref": "Physica D: Nonlinear Phenomena 417 (2021) 132809",
        "doi": "10.1016/j.physd.2020.132809",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the impact of COVID-19 on the populations and equity\nmarkets of 92 countries. We compare country-by-country equity market dynamics\nto cumulative COVID-19 case and death counts and new case trajectories. First,\nwe examine the multivariate time series of cumulative cases and deaths,\nparticularly regarding their changing structure over time. We reveal\nsimilarities between the case and death time series, and key dates that the\nstructure of the time series changed. Next, we classify new case time series,\ndemonstrate five characteristic classes of trajectories, and quantify\ndiscrepancy between them with respect to the behavior of waves of the disease.\nFinally, we show there is no relationship between countries' equity market\nperformance and their success in managing COVID-19. Each country's equity index\nhas been unresponsive to the domestic or global state of the pandemic. Instead,\nthese indices have been highly uniform, with most movement in March.\n"
    },
    {
        "paper_id": 2009.11743,
        "authors": "Philipp Koch, Clemens Fessler",
        "title": "A test for Heckscher-Ohlin using value-added exports",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical evidence for the Heckscher-Ohlin model has been inconclusive. We\ntest whether the predictions of the Heckscher-Ohlin Theorem with respect to\nlabor and capital find support in value-added trade. Defining labor-capital\nintensities and endowments as the ratio of hours worked to the nominal capital\nstock, we find evidence against Heckscher-Ohlin. However, taking the ratio of\ntotal factor compensations, and thus accounting for differences in\ntechnologies, we find strong support for it. That is, labor-abundant countries\ntend to export value-added in goods of labor-intensive industries. Moreover,\ndifferentiating between broad industries, we find support for nine out of\ntwelve industries.\n"
    },
    {
        "paper_id": 2009.11867,
        "authors": "Samuel Dooley, John P. Dickerson",
        "title": "The Affiliate Matching Problem: On Labor Markets where Firms are Also\n  Interested in the Placement of Previous Workers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many labor markets, workers and firms are connected via affiliative\nrelationships. A management consulting firm wishes to both accept the best new\nworkers but also place its current affiliated workers at strong firms.\nSimilarly, a research university wishes to hire strong job market candidates\nwhile also placing its own candidates at strong peer universities. We model\nthis affiliate matching problem in a generalization of the classic stable\nmarriage setting by permitting firms to state preferences over not just which\nworkers to whom they are matched, but also to which firms their affiliated\nworkers are matched. Based on results from a human survey, we find that\nparticipants (acting as firms) give preference to their own affiliate workers\nin surprising ways that violate some assumptions of the classical stable\nmarriage problem. This motivates a nuanced discussion of how stability could be\ndefined in affiliate matching problems; we give an example of a marketplace\nwhich admits a stable match under one natural definition of stability, and does\nnot for that same marketplace under a different, but still natural, definition.\nWe conclude by setting a research agenda toward the creation of a centralized\nclearing mechanism in this general setting.\n"
    },
    {
        "paper_id": 2009.11917,
        "authors": "Benson Tsz Kin Leung",
        "title": "Learning in a Small/Big World",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complexity and limited ability have profound effect on how we learn and make\ndecisions under uncertainty. Using the theory of finite automaton to model\nbelief formation, this paper studies the characteristics of optimal learning\nbehavior in small and big worlds, where the complexity of the environment is\nlow and high, respectively, relative to the cognitive ability of the decision\nmaker. Optimal behavior is well approximated by the Bayesian benchmark in very\nsmall world but is more different as the world gets bigger. In addition, in big\nworlds, the optimal learning behavior could exhibit a wide range of\nwell-documented non-Bayesian learning behavior, including the use of\nheuristics, correlation neglect, persistent over-confidence, inattentive\nlearning, and other behaviors of model simplification or misspecification.\nThese results establish a clear and testable relationship among the prominence\nof non-Bayesian learning behavior, complexity, and cognitive ability.\n"
    },
    {
        "paper_id": 2009.12092,
        "authors": "Meng-Jou Lu, Cathy Yi-Hsuan Chen, Wolfgang Karl H\\\"ardle",
        "title": "Copula-Based Factor Model for Credit Risk Analysis",
        "comments": null,
        "journal-ref": "Review of Quantitative Finance and Accounting, 49, pages 949 to\n  971, 2017",
        "doi": "10.1007/s11156-016-0613-x",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A standard quantitative method to access credit risk employs a factor model\nbased on joint multivariate normal distribution properties. By extending a\none-factor Gaussian copula model to make a more accurate default forecast, this\npaper proposes to incorporate a state-dependent recovery rate into the\nconditional factor loading, and model them by sharing a unique common factor.\nThe common factor governs the default rate and recovery rate simultaneously and\ncreates their association implicitly. In accordance with Basel III, this paper\nshows that the tendency of default is more governed by systematic risk rather\nthan idiosyncratic risk during a hectic period. Among the models considered,\nthe one with random factor loading and a state-dependent recovery rate turns\nout to be the most superior on the default prediction.\n"
    },
    {
        "paper_id": 2009.12121,
        "authors": "Xinwen Ni, Wolfgang Karl H\\\"ardle, Taojun Xie",
        "title": "A Machine Learning Based Regulatory Risk Index for Cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies' values often respond aggressively to major policy changes,\nbut none of the existing indices informs on the market risks associated with\nregulatory changes. In this paper, we quantify the risks originating from new\nregulations on FinTech and cryptocurrencies (CCs), and analyse their impact on\nmarket dynamics. Specifically, a Cryptocurrency Regulatory Risk IndeX (CRRIX)\nis constructed based on policy-related news coverage frequency. The unlabeled\nnews data are collected from the top online CC news platforms and further\nclassified using a Latent Dirichlet Allocation model and Hellinger distance.\nOur results show that the machine-learning-based CRRIX successfully captures\nmajor policy-changing moments. The movements for both the VCRIX, a market\nvolatility index, and the CRRIX are synchronous, meaning that the CRRIX could\nbe helpful for all participants in the cryptocurrency market. The algorithms\nand Python code are available for research purposes on www.quantlet.de.\n"
    },
    {
        "paper_id": 2009.12129,
        "authors": "Shi Chen, Cathy Yi-Hsuan Chen, Wolfgang Karl H\\\"ardle",
        "title": "A first econometric analysis of the CRIX family",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In order to price contingent claims one needs to first understand the\ndynamics of these indices. Here we provide a first econometric analysis of the\nCRIX family within a time-series framework. The key steps of our analysis\ninclude model selection, estimation and testing. Linear dependence is removed\nby an ARIMA model, the diagnostic checking resulted in an ARIMA(2,0,2) model\nfor the available sample period from Aug 1st, 2014 to April 6th, 2016. The\nmodel residuals showed the well known phenomenon of volatility clustering.\nTherefore a further refinement lead us to an ARIMA(2,0,2)-t-GARCH(1,1) process.\nThis specification conveniently takes care of fat-tail properties that are\ntypical for financial markets. The multivariate GARCH models are implemented on\nthe CRIX index family to explore the interaction.\n"
    },
    {
        "paper_id": 2009.12155,
        "authors": "Evans Rozario, Samuel Holt, James West, Shaun Ng",
        "title": "A Decade of Evidence of Trend Following Investing in Cryptocurrencies",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrency markets have many of the characteristics of 20th century\ncommodities markets, making them an attractive candidate for trend following\nstrategies. We present a decade of evidence from the infancy of bitcoin,\nshowcasing the potential investor returns in cryptocurrency trend following,\n255% walkforward annualised returns. We find that cryptocurrencies offer\nsimilar returns characteristics to commodities with similar risk-adjusted\nreturns, and strong bear market diversification against traditional equities.\nCode available at https://github.com/Globe-Research/bittrends.\n"
    },
    {
        "paper_id": 2009.12217,
        "authors": "Swen Kuh, Grace S. Chiu, Anton H. Westveld",
        "title": "Latent Causal Socioeconomic Health Index",
        "comments": "51 pages with supplementary materials. Additional simulation studies\n  section and updated figures. arXiv admin note: substantial text overlap with\n  arXiv:1911.00512",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. Motivated by the need for a holistic\nnational well-being index, we build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. LHFI integratively models the relationship between metrics, latent\nhealth, and covariates that drive the notion of health. In this paper, the LHFI\nstructure is integrated with spatial modeling and statistical causal modeling.\nOur efforts are focused on developing the integrated framework to facilitate\nthe understanding of how an observational continuous variable might have\ncausally affected a latent trait that exhibits spatial correlation. A novel\nvisualization technique to evaluate covariate balance is also introduced for\nthe case of a continuous policy (treatment) variable. Our resulting LACSH\nframework and visualization tool are illustrated through two global case\nstudies on national socioeconomic health (latent trait), each with various\nmetrics and covariates pertaining to different aspects of societal health, and\nthe treatment variable being mandatory maternity leave days and government\nexpenditure on healthcare, respectively. We validate our model by two\nsimulation studies. All approaches are structured in a Bayesian hierarchical\nframework and results are obtained by Markov chain Monte Carlo techniques.\n"
    },
    {
        "paper_id": 2009.12274,
        "authors": "Manuel Guerra and Alexandra B. Moura",
        "title": "Reinsurance of multiple risks with generic dependence structures",
        "comments": "A mistake on Proposition 3.2 was corrected. A new section on\n  computation of optimal strategies was added. Several additions to the text.\n  Examples were corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal reinsurance problem from the point of view of a\ndirect insurer owning several dependent risks, assuming a maximal expected\nutility criterion and independent negotiation of reinsurance for each risk.\nWithout any particular hypothesis on the dependency structure, we show that\noptimal treaties exist in a class of independent randomized contracts. We\nderive optimality conditions and show that under mild assumptions the optimal\ncontracts are of classical (non-randomized) type. A specific for mof the\noptimality conditions applies in that case. We present a numerical scheme to\nsolve the optimality conditions.\n"
    },
    {
        "paper_id": 2009.12335,
        "authors": "Areejit Samal, Hirdesh K. Pharasi, Sarath Jyotsna Ramaia, Harish\n  Kannan, Emil Saucan, J\\\"urgen Jost, and Anirban Chakraborti",
        "title": "Network geometry and market instability",
        "comments": "34 pages, 17 figures, including Supplementary Material",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The complexity of financial markets arise from the strategic interactions\namong agents trading stocks, which manifest in the form of vibrant correlation\npatterns among stock prices. Over the past few decades, complex financial\nmarkets have often been represented as networks whose interacting pairs of\nnodes are stocks, connected by edges that signify the correlation strengths.\nHowever, we often have interactions that occur in groups of three or more\nnodes, and these cannot be described simply by pairwise interactions but we\nalso need to take the relations between these interactions into account. Only\nrecently, researchers have started devoting attention to the higher-order\narchitecture of complex financial systems, that can significantly enhance our\nability to estimate systemic risk as well as measure the robustness of\nfinancial systems in terms of market efficiency. Geometry-inspired network\nmeasures, such as the Ollivier-Ricci curvature and Forman-Ricci curvature, can\nbe used to capture the network fragility and continuously monitor financial\ndynamics. Here, we explore the utility of such discrete Ricci curvatures in\ncharacterizing the structure of financial systems, and further, evaluate them\nas generic indicators of the market instability. For this purpose, we examine\nthe daily returns from a set of stocks comprising the USA S&P-500 and the\nJapanese Nikkei-225 over a 32-year period, and monitor the changes in the\nedge-centric network curvatures. We find that the different geometric measures\ncapture well the system-level features of the market and hence we can\ndistinguish between the normal or `business-as-usual' periods and all the major\nmarket crashes. This can be very useful in strategic designing of financial\nsystems and regulating the markets in order to tackle financial instabilities.\n"
    },
    {
        "paper_id": 2009.1235,
        "authors": "Anatoliy Swishchuk, Ana Roldan-Contreras, Elham Soufiani, Guillermo\n  Martinez, Mohsen Seifi, Nishant Agrawal, and Yao Yao",
        "title": "Practical Option Valuations of Futures Contracts with Negative\n  Underlying Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we propose two alternatives to Black 76 to value European option future\ncontracts in which the underlying market prices can be negative or mean\nreverting. The two proposed models are Ornstein-Uhlenbeck (OU) and continuous\ntime GARCH (generalized autoregressive conditionally heteroscedastic). We then\nanalyse the values and compare them with Black 76, the most commonly used\nmodel, when the underlying market prices are positive\n"
    },
    {
        "paper_id": 2009.12838,
        "authors": "Mario Ghossoub and David Saunders",
        "title": "On the Continuity of the Feasible Set Mapping in Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider the set of probability measures with given marginal distributions on\nthe product of two complete, separable metric spaces, seen as a correspondence\nwhen the marginal distributions vary. In problems of optimal transport,\ncontinuity of this correspondence from marginal to joint distributions is often\ndesired, in light of Berge's Maximum Theorem, to establish continuity of the\nvalue function in the marginal distributions, as well as stability of the set\nof optimal transport plans. Bergin (1999) established the continuity of this\ncorrespondence, and in this note, we present a novel and considerably shorter\nproof of this important result. We then examine an application to an assignment\ngame (transferable utility matching problem) with unknown type distributions.\n"
    },
    {
        "paper_id": 2009.12901,
        "authors": "Chris Kenyon",
        "title": "Client engineering of XVA in crisis and normality: Restructuring,\n  Mandatory Breaks and Resets",
        "comments": "14 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crises challenge client XVA management when continuous collateralization is\nnot possible because a derivative locks in the client credit level and the\nprovider's funding level, on the trade date, for the life of the trade. We\nprice XVA reduction strategies from the client point of view comparing multiple\ntrade strategies using Mandatory Breaks or Restructuring, to modifications of a\nsingle trade using a Reset. We analyse previous crises and recovery of CDS to\ninform our numerical examples. In our numerical examples Resets can be twice as\neffective as Mandatory Break/Restructuring if there is no credit recovery. When\nrecovery is at least 1/3 of the credit shock then Mandatory Break/Restructuring\ncan be more effective.\n"
    },
    {
        "paper_id": 2009.13076,
        "authors": "Ajit Mahata, Anish rai, Om Prakash, Md Nurujjaman",
        "title": "Modeling and analysis of the effect of COVID-19 on the stock price: V\n  and L-shape recovery",
        "comments": null,
        "journal-ref": "Physica A, Volume 574, 15 July 2021, 126008",
        "doi": "10.1016/j.physa.2021.126008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The emergence of the COVID-19 pandemic, a new and novel risk factor, leads to\nthe stock price crash due to the investors' rapid and synchronous sell-off.\nHowever, within a short period, the quality sectors start recovering from the\nbottom. A stock price model has been developed during such crises based on the\nnet-fund-flow ($\\Psi_t$) due to institutional investors, and financial\nantifragility ($\\phi$) of a company. We assume that during the crash, the stock\nprice fall is independent of the $\\phi$. We study the effects of shock lengths\nand $\\phi$ on the stock price during the crises period using the $\\Psi_t$\nobtained from synthetic and real fund flow data. We observed that the\npossibility of recovery of stock with $\\phi>0$, termed as quality stock,\ndecreases with an increase in shock-length beyond a specific period. A quality\nstock with higher $\\phi$ shows V-shape recovery and outperform others. The\nshock length and recovery period of quality stock are almost equal that is seen\nin the Indian market. Financially stressed stocks, i.e., the stocks with\n$\\phi<0$, show L-shape recovery during the pandemic. The stock data and model\nanalysis shows that the investors, in uncertainty like COVID-19, invest in\nquality stocks to restructure their portfolio to reduce the risk. The study may\nhelp the investors to make the right investment decision during a crisis.\n"
    },
    {
        "paper_id": 2009.13091,
        "authors": "Andrew Zeitlin",
        "title": "Teacher turnover in Rwanda",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite widely documented shortfalls of teacher skills and effort, there is\nlittle systematic evidence of rates of teacher turnover in low-income\ncountries. I investigate the incidence and consequences of teacher turnover in\nRwandan public primary schools over the period from 2016-2019. To do so, I\ncombine the universe of teacher placement records with student enrollment\nfigures and school-average Primary Leaving Exam scores in a nationally\nrepresentative sample of 259 schools. Results highlight five features of\nteacher turnover. First, rates of teacher turnover are high: annually, 20\npercent of teachers separate from their jobs, of which 11 percent exit from the\npublic-sector teaching workforce. Second, the burden of teacher churn is higher\nin schools with low learning levels and, perhaps surprisingly, in low\npupil-teacher-ratio schools. Third, teacher turnover is concentrated among\nearly-career teachers, male teachers, and those assigned to teach Math. Fourth,\nreplacing teachers quickly after they exit is a challenge; 23 percent of\nexiting teachers are not replaced the following year. And fifth, teacher\nturnover is associated with subsequent declines in learning outcomes. On\naverage, the loss of a teacher is associated with a reduction in learning\nlevels of 0.05 standard deviations. In addition to class-size increases, a\npossible mechanism for these learning outcomes is the prevalence of teachers\nteaching outside of their areas of subject expertise: in any given year, at\nleast 21 percent of teachers teach in subjects in which they have not been\ntrained. Taken together, these results suggest that the problem of teacher\nturnover is substantial in magnitude and consequential for learning outcomes in\nschools.\n"
    },
    {
        "paper_id": 2009.13103,
        "authors": "Plamen Nikolov, Andreas Pape, Ozlem Tonguc, Charlotte Williams",
        "title": "Predictors of Social Distancing and Mask-Wearing Behavior: Panel Survey\n  in Seven U.S. States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents preliminary summary results from a longitudinal study of\nparticipants in seven U.S. states during the COVID-19 pandemic. In addition to\nstandard socio-economic characteristics, we collect data on various economic\npreference parameters: time, risk, and social preferences, and risk perception\nbiases. We pay special attention to predictors that are both important drivers\nof social distancing and are potentially malleable and susceptible to policy\nlevers. We note three important findings: (1) demographic characteristics exert\nthe largest influence on social distancing measures and mask-wearing, (2) we\nshow that individual risk perception and cognitive biases exert a critical role\nin influencing the decision to adopt social distancing measures, (3) we\nidentify important demographic groups that are most susceptible to changing\ntheir social distancing behaviors. These findings can help inform the design of\npolicy interventions regarding targeting specific demographic groups, which can\nhelp reduce the transmission speed of the COVID-19 virus.\n"
    },
    {
        "paper_id": 2009.13215,
        "authors": "Xiu Xu, Andrija Mihoci, Wolfgang Karl H\\\"ardle",
        "title": "lCARE -- localizing Conditional AutoRegressive Expectiles",
        "comments": null,
        "journal-ref": "Journal of Empirical Finance, Volume 48, September 2018, Pages\n  198-220",
        "doi": "10.1016/j.jempfin.2018.06.006",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We account for time-varying parameters in the conditional expectile-based\nvalue at risk (EVaR) model. The EVaR downside risk is more sensitive to the\nmagnitude of portfolio losses compared to the quantile-based value at risk\n(QVaR). Rather than fitting the expectile models over ad-hoc fixed data\nwindows, this study focuses on parameter instability of tail risk dynamics by\nutilising a local parametric approach. Our framework yields a data-driven\noptimal interval length at each time point by a sequential test. Empirical\nevidence at three stock markets from 2005-2016 shows that the selected lengths\naccount for approximately 3-6 months of daily observations. This method\nperforms favorable compared to the models with one-year fixed intervals, as\nwell as quantile based candidates while employing a time invariant portfolio\nprotection (TIPP) strategy for the DAX, FTSE 100 and S&P 500 portfolios. The\ntail risk measure implied by our model finally provides valuable insights for\nasset allocation and portfolio insurance.\n"
    },
    {
        "paper_id": 2009.13222,
        "authors": "Lining Yu, Wolfgang Karl H\\\"ardle, Lukas Borke, Thijs Benschop",
        "title": "An AI approach to measuring financial risk",
        "comments": null,
        "journal-ref": "The Singapore Economic Review (2019): pp. 1 to 21",
        "doi": "10.1142/S0217590819500668",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  AI artificial intelligence brings about new quantitative techniques to assess\nthe state of an economy. Here we describe a new measure for systemic risk: the\nFinancial Risk Meter (FRM). This measure is based on the penalization parameter\n(lambda) of a linear quantile lasso regression. The FRM is calculated by taking\nthe average of the penalization parameters over the 100 largest US publicly\ntraded financial institutions. We demonstrate the suitability of this AI based\nrisk measure by comparing the proposed FRM to other measures for systemic risk,\nsuch as VIX, SRISK and Google Trends. We find that mutual Granger causality\nexists between the FRM and these measures, which indicates the validity of the\nFRM as a systemic risk measure. The implementation of this project is carried\nout using parallel computing, the codes are published on www.quantlet.de with\nkeyword FRM. The R package RiskAnalytics is another tool with the purpose of\nintegrating and facilitating the research, calculation and analysis methods\naround the FRM project. The visualization and the up-to-date FRM can be found\non hu.berlin/frm.\n"
    },
    {
        "paper_id": 2009.13235,
        "authors": "Daniel Perez, Sam M. Werner, Jiahua Xu, Benjamin Livshits",
        "title": "Liquidations: DeFi on a Knife-edge",
        "comments": null,
        "journal-ref": "Financial Cryptography and Data Security (2021) 457-476",
        "doi": "10.1007/978-3-662-64331-0_24",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The trustless nature of permissionless blockchains renders\novercollateralization a key safety component relied upon by decentralized\nfinance (DeFi) protocols. Nonetheless, factors such as price volatility may\nundermine this mechanism. In order to protect protocols from suffering losses,\nundercollateralized positions can be liquidated. In this paper, we present the\nfirst in-depth empirical analysis of liquidations on protocols for loanable\nfunds (PLFs). We examine Compound, one of the most widely used PLFs, for a\nperiod starting from its conception to September 2020. We analyze participants'\nbehavior and risk-appetite in particular, to elucidate recent developments in\nthe dynamics of the protocol. Furthermore, we assess how this has changed with\na modification in Compound's incentive structure and show that variations of\nonly 3% in an asset's dollar price can result in over 10m USD becoming\nliquidable. To further understand the implications of this, we investigate the\nefficiency of liquidators. We find that liquidators' efficiency has improved\nsignificantly over time, with currently over 70% of liquidable positions being\nimmediately liquidated. Lastly, we provide a discussion on how a false sense of\nsecurity fostered by a misconception of the stability of non-custodial\nstablecoins, increases the overall liquidation risk faced by Compound\nparticipants.\n"
    },
    {
        "paper_id": 2009.13384,
        "authors": "Michael B\\\"ucker and Gero Szepannek and Alicja Gosiewska and\n  Przemyslaw Biecek",
        "title": "Transparency, Auditability and eXplainability of Machine Learning Models\n  in Credit Scoring",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major requirement for credit scoring models is to provide a maximally\naccurate risk prediction. Additionally, regulators demand these models to be\ntransparent and auditable. Thus, in credit scoring, very simple predictive\nmodels such as logistic regression or decision trees are still widely used and\nthe superior predictive power of modern machine learning algorithms cannot be\nfully leveraged. Significant potential is therefore missed, leading to higher\nreserves or more credit defaults. This paper works out different dimensions\nthat have to be considered for making credit scoring models understandable and\npresents a framework for making ``black box'' machine learning models\ntransparent, auditable and explainable. Following this framework, we present an\noverview of techniques, demonstrate how they can be applied in credit scoring\nand how results compare to the interpretability of score cards. A real world\ncase study shows that a comparable degree of interpretability can be achieved\nwhile machine learning techniques keep their ability to improve predictive\npower.\n"
    },
    {
        "paper_id": 2009.1339,
        "authors": "Raymond Ka-Kay Pang, Oscar Granados, Harsh Chhajer and Erika Fille\n  Legara",
        "title": "An analysis of network filtering methods to sovereign bond yields during\n  COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we investigate the impact of the COVID-19 pandemic on sovereign\nbond yields. We consider the temporal changes from financial correlations using\nnetwork filtering methods. These methods consider a subset of links within the\ncorrelation matrix, which gives rise to a network structure. We use sovereign\nbond yield data from 17 European countries between the 2010 and 2020 period. We\nfind the mean correlation to decrease across all filtering methods during the\nCOVID-19 period. We also observe a distinctive trend between filtering methods\nunder multiple network centrality measures. We then relate the significance of\neconomic and health variables towards filtered networks within the COVID-19\nperiod. Under an exponential random graph model, we are able to identify key\nrelations between economic groups across different filtering methods.\n"
    },
    {
        "paper_id": 2009.13484,
        "authors": "Carlos B. Carneiro, I\\'uri H. Ferreira, Marcelo C. Medeiros, Henrique\n  F. Pires and Eduardo Zilberman",
        "title": "Lockdown effects in US states: an artificial counterfactual approach",
        "comments": "Updated versions of this paper will be available on\n  http://139.82.34.174/mcm/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We adopt an artificial counterfactual approach to assess the impact of\nlockdowns on the short-run evolution of the number of cases and deaths in some\nUS states. To do so, we explore the different timing in which US states adopted\nlockdown policies, and divide them among treated and control groups. For each\ntreated state, we construct an artificial counterfactual. On average, and in\nthe very short-run, the counterfactual accumulated number of cases would be two\ntimes larger if lockdown policies were not implemented.\n"
    },
    {
        "paper_id": 2009.13595,
        "authors": "Kasun Chandrarathna, Arman Edalati, AhmadReza Fourozan tabar",
        "title": "Forecasting Short-term load using Econometrics time series model with\n  T-student Distribution",
        "comments": "6 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By significant improvements in modern electrical systems, planning for unit\ncommitment and power dispatching of them are two big concerns between the\nresearchers. Short-term load forecasting plays a significant role in planning\nand dispatching them. In recent years, numerous works have been done on\nShort-term load forecasting. Having an accurate model for predicting the load\ncan be beneficial for optimizing the electrical sources and protecting energy.\nSeveral models such as Artificial Intelligence and Statistics model have been\nused to improve the accuracy of load forecasting. Among the statistics models,\ntime series models show a great performance. In this paper, an Autoregressive\nintegrated moving average (SARIMA) - generalized autoregressive conditional\nheteroskedasticity (GARCH) model as a powerful tool for modeling the\nconditional mean and volatility of time series with the T-student Distribution\nis used to forecast electric load in short period of time. The attained model\nis compared with the ARIMA model with Normal Distribution. Finally, the\neffectiveness of the proposed approach is validated by applying real electric\nload data from the Electric Reliability Council of Texas (ERCOT). KEYWORDS:\nElectricity load, Forecasting, Econometrics Time Series Forecasting, SARIMA\n"
    },
    {
        "paper_id": 2009.14097,
        "authors": "Reza Mousavi and Bin Gu",
        "title": "When Local Governments' Stay-at-Home Orders Meet the White House's\n  \"Opening Up America Again\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On April 16th, The White House launched \"Opening up America Again\" (OuAA)\ncampaign while many U.S. counties had stay-at-home orders in place. We created\na panel data set of 1,563 U.S. counties to study the impact of U.S. counties'\nstay-at-home orders on community mobility before and after The White House's\ncampaign to reopen the country. Our results suggest that before the OuAA\ncampaign stay-at-home orders brought down time spent in retail and recreation\nbusinesses by about 27% for typical conservative and liberal counties. However,\nafter the launch of OuAA campaign, the time spent at retail and recreational\nbusinesses in a typical conservative county increased significantly more than\nin liberal counties (15% increase in a typical conservative county Vs. 9%\nincrease in a typical liberal county). We also found that in conservative\ncounties with stay-at-home orders in place, time spent at retail and\nrecreational businesses increased less than that of conservative counties\nwithout stay-at-home orders. These findings illuminate to what extent\nresidents' political ideology could determine to what extent they follow local\norders and to what extent the White House's OuAA campaign polarized the\nobedience between liberal and conservative counties. The silver lining in our\nstudy is that even when the federal government was reopening the country, the\nlocal authorities that enforced stay-at-home restrictions were to some extent\neffective.\n"
    },
    {
        "paper_id": 2009.14113,
        "authors": "Azwar Abdulsalam, Gowri Jayprakash, Abhijeet Chandra",
        "title": "On the Pricing of Currency Options under Variance Gamma Process",
        "comments": "15 pages, 1 figure, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The pricing of currency options is largely dependent on the dynamic\nrelationship between a pair of currencies. Typically, the pricing of options\nwith payoffs dependent on multi-assets becomes tricky for reasons such as the\nnon-Gaussian distribution of financial variable and non-linear macroeconomic\nrelations between these markets. We study the options based on the currency\npair US dollar and Indian rupee (USD-INR) and test several pricing formulas to\nevaluate the performance under different volatility regimes. We show the\nperformance of the variance gamma and the symmetric variance gamma models\nduring different volatility periods as well as for different moneyness, in\ncomparison to the modified Black-Scholes model. In all cases, variance gamma\nmodel outperforms Black-Scholes. This can be attributed to the control of\nkurtosis and skewness of the distribution that is possible using the variance\ngamma model. Our findings support the superiority of variance gamma process of\ncurrency option pricing in better risk management strategies.\n"
    },
    {
        "paper_id": 2009.14136,
        "authors": "Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay",
        "title": "Time your hedge with Deep Reinforcement Learning",
        "comments": "9 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Can an asset manager plan the optimal timing for her/his hedging strategies\ngiven market conditions? The standard approach based on Markowitz or other more\nor less sophisticated financial rules aims to find the best portfolio\nallocation thanks to forecasted expected returns and risk but fails to fully\nrelate market conditions to hedging strategies decision. In contrast, Deep\nReinforcement Learning (DRL) can tackle this challenge by creating a dynamic\ndependency between market information and hedging strategies allocation\ndecisions. In this paper, we present a realistic and augmented DRL framework\nthat: (i) uses additional contextual information to decide an action, (ii) has\na one period lag between observations and actions to account for one day lag\nturnover of common asset managers to rebalance their hedge, (iii) is fully\ntested in terms of stability and robustness thanks to a repetitive train test\nmethod called anchored walk forward training, similar in spirit to k fold cross\nvalidation for time series and (iv) allows managing leverage of our hedging\nstrategy. Our experiment for an augmented asset manager interested in sizing\nand timing his hedges shows that our approach achieves superior returns and\nlower risk.\n"
    },
    {
        "paper_id": 2009.14278,
        "authors": "Victor Olkhov",
        "title": "Price, Volatility and the Second-Order Economic Theory",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the price probability measure {\\eta}(p;t) that defines the mean\nprice p(1;t), mean square price p(2;t), price volatility {\\sigma}p2(t)and all\nprice n-th statistical moments p(n;t) as ratio of sums of n-th degree values\nC(n;t) and volumes U(n;t) of market trades aggregated during certain time\ninterval {\\Delta}. The definition of the mean price p(1;t) coincides with\ndefinition of the volume weighted average price (VWAP) introduced at least 30\nyears ago. We show that price volatility {\\sigma}p2(t) forecasting requires\nmodeling evolution of the sums of second-degree values C(2;t) and volumes\nU(2;t). We call this model as second-order economic theory. We use numerical\ncontinuous risk ratings as ground for risk assessment of economic agents and\ndistribute agents by risk ratings as coordinates. We introduce continuous\neconomic media approximation of squares of values and volumes of agents trades\nand their flows aggregated during time interval {\\Delta}. We take into account\nexpectations that govern agents trades and introduce aggregated expectations\nalike to aggregated trades. We derive equations for continuous economic media\napproximation on the second-degree trades. In the linear approximation we\nderive mean square price p(2;t) and volatility {\\sigma}p2(t) disturbances as\nfunctions of the first and second-degree trades disturbances. Description of\neach next n-th price statistical moment p(n;t) with respect to the unit price\nmeasure {\\eta}(p;t) depends on sums of n-th degree values C(n;t) and volumes\nU(n;t) of market trades and hence requires development of the corresponding\nn-th order economic theory.\n"
    },
    {
        "paper_id": 2009.14282,
        "authors": "Tarun Bhatia",
        "title": "Predicting Non Farm Employment",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  U.S. Nonfarm employment is considered one of the key indicators for assessing\nthe state of the labor market. Considerable deviations from the expectations\ncan cause market moving impacts. In this paper, the total U.S. nonfarm payroll\nemployment is predicted before the release of the BLS employment report. The\ncontent herein outlines the process for extracting predictive features from the\naggregated payroll data and training machine learning models to make accurate\npredictions. Publically available revised employment report by BLS is used as a\nbenchmark. Trained models show excellent behaviour with R2 of 0.9985 and 99.99%\ndirectional accuracy on out of sample periods from January 2012 to March 2020.\n  Keywords Machine Learning; Economic Indicators; Ensembling; Regression, Total\nNonfarm Payroll\n"
    },
    {
        "paper_id": 2009.14378,
        "authors": "Katsuaki Tanabe",
        "title": "Pareto's 80/20 Rule and the Gaussian Distribution",
        "comments": "15 pages, 5 figures",
        "journal-ref": "Physica A 510, 635 (2018)",
        "doi": "10.1016/j.physa.2018.07.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The statistical state for the empirical Pareto's 80/20 rule has been found to\ncorrespond to a normal or Gaussian distribution with a standard deviation that\nis twice the mean. This finding represents large characteristic variations in\nour society and nature. In this distribution, the rule can be also referred to\nas, for example, the 25/5, 45/10, 60/15, or 90/25 rule. In addition, our result\nsuggests the existence of implicit negative contributors.\n"
    },
    {
        "paper_id": 2009.14408,
        "authors": "Amr A. Adly",
        "title": "On The Quest For Economic Prosperity: A Higher Education Strategic\n  Perspective For The Mena Region",
        "comments": "12 pages total. Within these pages, the manuscript contains 11\n  figures and one table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In a fast-changing technology-driven era, drafting an implementable strategic\nroadmap to achieve economic prosperity becomes a real challenge. Although the\nnational and international strategic development plans may vary, they usually\ntarget the improvement of the quality of living standards through boosting the\nnational GDP per capita and the creation of decent jobs. There is no doubt that\nhuman capacity building, through higher education, is vital to the availability\nof highly qualified workforce supporting the implementation of the\naforementioned strategies. In other words, fulfillment of most strategic\ndevelopment plan goals becomes dependent on the drafting and implementation of\nsuccessful higher education strategies. For MENA region countries, this is\nparticularly crucial due to many specific challenges, some of which are\ndifferent from those facing developed nations. More details on the MENA region\nhigher education strategic planning challenges as well as the proposed higher\neducation strategic requirements to support national economic prosperity and\nfulfill the 2030 UN SDGs are given in the paper.\n"
    },
    {
        "paper_id": 2009.14559,
        "authors": "J\\\"orn Sass, Dorothee Westphal",
        "title": "Robust Utility Maximization in a Multivariate Financial Market with\n  Stochastic Drift",
        "comments": "26 pages, 1 figure",
        "journal-ref": "International Journal of Theoretical and Applied Finance 24 (4),\n  2021",
        "doi": "10.1142/S0219024921500205",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a utility maximization problem in a financial market with a\nstochastic drift process, combining a worst-case approach with filtering\ntechniques. Drift processes are difficult to estimate from asset prices, and at\nthe same time optimal strategies in portfolio optimization problems depend\ncrucially on the drift. We approach this problem by setting up a worst-case\noptimization problem with a time-dependent uncertainty set for the drift.\nInvestors assume that the worst possible drift process with values in the\nuncertainty set will occur. This leads to local optimization problems, and the\nresulting optimal strategy needs to be updated continuously in time. We prove a\nminimax theorem for the local optimization problems and derive the optimal\nstrategy. Further, we show how an ellipsoidal uncertainty set can be defined\nbased on filtering techniques and demonstrate that investors need to choose a\nrobust strategy to be able to profit from additional information.\n"
    },
    {
        "paper_id": 2009.14561,
        "authors": "Nektarios Aslanidis, Aurelio F. Bariviera, Alejandro Perez-Laborda",
        "title": "Are cryptocurrencies becoming more interconnected?",
        "comments": "15 pages, 5 figures, 5 tables, supplementary material included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the dynamic market linkages among cryptocurrencies during\nAugust 2015 - July 2020 and finds a substantial increase in market linkages for\nboth returns and volatilities. We use different methodologies to check the\ndifferent aspects of market linkages. Financial and regulatory implications are\ndiscussed.\n"
    },
    {
        "paper_id": 2009.14682,
        "authors": "Bent Flyvbjerg, Alexander Budzier, Daniel Lunn",
        "title": "Regression to the Tail: Why the Olympics Blow Up",
        "comments": "arXiv admin note: text overlap with arXiv:1607.04484",
        "journal-ref": null,
        "doi": "10.1177/0308518X20958724",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Olympic Games are the largest, highest-profile, and most expensive\nmegaevent hosted by cities and nations. Average sports-related costs of hosting\nare $12.0 billion. Non-sports-related costs are typically several times that.\nEvery Olympics since 1960 has run over budget, at an average of 172 percent in\nreal terms, the highest overrun on record for any type of megaproject. The\npaper tests theoretical statistical distributions against empirical data for\nthe costs of the Games, in order to explain the cost risks faced by host cities\nand nations. It is documented, for the first time, that cost and cost overrun\nfor the Games follow a power-law distribution. Olympic costs are subject to\ninfinite mean and variance, with dire consequences for predictability and\nplanning. We name this phenomenon \"regression to the tail\": it is only a matter\nof time until a new extreme event occurs, with an overrun larger than the\nlargest so far, and thus more disruptive and less plannable. The generative\nmechanism for the Olympic power law is identified as strong convexity prompted\nby six causal drivers: irreversibility, fixed deadlines, the Blank Check\nSyndrome, tight coupling, long planning horizons, and an Eternal Beginner\nSyndrome. The power law explains why the Games are so difficult to plan and\nmanage successfully, and why cities and nations should think twice before\nbidding to host. Based on the power law, two heuristics are identified for\nbetter decision making on hosting. Finally, the paper develops measures for\ngood practice in planning and managing the Games, including how to mitigate the\nextreme risks of the Olympic power law.\n"
    },
    {
        "paper_id": 2009.14764,
        "authors": "Orcan Ogetbil, Narayan Ganesan, Bernhard Hientzsch",
        "title": "Calibrating Local Volatility Models with Stochastic Drift and Diffusion",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance,\n  25(02):2250011, 2022",
        "doi": "10.1142/S021902492250011X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose Monte Carlo calibration algorithms for three models: local\nvolatility with stochastic interest rates, stochastic local volatility with\ndeterministic interest rates, and finally stochastic local volatility with\nstochastic interest rates. For each model, we include detailed derivations of\nthe corresponding SDE systems, and list the required input data and steps for\ncalibration. We give conditions under which a local volatility can exist given\nEuropean option prices, stochastic interest rate model parameters, and\ncorrelations. The models are posed in a foreign exchange setting. The drift\nterm for the exchange rate is given as a difference of two stochastic short\nrates, domestic and foreign, each modeled by a G1++ process. For stochastic\nvolatility, we model the variance for the exchange rate by a CIR process. We\ninclude tests to show the convergence and the accuracy of the proposed\nalgorithms.\n"
    },
    {
        "paper_id": 2009.14818,
        "authors": "Xuan Tao, Andrew Day, Lan Ling, Samuel Drapeau",
        "title": "On Detecting Spoofing Strategies in High Frequency Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spoofing is an illegal act of artificially modifying the supply to drive\ntemporarily prices in a given direction for profit. In practice, detection of\nsuch an act is challenging due to the complexity of modern electronic platforms\nand the high frequency at which orders are channeled. We present a\nmicro-structural study of spoofing in a simple static setting. A multilevel\nimbalance which influences the resulting price movement is introduced upon\nwhich we describe the optimization strategy of a potential spoofer. We provide\nconditions under which a market is more likely to admit spoofing behavior as a\nfunction of the characteristics of the market. We describe the optimal spoofing\nstrategy after optimization which allows us to quantify the resulting impact on\nthe imbalance after spoofing. Based on these results we calibrate the model to\nreal Level 2 datasets from TMX, and provide some monitoring procedures based on\nthe Wasserstein distance to detect spoofing strategies in real time.\n"
    },
    {
        "paper_id": 2010.00212,
        "authors": "Jean Bernard Chatelain, Kirsten Ralf",
        "title": "How Macroeconomists Lost Control of Stabilization Policy: Towards Dark\n  Ages",
        "comments": null,
        "journal-ref": "The European Journal of the History of Economic Thought (2020)\n  27(6)",
        "doi": "10.1080/09672567.2020.1817119",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is a study of the history of the transplant of mathematical tools\nusing negative feedback for macroeconomic stabilization policy from 1948 to\n1975 and the subsequent break of the use of control for stabilization policy\nwhich occurred from 1975 to 1993. New-classical macroeconomists selected a\nsubset of the tools of control that favored their support of rules against\ndiscretionary stabilization policy. The Lucas critique and Kydland and\nPrescott's time-inconsistency were over-statements that led to the \"dark ages\"\nof the prevalence of the stabilization-policy-ineffectiveness idea. These\nover-statements were later revised following the success of the Taylor rule.\n"
    },
    {
        "paper_id": 2010.00413,
        "authors": "Amine Ouazad",
        "title": "Resilient Urban Housing Markets: Shocks vs. Fundamentals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the face of a pandemic, urban protests, and an affordability crisis, is\nthe desirability of dense urban settings at a turning point? Assessing cities'\nlong term trends remains challenging. The first part of this chapter describes\nthe short-run dynamics of the housing market in 2020. Evidence from prices and\nprice-to-rent ratios suggests expectations of resilience. Zip-level evidence\nsuggests a short-run trend towards suburbanization, and some impacts of urban\nprotests on house prices. The second part of the chapter analyzes the long-run\ndynamics of urban growth between 1970 and 2010. It analyzes what, in such urban\ngrowth, is explained by short-run shocks as opposed to fundamentals such as\neducation, industrial specialization, industrial diversification, urban\nsegregation, and housing supply elasticity. This chapter's original results as\nwell as a large established body of literature suggest that fundamentals are\nthe key drivers of growth. The chapter illustrates this finding with two case\nstudies: the New York City housing market after September 11, 2001; and the San\nFrancisco Bay Area in the aftermath of the 1989 Loma Prieta earthquake. Both\nareas rebounded strongly after these shocks, suggesting the resilience of the\nurban metropolis.\n"
    },
    {
        "paper_id": 2010.01031,
        "authors": "Xiao Fan Liu, Xin-Jian Jiang, Si-Hao Liu, Chi Kong Tse",
        "title": "Knowledge Discovery in Cryptocurrency Transactions: A Survey",
        "comments": "60 pages, 217 refs, 6 tables, and 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies gain trust in users by publicly disclosing the full creation\nand transaction history. In return, the transaction history faithfully records\nthe whole spectrum of cryptocurrency user behaviors. This article analyzes and\nsummarizes the existing research on knowledge discovery in the cryptocurrency\ntransactions using data mining techniques. Specifically, we classify the\nexisting research into three aspects, i.e., transaction tracings and blockchain\naddress linking, the analyses of collective user behaviors, and the study of\nindividual user behaviors. For each aspect, we present the problems, summarize\nthe methodologies, and discuss major findings in the literature. Furthermore,\nan enumeration of transaction data parsing and visualization tools and services\nis also provided. Finally, we outline several future directions in this\nresearch area, such as the current rapid development of Decentralized Finance\n(De-Fi) and digital fiat money.\n"
    },
    {
        "paper_id": 2010.01043,
        "authors": "Peng-Fei Dai, Xiong Xiong, Zhifeng Liu, Toan Luu Duc Huynh, Jianjun\n  Sun",
        "title": "Preventing crash in stock market: The role of economic policy\n  uncertainty during COVID-19",
        "comments": "25 pages",
        "journal-ref": "Financial Innovation, 2021, 7(1): 1-15,",
        "doi": "10.1186/s40854-021-00248-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the impact of economic policy uncertainty (EPU) on\nthe crash risk of US stock market during the COVID-19 pandemic. To this end, we\nuse the GARCH-S (GARCH with skewness) model to estimate daily skewness as a\nproxy for the stock market crash risk. The empirical results show the\nsignificantly negative correlation between EPU and stock market crash risk,\nindicating the aggravation of EPU increase the crash risk. Moreover, the\nnegative correlation gets stronger after the global COVID-19 outbreak, which\nshows the crash risk of the US stock market will be more affected by EPU during\nthe pandemic.\n"
    },
    {
        "paper_id": 2010.01105,
        "authors": "Marica Valente",
        "title": "Policy evaluation of waste pricing programs using heterogeneous causal\n  effect estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using machine learning methods in a quasi-experimental setting, I study the\nheterogeneous effects of introducing waste prices - unit prices on household\nunsorted waste disposal on - waste demands, municipal costs and pollution.\nUsing a unique panel of Italian municipalities with large variation in prices\nand observables, I show that waste demands are nonlinear. I find evidence of\nconstant elasticities at low prices, and increasing elasticities at high prices\ndriven by income effects and waste habits before policy. The policy reduces\nwaste management costs and pollution in all municipalities after three years of\nadoption, when prices cause significant waste avoidance.\n"
    },
    {
        "paper_id": 2010.01157,
        "authors": "Miroslav Fil",
        "title": "Gold Standard Pairs Trading Rules: Are They Valid?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pairs trading is a strategy based on exploiting mean reversion in prices of\nsecurities. It has been shown to generate significant excess returns, but its\nprofitability has dropped significantly in recent periods. We employ the most\ncommon distance and cointegration methods on US equities from 1990 to 2020\nincluding the Covid-19 crisis. The strategy overall fails to outperform the\nmarket benchmark even with hyperparameter tuning, but it performs very strongly\nduring bear markets. Furthermore, we demonstrate that market factors have a\nstrong relationship with the optimal parametrization for the strategy, and\nadjustments are appropriate for modern market conditions.\n"
    },
    {
        "paper_id": 2010.01193,
        "authors": "Ricardo A. Pasquini",
        "title": "Quadratic Funding and Matching Funds Requirements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we examine the mechanism proposed by Buterin, Hitzig, and Weyl\n(2019) for public goods financing, particularly regarding its matching funds\nrequirements, related efficiency implications, and incentives for strategic\nbehavior. Then, we use emerging evidence from Gitcoin Grants, to identify\nstylized facts in contribution giving and test our propositions. Because of its\nquadratic design, matching funds requirements scale rapidly, particularly by\nmore numerous and equally contributed projects. As a result, matching funds are\nexhausted early in the funding rounds, and much space remains for social\nefficiency improvement. Empirically, there is also a tendency by contributors\nto give small amounts, scattered among multiple projects, which accelerates\nthis process. Among other findings, we also identify a significant amount of\nreciprocal backing, which could be consistent with the kind of strategic\nbehavior we discuss.\n"
    },
    {
        "paper_id": 2010.01197,
        "authors": "Xing Wang, Yijun Wang, Bin Weng, Aleksandr Vinel",
        "title": "Stock2Vec: A Hybrid Deep Learning Framework for Stock Market Prediction\n  with Representation Learning and Temporal Convolutional Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have proposed to develop a global hybrid deep learning framework to\npredict the daily prices in the stock market. With representation learning, we\nderived an embedding called Stock2Vec, which gives us insight for the\nrelationship among different stocks, while the temporal convolutional layers\nare used for automatically capturing effective temporal patterns both within\nand across series. Evaluated on S&P 500, our hybrid framework integrates both\nadvantages and achieves better performance on the stock price prediction task\nthan several popular benchmarked models.\n"
    },
    {
        "paper_id": 2010.01199,
        "authors": "Caglar Tuncay",
        "title": "Market laws",
        "comments": "12 pages, 12 figures. Another version may appear in Physica A, soon",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  More than one billion data sampled with different frequencies from several\nfinancial instruments were investigated with the aim of testing whether they\ninvolve power law. As a result, a known power law with the power exponent\naround -4 was detected in the empirical distributions of the relative returns.\nMoreover, a number of new power law behaviors with various power exponents were\nexplored in the same data. Further on, a model based on finite sums over\nnumerous Maxwell-Boltzmann type distribution functions with random\n(pseudorandom) multipliers in the exponent were proposed to deal with the\nempirical distributions involving power laws. The results indicate that the\nproposed model may be universal.\n"
    },
    {
        "paper_id": 2010.01241,
        "authors": "Rakshit Jha, Mattijs De Paepe, Samuel Holt, James West, Shaun Ng",
        "title": "Deep Learning for Digital Asset Limit Order Books",
        "comments": "9 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows that temporal CNNs accurately predict bitcoin spot price\nmovements from limit order book data. On a 2 second prediction time horizon we\nachieve 71\\% walk-forward accuracy on the popular cryptocurrency exchange\ncoinbase. Our model can be trained in less than a day on commodity GPUs which\ncould be installed into colocation centers allowing for model sync with\nexisting faster orderbook prediction models. We provide source code and data at\nhttps://github.com/Globe-Research/deep-orderbook.\n"
    },
    {
        "paper_id": 2010.01265,
        "authors": "Chuheng Zhang, Yuanqi Li, Xi Chen, Yifei Jin, Pingzhong Tang, Jian Li",
        "title": "DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and\n  Feature Selection for Financial Data Analysis",
        "comments": "This paper was published in ICDM 2020. We have fixed several typos\n  and polished the writing in this revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern machine learning models (such as deep neural networks and boosting\ndecision tree models) have become increasingly popular in financial market\nprediction, due to their superior capacity to extract complex non-linear\npatterns. However, since financial datasets have very low signal-to-noise ratio\nand are non-stationary, complex models are often very prone to overfitting and\nsuffer from instability issues. Moreover, as various machine learning and data\nmining tools become more widely used in quantitative trading, many trading\nfirms have been producing an increasing number of features (aka factors).\nTherefore, how to automatically select effective features becomes an imminent\nproblem. To address these issues, we propose DoubleEnsemble, an ensemble\nframework leveraging learning trajectory based sample reweighting and shuffling\nbased feature selection. Specifically, we identify the key samples based on the\ntraining dynamics on each sample and elicit key features based on the ablation\nimpact of each feature via shuffling. Our model is applicable to a wide range\nof base models, capable of extracting complex patterns, while mitigating the\noverfitting and instability issues for financial market prediction. We conduct\nextensive experiments, including price prediction for cryptocurrencies and\nstock trading, using both DNN and gradient boosting decision tree as base\nmodels. Our experiment results demonstrate that DoubleEnsemble achieves a\nsuperior performance compared with several baseline methods.\n"
    },
    {
        "paper_id": 2010.01312,
        "authors": "Samuel Mugel, Enrique Lizaso, Roman Orus",
        "title": "Use Cases of Quantum Optimization for Finance",
        "comments": "10 pages, conference proceedings, to appear in Quantum Computing in\n  Econometrics and Quantum Economics and Related Topics, edited by Songsak\n  Sriboonchitta, Vladik Kreinovich, Woraphon Yamaka",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we briefly review two recent use-cases of quantum optimization\nalgorithms applied to hard problems in finance and economy. Specifically, we\ndiscuss the prediction of financial crashes as well as dynamic portfolio\noptimization. We comment on the different types of quantum strategies to carry\non these optimizations, such as those based on quantum annealers, universal\ngate-based quantum processors, and quantum-inspired Tensor Networks.\n"
    },
    {
        "paper_id": 2010.01319,
        "authors": "Lorenc Kapllani and Long Teng",
        "title": "Deep learning algorithms for solving high dimensional nonlinear backward\n  stochastic differential equations",
        "comments": "28 pages, 16 figures, 10 tables",
        "journal-ref": "Discrete Contin. Dyn. Syst. - B, 29 (2024) 1695-1729",
        "doi": "10.3934/dcdsb.2023151",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose a new deep learning-based scheme for solving high\ndimensional nonlinear backward stochastic differential equations (BSDEs). The\nidea is to reformulate the problem as a global optimization, where the local\nloss functions are included. Essentially, we approximate the unknown solution\nof a BSDE using a deep neural network and its gradient with automatic\ndifferentiation. The approximations are performed by globally minimizing the\nquadratic local loss function defined at each time step, which always includes\nthe terminal condition. This kind of loss functions are obtained by iterating\nthe Euler discretization of the time integrals with the terminal condition. Our\nformulation can prompt the stochastic gradient descent algorithm not only to\ntake the accuracy at each time layer into account, but also converge to a good\nlocal minima. In order to demonstrate performances of our algorithm, several\nhigh-dimensional nonlinear BSDEs including pricing problems in finance are\nprovided.\n"
    },
    {
        "paper_id": 2010.01335,
        "authors": "Merrick Wang, Robert Johnston",
        "title": "Wealth and Poverty: The Effect of Poverty on Communities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the differences in poverty in high wealth communities and\nlow wealth communities. We first discuss methods of measuring poverty and\nanalyze the causes of individual poverty and poverty in the Bay Area. Three\ncases are considered regarding relative poverty. The first two cases involve\nneighborhoods in the Bay Area while the third case evaluates two neighborhoods\nwithin the city of San Jose, CA. We find that low wealth communities have more\ncrime, more teen births, and more cost-burdened renters because of high\nconcentrations of temporary and seasonal workers, extensive regulations on\ngreenhouse gas emissions, minimum wage laws, and limited housing supply. In the\nconclusion, we review past attempts to alleviate the effects of poverty and\ngive suggestions on how future policy can be influenced to eventually create a\nfuture free of poverty.\n"
    },
    {
        "paper_id": 2010.01337,
        "authors": "Merrick Wang",
        "title": "Bitcoin and its impact on the economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to review the concept of cryptocurrencies in our\neconomy. First, Bitcoin and alternative cryptocurrencies' histories are\nanalyzed. We then study the implementation of Bitcoin in the airline and real\nestate industries. Our study finds that many Bitcoin companies partner with\nairlines in order to decrease processing times, to provide ease of access for\nspending in international airports, and to reduce fees on foreign exchanges for\nfuel expenses, maintenance, and flight operations. Bitcoin transactions have\noccurred in the real estate industry, but many businesses are concerned with\nBitcoin's potential interference with the U.S. government and its high\nvolatility. As Bitcoin's price has been growing rapidly, we assessed Bitcoin's\nreal value; Bitcoin derives value from its scarcity, utility, and public trust.\nIn the conclusion, we discuss Bitcoin's future and conclude that Bitcoin may\nchange from a short-term profit investment to a more steady industry as we\nidentify Bitcoin with the \"greater fool theory\", and as the number of available\nBitcoins to be mined dwindles and technology becomes more expensive.\n"
    },
    {
        "paper_id": 2010.01428,
        "authors": "Delia Coculescu and Freddy Delbaen",
        "title": "Group cohesion under individual regulatory constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a group consisting of N business units. We suppose there are\nregulatory constraints for each unit, more precisely, the net worth of each\nbusiness unit is required to belong to a set of acceptable risks, assumed to be\na convex cone. Because of these requirements, there are less incentives to\noperate under a group structure, as creating one single business unit, or\naltering the liability repartition among units, may allow to reduce the\nrequired capital. We analyse the possibilities for the group to benefit from a\ndiversification effect and economise on the cost of capital. We define and\nstudy the risk measures that allow for any group to achieve the minimal\ncapital, as if it were a single unit, without altering the liability of\nbusiness units, and despite the individual admissibility constraints. We call\nthese risk measures cohesive risk measures.\n"
    },
    {
        "paper_id": 2010.01687,
        "authors": "Shi Yu, Haoran Wang, Chaosheng Dong",
        "title": "Learning Risk Preferences from Investment Portfolios Using Inverse\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fundamental principle in Modern Portfolio Theory (MPT) is based on the\nquantification of the portfolio's risk related to performance. Although MPT has\nmade huge impacts on the investment world and prompted the success and\nprevalence of passive investing, it still has shortcomings in real-world\napplications. One of the main challenges is that the level of risk an investor\ncan endure, known as \\emph{risk-preference}, is a subjective choice that is\ntightly related to psychology and behavioral science in decision making. This\npaper presents a novel approach of measuring risk preference from existing\nportfolios using inverse optimization on the mean-variance portfolio allocation\nframework. Our approach allows the learner to continuously estimate real-time\nrisk preferences using concurrent observed portfolios and market price data. We\ndemonstrate our methods on real market data that consists of 20 years of asset\npricing and 10 years of mutual fund portfolio holdings. Moreover, the\nquantified risk preference parameters are validated with two well-known risk\nmeasurements currently applied in the field. The proposed methods could lead to\npractical and fruitful innovations in automated/personalized portfolio\nmanagement, such as Robo-advising, to augment financial advisors' decision\nintelligence in a long-term investment horizon.\n"
    },
    {
        "paper_id": 2010.01727,
        "authors": "Bruce Knuteson",
        "title": "Strikingly Suspicious Overnight and Intraday Returns",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The world's stock markets display a strikingly suspicious pattern of\novernight and intraday returns. Overnight returns to major stock market indices\nover the past few decades have been wildly positive, while intraday returns\nhave been disturbingly negative. The cause of these astonishingly consistent\nreturn patterns is unknown. We highlight the features of these extraordinary\npatterns that have hindered the construction of any plausible innocuous\nexplanation. We then use those same features to deduce the only plausible\nexplanation so far advanced for these strikingly suspicious returns.\n"
    },
    {
        "paper_id": 2010.01905,
        "authors": "L\\'eo Touzo, Matteo Marsili, Don Zagier",
        "title": "Information thermodynamics of financial markets: the Glosten-Milgrom\n  model",
        "comments": "20 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/abe59b",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Glosten-Milgrom model describes a single asset market, where informed\ntraders interact with a market maker, in the presence of noise traders. We\nderive an analogy between this financial model and a Szil\\'ard information\nengine by {\\em i)} showing that the optimal work extraction protocol in the\nlatter coincides with the pricing strategy of the market maker in the former\nand {\\em ii)} defining a market analogue of the physical temperature from the\nanalysis of the distribution of market orders. Then we show that the expected\ngain of informed traders is bounded above by the product of this market\ntemperature with the amount of information that informed traders have, in exact\nanalogy with the corresponding formula for the maximal expected amount of work\nthat can be extracted from a cycle of the information engine. This suggests\nthat recent ideas from information thermodynamics may shed light on financial\nmarkets, and lead to generalised inequalities, in the spirit of the extended\nsecond law of thermodynamics.\n"
    },
    {
        "paper_id": 2010.01996,
        "authors": "Junfeng Hu, Xiaosa Li, Yuru Xu, Shaowu Wu, Bin Zheng",
        "title": "Evaluation of company investment value based on machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, company investment value evaluation models are established\nbased on comprehensive company information. After data mining and extracting a\nset of 436 feature parameters, an optimal subset of features is obtained by\ndimension reduction through tree-based feature selection, followed by the\n5-fold cross-validation using XGBoost and LightGBM models. The results show\nthat the Root-Mean-Square Error (RMSE) reached 3.098 and 3.059, respectively.\nIn order to further improve the stability and generalization capability,\nBayesian Ridge Regression has been used to train a stacking model based on the\nXGBoost and LightGBM models. The corresponding RMSE is up to 3.047. Finally,\nthe importance of different features to the LightGBM model is analysed.\n"
    },
    {
        "paper_id": 2010.02378,
        "authors": "Niklas Potrafke, Fabian Ruthardt, Kaspar W\\\"uthrich",
        "title": "Protectionism and economic growth: Causal evidence from the first era of\n  globalization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate how protectionist policies influence economic growth. Our\nempirical strategy exploits an extraordinary tax scandal that gave rise to an\nunexpected change of government in Sweden. A free-trade majority in parliament\nwas overturned by a protectionist majority in 1887. The protectionist\ngovernment increased tariffs. We employ the synthetic control method to select\ncontrol countries against which economic growth in Sweden can be compared. We\ndo not find evidence suggesting that protectionist policies influenced economic\ngrowth and examine channels why. The new tariff laws increased government\nrevenue. However, the results do not suggest that the protectionist government\nstimulated the economy by increasing government expenditure.\n"
    },
    {
        "paper_id": 2010.02511,
        "authors": "Kevin Engelbrecht and Saul Jacka",
        "title": "Pricing and Hedging the No-Negative-Equity Guarantee in Equity-Release\n  Mortgages",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a practical superhedging strategy for the pricing and hedging of\nthe No-Negative-Equity-Guarantee (NNEG) found in Equity-Release Mortgages\n(ERMs), or reverse mortgages, using a discrete-time model. In contrast to many\npapers on the NNEG and industry practice we work in an incomplete market\nsetting so that deaths and property prices are not independent under most\npricing measures. We give theoretical results and numerical illustrations to\nshow that the assumption of market completeness leads to a considerable\nundervaluation of the NNEG. By introducing an Excess-of-Loss reinsurance asset,\nwe show that it is possible to reduce the cost of the superhedge for a\nportfolio of ERMs with the average cost decreasing rapidly as the number of\nlives in the portfolio increases. All the hedging assets, with the exception of\ncash, have a term of one year making the availability of a property hedging\nasset from over-the-counter derivative providers more realistic. We outline how\na practical multi-period ERM pricing and hedging model can be built. Although\nthe prices identified by this model will be higher than prices under the\ncompleteness assumption, they are considerably lower than those under the\nEquivalent Value Test mandated by the UK's Prudential Regulatory Authority.\n"
    },
    {
        "paper_id": 2010.02614,
        "authors": "Arjun Gupta and Soudeh Mirghasemi and Mohammad Arshad Rahman",
        "title": "Heterogeneity in Food Expenditure amongst US families: Evidence from\n  Longitudinal Quantile Regression",
        "comments": "25 Pages, 1 Figure and 5 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical studies on food expenditure are largely based on cross-section data\nand for a few studies based on longitudinal (or panel) data the focus has been\non the conditional mean. While the former, by construction, cannot model the\ndependencies between observations across time, the latter cannot look at the\nrelationship between food expenditure and covariates (such as income,\neducation, etc.) at lower (or upper) quantiles, which are of interest to\npolicymakers. This paper analyzes expenditures on total food (TF), food at home\n(FAH), and food away from home (FAFH) using mean regression and quantile\nregression models for longitudinal data to examine the impact of economic\nrecession and various demographic, socioeconomic, and geographic factors. The\ndata is taken from the Panel Study of Income Dynamics (PSID) and comprises of\n2174 families in the United States (US) observed between 2001-2015. Results\nindicate that age and education of the head, family income, female headed\nfamily, marital status, and economic recession are important determinants for\nall three types of food expenditure. Spouse education, family size, and some\nregional indicators are important for expenditures on TF and FAH, but not for\nFAFH. Quantile analysis reveals considerable heterogeneity in the covariate\neffects for all types of food expenditure, which cannot be captured by models\nfocused on conditional mean. The study ends by showing that modeling\nconditional dependence between observations across time for the same family\nunit is crucial to reducing/avoiding heterogeneity bias and better model\nfitting.\n"
    },
    {
        "paper_id": 2010.02658,
        "authors": "Jonas B{\\aa}{\\aa}th, Adel Daoud",
        "title": "Extending Social Resource Exchange to Events of Abundance and\n  Sufficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article identifies how scarcity, abundance, and sufficiency influence\nexchange behavior. Analyzing the mechanisms governing exchange of resources\nconstitutes the foundation of several social-science perspectives. Neoclassical\neconomics provides one of the most well-known perspectives of how rational\nindividuals allocate and exchange resources. Using Rational Choice Theory\n(RCT), neoclassical economics assumes that exchange between two individuals\nwill occur when resources are scarce and that these individuals interact\nrationally to satisfy their requirements (i.e., preferences). While RCT is\nuseful to characterize interaction in closed and stylized systems, it proves\ninsufficient to capture social and psychological reality where culture,\nemotions, and habits play an integral part in resource exchange. Social\nResource Theory (SRT) improves on RCT in several respects by making the social\nnature of resources the object of study. SRT shows how human interaction is\ndriven by an array of psychological mechanisms, from emotions to heuristics.\nThus, SRT provides a more realistic foundation for analyzing and explaining\nsocial exchange than the stylized instrumental rationality of RCT. Yet SRT has\nno clear place for events of abundance and sufficiency as additional\nmotivations to exchange resources. This article synthesize and formalize a\nfoundation for SRT using not only scarcity but also abundance and sufficiency.\n"
    },
    {
        "paper_id": 2010.0267,
        "authors": "Saeed Nosratabadi, Amir Mosavi, Ramin Keivani, Sina Ardabili, and\n  Farshid Aram",
        "title": "State of the Art Survey of Deep Learning and Machine Learning Models for\n  Smart Cities and Urban Sustainability",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-030-36841-8_22",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep learning (DL) and machine learning (ML) methods have recently\ncontributed to the advancement of models in the various aspects of prediction,\nplanning, and uncertainty analysis of smart cities and urban development. This\npaper presents the state of the art of DL and ML methods used in this realm.\nThrough a novel taxonomy, the advances in model development and new application\ndomains in urban sustainability and smart cities are presented. Findings reveal\nthat five DL and ML methods have been most applied to address the different\naspects of smart cities. These are artificial neural networks; support vector\nmachines; decision trees; ensembles, Bayesians, hybrids, and neuro-fuzzy; and\ndeep learning. It is also disclosed that energy, health, and urban transport\nare the main domains of smart cities that DL and ML methods contributed in to\naddress their problems.\n"
    },
    {
        "paper_id": 2010.02673,
        "authors": "Sina Ardabili, Amir Mosavi, Asghar Mahmoudi, Tarahom Mesri\n  Gundoshmian, Saeed Nosratabadi, and Annamaria R. Varkonyi-Koczy",
        "title": "Modelling Temperature Variation of Mushroom Growing Hall Using\n  Artificial Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-030-36841-8_3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent developments of computer and electronic systems have made the use\nof intelligent systems for the automation of agricultural industries. In this\nstudy, the temperature variation of the mushroom growing room was modeled by\nmulti-layered perceptron and radial basis function networks based on\nindependent parameters including ambient temperature, water temperature, fresh\nair and circulation air dampers, and water tap. According to the obtained\nresults from the networks, the best network for MLP was in the second\nrepetition with 12 neurons in the hidden layer and in 20 neurons in the hidden\nlayer for radial basis function network. The obtained results from comparative\nparameters for two networks showed the highest correlation coefficient (0.966),\nthe lowest root mean square error (RMSE) (0.787) and the lowest mean absolute\nerror (MAE) (0.02746) for radial basis function. Therefore, the neural network\nwith radial basis function was selected as a predictor of the behavior of the\nsystem for the temperature of mushroom growing halls controlling system.\n"
    },
    {
        "paper_id": 2010.02678,
        "authors": "Saeed Nosratabadi, Parvaneh Bahrami, Khodayar Palouzian, Amir Mosavi",
        "title": "Leader Cultural Intelligence and Organizational Performance",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/23311975.2020.1809310",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the challenges for international companies is to manage multicultural\nenvironments effectively. Cultural intelligence (CQ) is a soft skill required\nof the leaders of organizations working in cross-cultural contexts to be able\nto communicate effectively in such environments. On the other hand,\norganizational structure plays an active role in developing and promoting such\nskills in an organization. Therefore, this study aimed to investigate the\neffect of leader CQ on organizational performance mediated by organizational\nstructure. To achieve the objective of this research, first, conceptual models\nand hypotheses of this research were formed based on the literature. Then, a\nquantitative empirical research design using a questionnaire, as a tool for\ndata collection, and structural equation modeling, as a tool for data analysis,\nwas employed among executives of knowledge-based companies in the Science and\nTechnology Park, Bushehr, Iran. The results disclosed that leader CQ directly\nand indirectly (i.e., through the organizational structure) has a positive and\nsignificant effect on organizational performance. In other words, in\norganizations that operate in a multicultural environment, the higher the level\nof leader CQ, the higher the performance of that organization. Accordingly,\nsuch companies are encouraged to invest in improving the cultural intelligence\nof their leaders to improve their performance in cross-cultural environments,\nand to design appropriate organizational structures for the development of\ntheir intellectual capital.\n"
    },
    {
        "paper_id": 2010.02827,
        "authors": "Joffrey Derchu, Philippe Guillot, Thibaut Mastrolia and Mathieu\n  Rosenbaum",
        "title": "AHEAD : Ad-Hoc Electronic Auction Design",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new matching design for financial transactions in an\nelectronic market. In this mechanism, called ad-hoc electronic auction design\n(AHEAD), market participants can trade between themselves at a fixed price and\ntrigger an auction when they are no longer satisfied with this fixed price. In\nthis context, we prove that a Nash equilibrium is obtained between market\nparticipants. Furthermore, we are able to assess quantitatively the relevance\nof ad-hoc auctions and to compare them with periodic auctions and continuous\nlimit order books. We show that from the investors' viewpoint, the\nmicrostructure of the asset is usually significantly improved when using AHEAD.\n"
    },
    {
        "paper_id": 2010.03168,
        "authors": "Mario Coccia",
        "title": "Cyclical phenomena in technological change",
        "comments": "39 pages; 3 figures; 4 tables, 1 Appendix (Revised Figures). arXiv\n  admin note: text overlap with arXiv:1907.12406",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The process of technological change can be regarded as a non-deterministic\nsystem governed by factors of a cumulative nature that generate cyclical\nphenomena. In this context, the process of growth and decline of technology can\nbe systematically analyzed to design best practices for technology management\nof firms and innovation policy of nations. In this perspective, this study\nfocuses on the evolution of technologies in the U.S. recorded music industry.\nEmpirical findings reveal that technological change in the sector under study\nhere has recurring fluctuations of technological innovations. In particular,\ncycle of technology has up wave phase longer than down wave phase in the\nprocess of evolution in markets before it is substituted by a new technology.\nResults suggest that radical innovation is one of the main sources of cyclical\nphenomena for industrial and corporate change, and as a consequence, economic\nand social change.\n"
    },
    {
        "paper_id": 2010.03315,
        "authors": "Bruno Spilak, Wolfgang Karl H\\\"ardle",
        "title": "Tail-risk protection: Machine Learning meets modern Econometrics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Tail risk protection is in the focus of the financial industry and requires\nsolid mathematical and statistical tools, especially when a trading strategy is\nderived. Recent hype driven by machine learning (ML) mechanisms has raised the\nnecessity to display and understand the functionality of ML tools. In this\npaper, we present a dynamic tail risk protection strategy that targets a\nmaximum predefined level of risk measured by Value-At-Risk while controlling\nfor participation in bull market regimes. We propose different weak\nclassifiers, parametric and non-parametric, that estimate the exceedance\nprobability of the risk level from which we derive trading signals in order to\nhedge tail events. We then compare the different approaches both with\nstatistical and trading strategy performance, finally we propose an ensemble\nclassifier that produces a meta tail risk protection strategy improving both\ngeneralization and trading performance.\n"
    },
    {
        "paper_id": 2010.0335,
        "authors": "Suryadeepto Nag, Sankarshan Basu, Siddhartha P. Chakrabarty",
        "title": "Modeling the commodity prices of base metals in Indian commodity market\n  using a Higher Order Markovian Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Higher Order Markovian (HOM) model to capture the dynamics of commodity\nprices is proposed as an alternative to a Markovian model. In particular, the\norder of the former model, is taken to be the delay, in the response of the\nindustry, to the market information. This is then empirically analyzed for the\nprices of Copper Mini and four other bases metals, namely Aluminum, Lead,\nNickel and Zinc, in the Indian commodities market. In case of Copper Mini, the\nusage of the HOM approach consistently offer improvement, over the Markovian\napproach, in terms of the errors in forecasting. Similar trends were observed\nfor the other base metals considered, with the exception of Aluminum, which can\nbe attributed the volatility in the Asian market during the COVID-19 outbreak.\n"
    },
    {
        "paper_id": 2010.03371,
        "authors": "Dario Blanco-Fernandez, Stephan Leitner, Alexandra Rausch",
        "title": "Dynamic coalitions in complex task environments: To change or not to\n  change a winning team?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decision makers are often confronted with complex tasks which cannot be\nsolved by an individual alone, but require collaboration in the form of a\ncoalition. Previous literature argues that instability, in terms of the\nre-organization of a coalition with respect to its members over time, is\ndetrimental to performance. Other lines of research, such as the dynamic\ncapabilities framework, challenge this view. Our objective is to understand the\neffects of instability on the performance of coalitions which are formed to\nsolve complex tasks. In order to do so, we adapt the NK-model to the context of\nhuman decision-making in coalitions, and introduce an auction-based mechanism\nfor autonomous coalition formation and a learning mechanism for human agents.\nPreliminary results suggest that re-organizing innovative and well-performing\nteams is beneficial, but that this is true only in certain situations.\n"
    },
    {
        "paper_id": 2010.03455,
        "authors": "Pedro M. Gardete, Carlos D. Santos",
        "title": "No data? No problem! A Search-based Recommendation System with Cold\n  Starts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recommendation systems are essential ingredients in producing matches between\nproducts and buyers. Despite their ubiquity, they face two important\nchallenges. First, they are data-intensive, a feature that precludes\nsophisticated recommendations by some types of sellers, including those selling\ndurable goods. Second, they often focus on estimating fixed evaluations of\nproducts by consumers while ignoring state-dependent behaviors identified in\nthe Marketing literature.\n  We propose a recommendation system based on consumer browsing behaviors,\nwhich bypasses the \"cold start\" problem described above, and takes into account\nthe fact that consumers act as \"moving targets,\" behaving differently depending\non the recommendations suggested to them along their search journey. First, we\nrecover the consumers' search policy function via machine learning methods.\nSecond, we include that policy into the recommendation system's dynamic problem\nvia a Bellman equation framework.\n  When compared with the seller's own recommendations, our system produces a\nprofit increase of 33%. Our counterfactual analyses indicate that browsing\nhistory along with past recommendations feature strong complementary effects in\nvalue creation. Moreover, managing customer churn effectively is a big part of\nvalue creation, whereas recommending alternatives in a forward-looking way\nproduces moderate effects.\n"
    },
    {
        "paper_id": 2010.03464,
        "authors": "Arthur Fishman and Doron Klunover",
        "title": "To Act or not to Act? Political competition in the presence of a threat",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a model of political competition in which an incumbent politician,\nmay implement a costly policy to prevent a possible threat to, for example,\nnational security or a natural disaster.\n"
    },
    {
        "paper_id": 2010.03493,
        "authors": "Grzegorz Krochmal",
        "title": "Sentiment of tweets and socio-economic characteristics as the\n  determinants of voting behavior at the regional level. Case study of 2019\n  Polish parliamentary election",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is dedicated to finding the determinants of voting behavior in\nPoland at the poviat level. 2019 parliamentary election has been analyzed and\nan attempt to explain vote share for the winning party (Law and Justice) has\nbeen made. Sentiment analysis of tweets in Polish (original) and English\n(machine-translations), collected in the period around the election, has been\napplied. Amid multiple machine learning approaches tested, the best\nclassification accuracy has been achieved by Huggingface BERT on\nmachine-translated tweets. OLS regression, with sentiment of tweets and\nselected socio-economic features as independent variables, has been utilized to\nexplain Law and Justice vote share in poviats. Sentiment of tweets has been\nfound to be a significant predictor, as stipulated by the literature of the\nfield.\n"
    },
    {
        "paper_id": 2010.0358,
        "authors": "Christa Cuchiero, Stefan Rigger, Sara Svaluto-Ferro",
        "title": "Propagation of minimality in the supercooled Stefan problem",
        "comments": "To appear in Annals of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Supercooled Stefan problems describe the evolution of the boundary between\nthe solid and liquid phases of a substance, where the liquid is assumed to be\ncooled below its freezing point. Following the methodology of Delarue,\nNadtochiy and Shkolnikov, we construct solutions to the one-phase\none-dimensional supercooled Stefan problem through a certain McKean-Vlasov\nequation, which allows to define global solutions even in the presence of\nblow-ups. Solutions to the McKean-Vlasov equation arise as mean-field limits of\nparticle systems interacting through hitting times, which is important for\nsystemic risk modeling. Our main contributions are: (i) we prove a general\ntightness theorem for the Skorokhod M1-topology which applies to processes that\ncan be decomposed into a continuous and a monotone part. (ii) We prove\npropagation of chaos for a perturbed version of the particle system for general\ninitial conditions. (iii) We prove a conjecture of Delarue, Nadtochiy and\nShkolnikov, relating the solution concepts of so-called minimal and physical\nsolutions, showing that minimal solutions of the McKean-Vlasov equation are\nphysical whenever the initial condition is integrable.\n"
    },
    {
        "paper_id": 2010.03677,
        "authors": "Pradipta Banerjee, Subhrabrata Choudhury",
        "title": "Agent Based Computational Model Aided Approach to Improvise the\n  Inequality-Adjusted Human Development Index (IHDI) for Greater Parity in Real\n  Scenario Assessments",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To design, evaluate and tune policies for all-inclusive human development,\nthe primary requisite is to assess the true state of affairs of the society.\nStatistical indices like GDP, Gini Coefficients have been developed to\naccomplish the evaluation of the socio-economic systems. They have remained\nprevalent in the conventional economic theories but little do they have in the\noffing regarding true well-being and development of humans. Human Development\nIndex (HDI) and thereafter Inequality-adjusted Human Development Index (IHDI)\nhas been the path changing composite-index having the focus on human\ndevelopment. However, even though its fundamental philosophy has an\nall-inclusive human development focus, the composite-indices appear to be\nunable to grasp the actual assessment in several scenarios. This happens due to\nthe dynamic non-linearity of social-systems where superposition principle\ncannot be applied between all of its inputs and outputs of the system as the\nsystem's own attributes get altered upon each input. We would discuss the\napparent shortcomings and probable refinement of the existing index using an\nagent based computational system model approach.\n"
    },
    {
        "paper_id": 2010.04129,
        "authors": "John Gathergood, Benedict Guttman-Kenney",
        "title": "The English Patient: Evaluating Local Lockdowns Using Real-Time COVID-19\n  & Consumption Data",
        "comments": null,
        "journal-ref": "CEPR Covid Economics 64:73-100, January 2021",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find UK 'local lockdowns' of cities and small regions, focused on limiting\nhow many people a household can interact with and in what settings, are\neffective in turning the tide on rising positive COVID-19 cases. Yet, by\nfocusing on household mixing within the home, these local lockdowns have not\ninflicted the large declines in consumption observed in March 2020 when the\nfirst virus wave and first national lockdown occurred. Our study harnesses a\nnew source of real-time, transaction-level consumption data that we show to be\nhighly correlated with official statistics. The effectiveness of local\nlockdowns are evaluated applying a difference-in-difference approach which\nexploits nearby localities not subject to local lockdowns as comparison groups.\nOur findings indicate that policymakers may be able to contain virus outbreaks\nwithout killing local economies. However, the ultimate effectiveness of local\nlockdowns is expected to be highly dependent on co-ordination between regions\nand an effective system of testing.\n"
    },
    {
        "paper_id": 2010.0414,
        "authors": "Marco Avellaneda and Juan Andr\\'es Serur",
        "title": "Hierarchical PCA and Modeling Asset Correlations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling cross-sectional correlations between thousands of stocks, across\ncountries and industries, can be challenging. In this paper, we demonstrate the\nadvantages of using Hierarchical Principal Component Analysis (HPCA) over the\nclassic PCA. We also introduce a statistical clustering algorithm for\nidentifying of homogeneous clusters of stocks, or \"synthetic sectors\". We apply\nthese methods to study cross-sectional correlations in the US, Europe, China,\nand Emerging Markets.\n"
    },
    {
        "paper_id": 2010.04287,
        "authors": "Nishant Agrawal and Yaozhong Hu",
        "title": "Jump Models with delay -- option pricing and logarithmic Euler-Maruyama\n  scheme",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we obtain the existence, uniqueness and positivity of the\nsolution to delayed stochastic differential equations with jumps. This equation\nis then applied to model the price movement of the risky asset in a financial\nmarket and the Black-Scholes formula for the price of European options is\nobtained together with the hedging portfolios. The option price is evaluated\nanalytically at the last delayed period by using the Fourier transformation\ntechnique. But in general, there is no analytical expression for the option\nprice. To evaluate the price numerically we then use the Monte-Carlo method. To\nthis end, we need to simulate the delayed stochastic differential equations\nwith jumps. We propose a logarithmic Euler-Maruyama scheme to approximate the\nequation and prove that all the approximations remain positive and the rate of\nconvergence of the scheme is proved to be $0.5$.\n"
    },
    {
        "paper_id": 2010.04404,
        "authors": "Miquel Noguer i Alonso and Sonam Srivastava",
        "title": "Deep Reinforcement Learning for Asset Allocation in US Equities",
        "comments": "Submitting to Journal of Machine Learning in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reinforcement learning is a machine learning approach concerned with solving\ndynamic optimization problems in an almost model-free way by maximizing a\nreward function in state and action spaces. This property makes it an exciting\narea of research for financial problems. Asset allocation, where the goal is to\nobtain the weights of the assets that maximize the rewards in a given state of\nthe market considering risk and transaction costs, is a problem easily framed\nusing a reinforcement learning framework. It is first a prediction problem for\nexpected returns and covariance matrix and then an optimization problem for\nreturns, risk, and market impact. Investors and financial researchers have been\nworking with approaches like mean-variance optimization, minimum variance, risk\nparity, and equally weighted and several methods to make expected returns and\ncovariance matrices' predictions more robust. This paper demonstrates the\napplication of reinforcement learning to create a financial model-free solution\nto the asset allocation problem, learning to solve the problem using time\nseries and deep neural networks. We demonstrate this on daily data for the top\n24 stocks in the US equities universe with daily rebalancing. We use a deep\nreinforcement model on US stocks using different architectures. We use Long\nShort Term Memory networks, Convolutional Neural Networks, and Recurrent Neural\nNetworks and compare them with more traditional portfolio management. The Deep\nReinforcement Learning approach shows better results than traditional\napproaches using a simple reward function and only being given the time series\nof stocks. In Finance, no training to test error generalization results come\nguaranteed. We can say that the modeling framework can deal with time series\nprediction and asset allocation, including transaction costs.\n"
    },
    {
        "paper_id": 2010.0461,
        "authors": "Anine E. Bolko, Kim Christensen, Mikko S. Pakkanen, Bezirgen Veliyev",
        "title": "A GMM approach to estimate the roughness of stochastic volatility",
        "comments": "52 pages, 5 figures, v4: title of previous version in footnote\n  corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a GMM approach for estimation of log-normal stochastic volatility\nmodels driven by a fractional Brownian motion with unrestricted Hurst exponent.\nWe show that a parameter estimator based on the integrated variance is\nconsistent and, under stronger conditions, asymptotically normally distributed.\nWe inspect the behavior of our procedure when integrated variance is replaced\nwith a noisy measure of volatility calculated from discrete high-frequency\ndata. The realized estimator contains sampling error, which skews the fractal\ncoefficient toward \"illusive roughness.\" We construct an analytical approach to\ncontrol the impact of measurement error without introducing nuisance\nparameters. In a simulation study, we demonstrate convincing small sample\nproperties of our approach based both on integrated and realized variance over\nthe entire memory spectrum. We show the bias correction attenuates any\nsystematic deviance in the parameter estimates. Our procedure is applied to\nempirical high-frequency data from numerous leading equity indexes. With our\nrobust approach the Hurst index is estimated around 0.05, confirming roughness\nin stochastic volatility.\n"
    },
    {
        "paper_id": 2010.04719,
        "authors": "Behram Wali, Asad Khattak, Thomas Karnowski",
        "title": "The relationship between driving volatility in time to collision and\n  crash injury severity in a naturalistic driving environment",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.amar.2020.100136",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a key indicator of unsafe driving, driving volatility characterizes the\nvariations in microscopic driving decisions. This study characterizes\nvolatility in longitudinal and lateral driving decisions and examines the links\nbetween driving volatility in time to collision and crash injury severity. By\nusing a unique real-world naturalistic driving database from the 2nd Strategic\nHighway Research Program (SHRP), a test set of 671 crash events featuring\naround 0.2 million temporal samples of real world driving are analyzed. Based\non different driving performance measures, 16 different volatility indices are\ncreated. To explore the relationships between crash-injury severity outcomes\nand driving volatility, the volatility indices are then linked with individual\ncrash events including information on crash severity, drivers' pre crash\nmaneuvers and behaviors, secondary tasks and durations, and other factors. As\ndriving volatility prior to crash involvement can have different components, an\nindepth analysis is conducted using the aggregate as well as segmented (based\non time to collision) real world driving data. To account for the issues of\nobserved and unobserved heterogeneity, fixed and random parameter logit models\nwith heterogeneity in parameter means and variances are estimated. The\nempirical results offer important insights regarding how driving volatility in\ntime to collision relates to crash severity outcomes. Overall, statistically\nsignificant positive correlations are found between the aggregate (as well as\nsegmented) volatility measures and crash severity outcomes. The findings\nsuggest that greater driving volatility (both in longitudinal and lateral\ndirection) in time to collision increases the likelihood of police reportable\nor most severe crash events... ...\n"
    },
    {
        "paper_id": 2010.04771,
        "authors": "Geoff Boeing",
        "title": "Off the Grid... and Back Again? The Recent Evolution of American Street\n  Network Planning and Design",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/01944363.2020.1819382",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This morphological study identifies and measures recent nationwide trends in\nAmerican street network design. Historically, orthogonal street grids provided\nthe interconnectivity and density that researchers identify as important\nfactors for reducing vehicular travel and emissions and increasing road safety\nand physical activity. During the 20th century, griddedness declined in\nplanning practice alongside declines in urban form compactness, density, and\nconnectivity as urbanization sprawled around automobile dependence. But less is\nknown about comprehensive empirical trends across US neighborhoods, especially\nin recent years. This study uses public and open data to examine tract-level\nstreet networks across the entire US. It develops theoretical and measurement\nframeworks for a quality of street networks defined here as griddedness. It\nmeasures how griddedness, orientation order, straightness, 4-way intersections,\nand intersection density declined from 1940 through the 1990s while dead-ends\nand block lengths increased. However, since 2000, these trends have rebounded,\nshifting back toward historical design patterns. Yet, despite this rebound,\nwhen controlling for topography and built environment factors all decades\npost-1939 are associated with lower griddedness than pre-1940. Higher\ngriddedness is associated with less car ownership - which itself has a\nwell-established relationship with vehicle kilometers traveled and greenhouse\ngas emissions - while controlling for density, home and household size, income,\njobs proximity, street network grain, and local topography. Interconnected\ngrid-like street networks offer practitioners an important tool for curbing car\ndependence and emissions. Once established, street patterns determine urban\nspatial structure for centuries, so proactive planning is essential.\n"
    },
    {
        "paper_id": 2010.04827,
        "authors": "Eren Kurshan and Hongda Shen and Jiahao Chen",
        "title": "Towards Self-Regulating AI: Challenges and Opportunities of AI Model\n  Governance in Financial Services",
        "comments": "8 pages, 7 figures",
        "journal-ref": "Proceedings of the 1st International Conference on AI in Finance\n  (ICAIF '20), October 15-16, 2020, New York",
        "doi": "10.1145/3383455.3422564",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  AI systems have found a wide range of application areas in financial\nservices. Their involvement in broader and increasingly critical decisions has\nescalated the need for compliance and effective model governance. Current\ngovernance practices have evolved from more traditional financial applications\nand modeling frameworks. They often struggle with the fundamental differences\nin AI characteristics such as uncertainty in the assumptions, and the lack of\nexplicit programming. AI model governance frequently involves complex review\nflows and relies heavily on manual steps. As a result, it faces serious\nchallenges in effectiveness, cost, complexity, and speed. Furthermore, the\nunprecedented rate of growth in the AI model complexity raises questions on the\nsustainability of the current practices. This paper focuses on the challenges\nof AI model governance in the financial services industry. As a part of the\noutlook, we present a system-level framework towards increased self-regulation\nfor robustness and compliance. This approach aims to enable potential solution\nopportunities through increased automation and the integration of monitoring,\nmanagement, and mitigation capabilities. The proposed framework also provides\nmodel governance and risk management improved capabilities to manage model risk\nduring deployment.\n"
    },
    {
        "paper_id": 2010.04833,
        "authors": "Pradipta Banerjee and Subhrabrata Choudhury",
        "title": "Pandemic Lessons -- Devising an assessment framework to analyse policies\n  for sustainability",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  COVID-19 pandemic has sharply projected the globally persistent\nmulti-dimensional fundamental challenges in securing general socio-economic\nwellbeing of the society. The problems intensify with increasing population\ndensities and also vary with several socio-economic-geo-cultural activity\nparameters. These problems directly highlight the urgent need for accomplishing\nthe interdependent United Nations Sustainable Development Goals (SDGs) to\nensure that in future we do not enter into vicious loops of contracting newer\nzoonotic viruses and need not search for their vaccines while incurring\nsocio-economic havoc. Behavioural changes in human activities/responses are\nindispensable for achieving the interdependent SDGs. Using root cause analysis\napproach, we have developed a yearly assessment framework for viably analysing\nand identifying requisite region-specific downstream/upstream socio-economic\npolicies to reach the SDGs. The framework makes use of an infographic bar chart\nrepresentation based on the normalised values of 20 human activity/impact\nparameters classified under three categories as - negative, limiting and\npositive. With a holistic view encompassing the SDGs, we illustrate through\nthis framework the impact and urgent need of region-specific human behavioural\nreforms. This framework enables the foresight about policies regarding their\npotential in bringing down the negative parameter values to the desired zero\nlevel for accomplishing the SDGs through planetary health.\n"
    },
    {
        "paper_id": 2010.05058,
        "authors": "David S. Lee, Justin McCrary, Marcelo J. Moreira, Jack Porter",
        "title": "Valid t-ratio Inference for IV",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the single IV model, current practice relies on the first-stage F\nexceeding some threshold (e.g., 10) as a criterion for trusting t-ratio\ninferences, even though this yields an anti-conservative test. We show that a\ntrue 5 percent test instead requires an F greater than 104.7. Maintaining 10 as\na threshold requires replacing the critical value 1.96 with 3.43. We re-examine\n57 AER papers and find that corrected inference causes half of the initially\npresumed statistically significant results to be insignificant. We introduce a\nmore powerful test, the tF procedure, which provides F-dependent adjusted\nt-ratio critical values.\n"
    },
    {
        "paper_id": 2010.05172,
        "authors": "Yucheng Yang, Yue Pang, Guanhua Huang, Weinan E",
        "title": "The Knowledge Graph for Macroeconomic Analysis with Alternative Big Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The current knowledge system of macroeconomics is built on interactions among\na small number of variables, since traditional macroeconomic models can mostly\nhandle a handful of inputs. Recent work using big data suggests that a much\nlarger number of variables are active in driving the dynamics of the aggregate\neconomy. In this paper, we introduce a knowledge graph (KG) that consists of\nnot only linkages between traditional economic variables but also new\nalternative big data variables. We extract these new variables and the linkages\nby applying advanced natural language processing (NLP) tools on the massive\ntextual data of academic literature and research reports. As one example of the\npotential applications, we use it as the prior knowledge to select variables\nfor economic forecasting models in macroeconomics. Compared to statistical\nvariable selection methods, KG-based methods achieve significantly higher\nforecasting accuracy, especially for long run forecasts.\n"
    },
    {
        "paper_id": 2010.05221,
        "authors": "Annabelle Doerr, Anthony Strittmatter",
        "title": "Identifying causal channels of policy reforms with multiple treatments\n  and different types of selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the identification of channels of policy reforms with multiple\ntreatments and different types of selection for each treatment. We disentangle\nreform effects into policy effects, selection effects, and time effects under\nthe assumption of conditional independence, common trends, and an additional\nexclusion restriction on the non-treated. Furthermore, we show the\nidentification of direct- and indirect policy effects after imposing additional\nsequential conditional independence assumptions on mediating variables. We\nillustrate the approach using the German reform of the allocation system of\nvocational training for unemployed persons. The reform changed the allocation\nof training from a mandatory system to a voluntary voucher system.\nSimultaneously, the selection criteria for participants changed, and the reform\naltered the composition of course types. We consider the course composition as\na mediator of the policy reform. We show that the empirical evidence from\nprevious studies reverses when considering the course composition. This has\nimportant implications for policy conclusions.\n"
    },
    {
        "paper_id": 2010.05238,
        "authors": "V Simovic, V Simovic",
        "title": "Specilized day trading -- a new view on an old game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After the U.S market earned strong returns in 2003, day trading made a\ncomeback and once again became a popular trading method among traders. Although\nthere is no comprehensive empirical evidence available to answer the question\ndo individual day traders make money, there is a number of studies that point\nout that only few are able to consistently earn profits sufficient to cover\ntransaction costs and thus make money. The day trading concept of buying and\nselling stocks on margin alone suggests that it is more risky than the usual\ngoing long way of making profit. This paper offers a new approach to day\ntrading, an approach that eliminates some of the risks of day trading through\nspecialization. The concept is that the trader should specialize himself in\njust one (blue chip) stock and use existing day trading techniques (trend\nfollowing, playing news, range trading, scalping, technical analysis, covering\nspreads) to make money.\n"
    },
    {
        "paper_id": 2010.05311,
        "authors": "Yucheng Yang, Zhong Zheng, Weinan E",
        "title": "Interpretable Neural Networks for Panel Data Analysis in Economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The lack of interpretability and transparency are preventing economists from\nusing advanced tools like neural networks in their empirical research. In this\npaper, we propose a class of interpretable neural network models that can\nachieve both high prediction accuracy and interpretability. The model can be\nwritten as a simple function of a regularized number of interpretable features,\nwhich are outcomes of interpretable functions encoded in the neural network.\nResearchers can design different forms of interpretable functions based on the\nnature of their tasks. In particular, we encode a class of interpretable\nfunctions named persistent change filters in the neural network to study time\nseries cross-sectional data. We apply the model to predicting individual's\nmonthly employment status using high-dimensional administrative data. We\nachieve an accuracy of 94.5% in the test set, which is comparable to the best\nperformed conventional machine learning methods. Furthermore, the\ninterpretability of the model allows us to understand the mechanism that\nunderlies the prediction: an individual's employment status is closely related\nto whether she pays different types of insurances. Our work is a useful step\ntowards overcoming the black-box problem of neural networks, and provide a new\ntool for economists to study administrative and proprietary big data.\n"
    },
    {
        "paper_id": 2010.05342,
        "authors": "Wenhao Li",
        "title": "Using Information to Amplify Competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I characterize the consumer-optimal market segmentation in competitive\nmarkets where multiple firms selling differentiated products to consumers with\nunit demand. This segmentation is public---in that each firm observes the same\nmarket segments---and takes a simple form: in each market segment, there is a\ndominant firm favored by all consumers in that segment. By segmenting the\nmarket, all but the dominant firm maximally compete to poach the consumer's\nbusiness, setting price to equal marginal cost. Information, thus, is being\nused to amplify competition. This segmentation simultaneously generates an\nefficient allocation and delivers to each firm its minimax profit.\n"
    },
    {
        "paper_id": 2010.05398,
        "authors": "Derek Singh, Shuzhong Zhang",
        "title": "Tight Bounds for a Class of Data-Driven Distributionally Robust Risk\n  Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper expands the notion of robust moment problems to incorporate\ndistributional ambiguity using Wasserstein distance as the ambiguity measure.\nThe classical Chebyshev-Cantelli (zeroth partial moment) inequalities, Scarf\nand Lo (first partial moment) bounds, and semideviation (second partial moment)\nin one dimension are investigated. The infinite dimensional primal problems are\nformulated and the simpler finite dimensional dual problems are derived. A\nprincipal motivating question is how does data-driven distributional ambiguity\naffect the moment bounds. Towards answering this question, some theory is\ndeveloped and computational experiments are conducted for specific problem\ninstances in inventory control and portfolio management. Finally some open\nquestions and suggestions for future research are discussed.\n"
    },
    {
        "paper_id": 2010.05462,
        "authors": "F. Antonacci, C. Costantini, F. D'Ippoliti and M. Papi",
        "title": "Inflation, ECB and short-term interest rates: A new model, with\n  calibration to market data",
        "comments": "24 pages and 2 figures. arXiv admin note: substantial text overlap\n  with arXiv:1911.00386",
        "journal-ref": "International Journal of Theoretical and Applied Finance, Vol. 24,\n  No. 8 (2021)",
        "doi": "10.1142/S0219024921500424",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model for the joint evolution of the European inflation\nrate, the European Central Bank official interest rate and the short-term\ninterest rate, in a stochastic, continuous time setting.\n  We derive the valuation equation for a contingent claim and show that it has\na unique solution. The contingent claim payoff may depend on all three economic\nfactors of the model and the discount factor is allowed to include inflation.\n  Taking as a benchmark the model of Ho, H.W., Huang, H.H. and Yildirim, Y.,\nAffine model of inflation-indexed derivatives and inflation risk premium,\n(European Journal of Operational Researc, 2014), we show that our model\nperforms better on market data from 2008 to 2015.\n  Our model is not an affine model. Although in some special cases the solution\nof the valuation equation might admit a closed form, in general it has to be\nsolved numerically. This can be done efficiently by the algorithm that we\nprovide. Our model uses many fewer parameters than the benchmark model, which\npartly compensates the higher complexity of the numerical procedure and also\nsuggests that our model describes the behaviour of the economic factors more\nclosely.\n"
    },
    {
        "paper_id": 2010.05601,
        "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers",
        "title": "The loss optimisation of loan recovery decision times using forecast\n  cash flows",
        "comments": "29 pages (including appendix), 12 figures",
        "journal-ref": "Journal of Credit Risk (2020)",
        "doi": "10.21314/JCR.2020.275",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A theoretical method is empirically illustrated in finding the best time to\nforsake a loan such that the overall credit loss is minimised. This is\npredicated by forecasting the future cash flows of a loan portfolio up to the\ncontractual term, as a remedy to the inherent right-censoring of real-world\n`incomplete' portfolios. Two techniques, a simple probabilistic model as well\nas an eight-state Markov chain, are used to forecast these cash flows\nindependently. We train both techniques from different segments within\nresidential mortgage data, provided by a large South African bank, as part of a\ncomparative experimental framework. As a result, the recovery decision's\nimplied timing is empirically illustrated as a multi-period optimisation\nproblem across uncertain cash flows and competing costs. Using a delinquency\nmeasure as a central criterion, our procedure helps to find a loss-optimal\nthreshold at which loan recovery should ideally occur for a given portfolio.\nFurthermore, both the portfolio's historical risk profile and forecasting\nthereof are shown to influence the timing of the recovery decision. This work\ncan therefore facilitate the revision of relevant bank policies or strategies\ntowards optimising the loan collections process, especially that of secured\nlending.\n"
    },
    {
        "paper_id": 2010.05712,
        "authors": "Roland Pongou",
        "title": "Twin Estimates of the Effects of Prenatal Environment, Child Biology,\n  and Parental Bias on Sex Differences in Early Age Mortality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sex differences in early age mortality have been explained in prior\nliterature by differences in biological make-up and gender discrimination in\nthe allocation of household resources. Studies estimating the effects of these\nfactors have generally assumed that offspring sex ratio is random, which is\nimplausible in view of recent evidence that the sex of a child is partly\ndetermined by prenatal environmental factors. These factors may also affect\nchild health and survival in utero or after birth, which implies that\nconventional approaches to explaining sex differences in mortality are likely\nto yield biased estimates. We propose a methodology for decomposing these\ndifferences into the effects of prenatal environment, child biology, and\nparental preferences. Using a large sample of twins, we compare mortality rates\nin male-female twin pairs in India, a region known for discriminating against\ndaughters, and sub-Saharan Africa, a region where sons and daughters are\nthought to be valued by their parents about equally. We find that: (1) prenatal\nenvironment positively affects the mortality of male children; (2) biological\nmake-up of the latter contributes to their excess mortality, but its effect has\nbeen previously overestimated; and (3) parental discrimination against female\nchildren in India negatively affects their survival; but failure to control for\nthe effects of prenatal and biological factors leads conventional approaches to\nunderestimating its effect by 237 percent during infancy, and 44 percent during\nchildhood.\n"
    },
    {
        "paper_id": 2010.05867,
        "authors": "David Byrd and Antigoni Polychroniadou",
        "title": "Differentially Private Secure Multi-Party Computation for Federated\n  Learning in Financial Applications",
        "comments": "ACM ICAIF 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Federated Learning enables a population of clients, working with a trusted\nserver, to collaboratively learn a shared machine learning model while keeping\neach client's data within its own local systems. This reduces the risk of\nexposing sensitive data, but it is still possible to reverse engineer\ninformation about a client's private data set from communicated model\nparameters. Most federated learning systems therefore use differential privacy\nto introduce noise to the parameters. This adds uncertainty to any attempt to\nreveal private client data, but also reduces the accuracy of the shared model,\nlimiting the useful scale of privacy-preserving noise. A system can further\nreduce the coordinating server's ability to recover private client information,\nwithout additional accuracy loss, by also including secure multiparty\ncomputation. An approach combining both techniques is especially relevant to\nfinancial firms as it allows new possibilities for collaborative learning\nwithout exposing sensitive client data. This could produce more accurate models\nfor important tasks like optimal trade execution, credit origination, or fraud\ndetection. The key contributions of this paper are: We present a\nprivacy-preserving federated learning protocol to a non-specialist audience,\ndemonstrate it using logistic regression on a real-world credit card fraud data\nset, and evaluate it using an open-source simulation platform which we have\nadapted for the development of federated learning systems.\n"
    },
    {
        "paper_id": 2010.0597,
        "authors": "Hannes Mueller, Andre Groger, Jonathan Hersh, Andrea Matranga and Joan\n  Serrat",
        "title": "Monitoring War Destruction from Space: A Machine Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.2025400118",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existing data on building destruction in conflict zones rely on eyewitness\nreports or manual detection, which makes it generally scarce, incomplete and\npotentially biased. This lack of reliable data imposes severe limitations for\nmedia reporting, humanitarian relief efforts, human rights monitoring,\nreconstruction initiatives, and academic studies of violent conflict. This\narticle introduces an automated method of measuring destruction in\nhigh-resolution satellite images using deep learning techniques combined with\ndata augmentation to expand training samples. We apply this method to the\nSyrian civil war and reconstruct the evolution of damage in major cities across\nthe country. The approach allows generating destruction data with unprecedented\nscope, resolution, and frequency - only limited by the available satellite\nimagery - which can alleviate data limitations decisively.\n"
    },
    {
        "paper_id": 2010.06227,
        "authors": "Jonathan Berrisch, Florian Ziel",
        "title": "Distributional Modeling and Forecasting of Natural Gas Prices",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/for.2853",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the problem of modeling and forecasting European Day-Ahead and\nMonth-Ahead natural gas prices. For this, we propose two distinct probabilistic\nmodels that can be utilized in risk- and portfolio management. We use daily\npricing data ranging from 2011 to 2020. Extensive descriptive data analysis\nshows that both time series feature heavy tails, conditional\nheteroscedasticity, and show asymmetric behavior in their differences. We\npropose state-space time series models under skewed, heavy-tailed distributions\nto capture all stylized facts of the data. They include the impact of\nautocorrelation, seasonality, risk premia, temperature, storage levels, the\nprice of European Emission Allowances, and related fuel prices of oil, coal,\nand electricity. We provide rigorous model diagnostics and interpret all model\ncomponents in detail. Additionally, we conduct a probabilistic forecasting\nstudy with significance tests and compare the predictive performance against\nliterature benchmarks. The proposed Day-Ahead (Month-Ahead) model leads to a\n13% (9%) reduction in out-of-sample continuous ranked probability score (CRPS)\ncompared to the best performing benchmark model, mainly due to adequate\nmodeling of the volatility and heavy tails.\n"
    },
    {
        "paper_id": 2010.06306,
        "authors": "Fatemeh Gharari, Karina Arias-Calluari, Fernando Alonso-Marroquin,\n  Morteza. N. Najafi",
        "title": "Local and Non-local Fractional Porous Media Equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently it was observed that the probability distribution of the price\nreturn in S\\&P500 can be modeled by $q$-Gaussian distributions, where various\nphases (weak, strong super diffusion and normal diffusion) are separated by\ndifferent fitting parameters (Phys Rev. E 99, 062313, 2019). Here we analyze\nthe fractional extensions of the porous media equation and show that all of\nthem admit solutions in terms of generalized $q$-Gaussian functions. Three\nkinds of \"fractionalization\" are considered: \\textit{local}, referring to the\nsituation where the fractional derivatives for both space and time are local;\n\\textit{non-local}, where both space and time fractional derivatives are\nnon-local; and \\textit{mixed}, where one derivative is local, and another is\nnon-local. Although, for the \\textit{local} and \\textit{non-local} cases we\nfind $q$-Gaussian solutions , they differ in the number of free parameters.\nThis makes differences to the quality of fitting to the real data. We test the\nresults for the S\\&P 500 price return and found that the local and non-local\nschemes fit the data better than the classic porous media equation.\n"
    },
    {
        "paper_id": 2010.06417,
        "authors": "Hamed Vaheb",
        "title": "Asset Price Forecasting using Recurrent Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis serves three primary purposes, first of which is to forecast two\nstocks, i.e. Goldman Sachs (GS) and General Electric (GE). In order to forecast\nstock prices, we used a long short-term memory (LSTM) model in which we\ninputted the prices of two other stocks that lie in rather close correlation\nwith GS. Other models such as ARIMA were used as benchmark. Empirical results\nmanifest the practical challenges when using LSTM for forecasting stocks. One\nof the main upheavals was a recurring lag which we called \"forecasting lag\".\n  The second purpose is to develop a more general and objective perspective on\nthe task of time series forecasting so that it could be applied to assist in an\narbitrary that of forecasting by ANNs. Thus, attempts are made for\ndistinguishing previous works by certain criteria (introduced by a review paper\nwritten by Ahmed Tealab) so as to summarise those including effective\ninformation. The summarised information is then unified and expressed through a\ncommon terminology that can be applied to different steps of a time series\nforecasting task.\n  The last but not least purpose of this thesis is to elaborate on a\nmathematical framework on which ANNs are based. We are going to use the\nframework introduced in the book \"Neural Networks in Mathematical Framework\" by\nAnthony L. Caterini in which the structure of a generic neural network is\nintroduced and the gradient descent algorithm (which incorporates\nbackpropagation) is introduced in terms of their described framework. In the\nend, we use this framework for a specific architecture, which is recurrent\nneural networks on which we concentrated and our implementations are based. The\nbook proves its theorems mostly for classification case. Instead, we proved\ntheorems for regression case, which is the case of our problem.\n"
    },
    {
        "paper_id": 2010.06452,
        "authors": "S\\\"oren Christensen, Berenice Anne Neumann, Tobias Sohr",
        "title": "Competition versus Cooperation: A class of solvable mean field impulse\n  control problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a class of explicitly solvable mean field type control\nproblems/mean field games with a clear economic interpretation. More precisely,\nwe consider long term average impulse control problems with underlying general\none-dimensional diffusion processes motivated by optimal harvesting problems in\nnatural resource management. We extend the classical stochastic Faustmann\nmodels by allowing the prices to depend on the state of the market using a mean\nfield structure. In a competitive market model, we prove that, under natural\nconditions, there exists an equilibrium strategy of threshold-type and\nfurthermore characterize the threshold explicitly. If the agents cooperate with\neach other, we are faced with the mean field type control problem. Using a\nLagrange-type argument, we prove that the optimizer of this non-standard\nimpulse control problem is of threshold-type as well and characterize the\noptimal threshold. Furthermore, we compare the solutions and illustrate the\nfindings in an example.\n"
    },
    {
        "paper_id": 2010.06568,
        "authors": "Daniel Straulino, Mattie Landman and Neave O'Clery",
        "title": "A bi-directional approach to comparing the modular structure of networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we propose a new method to compare the modular structure of a pair of\nnode-aligned networks. The majority of current methods, such as normalized\nmutual information, compare two node partitions derived from a community\ndetection algorithm yet ignore the respective underlying network topologies.\nAddressing this gap, our method deploys a community detection quality function\nto assess the fit of each node partition with respect to the other network's\nconnectivity structure. Specifically, for two networks A and B, we project the\nnode partition of B onto the connectivity structure of A. By evaluating the fit\nof B's partition relative to A's own partition on network A (using a standard\nquality function), we quantify how well network A describes the modular\nstructure of B. Repeating this in the other direction, we obtain a\ntwo-dimensional distance measure, the bi-directional (BiDir) distance. The\nadvantages of our methodology are three-fold. First, it is adaptable to a wide\nclass of community detection algorithms that seek to optimize an objective\nfunction. Second, it takes into account the network structure, specifically the\nstrength of the connections within and between communities, and can thus\ncapture differences between networks with similar partitions but where one of\nthem might have a more defined or robust community structure. Third, it can\nalso identify cases in which dissimilar optimal partitions hide the fact that\nthe underlying community structure of both networks is relatively similar. We\nillustrate our method for a variety of community detection algorithms,\nincluding multi-resolution approaches, and a range of both simulated and real\nworld networks.\n"
    },
    {
        "paper_id": 2010.067,
        "authors": "Rui Fang, Maochao Xu, and Peng Zhao",
        "title": "Should the Ransomware be Paid?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ransomware has emerged as one of the most concerned cyber risks in recent\nyears, which has caused millions of dollars monetary loss over the world. It\ntypically demands a certain amount of ransom payment within a limited timeframe\nto decrypt the encrypted victim's files. This paper explores whether the\nransomware should be paid in a novel game-theoretic model from the perspective\nof Bayesian game. In particular, the new model analyzes the ransom payment\nstrategies within the framework of incomplete information for both hacker and\nvictim. Our results show that there exist pure and randomized Bayesian Nash\nequilibria under some mild conditions for the hacker and victim. The sufficient\nconditions that when the ransom should be paid are presented when an\norganization is compromised by the ransomware attack. We further study how the\ncosts and probabilities of cracking or recovering affect the expected payoffs\nof the hacker and the victim in the equilibria. In particular, it is found that\nthe backup option for computer files is not always beneficial, which actually\ndepends on the related cost. Moreover, it is discovered that fake ransomware\nmay be more than expected because of the potential high payoffs. Numerical\nexamples are also presented for illustration.\n"
    },
    {
        "paper_id": 2010.06723,
        "authors": "Jay Fuhrman (1,2), Andres F. Clarens (1), Haewon McJeon (2), Pralit\n  Patel (2), Scott C. Doney (3), William M. Shobe (4), Shreekar Pradhan (1)\n  ((1) Department of Engineering Systems and Environment, University of\n  Virginia, Charlottesville, Virginia, USA (2) Joint Global Change Research\n  Institute, University of Maryland and Pacific Northwest National Laboratory,\n  College Park, Maryland, USA (3) Department of Environmental Sciences,\n  University of Virginia, Charlottesville, Virginia, USA (4) Batten School of\n  Leadership and Public Policy, University of Virginia, Charlottesville,\n  Virginia, USA)",
        "title": "The role of negative emissions in meeting China's 2060 carbon neutrality\n  goal",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China's pledge to reach carbon neutrality before 2060 is an ambitious goal\nand could provide the world with much-needed leadership on how to limit warming\nto +1.5C warming above pre-industrial levels by the end of the century. But the\npathways that would achieve net zero by 2060 are still unclear, including the\nrole of negative emissions technologies. We use the Global Change Analysis\nModel to simulate how negative emissions technologies, in general, and direct\nair capture (DAC) in particular, could contribute to China's meeting this\ntarget. Our results show that negative emissions could play a large role,\noffsetting on the order of 3 GtCO2 per year from difficult-to-mitigate sectors\nsuch as freight transportation and heavy industry. This includes up to a 1.6\nGtCO2 per year contribution from DAC, constituting up to 60% of total projected\nnegative emissions in China. But DAC, like bioenergy with carbon capture and\nstorage and afforestation, has not yet been demonstrated at anywhere\napproaching the scales required to meaningfully contribute to climate\nmitigation. Deploying NETs at these scales will have widespread impacts on\nfinancial systems and natural resources such as water, land, and energy in\nChina.\n"
    },
    {
        "paper_id": 2010.06747,
        "authors": "Mauricio Contreras G",
        "title": "An Application of Dirac's Interaction Picture to Option Pricing",
        "comments": "23 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the Dirac's quantum mechanical interaction picture is applied\nto option pricing to obtain a solution of the Black-Scholes equation in the\npresence of a time-dependent arbitrage bubble. In particular, for the case of a\ncall perturbed by a square bubble, an approximate solution (valid up third\norder in a perturbation series) is given in terms of the three first Greeks:\nDelta, Gamma, and Speed. Then an exact solution is constructed in terms of all\nhigher order $S$-derivatives of the Black-Scholes formula. It is also shown\nthat the interacting Black-Scholes equation is invariant under a discrete\ntransformation that interchanges the interest rate with the mean of the\nunderlying asset and vice versa. This implies that the interacting\nBlack-Scholes equation can be written in a 'low energy' and a 'high energy'\nform, in such a way that the high-interaction limit of the low energy form\ncorresponds to the weak-interaction limit of the high energy form. One can\napply a perturbative analysis to the high energy form to study the\nhigh-interaction limit of the low energy form.\n"
    },
    {
        "paper_id": 2010.06856,
        "authors": "Pijush Kanti Das",
        "title": "Catastrophic health expenditure and inequalities -- a district level\n  study of West Bengal",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, I aimed to estimate the incidence of catastrophic health\nexpenditure and analyze the extent of inequalities in out-of-pocket health\nexpenditure and its decomposition according to gender, sector, religion and\nsocial groups of the households across Districts of West Bengal. I analysed\nhealth spending in West Bengal, using National Sample Survey 71st round pooled\ndata suitably represented to estimate up to district level. We measured CHE at\ndifferent thresholds when OOP in health expenditure. Gini Coefficients and its\ndecomposition techniques were applied to assess the degree of inequality in OOP\nhealth expenditures and between different socio geographic factors across\ndistricts. The incidence of catastrophic payments varies considerably across\ndistricts. Only 14.1 percent population of West Bengal was covered under health\ncoverage in 2014. The inequality in OOP health expenditure for West Bengal has\nbeen observed with gini coefficient of 0.67. Based on the findings from this\nanalysis, more attention is needed on effective financial protection for people\nof West Bengal to promote fairness, with special focus on the districts with\nhigher inequality. This study only provides the extent of CHE and inequality\nacross Districts of West Bengal but the causality may be taken in future scope\nof study.\n"
    },
    {
        "paper_id": 2010.06954,
        "authors": "Anand Sahasranaman",
        "title": "Long term dynamics of poverty transitions in India",
        "comments": "17 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the dynamics of poverty using a stochastic model of Geometric\nBrownian Motion with reallocation (RGBM) and explore both transient and\npersistent poverty over 1952-2006. We find that annual transitions in and out\nof poverty are common and show a rising trend, with the rise largely being\ndriven by transitions out of poverty. Despite this promising trend, even toward\nthe end of the time frame, there is a non-trivial proportion of individuals\nstill transitioning annually into poverty, indicative of the economic fragility\nof those near the poverty line. We also find that there is still a marked\npersistence of poverty over time, though the probability of poverty persistence\nis slowly declining. Particularly concerning in this context are the poverty\ntrajectories of those at the very bottom of the income distribution. The choice\nof poverty line appears to impact the dynamics, with higher poverty lines\ncorresponding to lower transitions and higher persistence probabilities. The\ndistinct nature of emergent transient and persistence dynamics suggests that\nthe approaches to counter these phenomena need to be different, possibly\nincorporating both missing financial markets and state action.\n"
    },
    {
        "paper_id": 2010.0722,
        "authors": "Nicole B\\\"auerle and Alexander Glauner",
        "title": "Markov Decision Processes with Recursive Risk Measures",
        "comments": null,
        "journal-ref": "European Journal of Operational Research 2021",
        "doi": "10.1016/j.ejor.2021.04.030",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider risk-sensitive Markov Decision Processes (MDPs)\nwith Borel state and action spaces and unbounded cost under both finite and\ninfinite planning horizons. Our optimality criterion is based on the recursive\napplication of static risk measures. This is motivated by recursive utilities\nin the economic literature, has been studied before for the entropic risk\nmeasure and is extended here to an axiomatic characterization of suitable risk\nmeasures. We derive a Bellman equation and prove the existence of Markovian\noptimal policies. For an infinite planning horizon, the model is shown to be\ncontractive and the optimal policy to be stationary. Moreover, we establish a\nconnection to distributionally robust MDPs, which provides a global\ninterpretation of the recursively defined objective function. Monotone models\nare studied in particular.\n"
    },
    {
        "paper_id": 2010.07289,
        "authors": "Paul Glasserman, Kriste Krstovski, Paul Laliberte, Harry Mamaysky",
        "title": "Choosing News Topics to Explain Stock Market Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze methods for selecting topics in news articles to explain stock\nreturns. We find, through empirical and theoretical results, that supervised\nLatent Dirichlet Allocation (sLDA) implemented through Gibbs sampling in a\nstochastic EM algorithm will often overfit returns to the detriment of the\ntopic model. We obtain better out-of-sample performance through a random search\nof plain LDA models. A branching procedure that reinforces effective topic\nassignments often performs best. We test methods on an archive of over 90,000\nnews articles about S&P 500 firms.\n"
    },
    {
        "paper_id": 2010.07383,
        "authors": "Corina Birghila and Tim J. Boonen and Mario Ghossoub",
        "title": "Optimal Insurance under Maxmin Expected Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine a problem of demand for insurance indemnification, when the\ninsured is sensitive to ambiguity and behaves according to the Maxmin-Expected\nUtility model of Gilboa and Schmeidler (1989), whereas the insurer is a\n(risk-averse or risk-neutral) Expected-Utility maximizer. We characterize\noptimal indemnity functions both with and without the customary ex ante\nno-sabotage requirement on feasible indemnities, and for both concave and\nlinear utility functions for the two agents. This allows us to provide a\nunifying framework in which we examine the effects of the no-sabotage\ncondition, marginal utility of wealth, belief heterogeneity, as well as\nambiguity (multiplicity of priors) on the structure of optimal indemnity\nfunctions. In particular, we show how the singularity in beliefs leads to an\noptimal indemnity function that involves full insurance on an event to which\nthe insurer assigns zero probability, while the decision maker assigns a\npositive probability. We examine several illustrative examples, and we provide\nnumerical studies for the case of a Wasserstein and a Renyi ambiguity set.\n"
    },
    {
        "paper_id": 2010.07402,
        "authors": "Yeguang Chi, Wenyan Hao",
        "title": "A Horserace of Volatility Models for Cryptocurrency: Evidence from\n  Bitcoin Spot and Option Markets",
        "comments": "41 pages, 6 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test various volatility models using the Bitcoin spot price series. Our\nmodels include HIST, EMA ARCH, GARCH, and EGARCH, models. Both of our\nin-sample-fit and out-of-sample-forecast results suggest that GARCH and EGARCH\nmodels perform much better than other models. Moreover, the EGARCH model's\nasymmetric term is positive and insignificant, which suggests that Bitcoin\nprices lack the asymmetric volatility response to past returns. Finally, we\nformulate an option trading strategy by exploiting the volatility spread\nbetween the GARCH volatility forecast and the option's implied volatility. We\nshow that a simple volatility-spread trading strategy with delta-hedging can\nyield robust profits.\n"
    },
    {
        "paper_id": 2010.07403,
        "authors": "Natalia A. Sadovnikova, Olga A. Zolotareva",
        "title": "The application of multivariate classification in evaluating the\n  regional differentiation by population income in Russia",
        "comments": "14 pages, 2 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article presents the results of multivariate classification of Russian\nregions by the indicators characterizing the population income and their\nconcentration. The clusterization was performed upon an author approach to\nselecting the characteristics which determines the academic novelty in the\nevaluation of regional differentiation by population income and the\ninterconnected characteristics. The performed analysis was aimed at the\nevaluation of the real scale of disproportions in spatial development of the\ncountry territories by the considered characteristics. The clusterization\nresults allowed to formulate the condition of a relatively \"strong\" position of\na group of high-income regions (the changes in the array of regions\nconstituting it is highly unlikely in the foreseeable future). Additionally\nthere has been revealed a group of Russian regions that the population is\nstruggling to live on quite low income. These so-called \"poor\" regions, within\nthe crisis conditions caused by Covid-19 are in need of additional public\nsupport, without which their population will impoverish.\n"
    },
    {
        "paper_id": 2010.07404,
        "authors": "Qi Zhao",
        "title": "A Deep Learning Framework for Predicting Digital Asset Price Movement\n  from Trade-by-trade Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a deep learning framework based on Long Short-term Memory\nNetwork(LSTM) that predicts price movement of cryptocurrencies from\ntrade-by-trade data. The main focus of this study is on predicting short-term\nprice changes in a fixed time horizon from a looking back period. By carefully\ndesigning features and detailed searching for best hyper-parameters, the model\nis trained to achieve high performance on nearly a year of trade-by-trade data.\nThe optimal model delivers stable high performance(over 60% accuracy) on\nout-of-sample test periods. In a realistic trading simulation setting, the\nprediction made by the model could be easily monetized. Moreover, this study\nshows that the LSTM model could extract universal features from trade-by-trade\ndata, as the learned parameters well maintain their high performance on other\ncryptocurrency instruments that were not included in training data. This study\nexceeds existing researches in term of the scale and precision of data used, as\nwell as the high prediction accuracy achieved.\n"
    },
    {
        "paper_id": 2010.07826,
        "authors": "Gjalt Huppes, Ruben Huele",
        "title": "Mass Flow Analysis of SARS-CoV-2 for quantified COVID-19 Risk Analysis",
        "comments": "18 pages. 5 figures and 3 tables, included in the text body. Paper\n  under review in the Journal of Industrial Ecology. Keywords: SARS-CoV-2;\n  COVID-19; Virion Mass Balance; Routes to Exposure; Epidemic; Pandemic;\n  Ventilation Rate; Risk Analysis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How may exposure risks to SARS-CoV-2 be assessed quantitatively? The material\nmetabolism approach of Industrial Ecology can be applied to the mass flows of\nthese virions by their numbers, as a key step in the analysis of the current\npandemic. Several transmission routes of SARS-2 from emission by a person to\nexposure of another person have been modelled and quantified. Start is a\nCOVID-19 illness progression model specifying rising emissions by an infected\nperson: the human virion factory. The first route covers closed spaces, with an\nemission, concentration, and decay model quantifying exposure. A next set of\nroutes covers person-to-person contacts mostly in open spaces, modelling the\nspatial distribution of exhales towards inhalation. These models also cover\nincidental exposures, like coughs and sneezes, and exposure through objects.\nRoutes through animal contacts, excrements, and food, have not been quantified.\nPotential exposures differ by six orders of magnitude. Closed rooms, even with\nreasonably (VR 2) to good (VR 5) ventilation, constitute the major exposure\nrisks. Close person-to-person contacts of longer duration create two orders of\nmagnitude lower exposure risks. Open spaces may create risks an order of\nmagnitude lower again. Burst of larger droplets may cause a common cold but not\nviral pneumonia as the virions in such droplets cannot reach the alveoli.\nFomites have not shown viable viruses in hospitals, let alone infections.\nInfection by animals might be possible, as by cats and ferrets kept as pets.\nThese results indicate priority domains for individual and collective measures.\nThe wide divergence in outcomes indicates robustness to most modelling and data\nimprovements, hardly leading to major changes in relative exposure potentials.\nHowever, models and data can substantially be improved.\n"
    },
    {
        "paper_id": 2010.08028,
        "authors": "Roberto Baviera",
        "title": "The measure of model risk in credit capital requirements",
        "comments": "13 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit capital requirements in Internal Rating Based approaches require the\ncalibration of two key parameters: the probability of default and the\nloss-given-default. This letter considers the uncertainty about these two\nparameters and models this uncertainty in an elementary way: it shows how this\nestimation risk can be computed and properly taken into account in regulatory\ncapital.\n  We analyse two standard real datasets: one composed by all corporates rated\nby Moody's and one limited only to the speculative grade ones. We statistically\ntest model hypotheses on both marginal distributions and parameter dependency.\nWe compute the estimation risk impact and observe that parameter dependency\nraises substantially the tail risk in capital requirements. The results are\nstriking with a required increase in regulatory capital in the range\n$38\\%$-$66\\%$.\n"
    },
    {
        "paper_id": 2010.08259,
        "authors": "Demetrio Lacava and Giampiero M. Gallo and Edoardo Otranto",
        "title": "Unconventional Policies Effects on Stock Market Volatility: A MAP\n  Approach",
        "comments": "26 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Taking the European Central Bank unconventional policies as a reference, we\nsuggest a class of Multiplicative Error Models (MEM) taylored to analyze the\nimpact such policies have on stock market volatility. The new set of models,\ncalled MEM with Asymmetry and Policy effects (MAP), keeps the base volatility\ndynamics separate from a component reproducing policy effects, with an increase\nin volatility on announcement days and a decrease unfolding implementation\neffects. When applied to four Eurozone markets, a Model Confidence Set approach\nfinds a significant improvement of the forecasting power of the proxy after the\nExpanded Asset Purchase Programme implementation; a multi--step ahead\nforecasting exercise estimates the duration of the effect, and, by shocking the\npolicy variable, we are able to quantify the reduction in volatility which is\nmore marked for debt--troubled countries.\n"
    },
    {
        "paper_id": 2010.08263,
        "authors": "Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, Qi Wu",
        "title": "Parsimonious Quantile Regression of Financial Asset Tail Dynamics via\n  Sequential Learning",
        "comments": "NeurIPS 2018:1582-1592",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a parsimonious quantile regression framework to learn the dynamic\ntail behaviors of financial asset returns. Our model captures well both the\ntime-varying characteristic and the asymmetrical heavy-tail property of\nfinancial time series. It combines the merits of a popular sequential neural\nnetwork model, i.e., LSTM, with a novel parametric quantile function that we\nconstruct to represent the conditional distribution of asset returns. Our model\nalso captures individually the serial dependences of higher moments, rather\nthan just the volatility. Across a wide range of asset classes, the\nout-of-sample forecasts of conditional quantiles or VaR of our model outperform\nthe GARCH family. Further, the proposed approach does not suffer from the issue\nof quantile crossing, nor does it expose to the ill-posedness comparing to the\nparametric probability density function approach.\n"
    },
    {
        "paper_id": 2010.08386,
        "authors": "Johnny Tang",
        "title": "Individual Heterogeneity and Cultural Attitudes in Credence Goods\n  Provision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the heterogeneity of credence goods provision in taxi drivers taking\ndetours in New York City. First, I document that there is significant detouring\non average by drivers. Second, there is significant heterogeneity in cheating\nacross individuals, yet each individual's propensity to take detours is stable:\ndrivers who detour almost always detour, while those who do not detour almost\nnever do. Drivers who take longer detours on each trip also take such trips\nmore often. Third, cultural attitudes plausibly explain some of this\nheterogeneity in behavior across individuals.\n"
    },
    {
        "paper_id": 2010.084,
        "authors": "Tahir Miriyev, Alessandro Contu, Kevin Schafers, Ion Gabriel Ion",
        "title": "Hybrid Modelling Approaches for Forecasting Energy Spot Prices in EPEC\n  market",
        "comments": "European Consortium for Mathematics in Industry",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we considered several hybrid modelling approaches for\nforecasting energy spot prices in EPEC market. Hybridization is performed\nthrough combining a Naive model, Fourier analysis, ARMA and GARCH models, a\nmean-reversion and jump-diffusion model, and Recurrent Neural Networks (RNN).\nTraining data was given in terms of electricity prices for 2013-2014 years, and\ntest data as a year of 2015.\n"
    },
    {
        "paper_id": 2010.08407,
        "authors": "Mike Ludkovski and Yuri Saporito",
        "title": "KrigHedge: Gaussian Process Surrogates for Delta Hedging",
        "comments": "33 pages, 6 figures, plus RMarkdown supplement",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a machine learning approach to option Greeks approximation\nbased on Gaussian process (GP) surrogates. The method takes in noisily observed\noption prices, fits a nonparametric input-output map and then analytically\ndifferentiates the latter to obtain the various price sensitivities. Our\nmotivation is to compute Greeks in cases where direct computation is expensive,\nsuch as in local volatility models, or can only ever be done approximately. We\nprovide a detailed analysis of numerous aspects of GP surrogates, including\nchoice of kernel family, simulation design, choice of trend function and impact\nof noise.\n  We further discuss the application to Delta hedging, including a new Lemma\nthat relates quality of the Delta approximation to discrete-time hedging loss.\nResults are illustrated with two extensive case studies that consider\nestimation of Delta, Theta and Gamma and benchmark approximation quality and\nuncertainty quantification using a variety of statistical metrics. Among our\nkey take-aways are the recommendation to use Matern kernels, the benefit of\nincluding virtual training points to capture boundary conditions, and the\nsignificant loss of fidelity when training on stock-path-based datasets.\n"
    },
    {
        "paper_id": 2010.08497,
        "authors": "Eric Benhamou and David Saltiel and Sandrine Ungari and Abhishek\n  Mukhopadhyay and Jamal Atif",
        "title": "AAMDRL: Augmented Asset Management with Deep Reinforcement Learning",
        "comments": "9 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2009.14136",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Can an agent learn efficiently in a noisy and self adapting environment with\nsequential, non-stationary and non-homogeneous observations? Through trading\nbots, we illustrate how Deep Reinforcement Learning (DRL) can tackle this\nchallenge. Our contributions are threefold: (i) the use of contextual\ninformation also referred to as augmented state in DRL, (ii) the impact of a\none period lag between observations and actions that is more realistic for an\nasset management environment, (iii) the implementation of a new repetitive\ntrain test method called walk forward analysis, similar in spirit to cross\nvalidation for time series. Although our experiment is on trading bots, it can\neasily be translated to other bot environments that operate in sequential\nenvironment with regime changes and noisy data. Our experiment for an augmented\nasset manager interested in finding the best portfolio for hedging strategies\nshows that AAMDRL achieves superior returns and lower risk.\n"
    },
    {
        "paper_id": 2010.08601,
        "authors": "Feng Zhang, Ruite Guo and Honggao Cao",
        "title": "Information Coefficient as a Performance Measure of Stock Selection\n  Models",
        "comments": "15 pages, 2 figures, and 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information coefficient (IC) is a widely used metric for measuring investment\nmanagers' skills in selecting stocks. However, its adequacy and effectiveness\nfor evaluating stock selection models has not been clearly understood, as IC\nfrom a realistic stock selection model can hardly be materially different from\nzero and is often accompanies with high volatility. In this paper, we\ninvestigate the behavior of IC as a performance measure of stick selection\nmodels. Through simulation and simple statistical modeling, we examine the IC\nbehavior both statically and dynamically. The examination helps us propose two\npractical procedures that one may use for IC-based ongoing performance\nmonitoring of stock selection models.\n"
    },
    {
        "paper_id": 2010.08698,
        "authors": "Dan Wang, Tianrui Wang, Ionu\\c{t} Florescu",
        "title": "Is Image Encoding Beneficial for Deep Learning in Finance? An Analysis\n  of Image Encoding Methods for the Application of Convolutional Neural\n  Networks in Finance",
        "comments": "12 pages, 7 figures, 13 tables in the main content. IEEE Internet of\n  Things Journal",
        "journal-ref": null,
        "doi": "10.1109/JIOT.2020.3030492",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 2012, SEC mandated all corporate filings for any company doing business in\nUS be entered into the Electronic Data Gathering, Analysis, and Retrieval\n(EDGAR) system. In this work we are investigating ways to analyze the data\navailable through EDGAR database. This may serve portfolio managers (pension\nfunds, mutual funds, insurance, hedge funds) to get automated insights into\ncompanies they invest in, to better manage their portfolios. The analysis is\nbased on Artificial Neural Networks applied to the data.} In particular, one of\nthe most popular machine learning methods, the Convolutional Neural Network\n(CNN) architecture, originally developed to interpret and classify images, is\nnow being used to interpret financial data. This work investigates the best way\nto input data collected from the SEC filings into a CNN architecture. We\nincorporate accounting principles and mathematical methods into the design of\nthree image encoding methods. Specifically, two methods are derived from\naccounting principles (Sequential Arrangement, Category Chunk Arrangement) and\none is using a purely mathematical technique (Hilbert Vector Arrangement). In\nthis work we analyze fundamental financial data as well as financial ratio data\nand study companies from the financial, healthcare and IT sectors in the United\nStates. We find that using imaging techniques to input data for CNN works\nbetter for financial ratio data but is not significantly better than simply\nusing the 1D input directly for fundamental data. We do not find the Hilbert\nVector Arrangement technique to be significantly better than other imaging\ntechniques.\n"
    },
    {
        "paper_id": 2010.08835,
        "authors": "Makoto Muto and Tamotsu Onozaki and Yoshitaka Saiki",
        "title": "Regional Synchronization during Economic Contraction: The Case of the\n  U.S. and Japan",
        "comments": "40 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two decades of studies have found significant regional differences in the\ntiming of transitions in national business cycles and their durations. Earlier\nstudies partly detect regional synchronization during business cycle expansions\nand contractions in Europe, the United States, and Japan. We examine this\npossibility applying a sophisticated method for identifying the time-varying\ndegree of synchronization to regional business cycle data in the U.S. and\nJapan. The method is prominent in nonlinear sciences but has been infrequently\napplied in business cycle studies.We find that synchronization in regional\nbusiness cycles increased during contractions and decreased during expansions\nthroughout the period under study.Such asymmetry between the contraction and\nexpansion phases of a business cycle will contribute our better understanding\nof the phenomenon of business cycles.\n"
    },
    {
        "paper_id": 2010.0889,
        "authors": "Ioannis P. Antoniades, Giuseppe Brandi, L. G. Magafas, T. Di Matteo",
        "title": "The use of scaling properties to detect relevant changes in financial\n  time series: a new visual warning tool",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.125561",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The dynamical evolution of multiscaling in financial time series is\ninvestigated using time-dependent Generalized Hurst Exponents (GHE), $H_q$, for\nvarious values of the parameter $q$. Using $H_q$, we introduce a new visual\nmethodology to algorithmically detect critical changes in the scaling of the\nunderlying complex time-series. The methodology involves the degree of\nmultiscaling at a particular time instance, the multiscaling trend which is\ncalculated by the Change-Point Analysis method, and a rigorous evaluation of\nthe statistical significance of the results. Using this algorithm, we have\nidentified particular patterns in the temporal co-evolution of the different\n$H_q$ time-series. These GHE patterns, distinguish in a statistically robust\nway, not only between time periods of uniscaling and multiscaling, but also\namong different types of multiscaling: symmetric multiscaling (M) and\nasymmetric multiscaling (A). We apply the visual methodology to time-series\ncomprising of daily close prices of four stock market indices: two major ones\n(S\\&P~500 and NIKKEI) and two peripheral ones (Athens Stock Exchange general\nIndex and Bombay-SENSEX). Results show that multiscaling varies greatly with\ntime: time periods of strong multiscaling behavior and time periods of\nuniscaling behavior are interchanged while transitions from uniscaling to\nmultiscaling behavior occur before critical market events, such as stock market\nbubbles. Moreover, particular asymmetric multiscaling patterns appear during\ncritical stock market eras and provide useful information about market\nconditions. In particular, they can be used as 'fingerprints' of a turbulent\nmarket period as well as provide warning signals for an upcoming stock market\n'bubble'. The applied visual methodology also appears to distinguish between\nexogenous and endogenous stock market crises, based on the observed patterns\nbefore the actual events.\n"
    },
    {
        "paper_id": 2010.089,
        "authors": "Tetsuo Kurosaki, Young Shin Kim",
        "title": "Cryptocurrency portfolio optimization with multivariate normal tempered\n  stable processes and Foster-Hart risk",
        "comments": "15 pages, 5 tables, 1 figure",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2021.102143",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study portfolio optimization of four major cryptocurrencies. Our time\nseries model is a generalized autoregressive conditional heteroscedasticity\n(GARCH) model with multivariate normal tempered stable (MNTS) distributed\nresiduals used to capture the non-Gaussian cryptocurrency return dynamics.\nBased on the time series model, we optimize the portfolio in terms of\nFoster-Hart risk. Those sophisticated techniques are not yet documented in the\ncontext of cryptocurrency. Statistical tests suggest that the MNTS distributed\nGARCH model fits better with cryptocurrency returns than the competing\nGARCH-type models. We find that Foster-Hart optimization yields a more\nprofitable portfolio with better risk-return balance than the prevailing\napproach.\n"
    },
    {
        "paper_id": 2010.08962,
        "authors": "Wen-Juan Xu, Chen-Yang Zhong, Fei Ren, Tian Qiu, Rong-Da Chen, Yun-Xin\n  He, Li-Xin Zhong",
        "title": "Evolutionary dynamics in financial markets with heterogeneities in\n  strategies and risk tolerance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In nature and human societies, the effects of homogeneous and heterogeneous\ncharacteristics on the evolution of collective behaviors are quite different\nfrom each other. It is of great importance to understand the underlying\nmechanisms of the occurrence of such differences. By incorporating pair pattern\nstrategies and reference point strategies into an agent-based model, we have\ninvestigated the coupled effects of heterogeneous investment strategies and\nheterogeneous risk tolerance on price fluctuations. In the market flooded with\nthe investors with homogeneous investment strategies or homogeneous risk\ntolerance, large price fluctuations are easy to occur. In the market flooded\nwith the investors with heterogeneous investment strategies or heterogeneous\nrisk tolerance, the price fluctuations are suppressed. For a heterogeneous\npopulation, the coexistence of investors with pair pattern strategies and\nreference point strategies causes the price to have a slow fluctuation around a\ntypical equilibrium point and both a large price fluctuation and a no-trading\nstate are avoided, in which the pair pattern strategies push the system far\naway from the equilibrium while the reference point strategies pull the system\nback to the equilibrium. A theoretical analysis indicates that the evolutionary\ndynamics in the present model is governed by the competition between different\nstrategies. The strategy that causes large price fluctuations loses more while\nthe strategy that pulls the system back to the equilibrium gains more.\nOverfrequent trading does harm to one's pursuit for more wealth.\n"
    },
    {
        "paper_id": 2010.08985,
        "authors": "Xin Huang, Duan Li, Daniel Zhuoyu Long",
        "title": "Scenario-decomposition Solution Framework for Nonseparable Stochastic\n  Control Problems",
        "comments": "Working paper. Under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When stochastic control problems do not possess separability and/or\nmonotonicity, the dynamic programming pioneered by Bellman in 1950s fails to\nwork as a time-decomposition solution method. Such cases have posted a great\nchallenge to the control society in both theoretical foundation and solution\nmethodologies for many years. With the help of the progressive hedging\nalgorithm proposed by Rockafellar and Wets in 1991, we develop a novel\nscenario-decomposition solution framework for stochastic control problems which\ncould be nonseparable and/or non-monotonic, thus extending the reach of\nstochastic optimal control. We discuss then some of its promising applications,\nincluding online quadratic programming problems and dynamic portfolio selection\nproblems with smoothing properties.\n"
    },
    {
        "paper_id": 2010.08992,
        "authors": "Isao Yagi, Mahiro Hoshino, and Takanobu Mizuta",
        "title": "Analysis of the impact of maker-taker fees on the stock market using\n  agent-based simulation",
        "comments": "ACM International Conference on AI in Finance 2020 (ICAIF '20)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, most stock exchanges in the U.S. employ maker-taker fees, in which\nan exchange pays rebates to traders placing orders in the order book and\ncharges fees to traders taking orders from the order book. Maker-taker fees\nencourage traders to place many orders that provide market liquidity to the\nexchange. However, it is not clear how maker-taker fees affect the total cost\nof a taking order, including all the charged fees and the market impact. In\nthis study, we investigated the effect of maker-taker fees on the total cost of\na taking order with our artificial market model, which is an agent-based model\nfor financial markets. We found that maker-taker fees encourage market\nefficiency but increase the total costs of taking orders.\n"
    },
    {
        "paper_id": 2010.09068,
        "authors": "Natalia A. Sadovnikova, Leysan A. Davletshina, Olga A. Zolotareva,\n  Olga O. Lebedinskaya",
        "title": "Differentiation of subjects of the Russian Federation according to the\n  main parameters of socio-economic development",
        "comments": "This research was performed in the framework of the state task in the\n  field of scientific activity of the Ministry of Science and Higher Education\n  of the Russian Federation, project \"Development of the methodology and a\n  software platform for the construction of digital twins, intellectual\n  analysis and forecast of complex economic systems\", grant no. FSSW-2020-0008",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article presents the results of a cluster analysis of the regions of the\nRussian Federation in terms of the main parameters of socio-economic\ndevelopment according to the data presented in the official data sources of the\nFederal State Statistics Service (Rosstat). Studied and analyzed the domestic\nand foreign (Eurostat) methodology for assessing the socio-economic development\nof territories. The aim of the study is to determine the main parameters of\nterritorial differentiation and to identify key indicators that affect the\nsocio-economic development of Russian regions. The authors have carried out a\nclassification of the constituent entities of the Russian Federation not in\nterms of territorial location and geographical features, but in terms of the\nspecifics and key parameters of the socio-economic situation.\n"
    },
    {
        "paper_id": 2010.09108,
        "authors": "Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay",
        "title": "Bridging the gap between Markowitz planning and deep reinforcement\n  learning",
        "comments": "10 pages, ICAPS PRL. arXiv admin note: substantial text overlap with\n  arXiv:2009.14136, arXiv:2010.08497",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While researchers in the asset management industry have mostly focused on\ntechniques based on financial and risk planning techniques like Markowitz\nefficient frontier, minimum variance, maximum diversification or equal risk\nparity, in parallel, another community in machine learning has started working\non reinforcement learning and more particularly deep reinforcement learning to\nsolve other decision making problems for challenging task like autonomous\ndriving, robot learning, and on a more conceptual side games solving like Go.\nThis paper aims to bridge the gap between these two approaches by showing Deep\nReinforcement Learning (DRL) techniques can shed new lights on portfolio\nallocation thanks to a more general optimization setting that casts portfolio\nallocation as an optimal control problem that is not just a one-step\noptimization, but rather a continuous control optimization with a delayed\nreward. The advantages are numerous: (i) DRL maps directly market conditions to\nactions by design and hence should adapt to changing environment, (ii) DRL does\nnot rely on any traditional financial risk assumptions like that risk is\nrepresented by variance, (iii) DRL can incorporate additional data and be a\nmulti inputs method as opposed to more traditional optimization methods. We\npresent on an experiment some encouraging results using convolution networks.\n"
    },
    {
        "paper_id": 2010.09186,
        "authors": "Masaaki Fujii and Akihiko Takahashi",
        "title": "Strong Convergence to the Mean-Field Limit of A Finite Agent Equilibrium",
        "comments": "To appear in SIAM J. Financ. Math",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an equilibrium-based continuous asset pricing problem for the\nsecurities market. In the previous work [16], we have shown that a certain\nprice process, which is given by the solution to a forward backward stochastic\ndifferential equation of conditional McKean-Vlasov type, asymptotically clears\nthe market in the large population limit. In the current work, under suitable\nconditions, we show the existence of a finite agent equilibrium and its strong\nconvergence to the corresponding mean-field limit given in [16]. As an\nimportant byproduct, we get the direct estimate on the difference of the\nequilibrium price between the two markets; one consisting of heterogeneous\nagents of finite population size and the other of homogeneous agents of\ninfinite population size.\n"
    },
    {
        "paper_id": 2010.09227,
        "authors": "Francisco Cisternas, Wee Chaimanowong, Alan Montgomery, Timothy\n  Derdenger",
        "title": "Influencing Competition Through Shelf Design",
        "comments": "51 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Shelf design decisions strongly influence product demand. In particular,\nplacing products in desirable locations increases demand. This primary effect\non shelf position is clear, but there is a secondary effect based on the\nrelative positioning of nearby products. Intuitively, products located next to\neach other are more likely to be compared having positive and negative effects.\nOn the one hand, locations closer to relatively strong products will be\nundesirable, as these strong products will draw demand from others -- an effect\nthat is stronger for those in close proximity. On the other hand, because\nstrong products tend to attract more traffic, locations closer to them elicit\nhigh consumer attention by increased visibility. Modifying the GEV class of\nmodels to allow demand to be moderated by competitors' proximity, these two\neffects emerge naturally. We found that although the competition effect is\nusually stronger, it is not always the dominating effect. Shelf displays can\nachieve higher profits by exploiting the relative influence on competition from\nshelf design to shift demand to higher profitability products. In the paper\ntowel category, we found profitability differences of up to 7\\% and displays\nwith 3\\% higher gross profits over the best shelf design present in our data.\n"
    },
    {
        "paper_id": 2010.09246,
        "authors": "Elior Nehemya and Yael Mathov and Asaf Shabtai and Yuval Elovici",
        "title": "Taking Over the Stock Market: Adversarial Perturbations Against\n  Algorithmic Traders",
        "comments": "Accepted to ECML PKDD 2021\n  https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_386.pdf",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, machine learning has become prevalent in numerous tasks,\nincluding algorithmic trading. Stock market traders utilize machine learning\nmodels to predict the market's behavior and execute an investment strategy\naccordingly. However, machine learning models have been shown to be susceptible\nto input manipulations called adversarial examples. Despite this risk, the\ntrading domain remains largely unexplored in the context of adversarial\nlearning. In this study, we present a realistic scenario in which an attacker\ninfluences algorithmic trading systems by using adversarial learning techniques\nto manipulate the input data stream in real time. The attacker creates a\nuniversal perturbation that is agnostic to the target model and time of use,\nwhich, when added to the input stream, remains imperceptible. We evaluate our\nattack on a real-world market data stream and target three different trading\nalgorithms. We show that when added to the input stream, our perturbation can\nfool the trading algorithms at future unseen data points, in both white-box and\nblack-box settings. Finally, we present various mitigation methods and discuss\ntheir limitations, which stem from the algorithmic trading domain. We believe\nthat these findings should serve as an alert to the finance community about the\nthreats in this area and promote further research on the risks associated with\nusing automated learning models in the trading domain.\n"
    },
    {
        "paper_id": 2010.09285,
        "authors": "Ren\\'e Aid, Andrea Cosso (UNIBO), Huy\\^en Pham (LPSM (UMR\\_8001),\n  ENSAE ParisTech )",
        "title": "Equilibrium price in intraday electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate an equilibrium model of intraday trading in electricity markets.\nAgents face balancing constraints between their customers consumption plus\nintraday sales and their production plus intraday purchases. They have\ncontinuously updated forecast of their customers consumption at maturity with\ndecreasing volatility error. Forecasts are prone to idiosyncratic noise as well\nas common noise (weather). Agents production capacities are subject to\nindependent random outages, which are each modelled by a Markov chain. The\nequilibrium price is defined as the price that minimises trading cost plus\nimbalance cost of each agent and satisfies the usual market clearing condition.\nExistence and uniqueness of the equilibrium are proved, and we show that the\nequilibrium price and the optimal trading strategies are martingales. The main\neconomic insights are the following. (i) When there is no uncertainty on\ngeneration, it is shown that the market price is a convex combination of\nforecasted marginal cost of each agent, with deterministic weights.\nFurthermore, the equilibrium market price follows Almgren and Chriss's model\nand we identify the fundamental part as well as the permanent market impact. It\nturns out that heterogeneity across agents is a necessary condition for the\nSamuelson's effect to hold. (ii) When there is production uncertainty, the\nprice volatility becomes stochastic but converges to the case without\nproduction uncertainty when the number of agents increases to infinity.\nFurther, on a two-agent case, we show that the potential outages of a low\nmarginal cost producer reduces her sales position.\n"
    },
    {
        "paper_id": 2010.09937,
        "authors": "Marcin Pitera and Thorsten Schmidt",
        "title": "Estimating and backtesting risk under heavy tails",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the {estimation} of risk is an important question in the daily business\nof banking and insurance, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk, expected shortfall and expectile value-at-risk. Motivated by the\nconsistent underestimation of risk by plug-in procedures, we propose a new\nalgorithm for bias correction and show how to apply it for generalized Pareto\ndistributions to the i.i.d. setting and to a GARCH(1,1) time series. In\nparticular, we show that the application of our algorithm leads to gain in\nefficiency when heavy tails or heteroscedasticity exists in the data.\n"
    },
    {
        "paper_id": 2010.10086,
        "authors": "Alexander Moerchel, Frank Tietze, Leonidas Aristodemou, Pratheeba\n  Vimalnath",
        "title": "Identifying Crisis-Critical Intellectual Property Challenges during the\n  Covid-19 Pandemic: A scenario analysis and conceptual extrapolation of\n  innovation ecosystem dynamics using a visual mapping approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.17863/CAM.58372",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Covid-19 pandemic exposed firms, organisations and their respective\nsupply chains which are directly involved in the manufacturing of products that\nare critical to alleviating the effects of the health crisis, collectively\nreferred to as the Crisis-Critical Sector,to unprecedented challenges. Firms\nfrom other sectors, such as automotive, luxury and home appliances, have rushed\ninto the Crisis-Critical Sector in order to support the effort to upscale\nincumbent manufacturing capacities, thereby introducing Intellectual Property\n(IP)related dynamics and challenges. We apply an innovation ecosystem\nperspective on the Crisis-Critical Sector and adopt a novel visual mapping\napproach to identify IP associated challenges and IP specific dynamic\ndevelopments during and potentially beyond the crisis.In this paper, we add\nmethodologically by devising and testing a visual approach to capturing IP\nrelated dynamics in evolving innovation ecosystems and contribute to literature\non IP management in the open innovation context by proposing paraground IP as a\nnovel IP type.Finally, we also deduce managerial implications for IP management\npractitioners at both incumbent firms and new entrants for navigating\ninnovation ecosystems subject to crisis-induced dynamic shifts.\n"
    },
    {
        "paper_id": 2010.10132,
        "authors": "Peiwan Wang and Lu Zong",
        "title": "Are Crises Predictable? A Review of the Early Warning Systems in\n  Currency and Stock Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study efforts to explore and extend the crisis predictability by\nsynthetically reviewing and comparing a full mixture of early warning models\ninto two constitutions: crisis identifications and predictive models. Given\nempirical results on Chinese currency and stock markets, three-strata findings\nare concluded as (i) the SWARCH model conditional on an elastic thresholding\nmethodology can most accurately classify crisis observations and greatly\ncontribute to boosting the predicting precision, (ii) stylized machine learning\nmodels are preferred given higher precision in predicting and greater benefit\nin practicing, (iii) leading factors sign the crisis in a diversified way for\ndifferent types of markets and varied prediction periods.\n"
    },
    {
        "paper_id": 2010.10208,
        "authors": "Sadasiba Tripathy and Dr. Sandhyarani Das",
        "title": "Impact of crop diversification on tribal farmer's income: A case study\n  from Eastern ghats of India",
        "comments": "9 pages, 1 figure, submitted to International Journal of Agricultural\n  Sustainability by Taylor & Francis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this investigation we analyze impact of diversification of agriculture on\nfarmer's income, a study from primitive tribal groups from eastern ghats of\nIndia. We have taken crop diversification index to measure the extent and\nregression formalism to analyze the impact, of crop diversification.\nDescriptive statistics is employed to know the average income of the farmers,\npaired results of crop diversification index. We observed a positive impact on\ncrop diversification in scheduled areas and investigated reasons where it did\nnot work.\n"
    },
    {
        "paper_id": 2010.1026,
        "authors": "Sergey Rashkovskiy",
        "title": "Thermodynamics of markets",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.125699",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the thermodynamic approach to the description of economic systems\nand processes. The first and second laws of thermodynamics as applied to\neconomic systems are derived and analyzed. It is shown that there is a deep\nanalogy between the parameters of thermodynamic and economic systems (markets);\nin particular, each thermodynamic parameter can be associated with a certain\neconomic parameter or indicator. The economic meaning of such primordially\nthermodynamic concepts as pressure, volume, internal energy, heat, etc. has\nbeen established. The thermostatistics of the market is considered. It is shown\nthat, as in conventional thermostatistics, many market parameters, such as\nprice of goods, quantity of goods, etc., as well as their fluctuations can be\ncalculated formally using the partition function of an economic system.\n"
    },
    {
        "paper_id": 2010.10625,
        "authors": "Alexander V. Bezrukov",
        "title": "Analysis of Regional Cluster Structure By Principal Components Modelling\n  in Russian Federation",
        "comments": "15 pages, 7 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper it is demonstrated that the application of principal components\nanalysis for regional cluster modelling and analysis is essential in the\nsituations where there is significant multicollinearity among several\nparameters, especially when the dimensionality of regional data is measured in\ntens. The proposed principal components model allows for same-quality\nrepresentation of the clustering of regions. In fact, the clusters become more\ndistinctive and the apparent outliers become either more pronounced with the\ncomponent model clustering or are alleviated with the respective hierarchical\ncluster. Thus, a five-component model was obtained and validated upon 85\nregions of Russian Federation and 19 socio-economic parameters. The principal\ncomponents allowed to describe approximately 75 percent of the initial\nparameters variation and enable further simulations upon the studied variables.\nThe cluster analysis upon the principal components modelling enabled better\nexposure of regional structure and disparity in economic development in Russian\nFederation, consisting of four main clusters: the few-numbered highest\ndevelopment regions, the clusters with mid-to-high and low economic\ndevelopment, and the \"poorest\" regions. It is observable that the development\nin most regions relies upon resource economy, and the industrial potential as\nwell as inter-regional infrastructural potential are not realized to their\nfullest, while only the wealthiest regions show highly developed economy, while\nthe industry in other regions shows signs of stagnation which is scaled further\ndue to the conditions entailed by economic sanctions and the recent Covid-19\npandemic. Most Russian regions are in need of additional public support and\nindustrial development, as their capital assets potential is hampered and,\nwhile having sufficient labor resources, their donorship will increase.\n"
    },
    {
        "paper_id": 2010.10703,
        "authors": "Brian P. Hanley",
        "title": "Cancellation of principal in banking: Four radical ideas emerge from\n  deep examination of double entry bookkeeping in banking",
        "comments": "26 pages, 6 figures, 6 tables. Formatting and response to comments",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Four radical ideas are presented. First, that the rationale for cancellation\nof principal can be modified in modern banking. Second, that non-cancellation\nof loan principal upon payment may cure an old problem of maintenance of\npositive equity in the non-governmental sector. Third, that crediting this\nmoney to local/state government, and fourth crediting to at-risk loans that\ncreate new utility value, creates an additional virtuous monetary circuit that\nties finances of government directly to commercial activity.\n  Taking these steps can cure a problem I have identified with modern monetary\ntheory, which is that breaking the monetary circuit of taxation in the minds of\npoliticians will free them from centuries of restraint, optimizing their\nopportunities for implementing tyranny. It maintains and strengthens the\ncurrent circuit, creating a new, more direct monetary circuit that in some\nrespects combats inequality.\n"
    },
    {
        "paper_id": 2010.10794,
        "authors": "Jun-ya Gotoh, Michael Jong Kim, Andrew E.B.Lim",
        "title": "Worst-case sensitivity",
        "comments": "27 Pages + 11 page Appendix, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the notion of Worst-Case Sensitivity, defined as the worst-case\nrate of increase in the expected cost of a Distributionally Robust Optimization\n(DRO) model when the size of the uncertainty set vanishes. We show that\nworst-case sensitivity is a Generalized Measure of Deviation and that a large\nclass of DRO models are essentially mean-(worst-case) sensitivity problems when\nuncertainty sets are small, unifying recent results on the relationship between\nDRO and regularized empirical optimization with worst-case sensitivity playing\nthe role of the regularizer. More generally, DRO solutions can be sensitive to\nthe family and size of the uncertainty set, and reflect the properties of its\nworst-case sensitivity. We derive closed-form expressions of worst-case\nsensitivity for well known uncertainty sets including smooth $\\phi$-divergence,\ntotal variation, \"budgeted\" uncertainty sets, uncertainty sets corresponding to\na convex combination of expected value and CVaR, and the Wasserstein metric.\nThese can be used to select the uncertainty set and its size for a given\napplication.\n"
    },
    {
        "paper_id": 2010.1103,
        "authors": "Ulrich Bindseil, Edoardo Lanari",
        "title": "Fire Sales, the LOLR and Bank Runs with Continuous Asset Liquidity",
        "comments": "19 pages, comments are very welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bank's asset fire sales and recourse to central bank credit are modelled with\ncontinuous asset liquidity, allowing to derive the liability structure of a\nbank. Both asset sales liquidity and the central bank collateral framework are\nmodeled as power functions within the unit interval. Funding stability is\ncaptured as a strategic bank run game in pure strategies between depositors.\nFire sale liquidity and the central bank collateral framework determine jointly\nthe ability of the banking system to deliver maturity transformation without\nendangering financial stability. The model also explains why banks tend to use\nthe least liquid eligible collateral with the central bank and why a sudden\nnon-anticipated reduction of asset liquidity, or a tightening of the collateral\nframework, can trigger a bank run. The model also shows that the collateral\nframework can be understood, beyond its aim to protect the central bank, as\nfinancial stability and non-conventional monetary policy instrument.\n"
    },
    {
        "paper_id": 2010.11261,
        "authors": "Marta Boczon",
        "title": "Quantifying Uncertainties in Estimates of Income and Wealth Inequality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I measure the uncertainty affecting estimates of economic inequality in the\nUS and investigate how accounting for properly estimated standard errors can\naffect the results of empirical and structural macroeconomic studies. In my\nanalysis, I rely upon two data sets: the Survey of Consumer Finances (SCF),\nwhich is a triennial survey of household financial condition, and the\nIndividual Tax Model Public Use File (PUF), an annual sample of individual\nincome tax returns. While focusing on the six income and wealth shares of the\ntop 10 to the top 0.01 percent between 1988 and 2018, my results suggest that\nignoring uncertainties in estimated wealth and income shares can lead to\nerroneous conclusions about the current state of the economy and, therefore,\nlead to inaccurate predictions and ineffective policy recommendations. My\nanalysis suggests that for the six top-decile income shares under\nconsideration, the PUF estimates are considerably better than those constructed\nusing the SCF; for wealth shares of the top 10 to the top 0.5 percent, the SCF\nestimates appear to be more reliable than the PUF estimates; finally, for the\ntwo most granular wealth shares, the top 0.1 and 0.01 percent, both data sets\npresent non-trivial challenges that cannot be readily addressed.\n"
    },
    {
        "paper_id": 2010.11388,
        "authors": "Yaser Faghan, Nancirose Piazza, Vahid Behzadan, Ali Fathi",
        "title": "Adversarial Attacks on Deep Algorithmic Trading Policies",
        "comments": "17 pages - under submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep Reinforcement Learning (DRL) has become an appealing solution to\nalgorithmic trading such as high frequency trading of stocks and\ncyptocurrencies. However, DRL have been shown to be susceptible to adversarial\nattacks. It follows that algorithmic trading DRL agents may also be compromised\nby such adversarial techniques, leading to policy manipulation. In this paper,\nwe develop a threat model for deep trading policies, and propose two attack\ntechniques for manipulating the performance of such policies at test-time.\nFurthermore, we demonstrate the effectiveness of the proposed attacks against\nbenchmark and real-world DQN trading agents.\n"
    },
    {
        "paper_id": 2010.1146,
        "authors": "Shreya Biswas, Upasak Das, Prasenjit Sarkhel",
        "title": "Duration of exposure to inheritance law in India: Examining the\n  heterogeneous effects on empowerment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Higher duration of programs that involve legal protection may entail gradual\npositive changes in social norms that can be leveraged by potential\nbeneficiaries in their favor. This paper examines the heterogeneous impact of\nthe duration of exposure to gender-neutral reforms in the inheritance law in\nIndia on two latent domains of women empowerment: intrinsic, which pertains to\nexpansion of agency and instrumental which relates to ability to make\ndecisions. The time lag between the year of the amendment in the respective\nstates and the year of marriage generate exogenous variation in reform exposure\nacross women. The findings indicate a significant non-linear increase in the\ninstrumental as well as intrinsic empowerment. Importantly, improvements in\neducation along with increase in the age of marriage and changes in family\nstructure are found to be the potential channels that signal gradual relaxation\nof social norms and explain the higher returns to exposure on empowerment.\n"
    },
    {
        "paper_id": 2010.11515,
        "authors": "Alessandro Doldi and Marco Frittelli",
        "title": "Conditional Systemic Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate to which extent the relevant features of (static) Systemic\nRisk Measures can be extended to a conditional setting. After providing a\ngeneral dual representation result, we analyze in greater detail Conditional\nShortfall Systemic Risk Measures. In the particular case of exponential\npreferences, we provide explicit formulas that also allow us to show a time\nconsistency property. Finally, we provide an interpretation of the allocations\nassociated to Conditional Shortfall Systemic Risk Measures as suitably defined\nequilibria. Conceptually, the generalization from static to conditional\nSystemic Risk Measures can be achieved in a natural way, even though the proofs\nbecome more technical than in the unconditional framework.\n"
    },
    {
        "paper_id": 2010.11841,
        "authors": "Fabian Stephany",
        "title": "When Does it Pay Off to Learn a New Skill? Revealing the Complementary\n  Benefit of Cross-Skilling",
        "comments": "25 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This work examines the economic benefits of learning a new skill from a\ndifferent domain: cross-skilling. To assess this, a network of skills from the\njob profiles of 14,790 online freelancers is constructed. Based on this skill\nnetwork, relationships between 3,480 different skills are revealed and marginal\neffects of learning a new skill can be calculated via workers' wages. The\nresults indicate that learning in-demand skills, such as popular programming\nlanguages, is beneficial in general, and that diverse skill sets tend to be\nprofitable, too. However, the economic benefit of a new skill is individual, as\nit complements the existing skill bundle of each worker. As technological and\nsocial transformation is reshuffling jobs' task profiles at a fast pace, the\nfindings of this study help to clarify skill sets required for designing\nindividual re-skilling pathways. This can help to increase employability and\nreduce labour market shortages.\n"
    },
    {
        "paper_id": 2010.11912,
        "authors": "Fernando N\\'u\\~nez, David Canca, \\'Angel Arcos-Vargas",
        "title": "An assessment of European electricity arbitrage using storage systems",
        "comments": "22 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.1016/j.energy.2021.122916",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study analyses the current viability of this business based on a sample\nof European countries in the year 2019; countries where electricity prices\n(day-ahead market) and financial conditions show a certain degree of\nheterogeneity. We basically follow a sequence of three analyses in our study.\nFirstly, a Linear Mixed-Integrated Programming model has been developed to\noptimize the arbitrage strategy for each country in the sample. Secondly, using\nthe cash-flows from the optimization model, we calculate two financial\nindicators (NPV and IRR) in order to select the optimal converter size for each\ncountry. Tax and discount rates specific to each country have been used with\nthe calculation of this second rate following the methodology proposed by the\nSpanish regulator. Thirdly, a mixed linear regression model is proposed in\norder to investigate the importance of observed and unobserved heterogeneity\n(at country level) in explaining the business profitability.\n"
    },
    {
        "paper_id": 2010.12002,
        "authors": "Metod Jazbec, Barna P\\'asztor, Felix Faltings, Nino Antulov-Fantulin,\n  Petter N. Kolm",
        "title": "On the impact of publicly available news and information transfer to\n  financial markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify the propagation and absorption of large-scale publicly available\nnews articles from the World Wide Web to financial markets. To extract publicly\navailable information, we use the news archives from the Common Crawl, a\nnonprofit organization that crawls a large part of the web. We develop a\nprocessing pipeline to identify news articles associated with the constituent\ncompanies in the S\\&P 500 index, an equity market index that measures the stock\nperformance of U.S. companies. Using machine learning techniques, we extract\nsentiment scores from the Common Crawl News data and employ tools from\ninformation theory to quantify the information transfer from public news\narticles to the U.S. stock market. Furthermore, we analyze and quantify the\neconomic significance of the news-based information with a simple\nsentiment-based portfolio trading strategy. Our findings provides support for\nthat information in publicly available news on the World Wide Web has a\nstatistically and economically significant impact on events in financial\nmarkets.\n"
    },
    {
        "paper_id": 2010.12017,
        "authors": "Behram Wali, Asad Khattak",
        "title": "Harnessing Ambient Sensing & Naturalistic Driving Systems to Understand\n  Links Between Driving Volatility and Crash Propensity in School Zones: A\n  generalized hierarchical mixed logit framework",
        "comments": null,
        "journal-ref": "Volume 114, May 2020, Pages 405-424",
        "doi": "10.1016/j.trc.2020.01.028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the advent of seemingly unstructured big data, and through seamless\nintegration of computation and physical components, cyber-physical systems\n(CPS) provide an innovative way to enhance safety and resiliency of transport\ninfrastructure. This study focuses on real world microscopic driving behavior\nand its relevance to school zone safety expanding the capability, usability,\nand safety of dynamic physical systems through data analytics. Driving behavior\nand school zone safety is a public health concern. The sequence of\ninstantaneous driving decisions and its variations prior to involvement in\nsafety critical events, defined as driving volatility, can be a leading\nindicator of safety. By harnessing unique naturalistic data on more than 41,000\nnormal, crash, and near-crash events featuring over 9.4 million temporal\nsamples of real-world driving, a characterization of volatility in microscopic\ndriving decisions is sought at school and non-school zone locations. A big data\nanalytic methodology is proposed for quantifying driving volatility in\nmicroscopic real-world driving decisions. Eight different volatility measures\nare then linked with detailed event specific characteristics, health history,\ndriving history, experience, and other factors to examine crash propensity at\nschool zones. A comprehensive yet fully flexible state-of-the-art generalized\nmixed logit framework is employed to fully account for distinct yet related\nmethodological issues of scale and random heterogeneity, containing multinomial\nlogit, random parameter logit, scaled logit, hierarchical scaled logit, and\nhierarchical generalized mixed logit as special cases. The results reveal that\nboth for school and non-school locations, drivers exhibited greater intentional\nvolatility prior to safety-critical events... ...\n"
    },
    {
        "paper_id": 2010.12038,
        "authors": "Miguel Baritto, Md Mashum Billal, S. M. Muntasir Nasim, Rumana Afroz\n  Sultana, Mohammad Arani, Ahmed Jawad Qureshi",
        "title": "Supporting Tool for The Transition of Existing Small and Medium\n  Enterprises Towards Industry 4.0",
        "comments": "This paper is accepted by the \"International Conference on Data\n  Analytics for Business and Industry\", and it will be published by IEEE\n  Xplore. (http://data20.uob.edu.bh/)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rapid growth of Industry 4.0 technologies such as big data, cloud\ncomputing, smart sensors, machine learning (ML), radio-frequency identification\n(RFID), robotics, 3D-printing, and Internet of Things (IoT) offers Small and\nMedium Enterprises (SMEs) the chance to improve productivity and efficiency,\nreduce cost and provide better customer experience, among other benefits. The\nmain purpose of this work is to propose a methodology to support SMEs managers\nin better understanding the specific requirements for the implementation of\nIndustry 4.0 solutions and the derived benefits within their firms. A proposed\nmethodology will be helpful for SMEs manager to take a decision regarding when\nand how to migrate to Industry 4.0.\n"
    },
    {
        "paper_id": 2010.12043,
        "authors": "Robert L. Ceres, Chris E. Forest, Klaus Keller",
        "title": "Trade-offs and synergies in managing coastal flood risk: A case study\n  for New York City",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decisions on how to manage future flood risks are frequently informed by both\nsophisticated and computationally expensive models. This complexity often\nlimits the representation of uncertainties and the consideration of strategies.\nHere, we use an intermediate complexity model framework that enables us to\nanalyze a rich set of strategies, objectives, and uncertainties. We find that\nallowing for more combinations of risk mitigation strategies can expand the\nsolution set, help explain synergies and trade-offs, and point to strategies\nthat can improve outcomes.\n"
    },
    {
        "paper_id": 2010.12158,
        "authors": "Xia Han, Zhibin Liang",
        "title": "Optimal per-loss reinsurance and investment to minimize the probability\n  of drawdown",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study an optimal reinsurance-investment problem in a risk\nmodel with two dependent classes of insurance business, where the two claim\nnumber processes are correlated through a common shock component. We assume\nthat the insurer can purchase per-loss reinsurance for each line of business\nand invest its surplus in a financial market consisting of a risk-free asset\nand a risky asset. Under the criterion of minimizing the probability of\ndrawdown, the closed-form expressions of the optimal reinsurance-investment\nstrategy and the corresponding value function are obtained. We show that the\noptimal reinsurance strategy is in the form of pure excess-of-loss reinsurance\nstrategy under the expected value principle, and under the variance premium\nprinciple, the optimal reinsurance strategy is in the form of pure quota-share\nreinsurance. Furthermore, we extend our model to the case where the insurance\ncompany involves $n$ $(n\\geq3)$ dependent classes of insurance business and the\noptimal results are derived explicitly as well.\n"
    },
    {
        "paper_id": 2010.12245,
        "authors": "Edoardo Vittori, Michele Trapletti, Marcello Restelli",
        "title": "Option Hedging with Risk Averse Reinforcement Learning",
        "comments": "Published to ICAIF2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show how risk-averse reinforcement learning can be used to\nhedge options. We apply a state-of-the-art risk-averse algorithm: Trust Region\nVolatility Optimization (TRVO) to a vanilla option hedging environment,\nconsidering realistic factors such as discrete time and transaction costs.\nRealism makes the problem twofold: the agent must both minimize volatility and\ncontain transaction costs, these tasks usually being in competition. We use the\nalgorithm to train a sheaf of agents each characterized by a different risk\naversion, so to be able to span an efficient frontier on the volatility-p\\&l\nspace. The results show that the derived hedging strategy not only outperforms\nthe Black \\& Scholes delta hedge, but is also extremely robust and flexible, as\nit can efficiently hedge options with different characteristics and work on\nmarkets with different behaviors than what was used in training.\n"
    },
    {
        "paper_id": 2010.1227,
        "authors": "Jun-ichi Maskawa and Koji Kuroda",
        "title": "Model of continuous random cascade processes in financial markets",
        "comments": "26 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article present a continuous cascade model of volatility formulated as a\nstochastic differential equation. Two independent Brownian motions are\nintroduced as random sources triggering the volatility cascade. One\nmultiplicatively combines with volatility; the other does so additively.\nAssuming that the latter acts perturbatively on the system, then the model\nparameters are estimated by application to an actual stock price time series.\nNumerical calculation of the Fokker--Planck equation derived from the\nstochastic differential equation is conducted using the estimated values of\nparameters. The results reproduce the pdf of the empirical volatility, the\nmultifractality of the time series, and other empirical facts.\n"
    },
    {
        "paper_id": 2010.1235,
        "authors": "Upasak Das, Prasenjit Sarkhel, Sania Ashraf",
        "title": "Love Thy Neighbor? Perceived Community Abidance and Private Compliance\n  to COVID-19 Norms in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Compliance with measures like social distancing, hand-washing and wearing\nmasks have emerged as the dominant strategy to combat health risk from the\nCOVID-19 pandemic. These behaviors are often argued to be pro-social, where one\nmust incur private cost to benefit or protect others. Using self-reported data\nacross India (n=934) through online survey, we assess if changes in perceived\ncommunity compliance can predict changes in individual compliance behavior,\ncontrolling for the potential confounders. We observe statistically significant\nand positive relationship between the two, even after accounting for omitted\nvariable bias, plausibly allowing us to view the results from a plausible\ncausal lens. Further, we find subsequent lockdowns such as the ones imposed in\nIndia, have a detrimental effect on individual compliance though the gains from\nhigher perceived community compliance seems to offset this loss. We also find\nthat sensitization through community can be particularly effective for people\nwith pre-existing co-morbidities. Our findings underscore the need for\nmulti-level behavioral interventions involving local actors and community\ninstitutions to sustain private compliance during the pandemic.\n"
    },
    {
        "paper_id": 2010.12351,
        "authors": "Yuhan Zhang, Cheng Chang",
        "title": "Modeling the US-China trade conflict: a utility theory approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper models the US-China trade conflict and attempts to analyze the\n(optimal) strategic choices. In contrast to the existing literature on the\ntopic, we employ the expected utility theory and examine the conflict\nmathematically. In both perfect information and incomplete information games,\nwe show that expected net gains diminish as the utility of winning increases\nbecause of the costs incurred during the struggle. We find that the best\nresponse function exists for China but not for the US during the conflict. We\nargue that the less the US coerces China to change its existing trade\npractices, the higher the US expected net gains. China's best choice is to\nmaintain the status quo, and any further aggression in its policy and behavior\nwill aggravate the situation.\n"
    },
    {
        "paper_id": 2010.12415,
        "authors": "J\\\"urgen E. Schatzmann and Bernhard Haslhofer",
        "title": "Exploring investor behavior in Bitcoin: a study of the disposition\n  effect",
        "comments": null,
        "journal-ref": "Digit Finance (2023)",
        "doi": "10.1007/s42521-023-00086-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investors commonly exhibit the disposition effect - the irrational tendency\nto sell their winning investments and hold onto their losing ones. While this\nphenomenon has been observed in many traditional markets, it remains unclear\nwhether it also applies to atypical markets like cryptoassets. This paper\ninvestigates the prevalence of the disposition effect in Bitcoin using\ntransactions targeting cryptoasset exchanges as proxies for selling\ntransactions. Our findings suggest that investors in Bitcoin were indeed\nsubject to the disposition effect, with varying intensity. They also show that\nthe disposition effect was not consistently present throughout the observation\nperiod. Its prevalence was more evident from the boom and bust year 2017\nonwards, as confirmed by various technical indicators. Our study suggests\nirrational investor behavior is also present in atypical markets like Bitcoin.\n"
    },
    {
        "paper_id": 2010.1255,
        "authors": "David Haritone Shikumo and Mwangi Mirie",
        "title": "Determinants of Lending to Small and Medium Enterprises by Commercial\n  Banks in Kenya",
        "comments": "7 Pages",
        "journal-ref": "IOSR Journal of Economics and Finance (IOSR-JEF) e-ISSN:\n  2321-5933, p-ISSN: 2321-5925.Volume 7, Issue 4. Ver. IV (Jul. - Aug. 2016),\n  PP 57-63 www.iosrjournals.org",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Small and Medium Enterprises (SMEs) access to external finance is an issue of\nsignificant research interest to academicians. Commercial banks consider many\nSMEs not to be credit worthy because of their inability to meet some banking\nrequirements. Hence, the objective of this study was to investigate what\ndetermines lending to SMEs by commercial banks in Kenya. To achieve the study\nobjectives, a descriptive research design was employed. The study undertook a\ncensus of the 43 commercial banks in Kenya, with full data being obtained for\n36 institutions. The study used secondary data from the annual published\nreports of commercial banks in Kenya for a period of 5 years from 2010-2014.\nThe data collected was analyzed through the multiple linear regression using\nthe Statistical Package for Social Studies version 20.The study established\nthat bank size and liquidity significantly influences (positively and\nnegatively, respectively) lending to SMEs by commercial banks in Kenya while\ncredit risk and interest rates have no significant influence on lending to SMEs\nby commercial banks in Kenya. The study recommends that lending to SMEs by\ncommercial banks in Kenya be enhanced by adopting policies that grow the\ncommercial banks.\n"
    },
    {
        "paper_id": 2010.12569,
        "authors": "King'ori S. Ngumo, Kioko W. Collins and Shikumo H. David",
        "title": "Determinants of Financial Performance of Microfinance Banks in Kenya",
        "comments": "8 pages",
        "journal-ref": "Research Journal of Finance and Accounting ISSN 2222-1697 (Paper)\n  ISSN 2222-2847 (Online) Vol.8, No.16, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Microfinance provides strength to boost the economic activities of low-income\nearners and thus contributes to eradication of poverty. However, microfinance\ninstitutions face stringent competition from commercial banks; the growth of\nmicroloan activities of commercial banks may confront microfinance institutions\nwith increased competition for borrowers. In Kenya, the micro finance sector\nhas extremely high competition indicated by the shifting market share and\nprofitability. This study sought to examine the determinants of financial\nperformance of Microfinance banks in Kenya. The study adopted a descriptive\nresearch design and used secondary data from 7 Microfinance banks for a period\nof 5 years from 2011 to 2015. The data collected was analyzed using correlation\nand regression analysis. The study found a positive and statistically\nsignificant relationship between operational efficiency, capital adequacy, firm\nsize and financial performance of microfinance banks in Kenya. However, the\nstudy found an insignificant negative relationship between liquidity risk,\ncredit risk and financial performance of microfinance banks in Kenya. The study\nconcluded that there is direct relationship between operational efficiency,\ncapital adequacy, firm size and financial performance of microfinance banks in\nKenya.\n"
    },
    {
        "paper_id": 2010.12577,
        "authors": "Hansjoerg Albrecher and Pierre-Olivier Goffard",
        "title": "On the profitability of selfish blockchain mining under consideration of\n  ruin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mining blocks on a blockchain equipped with a proof of work consensus\nprotocol is well-known to be resource-consuming. A miner bears the operational\ncost, mainly electricity consumption and IT gear, of mining, and is compensated\nby a capital gain when a block is discovered. This paper aims at quantifying\nthe profitability of mining when the possible event of ruin is also considered.\nThis is done by formulating a tractable stochastic model and using tools from\napplied probability and analysis, including the explicit solution of a certain\ntype of advanced functional differential equation. The expected profit at a\nfuture time point is determined for the situation when the miner follows the\nprotocol as well as when he/she withholds blocks. The obtained explicit\nexpressions allow to analyze the sensitivity with respect to the different\nmodel ingredients and to identify conditions under which selfish mining is a\nstrategic advantage.\n"
    },
    {
        "paper_id": 2010.12596,
        "authors": "David Haritone Shikumo, Oluoch Oluoch and Joshua Matanda Wepukhulu",
        "title": "Effect of Long-Term Debt on the Financial Growth of Non-Financial Firms\n  Listed at the Nairobi Securities Exchange",
        "comments": "9 pages",
        "journal-ref": "http://www.iosrjournals.org/iosr-jef/papers/Vol11-Issue5/Series-2/A1105020109.pdf\n  2020",
        "doi": "10.9790/5933-1105020109",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A significant number of the non-financial firms listed at Nairobi Securities\nExchange (NSE) have been experiencing declining financial performance which\ndeter investors from investing in such firms. The lenders are also not willing\nto lend to such firms. As such, the firms struggle to raise funds for their\noperations. Prudent financing decisions can lead to financial growth of the\nfirm. The purpose of this study is to assess the effect of Long-term debt on\nthe financial growth of Non-financial firms listed at Nairobi Securities\nExchange. Financial firms were excluded because of their specific sector\ncharacteristics and stringent regulatory framework. The study is guided by\nTrade-Off Theory and Theory of Growth of the Firm. Explanatory research design\nwas adopted. The population of the study comprised of 45 non-financial firms\nlisted at the NSE for a period of ten years from 2008 to 2017. The study\nconducted both descriptive statistics analysis and panel data analysis. The\nresult indicates that Long term debt explains 21.6% and 5.16% of variation in\nfinancial growth as measured by growth in earnings per share and growth in\nmarket capitalization respectively. Long term debt positively and significantly\ninfluences financial growth measured using both growth in earnings per share\nand growth in market capitalization. The study recommends that, the management\nof non-financial firms listed at Nairobi Securities Exchange to employ\nfinancing means that can improve the earnings per share, market capitalization\nand enhance the value of the firm for the benefit of its stakeholders.\n"
    },
    {
        "paper_id": 2010.12651,
        "authors": "Aur\\'elien Alfonsi and Adel Cherchali and Jose Arturo Infante Acevedo",
        "title": "Multilevel Monte-Carlo for computing the SCR with the standard formula\n  and other stress tests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the multilevel Monte-Carlo estimator for the expectation\nof a maximum of conditional expectations. This problem arises naturally when\nconsidering many stress tests and appears in the calculation of the interest\nrate module of the standard formula for the SCR. We obtain theoretical\nconvergence results that complements the recent work of Giles and Goda and\ngives some additional tractability through a parameter that somehow describes\nregularity properties around the maximum. We then apply the MLMC estimator to\nthe calculation of the SCR at future dates with the standard formula for an ALM\nsavings business on life insurance. We compare it with estimators obtained with\nLeast Square Monte-Carlo or Neural Networks. We find that the MLMC estimator is\ncomputationally more efficient and has the main advantage to avoid regression\nissues, which is particularly significant in the context of projection of a\nbalance sheet by an insurer due to the path dependency. Last, we discuss the\npotentiality of this numerical method and analyze in particular the effect of\nthe portfolio allocation on the SCR at future~dates.\n"
    },
    {
        "paper_id": 2010.12736,
        "authors": "Khanh Q. Nguyen",
        "title": "Conditional beta and uncertainty factor in the cryptocurrency pricing\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research is to assess cryptocurrencies with the conditional beta,\ncompared with prior studies based on unconditional beta or fixed beta. It is a\nnew approach to building a pricing model for cryptocurrencies. Therefore, we\nexpect that the use of conditional beta will increase the explanatory ability\nof factors in previous pricing models. Besides, this research is also a pioneer\nin placing the uncertainty factor in the cryptocurrency pricing model. Earlier\nstudies on cryptocurrency pricing have ignored this factor. However, it is a\nsignificant factor in the valuation of cryptocurrencies because uncertainty\nleads to investor sentiment and affects prices.\n"
    },
    {
        "paper_id": 2010.13036,
        "authors": "Isao Yagi, Shunya Maruyama, and Takanobu Mizuta",
        "title": "Trading Strategies of a Leveraged ETF in a Continuous Double Auction\n  Market Using an Agent-Based Simulation",
        "comments": null,
        "journal-ref": "Complexity, 3497689, Vol. 2020",
        "doi": "10.1155/2020/3497689",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A leveraged ETF is a fund aimed at achieving a rate of return several times\ngreater than that of the underlying asset such as Nikkei 225 futures. Recently,\nit has been suggested that rebalancing trades of a leveraged ETF may\ndestabilize the financial markets. An empirical study using an agent-based\nsimulation indicated that a rebalancing trade strategy could affect the price\nformation of an underlying asset market. However, no leveraged ETF trading\nmethod for suppressing the increase in volatility as much as possible has yet\nbeen proposed. In this paper, we compare different strategies of trading for a\nproposed trading model and report the results of our investigation regarding\nhow best to suppress an increase in market volatility. As a result, it was\nfound that as the minimum number of orders in a rebalancing trade increases,\nthe impact on the market price formation decreases.\n"
    },
    {
        "paper_id": 2010.13038,
        "authors": "Isao Yagi, Yuji Masuda, and Takanobu Mizuta",
        "title": "Analysis of the Impact of High-Frequency Trading on Artificial Market\n  Liquidity",
        "comments": null,
        "journal-ref": "IEEE Transactions on Computational Social Systems 2020",
        "doi": "10.1109/TCSS.2020.3019352",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many empirical studies have discussed market liquidity, which is regarded as\na measure of a booming financial market. Further, various indicators for\nobjectively evaluating market liquidity have also been proposed and their\nmerits have been discussed. In recent years, the impact of high-frequency\ntraders (HFTs) on financial markets has been a focal concern, but no studies\nhave systematically discussed their relationship with major market liquidity\nindicators, including volume, tightness, resiliency, and depth. In this study,\nwe used agent-based simulations to compare the major liquidity indicators in an\nartificial market where an HFT participated was compared to one where no HFT\nparticipated. The results showed that all liquidity indicators in the market\nwhere an HFT participated improved more than those in the market where no HFT\nparticipated. Furthermore, as a result of investigating the correlations\nbetween the major liquidity indicators in our simulations and the extant\nempirical literature, we found that market liquidity can be measured not only\nby the major liquidity indicators but also by execution rate. Therefore, it is\nsuggested that it could be appropriate to employ execution rate as a novel\nliquidity indicator in future studies.\n"
    },
    {
        "paper_id": 2010.13245,
        "authors": "Zhipu Zhou and Alexander Shkolnik and Sang-Yun Oh",
        "title": "Endogenous Representation of Asset Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Factor modeling of asset returns has been a dominant practice in investment\nscience since the introduction of the Capital Asset Pricing Model (CAPM) and\nthe Arbitrage Pricing Theory (APT). The factors, which account for the\nsystematic risk, are either specified or interpreted to be exogenous. They\nexplain a significant portion of the risk in large portfolios. We propose a\nframework that asks how much of the risk, that we see in equity markets, may be\nexplained by the asset returns themselves. To answer this question, we\ndecompose the asset returns into an endogenous component and the remainder, and\nanalyze the properties of the resulting risk decomposition. Statistical methods\nto estimate this decomposition from data are provided along with empirical\ntests. Our results point to the possibility that most of the risk in equity\nmarkets may be explained by a sparse network of interacting assets (or their\nissuing firms). This sparse network can give the appearance of a set exogenous\nfactors where, in fact, there may be none. We illustrate our results with\nseveral case studies.\n"
    },
    {
        "paper_id": 2010.1334,
        "authors": "Naftali Cohen, Simran Lamba, Prashant Reddy",
        "title": "What can be learned from satisfaction assessments?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Companies survey their customers to measure their satisfaction levels with\nthe company and its services. The received responses are crucial as they allow\ncompanies to assess their respective performances and find ways to make needed\nimprovements. This study focuses on the non-systematic bias that arises when\ncustomers assign numerical values in ordinal surveys. Using real customer\nsatisfaction survey data of a large retail bank, we show that the common\npractice of segmenting ordinal survey responses into uneven segments limit the\nvalue that can be extracted from the data. We then show that it is possible to\nassess the magnitude of the irreducible error under simple assumptions, even in\nreal surveys, and place the achievable modeling goal in perspective. We finish\nthe study by suggesting that a thoughtful survey design, which uses either a\ncareful binning strategy or proper calibration, can reduce the compounding\nnon-systematic error even in elaborated ordinal surveys. A possible application\nof the calibration method we propose is efficiently conducting targeted surveys\nusing active learning.\n"
    },
    {
        "paper_id": 2010.13397,
        "authors": "A. Georgantas",
        "title": "Robust Optimization Approaches for Portfolio Selection: A Computational\n  and Comparative Analysis",
        "comments": "64 pages, MSc Thesis, Department of Production Engineering and\n  Management, Technical University of Crete",
        "journal-ref": null,
        "doi": "10.26233/heallink.tuc.76532",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The field of portfolio selection is an active research topic, which combines\nelements and methodologies from various fields, such as optimization, decision\nanalysis, risk management, data science, forecasting, etc. The modeling and\ntreatment of deep uncertainties for future asset returns is a major issue for\nthe success of analytical portfolio selection models. Recently, robust\noptimization (RO) models have attracted a lot of interest in this area. RO\nprovides a computationally tractable framework for portfolio optimization based\non relatively general assumptions on the probability distributions of the\nuncertain risk parameters. Thus, RO extends the framework of traditional linear\nand non-linear models (e.g., the well-known mean-variance model), incorporating\nuncertainty through a formal and analytical approach into the modeling process.\nRobust counterparts of existing models can be considered as worst-case\nre-formulations as far as deviations of the uncertain parameters from their\nnominal values are concerned. Although several RO models have been proposed in\nthe literature focusing on various risk measures and different types of\nuncertainty sets about asset returns, analytical empirical assessments of their\nperformance have not been performed in a comprehensive manner. The objective of\nthis study is to fill in this gap in the literature. More specifically, we\nconsider different types of RO models based on popular risk measures and\nconduct an extensive comparative analysis of their performance using data from\nthe US market during the period 2005-2016.\n"
    },
    {
        "paper_id": 2010.13411,
        "authors": "Kamran Zakaria, Saeed Hafeez",
        "title": "Options Pricing for Two Stocks by Black Sholes Time Fractional Order\n  NonLinear Partial Differential Equation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/iCoMET48670.2020.9073866",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The BS equations with fractional order two asset price models give a better\nprediction of options pricing in the monetary market. In this paper, the\nchanged form of BS-condition with two asset price models dependent on the\nLiovelle-Caputo derivative for good predictions of options prices are utilized.\nThe analytical solution is demonstrated in form of convergent infinite series\nand obtained by the properties of Samudu Transform.\n"
    },
    {
        "paper_id": 2010.13438,
        "authors": "Andrea Araldo, Andr\\'e de Palma, Souhila Arib, Vincent Gauthier,\n  Romain Sere, Youssef Chaabouni, Oussama Kharouaa, and Ado Adamou Abba Ari",
        "title": "Pooling for First and Last Mile: Integrating Carpooling and Transit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While carpooling is widely adopted for long travels, it is by construction\ninefficient for daily commuting, where it is difficult to match drivers and\nriders, sharing similar origin, destination and time. To overcome this\nlimitation, we present an Integrated system, which integrates carpooling into\ntransit, in the line of the philosophy of Mobility as a Service. Carpooling\nacts as feeder to transit and transit stations act as consolidation points,\nwhere trips of riders and drivers meet, increasing potential matching. We\npresent algorithms to construct multimodal rider trips (including transit and\ncarpooling legs) and driver detours. Simulation shows that our Integrated\nsystem increases transit ridership and reduces auto-dependency, with respect to\ncurrent practice, in which carpooling and transit are operated separately.\nIndeed, the Integrated system decreases the number of riders who are left with\nno feasible travel option and would thus be forced to use private cars. The\nsimulation code is available as open source.\n"
    },
    {
        "paper_id": 2010.13471,
        "authors": "Antti J. Tanskanen",
        "title": "Deep reinforced learning enables solving rich discrete-choice life cycle\n  models to analyze social security reforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Discrete-choice life cycle models of labor supply can be used to estimate how\nsocial security reforms influence employment rate. In a life cycle model,\noptimal employment choices during the life course of an individual must be\nsolved. Mostly, life cycle models have been solved with dynamic programming,\nwhich is not feasible when the state space is large, as often is the case in a\nrealistic life cycle model. Solving a complex life cycle model requires the use\nof approximate methods, such as reinforced learning algorithms. We compare how\nwell a deep reinforced learning algorithm ACKTR and dynamic programming solve a\nrelatively simple life cycle model. To analyze results, we use a selection of\nstatistics and also compare the resulting optimal employment choices at various\nstates. The statistics demonstrate that ACKTR yields almost as good results as\ndynamic programming. Qualitatively, dynamic programming yields more spiked\naggregate employment profiles than ACKTR. The results obtained with ACKTR\nprovide a good, yet not perfect, approximation to the results of dynamic\nprogramming. In addition to the baseline case, we analyze two social security\nreforms: (1) an increase of retirement age, and (2) universal basic income. Our\nresults suggest that reinforced learning algorithms can be of significant value\nin developing social security reforms.\n"
    },
    {
        "paper_id": 2010.13541,
        "authors": "Dongming Wei and Yogi Ahmad Erlangga and Gulzat Zhumakhanova",
        "title": "A Finite Element Approach to the Numerical Solutions of Leland's Mode",
        "comments": "13 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, finite element method is applied to Leland's model for\nnumerical simulation of option pricing with transaction costs. Spatial finite\nelement models based on P1 and/or P2 elements are formulated in combination\nwith a Crank-Nicolson-type temporal scheme. The temporal scheme is implemented\nusing the Rannacher approach. Examples with several sets of parameter values\nare presented and compared with finite difference results in the literature.\nSpatial-temporal mesh-size ratios are observed for controlling the stability of\nour method. Our results compare favorably with the finite difference results in\nthe literature for the model.\n"
    },
    {
        "paper_id": 2010.1363,
        "authors": "N.S. Gonchar",
        "title": "Derivatives Pricing in Non-Arbitrage Market",
        "comments": "67 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The general method is proposed for constructing a family of martingale\nmeasures for a wide class of evolution of risky assets. The sufficient\nconditions are formulated for the evolution of risky assets under which the\nfamily of equivalent martingale measures to the original measure is a non-empty\nset. The set of martingale measures is constructed from a set of strictly\nnonneg ative random variables, satisfying certain conditions. The inequalities\nare obtained for the non-negative random variables satisfying certain\nconditions. Using these inequalities, a new simple proof of optional\ndecomposition theorem for the nonnegative super-martingale is proposed. The\nfamily of spot measures is introduced and the representation is found for them.\nThe conditions are found under which each martingale measure is an integral\nover the set of spot measures. On the basis of nonlinear processes such as ARCH\nand GARCH, the parametric family of random processes is introduced for which\nthe interval of non-arbitrage prices are found. The formula is obtained for the\nfair price of the contract with option of European type for the considered\nparametric processes. The parameters of the introduced random processes are\nestimated and the estimate is found at which the fair price of contract with\noption is the least.\n"
    },
    {
        "paper_id": 2010.13843,
        "authors": "Kristoffer Andersson and Cornelis W. Oosterlee",
        "title": "Deep learning for CVA computations of large portfolios of financial\n  derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a neural network-based method for CVA computations\nof a portfolio of derivatives. In particular, we focus on portfolios consisting\nof a combination of derivatives, with and without true optionality,\n\\textit{e.g.,} a portfolio of a mix of European- and Bermudan-type derivatives.\nCVA is computed, with and without netting, for different levels of WWR and for\ndifferent levels of credit quality of the counterparty. We show that the CVA is\noverestimated with up to 25\\% by using the standard procedure of not adjusting\nthe exercise strategy for the default-risk of the counterparty. For the\nExpected Shortfall of the CVA dynamics, the overestimation was found to be more\nthan 100\\% in some non-extreme cases.\n"
    },
    {
        "paper_id": 2010.13891,
        "authors": "Sidra Mehtab and Jaydip Sen",
        "title": "Stock Price Prediction Using CNN and LSTM-Based Deep Learning Models",
        "comments": "The paper consists of 7 pages, 10 figures, and 5 tables. This is the\n  accepted version of our paper in the IEEE International Conference on\n  Decision Aid Sciences and Applications (DASA'20), November 8-9, 2020, Bahrain",
        "journal-ref": null,
        "doi": "10.1109/DASA51403.2020.9317207",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing robust and accurate predictive models for stock price prediction\nhas been an active area of research for a long time. While on one side, the\nsupporters of the efficient market hypothesis claim that it is impossible to\nforecast stock prices accurately, many researchers believe otherwise. There\nexist propositions in the literature that have demonstrated that if properly\ndesigned and optimized, predictive models can very accurately and reliably\npredict future values of stock prices. This paper presents a suite of deep\nlearning based models for stock price prediction. We use the historical records\nof the NIFTY 50 index listed in the National Stock Exchange of India, during\nthe period from December 29, 2008 to July 31, 2020, for training and testing\nthe models. Our proposition includes two regression models built on\nconvolutional neural networks and three long and short term memory network\nbased predictive models. To forecast the open values of the NIFTY 50 index\nrecords, we adopted a multi step prediction technique with walk forward\nvalidation. In this approach, the open values of the NIFTY 50 index are\npredicted on a time horizon of one week, and once a week is over, the actual\nindex values are included in the training set before the model is trained\nagain, and the forecasts for the next week are made. We present detailed\nresults on the forecasting accuracies for all our proposed models. The results\nshow that while all the models are very accurate in forecasting the NIFTY 50\nopen values, the univariate encoder decoder convolutional LSTM with the\nprevious two weeks data as the input is the most accurate model. On the other\nhand, a univariate CNN model with previous one week data as the input is found\nto be the fastest model in terms of its execution speed.\n"
    },
    {
        "paper_id": 2010.13892,
        "authors": "Amir Mukeri, Habibullah Shaikh, Dr. D.P. Gaikwad",
        "title": "Financial Data Analysis Using Expert Bayesian Framework For Bankruptcy\n  Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, bankruptcy forecasting has gained lot of attention from\nresearchers as well as practitioners in the field of financial risk management.\nFor bankruptcy prediction, various approaches proposed in the past and\ncurrently in practice relies on accounting ratios and using statistical\nmodeling or machine learning methods. These models have had varying degrees of\nsuccesses. Models such as Linear Discriminant Analysis or Artificial Neural\nNetwork employ discriminative classification techniques. They lack explicit\nprovision to include prior expert knowledge. In this paper, we propose another\nroute of generative modeling using Expert Bayesian framework. The biggest\nadvantage of the proposed framework is an explicit inclusion of expert judgment\nin the modeling process. Also the proposed methodology provides a way to\nquantify uncertainty in prediction. As a result the model built using Bayesian\nframework is highly flexible, interpretable and intuitive in nature. The\nproposed approach is well suited for highly regulated or safety critical\napplications such as in finance or in medical diagnosis. In such cases accuracy\nin the prediction is not the only concern for decision makers. Decision makers\nand other stakeholders are also interested in uncertainty in the prediction as\nwell as interpretability of the model. We empirically demonstrate these\nbenefits of proposed framework on real world dataset using Stan, a\nprobabilistic programming language. We found that the proposed model is either\ncomparable or superior to the other existing methods. Also resulting model has\nmuch less False Positive Rate compared to many existing state of the art\nmethods. The corresponding R code for the experiments is available at Github\nrepository.\n"
    },
    {
        "paper_id": 2010.13915,
        "authors": "Szymon Peszat, Dariusz Zawisza",
        "title": "The investor problem based on the HJM model",
        "comments": "v2 - 26 pages, detailed calculations of G2++ model, extended proof of\n  theorem 4.1, two references added( [2] and [33]), v3 - 28 pages, revised\n  version after reviews, (v4) - 30 pages, language corrections, (v5),(v6) - 29\n  pages, final corrections",
        "journal-ref": "Annales Polonici Mathematici 127 (2021), 241-269",
        "doi": "10.4064/ap210429-12-11",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a consumption-investment problem (both on finite and infinite\ntime horizon) in which the investor has an access to the bond market. In our\napproach prices of bonds with different maturities are described by the general\nHJM factor model. We assume that the bond market consists of entire family of\nrolling bonds and the investment strategy is a general signed measure\ndistributed on all real numbers representing time to maturity specifications\nfor different rolling bonds. In particular, we can consider portfolio of coupon\nbonds. The investor's objective is to maximize time-additive utility of the\nconsumption process. We solve the problem by means of the HJB equation for\nwhich we prove required regularity of its solution and all required estimates\nto ensure applicability of the verification theorem. Explicit calculations for\naffine models are presented.\n"
    },
    {
        "paper_id": 2010.13928,
        "authors": "Agostino Capponi and Zhaoyu Zhang",
        "title": "Risk Preferences and Efficiency of Household Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach to infer investors' risk preferences from their\nportfolio choices, and then use the implied risk preferences to measure the\nefficiency of investment portfolios. We analyze a dataset spanning a period of\nsix years, consisting of end of month stock trading records, along with\ninvestors' demographic information and self-assessed financial knowledge.\nUnlike estimates of risk aversion based on the share of risky assets, our\nstatistical analysis suggests that the implied risk aversion coefficient of an\ninvestor increases with her wealth and financial literacy. Portfolio\ndiversification, Sharpe ratio, and expected portfolio returns correlate\npositively with the efficiency of the portfolio, whereas a higher standard\ndeviation reduces the efficiency of the portfolio. We find that affluent and\nfinancially educated investors as well as those holding retirement related\naccounts hold more efficient portfolios.\n"
    },
    {
        "paper_id": 2010.14113,
        "authors": "Philip Chen, Edward J Oughton, Pete Tyler, Mo Jia and Jakub Zagdanski",
        "title": "Evaluating the impact of next generation broadband on local business\n  creation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Basic broadband connectivity is regarded as generally having a positive\nmacroeconomic effect. However, over the past decade there has been an emerging\nschool of thought suggesting the impacts of upgrading to higher speed broadband\nhave been overstated, potentially leading to the inefficient allocation of\ntaxpayer-funded subsidies. In this analysis we model the impacts of Next\nGeneration Access on new business creation using high-resolution panel data.\nAfter controlling for a range of factors, the results provide evidence of a\nsmall but significant negative impact of high-speed broadband on new business\ncreation over the study period which we suggest could be due to two factors.\nFirstly, moving from basic to high-speed broadband provides few benefits to\nenable new businesses being formed. Secondly, strong price competition and\nmarket consolidation from online service providers (e.g. Amazon etc.) may be\ndeterring new business start-ups. This analysis provides another piece of\nevidence to suggest that the economic impact of broadband is more nuanced than\nthe debate has traditionally suggested. Our conjecture is that future policy\ndecisions need to be more realistic about the potential economic impacts of\nbroadband, including those effects that could be negative on the stock of local\nbusinesses and therefore the local tax base.\n"
    },
    {
        "paper_id": 2010.14646,
        "authors": "Erhan Bayraktar, Gaoyue Guo, Wenpin Tang, Yuming Zhang",
        "title": "McKean-Vlasov equations involving hitting times: blow-ups and global\n  solvability",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with the analysis of blow-ups for two McKean-Vlasov\nequations involving hitting times. Let $(B(t); \\, t \\ge 0)$ be standard\nBrownian motion, and $\\tau:= \\inf\\{t \\ge 0: X(t) \\le 0\\}$ be the hitting time\nto zero of a given process $X$. The first equation is $X(t) = X(0) + B(t) -\n\\alpha \\mathbb{P}(\\tau \\le t)$. We provide a simple condition on $\\alpha$ and\nthe distribution of $X(0)$ such that the corresponding Fokker-Planck equation\nhas no blow-up, and thus the McKean-Vlasov dynamics is well-defined for all\ntime $t \\ge 0$. Our approach relies on a connection between the McKean-Vlasov\nequation and the supercooled Stefan problem, as well as several comparison\nprinciples. The second equation is $X(t) = X(0) + \\beta t + B(t) + \\alpha \\log\n\\mathbb{P}(\\tau > t)$, whose Fokker-Planck equation is non-local. We prove that\nfor $\\beta > 0$ sufficiently large and $\\alpha$ no greater than a sufficiently\nsmall positive constant, there is no blow-up and the McKean-Vlasov dynamics is\nwell-defined for all time $t \\ge 0$. The argument is based on a new transform,\nwhich removes the non-local term, followed by a relative entropy analysis.\n"
    },
    {
        "paper_id": 2010.14651,
        "authors": "Frederik Plesner Lyngse",
        "title": "Liquidity Constraints and Demand for Healthcare: Evidence from Danish\n  Welfare Recipients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Are low-income individuals relying on government transfers liquidity\nconstrained by the end of the month to a degree that they postpone medical\ntreatment? I investigate this question using Danish administrative data\ncomprising the universe of welfare recipients and the filling of all\nprescription drugs. I find that on transfer income payday, recipients have a\n52% increase in the propensity to fill a prescription. By separating\nprophylaxis drugs used to treat chronic conditions, where the patient can\nanticipate the need to fill the prescription, e.g. cholesterol-lowering\nstatins, I find an increase of up to 99% increase on payday. Even for drugs\nused to treat acute conditions, where timely treatment is essential, I find a\n22% increase on payday for antibiotics and a 5-8% decrease in the four days\npreceding payday. Lastly, exploiting the difference in day the doctor write the\nprescription and the day the patient fill it, I show that liquidity constraints\nis the key operating mechanism for postponing antibiotic treatment.\n"
    },
    {
        "paper_id": 2010.14668,
        "authors": "Alessandro Cantelmo and Giovanni Melina",
        "title": "Sectoral Labor Mobility and Optimal Monetary Policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How should central banks optimally aggregate sectoral inflation rates in the\npresence of imperfect labor mobility across sectors? We study this issue in a\ntwo-sector New-Keynesian model and show that a lower degree of sectoral labor\nmobility, ceteris paribus, increases the optimal weight on inflation in a\nsector that would otherwise receive a lower weight. We analytically and\nnumerically find that, with limited labor mobility, adjustment to asymmetric\nshocks cannot fully occur through the reallocation of labor, thus putting more\npressure on wages, causing inefficient movements in relative prices, and\ncreating scope for central banks intervention. These findings challenge\nstandard central banks practice of computing sectoral inflation weights based\nsolely on sector size, and unveil a significant role for the degree of sectoral\nlabor mobility to play in the optimal computation. In an extended estimated\nmodel of the U.S. economy, featuring customary frictions and shocks, the\nestimated inflation weights imply a decrease in welfare up to 10 percent\nrelative to the case of optimal weights.\n"
    },
    {
        "paper_id": 2010.14669,
        "authors": "John R. Moser",
        "title": "Minimum Wage, Labor Equilibrium, and the Productivity Horizon: A Visual\n  Examination",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, I present a visual representation of the relationship between\nmean hourly total compensation divided by per-capita GDP, hours worked per\ncapita, and the labor share, and show the represented labor equilibrium\nequation is the definition of the labor share. I also present visual\nexamination of the productivity horizon and wage compression, and use these to\nshow the relationship between productivity, available employment per capita,\nand minimum wage. From this I argue that wages are measured in relation to\nper-capita GDP, and that minimum wage controls income inequality and\nproductivity growth.\n"
    },
    {
        "paper_id": 2010.14673,
        "authors": "Mario Ghossoub and Jesse Hall and David Saunders",
        "title": "Maximum Spectral Measures of Risk with given Risk Factor Marginal\n  Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of determining an upper bound for the value of a\nspectral risk measure of a loss that is a general nonlinear function of two\nfactors whose marginal distributions are known, but whose joint distribution is\nunknown. The factors may take values in complete separable metric spaces. We\nintroduce the notion of Maximum Spectral Measure (MSP), as a worst-case\nspectral risk measure of the loss with respect to the dependence between the\nfactors. The MSP admits a formulation as a solution to an optimization problem\nthat has the same constraint set as the optimal transport problem, but with a\nmore general objective function. We present results analogous to the\nKantorovich duality, and we investigate the continuity properties of the\noptimal value function and optimal solution set with respect to perturbation of\nthe marginal distributions. Additionally, we provide an asymptotic result\ncharacterizing the limiting distribution of the optimal value function when the\nfactor distributions are simulated from finite sample spaces. The special case\nof Expected Shortfall and the resulting Maximum Expected Shortfall is also\nexamined.\n"
    },
    {
        "paper_id": 2010.14695,
        "authors": "Erhan Bayraktar and Thomas Bernhardt",
        "title": "On the Continuity of the Root Barrier",
        "comments": "To appear in the Proceedings of the AMS. Keywords: Skorokhod\n  embedding problem; Root's solution; barrier function; continuity",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the barrier function in Root's solution to the Skorokhod\nembedding problem is continuous and finite at every point where the target\nmeasure has no atom and its absolutely continuous part is locally bounded away\nfrom zero.\n"
    },
    {
        "paper_id": 2010.14979,
        "authors": "Guido Ascari, Anna Florio, Alessandro Gobbi",
        "title": "Monetary-fiscal interactions under price level targeting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The adoption of a \"makeup\" strategy is one of the proposals in the ongoing\nreview of the Fed's monetary policy framework. Another suggestion, to avoid the\nzero lower bound, is a more active role for fiscal policy. We put together\nthese ideas to study monetary-fiscal interactions under price level targeting.\nUnder price level targeting and a fiscally-led regime, we find that following a\ndeflationary demand shock: (i) the central bank increases (rather than\ndecreases) the policy rate; (ii) the central bank, thus, avoids the zero lower\nbound; (iii) price level targeting is generally welfare improving if compared\nto inflation targeting.\n"
    },
    {
        "paper_id": 2010.15105,
        "authors": "Juan C. Henao-Londono, Sebastian M. Krause and Thomas Guhr",
        "title": "Price response functions and spread impact in correlated financial\n  markets",
        "comments": "19 pages, 12 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.1140/epjb/s10051-021-00077-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent research on the response of stock prices to trading activity revealed\nlong lasting effects, even across stocks of different companies. These results\nimply non-Markovian effects in price formation and when trading many stocks at\nthe same time, in particular trading costs and price correlations. How the\nprice response is measured depends on data set and research focus. However, it\nis important to clarify, how the details of the price response definition\nmodify the results. Here, we evaluate different price response implementations\nfor the Trades and Quotes (TAQ) data set from the NASDAQ stock market and find\nthat the results are qualitatively the same for two different definitions of\ntime scale, but the response can vary by up to a factor of two. Further, we\nshow the key importance of the order between trade signs and returns,\ndisplaying the changes in the signal strength. Moreover, we confirm the\ndominating contribution of immediate price response directly after a trade, as\nwe find that delayed responses are suppressed. Finally, we test the impact of\nthe spread in the price response, detecting that large spreads have stronger\nimpact.\n"
    },
    {
        "paper_id": 2010.15111,
        "authors": "Elizabeth Fons, Paula Dawson, Xiao-jun Zeng, John Keane and Alexandros\n  Iosifidis",
        "title": "Evaluating data augmentation for financial time series classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data augmentation methods in combination with deep neural networks have been\nused extensively in computer vision on classification tasks, achieving great\nsuccess; however, their use in time series classification is still at an early\nstage. This is even more so in the field of financial prediction, where data\ntends to be small, noisy and non-stationary. In this paper we evaluate several\naugmentation methods applied to stocks datasets using two state-of-the-art deep\nlearning models. The results show that several augmentation methods\nsignificantly improve financial performance when used in combination with a\ntrading strategy. For a relatively small dataset ($\\approx30K$ samples),\naugmentation methods achieve up to $400\\%$ improvement in risk adjusted return\nperformance; for a larger stock dataset ($\\approx300K$ samples), results show\nup to $40\\%$ improvement.\n"
    },
    {
        "paper_id": 2010.15165,
        "authors": "Alice Albonico, Guido Ascari, Alessandro Gobbi",
        "title": "The public debt multiplier",
        "comments": null,
        "journal-ref": "Journal of Economic Dynamics and Control,Volume 132, November\n  2021, 104204",
        "doi": "10.1016/j.jedc.2021.104204",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the effects on economic activity of a pure temporary change in\ngovernment debt and the relationship between the debt multiplier and the level\nof debt in an overlapping generations framework. The debt multiplier is\npositive but quite small during normal times while it is much larger during\ncrises. Moreover, it increases with the steady state level of debt. Hence, the\ncall for fiscal consolidation during recessions seems ill-advised. Finally, a\nrise in the steady state debt-to-GDP level increases the steady state real\ninterest rate providing more room for manoeuvre to monetary policy to fight\ndeflationary shocks.\n"
    },
    {
        "paper_id": 2010.15223,
        "authors": "Cheng Cheng",
        "title": "Design Diversity for Improving Efficiency and Reducing Risk in Oil and\n  Gas Well Stimulation under Uncertain Reservoir Conditions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hydraulic fracturing stimulates fracture swarm in reservoir formation though\npressurized injection fluid. However restricted by the availability of\nformation data, the variability embraced by reservoir keeps uncertain, driving\nunstable gas recovery along with low resource efficiency, being responsible for\nresource scarcity, contaminated water, and injection-induced earthquake.\nResource efficiency is qualified though new determined energy efficiency, a\nscale of recovery and associated environmental footprint. To maximize energy\nefficiency while minimize its' variation, we issue picked designs at reservoir\nconditions dependent optimal probabilities, assembling high efficiency\nportfolios and low risk portfolios for portfolio combination, which balance the\nvariation and efficiency at optimal by adjusting the proportion of each\nportfolio. Relative to regular design for one well, the optimal portfolio\ncombination applied in multiple wells receive remarkable variation reduction\nmeanwhile substantial energy efficiency increase, in response to the call of\nmore recovery per unit investment and less environment cost per unit nature gas\nextracted.\n"
    },
    {
        "paper_id": 2010.15254,
        "authors": "Zachary Feinstein and Andreas Sojmark",
        "title": "Dynamic Default Contagion in Heterogeneous Interbank Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we provide a simple setting that connects the structural\nmodelling approach of Gai-Kapadia interbank networks with the mean-field\napproach to default contagion. To accomplish this we make two key\ncontributions. First, we propose a dynamic default contagion model with\nendogenous early defaults for a finite set of banks, generalising the\nGai-Kapadia framework. Second, we reformulate this system as a stochastic\nparticle system leading to a limiting mean-field problem. We study the\nexistence of these clearing systems and, for the mean-field problem, the\ncontinuity of the system response.\n"
    },
    {
        "paper_id": 2010.15263,
        "authors": "Jean-Paul Renne and Guillaume Roussellet and Gustavo Schwenkler",
        "title": "Preventing COVID-19 Fatalities: State versus Federal Policies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Are COVID-19 fatalities large when a federal government does not enforce\ncontainment policies and instead allow states to implement their own policies?\nWe answer this question by developing a stochastic extension of a SIRD\nepidemiological model for a country composed of multiple states. Our model\nallows for interstate mobility. We consider three policies: mask mandates,\nstay-at-home orders, and interstate travel bans. We fit our model to daily U.S.\nstate-level COVID-19 death counts and exploit our estimates to produce various\npolicy counterfactuals. While the restrictions imposed by some states inhibited\na significant number of virus deaths, we find that more than two-thirds of U.S.\nCOVID-19 deaths could have been prevented by late November 2020 had the federal\ngovernment enforced federal mandates as early as some of the earliest states\ndid. Our results quantify the benefits of early actions by a federal government\nfor the containment of a pandemic.\n"
    },
    {
        "paper_id": 2010.15403,
        "authors": "Marcin W\\k{a}torek, Stanis{\\l}aw Dro\\.zd\\.z, Jaros{\\l}aw Kwapie\\'n,\n  Ludovico Minati, Pawe{\\l} O\\'swi\\k{e}cimka, Marek Stanuszek",
        "title": "Multiscale characteristics of the emerging global cryptocurrency market",
        "comments": null,
        "journal-ref": "Physics Reports 901 1-82 (2021)",
        "doi": "10.1016/j.physrep.2020.10.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The review introduces the history of cryptocurrencies, offering a description\nof the blockchain technology behind them. Differences between cryptocurrencies\nand the exchanges on which they are traded have been shown. The central part\nsurveys the analysis of cryptocurrency price changes on various platforms. The\nstatistical properties of the fluctuations in the cryptocurrency market have\nbeen compared to the traditional markets. With the help of the latest\nstatistical physics methods the non-linear correlations and multiscale\ncharacteristics of the cryptocurrency market are analyzed. In the last part the\nco-evolution of the correlation structure among the 100 cryptocurrencies having\nthe largest capitalization is retraced. The detailed topology of cryptocurrency\nnetwork on the Binance platform from bitcoin perspective is also considered.\nFinally, an interesting observation on the Covid-19 pandemic impact on the\ncryptocurrency market is presented and discussed: recently we have witnessed a\n\"phase transition\" of the cryptocurrencies from being a hedge opportunity for\nthe investors fleeing the traditional markets to become a part of the global\nmarket that is substantially coupled to the traditional financial instruments\nlike the currencies, stocks, and commodities.\n  The main contribution is an extensive demonstration that structural\nself-organization in the cryptocurrency markets has caused the same to attain\ncomplexity characteristics that are nearly indistinguishable from the Forex\nmarket at the level of individual time-series. However, the cross-correlations\nbetween the exchange rates on cryptocurrency platforms differ from it. The\ncryptocurrency market is less synchronized and the information flows more\nslowly, which results in more frequent arbitrage opportunities. The methodology\nused in the review allows the latter to be detected, and lead-lag relationships\nto be discovered.\n"
    },
    {
        "paper_id": 2010.15484,
        "authors": "Martine J Barons and Willy Aspinall",
        "title": "Anticipated impacts of Brexit scenarios on UK food prices and\n  implications for policies on poverty and health: a structured expert\n  judgement update",
        "comments": "6 pages one table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Food insecurity is associated with increased risk for several health\nconditions and with increased national burden of chronic disease. Key\ndeterminants for household food insecurity are income and food costs. Forecasts\nshow household disposable income for 2020 expected to fall and for 2021 to rise\nonly slightly. Prices are forecast to rise. Thus, future increased food prices\nwould be a significant driver of greater food insecurity. Structured expert\njudgement elicitation, a well-established method for quantifying uncertainty,\nusing experts. In July 2020, each expert estimated the median, 5th percentile\nand 95th percentile quantiles of changes in price to April 2022 for ten food\ncategories under three end-2020 settlement Brexit scenarios: A: full WTO terms;\nB: a moderately disruptive trade agreement (better than WTO); C: a minimally\ndisruptive trade agreement. When combined in proportions for calculate Consumer\nPrices Index food basket costs, the median food price change under full WTO\nterms is expected to be +17.9% [90% credible interval:+5.2%, +35.1%]; with\nmoderately disruptive trade agreement: +13.2% [+2.6%, +26.4%] and with a\nminimally disruptive trade agreement +9.3% [+0.8%, +21.9%]. The number of\nhouseholds experiencing food insecurity and its severity are likely to increase\nbecause of expected sizeable increases in median food prices in the months\nafter Brexit, whereas low income group spending on food is unlikely to\nincrease, and may be further eroded by other factors not considered here (e.g.\nCOVID-19). Higher increases are more likely than lower rises and towards the\nupper limits, these would entail severe impacts. Research showing a low food\nbudget leads to increasingly poor diet suggests that demand for health services\nin both the short and longer term is likely to increase due to the effects of\nfood insecurity on the incidence and management of diet-sensitive conditions.\n"
    },
    {
        "paper_id": 2010.15586,
        "authors": "Xianchao Wu",
        "title": "Event-Driven Learning of Systematic Behaviours in Stock Markets",
        "comments": "11 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is reported that financial news, especially financial events expressed in\nnews, provide information to investors' long/short decisions and influence the\nmovements of stock markets. Motivated by this, we leverage financial event\nstreams to train a classification neural network that detects latent\nevent-stock linkages and stock markets' systematic behaviours in the U.S. stock\nmarket. Our proposed pipeline includes (1) a combined event extraction method\nthat utilizes Open Information Extraction and neural co-reference resolution,\n(2) a BERT/ALBERT enhanced representation of events, and (3) an extended\nhierarchical attention network that includes attentions on event, news and\ntemporal levels. Our pipeline achieves significantly better accuracies and\nhigher simulated annualized returns than state-of-the-art models when being\napplied to predicting Standard\\&Poor 500, Dow Jones, Nasdaq indices and 10\nindividual stocks.\n"
    },
    {
        "paper_id": 2010.15611,
        "authors": "Faizaan Pervaiz, Christopher Goh, Ashley Pennington, Samuel Holt,\n  James West, Shaun Ng",
        "title": "Fear and Volatility in Digital Assets",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show Bitcoin implied volatility on a 5 minute time horizon is modestly\npredictable from price, volatility momentum and alternative data including\nsentiment and engagement. Lagged Bitcoin index price and volatility movements\ncontribute to the model alongside Google Trends with markets responding often\nseveral hours later. The code and datasets used in this paper can be found at\nhttps://github.com/Globe-Research/bitfear.\n"
    },
    {
        "paper_id": 2010.15709,
        "authors": "Dietmar Pfeifer, Doreen Strassburger, Joerg Philipps",
        "title": "Modelling and simulation of dependence structures in nonlife insurance\n  with Bernstein copulas",
        "comments": "paper presented on the International ASTIN Colloquium 2009, Helsinki",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we review Bernstein and grid-type copulas for arbitrary\ndimensions and general grid resolutions in connection with discrete random\nvectors possessing uniform margins. We further suggest a pragmatic way to fit\nthe dependence structure of multivariate data to Bernstein copulas via\ngrid-type copulas and empirical contingency tables. Finally, we discuss a Monte\nCarlo study for the simulation and PML estimation for aggregate dependent\nlosses form observed windstorm and flooding data.\n"
    },
    {
        "paper_id": 2010.15757,
        "authors": "Stefan Kremsner and Alexander Steinicke and Michaela Sz\\\"olgyenyi",
        "title": "A deep neural network algorithm for semilinear elliptic PDEs with\n  applications in insurance mathematics",
        "comments": null,
        "journal-ref": "Risks, 8(4):136, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In insurance mathematics optimal control problems over an infinite time\nhorizon arise when computing risk measures. Their solutions correspond to\nsolutions of deterministic semilinear (degenerate) elliptic partial\ndifferential equations. In this paper we propose a deep neural network\nalgorithm for solving such partial differential equations in high dimensions.\nThe algorithm is based on the correspondence of elliptic partial differential\nequations to backward stochastic differential equations with random terminal\ntime.\n"
    },
    {
        "paper_id": 2010.15779,
        "authors": "Carmine De Franco, Johann Nicolle and Huy\\^en Pham",
        "title": "Discrete-time portfolio optimization under maximum drawdown constraint\n  with partial information and deep learning resolution",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.21502.61766",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a discrete-time portfolio selection problem with partial information\nand maxi\\-mum drawdown constraint. Drift uncertainty in the multidimensional\nframework is modeled by a prior probability distribution. In this Bayesian\nframework, we derive the dynamic programming equation using an appropriate\nchange of measure, and obtain semi-explicit results in the Gaussian case. The\nlatter case, with a CRRA utility function is completely solved numerically\nusing recent deep learning techniques for stochastic optimal control problems.\nWe emphasize the informative value of the learning strategy versus the\nnon-learning one by providing empirical performance and sensitivity analysis\nwith respect to the uncertainty of the drift. Furthermore, we show numerical\nevidence of the close relationship between the non-learning strategy and a no\nshort-sale constrained Merton problem, by illustrating the convergence of the\nformer towards the latter as the maximum drawdown constraint vanishes.\n"
    },
    {
        "paper_id": 2010.15889,
        "authors": "Elisa Borowski and Jason Soria and Joseph Schofer and Amanda\n  Stathopoulos",
        "title": "Disparities in ridesourcing demand for mobility resilience: A multilevel\n  analysis of neighborhood effects in Chicago, Illinois",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mobility resilience refers to the ability of individuals to complete their\ndesired travel despite unplanned disruptions to the transportation system. The\npotential of new on-demand mobility options, such as ridesourcing services, to\nfill unpredicted gaps in mobility is an underexplored source of adaptive\ncapacity. Applying a natural experiment approach to newly released ridesourcing\ndata, we examine variation in the gap-filling role of on-demand mobility during\nsudden shocks to a transportation system by analyzing the change in use of\nridesourcing during unexpected rail transit service disruptions across the\nracially and economically diverse city of Chicago. Using a multilevel mixed\nmodel, we control not only for the immediate station attributes where the\ndisruption occurs, but also for the broader context of the community area and\ncity quadrant in a three-level structure. Thereby the unobserved variability\nacross neighborhoods can be associated with differences in factors such as\ntransit ridership, or socio-economic status of residents, in addition to\ncontrolling for station level effects. Our findings reveal that individuals use\nridesourcing as a gap-filling mechanism during rail transit disruptions, but\nthere is strong variation across situational and locational contexts.\nSpecifically, our results show larger increases in transit disruption\nresponsive ridesourcing during weekdays, nonholidays, and more severe\ndisruptions, as well as in community areas that have higher percentages of\nWhite residents and transit commuters, and on the more affluent northside of\nthe city. These findings point to new insights with far-reaching implications\non how ridesourcing complements existing transport networks by providing added\ncapacity during disruptions but does not appear to bring equitable gap-filling\nbenefits to low-income communities of color that typically have more limited\nmobility options.\n"
    },
    {
        "paper_id": 2010.15907,
        "authors": "Jasper Verschuur, Elco Koks, Jim Hall",
        "title": "The implications of large-scale containment policies on global maritime\n  trade during the COVID-19 pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The implementation of large-scale containment measures by governments to\ncontain the spread of the COVID-19 virus has resulted in a large supply and\ndemand shock throughout the global economy. Here, we use empirical vessel\ntracking data and a newly developed algorithm to estimate the global maritime\ntrade losses during the first eight months of the pandemic. Our results show\nwidespread trade losses on a port level with the largest absolute losses found\nfor ports in China, the Middle-East and Western Europe, associated with the\ncollapse of specific supply-chains (e.g. oil, vehicle manufacturing). In total,\nwe estimate that global maritime trade reduced by -7.0% to -9.6% during the\nfirst eight months of 2020, which is equal to around 206-286 million tonnes in\nvolume losses and up to 225-412 billion USD in value losses. The fishery,\nmining and quarrying, electrical equipment and machinery manufacturing, and\ntransport equipment manufacturing sectors are hit hardest, with losses up to\n11.8%. Moreover, we find a large geographical disparity in losses, with some\nsmall islands developing states and low-income economies suffering the largest\nrelative trade losses. We find a clear negative impact of COVID-19 related\nbusiness and public transport closures on country-wide exports. Overall, we\nshow how real-time indicators of economic activity can support governments and\ninternational organisations in economic recovery efforts and allocate funds to\nthe hardest hit economies and sectors.\n"
    },
    {
        "paper_id": 2010.1596,
        "authors": "Shanjukta Nath",
        "title": "Preference Estimation in Deferred Acceptance with Partial School\n  Rankings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Deferred Acceptance algorithm is a popular school allocation mechanism\nthanks to its strategy proofness. However, with application costs, strategy\nproofness fails, leading to an identification problem. In this paper, I address\nthis identification problem by developing a new Threshold Rank setting that\nmodels the entire rank order list as a one-step utility maximization problem. I\napply this framework to study student assignments in Chile. There are three\ncritical contributions of the paper. I develop a recursive algorithm to compute\nthe likelihood of my one-step decision model. Partial identification is\naddressed by incorporating the outside value and the expected probability of\nadmission into a linear cost framework. The empirical application reveals that\nalthough school proximity is a vital variable in school choice, student ability\nis critical for ranking high academic score schools. The results suggest that\npolicy interventions such as tutoring aimed at improving student ability can\nhelp increase the representation of low-income low-ability students in better\nquality schools in Chile.\n"
    },
    {
        "paper_id": 2010.16009,
        "authors": "Thomas Bernhardt, Catherine Donnelly",
        "title": "Quantifying the trade-off between income stability and the number of\n  members in a pooled annuity fund",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1017/asb.2020.33",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The number of people who receive a stable income for life from a closed\npooled annuity fund is studied. Income stability is defined as keeping the\nincome within a specified tolerance of the initial income in a fixed proportion\nof future scenarios. The focus is on quantifying the effect of the number of\nmembers, which drives the level of idiosyncratic longevity risk in the fund, on\nthe income stability. To do this, investment returns are held constant and\nsystematic longevity risk is omitted. An analytical expression that closely\napproximates the number of fund members who receive a stable income is derived\nand is seen to be independent of the mortality model. An application of the\nresult is to calculate the length of time for which the pooled annuity fund can\nprovide the desired level of income stability\n"
    },
    {
        "paper_id": 2010.16084,
        "authors": "Ye Zhang",
        "title": "Discrimination in the Venture Capital Industry: Evidence from Field\n  Experiments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines discrimination by early-stage investors based on startup\nfounders' gender and race using two complementary field experiments with real\nU.S. venture capitalists. Results show the following. (i) Discrimination varies\ndepending on the context. Investors implicitly discriminate against female and\nAsian founders when evaluating attractive startups, but they favor female and\nAsian founders when evaluating struggling startups. This helps to reconcile the\ncontradictory results in the extant literature and confirms the theoretical\npredictions of \"discrimination reversion\" and \"pro-cyclical discrimination\"\nphenomena. (ii) Among multiple coexisting sources of discrimination identified,\nstatistical discrimination and implicit discrimination are important reasons\nfor investors' \"anti-minority\" behaviors. A consistent estimator is developed\nto measure the polarization of investors' discrimination behaviors and their\nseparate driving forces. (iii) Homophily exists when investors provide\nanonymous encouragement to startups in a non-investment setting. (iv) There was\ntemporary, stronger discrimination against Asian founders during the COVID-19\noutbreak.\n"
    },
    {
        "paper_id": 2010.16102,
        "authors": "Jianmin Shi",
        "title": "Optimal control of multiple Markov switching stochastic system with\n  application to portfolio decision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we set up an optimal control framework for a hybrid stochastic\nsystem with dual or multiple Markov switching diffusion processes, while Markov\nchains governing these switching diffusions are not identical as assumed by the\nexisting literature. As an application and illustration of this model, we solve\na portfolio choice problem for an investor facing financial and labor markets\nthat are both regime switching. In continuous time context we combine two\nseparate Markov chains into one synthetic Markov chain and derive its\ncorresponding generator matrix, then state the HJB equations for the optimal\ncontrol problem with the newly synthesized Markov switching diffusion.\nFurthermore, we derive explicit solutions and value functions under some\nreasonable specifications.\n"
    },
    {
        "paper_id": 2010.16369,
        "authors": "Derek Singh, Shuzhong Zhang",
        "title": "Distributionally Robust Newsvendor with Moment Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper expands the work on distributionally robust newsvendor to\nincorporate moment constraints. The use of Wasserstein distance as the\nambiguity measure is preserved. The infinite dimensional primal problem is\nformulated; problem of moments duality is invoked to derive the simpler finite\ndimensional dual problem. An important research question is: How does\ndistributional ambiguity affect the optimal order quantity and the\ncorresponding profits/costs? To investigate this, some theory is developed and\na case study in auto sales is performed. We conclude with some comments on\ndirections for further research.\n"
    },
    {
        "paper_id": 2011.00312,
        "authors": "Viktor Stojkoski, Trifce Sandev, Lasko Basnarkov, Ljupco Kocarev and\n  Ralf Metzler",
        "title": "Generalised geometric Brownian motion: Theory and applications to option\n  pricing",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e22121432",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical option pricing schemes assume that the value of a financial asset\nfollows a geometric Brownian motion (GBM). However, a growing body of studies\nsuggest that a simple GBM trajectory is not an adequate representation for\nasset dynamics due to irregularities found when comparing its properties with\nempirical distributions. As a solution, we develop a generalisation of GBM\nwhere the introduction of a memory kernel critically determines the behavior of\nthe stochastic process. We find the general expressions for the moments,\nlog-moments, and the expectation of the periodic log returns, and obtain the\ncorresponding probability density functions by using the subordination\napproach. Particularly, we consider subdiffusive GBM (sGBM), tempered sGBM, a\nmix of GBM and sGBM, and a mix of sGBMs. We utilise the resulting generalised\nGBM (gGBM) to examine the empirical performance of a selected group of kernels\nin the pricing of European call options. Our results indicate that the\nperformance of a kernel ultimately depends on the maturity of the option and\nits moneyness.\n"
    },
    {
        "paper_id": 2011.00366,
        "authors": "Ambrish Dongre, Karan Singhal, Upasak Das",
        "title": "Presence of Women in Economics Academia: Evidence from India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper documents the representation of women in Economics academia in\nIndia by analyzing the share of women in faculty positions, and their\nparticipation in a prestigious conference held annually. Data from the elite\ninstitutions shows that the presence of women as the Economics faculty members\nremains low. Of the authors of the papers which were in the final schedule of\nthe prestigious research conference, the proportion of women authors is again\nfound to be disproportionately low. Our findings from further analysis indicate\nthat women are not under-represented at the post-graduate level. Further, the\nproportion of women in doctoral programmes has increased over time, and is now\nalmost proportionate. Tendency of women who earn a doctorate abroad, to not\nreturn to India, time needed to complete a doctoral program, and\nresponsibilities towards the family may explain lower presence of women in\nEconomics academia in India.\n"
    },
    {
        "paper_id": 2011.00432,
        "authors": "Joshua E. Blumenstock, Matthew Olckers",
        "title": "Gamblers Learn from Experience",
        "comments": "Revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mobile phone-based sports betting has exploded in popularity in many African\ncountries. Commentators worry that low-ability gamblers will not learn from\nexperience, and may rely on debt to gamble. Using data on financial\ntransactions for over 50 000 Kenyan smartphone users, we find that gamblers do\nlearn from experience. Gamblers are less likely to bet following poor results\nand more likely to bet following good results. The reaction to positive and\nnegative feedback is of equal magnitude and is consistent with a model of\nBayesian updating. Using an instrumental variables strategy, we find no\nevidence that increased gambling leads to increased debt.\n"
    },
    {
        "paper_id": 2011.00435,
        "authors": "Tae-Hwy Lee and Ekaterina Seregina",
        "title": "Optimal Portfolio Using Factor Graphical Lasso",
        "comments": "87 pages, 14 figures, 11 tables. arXiv admin note: text overlap with\n  arXiv:2011.04278",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Graphical models are a powerful tool to estimate a high-dimensional inverse\ncovariance (precision) matrix, which has been applied for a portfolio\nallocation problem. The assumption made by these models is a sparsity of the\nprecision matrix. However, when stock returns are driven by common factors,\nsuch assumption does not hold. We address this limitation and develop a\nframework, Factor Graphical Lasso (FGL), which integrates graphical models with\nthe factor structure in the context of portfolio allocation by decomposing a\nprecision matrix into low-rank and sparse components. Our theoretical results\nand simulations show that FGL consistently estimates the portfolio weights and\nrisk exposure and also that FGL is robust to heavy-tailed distributions which\nmakes our method suitable for financial applications. FGL-based portfolios are\nshown to exhibit superior performance over several prominent competitors\nincluding equal-weighted and Index portfolios in the empirical application for\nthe S&P500 constituents.\n"
    },
    {
        "paper_id": 2011.0052,
        "authors": "Edoardo Gallo, Alastair Langtry",
        "title": "Social networks, confirmation bias and shock elections",
        "comments": "43 pages. 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years online social networks have become increasingly prominent in\npolitical campaigns and, concurrently, several countries have experienced shock\nelection outcomes. This paper proposes a model that links these two phenomena.\nIn our set-up, the process of learning from others on a network is influenced\nby confirmation bias, i.e. the tendency to ignore contrary evidence and\ninterpret it as consistent with one's own belief. When agents pay enough\nattention to themselves, confirmation bias leads to slower learning in any\nsymmetric network, and it increases polarization in society. We identify a\nsubset of agents that become more/less influential with confirmation bias. The\nsocially optimal network structure depends critically on the information\navailable to the social planner. When she cannot observe agents' beliefs, the\noptimal network is symmetric, vertex-transitive and has no self-loops. We\nexplore the implications of these results for electoral outcomes and media\nmarkets. Confirmation bias increases the likelihood of shock elections, and it\npushes fringe media to take a more extreme ideology.\n"
    },
    {
        "paper_id": 2011.00552,
        "authors": "Vincenzo Candila, Giampiero M. Gallo, Lea Petrella",
        "title": "Mixed--frequency quantile regressions to forecast Value--at--Risk and\n  Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although quantile regression to calculate risk measures has been widely\nestablished in the financial literature, when considering data observed at\nmixed--frequency, an extension is needed. In this paper, a model is suggested\nbuilt on a mixed--frequency quantile regression to directly estimate the\nValue--at--Risk (VaR) and the Expected Shortfall (ES) measures. In particular,\nthe low--frequency component incorporates information coming from variables\nobserved at, typically, monthly or lower frequencies, while the high--frequency\ncomponent can include a variety of daily variables, like market indices or\nrealized volatility measures. The conditions for the weak stationarity of the\ndaily return process are derived and the finite sample properties are\ninvestigated in an extensive Monte Carlo exercise. The validity of the proposed\nmodel is then explored through a real data application using two energy\ncommodities, namely, Crude Oil and Gasoline futures. Results show that our\nmodel outperforms other competing specifications, on the basis of some popular\nVaR and ES backtesting test procedures.\n"
    },
    {
        "paper_id": 2011.00557,
        "authors": "Jaehyuk Choi, Lixin Wu",
        "title": "A note on the option price and 'Mass at zero in the uncorrelated SABR\n  model and implied volatility asymptotics'",
        "comments": null,
        "journal-ref": "Quantitative Finance, 21:1083, 2021",
        "doi": "10.1080/14697688.2021.1876908",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753-1765] provide a\nsmall-time asymptotics for the mass at zero under the uncorrelated\nstochastic-alpha-beta-rho (SABR) model by approximating the integrated variance\nwith a moment-matched lognormal distribution. We improve the accuracy of the\nnumerical integration by using the Gauss--Hermite quadrature. We further obtain\nthe option price by integrating the constant elasticity of variance (CEV)\noption prices in the same manner without resorting to the small-strike\nvolatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017,\n8(1), 709-737]. For the uncorrelated SABR model, the new option pricing method\nis accurate and arbitrage-free across all strike prices.\n"
    },
    {
        "paper_id": 2011.00572,
        "authors": "Qing Yang, Zhenning Hong, Ruyan Tian, Tingting Ye, Liangliang Zhang",
        "title": "Asset Allocation via Machine Learning and Applications to Equity\n  Portfolio Management",
        "comments": "16 pages, 5 figures and 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we document a novel machine learning based bottom-up approach\nfor static and dynamic portfolio optimization on, potentially, a large number\nof assets. The methodology applies to general constrained optimization problems\nand overcomes many major difficulties arising in current optimization schemes.\nTaking mean-variance optimization as an example, we no longer need to compute\nthe covariance matrix and its inverse, therefore the method is immune from the\nestimation error on this quantity. Moreover, no explicit calls of optimization\nroutines are needed. Applications to equity portfolio management in U.S. and\nChina equity markets are studied and we document significant excess returns to\nthe selected benchmarks.\n"
    },
    {
        "paper_id": 2011.00732,
        "authors": "Ashley Davey, Michael Monoyios and Harry Zheng",
        "title": "Duality for optimal consumption with randomly terminating income",
        "comments": "Major revision of earlier paper entitled \"Duality and deep learning\n  for optimal consumption with randomly terminating income\". arXiv admin note:\n  text overlap with arXiv:2006.04687, arXiv:2009.00972",
        "journal-ref": null,
        "doi": "10.1111/mafi.12322",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a rigorous duality theory, under No Unbounded Profit with\nBounded Risk, for an infinite horizon problem of optimal consumption in the\npresence of an income stream that can terminate randomly at an exponentially\ndistributed time, independent of the asset prices. We thus close a duality gap\nencountered by Vellekoop and Davis in a version of this problem in a\nBlack-Scholes market. Many of the classical tenets of duality theory hold, with\nthe notable exception that marginal utility at zero initial wealth is finite.\nWe use as dual variables a class of supermartingale deflators such that\ndeflated wealth plus cumulative deflated consumption in excess of income is a\nsupermartingale. We show that the space of discounted local martingale\ndeflators is dense in our dual domain, so that the dual problem can also be\nexpressed as an infimum over the discounted local martingale deflators. We\ncharacterise the optimal wealth process, showing that optimal deflated wealth\nis a potential decaying to zero, while deflated wealth plus cumulative deflated\nconsumption over income is a uniformly integrable martingale at the optimum. We\napply the analysis to the Vellekoop and Davis example and give a numerical\nsolution.\n"
    },
    {
        "paper_id": 2011.00838,
        "authors": "Michail Anthropelos, Tianran Geng, Thaleia Zariphopoulou",
        "title": "Competition in Fund Management and Forward Relative Performance Criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an Ito-diffusion market, two fund managers trade under relative\nperformance concerns. For both the asset specialization and diversification\nsettings, we analyze the passive and competitive cases. We measure the\nperformance of the managers' strategies via forward relative performance\ncriteria, leading to the respective notions of forward best-response criterion\nand forward Nash equilibrium. The motivation to develop such criteria comes\nfrom the need to relax various crucial, but quite stringent, existing\nassumptions -- such as, the a priori choices of both the market model and the\ninvestment horizon, the commonality of the latter for both managers as well as\nthe full a priori knowledge of the competitor's policies for the best-response\ncase. We focus on locally riskless criteria and deduce the random forward\nequations. We solve the CRRA cases, thus also extending the related results in\nthe classical setting. An important by-product of the work herein is the\ndevelopment of forward performance criteria for investment problems in\nIto-diffusion markets under the presence of correlated random endowment process\nfor both the perfectly and the incomplete market cases.\n"
    },
    {
        "paper_id": 2011.00909,
        "authors": "Dietmar Pfeifer, Olena Ragulina",
        "title": "Adaptive Bernstein Copulas and Risk Management",
        "comments": "corrected version; 27 pages, 58 figures, 17 tables",
        "journal-ref": "Mathematics 2020, 8, 2221",
        "doi": "10.3390/math8122221",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a constructive approach to Bernstein copulas with an admissible\ndiscrete skeleton in arbitrary dimensions when the underlying marginal grid\nsizes are smaller than the number of observations. This prevents an overfitting\nof the estimated dependence model and reduces the simulation effort for\nBernstein copulas a lot. In a case study, we compare different approaches of\nBernstein and Gaussian copulas w.r.t. the estimation of risk measures in risk\nmanagement.\n"
    },
    {
        "paper_id": 2011.0101,
        "authors": "Hal Ashton",
        "title": "Causal Campbell-Goodhart's law and Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Campbell-Goodhart's law relates to the causal inference error whereby\ndecision-making agents aim to influence variables which are correlated to their\ngoal objective but do not reliably cause it. This is a well known error in\nEconomics and Political Science but not widely labelled in Artificial\nIntelligence research. Through a simple example, we show how off-the-shelf deep\nReinforcement Learning (RL) algorithms are not necessarily immune to this\ncognitive error. The off-policy learning method is tricked, whilst the\non-policy method is not. The practical implication is that naive application of\nRL to complex real life problems can result in the same types of policy errors\nthat humans make. Great care should be taken around understanding the causal\nmodel that underpins a solution derived from Reinforcement Learning.\n"
    },
    {
        "paper_id": 2011.01092,
        "authors": "Philipp Bach, Victor Chernozhukov, Martin Spindler",
        "title": "Insights from Optimal Pandemic Shielding in a Multi-Group SEIR Framework",
        "comments": "39 pages, 23 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic constitutes one of the largest threats in recent\ndecades to the health and economic welfare of populations globally. In this\npaper, we analyze different types of policy measures designed to fight the\nspread of the virus and minimize economic losses. Our analysis builds on a\nmulti-group SEIR model, which extends the multi-group SIR model introduced by\nAcemoglu et al.~(2020). We adjust the underlying social interaction patterns\nand consider an extended set of policy measures. The model is calibrated for\nGermany. Despite the trade-off between COVID-19 prevention and economic\nactivity that is inherent to shielding policies, our results show that\nefficiency gains can be achieved by targeting such policies towards different\nage groups. Alternative policies such as physical distancing can be employed to\nreduce the degree of targeting and the intensity and duration of shielding. Our\nresults show that a comprehensive approach that combines multiple policy\nmeasures simultaneously can effectively mitigate population mortality and\neconomic harm.\n"
    },
    {
        "paper_id": 2011.01308,
        "authors": "Jeffrey Cohen, Clark Alexander",
        "title": "Picking Efficient Portfolios from 3,171 US Common Stocks with New\n  Quantum and Classical Solvers",
        "comments": "15 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze 3,171 US common stocks to create an efficient portfolio based on\nthe Chicago Quantum Net Score (CQNS) and portfolio optimization. We begin with\nclassical solvers and incorporate quantum annealing. We add a simulated\nbifurcator as a new classical solver and the new D-Wave Advantage(TM) quantum\nannealing computer as our new quantum solver.\n"
    },
    {
        "paper_id": 2011.01374,
        "authors": "Allison Koenecke and Hal Varian",
        "title": "Synthetic Data Generation for Economists",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As more tech companies engage in rigorous economic analyses, we are\nconfronted with a data problem: in-house papers cannot be replicated due to use\nof sensitive, proprietary, or private data. Readers are left to assume that the\nobscured true data (e.g., internal Google information) indeed produced the\nresults given, or they must seek out comparable public-facing data (e.g.,\nGoogle Trends) that yield similar results. One way to ameliorate this\nreproducibility issue is to have researchers release synthetic datasets based\non their true data; this allows external parties to replicate an internal\nresearcher's methodology. In this brief overview, we explore synthetic data\ngeneration at a high level for economic analyses.\n"
    },
    {
        "paper_id": 2011.01417,
        "authors": "Igor Halperin",
        "title": "Non-Equilibrium Skewness, Market Crises, and Option Pricing: Non-Linear\n  Langevin Model of Markets with Supersymmetry",
        "comments": "39 pages, 11 figures. Changes in Sect. 4.2 and in numerical examples",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a tractable model of non-linear dynamics of market\nreturns using a Langevin approach. Due to non-linearity of an interaction\npotential, the model admits regimes of both small and large return\nfluctuations. Langevin dynamics are mapped onto an equivalent quantum\nmechanical (QM) system. Borrowing ideas from supersymmetric quantum mechanics\n(SUSY QM), a parameterized ground state wave function (WF) of this QM system is\nused as a direct input to the model, which also fixes a non-linear Langevin\npotential. Using a two-component Gaussian mixture as a ground state WF with an\nasymmetric double well potential produces a tractable low-parametric model with\ninterpretable parameters, referred to as the NES (Non-Equilibrium Skew) model.\nSupersymmetry (SUSY) is then used to find time-dependent solutions of the model\nin an analytically tractable way. Additional approximations give rise to a\nfinal practical version of the NES model, where real-measure and risk-neutral\nreturn distributions are given by three component Gaussian mixtures. This\nproduces a closed-form approximation for option pricing in the NES model by a\nmixture of three Black-Scholes prices, providing accurate calibration to option\nprices for either benign or distressed market environments, while using only a\nsingle volatility parameter. These results stand in stark contrast to the most\nof other option pricing models such as local, stochastic, or rough volatility\nmodels that need more complex specifications of noise to fit the market data.\n"
    },
    {
        "paper_id": 2011.01508,
        "authors": "Gizem Bacaksizlar, Stefani Crabtree, Joshua Garland, Natalie\n  Grefenstette, Albert Kao, David Kinney, Artemy Kolchinsky, Tyler Marghetis,\n  Michael Price, Maria Riolo, Hajime Shimao, Ashley Teufel, Tamara van der\n  Does, Vicky Chuqiao Yang (Santa Fe Institute Postdocs)",
        "title": "Greetings from a Triparental Planet",
        "comments": "The original version of this report was produced by a team in just 72\n  hours. This version includes additional edits for style and formatting",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work of speculative science, scientists from a distant star system\nexplain the emergence and consequences of triparentalism, when three\nindividuals are required for sexual reproduction, which is the standard form of\nmating on their home world. The report details the evolution of their\nreproductive system--that is, the conditions under which triparentalism and\nthree self-avoiding mating types emerged as advantageous strategies for sexual\nreproduction. It also provides an overview of the biological consequences of\ntriparental reproduction with three mating types, including the genetic\nmechanisms of triparental reproduction, asymmetries between the three mating\ntypes, and infection dynamics arising from their different mode of sexual\nreproduction. The report finishes by discussing how central aspects of their\nsociety, such as short-lasting unions among individuals and the rise of a\nmonoculture, might have arisen as a result of their triparental system.\n"
    },
    {
        "paper_id": 2011.01712,
        "authors": "Haoqian Zhang, Cristina Basescu, and Bryan Ford",
        "title": "Economic Principles of PoPCoin, a Democratic Time-based Cryptocurrency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While democracy is founded on the principle of equal opportunity to manage\nour lives and pursue our fortunes, the forms of money we have inherited from\nmillenia of evolution has brought us to an unsustainable dead-end of exploding\ninequality. PoPCoin proposes to leverage the unique historical opportunities\nthat digital cryptocurrencies present for a \"clean-slate\" redesign of money, in\nparticular around long-term equitability and sustainability, rather than solely\nstability, as our primary goals. We develop and analyze a monetary policy for\nPoPCoin that embodies these equitability goals in two basic rules that maybe\nsummarized as supporting equal opportunity in \"space\" and \"time\": the first by\nregularly distributing new money equally to all participants much like a basic\nincome, the second by holding the aggregate value of these distributions to a\nconstant and non-diminishing portion of total money supply through demurrage.\nThrough preliminary economic analysis, we find that these rules in combination\nyield a unique form of money with numerous intriguing and promising properties,\nsuch as a quantifiable and provable upper bound on monetary inequality, a\nnatural \"early adopter's reward\" that could incentivize rapid growth while\ntapering off as participation saturates, resistance to the risk of deflationary\nspirals, and migration incentives opposite those created by conventional basic\nincomes.\n"
    },
    {
        "paper_id": 2011.01961,
        "authors": "Alexander Wong, Andrew Hryniowski, and Xiao Yu Wang",
        "title": "Insights into Fairness through Trust: Multi-scale Trust Quantification\n  for Financial Deep Learning",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The success of deep learning in recent years have led to a significant\nincrease in interest and prevalence for its adoption to tackle financial\nservices tasks. One particular question that often arises as a barrier to\nadopting deep learning for financial services is whether the developed\nfinancial deep learning models are fair in their predictions, particularly in\nlight of strong governance and regulatory compliance requirements in the\nfinancial services industry. A fundamental aspect of fairness that has not been\nexplored in financial deep learning is the concept of trust, whose variations\nmay point to an egocentric view of fairness and thus provide insights into the\nfairness of models. In this study we explore the feasibility and utility of a\nmulti-scale trust quantification strategy to gain insights into the fairness of\na financial deep learning model, particularly under different scenarios at\ndifferent scales. More specifically, we conduct multi-scale trust\nquantification on a deep neural network for the purpose of credit card default\nprediction to study: 1) the overall trustworthiness of the model 2) the trust\nlevel under all possible prediction-truth relationships, 3) the trust level\nacross the spectrum of possible predictions, 4) the trust level across\ndifferent demographic groups (e.g., age, gender, and education), and 5)\ndistribution of overall trust for an individual prediction scenario. The\ninsights for this proof-of-concept study demonstrate that such a multi-scale\ntrust quantification strategy may be helpful for data scientists and regulators\nin financial services as part of the verification and certification of\nfinancial deep learning solutions to gain insights into fairness and trust of\nthese solutions.\n"
    },
    {
        "paper_id": 2011.02026,
        "authors": "Hanie.Vahabi and Ali Namaki and Reza Raei",
        "title": "Comparing the collective behavior of banking industry",
        "comments": "e.g=14pages,9figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  One of the most important features of capital markets as an adaptive complex\nnetworks is their collective behavior. In this paper, we have analyzed the\nbanking sectors of 4 world stock markets,which composed of emerging and matures\nones. By applying one the important complexity notions, Random matrix\ntheory(RMT), it is founded that mature markets have a higher degree of\ncollective behavior,Even though we used RMT tools: participation ratio(PR),\nnode participation ratio(NPR)and relative participation ratio(RPR) , which NPR\nillustrated independent banks than whole market and RPR compared collective\nbehavior of markets by a normal range. By applying local and global\nperturbations, we concluded that mature markets are more vulnerable to\nperturbations due to the high level of collective behavior. Finally, by drawing\nthe dendrograms and heat maps of the correlation matrices,\n"
    },
    {
        "paper_id": 2011.02165,
        "authors": "Kazuya Kaneko, Koichi Miyamoto, Naoyuki Takeda, Kazuyoshi Yoshino",
        "title": "Quantum Speedup of Monte Carlo Integration with respect to the Number of\n  Dimensions and its Application to Finance",
        "comments": "13 pages, no figure",
        "journal-ref": "Quantum Inf Process 20, 185 (2021)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monte Carlo integration using quantum computers has been widely investigated,\nincluding applications to concrete problems. It is known that quantum\nalgorithms based on quantum amplitude estimation (QAE) can compute an integral\nwith a smaller number of iterative calls of the quantum circuit which\ncalculates the integrand, than classical methods call the integrand subroutine.\nHowever, the issues about the iterative operations in the integrand circuit\nhave not been discussed so much. That is, in the high-dimensional integration,\nmany random numbers are used for calculation of the integrand and in some cases\nsimilar calculations are repeated to obtain one sample value of the integrand.\nIn this paper, we point out that we can reduce the number of such repeated\noperations by a combination of the nested QAE and the use of pseudorandom\nnumbers (PRNs), if the integrand has the separable form with respect to\ncontributions from distinct random numbers. The use of PRNs, which the authors\noriginally proposed in the context of the quantum algorithm for Monte Carlo, is\nthe key factor also in this paper, since it enables parallel computation of the\nseparable terms in the integrand. Furthermore, we pick up one use case of this\nmethod in finance, the credit portfolio risk measurement, and estimate to what\nextent the complexity is reduced.\n"
    },
    {
        "paper_id": 2011.0229,
        "authors": "Shima Beigi",
        "title": "How do the Covid-19 Prevention Measures Interact with Sustainable\n  Development Goals?",
        "comments": "11 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.20944/preprints202010.0279.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Washing hands, social distancing and staying at home are the preventive\nmeasures set in place to contain the spread of the COVID-19, a disease caused\nby SARS-CoV-2. These measures, although straightforward to follow, highlight\nthe tip of an imbalanced socio-economic and socio-technological iceberg. Here,\na System Dynamic (SD) model of COVID-19 preventive measures and their\ncorrelation with the 17 Sustainable Development Goals (SDGs) is presented. The\nresult demonstrates a better informed view of the COVID-19 vulnerability\nlandscape. This novel qualitative approach refreshes debates on the future of\nSDGS amid the crisis and provides a powerful mental representation for decision\nmakers to find leverage points that aid in preventing long-term disruptive\nimpacts of this health crisis on people, planet and economy. There is a need\nfor further tailor-made and real-time qualitative and quantitative scientific\nresearch to calibrate the criticality of meeting the SDGS targets in different\ncountries according to ongoing lessons learned from this health crisis.\n"
    },
    {
        "paper_id": 2011.02362,
        "authors": "Lucia Savadori, Giuseppe Espa, Maria Michela Dickson",
        "title": "The polarizing impact of numeracy, economic literacy, and science\n  literacy on attitudes toward immigration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Political orientation polarizes the attitudes of more educated individuals on\ncontroversial issues. A highly controversial issue in Europe is immigration. We\nfound the same polarizing pattern for opinion toward immigration in a\nrepresentative sample of citizens of a southern European middle-size city.\nCitizens with higher numeracy, scientific and economic literacy presented a\nmore polarized view of immigration, depending on their worldview orientation.\nHighly knowledgeable individuals endorsing an egalitarian-communitarian\nworldview were more in favor of immigration, whereas highly knowledgeable\nindividuals with a hierarchical-individualist worldview were less in favor of\nimmigration. Those low in numerical, economic, and scientific literacy did not\nshow a polarized attitude. Results highlight the central role of\nsocio-political orientation over information theories in shaping attitudes\ntoward immigration.\n"
    },
    {
        "paper_id": 2011.02596,
        "authors": "T.R.B. den Haan, K.W. Chau, M. van der Schans, C.W. Oosterlee",
        "title": "Rule-based Strategies for Dynamic Life Cycle Investment",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s13385-021-00283-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we consider rule-based investment strategies for managing a\ndefined contribution saving scheme under the Dutch pension fund testing model.\nWe found that dynamic rule-based investment can outperform traditional static\nstrategies, by which we mean that the pensioner can achieve the target\nretirement income with higher probability and limit the shortfall when target\nis not met. In comparison with the popular dynamic programming technique, the\nrule-based strategy has a more stable asset allocation throughout time and\navoid excessive transactions, which may be hard to explain to the investor. We\nalso study a combined strategy of rule based target and dynamic programming in\nthis work. Another key feature of this work is that there is no risk-free asset\nunder our setting, instead, a matching portfolio is introduced for the investor\nto avoid unnecessary risk.\n"
    },
    {
        "paper_id": 2011.02612,
        "authors": "Shize Qin, Lena Klaa{\\ss}en, Ulrich Gallersd\\\"orfer, Christian Stoll,\n  Da Zhang",
        "title": "Bitcoin's future carbon footprint",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The carbon footprint of Bitcoin has drawn wide attention, but Bitcoin's\nlong-term impact on the climate remains uncertain. Here we present a framework\nto overcome uncertainties in previous estimates and project Bitcoin's\nelectricity consumption and carbon footprint in the long term. If we assume\nBitcoin's market capitalization grows in line with the one of gold, we find\nthat the annual electricity consumption of Bitcoin may increase from 60 to 400\nTWh between 2020 and 2100. The future carbon footprint of Bitcoin strongly\ndepends on the decarbonization pathway of the electricity sector. If the\nelectricity sector achieves carbon neutrality by 2050, Bitcoin's carbon\nfootprint has peaked already. However, in the business-as-usual scenario,\nemissions sum up to 2 gigatons until 2100, an amount comparable to 7% of global\nemissions in 2019. The Bitcoin price spike at the end of 2020 shows, however,\nthat progressive development of market capitalization could yield an\nelectricity consumption of more than 100 TWh already in 2021, and lead to\ncumulative emissions of over 5 gigatons by 2100. Therefore, we also discuss\npolicy instruments to reduce Bitcoin's future carbon footprint.\n"
    },
    {
        "paper_id": 2011.02776,
        "authors": "Jan Motl, Pavel Kord\\'ik",
        "title": "Fast and exact audit scheduling optimization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s42452-021-04778-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is concerned with the cost and time effective scheduling of\nfinancial auditors with Integer Linear Programming. The schedule optimization\ntakes into account 13 different constraints, staff scarcity, frequent\nalterations of the input data with the need to minimize the changes in the\ngenerated schedule, and scaling issues. We compared two exact formulations of\nthe problem and we found a multi-commodity network flow formulation to be 24\ntimes faster than a three-dimensional formulation. The delivered implementation\nreduced time to the first schedule from 3 man-days to 1 hour and the schedule\nupdate time from 1 man-day to 4 minutes.\n"
    },
    {
        "paper_id": 2011.0287,
        "authors": "Anna Ananova, Rama Cont and Renyuan Xu",
        "title": "Model-free Analysis of Dynamic Trading Strategies",
        "comments": "24 pages; 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model-free approach based on excursions of trading signals for\nanalyzing the risk and return for a broad class of dynamic trading strategies,\nincluding pairs trading and other statistical arbitrage strategies. We propose\na mathematical framework for the risk analysis of such strategies, based on a\ndescription in terms of excursions of prices away from a reference level, in a\npathwise setting without any probabilistic assumptions.\n  We introduce the notion of delta-excursion, defined as a path that deviates\nby delta from a reference level before returning to this level. We show that\nevery continuous path has a unique decomposition into delta-excursions, which\nis useful for the scenario analysis of dynamic trading strategies, leading to\nsimple expressions for the number of trades, realized profit, maximum loss, and\ndrawdown. We show that the high-frequency limit, which corresponds to the case\nwhere delta decreases to zero, is described by the (p-th order) local time of\nthe signal. In particular, our results yield a financial interpretation of the\nlocal time as the profit of a certain high-frequency mean-reversion trading\nstrategy.\n  Finally, we describe a non-parametric scenario simulation method for\ngenerating paths whose excursion properties match those observed in empirical\ndata.\n"
    },
    {
        "paper_id": 2011.02899,
        "authors": "Gaurab Aryal, Eduardo Fajnzylber, Maria F. Gabrielli, Manuel\n  Willington",
        "title": "Auctioning Annuities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and estimate a model of demand and supply of annuities. To this\nend, we use rich data from Chile, where annuities are bought and sold in a\nprivate market via a two-stage process: first-price auctions followed by\nbargaining. We model firms with private information about costs and retirees\nwith different mortalities and preferences for bequests and firms' risk\nratings. We find substantial costs and preference heterogeneity, and because\nthere are many firms, the market performs well. Counterfactuals show that\nsimplifying the current mechanism with English auctions and \"shutting down\"\nrisk ratings increase pensions, but only for high-savers.\n"
    },
    {
        "paper_id": 2011.02924,
        "authors": "Maria Petrova, Ananya Sen, Pinar Yildirim",
        "title": "Social Media and Political Contributions: The Impact of New Technology\n  on Political Competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Political campaigns are among the most sophisticated marketing exercises in\nthe United States. As part of their marketing communication strategy, an\nincreasing number of politicians adopt social media to inform their\nconstituencies. This study documents the returns from adopting a new\ntechnology, namely Twitter, for politicians running for Congress by focusing on\nthe change in campaign contributions received. We compare weekly donations\nreceived just before and just after a politician opens a Twitter account in\nregions with high and low levels of Twitter penetration, controlling for\npolitician-month fixed effects. Specifically, over the course of a political\ncampaign, we estimate that the differential effect of opening a Twitter account\nin regions with high vs low levels of Twitter penetration amounts to an\nincrease of 0.7-2% in donations for all politicians and 1-3.1% for new\npoliticians, who were never elected to the Congress before. In contrast, the\neffect of joining Twitter for experienced politicians remains negligibly small.\nWe find some evidence consistent with the explanation that the effect is driven\nby new information about the candidates, e.g., the effect is primarily driven\nby new donors rather than past donors, by candidates without Facebook accounts\nand tweeting more informatively. Overall, our findings imply that social media\ncan intensify political competition by lowering costs of disseminating\ninformation for new entrants to their constituents and thus may reduce the\nbarriers to enter politics.\n"
    },
    {
        "paper_id": 2011.0312,
        "authors": "Guilherme Jardim",
        "title": "How the Availability of Higher Education Affects Incentives? Evidence\n  from Federal University Openings in Brazil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the impact of an university opening on incentives for\nhuman capital accumulation of prospective students in its neighborhood. The\nopening causes an exogenous fall on the cost to attend university, through the\ndecrease in distance, leading to an incentive to increase effort - shown by the\npositive effect on students' grades. I use an event study approach with two-way\nfixed effects to retrieve a causal estimate, exploiting the variation across\ngroups of students that receive treatment at different times - mitigating the\nbias created by the decision of governments on the location of new\nuniversities. Results show an average increase of $0.038$ standard deviations\nin test grades, for the municipality where the university was established, and\nare robust to a series of potential problems, including some of the usual\nconcerns in event study models.\n"
    },
    {
        "paper_id": 2011.03297,
        "authors": "Friederike Wall and Stephan Leitner",
        "title": "Agent-based Computational Economics in Management Accounting Research:\n  Opportunities and Difficulties",
        "comments": "92 pages (including Appendix), 5 tables (including Appendix)",
        "journal-ref": null,
        "doi": "10.2308/jmar-19-073",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based computational economics (ACE) - while adopted comparably widely\nin other domains of managerial science - is a rather novel paradigm for\nmanagement accounting research (MAR). This paper provides an overview of\nopportunities and difficulties that ACE may have for research in management\naccounting and, in particular, introduces a framework that researchers in\nmanagement accounting may employ when considering ACE as a paradigm for their\nparticular research endeavor. The framework builds on the two interrelated\nparadigmatic elements of ACE: a set of theoretical assumptions on economic\nagents and the approach of agent-based modeling. Particular focus is put on\ncontrasting opportunities and difficulties of ACE in comparison to other\nresearch methods employed in MAR.\n"
    },
    {
        "paper_id": 2011.0331,
        "authors": "Olga A. Zolotareva, Aleksandr V. Bezrukov",
        "title": "Socio-demographic goals within digitalization environment: a gender\n  aspect",
        "comments": "pages 20, figure 6",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article pays close attention to obtaining gender assessments in the world\nof work, which made it possible to characterize the effectiveness of social\npolicy aimed at achieving gender equality.\n"
    },
    {
        "paper_id": 2011.03314,
        "authors": "John Armstrong, Damiano Brigo, Alex S.L. Tse",
        "title": "The importance of dynamic risk constraints for limited liability\n  operators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous literature shows that prevalent risk measures such as Value at Risk\nor Expected Shortfall are ineffective to curb excessive risk-taking by a\ntail-risk-seeking trader with S-shaped utility function in the context of\nportfolio optimisation. However, these conclusions hold only when the\nconstraints are static in the sense that the risk measure is just applied to\nthe terminal portfolio value. In this paper, we consider a portfolio\noptimisation problem featuring S-shaped utility and a dynamic risk constraint\nwhich is imposed throughout the entire trading horizon. Provided that the risk\ncontrol policy is sufficiently strict relative to the asset performance, the\ntrader's portfolio strategies and the resulting maximal expected utility can be\neffectively constrained by a dynamic risk measure. Finally, we argue that\ndynamic risk constraints might still be ineffective if the trader has access to\na derivatives market.\n"
    },
    {
        "paper_id": 2011.03339,
        "authors": "David Haritone Shikumo, Oluoch Oluoch and Joshua Matanda Wepukhulu",
        "title": "Effect of Short-Term Debt on Financial Growth of Non-Financial Firms\n  Listed at Nairobi Securities Exchange",
        "comments": "12 Pages. arXiv admin note: substantial text overlap with\n  arXiv:2010.12596",
        "journal-ref": "www.iiste.org, Research Journal of Finance and Accounting Vol.11,\n  No.20, 2020",
        "doi": "10.7176/RJFA/11-17-16 10.7176/RJFA/11-17-16 10.7176/RJFA/11-17-16",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A significant number of the non-financial firms listed at Nairobi Securities\nExchange (NSE) have been experiencing declining financial performance which\ndeter investors from investing in such firms. The lenders are also not willing\nto lend to such firms. As such, the firms struggle to raise funds for their\noperations. Prudent financing decisions can lead to financial growth of the\nfirm. The purpose of this study is to assess the effect of short-term debt on\nfinancial growth of non-financial firms listed at Nairobi Securities Exchange\nfor a period of ten years from 2008 to 2017. Financial firms were excluded\nbecause of their specific sector characteristics and stringent regulatory\nframework. The study is guided by Agency Theory and Theory of Growth of the\nFirm. Explanatory research design was adopted. The target population of the\nstudy comprised of 45 non-financial firms listed at the NSE for a period of ten\nyears from 2008 to 2017. The study conducted both descriptive statistics\nanalysis and panel data analysis. The result indicates that, short term debt\nexplains 45.99% and 25.6% of variations in financial growth as measured by\ngrowth in earnings per share and growth in market capitalization respectively.\nShort term debt positively and significantly influences financial growth\nmeasured using both growth in earnings per share and growth in market\ncapitalization. The study recommends that, the management of non-financial\nfirms listed at Nairobi Securities Exchange to employ financing means that can\nimprove the earnings per share, market capitalization and enhance the value of\nthe firm for the benefit of its stakeholders.\n"
    },
    {
        "paper_id": 2011.03392,
        "authors": "Robert Kaestner",
        "title": "Did Hurricane Katrina Reduce Mortality?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a recent article in the American Economic Review, Tatyana Deryugina and\nDavid Molitor (DM) analyzed the effect of Hurricane Katrina on the mortality of\nelderly and disabled residents of New Orleans. The authors concluded that\nHurricane Katrina improved the eight-year survival rate of elderly and disabled\nresidents of New Orleans by 3% and that most of this decline in mortality was\ndue to declines in mortality among those who moved to places with lower\nmortality. In this article, I provide a critical assessment of the evidence\nprovided by DM to support their conclusions. There are three main problems.\nFirst, DM generally fail to account for the fact that people of different ages,\nraces or sex will have different probabilities of dying as time goes by, and\nwhen they do allow for this, results change markedly. Second, DM do not account\nfor the fact that residents in New Orleans are likely to be selected\nnon-randomly on the basis of health because of the relatively high mortality\nrate in New Orleans compared to the rest of the country. Third, there is\nconsiderable evidence that among those who moved from New Orleans, the\ndestination chosen was non-random. Finally, DM never directly assessed changes\nin mortality of those who moved, or stayed, in New Orleans before and after\nHurricane Katrina. These problems lead me to conclude that the evidence\npresented by DM does not support their inferences.\n"
    },
    {
        "paper_id": 2011.03514,
        "authors": "Matthew Read",
        "title": "Monetary Policy and Firm Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Do firm dynamics matter for the transmission of monetary policy? Empirically,\nthe startup rate declines following a monetary contraction, while the exit rate\nincreases, both of which reduce aggregate employment. I present a model that\ncombines firm dynamics in the spirit of Hopenhayn (1992) with New-Keynesian\nfrictions and calibrate it to match cross-sectional evidence. The model can\nqualitatively account for the responses of entry and exit rates to a monetary\npolicy shock. However, the responses of macroeconomic variables closely\nresemble those in a representative-firm model. I discuss the equilibrium forces\nunderlying this approximate equivalence, and what may overturn this result.\n"
    },
    {
        "paper_id": 2011.03517,
        "authors": "Toma\\v{z} Fleischman and Paolo Dini",
        "title": "Balancing the Payment System",
        "comments": "20 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasingly complex economic and financial environment in which we live\nmakes the management of liquidity in payment systems and the economy in general\na persistent challenge. New technologies are making it possible to address this\nchallenge through alternative solutions that complement and strengthen existing\npayment systems. For example, the interbank balancing method can also be\napplied to private payment systems, complementary currencies, and trade credit\nclearing systems to provide better liquidity and risk management. In this paper\nwe introduce the concept of a balanced payment system and demonstrate the\neffects of balancing on a small example. We show how to construct a balanced\npayment subsystem that can be settled in full and, therefore, that can be\nremoved from the payment system to achieve liquidity-saving and payments\ngridlock resolution. We also briefly introduce a generalization of a payment\nsystem and of the method to balance it in the form of a specific application\n(Tetris Core Technologies), whose wider adoption could contribute to the\nfinancial stability of and better management of liquidity and risk for the\nwhole economy.\n"
    },
    {
        "paper_id": 2011.03541,
        "authors": "Sophie-Charlotte Klose",
        "title": "Identifying Latent Structures in Maternal Employment: Evidence on the\n  German Parental Benefit Reform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper identifies latent group structures in the effect of motherhood on\nemployment by employing the C-Lasso, a recently developed, purely data-driven\nclassification method. Moreover, I assess how the introduction of the generous\nGerman parental benefit reform in 2007 affects the different cluster groups by\ntaking advantage of an identification strategy that combines the sharp\nregression discontinuity design and hypothesis testing of predicted employment\nprobabilities. The C-Lasso approach enables heterogeneous employment effects\nacross mothers, which are classified into an a priori unknown number of cluster\ngroups, each with its own group-specific effect. Using novel German\nadministrative data, the C-Lasso identifies three different cluster groups pre-\nand post-reform. My findings reveal marked unobserved heterogeneity in maternal\nemployment and that the reform affects the identified cluster groups'\nemployment patterns differently.\n"
    },
    {
        "paper_id": 2011.03543,
        "authors": "Weijie Pang, Stephan Sturm",
        "title": "XVA Valuation under Market Illiquidity",
        "comments": "49 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Before the 2008 financial crisis, most research in financial mathematics\nfocused on pricing options without considering the effects of counterparties'\ndefaults, illiquidity problems, and the role of the sale and repurchase\nagreement (Repo) market. Recently, models were proposed to address this by\ncomputing a total valuation adjustment (XVA) of derivatives; however without\nconsidering a potential crisis in the market. In this article, we include a\npossible crisis by using an alternating renewal process to describe the\nswitching between a normal financial regime and a financial crisis. We develop\na framework to price the XVA of a European claim in this state-dependent\nsituation. The price is characterized as a solution to a backward stochastic\ndifferential equation (BSDE), and we prove the existence and uniqueness of this\nsolution. In a numerical study based on a deep learning algorithm for BSDEs, we\ncompare the effect of different parameters on the valuation of the XVA.\n"
    },
    {
        "paper_id": 2011.03695,
        "authors": "Justin Y.F. Lin and Haipeng Xing",
        "title": "Endogenous structural transformation in economic development",
        "comments": "43 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends Xing's (2023abcd) optimal growth models of catching-up\neconomies from the case of production function switching to that of economic\nstructure switching and argues how a country develops its economy by endogenous\nstructural transformation and efficient resource allocation in a market\nmechanism. To achieve this goal, the paper first summarizes three attributes of\neconomic structures from the literature, namely, structurality, durationality,\nand transformality, and discuss their implications for methods of economic\nmodeling. Then, with the common knowledge assumption, the paper extends Xing's\n(2023a) optimal growth model that is based on production function switching and\nconsiders an extended Ramsey model with endogenous structural transformation in\nwhich the social planner chooses the optimal industrial structure, recource\nallocation with the chosen structure, and consumption to maximize the\nrepresentative household's total utility subject to the resource constraint.\nThe paper next establishes the mathematical underpinning of the static,\ndynamic, and switching equilibria. The Ramsey growth model and its equilibria\nare then extended to economies with complicated economic structures consisting\nof hierarchical production, technology adoption and innovation, infrastructure,\nand economic and political institutions. The paper concludes with a brief\ndiscussion of applications of the proposed methodology to economic development\nproblems in other scenarios.\n"
    },
    {
        "paper_id": 2011.03741,
        "authors": "Constandina Koki, Stefanos Leonardos, Georgios Piliouras",
        "title": "Exploring the Predictability of Cryptocurrencies via Bayesian Hidden\n  Markov Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a variety of multi-state Hidden Markov models for\npredicting and explaining the Bitcoin, Ether and Ripple returns in the presence\nof state (regime) dynamics. In addition, we examine the effects of several\nfinancial, economic and cryptocurrency specific predictors on the\ncryptocurrency return series. Our results indicate that the Non-Homogeneous\nHidden Markov (NHHM) model with four states has the best one-step-ahead\nforecasting performance among all competing models for all three series. The\ndominance of the predictive densities over the single regime random walk model\nrelies on the fact that the states capture alternating periods with distinct\nreturn characteristics. In particular, the four state NHHM model distinguishes\nbull, bear and calm regimes for the Bitcoin series, and periods with different\nprofit and risk magnitudes for the Ether and Ripple series. Also, conditionally\non the hidden states, it identifies predictors with different linear and\nnon-linear effects on the cryptocurrency returns. These empirical findings\nprovide important insight for portfolio management and policy implementation.\n"
    },
    {
        "paper_id": 2011.03795,
        "authors": "Michele Azzone and Roberto Baviera",
        "title": "Synthetic forwards and cost of funding in the equity derivative market",
        "comments": null,
        "journal-ref": "Finance Research Letters Volume 41, July 2021, 101841",
        "doi": "10.1016/j.frl.2020.101841",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study introduces a new technique to recover the implicit discount factor\nin the derivative market using only European put and call prices: this discount\nis grounded in actual transactions in active markets. Moreover, this study\nidentifies the implied cost of funding, over OIS, of major market players. Does\na liquid equity market allow arbitrage? The key idea is that the (unique)\nforward contract -- built using the put-call parity relation -- contains\ninformation about the market discount factor: by no-arbitrage conditions we\nidentify the implicit interest rate such that the forward contract value does\nnot depend on the strike. The procedure is applied to options on S&P 500 and\nEURO STOXX 50 indices. There is statistical evidence that, in the EURO STOXX 50\nmarket, the implicit interest rate curve coincides with the EUR OIS one, while,\nin the S&P 500 market, a cost of funding of, on average, 34 basis points is\nadded on top of the USD OIS curve.\n"
    },
    {
        "paper_id": 2011.03878,
        "authors": "Quitz\\'e Valenzuela-Stookey",
        "title": "Redistribution Through Tax Relief",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies politically feasible policy solutions to inequities in\nlocal public goods provision. I focus in particular on the entwined issues of\nhigh property taxes, geographic income disparities, and inequalities in public\neducation prevalent in the United States. It has long been recognized that with\na mobile population, local administration and funding of schools leads to\ncompetition between districts. By accounting for heterogeneity in incomes and\nhome qualities, I am able to shed new light on this phenomenon, and make novel\npolicy recommendations. I characterize the equilibrium in a dynamic general\nequilibrium model of location choice and education investment with a\ncompetitive housing market, heterogeneous wealth levels and home qualities, and\nstrategic district governments. When all homes are owner-occupied, I show that\ncompetition between strategic districts leads to over-taxation in an attempt to\nattract wealthier residents. A simple class of policies that cap and/or tax the\nexpenditure of richer districts are Pareto improving, and thus politically\nfeasible. These policies reduce inequality in access to education while\nincreasing expenditure for under-funded schools. Gains are driven by mitigation\nof the negative externalities generated by excessive spending among wealthier\ndistricts. I also discuss the policy implications of the degree of\nhomeownership. The model sheds new light on observed patterns of homeownership,\nlocation choice, and income. Finally, I test the assumptions and implications\nempirically using a regression discontinuity design and data on property tax\nreferenda in Massachusetts.\n"
    },
    {
        "paper_id": 2011.03987,
        "authors": "Wieger Hinderks, Ralf Korn and Andreas Wagner",
        "title": "Unifying the theory of storage and the risk premium by an unobservable\n  intrinsic electricity price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new concept for modelling electricity prices\nthrough the introduction of an unobservable intrinsic electricity price\n$p(\\tau)$. We use it to connect the classical theory of storage with the\nconcept of a risk premium. We derive prices for all common contracts such as\nthe intraday spot price, the day-ahead spot price, and futures prices. Finally,\nwe propose an explicit model from the class of structural models and conduct an\nempirical analysis, where we find an overall negative risk premium.\n"
    },
    {
        "paper_id": 2011.04171,
        "authors": "Liao Zhu, Robert A. Jarrow, Martin T. Wells",
        "title": "Time-Invariance Coefficients Tests with the Adaptive Multi-Factor Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to test the time-invariance of the beta\ncoefficients estimated by the Adaptive Multi-Factor (AMF) model. The AMF model\nis implied by the generalized arbitrage pricing theory (GAPT), which implies\nconstant beta coefficients. The AMF model utilizes a Groupwise Interpretable\nBasis Selection (GIBS) algorithm to identify the relevant factors from among\nall traded ETFs. We compare the AMF model with the Fama-French 5-factor (FF5)\nmodel. We show that for nearly all time periods with length less than 6 years,\nthe beta coefficients are time-invariant for the AMF model, but not for the FF5\nmodel. This implies that the AMF model with a rolling window (such as 5 years)\nis more consistent with realized asset returns than is the FF5 model.\n"
    },
    {
        "paper_id": 2011.04256,
        "authors": "Matteo Gardini, Piergiacomo Sabino, Emanuela Sasso",
        "title": "A bivariate Normal Inverse Gaussian process with stochastic delay:\n  efficient simulations and applications to energy markets",
        "comments": "26 pages, 4 figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the concept of self-decomposable subordinators introduced in Gardini et\nal. [11], we build a new bivariate Normal Inverse Gaussian process that can\ncapture stochastic delays. In addition, we also develop a novel path simulation\nscheme that relies on the mathematical connection between self-decomposable\nInverse Gaussian laws and L\\'evy-driven Ornstein-Uhlenbeck processes with\nInverse Gaussian stationary distribution. We show that our approach provides an\nimprovement to the existing simulation scheme detailed in Zhang and Zhang [23]\nbecause it does not rely on an acceptance-rejection method. Eventually, these\nresults are applied to the modelling of energy markets and to the pricing of\nspread options using the proposed Monte Carlo scheme and Fourier techniques\n"
    },
    {
        "paper_id": 2011.04274,
        "authors": "Beatrice Acciaio, Mathias Beiglboeck, Gudmund Pammer",
        "title": "Weak Transport for Non-Convex Costs and Model-independence in a\n  Fixed-Income Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model-independent pricing problem in a fixed-income market and\nshow that it leads to a weak optimal transport problem as introduced by Gozlan\net al. We use this to characterize the extremal models for the pricing of\ncaplets on the spot rate and to establish a first robust super-replication\nresult that is applicable to fixed-income markets.\n  Notably, the weak transport problem exhibits a cost function which is\nnon-convex and thus not covered by the standard assumptions of the theory. In\nan independent section, we establish that weak transport problems for general\ncosts can be reduced to equivalent problems that do satisfy the convexity\nassumption, extending the scope of weak transport theory. This part could be of\nits own interest independent of our financial application, and is accessible to\nreaders who are not familiar with mathematical finance notations.\n"
    },
    {
        "paper_id": 2011.04278,
        "authors": "Ekaterina Seregina",
        "title": "A Basket Half Full: Sparse Portfolios",
        "comments": "48 pages, 4 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existing approaches to sparse wealth allocations (1) are limited to\nlow-dimensional setup when the number of assets is less than the sample size;\n(2) lack theoretical analysis of sparse wealth allocations and their impact on\nportfolio exposure; (3) are suboptimal due to the bias induced by an\n$\\ell_1$-penalty. We address these shortcomings and develop an approach to\nconstruct sparse portfolios in high dimensions. Our contribution is twofold:\nfrom the theoretical perspective, we establish the oracle bounds of sparse\nweight estimators and provide guidance regarding their distribution. From the\nempirical perspective, we examine the merit of sparse portfolios during\ndifferent market scenarios. We find that in contrast to non-sparse\ncounterparts, our strategy is robust to recessions and can be used as a hedging\nvehicle during such times.\n"
    },
    {
        "paper_id": 2011.04364,
        "authors": "Pooja Gupta, Angshul Majumdar, Emilie Chouzenoux, Giovanni Chierchia",
        "title": "SuperDeConFuse: A Supervised Deep Convolutional Transform based Fusion\n  Framework for Financial Trading Systems",
        "comments": "Accepted in Elsevier Expert Systems With Applications 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work proposes a supervised multi-channel time-series learning framework\nfor financial stock trading. Although many deep learning models have recently\nbeen proposed in this domain, most of them treat the stock trading time-series\ndata as 2-D image data, whereas its true nature is 1-D time-series data. Since\nthe stock trading systems are multi-channel data, many existing techniques\ntreating them as 1-D time-series data are not suggestive of any technique to\neffectively fusion the information carried by the multiple channels. To\ncontribute towards both of these shortcomings, we propose an end-to-end\nsupervised learning framework inspired by the previously established\n(unsupervised) convolution transform learning framework. Our approach consists\nof processing the data channels through separate 1-D convolution layers, then\nfusing the outputs with a series of fully-connected layers, and finally\napplying a softmax classification layer. The peculiarity of our framework -\nSuperDeConFuse (SDCF), is that we remove the nonlinear activation located\nbetween the multi-channel convolution layers and the fully-connected layers, as\nwell as the one located between the latter and the output layer. We compensate\nfor this removal by introducing a suitable regularization on the aforementioned\nlayer outputs and filters during the training phase. Specifically, we apply a\nlogarithm determinant regularization on the layer filters to break symmetry and\nforce diversity in the learnt transforms, whereas we enforce the non-negativity\nconstraint on the layer outputs to mitigate the issue of dead neurons. This\nresults in the effective learning of a richer set of features and filters with\nrespect to a standard convolutional neural network. Numerical experiments\nconfirm that the proposed model yields considerably better results than\nstate-of-the-art deep learning techniques for real-world problem of stock\ntrading.\n"
    },
    {
        "paper_id": 2011.04367,
        "authors": "Ivan Jericevich, Patrick Chang, Tim Gebbie",
        "title": "Comparing the market microstructure between two South African exchanges",
        "comments": "25 pages, 16 figures, 9 tables. Link to supporting Julia code:\n  https://github.com/CHNPAT005/PCIJAPTG-A2XvsJSE",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider shared listings on two South African equity exchanges: the\nJohannesburg Stock Exchange (JSE) and the A2X Exchange. A2X is an alternative\nexchange that provides for both shared listings and new listings within the\nfinancial market ecosystem of South Africa. From a science perspective it\nprovides the opportunity to compare markets trading similar shares, in a\nsimilar regulatory and economic environment, but with vastly different\nliquidity, costs and business models. A2X currently has competitive settlement\nand transaction pricing when compared to the JSE, but the JSE has deeper\nliquidity. In pursuit of an empirical understanding of how these differences\nrelate to their respective price response dynamics, we compare the\ndistributions and auto-correlations of returns on different time scales; we\ncompare price impact and master curves; and we compare the cost of trading on\neach exchange. This allows us to empirically compare the two markets. We find\nthat various stylised facts become similar as the measurement or sampling time\nscale increase. However, the same securities can have vastly different price\nresponses irrespective of time scales. This is not surprising given the\ndifferent liquidity and order-book resilience. Here we demonstrate that direct\ncosts dominate the cost of trading, and the importance of competitively\npositioning cost ceilings. Universality is crucial for being able to\nmeaningfully compare cross-exchange price responses, but in the case of A2X, it\nhas yet to emerge in a meaningful way due to the infancy of the exchange --\nmaking meaningful comparisons difficult.\n"
    },
    {
        "paper_id": 2011.04391,
        "authors": "Tadeu A. Ferreira",
        "title": "Reinforced Deep Markov Models With Applications in Automatic Trading",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by the developments in deep generative models, we propose a\nmodel-based RL approach, coined Reinforced Deep Markov Model (RDMM), designed\nto integrate desirable properties of a reinforcement learning algorithm acting\nas an automatic trading system. The network architecture allows for the\npossibility that market dynamics are partially visible and are potentially\nmodified by the agent's actions. The RDMM filters incomplete and noisy data, to\ncreate better-behaved input data for RL planning. The policy search\noptimisation also properly accounts for state uncertainty. Due to the\ncomplexity of the RKDF model architecture, we performed ablation studies to\nunderstand the contributions of individual components of the approach better.\nTo test the financial performance of the RDMM we implement policies using\nvariants of Q-Learning, DynaQ-ARIMA and DynaQ-LSTM algorithms. The experiments\nshow that the RDMM is data-efficient and provides financial gains compared to\nthe benchmarks in the optimal execution problem. The performance improvement\nbecomes more pronounced when price dynamics are more complex, and this has been\ndemonstrated using real data sets from the limit order book of Facebook, Intel,\nVodafone and Microsoft.\n"
    },
    {
        "paper_id": 2011.044,
        "authors": "Soumajyoti Sarkar",
        "title": "Bandits in Matching Markets: Ideas and Proposals for Peer Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by recent applications of sequential decision making in matching\nmarkets, in this paper we attempt at formulating and abstracting market designs\nfor P2P lending. We describe a paradigm to set the stage for how peer to peer\ninvestments can be conceived from a matching market perspective, especially\nwhen both borrower and lender preferences are respected. We model these\nspecialized markets as an optimization problem and consider different utilities\nfor agents on both sides of the market while also understanding the impact of\nequitable allocations to borrowers. We devise a technique based on sequential\ndecision making that allow the lenders to adjust their choices based on the\ndynamics of uncertainty from competition over time and that also impacts the\nrewards in return for their investments. Using simulated experiments we show\nthe dynamics of the regret based on the optimal borrower-lender matching and\nfind that the lender regret depends on the initial preferences set by the\nlenders which could affect their learning over decision making steps.\n"
    },
    {
        "paper_id": 2011.04466,
        "authors": "Vinay Reddy Venumuddala",
        "title": "Occupational Network Structure and Vector Assortativity for illustrating\n  patterns of social mobility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we arrive at a closed form expression for measuring vector\nassortativity in networks motivated by our use-case which is to observe\npatterns of social mobility in a society. Based on existing works on social\nmobility within economics literature, and social reproduction within sociology\nliterature, we motivate the construction of an occupational network structure\nto observe mobility patterns. Basing on existing literature, over this\nstructure, we define mobility as assortativity of occupations attributed by the\nrepresentation of categories such as gender, geography or social groups. We\ncompare the results from our vector assortativity measure and averaged scalar\nassortativity in the Indian context, relying on NSSO 68th round on employment\nand unemployment. Our findings indicate that the trends indicated by our vector\nassortativity measure is very similar to what is indicated by the averaged\nscalar assortativity index. We discuss some implications of this work and\nsuggest future directions.\n"
    },
    {
        "paper_id": 2011.04544,
        "authors": "Mariano Zeron, Ignacio Ruiz",
        "title": "Dynamic sensitivities and Initial Margin via Chebyshev Tensors",
        "comments": "21 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents how to use Chebyshev Tensors to compute dynamic\nsensitivities of financial instruments within a Monte Carlo simulation. Dynamic\nsensitivities are then used to compute Dynamic Initial Margin as defined by\nISDA (SIMM). The technique is benchmarked against the computation of dynamic\nsensitivities obtained by using pricing functions like the ones found in risk\nengines. We obtain high accuracy and computational gains for FX swaps and\nSpread Options.\n"
    },
    {
        "paper_id": 2011.04545,
        "authors": "Elizabeth Fons, Paula Dawson, Xiao-jun Zeng, John Keane, Alexandros\n  Iosifidis",
        "title": "Augmenting transferred representations for stock classification",
        "comments": "Draws heavily from arXiv:2010.15111",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock classification is a challenging task due to high levels of noise and\nvolatility of stocks returns. In this paper we show that using transfer\nlearning can help with this task, by pre-training a model to extract universal\nfeatures on the full universe of stocks of the S$\\&$P500 index and then\ntransferring it to another model to directly learn a trading rule. Transferred\nmodels present more than double the risk-adjusted returns than their\ncounterparts trained from zero. In addition, we propose the use of data\naugmentation on the feature space defined as the output of a pre-trained model\n(i.e. augmenting the aggregated time-series representation). We compare this\naugmentation approach with the standard one, i.e. augmenting the time-series in\nthe input space. We show that augmentation methods on the feature space leads\nto $20\\%$ increase in risk-adjusted return compared to a model trained with\ntransfer learning but without augmentation.\n"
    },
    {
        "paper_id": 2011.04587,
        "authors": "Lina Silva-Rodriguez (1, 2 and 3), Anibal Sanjab (1 and 2), Elena\n  Fumagalli (3), Ana Virag (1 and 2), Madeleine Gibescu (3) ((1) Flemish\n  Institute for Technological Research (VITO), Belgium, (2) EnergyVille,\n  Belgium, (3) Copernicus Institute of Sustainable Development - Utrecht\n  University, Netherlands)",
        "title": "Short Term Electricity Market Designs: Identified Challenges and\n  Promising Solutions",
        "comments": "19 pages, 1 figure, preprint submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The electricity market, which was initially designed for dispatchable power\nplants and inflexible demand, is being increasingly challenged by new trends,\nsuch as the high penetration of intermittent renewables and the transformation\nof the consumers energy space. To accommodate these new trends and improve the\nperformance of the market, several modifications to current market designs have\nbeen proposed in the literature. Given the vast variety of these proposals,\nthis paper provides a comprehensive investigation of the modifications proposed\nin the literature as well as a detailed assessment of their suitability for\nimproving market performance under the continuously evolving electricity\nlandscape. To this end, first, a set of criteria for an ideal market design is\nproposed, and the barriers present in current market designs hindering the\nfulfillment of these criteria are identified. Then, the different market\nsolutions proposed in the literature, which could potentially mitigate these\nbarriers, are extensively explored. Finally, a taxonomy of the proposed\nsolutions is presented, highlighting the barriers addressed by each proposal\nand the associated implementation challenges. The outcomes of this analysis\nshow that even though each barrier is addressed by at least one proposed\nsolution, no single proposal is able to address all the barriers\nsimultaneously. In this regard, a future-proof market design must combine\ndifferent elements of proposed solutions to comprehensively mitigate market\nbarriers and overcome the identified implementation challenges. Thus, by\nthoroughly reviewing this rich body of literature, this paper introduces key\ncontributions enabling the advancement of the state-of-the-art towards\nincreasingly efficient electricity market.\n"
    },
    {
        "paper_id": 2011.04804,
        "authors": "Tao Chen, Jiyoun Myung",
        "title": "Nonparametric Adaptive Bayesian Stochastic Control Under Model\n  Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a new methodology for solving a discrete time\nstochastic Markovian control problem under model uncertainty. By utilizing the\nDirichlet process, we model the unknown distribution of the underlying\nstochastic process as a random probability measure and achieve online learning\nin a Bayesian manner. Our approach integrates optimizing and dynamic learning.\nWhen dealing with model uncertainty, the nonparametric framework allows us to\navoid model misspecification that usually occurs in other classical control\nmethods. Then, we develop a numerical algorithm to handle the infinitely\ndimensional state space in this setup and utilizes Gaussian process surrogates\nto obtain a functional representation of the value function in the Bellman\nrecursion. We also build separate surrogates for optimal control to eliminate\nrepeated optimizations on out-of-sample paths and bring computational\nspeed-ups. Finally, we demonstrate the financial advantages of the\nnonparametric Bayesian framework compared to parametric approaches such as\nstrong robust and time consistent adaptive.\n"
    },
    {
        "paper_id": 2011.04889,
        "authors": "Silvana Pesenti, Qiuqi Wang and Ruodu Wang",
        "title": "Optimizing distortion riskmetrics with distributional uncertainty",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Optimization of distortion riskmetrics with distributional uncertainty has\nwide applications in finance and operations research. Distortion riskmetrics\ninclude many commonly applied risk measures and deviation measures, which are\nnot necessarily monotone or convex. One of our central findings is a unifying\nresult that allows us to convert an optimization of a non-convex distortion\nriskmetric with distributional uncertainty to a convex one, leading to great\ntractability. A sufficient condition to the unifying equivalence result is the\nnovel notion of closedness under concentration, a variation of which is also\nshown to be necessary for the equivalence. Our results include many special\ncases that are well studied in the optimization literature, including but not\nlimited to optimizing probabilities, Value-at-Risk, Expected Shortfall, Yaari's\ndual utility, and differences between distortion risk measures, under various\nforms of distributional uncertainty. We illustrate our theoretical results via\napplications to portfolio optimization, optimization under moment constraints,\nand preference robust optimization.\n"
    },
    {
        "paper_id": 2011.04939,
        "authors": "Ao Kong, Robert Azencott, Hongliang Zhu, Xindan Li",
        "title": "Pattern recognition in micro-trading behaviors before stock price jumps:\n  A framework based on multivariate time series analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Studying the micro-trading behaviors before stock price jumps is an important\nproblem for financial regulations and investment decisions. In this study, we\nprovide a new framework to study pre-jump trading behaviors based on\nmultivariate time series analysis. Different from the existing literature, our\nmethodology takes into account the temporal information embedded in the\ntrading-related attributes and can better evaluate and compare the abnormality\nlevels of different attributes. Moreover, it can explore the joint\ninformativeness of the attributes as well as select a subset of highly\ninformative but minimally redundant attributes to analyze the homogeneous and\nidiosyncratic patterns in the pre-jump trades of individual stocks. In\naddition, our analysis involves a set of technical indicators to describe\nmicro-trading behaviors. To illustrate the viability of the proposed\nmethodology, an application case is conducted based on the level-2 data of 189\nconstituent stocks of the China Security Index 300. The individual and joint\ninformativeness levels of the attributes in predicting price jumps are\nevaluated and compared. To this end, our experiment provides a set of jump\nindicators that can represent the pre-jump trading behaviors in the Chinese\nstock market and have detected some stocks with extremely abnormal pre-jump\ntrades.\n"
    },
    {
        "paper_id": 2011.05023,
        "authors": "Peter Bank and Yan Dolinsky",
        "title": "A Note on Utility Indifference Pricing with Delayed Information",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the Bachelier model with information delay where investment\ndecisions can be based only on observations from $H>0$ time units before.\nUtility indifference prices are studied for vanilla options and we compute\ntheir non-trivial scaling limit for vanishing delay when risk aversion is\nscaled liked $A/H$ for some constant $A$. Using techniques from [7], we develop\ndiscrete-time duality for this setting and show how the relaxed form of\nmartingale property introduced by [9] results in the scaling limit taking the\nform of a volatility control problem with quadratic penalty.\n"
    },
    {
        "paper_id": 2011.05067,
        "authors": "Miguel de Carvalho, Manuele Leonelli, Alex Rossi",
        "title": "Tracking change-points in multivariate extremes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we devise a statistical method for tracking and modeling\nchange-points on the dependence structure of multivariate extremes. The methods\nare motivated by and illustrated on a case study on crypto-assets.\n"
    },
    {
        "paper_id": 2011.05117,
        "authors": "Andreas A. Aigner and Walter Schrabmair",
        "title": "Startup & Unicorn Growth Valuation",
        "comments": "13 pages, 7 Tables, 7 Figures",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.33818.47048",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How do you value companies which have IPOed recently? How do you compare them\namongst their peers? Valuing companies using a linear extrapolation of their\nrevenues and profits leads to an ingenious method to benchmark stocks against\neach other. Here we present such a method, dubbed the growth average U1.\n"
    },
    {
        "paper_id": 2011.05278,
        "authors": "Ivan Arraut, Alan Au and Alan Ching-biu Tse",
        "title": "Spontaneous symmetry breaking in Quantum Finance",
        "comments": "7 pages, published in EPL",
        "journal-ref": "EPL, 131 (2020) 68003",
        "doi": "10.1209/0295-5075/131/68003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the phenomena of spontaneous symmetry breaking in Quantum Finance\nby using as a starting point the Black-Scholes (BS) and the Merton-Garman (MG)\nequations expressed in the Hamiltonian form. In this scenario the martingale\ncondition (state) corresponds to the vacuum state which becomes degenerate when\nthe symmetry of the system is spontaneously broken. We then analyze the broken\nsymmetries of the system and we interpret from the perspective of Financial\nmarkets the possible appearance of the Nambu-Goldstone bosons.\n"
    },
    {
        "paper_id": 2011.05381,
        "authors": "Eric Andr\\'e and Guillaume Coqueret",
        "title": "Dirichlet policies for reinforced factor portfolios",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article aims to combine factor investing and reinforcement learning\n(RL). The agent learns through sequential random allocations which rely on\nfirms' characteristics. Using Dirichlet distributions as the driving policy, we\nderive closed forms for the policy gradients and analytical properties of the\nperformance measure. This enables the implementation of REINFORCE methods,\nwhich we perform on a large dataset of US equities. Across a large range of\nparametric choices, our result indicates that RL-based portfolios are very\nclose to the equally-weighted (1/N) allocation. This implies that the agent\nlearns to be *agnostic* with regard to factors, which can partly be explained\nby cross-sectional regressions showing a strong time variation in the\nrelationship between returns and firm characteristics.\n"
    },
    {
        "paper_id": 2011.05458,
        "authors": "Atilla Aras",
        "title": "Solution to the Equity Premium Puzzle",
        "comments": "41 pages, 7 figures, 1 table, The Section of New Model was expanded,\n  A New Section with new references was added, no result changed",
        "journal-ref": "Research of Financial Economic and Social Studies 7 (2022) 612-631",
        "doi": "10.29106/fesa.1124492",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study provides a solution of the equity premium puzzle. Questioning the\nvalidity of the Arrow-Pratt measure of relative risk aversion for detecting the\nrisk behavior of investors under all conditions, a new tool, that is, the\nsufficiency factor of the model was developed to analyze the risk behavior of\ninvestors. The calculations of this newly tested model show that the value of\nthe coefficient of relative risk aversion is 1.033526 by assuming the value of\nthe subjective time discount factor as 0.99. Since these values are compatible\nwith the existing empirical studies, they confirm the validity of the newly\nderived model that provides a solution to the equity premium puzzle.\n"
    },
    {
        "paper_id": 2011.05588,
        "authors": "Alexey Averkin and Sergey Yarushev",
        "title": "Deep Neural Networks and Neuro-Fuzzy Networks for Intellectual Analysis\n  of Economic Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In tis paper we consider approaches for time series forecasting based on deep\nneural networks and neuro-fuzzy nets. Also, we make short review of researches\nin forecasting based on various models of ANFIS models. Deep Learning has\nproven to be an effective method for making highly accurate predictions from\ncomplex data sources. Also, we propose our models of DL and Neuro-Fuzzy\nNetworks for this task. Finally, we show possibility of using these models for\ndata science tasks. This paper presents also an overview of approaches for\nincorporating rule-based methodology into deep learning neural networks.\n"
    },
    {
        "paper_id": 2011.05589,
        "authors": "Guanxing Fu and Ulrich Horst and Xiaonyu Xia",
        "title": "Portfolio Liquidation Games with Self-Exciting Order Flow",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze novel portfolio liquidation games with self-exciting order flow.\nBoth the N-player game and the mean-field game are considered. We assume that\nplayers' trading activities have an impact on the dynamics of future market\norder arrivals thereby generating an additional transient price impact. Given\nthe strategies of her competitors each player solves a mean-field control\nproblem. We characterize open-loop Nash equilibria in both games in terms of a\nnovel mean-field FBSDE system with unknown terminal condition. Under a weak\ninteraction condition we prove that the FBSDE systems have unique solutions.\nUsing a novel sufficient maximum principle that does not require convexity of\nthe cost function we finally prove that the solution of the FBSDE systems do\nindeed provide existence and uniqueness of open-loop Nash equilibria.\n"
    },
    {
        "paper_id": 2011.05658,
        "authors": "Gian Maria Campedelli, Serena Favarin, Alberto Aziani, Alex R. Piquero",
        "title": "Disentangling Community-level Changes in Crime Trends During the\n  COVID-19 Pandemic in Chicago",
        "comments": "33 pages, 9 figures, published in Crime Science",
        "journal-ref": "Crime Sci 9, 21 (2020)",
        "doi": "10.1186/s40163-020-00131-8",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent studies exploiting city-level time series have shown that, around the\nworld, several crimes declined after COVID-19 containment policies have been\nput in place. Using data at the community-level in Chicago, this work aims to\nadvance our understanding on how public interventions affected criminal\nactivities at a finer spatial scale. The analysis relies on a two-step\nmethodology. First, it estimates the community-wise causal impact of social\ndistancing and shelter-in-place policies adopted in Chicago via Structural\nBayesian Time-Series across four crime categories (i.e., burglary, assault,\nnarcotics-related offenses, and robbery). Once the models detected the\ndirection, magnitude and significance of the trend changes, Firth's Logistic\nRegression is used to investigate the factors associated to the statistically\nsignificant crime reduction found in the first step of the analyses.\nStatistical results first show that changes in crime trends differ across\ncommunities and crime types. This suggests that beyond the results of aggregate\nmodels lies a complex picture characterized by diverging patterns. Second,\nregression models provide mixed findings regarding the correlates associated\nwith significant crime reduction: several relations have opposite directions\nacross crimes with population being the only factor that is stably and\npositively associated with significant crime reduction.\n"
    },
    {
        "paper_id": 2011.05809,
        "authors": "Fabian Scheller, Robert Burkhardt, Robert Schwarzeit, Russell McKenna,\n  Thomas Bruckner",
        "title": "Competition between simultaneous demand-side flexibility options: The\n  case of community electricity storage systems",
        "comments": null,
        "journal-ref": "Applied Energy, Volume 269, 2020, 114969",
        "doi": "10.1016/j.apenergy.2020.114969",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Community electricity storage systems for multiple applications promise\nbenefits over household electricity storage systems. More economical\nflexibility options such as demand response and sector coupling might reduce\nthe market size for storage facilities. This paper assesses the economic\nperformance of community electricity storage systems by taking competitive\nflexibility options into account. For this purpose, an actor-related,\nscenario-based optimization framework is applied. The results are in line with\nthe literature and show that community storage systems are economically more\nefficient than household storage systems. Relative storage capacity reductions\nof community storage systems over household storage systems are possible, as\nthe demand and generation profiles are balanced out among end users. On\naverage, storage capacity reductions of 9% per household are possible in the\nbase case, resulting in lower specific investments. The simultaneous\napplication of demand-side flexibility options such as sector coupling and\ndemand response enable a further capacity reduction of the community storage\nsize by up to 23%. At the same time, the competition between flexibility\noptions leads to smaller benefits regarding the community storage flexibility\npotential, which reduces the market viability for these applications. In the\nworst case, the cannibalization effects reach up to 38% between the flexibility\nmeasures. The losses of the flexibility benefits outweigh the savings of the\ncapacity reduction whereby sector coupling constitutes a far greater\ninfluencing factor than demand response. Overall, in consideration of the\nstated cost trends, the economies of scale, and the reduction possibilities, a\nprofitable community storage model might be reached between 2025 and 2035.\nFuture work should focus on the analysis of policy frameworks.\n"
    },
    {
        "paper_id": 2011.0583,
        "authors": "Philipp Andreas Gunkel, Claire Bergaentzl\\'e, Ida Gr{\\ae}sted Jensen,\n  Fabian Scheller",
        "title": "From passive to active: Flexibility from electric vehicles in the\n  context of transmission system development",
        "comments": null,
        "journal-ref": "Applied Energy, Volume 277, 2020, 115526",
        "doi": "10.1016/j.apenergy.2020.115526",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Electrification of transport in RES-based power system will support the\ndecarbonisation of the transportsector. However, due to the increase in energy\ndemand and the large peak effects of charging, the passiveintegration of\nelectric cars is likely to undermine sustainability efforts. This study\ninvestigates three differentcharging strategies for electric vehicle in Europe\noffering various degrees of flexibility: passive charging,smart charging and\nvehicle-to-grid, and puts this flexibility in perspective with the flexibility\noffered byinterconnections. We use the Balmorel optimization tool to represent\nthe short-term dispatch and long-terminvestment in the energy system and we\ncontribute to the state-of-the-art in developing new methodologiesto represent\nhome charging and battery degradation. Our results show how each step of\nincreased chargingflexibility reduces system costs, affects energy mix, impacts\nspot prices and reduces CO2 emissions untilthe horizon 2050. We quantify how\nflexible charging and variable generation mutually support each other(\u00bf100TWh\nfrom wind and solar energy in 2050) and restrict the business case for\nstationary batteries, whereaspassive charging results in a substitution of wind\nby solar energy. The comparison of each charging schemewith and without\ninterconnection expansion highlights the interplay between European countries\nin terms ofelectricity prices and CO2 emissions in the context of electrified\ntransport. Although the best outcome isreached under the most flexible scenario\nat the EU level, the situation of the countries with the cheapest andmost\ndecarbonised electricity mix is damaged, which calls for adapted coordination\npolicy at the EU level.\n"
    },
    {
        "paper_id": 2011.05839,
        "authors": "Upasak Das, Udayan Rathore, Prasenjit Sarkhel",
        "title": "Social Diversity and Spread of Pandemic: Evidence from India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Compliance with the public health guidelines during a pandemic requires\ncoordinated community actions which might be undermined in socially diverse\nareas. In this paper, we assess the relationship between caste-group diversity\nand the spread of COVID-19 infection during the nationwide lockdown and\nunlocking period in India. On the extensive margin, we find that\ncaste-homogeneous districts systematically took more days to cross the\nconcentration thresholds of 50 to 500 cases. Estimates on the intensive margin,\nusing daily cases, further show that caste-homogeneous districts experienced\nslower growth in infection. Overall, the effects of caste-group homogeneity\nremained positive and statistically significant for 2.5 months (about 76 days)\nafter the beginning of the lockdown and weakened with subsequent phases of the\nlockdown. The results hold even after accounting for the emergence of initial\nhotspots before lockdown, broader diffusion patterns through daily fixed\neffects, region fixed effects, and dynamic administrative response through\ntime-variant lagged COVID-19 fatalities at the district level. These effects\nare not found to be confounded by differential levels of testing and\nunderreporting of cases in some states. Consistent estimates from bias-adjusted\ntreatment effects also ensure that our findings remain robust even after\naccounting for other unobservables. We find suggestive evidence of higher\nengagement of community health workers in caste-homogenous localities, which\nfurther increased after the lockdown. We posit this as one potential channel\nthat can explain our results. Our findings reveal how caste-group diversity can\nbe used to identify potential hotspots during public health emergencies and\nemphasize the importance of community health workers and decentralized policy\nresponse.\n"
    },
    {
        "paper_id": 2011.05858,
        "authors": "Constantine Goulimis, Gaston Simone",
        "title": "Reel Stock Analysis for an Integrated Paper Packaging Company",
        "comments": "New version corrects a couple of typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The production of corrugated paper boxes accounts for roughly one third of\nthe world's total paper production and, as a result of both COVID-19 and the\nrise of e-commerce, is a growing market. We provide a fresh approach to\ndetermining near-optimal stock policies for integrated paper companies. The new\napproach shows that existing policies can be improved by a significant margin.\nIn a case study we saw a reduction in total waste by 9%, with a simultaneous\ndecrease in logistics costs.\n"
    },
    {
        "paper_id": 2011.05915,
        "authors": "Jann Michael Weinand, Fabian Scheller, Russell McKenna",
        "title": "Reviewing energy system modelling of decentralized energy autonomy",
        "comments": null,
        "journal-ref": "Energy, Volume 203, 2020, 117817",
        "doi": "10.1016/j.energy.2020.117817",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Research attention on decentralized autonomous energy systems has increased\nexponentially in the past three decades, as demonstrated by the absolute number\nof publications and the share of these studies in the corpus of energy system\nmodelling literature. This paper shows the status quo and future modelling\nneeds for research on local autonomous energy systems. A total of 359 studies\nare roughly investigated, of which a subset of 123 in detail. The studies are\nassessed with respect to the characteristics of their methodology and\napplications, in order to derive common trends and insights. Most case studies\napply to middle-income countries and only focus on the supply of electricity in\nthe residential sector. Furthermore, many of the studies are comparable\nregarding objectives and applied methods. Local energy autonomy is associated\nwith high costs, leading to levelized costs of electricity of 0.41 $/kWh on\naverage. By analysing the studies, many improvements for future studies could\nbe identified: the studies lack an analysis of the impact of autonomous energy\nsystems on surrounding energy systems. In addition, the robust design of\nautonomous energy systems requires higher time resolutions and extreme\nconditions. Future research should also develop methodologies to consider local\nstakeholders and their preferences for energy systems.\n"
    },
    {
        "paper_id": 2011.05984,
        "authors": "Hirdesh K. Pharasi, Eduard Seligman, Suchetana Sadhukhan, Parisa\n  Majari, and Thomas H.Seligman",
        "title": "Dynamics of market states and risk assessment",
        "comments": "22 pages and 27 figures. arXiv admin note: text overlap with\n  arXiv:2003.07058",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous research explored various conditions of financial markets based on\nthe similarity of correlation structures and classified as market states. We\nintroduce modifications to previous selection criteria for these market states,\nmainly due to increased attention to the transition matrix between the states.\nClustering and thus market states are fixed by the optimization of two\nparameters -- number of clusters and noise suppression, but in similar\nconditions, we give preference to the clustering which avoids large jumps in\nthe transition matrix. We found statistically significant results applying this\nmodel to the SP 500 and Nikkei 225 markets for the pre-COVID-19 pandemic era\n(2006-2019). Retaining the epoch length of 20 trading days but reducing the\nshift of the epoch to a single trading day we are led to the concept of a\ntrajectory of the market in the space of correlation matrices. We may visualize\nthese states after dimensional scaling to two or three dimensions. This\napproach, using dynamics, improves the options of risk assessment, opens the\ndoor to dynamical treatments of markets (e.g. hedging), and shows noise\nsuppression in a new light.\n"
    },
    {
        "paper_id": 2011.0606,
        "authors": "Deepanshu Sharma and Kritika Phulli",
        "title": "Forecasting and Analyzing the Military Expenditure of India Using\n  Box-Jenkins ARIMA Model",
        "comments": "12 pages,5 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The advancement in the field of statistical methodologies to economic data\nhas paved its path towards the dire need for designing efficient military\nmanagement policies. India is ranked as the third largest country in terms of\nmilitary spender for the year 2019. Therefore, this study aims at utilizing the\nBox-Jenkins ARIMA model for time series forecasting of the military expenditure\nof India in forthcoming times. The model was generated on the SIPRI dataset of\nIndian military expenditure of 60 years from the year 1960 to 2019. The trend\nwas analysed for the generation of the model that best fitted the forecasting.\nThe study highlights the minimum AIC value and involves ADF testing (Augmented\nDickey-Fuller) to transform expenditure data into stationary form for model\ngeneration. It also focused on plotting the residual error distribution for\nefficient forecasting. This research proposed an ARIMA (0,1,6) model for\noptimal forecasting of military expenditure of India with an accuracy of 95.7%.\nThe model, thus, acts as a Moving Average (MA) model and predicts the\nsteady-state exponential growth of 36.94% in military expenditure of India by\n2024.\n"
    },
    {
        "paper_id": 2011.06281,
        "authors": "Dietmar Pfeifer, Olena Ragulina",
        "title": "Generating unfavourable VaR scenarios with patchwork copulas",
        "comments": "26 pages, 14 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The central idea of the paper is to present a general simple patchwork\nconstruction principle for multivariate copulas that create unfavourable VaR\n(i.e. Value at Risk) scenarios while maintaining given marginal distributions.\nThis is of particular interest for the construction of Internal Models in the\ninsurance industry under Solvency II in the European Union. The method is\nexemplified with a 19-dimensional real-life data set of insurance losses.\n"
    },
    {
        "paper_id": 2011.06287,
        "authors": "Ruiqi Li, Lingyun Lu, Weiwei Gu, Shaodong Ma, Gang Xu and H. Eugene\n  Stanley",
        "title": "Assessing the attraction of cities on venture capital from a scaling law\n  perspective",
        "comments": null,
        "journal-ref": "IEEE Access, 2021",
        "doi": "10.1109/ACCESS.2021.3068317",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Cities are centers for the integration of capital and incubators of\ninvention, and attracting venture capital (VC) is of great importance for\ncities to advance in innovative technology and business models towards a\nsustainable and prosperous future. Yet we still lack a quantitative\nunderstanding of the relationship between urban characteristics and VC\nactivities. In this paper, we find a clear nonlinear scaling relationship\nbetween VC activities and the urban population of Chinese cities. In such\nnonlinear systems, the widely applied linear per capita indicators would be\neither biased to larger cities or smaller cities depends on whether it is\nsuperlinear or sublinear, while the residual of cities relative to the\nprediction of scaling law is a more objective and scale-invariant metric.\n%(i.e., independent of the city size). Such a metric can distinguish the\neffects of local dynamics and scaled growth induced by the change of population\nsize. The spatiotemporal evolution of such metrics on VC activities reveals\nthree distinct groups of cities, two of which stand out with increasing and\ndecreasing trends, respectively. And the taxonomy results together with spatial\nanalysis also signify different development modes between large urban\nagglomeration regions. Besides, we notice the evolution of scaling exponents on\nVC activities are of much larger fluctuations than on socioeconomic output of\ncities, and a conceptual model that focuses on the growth dynamics of different\nsized cities can well explain it, which we assume would be general to other\nscenarios.\n"
    },
    {
        "paper_id": 2011.0643,
        "authors": "Xingchen Wan, Jie Yang, Slavi Marinov, Jan-Peter Calliess, Stefan\n  Zohren, Xiaowen Dong",
        "title": "Sentiment Correlation in Financial News Networks and Associated Market\n  Movements",
        "comments": "12 pages, 5 figures, 1 table (29 pages including References and\n  Appendices). Published in Scientific Reports 11",
        "journal-ref": "Sci. Rep. 11, 3062 (2021)",
        "doi": "10.1038/s41598-021-82338-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an increasingly connected global market, news sentiment towards one\ncompany may not only indicate its own market performance, but can also be\nassociated with a broader movement on the sentiment and performance of other\ncompanies from the same or even different sectors. In this paper, we apply NLP\ntechniques to understand news sentiment of 87 companies among the most reported\non Reuters for a period of seven years. We investigate the propagation of such\nsentiment in company networks and evaluate the associated market movements in\nterms of stock price and volatility. Our results suggest that, in certain\nsectors, strong media sentiment towards one company may indicate a significant\nchange in media sentiment towards related companies measured as neighbours in a\nfinancial network constructed from news co-occurrence. Furthermore, there\nexists a weak but statistically significant association between strong media\nsentiment and abnormal market return as well as volatility. Such an association\nis more significant at the level of individual companies, but nevertheless\nremains visible at the level of sectors or groups of companies.\n"
    },
    {
        "paper_id": 2011.06474,
        "authors": "Anne G. Balter, Nikolaus Schweizer, Juan C. Vera",
        "title": "Contingent Capital with Stock Price Triggers in Interbank Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies existence and uniqueness of equilibrium prices in a model\nof the banking sector in which banks trade contingent convertible bonds with\nstock price triggers among each other. This type of financial product was\nproposed as an instrument for stabilizing the global banking system after the\nfinancial crisis. Yet it was recognized early on that these products may create\ncircularity problems in the definition of stock prices - even in the absence of\ntrade. We find that if conversion thresholds are such that bond holders are\nindifferent about marginal conversions, there exists a unique equilibrium\nirrespective of the network structure. When thresholds are lower, existence of\nequilibrium breaks down while higher thresholds may lead to multiplicity of\nequilibria. Moreover, there are complex network effects. One bank's conversion\nmay trigger further conversions - or prevent them, depending on the\nconstellations of asset values and conversion triggers.\n"
    },
    {
        "paper_id": 2011.06492,
        "authors": "Adam Bouland, Wim van Dam, Hamed Joorati, Iordanis Kerenidis, Anupam\n  Prakash",
        "title": "Prospects and challenges of quantum finance",
        "comments": "49 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum computers are expected to have substantial impact on the finance\nindustry, as they will be able to solve certain problems considerably faster\nthan the best known classical algorithms. In this article we describe such\npotential applications of quantum computing to finance, starting with the\nstate-of-the-art and focusing in particular on recent works by the QC Ware\nteam. We consider quantum speedups for Monte Carlo methods, portfolio\noptimization, and machine learning. For each application we describe the extent\nof quantum speedup possible and estimate the quantum resources required to\nachieve a practical speedup. The near-term relevance of these quantum finance\nalgorithms varies widely across applications - some of them are heuristic\nalgorithms designed to be amenable to near-term prototype quantum computers,\nwhile others are proven speedups which require larger-scale quantum computers\nto implement. We also describe powerful ways to bring these speedups closer to\nexperimental feasibility - in particular describing lower depth algorithms for\nMonte Carlo methods and quantum machine learning, as well as quantum annealing\nheuristics for portfolio optimization. This article is targeted at financial\nprofessionals and no particular background in quantum computation is assumed.\n"
    },
    {
        "paper_id": 2011.06592,
        "authors": "Shteryo Nozharov and Nina Nikolova",
        "title": "Shadow economy and populism-risk and uncertainty factors for\n  establishing low-carbon economy of Balkan countries-case study for Bulgaria",
        "comments": "scopus indexed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main purpose of the current publication is to formulate a scenario model\nfor analysis of the opportunities for low-carbon economy establishment in the\ncountries with transition economies.The model studies risk factors such as\nshadow economy level and populism based on the implementation and development\nof Balkan countries economic policy and at the same time shows future climate\nchanges tendencies and uncertainties of climate models.A transdisciplinary\napproach is implemented in the study. Climate change perception and\nunderstanding about low-carbon economy are examined through the public opinion\nand analysis of mass-media publications.The results of the research are\nimportant in order to clarify the multicultural divergences as a factor for\nrisk and uncertainty in the implementation process of the policy for climate\nchange.In this way geographical aspects of risk and uncertainty, which are not\nonly related to the economic development of the relevant countries, could be\nbrought out.\n"
    },
    {
        "paper_id": 2011.06618,
        "authors": "Jorge Gonz\\'alez C\\'azares and Aleksandar Mijatovi\\'c",
        "title": "Simulation of the drawdown and its duration in L\\'{e}vy models via\n  stick-breaking Gaussian approximation",
        "comments": "45 pages, 6 figures, 4 tables, short video on\n  https://youtu.be/EL0v2QUb5tQ",
        "journal-ref": "Finance and Stochastics, 26, pages 671-732 (2022)",
        "doi": "10.1007/s00780-022-00486-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a computational method for expected functionals of the drawdown\nand its duration in exponential L\\'evy models. It is based on a novel\nsimulation algorithm for the joint law of the state, supremum and time the\nsupremum is attained of the Gaussian approximation of a general L\\'evy process.\nWe bound the bias for various locally Lipschitz and discontinuous payoffs\narising in applications and analyse the computational complexities of the\ncorresponding Monte Carlo and multilevel Monte Carlo estimators. Monte Carlo\nmethods for L\\'evy processes (using Gaussian approximation) have been analysed\nfor Lipschitz payoffs, in which case the computational complexity of our\nalgorithm is up to two orders of magnitude smaller when the jump activity is\nhigh. At the core of our approach are bounds on certain Wasserstein distances,\nobtained via the novel SBG coupling between a L\\'evy process and its Gaussian\napproximation. Numerical performance, based on the implementation in the\ndedicated GitHub repository, exhibits a good agreement with our theoretical\nbounds.\n"
    },
    {
        "paper_id": 2011.06693,
        "authors": "Hamidreza Arian, Hossein Poorvasei, Azin Sharifi, Shiva Zamani",
        "title": "The Uncertain Shape of Grey Swans: Extreme Value Theory with Uncertain\n  Threshold",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Extreme Value Theory (EVT) is one of the most commonly used approaches in\nfinance for measuring the downside risk of investment portfolios, especially\nduring financial crises. In this paper, we propose a novel approach based on\nEVT called Uncertain EVT to improve its forecast accuracy and capture the\nstatistical characteristics of risk beyond the EVT threshold. In our framework,\nthe extreme risk threshold, which is commonly assumed a constant, is a dynamic\nrandom variable. More precisely, we model and calibrate the EVT threshold by a\nstate-dependent hidden variable, called Break-Even Risk Threshold (BRT), as a\nfunction of both risk and ambiguity. We will show that when EVT approach is\ncombined with the unobservable BRT process, the Uncertain EVT's predicted VaR\ncan foresee the risk of large financial losses, outperforms the original EVT\napproach out-of-sample, and is competitive to well-known VaR models when\nback-tested for validity and predictability.\n"
    },
    {
        "paper_id": 2011.06753,
        "authors": "David T. Frazier, Eric Renault, Lina Zhang, Xueyan Zhao",
        "title": "Weak Identification in Discrete Choice Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of weak identification in discrete choice models, and\nprovide insights into the determinants of identification strength in these\nmodels. Using these insights, we propose a novel test that can consistently\ndetect weak identification in commonly applied discrete choice models, such as\nprobit, logit, and many of their extensions. Furthermore, we demonstrate that\nwhen the null hypothesis of weak identification is rejected, Wald-based\ninference can be carried out using standard formulas and critical values. A\nMonte Carlo study compares our proposed testing approach against commonly\napplied weak identification tests. The results simultaneously demonstrate the\ngood performance of our approach and the fundamental failure of using\nconventional weak identification tests for linear models in the discrete choice\nmodel context. Furthermore, we compare our approach against those commonly\napplied in the literature in two empirical examples: married women labor force\nparticipation, and US food aid and civil conflicts.\n"
    },
    {
        "paper_id": 2011.06778,
        "authors": "Minoru Osawa, Takashi Akamatsu, and Yosuke Kogure",
        "title": "Stochastic stability of agglomeration patterns in an urban retail model",
        "comments": "26 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model of urban spatial structure proposed by Harris and Wilson\n(Environment and Planning A, 1978). The model consists of fast dynamics, which\nrepresent spatial interactions between locations by the entropy-maximizing\nprinciple, and slow dynamics, which represent the evolution of the spatial\ndistribution of local factors that facilitate such spatial interactions. One\nknown limitation of the Harris and Wilson model is that it can have multiple\nlocally stable equilibria, leading to a dependence of predictions on the\ninitial state. To overcome this, we employ equilibrium refinement by stochastic\nstability. We build on the fact that the model is a large-population potential\ngame and that stochastically stable states in a potential game correspond to\nglobal potential maximizers. Unlike local stability under deterministic\ndynamics, the stochastic stability approach allows a unique and unambiguous\nprediction for urban spatial configurations. We show that, in the most likely\nspatial configuration, the number of retail agglomerations decreases either\nwhen shopping costs for consumers decrease or when the strength of\nagglomerative effects increases.\n"
    },
    {
        "paper_id": 2011.06859,
        "authors": "Tina Krell, Fabian Braesemann, Fabian Stephany, Nicolas Friederici,\n  Philip Meier",
        "title": "A Mixed-Method Landscape Analysis of SME-focused B2B Platforms in\n  Germany",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3614485",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Digital platforms offer vast potential for increased value creation and\ninnovation, especially through cross-organizational data sharing. It appears\nthat SMEs in Germany are currently hesitant or unable to create their own\nplatforms. To get a holistic overview of the structure of the German\nSME-focused platform landscape (that is platforms that are led by or targeting\nSMEs), we applied a mixed method approach of traditional desk research and a\nquantitative analysis. The study identified large geographical disparity along\nthe borders of the new and old German federal states, and overall fewer\nplatform ventures by SMEs, rather than large companies and startups. Platform\nventures for SMEs are more likely set up as partnerships. We indicate that high\ncapital intensity might be a reason for that.\n"
    },
    {
        "paper_id": 2011.07161,
        "authors": "Kelton Minor, Andreas Bjerre-Nielsen, Sigga Svala Jonasdottir, Sune\n  Lehmann, Nick Obradovich",
        "title": "Ambient heat and human sleep",
        "comments": "29 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Ambient temperatures are rising globally, with the greatest increases\nrecorded at night. Concurrently, the prevalence of insufficient sleep is\nincreasing in many populations, with substantial costs to human health and\nwell-being. Even though nearly a third of the human lifespan is spent asleep,\nit remains unknown whether temperature and weather impact objective measures of\nsleep in real-world settings, globally. Here we link billions of sleep\nmeasurements from wearable devices comprising over 7 million nighttime sleep\nrecords across 68 countries to local daily meteorological data from 2015 to\n2017. Rising nighttime temperatures shorten within-person sleep duration\nprimarily through delayed onset, increasing the probability of insufficient\nsleep. The effect of temperature on sleep loss is substantially larger for\nresidents from lower income countries and older adults, and females are\naffected more than are males. Nighttime temperature increases inflict the\ngreatest sleep loss during summer and fall months, and we do not find evidence\nof short-term acclimatization. Coupling historical behavioral measurements with\noutput from climate models, we project that climate change will further erode\nhuman sleep, producing substantial geographic inequalities. Our findings have\nsignificant implications for adaptation planning and illuminate a pathway\nthrough which rising temperatures may globally impact public health.\n"
    },
    {
        "paper_id": 2011.07319,
        "authors": "Takayuki Sakuma",
        "title": "Application of deep quantum neural networks to finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent development of quantum computing gives us an opportunity to\nexplore its potential applications to many fields, with the field of finance\nbeing no exception. In this paper, we apply the deep quantum neural network\nproposed by Beer et al. (2020) and discuss such potential in the context of\nsimple experiments such as learning implied volatilities and option prices.\nFurthermore, Greeks such as delta and gamma, which are important measures in\nrisk management, can be computed analytically with the neural network, and our\nnumerical experiments show that the deep quantum neural network is a promising\ntechnique for solving such numerical problems arising in finance efficiently.\n"
    },
    {
        "paper_id": 2011.07326,
        "authors": "Sadasiba Tripathy and Sandhyarani Das",
        "title": "Impact of crop diversification on socio-economic life of tribal farmers:\n  A case study from Eastern ghats of India",
        "comments": "9 pages, 1 figure, submitted to Journal of Contemporary Asia by\n  Taylor & Francis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we investigated impact of crop diversification on\nsocio-economic life of tribal people from eastern ghats of India. We have\nadopted linear regression formalism to check impact of cross diversification.\nWe observe a positive intercept for almost all factors. Coefficient of\ncorrelation is calculated to examine the inter dependence of CDI and our\nvarious individually measured dependent variables. A positive correlation is\nobserved in almost all factors. This study shows that a positive change\noccurred in their social, economic life in the post diversification era.\n"
    },
    {
        "paper_id": 2011.07379,
        "authors": "Akber Datoo and Christopher D. Clack",
        "title": "Smart Close-out Netting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Smart Close-out Netting aims to standardise and automate specific operational\naspects of the legal and regulatory processes of close-out netting for\nprudentially regulated financial institutions. This article provides a review,\nanalysis and perspective of these operational processes, their benefits for\nprudentially regulated trading institutions, their current inefficiencies, and\nthe extent to which they are amenable to standardisation and automation. The\nmain concepts of Smart Close-out Netting are introduced, including the use of a\ncontrolled natural language in legal opinions and the use of a data-driven\nframework during netting determination.\n"
    },
    {
        "paper_id": 2011.0757,
        "authors": "Thomas Guhr and Andreas Schell",
        "title": "Exact Multivariate Amplitude Distributions for Non-Stationary Gaussian\n  or Algebraic Fluctuations of Covariances or Correlations",
        "comments": "39 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1088/1751-8121/abe3c8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex systems are often non-stationary, typical indicators are continuously\nchanging statistical properties of time series. In particular, the correlations\nbetween different time series fluctuate. Models that describe the multivariate\namplitude distributions of such systems are of considerable interest. Extending\nprevious work, we view a set of measured, non-stationary correlation matrices\nas an ensemble for which we set up a random matrix model. We use this ensemble\nto average the stationary multivariate amplitude distributions measured on\nshort time scales and thus obtain for large time scales multivariate amplitude\ndistributions which feature heavy tails. We explicitly work out four cases,\ncombining Gaussian and algebraic distributions. The results are either of\nclosed forms or single integrals. We thus provide, first, explicit multivariate\ndistributions for such non-stationary systems and, second, a tool that\nquantitatively captures the degree of non-stationarity in the correlations.\n"
    },
    {
        "paper_id": 2011.07597,
        "authors": "Tatiana Kozitsina (Babkina), Alexander Chaban, Evgeniya Lukinova,\n  Mikhail Myagkov",
        "title": "The effect of monetary incentives on sociality induced cooperation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines how the group membership fee influences the formation of\ngroups and the cooperation rate within the socialized groups. We found that\nmonetary transactions do not ruin the establishment of social ties and the\nformation of group relations.\n"
    },
    {
        "paper_id": 2011.07655,
        "authors": "Olivier F\\'eron, Peter Tankov, Laura Tinsi",
        "title": "Price formation and optimal trading in intraday electricity markets with\n  a major player",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study price formation in intraday electricity markets in the presence of\nintermittent renewable generation. We consider the setting where a major\nproducer may interact strategically with a large number of small producers.\nUsing stochastic control theory we identify the optimal strategies of agents\nwith market impact and exhibit the Nash equilibrium in closed form in the\nasymptotic framework of mean field games with a major player. This is a\ncompanion paper to [F\\'eron, Tankov, and Tinsi, Price formation and optimal\ntrading in intraday electricity markets, arXiv:2009.04786, 2020], where a\nsimilar model is developed in the setting of identical agents.\n"
    },
    {
        "paper_id": 2011.07809,
        "authors": "Linus Kalvelage, Javier Revilla Diez, Michael Bollig",
        "title": "Do tar roads bring tourism? Growth corridor policy and tourism\n  development in the Zambezi region, Namibia",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1057/s41287-021-00402-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are high aspirations to foster growth in Namibia's Zambezi region via\nthe development of tourism. The Zambezi region is a core element of the\nKavango-Zambezi Transfrontier Conservation Area (KAZA), a mosaic of areas with\nvarying degrees of protection, which is designed to combine nature conservation\nand rural development. These conservation areas serve as a resource base for\nwildlife tourism, and growth corridor policy aims to integrate the region into\ntourism global production networks (GPNs) by means of infrastructure\ndevelopment. Despite the increasing popularity of growth corridors, little is\nknown about the effectiveness of this development strategy at local level. The\nmixed-methods approach reveals that the improvement of infrastructure has led\nto increased tourism in the region. However, the establishment of a territorial\nconservation imaginary that results in the designation of conservation areas is\na necessary precondition for such a development. Despite the far-reaching\nterritorial claims associated with tourism, the benefits for rural residents\nare limited.\n"
    },
    {
        "paper_id": 2011.07871,
        "authors": "Flavio Angelini and Katia Colaneri and Stefano Herzel and Marco\n  Nicolosi",
        "title": "Implicit Incentives for Fund Managers with Partial Information",
        "comments": "19 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal asset allocation problem for a fund manager whose\ncompensation depends on the performance of her portfolio with respect to a\nbenchmark. The objective of the manager is to maximise the expected utility of\nher final wealth. The manager observes the prices but not the values of the\nmarket price of risk that drives the expected returns. The estimates of the\nmarket price of risk get more precise as more observations are available. We\nformulate the problem as an optimization\n  under partial information. The particular structure of the incentives makes\nthe objective function not concave. We solve the problem via the martingale\nmethod and, with a concavification procedure, we obtain the optimal wealth and\nthe investment strategy. A numerical example shows the effect of learning on\nthe optimal strategy.\n"
    },
    {
        "paper_id": 2011.07906,
        "authors": "Hamidreza Arian, Seyed Mohammad Sina Seyfi, Azin Sharifi",
        "title": "Forecasting Probability of Default for Consumer Loan Management with\n  Gaussian Mixture Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Credit scoring is an essential tool used by global financial institutions and\ncredit lenders for financial decision making. In this paper, we introduce a new\nmethod based on Gaussian Mixture Model (GMM) to forecast the probability of\ndefault for individual loan applicants. Clustering similar customers with each\nother, our model associates a probability of being healthy to each group. In\naddition, our GMM-based model probabilistically associates individual samples\nto clusters, and then estimates the probability of default for each individual\nbased on how it relates to GMM clusters. We provide applications for risk\nmanagers and decision makers in banks and non-bank financial institutions to\nmaximize profit and mitigate the expected loss by giving loans to those who\nhave a probability of default below a decision threshold. Our model has a\nnumber of advantages. First, it gives a probabilistic view of credit standing\nfor each individual applicant instead of a binary classification and therefore\nprovides more information for financial decision makers. Second, the expected\nloss on the train set calculated by our GMM-based default probabilities is very\nclose to the actual loss, and third, our approach is computationally efficient.\n"
    },
    {
        "paper_id": 2011.0792,
        "authors": "Oren Barkan, Jonathan Benchimol, Itamar Caspi, Eliya Cohen, Allon\n  Hammer, Noam Koenigstein",
        "title": "Forecasting CPI Inflation Components with Hierarchical Recurrent Neural\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a hierarchical architecture based on Recurrent Neural Networks\n(RNNs) for predicting disaggregated inflation components of the Consumer Price\nIndex (CPI). While the majority of existing research is focused mainly on\npredicting the inflation headline, many economic and financial entities are\nmore interested in its partial disaggregated components. To this end, we\ndeveloped the novel Hierarchical Recurrent Neural Network (HRNN) model that\nutilizes information from higher levels in the CPI hierarchy to improve\npredictions at the more volatile lower levels. Our evaluations, based on a\nlarge data-set from the US CPI-U index, indicate that the HRNN model\nsignificantly outperforms a vast array of well-known inflation prediction\nbaselines.\n"
    },
    {
        "paper_id": 2011.07994,
        "authors": "Seyed Mohammad Sina Seyfi, Azin Sharifi, Hamidreza Arian",
        "title": "Portfolio Risk Measurement Using a Mixture Simulation Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Monte Carlo Approaches for calculating Value-at-Risk (VaR) are powerful tools\nwidely used by financial risk managers across the globe. However, they are time\nconsuming and sometimes inaccurate. In this paper, a fast and accurate Monte\nCarlo algorithm for calculating VaR and ES based on Gaussian Mixture Models is\nintroduced. Gaussian Mixture Models are able to cluster input data with respect\nto market's conditions and therefore no correlation matrices are needed for\nrisk computation. Sampling from each cluster with respect to their weights and\nthen calculating the volatility-adjusted stock returns leads to possible\nscenarios for prices of assets. Our results on a sample of US stocks show that\nthe Gmm-based VaR model is computationally efficient and accurate. From a\nmanagerial perspective, our model can efficiently mimic the turbulent behavior\nof the market. As a result, our VaR measures before, during and after crisis\nperiods realistically reflect the highly non-normal behavior and non-linear\ncorrelation structure of the market.\n"
    },
    {
        "paper_id": 2011.08011,
        "authors": "Sidra Mehtab, Jaydip Sen and Subhasis Dasgupta",
        "title": "Robust Analysis of Stock Price Time Series Using CNN and LSTM-Based Deep\n  Learning Models",
        "comments": "The paper is the accepted version of our work in the 4th IEEE\n  International Conference on Electronics, Communication, and Aerospace\n  Technology (ICECA'20), November 5 - 7, 2020, Coimbatore, INDIA, The paper\n  consists of 10 pages. It contains 12 figures and 8 tables",
        "journal-ref": null,
        "doi": "10.1109/ICECA49313.2020.9297652",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of stock price and stock price movement patterns has always been a\ncritical area of research. While the well-known efficient market hypothesis\nrules out any possibility of accurate prediction of stock prices, there are\nformal propositions in the literature demonstrating accurate modeling of the\npredictive systems that can enable us to predict stock prices with a very high\nlevel of accuracy. In this paper, we present a suite of deep learning-based\nregression models that yields a very high level of accuracy in stock price\nprediction. To build our predictive models, we use the historical stock price\ndata of a well-known company listed in the National Stock Exchange (NSE) of\nIndia during the period December 31, 2012 to January 9, 2015. The stock prices\nare recorded at five minutes intervals of time during each working day in a\nweek. Using these extremely granular stock price data, we build four\nconvolutional neural network (CNN) and five long- and short-term memory\n(LSTM)-based deep learning models for accurate forecasting of the future stock\nprices. We provide detailed results on the forecasting accuracies of all our\nproposed models based on their execution time and their root mean square error\n(RMSE) values.\n"
    },
    {
        "paper_id": 2011.08128,
        "authors": "Marcos Vin\\'icius dos Santos Ara\\'ujo",
        "title": "Aplica\\c{c}\\~ao do Movimento Browniano Geom\\'etrico para Simula\\c{c}\\~ao\n  de Pre\\c{c}os de A\\c{c}\\~oes do \\'Indice Brasileiro de Small Caps",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.21731.99365",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work addressed the use of the geometric Brownian motion to simulate the\nprices of shares listed in the Small Caps index of the Brazilian stock exchange\nB3 (Brazil, Bolsa, Balc\\~ao). The data used refer to the price history from\nJanuary 2016 to December 2018. The price history of 2019 was used to be\ncompared with the simulated prices. The data was imported from the Yahoo\nFinance database using the Python programming language, and the simulations\nwere performed for each stock individually, and for portfolios formed based on\nexpected returns, risk and the Sharpe Index. The results were better for\nportfolios with higher returns, lower risks and higher Sharpe Indexes.\n"
    },
    {
        "paper_id": 2011.08275,
        "authors": "Gunduz Caginalp",
        "title": "Fat tails arise endogenously in asset prices from supply/demand, with or\n  without jump processes",
        "comments": null,
        "journal-ref": "Mathematics 6 4811 4846 2021",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the quotient of Levy processes of jump-diffusion type has a\nfat-tailed distribution. An application is to price theory in economics. We\nshow that fat tails arise endogenously from modeling of price change based on\nan excess demand analysis resulting in a quotient of arbitrarily correlated\ndemand and supply whether or not jump discontinuities are present. The\nassumption is that supply and demand are described by drift terms, Brownian\n(i.e., Gaussian) and compound Poisson jump processes. If $P^{-1}dP/dt$ (the\nrelative price change in an interval $dt$) is given by a suitable function of\nrelative excess demand, $\\left( \\mathcal{D}% -\\mathcal{S}\\right) /\\mathcal{S}$\n(where $\\mathcal{D}$ and $\\mathcal{S}$ are demand and supply), then the\ndistribution has tail behavior $F\\left( x\\right) \\sim x^{-\\zeta}$ for a power\n$\\zeta$ that depends on the function $G$ in $P^{-1}dP/dt=G\\left(\n\\mathcal{D}/\\mathcal{S}\\right) $. For $G\\left( x\\right) \\sim\\left\\vert\nx\\right\\vert ^{1/q}$ one has $\\zeta=q.$ The empirical data for assets typically\nyields a value, $\\zeta\\tilde{=}3,$ or $\\ \\zeta \\in\\left[ 3,5\\right] $ for some\nmarkets.\n  The discrepancy between the empirical result and theory never arises if one\nmodels price dynamics using basic economics methodology, i.e., generalized\nWalrasian adjustment, rather than the usual starting point for classical\nfinance which assumes a normal distribution of price changes. The function $G$\nis deterministic, and can be calibrated with a smaller data set. The results\nestablish a simple link between the decay exponent of the density function and\nthe price adjustment function, a feature that can improve methodology for risk\nassessment.\n  The mathematical results can be applied to other problems involving the\nrelative difference or quotient of Levy processes of jump-diffusion type.\n"
    },
    {
        "paper_id": 2011.08343,
        "authors": "Yuan Hu, Abootaleb Shirvani, W. Brent Lindquist, Frank J. Fabozzi and\n  Svetlozar T. Rachev",
        "title": "Option Pricing Incorporating Factor Dynamics in Complete Markets",
        "comments": "31 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the Donsker-Prokhorov invariance principle we extend the\nKim-Stoyanov-Rachev-Fabozzi option pricing model to allow for variably-spaced\ntrading instances, an important consideration for short-sellers of options.\nApplying the Cherny-Shiryaev-Yor invariance principles, we formulate a new\nbinomial path-dependent pricing model for discrete- and continuous-time\ncomplete markets where the stock price dynamics depends on the log-return\ndynamics of a market influencing factor. In the discrete case, we extend the\nresults of this new approach to a financial market with informed traders\nemploying a statistical arbitrage strategy involving trading of forward\ncontracts. Our findings are illustrated with numerical examples employing US\nfinancial market data. Our work provides further support for the conclusion\nthat any option pricing model must preserve valuable information on the\ninstantaneous mean log-return, the probability of the stock's upturn movement\n(per trading interval), and other market microstructure features.\n"
    },
    {
        "paper_id": 2011.08531,
        "authors": "Takanori Adachi, Katsushi Nakajima, Yoshihiro Ryu",
        "title": "Generalized Filtrations and Its Application to Binomial Asset Pricing\n  Models",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce generalized filtration with which we can represent situations\nsuch as some agents forget information at some specific time. The filtration is\ndefined as a functor to a category Prob whose objects are all probability\nspaces and whose arrows correspond to measurable functions satisfying an\nabsolutely continuous requirement [Adachi and Ryu, 2019]. As an application of\na generalized filtration, we develop a binomial asset pricing model, and\ninvestigate the valuations of financial claims along this type of non-standard\nfiltrations.\n"
    },
    {
        "paper_id": 2011.08553,
        "authors": "Vineeth S. Varma, Irinel-Constantin Morarescu, Samson Lasaulce and\n  Samuel Martin",
        "title": "Marketing resource allocation in duopolies over social networks",
        "comments": "IEEE Control Systems Letters (L-CSS), Vol. 2, No. 4, pp. 593-598,\n  June 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  One of the key features of this paper is that the agents' opinion of a social\nnetwork is assumed to be not only influenced by the other agents but also by\ntwo marketers in competition. One of our contributions is to propose a\npragmatic game-theoretical formulation of the problem and to conduct the\ncomplete corresponding equilibrium analysis (existence, uniqueness, dynamic\ncharacterization, and determination). Our analysis provides practical insights\nto know how a marketer should exploit its knowledge about the social network to\nallocate its marketing or advertising budget among the agents (who are the\nconsumers). By providing relevant definitions for the agent influence power\n(AIP) and the gain of targeting (GoT), the benefit of using a smart budget\nallocation policy instead of a uniform one is assessed and operating conditions\nunder which it is potentially high are identified.\n"
    },
    {
        "paper_id": 2011.0862,
        "authors": "Javier Pantoja Robayo (1), Juan C. Vera (2) ((1) School of Economics\n  and Finance, Universidad EAFIT. Medellin, Colombia. (2) Tilburg School of\n  Economics and Management, Tilburg University, The Netherlands)",
        "title": "Static Hedging of Weather and Price Risks in Electricity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present the closed-form solution to the problem of hedging price and\nquantity risks for energy retailers (ER), using financial instruments based on\nelectricity price and weather indexes. Our model considers an ER who is\nintermediary in a regulated electricity market. ERs buy a fixed quantity of\nelectricity at a variable cost and must serve a variable demand at a fixed\ncost. Thus ERs are subject to both price and quantity risks. To hedge such\nrisks, an ER could construct a portfolio of financial instruments based on\nprice and weather indexes. We construct the closed form solution for the\noptimal portfolio for the mean-Var model in the discrete setting. Our model\ndoes not make any distributional assumption.\n"
    },
    {
        "paper_id": 2011.08639,
        "authors": "I. C. Morarescu, V.S. Varma, L. Busoniu and S. Lasaulce",
        "title": "Space-time budget allocation policy design for viral marketing",
        "comments": "Journal on Nonlinear Analysis: Hybrid Systems (NAHS), Vol. 37, Aug.\n  2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We address formally the problem of opinion dynamics when the agents of a\nsocial network (e.g., consumers) are not only influenced by their neighbors but\nalso by an external influential entity referred to as a marketer. The\ninfluential entity tries to sway the overall opinion as close as possible to a\ndesired opinion by using a specific influence budget. We assume that the\nexogenous influences of the entity happen during discrete-time advertising\ncampaigns; consequently, the overall closed-loop opinion dynamics becomes a\nlinear-impulsive (hybrid) one. The main technical issue addressed is finding\nhow the marketer should allocate its budget over time (through marketing\ncampaigns) and over space (among the agents) such that the agents' opinion be\nas close as possible to the desired opinion. Our main results show that the\nmarketer has to prioritize certain agents over others based on their initial\ncondition, their influence power in the social graph and the size of the\ncluster they belong to. The corresponding space-time allocation problem is\nformulated and solved for several special cases of practical interest. Valuable\ninsights can be extracted from our analysis. For instance, for most cases, we\nprove that the marketer has an interest in investing most of its budget at the\nbeginning of the process and that budget should be shared among agents\naccording to the famous water-filling allocation rule. Numerical examples\nillustrate the analysis.\n"
    },
    {
        "paper_id": 2011.08717,
        "authors": "Rahul Goel, Lucas Javier Ford, Maksym Obrizan and Rajesh Sharma",
        "title": "COVID-19 and the stock market: evidence from Twitter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  COVID-19 has had a much larger impact on the financial markets compared to\nprevious epidemics because the news information is transferred over the social\nnetworks at a speed of light. Using Twitter's API, we compiled a unique dataset\nwith more than 26 million COVID-19 related Tweets collected from February 2nd\nuntil May 1st, 2020. We find that more frequent use of the word \"stock\" in\ndaily Tweets is associated with a substantial decline in log returns of three\nkey US indices - Dow Jones Industrial Average, S&P500, and NASDAQ. The results\nremain virtually unchanged in multiple robustness checks.\n"
    },
    {
        "paper_id": 2011.08721,
        "authors": "Dharani Dhar Burra, Sriganesh Lokanathan",
        "title": "Assessing the use of transaction and location based insights derived\n  from Automatic Teller Machines (ATMs) as near real time sensing systems of\n  economic shocks",
        "comments": "Presented at NeurIPS 2020 Workshop on Machine Learning for the\n  Developing World",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Big data sources provide a significant opportunity for governments and\ndevelopment stakeholders to sense and identify in near real time, economic\nimpacts of shocks on populations at high spatial and temporal resolutions. In\nthis study, we assess the potential of transaction and location based measures\nobtained from automatic teller machine (ATM) terminals, belonging to a major\nprivate sector bank in Indonesia, to sense in near real time, the impacts of\nshocks across income groups. For each customer and separately for years 2014\nand 2015, we model the relationship between aggregate measures of cash\nwithdrawals for each year, total inter-terminal distance traversed by the\ncustomer for the specific year and reported customer income group. Results\nreveal that the model was able to predict the corresponding income groups with\n80% accuracy, with high precision and recall values in comparison to the\nbaseline model, across both the years. Shapley values suggest that the total\ninter-terminal distance traversed by a customer in each year differed\nsignificantly between customer income groups. Kruskal-Wallis test further\nshowed that customers in the lower-middle class income group, have\nsignificantly high median values of inter-terminal distances traversed (7.21\nKms for 2014 and 2015) in comparison to high (2.55 Kms and 0.66 Kms for years\n2014 and 2015), and low (6.47 Kms for 2014 and 2015) income groups. Although no\nmajor shocks were noted in 2014 and 2015, our results show that lower-middle\nclass income group customers, exhibit relatively high mobility in comparison to\ncustomers in low and high income groups. Additional work is needed to leverage\nthe sensing capabilities of this data to provide insights on, who, where and by\nhow much is the population impacted by a shock to facilitate targeted\nresponses.\n"
    },
    {
        "paper_id": 2011.09003,
        "authors": "Yifan Yu, Shan Huang, Yuchen Liu, Yong Tan",
        "title": "Emotions in Online Content Diffusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Social media-transmitted online information, which is associated with\nemotional expressions, shapes our thoughts and actions. In this study, we\nincorporate social network theories and analyses and use a computational\napproach to investigate how emotional expressions, particularly\n\\textit{negative discrete emotional expressions} (i.e., anxiety, sadness,\nanger, and disgust), lead to differential diffusion of online content in social\nmedia networks. We rigorously quantify diffusion cascades' structural\nproperties (i.e., size, depth, maximum breadth, and structural virality) and\nanalyze the individual characteristics (i.e., age, gender, and network degree)\nand social ties (i.e., strong and weak) involved in the cascading process. In\nour sample, more than six million unique individuals transmitted 387,486\nrandomly selected articles in a massive-scale online social network, WeChat. We\ndetect the expression of discrete emotions embedded in these articles, using a\nnewly generated domain-specific and up-to-date emotion lexicon. We apply a\npartial-linear instrumental variable approach with a double machine learning\nframework to causally identify the impact of the negative discrete emotions on\nonline content diffusion. We find that articles with more expressions of\nanxiety spread to a larger number of individuals and diffuse more deeply,\nbroadly, and virally. Expressions of anger and sadness, however, reduce\ncascades' size and maximum breadth. We further show that the articles with\ndifferent degrees of negative emotional expressions tend to spread differently\nbased on individual characteristics and social ties. Our results shed light on\ncontent marketing and regulation, utilizing negative emotional expressions.\n"
    },
    {
        "paper_id": 2011.09109,
        "authors": "Atul Deshpande, John A Gubner and B. Ross Barmish",
        "title": "On Simultaneous Long-Short Stock Trading Controllers with Cross-Coupling",
        "comments": "Presented at IFAC World Congress, 2020. Will appear in\n  IFAC-PapersOnline",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Simultaneous Long-Short(SLS) controller for trading a single stock is\nknown to guarantee positive expected value of the resulting gain-loss function\nwith respect to a large class of stock price dynamics. In the literature, this\nis known as the Robust Positive Expectation(RPE)property. An obvious way to\nextend this theory to the trading of two stocks is to trade each one of them\nusing its own independent SLS controller. Motivated by the fact that such a\nscheme does not exploit any correlation between the two stocks, we study the\ncase when the relative sign between the drifts of the two stocks is known. The\nmain contributions of this paper are three-fold: First, we put forward a novel\narchitecture in which we cross-couple two SLS controllers for the two-stock\ncase. Second, we derive a closed-form expression for the expected value of the\ngain-loss function. Third, we use this closed-form expression to prove that the\nRPE property is guaranteed with respect to a large class of stock-price\ndynamics. When more information over and above the relative sign is assumed,\nadditional benefits of the new architecture are seen. For example, when bounds\nor precise values for the means and covariances of the stock returns are\nincluded in the model, numerical simulations suggest that our new controller\ncan achieve lower trading risk than a pair of decoupled SLS controllers for the\nsame level of expected trading gain.\n"
    },
    {
        "paper_id": 2011.09129,
        "authors": "Lan Ju, Zhiyong Tu, Changyong Xue",
        "title": "Pricing the Information Quantity in Artworks",
        "comments": "arXiv admin note: text overlap with arXiv:1910.03800",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the traditional art pricing models, the variables that capture the\npainting's content are often missing. Recent research starts to apply the\ncomputer graphic techniques to extract the information from the painting\ncontent. Most of the research concentrates on the reading of the color\ninformation from the painting images and analyzes how different color\ncompositions can affect the sales prices of paintings. This paper takes a\ndifferent approach, and tries to abstract away from the interpretation of the\ncontent information, while only focus on measuring the quantity of information\ncontained. We extend the concept of Shannon entropy in information theory to\nthe painting's scenario, and suggest using the variances of a painting's\ncomposing elements, i.e., line, color, value, shape/form and space, to measure\nthe amount of information in the painting. These measures are calculated at the\npixel level based on a picture's digital image. We include them into the\ntraditional hedonic regression model to test their significance based on the\nauction samples from two famous artists (Picasso and Renoir). We find that all\nthe variance measurements can significantly explain the sales price either at\n1% or 5% level. The adjusted R square is also increased by more than ten\npercent. Our method greatly improves the traditional pricing models, and may\nalso find applications in other areas such as art valuation and authentication.\n"
    },
    {
        "paper_id": 2011.09137,
        "authors": "Shenghuan Yang, lonut Florescu, Md Tariqul Islam",
        "title": "Principal Component Analysis and Factor Analysis for Feature Selection\n  in Credit Rating",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The credit rating is an evaluation of a company's credit risk that values the\nability to pay back the debt and predict the likelihood of the debtor\ndefaulting. There are various features influencing credit rating. Therefore, it\nis essential to select substantive features to explore the main reason for\ncredit rating change. To address this issue, this paper exploited Principal\nComponent Analysis and Factor Analysis as feature selection algorithms to\nselect important features, summarized the similar features together, and\nobtained a minimum set of features for four sectors, Financial Sector, Energy\nSector, Health Care Sector, Consumer Discretionary Sector. This paper used two\ndata sets, Financial Ratio and Balance Sheet, with two mappings, Detailed\nMapping, and Coarse Mapping, converting the target variable(credit rating) into\ncategorical variable. To test the accuracy of credit rating prediction, Random\nForest Classifier was used to test and train feature sets. The results showed\nthat the accuracy of Financial Ratio feature sets was higher than that of\nBalance Sheet feature sets. In addition, Factor Analysis can reduce the number\nof features significantly to obtain almost the same accuracy that can decrease\ndramatically the time spent on analyzing data; we also summarized seven\ndominant factors and ten dominant factors affecting credit rating change in\nFinancial Ratio and Balance Sheet by utilizing Factor Analysis, respectively,\nwhich can explain the reason of credit rating change better.\n"
    },
    {
        "paper_id": 2011.09147,
        "authors": "Nicola Cufaro Petroni, Piergiacomo Sabino",
        "title": "Tempered stable distributions and finite variation Ornstein-Uhlenbeck\n  processes",
        "comments": "28 pages, 3 Figure, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Constructing \\Levy-driven Ornstein-Uhlenbeck processes is a task closely\nrelated to the notion of self-decomposability. In particular, their transition\nlaws are linked to the properties of what will be hereafter called the\n\\emph{a-reminder} of their self-decomposable stationary laws. In the present\nstudy we fully characterize the L\\'evy triplet of these a-reminder s and we\nprovide a general framework to deduce the transition laws of the finite\nvariation Ornstein-Uhlenbeck processes associated with tempered stable\ndistributions. We focus finally on the subclass of the exponentially-modulated\ntempered stable laws and we derive the algorithms for an exact generation of\nthe skeleton of Ornstein-Uhlenbeck processes related to such distributions,\nwith the further advantage of adopting a procedure computationally more\nefficient than those already available in the existing literature.\n"
    },
    {
        "paper_id": 2011.09189,
        "authors": "Patrick Mellacher",
        "title": "Cooperation in the Age of COVID-19: Evidence from Public Goods Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Does COVID-19 change the willingness to cooperate? Four Austrian university\ncourses in economics play a public goods game in consecutive semesters on the\ne-learning platform Moodle: two of them in the year before the crisis, one\nimmediately after the beginning of the first lockdown in March 2020 and the\nlast one in the days before the announcement of the second lockdown in October\n2020. Between 67% and 76% of the students choose to cooperate, i.e. contribute\nto the public good, in the pre-crisis year. Immediately after the imposition of\nthe lockdown, 71% choose to cooperate. Seven months into the crisis, however,\ncooperation drops to 43%. Depending on whether two types of biases resulting\nfrom the experimental design are eliminated or not, probit and logit\nregressions show that this drop is statistically significant at the 0.05 or the\n0.1 significance level.\n"
    },
    {
        "paper_id": 2011.09226,
        "authors": "Shige Peng and Shuzhen Yang",
        "title": "Distributional uncertainty of the financial time series measured by\n  G-expectation",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on law of large numbers and central limit theorem under nonlinear\nexpectation, we introduce a new method of using G-normal distribution to\nmeasure financial risks. Applying max-mean estimators and small windows method,\nwe establish autoregressive models to determine the parameters of G-normal\ndistribution, i.e., the return, maximal and minimal volatilities of the time\nseries. Utilizing the value at risk (VaR) predictor model under G-normal\ndistribution, we show that the G-VaR model gives an excellent performance in\npredicting the VaR for a benchmark dataset comparing to many well-known VaR\npredictors.\n"
    },
    {
        "paper_id": 2011.09248,
        "authors": "Fabio Baione, Davide Biancalana, Paolo De Angelis",
        "title": "An application of Zero-One Inflated Beta regression models for\n  predicting health insurance reimbursement",
        "comments": "6 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In actuarial practice the dependency between contract limitations\n(deductibles, copayments) and health care expenditures are measured by the\napplication of the Monte Carlo simulation technique. We propose, for the same\ngoal, an alternative approach based on Generalized Linear Model for Location,\nScale and Shape (GAMLSS). We focus on the estimate of the ratio between the\none-year reimbursement amount (after the effect of limitations) and the one\nyear expenditure (before the effect of limitations). We suggest a regressive\nmodel to investigate the relation between this response variable and a set of\ncovariates, such as limitations and other rating factors related to health\nrisk. In this way a dependency structure between reimbursement and limitations\nis provided. The density function of the ratio is a mixture distribution,\nindeed it can continuously assume values mass at 0 and 1, in addition to the\nprobability density within (0, 1) . This random variable does not belong to the\nexponential family, then an ordinary Generalized Linear Model is not suitable.\nGAMLSS introduces a probability structure compliant with the density of the\nresponse variable, in particular zero-one inflated beta density is assumed. The\nlatter is a mixture between a Bernoulli distribution and a Beta distribution.\n"
    },
    {
        "paper_id": 2011.09254,
        "authors": "Fabio Baione, Davide Biancalana, Paolo De Angelis",
        "title": "A Risk Based approach for the Solvency Capital requirement for Health\n  Plans",
        "comments": "6 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The study deals with the assessment of risk measures for Health Plans in\norder to assess the Solvency Capital Requirement. For the estimation of the\nindividual health care expenditure for several episode types, we suggest an\noriginal approach based on a three-part regression model. We propose three\nGeneralized Linear Models (GLM) to assess claim counts, the allocation of each\nclaim to a specific episode and the severity average expenditures respectively.\nOne of the main practical advantages of our proposal is the reduction of the\nregression models compared to a traditional approach, where several two-part\nmodels for each episode types are requested. As most health plans require\nco-payments or co-insurance, considering at this stage the non-linearity\ncondition of the reimbursement function, we adopt a Montecarlo simulation to\nassess the health plan costs. The simulation approach provides the probability\ndistribution of the Net Asset Value of the Health Plan and the estimate of\nseveral risk measures.\n"
    },
    {
        "paper_id": 2011.09607,
        "authors": "Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang,\n  Bowen Xiao, Christina Dan Wang",
        "title": "FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading\n  in Quantitative Finance",
        "comments": "Deep Reinforcement Learning Workshop, 34th Conference on Neural\n  Information Processing Systems (NeurIPS2020), Vancouver, Canada",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As deep reinforcement learning (DRL) has been recognized as an effective\napproach in quantitative finance, getting hands-on experiences is attractive to\nbeginners. However, to train a practical DRL trading agent that decides where\nto trade, at what price, and what quantity involves error-prone and arduous\ndevelopment and debugging. In this paper, we introduce a DRL library FinRL that\nfacilitates beginners to expose themselves to quantitative finance and to\ndevelop their own stock trading strategies. Along with easily-reproducible\ntutorials, FinRL library allows users to streamline their own developments and\nto compare with existing schemes easily. Within FinRL, virtual environments are\nconfigured with stock market datasets, trading agents are trained with neural\nnetworks, and extensive backtesting is analyzed via trading performance.\nMoreover, it incorporates important trading constraints such as transaction\ncost, market liquidity and the investor's degree of risk-aversion. FinRL is\nfeatured with completeness, hands-on tutorial and reproducibility that favors\nbeginners: (i) at multiple levels of time granularity, FinRL simulates trading\nenvironments across various stock markets, including NASDAQ-100, DJIA, S&P 500,\nHSI, SSE 50, and CSI 300; (ii) organized in a layered architecture with modular\nstructure, FinRL provides fine-tuned state-of-the-art DRL algorithms (DQN,\nDDPG, PPO, SAC, A2C, TD3, etc.), commonly-used reward functions and standard\nevaluation baselines to alleviate the debugging workloads and promote the\nreproducibility, and (iii) being highly extendable, FinRL reserves a complete\nset of user-import interfaces. Furthermore, we incorporated three application\ndemonstrations, namely single stock trading, multiple stock trading, and\nportfolio allocation. The FinRL library will be available on Github at link\nhttps://github.com/AI4Finance-LLC/FinRL-Library.\n"
    },
    {
        "paper_id": 2011.0964,
        "authors": "Bo\\u{g}a\\c{c}han \\c{C}elen (1), Sen Geng (2), Huihui Li (2) ((1)\n  University of Melbourne, (2) Xiamen University)",
        "title": "Belief Error and Non-Bayesian Social Learning: Experimental Evidence",
        "comments": "45 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper experimentally studies whether individuals hold a first-order\nbelief that others apply Bayes' rule to incorporate private information into\ntheir beliefs, which is a fundamental assumption in many Bayesian and\nnon-Bayesian social learning models. We design a novel experimental setting in\nwhich the first-order belief assumption implies that social information is\nequivalent to private information. Our main finding is that participants'\nreported reservation prices of social information are significantly lower than\nthose of private information, which provides evidence that casts doubt on the\nfirst-order belief assumption. We also build a novel belief error model in\nwhich participants form a random posterior belief with a Bayesian posterior\nbelief kernel to explain the experimental findings. A structural estimation of\nthe model suggests that participants' sophisticated consideration of others'\nbelief error and their exaggeration of the error both contribute to the\ndifference in reservation prices.\n"
    },
    {
        "paper_id": 2011.10101,
        "authors": "Zehra Eksi and Damir Filipovi\\'c",
        "title": "Affine Pricing and Hedging of Collateralized Debt Obligations",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study deals with the pricing and hedging of single-tranche\ncollateralized debt obligations (STCDOs). We specify an affine two-factor model\nin which a catastrophic risk component is incorporated. Apart from being\nanalytically tractable, this model has the feature that it captures the\ndynamics of super-senior tranches, thanks to the catastrophic component. We\nestimate the factor model based on the iTraxx Europe data with six tranches and\nfour different maturities, using a quasi-maximum likelihood (QML) approach in\nconjunction with the Kalman filter. We derive the model-based\nvariance-minimizing strategy for the hedging of STCDOs with a dynamically\nrebalanced portfolio on the underlying swap index. We analyze the actual\nperformance of the variance-minimizing hedge on the iTraxx Europe data. In\norder to assess the hedging performance further, we run a simulation analysis\nwhere normal and extreme loss scenarios are generated via the method of\nimportance sampling. Both in-sample hedging and simulation analysis suggest\nthat the variance-minimizing strategy is most effective for mezzanine tranches\nin terms of yielding less riskier hedging portfolios and it fails to provide\nadequate hedge performance regarding equity tranches.\n"
    },
    {
        "paper_id": 2011.10113,
        "authors": "Damiano Brigo and Federico Graceffa and Eyal Neuman",
        "title": "Price Impact on Term Structure",
        "comments": "49 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a first theory of price impact in presence of an interest-rates\nterm structure. We explain how one can formulate instantaneous and transient\nprice impact on bonds with different maturities, including a cross price impact\nthat is endogenous to the term structure. We connect the introduced impact to\nclassic no-arbitrage theory for interest rate markets, showing that impact can\nbe embedded in the pricing measure and that no-arbitrage can be preserved. We\npresent pricing examples in presence of price impact and numerical examples of\nhow impact changes the shape of the term structure. Finally, to show that our\napproach is applicable we solve an optimal execution problem in interest rate\nmarkets with the type of price impact we developed in the paper.\n"
    },
    {
        "paper_id": 2011.10166,
        "authors": "Guohui Guan, Qitao Huang, Zongxia Liang, Fengyi Yuan",
        "title": "Retirement decision with addictive habit persistence in a jump diffusion\n  market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the optimal retirement decision, investment, and\nconsumption strategies in a market with jump diffusion, taking into account\nhabit persistence and stock-wage correlation. Our analysis considers multiple\nstocks and a finite time framework, intending to determine the retirement\nboundary of the ``wealth-habit-wage\" triplet $(x, h, w)$. To achieve this, we\nuse the habit reduction method and a duality approach to obtain the retirement\nboundary of the primal variables and feedback forms of optimal strategies. {\nWhen dealing with the dual problem, we address technical challenges in the\nproof of integral equation characterization of optimal retirement boundary\nusing a $C^1$ version of It$\\hat{\\rm o}$'s formula.} Our results show that when\nthe so-called ``de facto wealth\" exceeds a critical proportion of wage, an\nimmediate retirement is the optimal choice for the agent. Additionally, we find\nthat the introduction of jump risks allows for the possibility of discontinuous\ninvestment strategies within the working region, which is a novel and\ninsightful finding. Our numerical results effectively illustrate these findings\nby varying the parameters.\n"
    },
    {
        "paper_id": 2011.10242,
        "authors": "Michele Vodret, Iacopo Mastromatteo, Bence T\\'oth and Michael\n  Benzaquen",
        "title": "A Stationary Kyle Setup: Microfounding propagator models",
        "comments": "25 pages, 8 figures, added references",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/abe702",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an economically sound micro-foundation to linear price impact\nmodels, by deriving them as the equilibrium of a suitable agent-based system.\nOur setup generalizes the well-known Kyle model, by dropping the assumption of\na terminal time at which fundamental information is revealed so to describe a\nstationary market, while retaining agents' rationality and asymmetric\ninformation. We investigate the stationary equilibrium for arbitrary Gaussian\nnoise trades and fundamental information, and show that the setup is compatible\nwith universal price diffusion at small times, and non-universal mean-reversion\nat time scales at which fluctuations in fundamentals decay. Our model provides\na testable relation between volatility of prices, magnitude of fluctuations in\nfundamentals and level of volume traded in the market.\n"
    },
    {
        "paper_id": 2011.103,
        "authors": "Ben Hambly, Renyuan Xu and Huining Yang",
        "title": "Policy Gradient Methods for the Noisy Linear Quadratic Regulator over a\n  Finite Horizon",
        "comments": "49 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore reinforcement learning methods for finding the optimal policy in\nthe linear quadratic regulator (LQR) problem. In particular, we consider the\nconvergence of policy gradient methods in the setting of known and unknown\nparameters. We are able to produce a global linear convergence guarantee for\nthis approach in the setting of finite time horizon and stochastic state\ndynamics under weak assumptions. The convergence of a projected policy gradient\nmethod is also established in order to handle problems with constraints. We\nillustrate the performance of the algorithm with two examples. The first\nexample is the optimal liquidation of a holding in an asset. We show results\nfor the case where we assume a model for the underlying dynamics and where we\napply the method to the data directly. The empirical evidence suggests that the\npolicy gradient method can learn the global optimal solution for a larger class\nof stochastic systems containing the LQR framework and that it is more robust\nwith respect to model mis-specification when compared to a model-based\napproach. The second example is an LQR system in a higher dimensional setting\nwith synthetic data.\n"
    },
    {
        "paper_id": 2011.10384,
        "authors": "Alvaro Gonzalez-Castellanos (1), David Pozo (1), Aldo Bischi (1) ((1)\n  Skolkovo Institute of Science and Technology)",
        "title": "Pricing in Integrated Heat and Power Markets",
        "comments": "Presented at the conference MEDPOWER 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is a growing interest in the integration of energy infrastructures to\nincrease systems' flexibility and reduce operational costs. The most studied\ncase is the synergy between electric and heating networks. Even though\nintegrated heat and power markets can be described by a convex optimization\nproblem, prices derived from dual values do not guarantee cost recovery. In\nthis work, a two-step approach is presented for the calculation of the optimal\nenergy dispatch and prices. The proposed methodology guarantees cost-recovery\nfor each of the energy vectors and revenue-adequacy for the integrated market.\n"
    },
    {
        "paper_id": 2011.10485,
        "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer",
        "title": "Sequential Defaulting in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider financial networks, where banks are connected by contracts such\nas debts or credit default swaps. We study the clearing problem in these\nsystems: we want to know which banks end up in a default, and what portion of\ntheir liabilities can these defaulting banks fulfill. We analyze these networks\nin a sequential model where banks announce their default one at a time, and the\nsystem evolves in a step-by-step manner.\n  We first consider the reversible model of these systems, where banks may\nreturn from a default. We show that the stabilization time in this model can\nheavily depend on the ordering of announcements. However, we also show that\nthere are systems where for any choice of ordering, the process lasts for an\nexponential number of steps before an eventual stabilization. We also show that\nfinding the ordering with the smallest (or largest) number of banks ending up\nin default is an NP-hard problem. Furthermore, we prove that defaulting early\ncan be an advantageous strategy for banks in some cases, and in general,\nfinding the best time for a default announcement is NP-hard. Finally, we\ndiscuss how changing some properties of this setting affects the stabilization\ntime of the process, and then use these techniques to devise a monotone model\nof the systems, which ensures that every network stabilizes eventually.\n"
    },
    {
        "paper_id": 2011.10509,
        "authors": "Melvyn Weeks and Tobias Gabel Christiansen",
        "title": "Understanding the Distributional Aspects of Microcredit Expansions",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Various poverty reduction strategies are being implemented in the pursuit of\neliminating extreme poverty. One such strategy is increased access to\nmicrocredit in poor areas around the world. Microcredit, typically defined as\nthe supply of small loans to underserved entrepreneurs that originally aimed at\ndisplacing expensive local money-lenders, has been both praised and criticized\nas a development tool (Banerjee et al., 2015b). This paper presents an analysis\nof heterogeneous impacts from increased access to microcredit using data from\nthree randomised trials. In the spirit of recognising that in general the\nimpact of a policy intervention varies conditional on an unknown set of\nfactors, particular, we investigate whether heterogeneity presents itself as\ngroups of winners and losers, and whether such subgroups share characteristics\nacross RCTs. We find no evidence of impacts, neither average nor\ndistributional, from increased access to microcredit on consumption levels. In\ncontrast, the lack of average effects on profits seems to mask heterogeneous\nimpacts. The findings are, however, not robust to the specific machine learning\nalgorithm applied. Switching from the better performing Elastic Net to the\nworse performing Random Forest leads to a sharp increase in the variance of the\nestimates. In this context, methods to evaluate the relative performing machine\nlearning algorithm developed by Chernozhukov et al. (2019) provide a\ndisciplined way for the analyst to counter the uncertainty as to which\nalgorithm to deploy.\n"
    },
    {
        "paper_id": 2011.1063,
        "authors": "Marc Sabate-Vidales and David \\v{S}i\\v{s}ka and Lukasz Szpruch",
        "title": "Solving path dependent PDEs with LSTM networks and path signatures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a combination of recurrent neural networks and signature methods from\nthe rough paths theory we design efficient algorithms for solving parametric\nfamilies of path dependent partial differential equations (PPDEs) that arise in\npricing and hedging of path-dependent derivatives or from use of non-Markovian\nmodel, such as rough volatility models in Jacquier and Oumgari, 2019. The\nsolutions of PPDEs are functions of time, a continuous path (the asset price\nhistory) and model parameters. As the domain of the solution is infinite\ndimensional many recently developed deep learning techniques for solving PDEs\ndo not apply. Similarly as in Vidales et al. 2018, we identify the objective\nfunction used to learn the PPDE by using martingale representation theorem. As\na result we can de-bias and provide confidence intervals for then neural\nnetwork-based algorithm. We validate our algorithm using classical models for\npricing lookback and auto-callable options and report errors for approximating\nboth prices and hedging strategies.\n"
    },
    {
        "paper_id": 2011.10747,
        "authors": "Mengjin Zhao and Guangyan Jia",
        "title": "Continuous-Time Risk Contribution of the Terminal Variance and its\n  Related Risk Budgeting Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To achieve robustness of risk across different assets, risk parity investing\nrules, a particular state of risk contributions, have grown in popularity over\nthe previous few decades. To generalize the concept of risk contribution from\nthe simple covariance matrix case to the continuous-time case in which the\nterminal variance of wealth is used as the risk measure, we characterize risk\ncontributions and marginal risk contributions on various assets as predictable\nprocesses using the Gateaux differential and Doleans measure. Meanwhile, the\nrisk contributions we extend here have the aggregation property, namely that\ntotal risk can be represented as the aggregation of those among different\nassets and $(t,\\omega)$. Subsequently, as an inverse target -- allocating risk,\nthe risk budgeting problem of how to obtain policies whose risk contributions\ncoincide with pre-given risk budgets in the continuous-time case is also\nexplored in this paper. These policies are solutions to stochastic convex\noptimizations parametrized by the pre-given risk budgets. Moreover,\nsingle-period risk budgeting policies are explained as the projection of risk\nbudgeting policies in continuous-time cases. On the application side,\nvolatility-managed portfolios in [Moreira and Muir,2017] can be obtained by\nrisk budgeting optimization; similarly to previous findings, continuous-time\nmean-variance allocation in [Zhou and Li, 2000] appears to be concentrated in\nterms of risk contribution.\n"
    },
    {
        "paper_id": 2011.10826,
        "authors": "Viktor Stojkoski, Petar Jolakoski and Igor Ivanovski",
        "title": "The short-run impact of COVID-19 on the activity in the insurance\n  industry in the Republic of North Macedonia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the impact of the COVID-19 pandemic on the insurance\nindustry in the Republic of North Macedonia during the first half of 2020. By\nutilizing seasonal autoregressive models and data for 11 insurance classes, we\nfind that the insurance activity shrank by more than 10% compared to what was\nexpected. The total loss in the industry was, however, much less than the\namount of funds made available by the Insurance Supervision Agency. This was\nbecause the pandemic induced changes in the activity structure - the share of\nMotor vehicles class fell at the expense of the property classes.\n"
    },
    {
        "paper_id": 2011.1093,
        "authors": "Matthew Brigida",
        "title": "Real-Time Detection of Volatility in Liquidity Provision",
        "comments": "16 pages, 6 figures. Forthcoming in Applied Finance Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Previous research has found that high-frequency traders will vary the bid or\noffer price rapidly over periods of milliseconds. This is a benefit to fast\ntraders who can time their trades with microsecond precision, however it is a\ncost to the average market participant due to increased trade execution price\nuncertainty. In this analysis we attempt to construct real-time methods for\ndetermining whether the liquidity of a security is being altered rapidly. We\nfind a four-state Markov switching model identifies a state where liquidity is\nbeing rapidly varied about a mean value. This state can be used to generate a\nsignal to delay market participant orders until the price volatility subsides.\nOver our sample, the signal would delay orders, in aggregate, over 0 to 10% of\nthe trading day. Each individual delay would only last tens of milliseconds,\nand so would not be noticeable by the average market participant.\n"
    },
    {
        "paper_id": 2011.10957,
        "authors": "Suryo Budi Santoso and Herni Justiana Astuti",
        "title": "The power of islamic scholars lecture to decide using Islamic bank with\n  customer response strength approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of this study is to analyze the power of Islamic scholars\nlectures to decide using Islamic banks with a customer response strength\napproach. The sampling technique uses purposive sampling, and the respondents\nwere those who attended lectures on Islamic banks delivered by Islamic\nscholars. The number of respondents who met the requirements was 96\nrespondents. Data were analyzed using the customer response strength method.\nThe instrument has met the valid and reliable criteria. The results showed 99%\nof the total number of respondents acted according to their perceptions of the\ncontents of Islamic banks lectures. Lecture material delivered by scholars\nabout Islamic banks has a strong relationship with their responses ranging from\ngiving attention, interest, fostering desires, and beliefs to having an\ninterest in making transactions with Islamic banks.\n"
    },
    {
        "paper_id": 2011.10958,
        "authors": "Suryo Budi Santoso and Herni Justiana Astuti",
        "title": "A Framework for Conceptualizing Islamic Bank Socialization in Indonesia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of this study is the design model of Islamic bank socialization\nin terms of four pillars (Business Institution, Formal Education, Islamic\nScholar and Higher Education) through Synergy and Proactive. The location of\nthe study was conducted in the Regency of Banyumas, Indonesia. The results of\nthe survey on respondents obtained 145 respondents' answers that deserve to be\nanalyzed. Data were analyzed using SEM models with Partial Least Squares\napproach, designing measurement models (outer models) and designing inner\nmodels. The results of the calculation outside the model of all measurements\nare more than the minimum criteria required by removing Formal Education from\nthe model because it does not meet the requirements. While the inner model\nresults show that the socialization model was only built by the Business\nInstitution, Islamic Scholar and Higher Education through synergy and\nproactivity. All independent variables directly influence the dependent\nvariable, while the intervening variables also significantly influence except\nthe relationship of Islamic Scholar Islamic to Bank Socialization through\nProactive.\n"
    },
    {
        "paper_id": 2011.10966,
        "authors": "Shuzhen Yang",
        "title": "Discrete time multi-period mean-variance model: Bellman type strategy\n  and Empirical analysis",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we attempt to introduce the Bellman principle for a discrete\ntime multi-period mean-variance model. Based on this new take on the Bellman\nprinciple, we obtain a dynamic time-consistent optimal strategy and related\nefficient frontier. Furthermore, we develop a varying investment period\ndiscrete time multi-period mean-variance model and obtain a related dynamic\noptimal strategy and an optimal investment period. This paper compares the\nhighlighted dynamic optimal strategies of this study with the 1/n equality\nstrategy, and shows that we can secure a higher return with a smaller risk\nbased on the dynamic optimal strategies.\n"
    },
    {
        "paper_id": 2011.11017,
        "authors": "Michael Allan Ribers and Hannes Ullrich",
        "title": "Machine Predictions and Human Decisions with Variation in Payoffs and\n  Skill",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Human decision-making differs due to variation in both incentives and\navailable information. This constitutes a substantial challenge for the\nevaluation of whether and how machine learning predictions can improve decision\noutcomes. We propose a framework that incorporates machine learning on\nlarge-scale data into a choice model featuring heterogeneity in decision maker\npayoff functions and predictive skill. We apply this framework to the major\nhealth policy problem of improving the efficiency in antibiotic prescribing in\nprimary care, one of the leading causes of antibiotic resistance. Our analysis\nreveals large variation in physicians' skill to diagnose bacterial infections\nand in how physicians trade off the externality inherent in antibiotic use\nagainst its curative benefit. Counterfactual policy simulations show that the\ncombination of machine learning predictions with physician diagnostic skill\nresults in a 25.4 percent reduction in prescribing and achieves the largest\nwelfare gains compared to alternative policies for both estimated physician as\nwell as conservative social planner preference weights on the antibiotic\nresistance externality.\n"
    },
    {
        "paper_id": 2011.11274,
        "authors": "Rachel Heyard and Hanna Hottenrott",
        "title": "The Impact of Research Funding on Knowledge Creation and Dissemination:\n  A study of SNSF Research Grants",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the impact of competitive project-funding on\nresearchers' publication outputs. Using detailed information on applicants at\nthe Swiss National Science Foundation (SNSF) and their proposals' evaluation,\nwe employ a case-control design that accounts for individual heterogeneity of\nresearchers and selection into treatment (e.g. funding). We estimate the impact\nof grant award on a set of output indicators measuring the creation of new\nresearch results (the number of peer-reviewed articles), its relevance (number\nof citations and relative citation ratios), as well as its accessibility and\ndissemination as measured by the publication of preprints and by altmetrics.\nThe results show that the funding program facilitates the publication and\ndissemination of additional research amounting to about one additional article\nin each of the three years following the grant. The higher citation metrics and\naltmetrics of publications by funded researchers suggest that impact goes\nbeyond quantity, but that funding fosters quality and impact.\n"
    },
    {
        "paper_id": 2011.11281,
        "authors": "Patrick Chang, Etienne Pienaar, Tim Gebbie",
        "title": "The Epps effect under alternative sampling schemes",
        "comments": "15 pages, 13 figures. Link to our supporting Julia code:\n  https://github.com/CHNPAT005/PCRBTG-VT. Accepted: Physica A",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 583C (2021)\n  126329",
        "doi": "10.1016/j.physa.2021.126329",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Time and the choice of measurement time scales is fundamental to how we\nchoose to represent information and data in finance. This choice implies both\nthe units and the aggregation scales for the resulting statistical measurables\nused to describe a financial system. It also defines how we measure the\nrelationship between different traded instruments. As we move from\nhigh-frequency time scales, when individual trade and quote events occur, to\nthe mesoscales when correlations emerge in ways that can conform to various\nlatent models; it remains unclear what choice of time and sampling rates are\nappropriate to faithfully capture system dynamics and asset correlations for\ndecision making. The Epps effect is the key phenomenology that couples the\nemergence of correlations to the choice of sampling time scales. Here we\nconsider and compare the Epps effect under different sampling schemes in order\nto contrast three choices of time: calendar time, volume time and trade time.\nUsing a toy model based on a Hawkes process, we are able to achieve simulation\nresults that conform well with empirical dynamics. Concretely, we find that the\nEpps effect is present under all three definitions of time and that\ncorrelations emerge faster under trade time compared to calendar time, whereas\ncorrelations emerge linearly under volume time.\n"
    },
    {
        "paper_id": 2011.11394,
        "authors": "Gian Paolo Clemente and Alessandra Cornaro",
        "title": "Assessing Systemic Risk in the Insurance Sector via Network Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a framework for detecting relevant insurance companies in a\nsystemic risk perspective. Among the alternative methodologies for measuring\nsystemic risk, we propose a complex network approach where insurers are linked\nto form a global interconnected system. We model the reciprocal influence\nbetween insurers calibrating edge weights on the basis of specific risk\nmeasures. Therefore, we provide a suitable network indicator, the Weighted\nEffective Resistance Centrality, able to catch which is the effect of a\nspecific vertex on the network robustness. By means of this indicator, we\nassess the prominence of a company in spreading and receiving risk from the\nothers.\n"
    },
    {
        "paper_id": 2011.11776,
        "authors": "Faustino Prieto, Jos\\'e Mar\\'ia Sarabia and Enrique Calder\\'in-Ojeda",
        "title": "The risk of death in newborn businesses during the first years in market",
        "comments": "This is a preprint (30 pages, 4 tables, 7 figures)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyzed how business age and mortality are related during\nthe first years of life, and tested the different hypotheses proposed in the\nliterature. For that, we used data on U.S. business establishments, with 1-year\nresolution in the range of age of 0-5 years, in the period 1977-2016, published\nby the United States Census Bureau. First, we explored the adaptation of\nclassical techniques of survival analysis (the Life Table and Peto-Turnbull\nmethods) to the business survival analysis. Then, we considered nine parametric\nprobabilistic models, most of them well-known in reliability analysis and in\nthe actuarial literature, with different shapes of the hazard function, that we\nfitted by maximum likelihood method and compared with the Akaike information\ncriterion. Our findings show that newborn firms seem to have a decreasing\nfailure rate with the age during the first five years in market, with the\nexception of the first months of some years in which the risk can rise.\n"
    },
    {
        "paper_id": 2011.11801,
        "authors": "Nikolas Dawson, Mary-Anne Williams, Marian-Andrei Rizoiu",
        "title": "Skill-driven Recommendations for Job Transition Pathways",
        "comments": null,
        "journal-ref": "PLOS ONE 16(8): e0254722, 2021",
        "doi": "10.1371/journal.pone.0254722",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Job security can never be taken for granted, especially in times of rapid,\nwidespread and unexpected social and economic change. These changes can force\nworkers to transition to new jobs. This may be because new technologies emerge\nor production is moved abroad. Perhaps it is a global crisis, such as COVID-19,\nwhich shutters industries and displaces labor en masse. Regardless of the\nimpetus, people are faced with the challenge of moving between jobs to find new\nwork. Successful transitions typically occur when workers leverage their\nexisting skills in the new occupation. Here, we propose a novel method to\nmeasure the similarity between occupations using their underlying skills. We\nthen build a recommender system for identifying optimal transition pathways\nbetween occupations using job advertisements (ads) data and a longitudinal\nhousehold survey. Our results show that not only can we accurately predict\noccupational transitions (Accuracy = 76%), but we account for the asymmetric\ndifficulties of moving between jobs (it is easier to move in one direction than\nthe other). We also build an early warning indicator for new technology\nadoption (showcasing Artificial Intelligence), a major driver of rising job\ntransitions. By using real-time data, our systems can respond to labor demand\nshifts as they occur (such as those caused by COVID-19). They can be leveraged\nby policy-makers, educators, and job seekers who are forced to confront the\noften distressing challenges of finding new jobs.\n"
    },
    {
        "paper_id": 2011.12057,
        "authors": "Dario Sansone and Anna Zhu",
        "title": "Using Machine Learning to Create an Early Warning System for Welfare\n  Recipients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using high-quality nation-wide social security data combined with machine\nlearning tools, we develop predictive models of income support receipt\nintensities for any payment enrolee in the Australian social security system\nbetween 2014 and 2018. We show that off-the-shelf machine learning algorithms\ncan significantly improve predictive accuracy compared to simpler heuristic\nmodels or early warning systems currently in use. Specifically, the former\npredicts the proportion of time individuals are on income support in the\nsubsequent four years with greater accuracy, by a magnitude of at least 22% (14\npercentage points increase in the R2), compared to the latter. This gain can be\nachieved at no extra cost to practitioners since the algorithms use\nadministrative data currently available to caseworkers. Consequently, our\nmachine learning algorithms can improve the detection of long-term income\nsupport recipients, which can potentially provide governments with large\nsavings in accrued welfare costs.\n"
    },
    {
        "paper_id": 2011.12291,
        "authors": "Matthew F. Tomlinson, David Greenwood, Marcin Mucha-Kruczynski",
        "title": "Asymmetric excitation of left- and right-tail extreme events probed\n  using a Hawkes model: application to financial returns",
        "comments": "15 pages, 10 figures, and 8 tables",
        "journal-ref": "Phys. Rev. E 104, 024112 (2021)",
        "doi": "10.1103/PhysRevE.104.024112",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a two-tailed peak-over-threshold Hawkes model that captures\nasymmetric self- and cross-excitation in and between left- and right-tail\nextreme values within a time series. We demonstrate its applicability by\ninvestigating extreme gains and losses within the daily log-returns of the S&P\n500 equity index. We find that the arrivals of extreme losses and gains are\ndescribed by a common conditional intensity to which losses contribute twice as\nmuch as gains. However, the contribution of the former decays almost five times\nmore quickly than that of the latter. We attribute these asymmetries to the\ndifferent reactions of market traders to extreme upward and downward movements\nof asset prices: an example of negativity bias, wherein trauma is more salient\nthan euphoria.\n"
    },
    {
        "paper_id": 2011.12343,
        "authors": "Hamza Saad",
        "title": "Use Bagging Algorithm to Improve Prediction Accuracy for Evaluation of\n  Worker Performances at a Production Company",
        "comments": "7 pages, 5 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.4172/2169-0316.1000257",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many workers at the production department of Libyan Textile Company work with\ndifferent performances. Plan of company management is paying the money\naccording to the specific performance and quality requirements for each worker.\nThus, it is important to predict the accurate evaluation of workers to extract\nthe knowledge for management, how much money it will pay as salary and\nincentive. For example, if the evaluation is average, then management of the\ncompany will pay part of the salary. If the evaluation is good, then it will\npay full salary, moreover, if the evaluation is excellent, then it will pay\nsalary plus incentive percentage. Twelve variables with 121 instances for each\nvariable collected to predict the evaluation of the process for each worker.\nBefore starting classification, feature selection used to predict the\ninfluential variables which impact the evaluation process. Then, four\nalgorithms of decision trees used to predict the output and extract the\ninfluential relationship between inputs and output. To make sure get the\nhighest accuracy, ensemble algorithm (Bagging) used to deploy four algorithms\nof decision trees and predict the highest prediction result 99.16%. Standard\nerrors for four algorithms were very small; this means that there is a strong\nrelationship between inputs (7 variables) and output (Evaluation). The curve of\n(Receiver operating characteristics) for algorithms gave a high-level\nspecificity and sensitivity, and Gain charts were very close to together.\nAccording to the results, management of the company should take a logic\ndecision about the evaluation of production process and extract the important\nvariables that impact the evaluation.\n"
    },
    {
        "paper_id": 2011.12348,
        "authors": "Hamza Saad",
        "title": "The Application of Data Mining in the Production Processes",
        "comments": "8 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": "10.11648/j.ie.20180201.14",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Traditional statistical and measurements are unable to solve all industrial\ndata in the right way and appropriate time. Open markets mean the customers are\nincreased, and production must increase to provide all customer requirements.\nNowadays, large data generated daily from different production processes and\ntraditional statistical or limited measurements are not enough to handle all\ndaily data. Improve production and quality need to analyze data and extract the\nimportant information about the process how to improve. Data mining applied\nsuccessfully in the industrial processes and some algorithms such as mining\nassociation rules, and decision tree recorded high professional results in\ndifferent industrial and production fields. The study applied seven algorithms\nto analyze production data and extract the best result and algorithm in the\nindustry field. KNN, Tree, SVM, Random Forests, ANN, Na\\\"ive Bayes, and\nAdaBoost applied to classify data based on three attributes without neglect any\nvariables whether this variable is numerical or categorical. The best results\nof accuracy and area under the curve (ROC) obtained from Decision tree and its\nensemble algorithms (Random Forest and AdaBoost). Thus, a decision tree is an\nappropriate algorithm to handle manufacturing and production data especially\nthis algorithm can handle numerical and categorical data.\n"
    },
    {
        "paper_id": 2011.12523,
        "authors": "Eckhard Platen and Stefan Tappe",
        "title": "Exploiting arbitrage requires short selling",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that in a financial market given by semimartingales an arbitrage\nopportunity, provided it exists, can only be exploited through short selling.\nThis finding provides a theoretical basis for differences in regulation for\nfinancial services providers that are allowed to go short and those without\nshort sales. The privilege to be allowed to short sell gives access to\npotential arbitrage opportunities, which creates by design a bankruptcy risk.\n"
    },
    {
        "paper_id": 2011.12544,
        "authors": "Matthieu Stigler, David Lobell",
        "title": "On the benefits of index insurance in US agriculture: a large-scale\n  analysis using satellite data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Index insurance has been promoted as a promising solution for reducing\nagricultural risk compared to traditional farm-based insurance. By linking\npayouts to a regional factor instead of individual loss, index insurance\nreduces monitoring costs, and alleviates the problems of moral hazard and\nadverse selection. Despite its theoretical appeal, demand for index insurance\nhas remained low in many developing countries, triggering a debate on the\ncauses of the low uptake. Surprisingly, there has been little discussion in\nthis debate about the experience in the United States. The US is an unique case\nas both farm-based and index-based products have been available for more than\ntwo decades. Furthermore, the number of insurance zones is very large, allowing\ninteresting comparisons over space. As in developing countries, the adoption of\nindex insurance is rather low -- less than than 5\\% of insured acreage. Does\nthis mean that we should give up on index insurance?\n  In this paper, we investigate the low take-up of index insurance in the US\nleveraging a field-level dataset for corn and soybean obtained from satellite\npredictions. While previous studies were based either on county aggregates or\non relatively small farm-level dataset, our satellite-derived data gives us a\nvery large number of fields (close to 1.8 million) comprised within a large\nnumber of index zones (600) observed over 20 years. To evaluate the suitability\nof index insurance, we run a large-scale simulation comparing the benefits of\nboth insurance schemes using a new measure of farm-equivalent risk coverage of\nindex insurance. We make two main contributions. First, we show that in our\nsimulations, demand for index insurance is unexpectedly high, at about 30\\% to\n40\\% of total demand. This result is robust to relaxing several assumptions of\nthe model and to using prospect theory instead of expected utility.\n"
    },
    {
        "paper_id": 2011.12753,
        "authors": "Georgina Onuma Kalu, Chinemerem Dennis Ikpe, Benjamin Ifeanyichukwu\n  Oruh, Samuel Asante Gyamerah",
        "title": "State Space Vasicek Model of a Longevity Bond",
        "comments": "30 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Life expectancy have been increasing over the past years due to better health\ncare, feeding and conducive environment. To manage future uncertainty related\nto life expectancy, various insurance institutions have resolved to come up\nwith financial instruments that are indexed-linked to the longevity of the\npopulation. These new instrument is known as longevity bonds. In this article,\nwe present a novel classical Vasicek one factor affine model in modelling zero\ncoupon longevity bond price (ZCLBP) with financial and mortality risk. The\ninterest rate r(t) and the stochastic mortality of the constructed model are\ndependent but with uncorrelated driving noises. The model is presented in a\nlinear state-space representation of the contiuous-time infinite horizon and\nused Kalman filter for its estimation. The appropriate state equation and\nmeasurement equation derived from our model is used as a method of pricing a\nlongevity bond in a financial market. The empirical analysis results show that\nthe unobserved instantaneous interest rate shows a mean reverting behaviour in\nthe U.S. term structure. The zero-coupon bonds yields are used as inputs for\nthe estimation process. The results of the analysis are gotten from the monthly\nobservations of U.S. Treasury zero coupon bonds from December, 1992 to January,\n1993. The empirical evidence indicates that to model properly the historical\nmortality trends at different ages, both the survival rate and the yield data\nare needed to achieve a satisfactory empirical fit over the zero coupon\nlongevity bond term structure. The dynamics of the resulting model allowed us\nto perform simulation on the survival rates, which enables us to model\nlongevity risk.\n"
    },
    {
        "paper_id": 2011.12869,
        "authors": "Conny Grunicke, Jan Christian Schl\\\"uter, Jani-Pekka Jokinen",
        "title": "Implementation of a cost-benefit analysis of Demand-Responsive Transport\n  with a Multi-Agent Transport Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, the technical requirements to perform a cost-benefit analysis\nof a Demand Responsive Transport (DRT) service with the traffic simulation\nsoftware MATSim are elaborated in order to achieve the long-term goal of\nassessing the introduction of a DRT service in G\\\"ottingen and the surrounding\narea. The aim was to determine if the software is suitable for a cost-benefit\nanalysis while providing a user manual for building a basic simulation that can\nbe extended with public transport and DRT. The main result is that the software\nis suitable for a cost-benefit analysis of a DRT service. In particular, the\nmost important internal and external costs, such as usage costs of the various\nmodes of transport and emissions, can be integrated into the simulation\nscenarios. Thus, the scenarios presented in this paper can be extended by data\nfrom a mobility study of G\\\"ottingen and its surroundings in order to achieve\nthe long-term goal. This paper is aimed at transport economists and researchers\nwho are not familiar with MATSim, to provide them with a guide for the first\nsteps in working with a traffic simulation software.\n"
    },
    {
        "paper_id": 2011.13113,
        "authors": "Djoumbissie David Romain",
        "title": "Predicting S&P500 Index direction with Transfer Learning and a Causal\n  Graph as main Input",
        "comments": "Revised description in section II",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a unified multi-tasking framework to represent the complex and\nuncertain causal process of financial market dynamics, and then to predict the\nmovement of any type of index with an application on the monthly direction of\nthe S&P500 index. our solution is based on three main pillars: (i) the use of\ntransfer learning to share knowledge and feature (representation, learning)\nbetween all financial markets, increase the size of the training sample and\npreserve the stability between training, validation and test sample. (ii) The\ncombination of multidisciplinary knowledge (Financial economics, behavioral\nfinance, market microstructure and portfolio construction theories) to\nrepresent a global top-down dynamics of any financial market, through a graph.\n(iii) The integration of forward looking unstructured data, different types of\ncontexts (long, medium and short term) through latent variables/nodes and then,\nuse a unique VAE network (parameter sharing) to learn simultaneously their\ndistributional representation. We obtain Accuracy, F1-score, and Matthew\nCorrelation of 74.3 %, 67 % and 0.42 above the industry and other benchmark on\n12 years test period which include three unstable and difficult sub-period to\npredict.\n"
    },
    {
        "paper_id": 2011.13132,
        "authors": "Xiangqian Sun, Xing Yan, Qi Wu",
        "title": "Generative Learning of Heterogeneous Tail Dependence",
        "comments": "Major technical flaws in theoretical aspects",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a multivariate generative model to capture the complex dependence\nstructure often encountered in business and financial data. Our model features\nheterogeneous and asymmetric tail dependence between all pairs of individual\ndimensions while also allowing heterogeneity and asymmetry in the tails of the\nmarginals. A significant merit of our model structure is that it is not prone\nto error propagation in the parameter estimation process, hence very scalable,\nas the dimensions of datasets grow large. However, the likelihood methods are\ninfeasible for parameter estimation in our case due to the lack of a\nclosed-form density function. Instead, we devise a novel moment learning\nalgorithm to learn the parameters. To demonstrate the effectiveness of the\nmodel and its estimator, we test them on simulated as well as real-world\ndatasets. Results show that this framework gives better finite-sample\nperformance compared to the copula-based benchmarks as well as recent similar\nmodels.\n"
    },
    {
        "paper_id": 2011.1324,
        "authors": "Min-Bin Lin, Kainat Khowaja, Cathy Yi-Hsuan Chen, Wolfgang Karl\n  H\\\"ardle",
        "title": "Blockchain mechanism and distributional characteristics of cryptos",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the relationship between underlying blockchain mechanism of\ncryptocurrencies and its distributional characteristics. In addition to price,\nwe emphasise on using actual block size and block time as the operational\nfeatures of cryptos. We use distributional characteristics such as fourier\npower spectrum, moments, quantiles, global we optimums, as well as the measures\nfor long term dependencies, risk and noise to summarise the information from\ncrypto time series. With the hypothesis that the blockchain structure explains\nthe distributional characteristics of cryptos, we use characteristic based\nspectral clustering to cluster the selected cryptos into five groups. We\nscrutinise these clusters and find that indeed, the clusters of cryptos share\nsimilar mechanism such as origin of fork, difficulty adjustment frequency, and\nthe nature of block size. This paper provides crypto creators and users with a\nbetter understanding toward the connection between the blockchain protocol\ndesign and distributional characteristics of cryptos.\n"
    },
    {
        "paper_id": 2011.13268,
        "authors": "David Saunders, Luis Seco, Markus Senn",
        "title": "Price of liquidity in the reinsurance of fund returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to extend downside protection to a hedge fund investment\nportfolio based on shared loss fee structures that have become increasing\npopular in the market. In particular, we consider a second tranche and suggest\nthe purchase of an upfront reinsurance contract for any losses on the fund\nbeyond the threshold covered by the first tranche, i.e. gaining full portfolio\nprotection. We identify a fund's underlying liquidity as a key parameter and\nstudy the pricing of this additional reinsurance using two approaches: First,\nan analytic closed-form solution based on the Black-Scholes framework and\nsecond, a numerical simulation using a Markov-switching model. In addition, a\nsimplified backtesting method is implemented to evaluate the practical\napplication of the concept.\n"
    },
    {
        "paper_id": 2011.13369,
        "authors": "Frank Schweitzer, Giona Casiraghi, Mario V. Tomasello, David Garcia",
        "title": "Fragile, yet resilient: Adaptive decline in a collaboration network of\n  firms",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3389/fams.2021.634006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamics of collaboration networks of firms follow a life-cycle of growth\nand decline. That does not imply they also become less resilient. Instead,\ndeclining collaboration networks may still have the ability to mitigate shocks\nfrom firms leaving, and to recover from these losses by adapting to new\npartners. To demonstrate this, we analyze 21.500 R\\&D collaborations of 14.500\nfirms in six different industrial sectors over 25 years. We calculate\ntime-dependent probabilities of firms leaving the network and simulate drop-out\ncascades, to determine the expected dynamics of decline. We then show that\ndeviations from these expectations result from the adaptivity of the network,\nwhich mitigates the decline. These deviations can be used as a measure of\nnetwork resilience.\n"
    },
    {
        "paper_id": 2011.13474,
        "authors": "Subhojit Biswas, Diganta Mukherjee and Indranil SenGupta",
        "title": "Multi-asset Generalised Variance Swaps in Barndorff-Nielsen and Shephard\n  model",
        "comments": "Accepted in International Journal of Financial Engineering",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes swaps on two important new measures of generalized\nvariance, namely the maximum eigenvalue and trace of the covariance matrix of\nthe assets involved. We price these generalized variance swaps for\nBarndorff-Nielsen and Shephard model used in financial markets. We consider\nmultiple assets in the portfolio for theoretical purpose and demonstrate our\napproach with numerical examples taking three stocks in the portfolio. The\nresults obtained in this paper have important implications for the commodity\nsector where such swaps would be useful for hedging risk.\n"
    },
    {
        "paper_id": 2011.13625,
        "authors": "Johannes Muhle-Karbe, Xiaofei Shi, Chen Yang",
        "title": "An Equilibrium Model for the Cross-Section of Liquidity Premia",
        "comments": "35 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a risk-sharing economy where an arbitrary number of heterogenous\nagents trades an arbitrary number of risky assets subject to quadratic\ntransaction costs. For linear state dynamics, the forward-backward stochastic\ndifferential equations characterizing equilibrium asset prices and trading\nstrategies in this context reduce to a system of matrix-valued Riccati\nequations. We prove the existence of a unique global solution and provide\nexplicit asymptotic expansions that allow us to approximate the corresponding\nequilibrium for small transaction costs. These tractable approximation formulas\nmake it feasible to calibrate the model to time series of prices and trading\nvolume, and to study the cross-section of liquidity premia earned by assets\nwith higher and lower trading costs. This is illustrated by an empirical case\nstudy.\n"
    },
    {
        "paper_id": 2011.13637,
        "authors": "Jan Rosenzweig",
        "title": "Fat Tailed Factors",
        "comments": null,
        "journal-ref": "Risk, March 2022",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Standard, PCA-based factor analysis suffers from a number of well known\nproblems due to the random nature of pairwise correlations of asset returns. We\nanalyse an alternative based on ICA, where factors are identified based on\ntheir non-Gaussianity, instead of their variance. Generalizations of portfolio\nconstruction to the ICA framework leads to two semi-optimal portfolio\nconstruction methods: a fat-tailed portfolio, which maximises return per unit\nof non-Gaussianity, and the hybrid portfolio, which asymptotically reduces\nvariance and non-Gaussianity in parallel. For fat-tailed portfolios, the\nportfolio weights scale like performance to the power of $1/3$, as opposed to\nlinear scaling of Kelly portfolios; such portfolio construction significantly\nreduces portfolio concentration, and the winner-takes-all problem inherent in\nKelly portfolios. For hybrid portfolios, the variance is diversified at the\nsame rate as Kelly PCA-based portfolios, but excess kurtosis is diversified\nmuch faster than in Kelly, at the rate of $n^{-2}$ compared to Kelly\nportfolios' $n^{-1}$ for increasing number of components $n$.\n"
    },
    {
        "paper_id": 2011.13687,
        "authors": "Marc Chataigner, Stephane Crepey, and Jiang Pu",
        "title": "Nowcasting Networks",
        "comments": "his article has been accepted for publication in Journal of\n  Computational Finance, publishedby Incisive Media Ltd",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We devise a neural network based compression/completion methodology for\nfinancial nowcasting. The latter is meant in a broad sense encompassing\ncompletion of gridded values, interpolation, or outlier detection, in the\ncontext of financial time series of curves or surfaces (also applicable in\nhigher dimensions, at least in theory). In particular, we introduce an original\narchitecture amenable to the treatment of data defined at variable grid nodes\n(by far the most common situation in financial nowcasting applications, so that\nPCA or classical autoencoder methods are not applicable). This is illustrated\nby three case studies on real data sets. First, we introduce our approach on\nrepo curves data (with moving time-to-maturity as calendar time passes).\nSecond, we show that our approach outperforms elementary interpolation\nbenchmarks on an equity derivative surfaces data set (with moving\ntime-to-maturity again). We also obtain a satisfying performance for outlier\ndetection and surface completion. Third, we benchmark our approach against PCA\non at-the-money swaption surfaces redefined at constant expiry/tenor grid\nnodes. Our approach is then shown to perform as well as (even if not obviously\nbetter than) the PCA which, however, is not be applicable to the native, raw\ndata defined on a moving time-to-expiry grid).\n"
    },
    {
        "paper_id": 2011.139,
        "authors": "Mark Whitmeyer",
        "title": "Persuasion Produces the (Diamond) Paradox",
        "comments": "Update on 7 Dec 2020: fixed minor errors throughout, corrected the\n  proofs of several lemmata in Section 2.1 (leaving the results unchanged),\n  added in an example (Example 2.7), and removed the final proposition",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper extends the sequential search model of Wolinsky (1986) by allowing\nfirms to choose how much match value information to disclose to visiting\nconsumers. This restores the Diamond paradox (Diamond 1971): there exist no\nsymmetric equilibria in which consumers engage in active search, so consumers\nobtain zero surplus and firms obtain monopoly profits. Modifying the scenario\nto one in which prices are advertised, we discover that the no-active-search\nresult persists, although the resulting symmetric equilibria are ones in which\nfirms price at marginal cost.\n"
    },
    {
        "paper_id": 2011.13954,
        "authors": "Chenxing Li, Yang Yu, Andrew Chi-Chih Yao, Da Zhang, Xiliang Zhang",
        "title": "An authenticated and secure accounting system for international\n  emissions trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expanding multi-country emissions trading system is considered as crucial to\nfill the existing mitigation gap for the 2\\degree C climate target. Trustworthy\nemissions accounting is the cornerstone of such a system encompassing different\njurisdictions. However, traditional emissions measuring, reporting, and\nverification practices that support data authenticity might not be applicable\nas detailed data from large utilities and production facilities to be covered\nin the multi-country emissions trading system are usually highly sensitive and\nof severe national security concern. In this study, we propose a cryptographic\nframework for an authenticated and secure emissions accounting system that can\nresolve this data dilemma. We demonstrate that integrating a sequence of\ncryptographic protocols can preserve data authenticity and security for a\nstylized multi-country emissions trading system. We call for more research to\npromote applications of modern cryptography in future international climate\ngovernance to build trust and strengthen collaboration.\n"
    },
    {
        "paper_id": 2011.1396,
        "authors": "Navonil Deb, Abhinandan Dalal and Gopal Krishna Basak",
        "title": "Finding Optimal Cancer Treatment using Markov Decision Process to\n  Improve Overall Health and Quality of Life",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markov Decision Processes and Dynamic Treatment Regimes have grown\nincreasingly popular in the treatment of diseases, including cancer. However,\ncancer treatment often impacts quality of life drastically, and people often\nfail to take treatments that are sustainable, affordable and can be adhered to.\nIn this paper, we emphasize the usage of ambient factors like profession,\nradioactive exposure, food habits on the treatment choice, keeping in mind that\nthe aim is not just to relieve the patient of his disease, but rather to\nmaximize his overall physical, social and mental well being. We delineate a\ngeneral framework which can directly incorporate a net benefit function from a\nphysician as well as patient's utility, and can incorporate the varying\nprobabilities of exposure and survival of patients of varying medical profiles.\nWe also show by simulations that the optimal choice of actions often is\nsensitive to extraneous factors, like the financial status of a person (as a\nproxy for the affordability of treatment), and that these actions should be\nwelcome keeping in mind the overall quality of life.\n"
    },
    {
        "paper_id": 2011.14094,
        "authors": "Giampiero M. Gallo, Demetrio Lacava and Edoardo Otranto",
        "title": "On Classifying the Effects of Policy Announcements on Volatility",
        "comments": "23 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The financial turmoil surrounding the Great Recession called for\nunprecedented intervention by Central Banks: unconventional policies affected\nvarious areas in the economy, including stock market volatility. In order to\nevaluate such effects, by including Markov Switching dynamics within a recent\nMultiplicative Error Model, we propose a model--based classification of the\ndates of a Central Bank's announcements to distinguish the cases where the\nannouncement implies an increase or a decrease in volatility, or no effect. In\ndetail, we propose two smoothed probability--based classification methods,\nobtained as a by--product of the model estimation, which provide very similar\nresults to those coming from a classical k--means clustering procedure. The\napplication on four Eurozone market volatility series shows a successful\nclassification of 144 European Central Bank announcements.\n"
    },
    {
        "paper_id": 2011.14112,
        "authors": "Elnaz Gholipour (1), B\\'ela Vizv\\'ari (1) and Zolt\\'an Lakner (2) ((1)\n  Eastern Mediterranean University, (2) St. Stephen University)",
        "title": "Reconstruction Rating Model of Sovereign Debt by Logical Analysis of\n  Data",
        "comments": "18 pages, 1 figure, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sovereign debt ratings provided by rating agencies measure the solvency of a\ncountry, as gauged by a lender or an investor. It is an indication of the risk\ninvolved in investment, and should be determined correctly and in a well timed\nmanner. The present study reconstructs sovereign debt ratings through logical\nanalysis of data, which is based on the theory of Boolean functions. It\norganizes groups of countries according to twenty World Bank defined variables\nfor the period 2012 till 2015. The Fitch Rating Agency, one of the three big\nglobal rating agencies, is used as a case study. An approximate algorithm was\ncrucial in exploring the rating method, in correcting the agencys errors, and\nin determining the estimated rating of otherwise non rated countries. The\noutcome was a decision tree for each year. Each country was assigned a rating.\nOn average, the algorithm reached almost ninety eight percentage matched\nratings in the training set, and was verified by eighty four percentage in the\ntest set. This was a considerable achievement.\n"
    },
    {
        "paper_id": 2011.14424,
        "authors": "Niko Hauzenberger and Michael Pfarrhofer and Anna Stelzer",
        "title": "On the effectiveness of the European Central Bank's conventional and\n  unconventional policies under uncertainty",
        "comments": "JEL: C32, E32, E52, E58 KEYWORDS: Euro area, monetary policy,\n  Bayesian smooth-transition vector autoregression, hierarchical global-local\n  shrinkage",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we investigate the effectiveness of conventional and\nunconventional monetary policy measures by the European Central Bank (ECB)\nconditional on the prevailing level of uncertainty. To obtain exogenous\nvariation in central bank policy, we rely on high-frequency surprises in\nfinancial market data for the euro area (EA) around policy announcement dates.\nWe trace the dynamic effects of shocks to the short-term policy rate, forward\nguidance and quantitative easing on several key macroeconomic and financial\nquantities alongside survey-based measures of expectations. For this purpose,\nwe propose a Bayesian smooth-transition vector autoregression (ST-VAR). Our\nresults suggest that transmission channels are impaired when uncertainty is\nelevated. While conventional monetary policy is less effective during such\nperiods, and sometimes also forward guidance, quantitative easing measures seem\nto work comparatively well in uncertain times.\n"
    },
    {
        "paper_id": 2011.14756,
        "authors": "Vasily Korovkin and Alexey Makarin",
        "title": "Production Networks and War",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do severe shocks such as war alter the economy? We study how a country's\nproduction network is affected by a devastating but localized conflict. Using\nunique transaction-level data on Ukrainian railway shipments around the start\nof the 2014 Russia-Ukraine crisis, we uncover several novel indirect effects of\nconflict on firms. First, we document substantial propagation effects on\ninterfirm trade--trade declines even between partners outside the conflict\nareas if one of them had traded with those areas before the conflict events.\nThe magnitude of such second-degree effect of conflict is one-fifth of the\nfirst-degree effect. Ignoring this propagation would lead to an underestimate\nof the total impact of conflict on trade by about 67\\%. Second, war induces\nsudden changes in the production-network structure that influence firm\nperformance. Specifically, we find that firms that exogenously became more\ncentral--after the conflict practically cut off certain regions from the rest\nof Ukraine--received a relative boost to their revenues. Finally, in a\nproduction-network model, we separately estimate the effects of the exogenous\nfirm removal and the subsequent endogenous network adjustment on firm revenue\ndistribution. For a median firm, network adjustment compensates for 80\\% of the\nnetwork destruction a year after the conflict onset.\n"
    },
    {
        "paper_id": 2011.14809,
        "authors": "Luiz G. A. Alves, Higor Y. D. Sigaki, Matjaz Perc, Haroldo V. Ribeiro",
        "title": "Collective dynamics of stock market efficiency",
        "comments": "11 pages, 4 figures, supplementary information; accepted for\n  publication in Scientific Reports",
        "journal-ref": "Sci. Rep. 10, 21992 (2020)",
        "doi": "10.1038/s41598-020-78707-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Summarized by the efficient market hypothesis, the idea that stock prices\nfully reflect all available information is always confronted with the behavior\nof real-world markets. While there is plenty of evidence indicating and\nquantifying the efficiency of stock markets, most studies assume this\nefficiency to be constant over time so that its dynamical and collective\naspects remain poorly understood. Here we define the time-varying efficiency of\nstock markets by calculating the permutation entropy within sliding\ntime-windows of log-returns of stock market indices. We show that major world\nstock markets can be hierarchically classified into several groups that display\nsimilar long-term efficiency profiles. However, we also show that efficiency\nranks and clusters of markets with similar trends are only stable for a few\nmonths at a time. We thus propose a network representation of stock markets\nthat aggregates their short-term efficiency patterns into a global and coherent\npicture. We find this financial network to be strongly entangled while also\nhaving a modular structure that consists of two distinct groups of stock\nmarkets. Our results suggest that stock market efficiency is a collective\nphenomenon that can drive its operation at a high level of informational\nefficiency, but also places the entire system under risk of failure.\n"
    },
    {
        "paper_id": 2011.14817,
        "authors": "Sla{\\dj}ana Babi\\'c and Christophe Ley and Lorenzo Ricci and David\n  Veredas",
        "title": "TailCoR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic and financial crises are characterised by unusually large events.\nThese tail events co-move because of linear and/or nonlinear dependencies. We\nintroduce TailCoR, a metric that combines (and disentangles) these linear and\nnon-linear dependencies. TailCoR between two variables is based on the tail\ninter quantile range of a simple projection. It is dimension-free, it performs\nwell in small samples, and no optimisations are needed.\n"
    },
    {
        "paper_id": 2011.14823,
        "authors": "Abdelghani Maddi and Yves Gingras",
        "title": "Gender diversity in research teams and citation impact in Economics and\n  Management",
        "comments": "23 pages,6 Figures, 8 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this paper is twofold:1)contribute to a better understanding of\nthe place of women in Economics and Management disciplines by characterizing\nthe difference in levels of scientific collaboration between men and women at\nthe specialties level;2) Investigate the relationship between gender diversity\nand citation impact in Economics and Management. Our data, extracted from the\nWeb of Science database, cover global production as indexed in 302 journals in\nEconomics and 370 journals in Management, with respectively 153 667 and 163 567\narticles published between 2008 and 2018. Results show that collaborative\npractices between men and women are quite different in Economics and\nManagement. We also find that there is a positive and significant effect of\ngender diversity on the academic impact of publications. Mixed-gender\npublications (co-authored by men and women) receive more citations than\nnon-mixed papers (written by same-gender author teams) or single-author\npublications. The effect is slightly stronger in Management. The regression\nanalysis also indicates that there is, for both disciplines, a small negative\neffect on citations received if the corresponding author is a woman.\n"
    },
    {
        "paper_id": 2011.14834,
        "authors": "Sang T. Truong",
        "title": "The Effect of Education on Smoking Decisions in the United States",
        "comments": "Published at the 19th BGSU Undergraduate Economics Student Paper\n  Contest and Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the link between education and the decision to start\nsmoking as well as the decision to quit smoking. Data is gathered from IPUMS\nCPS and Centers for Disease Control and Prevention. Probit analysis (with the\nuse of probability weight and robust standard error) indicates that every\nadditional year of education will reduce the 2.3 percentage point of the\nsmoking probability and will add 3.53 percentage point in quitting likelihood,\nholding home restriction, public restriction, cigarette price, family income,\nage, gender, race, and ethnicity constant. I believe that tobacco epidemic is a\nserious global issue that may be mitigated by using careful regulations on\nsmoking restriction and education.\n"
    },
    {
        "paper_id": 2011.15068,
        "authors": "Xunyi Wang, Reza Mousavi, Yili Hong",
        "title": "The Unintended Consequences of Stay-at-Home Policies on Work Outcomes:\n  The Impacts of Lockdown Orders on Content Creation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has posed an unprecedented challenge to individuals\naround the globe. To mitigate the spread of the virus, many states in the U.S.\nissued lockdown orders to urge their residents to stay at their homes, avoid\nget-togethers, and minimize physical interactions. While many offline workers\nare experiencing significant challenges performing their duties, digital\ntechnologies have provided ample tools for individuals to continue working and\nto maintain their productivity. Although using digital platforms to build\nresilience in remote work is effective, other aspects of remote work (beyond\nthe continuation of work) should also be considered in gauging true resilience.\nIn this study, we focus on content creators, and investigate how restrictions\nin individual's physical environment impact their online content creation\nbehavior. Exploiting a natural experimental setting wherein four states issued\nstate-wide lockdown orders on the same day whereas five states never issued a\nlockdown order, and using a unique dataset collected from a short video-sharing\nsocial media platform, we study the impact of lockdown orders on content\ncreators' behaviors in terms of content volume, content novelty, and content\noptimism. We combined econometric methods (difference-in-differences\nestimations of a matched sample) with machine learning-based natural language\nprocessing to show that on average, compared to the users residing in\nnon-lockdown states, the users residing in lockdown states create more content\nafter the lockdown order enforcement. However, we find a decrease in the\nnovelty level and optimism of the content generated by the latter group. Our\nfindings have important contributions to the digital resilience literature and\nshed light on managers' decision-making process related to the adjustment of\nemployees' work mode in the long run.\n"
    },
    {
        "paper_id": 2012.00095,
        "authors": "P.G.J. Persoon, R.N.A. Bekkers, F. Alkemade",
        "title": "How cumulative is technological knowledge?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Technological cumulativeness is considered one of the main mechanisms for\ntechnological progress, yet its exact meaning and dynamics often remain\nunclear. To develop a better understanding of this mechanism we approach a\ntechnology as a body of knowledge consisting of interlinked inventions.\nTechnological cumulativeness can then be understood as the extent to which\ninventions build on other inventions within that same body of knowledge. The\ncumulativeness of a technology is therefore characterized by the structure of\nits knowledge base, which is different from, but closely related to, the size\nof its knowledge base. We analytically derive equations describing the relation\nbetween the cumulativeness and the size of the knowledge base. In addition, we\nempirically test our ideas for a number of selected technologies, using patent\ndata. Our results suggest that cumulativeness increases proportionally with the\nsize of the knowledge base, at a rate which varies considerably across\ntechnologies. At the same time we find that across technologies, this rate is\ninversely related to the rate of invention over time. This suggests that the\ncumulativeness increases relatively slow in rapidly growing technologies. In\nsum, the presented approach allows for an in-depth, systematic analysis of\ncumulativeness variations across technologies and the knowledge dynamics\nunderlying technology development.\n"
    },
    {
        "paper_id": 2012.00103,
        "authors": "Richard S.J. Tol",
        "title": "Rise of the Kniesians: The professor-student network of Nobel laureates\n  in economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents the professor-student network of Nobel laureates in\neconomics. 74 of the 79 Nobelists belong to one family tree. The remaining 5\nbelong to 3 separate trees. There are 350 men in the graph, and 4 women. Karl\nKnies is the central-most professor, followed by Wassily Leontief. No classical\nand few neo-classical economists have left notable descendants. Harvard is the\ncentral-most university, followed by Chicago and Berlin. Most candidates for\nthe Nobel prize belong to the main family tree, but new trees may arise for the\nstudents of Terence Gorman and Denis Sargan.\n"
    },
    {
        "paper_id": 2012.00206,
        "authors": "Ben-Hur Francisco Cardoso, Sebasti\\'an Gon\\c{c}alves and Jos\\'e\n  Roberto Iglesias",
        "title": "Wealth concentration in systems with unbiased binary exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126123",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Aiming to describe the wealth distribution evolution, several models consider\nan ensemble of interacting economic agents that exchange wealth in binary\nfashion. Intriguingly, models that consider an unbiased market, that gives to\neach agent the same chances to win in the game, are always out of equilibrium\nuntil the perfect inequality of the final state is attained. Here we present a\nrigorous analytical demonstration that any system driven by unbiased binary\nexchanges are doomed to drive the system to perfect inequality and zero\nmobility.\n"
    },
    {
        "paper_id": 2012.00345,
        "authors": "Xue Dong He and Zhaoli Jiang",
        "title": "Optimal Payoff under the Generalized Dual Theory of Choice",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2008.10257",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider portfolio optimization under a preference model in a\nsingle-period, complete market. This preference model includes Yaari's dual\ntheory of choice and quantile maximization as special cases. We characterize\nwhen the optimal solution exists and derive the optimal solution in closed form\nwhen it exists. The payoff of the optimal portfolio is a digital option: it\nyields an in-the-money payoff when the market is good and zero payoff\notherwise. When the initial wealth increases, the set of good market scenarios\nremains unchanged while the payoff in these scenarios increases. Finally, we\nextend our portfolio optimization problem by imposing a dependence structure\nwith a given benchmark payoff and derive similar results.\n"
    },
    {
        "paper_id": 2012.00359,
        "authors": "Delia Coculescu and Aditi Dandapani",
        "title": "Insiders and their Free Lunches: the Role of Short Positions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given a stock price process, we analyse the potential of arbitrage by\ninsiders in a context of short-selling prohibitions. We introduce the notion of\nminimal supermartingale measure, and we analyse its properties in connection to\nthe minimal martingale measure. In particular, we establish conditions when\nboth fail to exist. These correspond to the case when the insider's information\nset includes some non null events that are perceived as having null\nprobabilities by the uninformed market investors. These results may have\ndifferent applications, such as in problems related to the local\nrisk-minimisation for insiders whenever strategies are implemented without\nshort selling.\n"
    },
    {
        "paper_id": 2012.00729,
        "authors": "Mike Ludkovski",
        "title": "mlOSP: Towards a Unified Implementation of Regression Monte Carlo\n  Algorithms",
        "comments": "Package repository is at http://github.com/mludkov/mlOSP 46 pages\n  with 7 figures and 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce mlOSP, a computational template for Machine Learning for Optimal\nStopping Problems. The template is implemented in the R statistical environment\nand publicly available via a GitHub repository. mlOSP presents a unified\nnumerical implementation of Regression Monte Carlo (RMC) approaches to optimal\nstopping, providing a state-of-the-art, open-source, reproducible and\ntransparent platform. Highlighting its modular nature, we present multiple\nnovel variants of RMC algorithms, especially in terms of constructing\nsimulation designs for training the regressors, as well as in terms of machine\nlearning regression modules. Furthermore, mlOSP nests most of the existing RMC\nschemes, allowing for a consistent and verifiable benchmarking of extant\nalgorithms. The article contains extensive R code snippets and figures, and\nserves as a vignette to the underlying software package.\n"
    },
    {
        "paper_id": 2012.00821,
        "authors": "Aaron Wray and Matthew Meades and Dave Cliff",
        "title": "Automated Creation of a High-Performing Algorithmic Trader via Deep\n  Learning on Level-2 Limit Order Book Data",
        "comments": "To be presented at the IEEE Symposium on Computational Intelligence\n  in Financial Engineering (CIFEr2020) in Canberra, Australia, 1-4 December\n  2020; 8 pages; 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present results demonstrating that an appropriately configured deep\nlearning neural network (DLNN) can automatically learn to be a high-performing\nalgorithmic trading system, operating purely from training-data inputs\ngenerated by passive observation of an existing successful trader T. That is,\nwe can point our black-box DLNN system at trader T and successfully have it\nlearn from T's trading activity, such that it trades at least as well as T. Our\nsystem, called DeepTrader, takes inputs derived from Level-2 market data, i.e.\nthe market's Limit Order Book (LOB) or Ladder for a tradeable asset. Unusually,\nDeepTrader makes no explicit prediction of future prices. Instead, we train it\npurely on input-output pairs where in each pair the input is a snapshot S of\nLevel-2 LOB data taken at the time when T issued a quote Q (i.e. a bid or an\nask order) to the market; and DeepTrader's desired output is to produce Q when\nit is shown S. That is, we train our DLNN by showing it the LOB data S that T\nsaw at the time when T issued quote Q, and in doing so our system comes to\nbehave like T, acting as an algorithmic trader issuing specific quotes in\nresponse to specific LOB conditions. We train DeepTrader on large numbers of\nthese S/Q snapshot/quote pairs, and then test it in a variety of market\nscenarios, evaluating it against other algorithmic trading systems in the\npublic-domain literature, including two that have repeatedly been shown to\noutperform human traders. Our results demonstrate that DeepTrader learns to\nmatch or outperform such existing algorithmic trading systems. We analyse the\nsuccessful DeepTrader network to identify what features it is relying on, and\nwhich features can be ignored. We propose that our methods can in principle\ncreate an explainable copy of an arbitrary trader T via \"black-box\" deep\nlearning methods.\n"
    }
]