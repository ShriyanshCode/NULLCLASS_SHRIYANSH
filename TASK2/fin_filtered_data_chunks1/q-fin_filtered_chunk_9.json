[
    {
        "paper_id": 1901.05024,
        "authors": "Victor Olkhov",
        "title": "Econophysics of Asset Price, Return and Multiple Expectations",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": "10.21314/JNTF.2019.055",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes asset price and return disturbances as result of\nrelations between transactions and multiple kinds of expectations. We show that\ndisturbances of expectations can cause fluctuations of trade volume, price and\nreturn. We model price disturbances for transactions made under all types of\nexpectations as weighted sum of partial price and trade volume disturbances for\ntransactions made under separate kinds of expectations. Relations on price\nallow present return as weighted sum of partial return and trade volume\n\"return\" for transactions made under separate expectations. Dependence of price\ndisturbances on trade volume disturbances as well as dependence of return on\ntrade volume \"return\" cause dependence of volatility and statistical\ndistributions of price and return on statistical properties of trade volume\ndisturbances and trade volume \"return\" respectively.\n"
    },
    {
        "paper_id": 1901.05053,
        "authors": "Elena Green and Daniel M. Heffernan",
        "title": "An Agent-Based Model to Explain the Emergence of Stylised Facts in Log\n  Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper outlines an agent-based model of a simple financial market in\nwhich a single asset is available for trade by three different types of\ntraders. The model was first introduced in the PhD thesis of one of the\nauthors, see reference [1]. The simulated log returns are examined for the\npresence of the stylised facts of financial data. The features of\nleptokurtosis, volatility clustering and aggregational Gaussianity are\nespecially highlighted and studied in detail. The following ingredients are\nfound to be essential for the production of these stylised facts: the memory of\nnoise traders who make random trade decisions; the inclusion of technical\ntraders that trade in line with trends in the price and the inclusion of\nfundamental traders who know the \"fundamental value\" of the stock and trade\naccordingly. When these three basic types of traders are included log returns\nare produced with a leptokurtic distribution and volatility clustering as well\nas some further statistical features of empirical data. This enhances and\nbroadens our understanding of the fundamental processes involved in the\nproduction of empirical data by the market.\n"
    },
    {
        "paper_id": 1901.05113,
        "authors": "Lars Tyge Nielsen",
        "title": "Instantaneous Arbitrage and the CAPM",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the concept of instantaneous arbitrage in continuous time\nand its relation to the instantaneous CAPM. Absence of instantaneous arbitrage\nis equivalent to the existence of a trading strategy which satisfies the CAPM\nbeta pricing relation in place of the market. Thus the difference between the\narbitrage argument and the CAPM argument in Black and Scholes (1973) is this:\nthe arbitrage argument assumes that there exists some portfolio satisfying the\ncapm equation, whereas the CAPM argument assumes, in addition, that this\nportfolio is the market portfolio.\n"
    },
    {
        "paper_id": 1901.05332,
        "authors": "Fr\\'ed\\'eric Bucci, Michael Benzaquen, Fabrizio Lillo, Jean-Philippe\n  Bouchaud",
        "title": "Slow decay of impact in equity markets: insights from the ANcerno\n  database",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an empirical study of price reversion after the executed\nmetaorders. We use a data set with more than 8 million metaorders executed by\ninstitutional investors in the US equity market. We show that relaxation takes\nplace as soon as the metaorder ends:{while at the end of the same day it is on\naverage $\\approx 2/3$ of the peak impact, the decay continues the next days,\nfollowing a power-law function at short time scales, and converges to a\nnon-zero asymptotic value at long time scales (${\\sim 50}$ days) equal to\n$\\approx 1/2$ of the impact at the end of the first day.} Due to a significant,\nmultiday correlation of the sign of executed metaorders, a careful\ndeconvolution of the \\emph{observed} impact must be performed to extract the\nestimate of the impact decay of isolated metaorders.\n"
    },
    {
        "paper_id": 1901.05672,
        "authors": "J\\'er\\^ome Lelong (DAO)",
        "title": "Pricing path-dependent Bermudan options using Wiener chaos expansion: an\n  embarrassingly parallel approach",
        "comments": "The Journal of Computational Finance, Incisive Media, In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose a new policy iteration algorithm for pricing\nBermudan options when the payoff process cannot be written as a function of a\nlifted Markov process. Our approach is based on a modification of the\nwell-known Longstaff Schwartz algorithm, in which we basically replace the\nstandard least square regression by a Wiener chaos expansion. Not only does it\nallow us to deal with a non Markovian setting, but it also breaks the\nbottleneck induced by the least square regression as the coefficients of the\nchaos expansion are given by scalar products on the L^2 space and can therefore\nbe approximated by independent Monte Carlo computations. This key feature\nenables us to provide an embarrassingly parallel algorithm.\n"
    },
    {
        "paper_id": 1901.05802,
        "authors": "Marcel Nutz, Yuchong Zhang",
        "title": "Conditional Optimal Stopping: A Time-Inconsistent Optimization",
        "comments": "Forthcoming in 'Annals of Applied Probability'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by recent work of P.-L. Lions on conditional optimal control, we\nintroduce a problem of optimal stopping under bounded rationality: the\nobjective is the expected payoff at the time of stopping, conditioned on\nanother event. For instance, an agent may care only about states where she is\nstill alive at the time of stopping, or a company may condition on not being\nbankrupt. We observe that conditional optimization is time-inconsistent due to\nthe dynamic change of the conditioning probability and develop an equilibrium\napproach in the spirit of R. H. Strotz' work for sophisticated agents in\ndiscrete time. Equilibria are found to be essentially unique in the case of a\nfinite time horizon whereas an infinite horizon gives rise to non-uniqueness\nand other interesting phenomena. We also introduce a theory which generalizes\nthe classical Snell envelope approach for optimal stopping by considering a\npair of processes with Snell-type properties.\n"
    },
    {
        "paper_id": 1901.05872,
        "authors": "Rebekka Burkholz, Frank Schweitzer",
        "title": "International crop trade networks: The impact of shocks and cascades",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1748-9326/ab4864",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Analyzing available FAO data from 176 countries over 21 years, we observe an\nincrease of complexity in the international trade of maize, rice, soy, and\nwheat. A larger number of countries play a role as producers or intermediaries,\neither for trade or food processing. In consequence, we find that the trade\nnetworks become more prone to failure cascades caused by exogenous shocks. In\nour model, countries compensate for demand deficits by imposing export\nrestrictions. To capture these, we construct higher-order trade dependency\nnetworks for the different crops and years. These networks reveal hidden\ndependencies between countries and allow to discuss policy implications.\n"
    },
    {
        "paper_id": 1901.06021,
        "authors": "Martin Tegn\\'er and Stephen Roberts",
        "title": "A Probabilistic Approach to Nonparametric Local Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The local volatility model is a widely used for pricing and hedging financial\nderivatives. While its main appeal is its capability of reproducing any given\nsurface of observed option prices---it provides a perfect fit---the essential\ncomponent is a latent function which can be uniquely determined only in the\nlimit of infinite data. To (re)construct this function, numerous calibration\nmethods have been suggested involving steps of interpolation and extrapolation,\nmost often of parametric form and with point-estimate representations. We look\nat the calibration problem in a probabilistic framework with a nonparametric\napproach based on a Gaussian process prior. This immediately gives a way of\nencoding prior beliefs about the local volatility function and a hypothesis\nmodel which is highly flexible yet not prone to over-fitting. Besides providing\na method for calibrating a (range of) point-estimate(s), we draw posterior\ninference from the distribution over local volatility. This leads to a better\nunderstanding of uncertainty associated with the calibration in particular, and\nwith the model in general. Further, we infer dynamical properties of local\nvolatility by augmenting the hypothesis space with a time dimension. Ideally,\nthis provides predictive distributions not only locally, but also for entire\nsurfaces forward in time. We apply our approach to S&P 500 market data.\n"
    },
    {
        "paper_id": 1901.06309,
        "authors": "Josef Anton Strini and Stefan Thonhauser",
        "title": "On a dividend problem with random funding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a modification of the dividend maximization problem from ruin\ntheory. Based on a classical risk process we maximize the difference of\nexpected cumulated discounted dividends and total expected discounted\nadditional funding (subject to some proportional transaction costs). For\nmodelling dividends we use the common approach whereas for the funding\nopportunity we use the jump times of another independent Poisson process at\nwhich we choose an appropriate funding height. In case of exponentially\ndistributed claims we are able to determine an explicit solution to the problem\nand derive an optimal strategy whose nature heavily depends on the size of the\ntransaction costs.\n"
    },
    {
        "paper_id": 1901.06467,
        "authors": "Jose Cruz, Daniel Sevcovic",
        "title": "Option Pricing in Illiquid Markets with Jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical linear Black--Scholes model for pricing derivative securities\nis a popular model in financial industry. It relies on several restrictive\nassumptions such as completeness, and frictionless of the market as well as the\nassumption on the underlying asset price dynamics following a geometric\nBrownian motion. The main purpose of this paper is to generalize the classical\nBlack--Scholes model for pricing derivative securities by taking into account\nfeedback effects due to an influence of a large trader on the underlying asset\nprice dynamics exhibiting random jumps. The assumption that an investor can\ntrade large amounts of assets without affecting the underlying asset price\nitself is usually not satisfied, especially in illiquid markets. We generalize\nthe Frey--Stremme nonlinear option pricing model for the case the underlying\nasset follows a Levy stochastic process with jumps. We derive and analyze a\nfully nonlinear parabolic partial-integro differential equation for the price\nof the option contract. We propose a semi-implicit numerical discretization\nscheme and perform various numerical experiments showing influence of a large\ntrader and intensity of jumps on the option price.\n"
    },
    {
        "paper_id": 1901.06609,
        "authors": "Barbara Burgess-Wilkerson, Clovia Hamilton, Chlotia Garrison, Keith\n  Robbins",
        "title": "Preparing millennials as digital citizens and socially and\n  environmentally responsible business professionals in a socially\n  irresponsible climate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As of 2015, a millennial born in the 1990's became the largest population in\nthe workplace and are still growing. Studies indicate that a millennial is tech\nsavvy but lag in the exercise of digital responsibility. In addition, they are\npassive towards environmental sustainability and fail to grasp the importance\nof social responsibility. This paper provides a review of such findings\nrelating to business communications educators in their classrooms. The\nliterature should enable the development of a millennial as an excellent global\ncitizen through business communications curricula that emphasizes digital\ncitizenship, environmental sustainability and social responsibility. The\nimpetus for this work is to provide guidance in the development of courses and\nteaching strategies customized to the development of each millennial as a\ndigital, environmental and socially responsible global citizen.\n"
    },
    {
        "paper_id": 1901.0668,
        "authors": "Zuo Quan Xu, Fahuai Yi",
        "title": "Optimal redeeming strategy of stock loans under drift uncertainty",
        "comments": null,
        "journal-ref": "Mathematics of Operations Research, Vol. 45, 2020, 384-401",
        "doi": "10.1287/moor.2019.0995",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In practice, one must recognize the inevitable incompleteness of information\nwhile making decisions. In this paper, we consider the optimal redeeming\nproblem of stock loans under a state of incomplete information presented by the\nuncertainty in the (bull or bear) trends of the underlying stock. This is\ncalled drift uncertainty. Due to the unavoidable need for the estimation of\ntrends while making decisions, the related Hamilton-Jacobi-Bellman (HJB)\nequation is of a degenerate parabolic type. Hence, it is very hard to obtain\nits regularity using the standard approach, making the problem different from\nthe existing optimal redeeming problems without drift uncertainty. We present a\nthorough and delicate probabilistic and functional analysis to obtain the\nregularity of the value function and the optimal redeeming strategies. The\noptimal redeeming strategies of stock loans appear significantly different in\nthe bull and bear trends.\n"
    },
    {
        "paper_id": 1901.06715,
        "authors": "Zhiyi Shen and Chengguo Weng",
        "title": "A Backward Simulation Method for Stochastic Optimal Control Problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A number of optimal decision problems with uncertainty can be formulated into\na stochastic optimal control framework. The Least-Squares Monte Carlo (LSMC)\nalgorithm is a popular numerical method to approach solutions of such\nstochastic control problems as analytical solutions are not tractable in\ngeneral. This paper generalizes the LSMC algorithm proposed in Shen and Weng\n(2017) to solve a wide class of stochastic optimal control models. Our\nalgorithm has three pillars: a construction of auxiliary stochastic control\nmodel, an artificial simulation of the post-action value of state process, and\na shape-preserving sieve estimation method which equip the algorithm with a\nnumber of merits including bypassing forward simulation and control\nrandomization, evading extrapolating the value function, and alleviating\ncomputational burden of the tuning parameter selection. The efficacy of the\nalgorithm is corroborated by an application to pricing equity-linked insurance\nproducts.\n"
    },
    {
        "paper_id": 1901.06855,
        "authors": "Roberto Baviera and Aldo Nassigh and Emanuele Nastasi",
        "title": "A closed formula for illiquid corporate bonds and an application to the\n  European market",
        "comments": "24 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an option approach for pricing bond illiquidity that is\nreminiscent of the celebrated work of Longstaff (1995) on the non-marketability\nof some non-dividend-paying shares in IPOs. This approach describes a quite\ncommon situation in the fixed income market: it is rather usual to find issuers\nthat, besides liquid benchmark bonds, issue some other bonds that either are\nplaced to a small number of investors in private placements or have a limited\nissue size.\n  We model interest rate and credit risks via a convenient reduced-form\napproach. We deduce a simple closed formula for illiquid corporate coupon bond\nprices when liquid bonds with similar characteristics (e.g. maturity) are\npresent in the market for the same issuer. The key model parameter is the\ntime-to-liquidate a position, i.e. the time that an experienced bond trader\ntakes to liquidate a given position on a corporate coupon bond. We show that\nilliquid bonds present an additional liquidity spread that depends on the\ntime-to-liquidate aside from bond volatility.\n  We provide a detailed application for two issuers in the European market.\n"
    },
    {
        "paper_id": 1901.07241,
        "authors": "Olena Kostylenko, Helena Sofia Rodrigues, Delfim F. M. Torres",
        "title": "The spread of a financial virus through Europe and beyond",
        "comments": "This is a preprint of a paper whose final and definite form is with\n  'AIMS Mathematics 4 (2019), no. 1, 86--98', available in open access from\n  [http://dx.doi.org/10.3934/Math.2019.1.86]. Submitted 01-Nov-2018; Revised\n  14-Jan-2019; Accepted 21-Jan-2019",
        "journal-ref": "AIMS Mathematics 4 (2019), no. 1, 86--98",
        "doi": "10.3934/Math.2019.1.86",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the importance of international relations between countries on the\nfinancial stability. The contagion effect in the network is tested by\nimplementing an epidemiological model, comprising a number of European\ncountries and using bilateral data on foreign claims between them. Banking\nstatistics of consolidated foreign claims on ultimate risk bases, obtained from\nthe Banks of International Settlements, allow us to measure the exposure of\ncontagion spreading from a particular country to the other national banking\nsystems. We show that the financial system of some countries, experiencing the\ndebt crisis, is a source of global systemic risk because they threaten the\nstability of a larger system, being a global threat to the intoxication of the\nworld economy and resulting in what we call a `financial virus'. Illustrative\nsimulations were done in the NetLogo multi-agent programmable modelling\nenvironment and in MATLAB.\n"
    },
    {
        "paper_id": 1901.0745,
        "authors": "Julio Backhoff-Veraguas, Daniel Bartl, Mathias Beiglb\\\"ock and Manu\n  Eder",
        "title": "Adapted Wasserstein Distances and Stability in Mathematical Finance",
        "comments": "An author's name had been wrongfully given",
        "journal-ref": "Finance and Stochastics, 24(3):601-632, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assume that an agent models a financial asset through a measure Q with the\ngoal to price / hedge some derivative or optimize some expected utility. Even\nif the model Q is chosen in the most skilful and sophisticated way, she is left\nwith the possibility that Q does not provide an \"exact\" description of reality.\nThis leads us to the following question: will the hedge still be somewhat\nmeaningful for models in the proximity of Q?\n  If we measure proximity with the usual Wasserstein distance (say), the answer\nis NO. Models which are similar w.r.t. Wasserstein distance may provide\ndramatically different information on which to base a hedging strategy.\n  Remarkably, this can be overcome by considering a suitable \"adapted\" version\nof the Wasserstein distance which takes the temporal structure of pricing\nmodels into account. This adapted Wasserstein distance is most closely related\nto the nested distance as pioneered by Pflug and Pichler\n\\cite{Pf09,PfPi12,PfPi14}. It allows us to establish Lipschitz properties of\nhedging strategies for semimartingale models in discrete and continuous time.\nNotably, these abstract results are sharp already for Brownian motion and\nEuropean call options.\n"
    },
    {
        "paper_id": 1901.07542,
        "authors": "Robert Chuchro, Kyle D'Souza, Darren Mei",
        "title": "Dancing with Donald: Polarity in the 2016 Presidential Election",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In almost every election cycle, the validity of the United States Electoral\nCollege is brought into question. The 2016 Presidential Election again brought\nup the issue of a candidate winning the popular vote but not winning the\nElectoral College, with Hillary Clinton receiving close to three million more\nvotes than Donald Trump. However, did the popular vote actually determine the\nmost liked candidate in the election? In this paper, we demonstrate that\ndifferent voting policies can alter which candidate is elected. Additionally,\nwe explore the trade-offs between each of these mechanisms. Finally, we\nintroduce two novel mechanisms with the intent of electing the least polarizing\ncandidate.\n"
    },
    {
        "paper_id": 1901.07721,
        "authors": "Dusan Stosic, Darko Stosic, Tatijana Stosic",
        "title": "Nonextensive triplets in stock market indices",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.03.093",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market indices are one of the most investigated complex systems in\neconophysics. Here we extend the existing literature on stock markets in\nconnection with nonextensive statistical mechanics. We explore the\nnonextensivity of price volatilities for 34 major stock market indices between\n2010 and 2019. We discover that stock markets follow nonextensive statistics\nregarding equilibrium, relaxation and sensitivity. We find nonextensive\nbehavior in stock markets for developed countries, but not for developing\ncountries. Distances between nonextensive triplets suggest that some stock\nmarkets might share similar nonextensive dynamics, while others are widely\ndifferent. The current findings strongly indicate that the stock market\nrepresents a system whose physics is properly described by nonextensive\nstatistical mechanics. Our results shed light on the complex nature of stock\nmarket indices, and establish another formal link with the nonextensive theory.\n"
    },
    {
        "paper_id": 1901.07725,
        "authors": "Dongbo Shi, Yeyanran Ge",
        "title": "Academic Engagement and Commercialization in an Institutional Transition\n  Environment: Evidence from Shanghai Maritime University",
        "comments": "30pages,4tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Does academic engagement accelerate or crowd out the commercialization of\nuniversity knowledge? Research on this topic seldom considers the impact of the\ninstitutional environment, especially when a formal institution for encouraging\nthe commercial activities of scholars has not yet been established. This study\ninvestigates this question in the context of China, which is in the\ninstitutional transition stage. Based on a survey of scholars from Shanghai\nMaritime University, we demonstrate that academic engagement has a positive\nimpact on commercialization and that this impact is greater for risk-averse\nscholars than for other risk-seeking scholars. Our results suggest that in an\ninstitutional transition environment, the government should consider\nencouraging academic engagement to stimulate the commercialization activities\nof conservative scholars.\n"
    },
    {
        "paper_id": 1901.08112,
        "authors": "Benedikt S. L. Fritz and Robert A. Manduca",
        "title": "The Economic Complexity of US Metropolitan Areas",
        "comments": "80 pages, 13 figures",
        "journal-ref": null,
        "doi": "10.1080/00343404.2021.1884215",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We calculate measures of economic complexity for US metropolitan areas for\nthe years 2007-2015 based on industry employment data. We show that the concept\nof economic complexity translates well from the cross-country to the regional\nsetting, and is able to incorporate local as well as traded industries. The\nlargest cities and the Northeast of the US have the highest average complexity,\nwhile traded industries are more complex than local-serving ones on average,\nbut with some exceptions. On average, regions with higher complexity have a\nhigher income per capita, but those regions also were more affected by the\nfinancial crisis. Finally, economic complexity is a significant predictor of\nwithin-decreases in income per capita and population. Our findings highlight\nthe importance of subnational regions, and particularly metropolitan areas, as\nunits of economic geography.\n"
    },
    {
        "paper_id": 1901.08133,
        "authors": "Ulrik W. Nash",
        "title": "The Wisdom of a Kalman Crowd",
        "comments": "29 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Kalman Filter has been called one of the greatest inventions in\nstatistics during the 20th century. Its purpose is to measure the state of a\nsystem by processing the noisy data received from different electronic sensors.\nIn comparison, a useful resource for managers in their effort to make the right\ndecisions is the wisdom of crowds. This phenomenon allows managers to combine\njudgments by different employees to get estimates that are often more accurate\nand reliable than estimates, which managers produce alone. Since harnessing the\ncollective intelligence of employees, and filtering signals from multiple noisy\nsensors appear related, we looked at the possibility of using the Kalman Filter\non estimates by people. Our predictions suggest, and our findings based on the\nSurvey of Professional Forecasters reveal, that the Kalman Filter can help\nmanagers solve their decision-making problems by giving them stronger signals\nbefore they choose. Indeed, when used on a subset of forecasters identified by\nthe Contribution Weighted Model, the Kalman Filter beat that rule clearly,\nacross all the forecasting horizons in the survey.\n"
    },
    {
        "paper_id": 1901.0828,
        "authors": "Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis",
        "title": "Temporal Logistic Neural Bag-of-Features for Financial Time series\n  Forecasting leveraging Limit Order Book Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time series forecasting is a crucial component of many important\napplications, ranging from forecasting the stock markets to energy load\nprediction. The high-dimensionality, velocity and variety of the data collected\nin these applications pose significant and unique challenges that must be\ncarefully addressed for each of them. In this work, a novel Temporal Logistic\nNeural Bag-of-Features approach, that can be used to tackle these challenges,\nis proposed. The proposed method can be effectively combined with deep neural\nnetworks, leading to powerful deep learning models for time series analysis.\nHowever, combining existing BoF formulations with deep feature extractors pose\nsignificant challenges: the distribution of the input features is not\nstationary, tuning the hyper-parameters of the model can be especially\ndifficult and the normalizations involved in the BoF model can cause\nsignificant instabilities during the training process. The proposed method is\ncapable of overcoming these limitations by a employing a novel adaptive scaling\nmechanism and replacing the classical Gaussian-based density estimation\ninvolved in the regular BoF model with a logistic kernel. The effectiveness of\nthe proposed approach is demonstrated using extensive experiments on a\nlarge-scale financial time series dataset that consists of more than 4 million\nlimit orders.\n"
    },
    {
        "paper_id": 1901.08356,
        "authors": "Giorgia Callegaro, Claudia Ceci, Giorgio Ferrari",
        "title": "Optimal Reduction of Public Debt under Partial Observation of the\n  Economic Growth",
        "comments": "31 pages, no figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a government that aims at reducing the debt-to-gross domestic\nproduct (GDP) ratio of a country. The government observes the level of the\ndebt-to-GDP ratio and an indicator of the state of the economy, but does not\ndirectly observe the development of the underlying macroeconomic conditions.\nThe government's criterion is to minimize the sum of the total expected costs\nof holding debt and of debt's reduction policies. We model this problem as a\nsingular stochastic control problem under partial observation. The contribution\nof the paper is twofold. Firstly, we provide a general formulation of the model\nin which the level of debt-to-GDP ratio and the value of the macroeconomic\nindicator evolve as a diffusion and a jump-diffusion, respectively, with\ncoefficients depending on the regimes of the economy. These are described\nthrough a finite-state continuous-time Markov chain. We reduce via filtering\ntechniques the original problem to an equivalent one with full information (the\nso-called separated problem), and we provide a general verification result in\nterms of a related optimal stopping problem under full information. Secondly,\nwe specialize to a case study in which the economy faces only two regimes, and\nthe macroeconomic indicator has a suitable diffusive dynamics. In this setting\nwe provide the optimal debt reduction policy. This is given in terms of the\ncontinuous free boundary arising in an auxiliary fully two-dimensional optimal\nstopping problem.\n"
    },
    {
        "paper_id": 1901.08764,
        "authors": "O.A.Malafeyev, S.A.Nemnyugin",
        "title": "Lattice investment projects support process model with corruption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lattice investment projects support process model with corruption is\nformulated and analyzed. The model is based on the Ising lattice model of\nferromagnetic but takes deal with the social phenomenon. Set of corruption\nagents is considered. It is supposed that agents are placed in sites of the\nlattice. Agents take decision about participation in corruption activity at\ndiscrete moments of time. The decision may lead to profit or to loss. It\ndepends on prehistory of the system. Profit and its dynamics are defined by\nstochastic Markov process. Stochastic nature of the process models influence of\nexternal and individual factors on agents profits. The model is formulated\nalgorithmically and is studied by means of computer simulation. Numerical\nresults are given which demonstrate different asymptotic state of a corruption\nnetwork for various conditions of simulation.\n"
    },
    {
        "paper_id": 1901.08772,
        "authors": "O.A.Malafeyev, A.N.Malova, A.E.Tsybaeva",
        "title": "Psychological model of the investor and manager behavior in risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1063/1.5138093",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  All people have to make risky decisions in everyday life. And we do not know\nhow true they are. But is it possible to mathematically assess the correctness\nof our choice? This article discusses the model of decision making under risk\non the example of project management. This is a game with two players, one of\nwhich is Investor, and the other is the Project Manager. Each player makes a\nrisky decision for himself, based on his past experience. With the help of a\nmathematical model, the players form a level of confidence, depending on who\nthe player accepts the strategy or does not accept. The project manager\nassesses the costs and compares them with the level of confidence. An investor\nevaluates past results. Also visit the case where the strategy of the player\naccepts the part.\n"
    },
    {
        "paper_id": 1901.08826,
        "authors": "Tobias Fissler and Johanna F. Ziegel",
        "title": "Supplement to \"Erratum: Higher Order Elicitability and Osband's\n  Principle\"",
        "comments": "12 pages, 1 figure, to appear as a supplement in the Annals of\n  Statistics",
        "journal-ref": "Ann. Statist., Volume 49, Number 1 (2021), 614",
        "doi": "10.1214/20-AOS2014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note corrects conditions in Proposition 3.4 and Theorem 5.2(ii) and\ncomments on imprecisions in Propositions 4.2 and 4.4 in Fissler and Ziegel\n(2016).\n"
    },
    {
        "paper_id": 1901.08938,
        "authors": "Peng Wu, Marcello Rambaldi, Jean-Fran\\c{c}ois Muzy, Emmanuel Bacry",
        "title": "Queue-reactive Hawkes models for the order flow",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we introduce two variants of multivariate Hawkes models with an\nexplicit dependency on various queue sizes aimed at modeling the stochastic\ntime evolution of a limit order book. The models we propose thus integrate the\ninfluence of both the current book state and the past order flow. The first\nvariant considers the flow of order arrivals at a specific price level as\nindependent from the other one and describes this flow by adding a Hawkes\ncomponent to the arrival rates provided by the continuous time Markov \"Queue\nReactive\" model of Huang et al. Empirical calibration using Level-I order book\ndata from Eurex future assets (Bund and DAX) show that the Hawkes term\ndramatically improves the pure \"Queue-Reactive\" model not only for the\ndescription of the order flow properties (as e.g. the statistics of inter-event\ntimes) but also with respect to the shape of the queue distributions. The\nsecond variant we introduce describes the joint dynamics of all events\noccurring at best bid and ask sides of some order book during a trading day.\nThis model can be considered as a queue dependent extension of the multivariate\nHawkes order-book model of Bacry et al. We provide an explicit way to calibrate\nthis model either with a Maximum-Likelihood method or with a Least-Square\napproach. Empirical estimation from Bund and DAX level-I order book data allow\nus to recover the main features of Hawkes interactions uncovered in Bacry et\nal. but also to unveil their joint dependence on bid and ask queue sizes. We\nnotably find that while the market order or mid-price changes rates can mainly\nbe functions on the volume imbalance this is not the case for the arrival rate\nof limit or cancel orders. Our findings also allows us to clearly bring to\nlight various features that distinguish small and large tick assets.\n"
    },
    {
        "paper_id": 1901.08943,
        "authors": "Shuaiqiang Liu, Cornelis W. Oosterlee, Sander M.Bohte",
        "title": "Pricing options and computing implied volatilities using neural networks",
        "comments": null,
        "journal-ref": "Risks, 7(1) (2019)",
        "doi": "10.3390/risks7010016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a data-driven approach, by means of an Artificial Neural\nNetwork (ANN), to value financial options and to calculate implied volatilities\nwith the aim of accelerating the corresponding numerical methods. With ANNs\nbeing universal function approximators, this method trains an optimized ANN on\na data set generated by a sophisticated financial model, and runs the trained\nANN as an agent of the original solver in a fast and efficient way. We test\nthis approach on three different types of solvers, including the analytic\nsolution for the Black-Scholes equation, the COS method for the Heston\nstochastic volatility model and Brent's iterative root-finding method for the\ncalculation of implied volatilities. The numerical results show that the ANN\nsolver can reduce the computing time significantly.\n"
    },
    {
        "paper_id": 1901.08986,
        "authors": "Irina Georgescu, Jani Kinnunen",
        "title": "How the investor's risk preferences influence the optimal allocation in\n  a credibilistic portfolio problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A classical portfolio theory deals with finding the optimal proportion in\nwhich an agent invests a wealth in a risk-free asset and a probabilistic risky\nasset. Formulating and solving the problem depend on how the risk is\nrepresented and how, combined with the utility function defines a notion of\nexpected utility. In this paper the risk is a fuzzy variable and the notion of\nexpected utility is defined in the setting of Liu's credibility theory. Thus\nthe portfolio choice problem is formulated as an optimization problem in which\nthe objective function is a credibilistic expected utility. Different\napproximation calculation formulas for the optimal allocation of the\ncredibilistic risky asset are proved. These formulas contain two types of\nparameters: various credibilistic moments associated with fuzzy variables\n(expected value, variance, skewness and kurtosis) and the risk aversion,\nprudence and temperance indicators of the utility function.\n"
    },
    {
        "paper_id": 1901.09073,
        "authors": "Mario Coccia",
        "title": "Technological Parasitism",
        "comments": "44 pages; 6 figures; 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological parasitism is a new theory to explain the evolution of\ntechnology in society. In this context, this study proposes a model to analyze\nthe interaction between a host technology (system) and a parasitic technology\n(subsystem) to explain evolutionary pathways of technologies as complex\nsystems. The coefficient of evolutionary growth of the model here indicates the\ntypology of evolution of parasitic technology in relation to host technology:\ni.e., underdevelopment, growth and development. This approach is illustrated\nwith realistic examples using empirical data of product and process\ntechnologies. Overall, then, the theory of technological parasitism can be\nuseful for bringing a new perspective to explain and generalize the evolution\nof technology and predict which innovations are likely to evolve rapidly in\nsociety.\n"
    },
    {
        "paper_id": 1901.09143,
        "authors": "Leonardo Felizardo, Afonso Pinto",
        "title": "A Study on Neural Network Architecture Applied to the Prediction of\n  Brazilian Stock Returns",
        "comments": "17 pages, in Portuguese, 4 figures, 12 tables, Poster presented in\n  EnANPAD Conference",
        "journal-ref": "EnANPAD -\n  http://www.anpad.org.br/~anpad/diversos/2017/enanpad/posters/2017_EnANPAD_POSTER_FIN2093.pdf",
        "doi": "10.6084/m9.figshare.7644959",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a statistical analysis about the characteristics\nthat we intend to influence in the performance of the neural networks in terms\nof assertiveness in the prediction of Brazilian stock returns. We created a\npopulation of architectures for analysis and extracted the sample that had the\nbest assertive performance. It was verified how the characteristics of this\nsample stand out and affect the neural networks. In addition, we make\ninferences about what kind of influence the different architectures have on the\nperformance of neural networks. In the study, the prediction of the return of a\nBrazilian stock traded on the stock exchange of S\\~ao Paulo to measure the\nerror committed by the different architectures of constructed neural networks.\nThe results are promising and indicate that some aspects of the neural network\narchitecture have a significant impact on the assertiveness of the model.\n"
    },
    {
        "paper_id": 1901.09145,
        "authors": "Maria C Mariani, Md Al Masum Bhuiyan, Osei K Tweneboah, Hector\n  Gonzalez-Huizar, Ionut Florescu",
        "title": "Volatility Models Applied to Geophysics and High Frequency Financial\n  Market Data",
        "comments": "30 Pages, 23 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 503,\n  1 August 2018, Pages 304-321",
        "doi": "10.1016/j.physa.2018.02.167",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This work is devoted to the study of modeling geophysical and financial time\nseries. A class of volatility models with time-varying parameters is presented\nto forecast the volatility of time series in a stationary environment. The\nmodeling of stationary time series with consistent properties facilitates\nprediction with much certainty. Using the GARCH and stochastic volatility\nmodel, we forecast one-step-ahead suggested volatility with +/- 2 standard\nprediction errors, which is enacted via Maximum Likelihood Estimation. We\ncompare the stochastic volatility model relying on the filtering technique as\nused in the conditional volatility with the GARCH model. We conclude that the\nstochastic volatility is a better forecasting tool than GARCH (1, 1), since it\nis less conditioned by autoregressive past information.\n"
    },
    {
        "paper_id": 1901.09309,
        "authors": "Jorge Guijarro-Ordonez",
        "title": "High-dimensional statistical arbitrage with factor models and stochastic\n  control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The present paper provides a study of high-dimensional statistical arbitrage\nthat combines factor models with the tools from stochastic control, obtaining\nclosed-form optimal strategies which are both interpretable and computationally\nimplementable in a high-dimensional setting. Our setup is based on a general\nstatistically-constructed factor model with mean-reverting residuals, in which\nwe show how to construct analytically market-neutral portfolios and we analyze\nthe problem of investing optimally in continuous time and finite horizon under\nexponential and mean-variance utilities. We also extend our model to\nincorporate constraints on the investor's portfolio like dollar-neutrality and\nmarket frictions in the form of temporary quadratic transaction costs, provide\nextensive Monte Carlo simulations of the previous strategies with 100 assets,\nand describe further possible extensions of our work.\n"
    },
    {
        "paper_id": 1901.09469,
        "authors": "Yukio Ohsawa, Teruaki Hayashi, Takaaki Yoshino",
        "title": "Tangled String for Multi-Scale Explanation of Contextual Shifts in Stock\n  Market",
        "comments": "16 pages and 7 figures. The author started to write this paper as an\n  extension of the paper [20] in the reference list, but the content came to be\n  changed substantially, not by only minor extension but to a new paper",
        "journal-ref": "Information (https://www.mdpi.com/2078-2489/10/3/118)",
        "doi": "10.3390/info10030118",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The original research question here is given by marketers in general, i.e.,\nhow to explain the changes in the desired timescale of the market. Tangled\nString, a sequence visualization tool based on the metaphor where contexts in a\nsequence are compared to tangled pills in a string, is here extended and\ndiverted to detecting stocks that trigger changes in the market and to\nexplaining the scenario of contextual shifts in the market. Here, the\nsequential data on the stocks of top 10 weekly increase rates in the First\nSection of the Tokyo Stock Exchange for 12 years are visualized by Tangled\nString. The changing in the prices of stocks is a mixture of various timescales\nand can be explained in the time-scale set as desired by using TS. Also, it is\nfound that the change points found by TS coincided by high precision with the\nreal changes in each stock price. As TS has been created from the data-driven\ninnovation platform called Innovators Marketplace on Data Jackets and is\nextended to satisfy data users, this paper is as evidence of the contribution\nof the market of data to data-driven innovations.\n"
    },
    {
        "paper_id": 1901.09629,
        "authors": "Jos\\'e Moran, Jean-Philippe Bouchaud",
        "title": "May's Instability in Large Economies",
        "comments": null,
        "journal-ref": "Phys. Rev. E 100, 032307 (2019)",
        "doi": "10.1103/PhysRevE.100.032307",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Will a large economy be stable? Building on Robert May's original argument\nfor large ecosystems, we conjecture that evolutionary and behavioural forces\nconspire to drive the economy towards marginal stability. We study networks of\nfirms in which inputs for production are not easily substitutable, as in\nseveral real-world supply chains. Relying on results from Random Matrix Theory,\nwe argue that such networks generically become dysfunctional when their size\nincreases, when the heterogeneity between firms becomes too strong or when\nsubstitutability of their production inputs is reduced. At marginal stability\nand for large heterogeneities, we find that the distribution of firm sizes\ndevelops a power-law tail, as observed empirically. Crises can be triggered by\nsmall idiosyncratic shocks, which lead to \"avalanches\" of defaults\ncharacterized by a power-law distribution of total output losses. This scenario\nwould naturally explain the well-known \"small shocks, large business cycles\"\npuzzle, as anticipated long ago by Bak, Chen, Scheinkman and Woodford.\n"
    },
    {
        "paper_id": 1901.09647,
        "authors": "Blanka Horvath, Aitor Muguruza, and Mehdi Tomas",
        "title": "Deep Learning Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a neural network based calibration method that performs the\ncalibration task within a few milliseconds for the full implied volatility\nsurface. The framework is consistently applicable throughout a range of\nvolatility models -including the rough volatility family- and a range of\nderivative contracts. The aim of neural networks in this work is an off-line\napproximation of complex pricing functions, which are difficult to represent or\ntime-consuming to evaluate by other means. We highlight how this perspective\nopens new horizons for quantitative modelling: The calibration bottleneck posed\nby a slow pricing of derivative contracts is lifted. This brings several\nnumerical pricers and model families (such as rough volatility models) within\nthe scope of applicability in industry practice. The form in which information\nfrom available data is extracted and stored influences network performance:\nThis approach is inspired by representing the implied volatility and option\nprices as a collection of pixels. In a number of applications we demonstrate\nthe prowess of this modelling approach regarding accuracy, speed, robustness\nand generality and also its potentials towards model recognition.\n"
    },
    {
        "paper_id": 1901.09729,
        "authors": "Micha{\\l} Narajewski and Florian Ziel",
        "title": "Estimation and simulation of the transaction arrival process in intraday\n  electricity markets",
        "comments": null,
        "journal-ref": "Energies 2019, 12(23), 4518",
        "doi": "10.3390/en12234518",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the novel problem of the estimation of transaction arrival\nprocesses in the intraday electricity markets. We model the inter-arrivals\nusing multiple time-varying parametric densities based on the generalized F\ndistribution estimated by maximum likelihood. We analyse both the in-sample\ncharacteristics and the probabilistic forecasting performance. In a rolling\nwindow forecasting study, we simulate many trajectories to evaluate the\nforecasts and gain significant insights into the model fit. The prediction\naccuracy is evaluated by a functional version of the MAE (mean absolute error),\nRMSE (root mean squared error) and CRPS (continuous ranked probability score)\nfor the simulated count processes. This paper fills the gap in the literature\nregarding the intensity estimation of transaction arrivals and is a major\ncontribution to the topic, yet leaves much of the field for further\ndevelopment. The study presented in this paper is conducted based on the German\nIntraday Continuous electricity market data, but this method can be easily\napplied to any other continuous intraday electricity market. For the German\nmarket, a specific generalized gamma distribution setup explains the overall\nbehaviour significantly best, especially as the tail behaviour of the process\nis well covered.\n"
    },
    {
        "paper_id": 1901.09795,
        "authors": "Marco Bardoscia, Daniele d'Arienzo, Matteo Marsili and Valerio Volpati",
        "title": "Lost in Diversification",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.crhy.2019.05.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As financial instruments grow in complexity more and more information is\nneglected by risk optimization practices. This brings down a curtain of opacity\non the origination of risk, that has been one of the main culprits in the\n2007-2008 global financial crisis. We discuss how the loss of transparency may\nbe quantified in bits, using information theoretic concepts. We find that {\\em\ni)} financial transformations imply large information losses, {\\em ii)}\nportfolios are more information sensitive than individual stocks only if\nfundamental analysis is sufficiently informative on the co-movement of assets,\nthat {\\em iii)} securitisation, in the relevant range of parameters, yields\nassets that are less information sensitive than the original stocks and that\n{\\em iv)} when diversification (or securitisation) is at its best (i.e. when\nassets are uncorrelated) information losses are maximal. We also address the\nissue of whether pricing schemes can be introduced to deal with information\nlosses. This is relevant for the transmission of incentives to gather\ninformation on the risk origination side. Within a simple mean variance scheme,\nwe find that market incentives are not generally sufficient to make information\nharvesting sustainable.\n"
    },
    {
        "paper_id": 1901.10534,
        "authors": "Faisal I Qureshi",
        "title": "Investigating Limit Order Book Characteristics for Short Term Price\n  Prediction: a Machine Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the proliferation of algorithmic high-frequency trading in financial\nmarkets, the Limit Order Book has generated increased research interest.\nResearch is still at an early stage and there is much we do not understand\nabout the dynamics of Limit Order Books. In this paper, we employ a machine\nlearning approach to investigate Limit Order Book features and their potential\nto predict short term price movements. This is an initial broad-based\ninvestigation that results in some novel observations about LOB dynamics and\nidentifies several promising directions for further research. Furthermore, we\nobtain prediction results that are significantly superior to a baseline\npredictor.\n"
    },
    {
        "paper_id": 1901.10544,
        "authors": "Jasmina Jekni\\'c-Dugi\\'c, Sonja Radi\\' c, Igor Petrovi\\'c, Momir\n  Arsenijevi\\'c, Miroljub Dugi\\'c",
        "title": "Quantum Brownian oscillator for the stock market",
        "comments": "15 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We pursue the quantum-mechanical challenge to the efficient market hypothesis\nfor the stock market by employing the quantum Brownian motion model. We utilize\nthe quantum Caldeira-Leggett master equation as a possible phenomenological\nmodel for the stock-market-prices fluctuations while introducing the external\nharmonic field for the Brownian particle. Two quantum regimes are of particular\ninterest: the exact regime as well as the approximate regime of the pure\ndecoherence (\"recoilless\") limit of the Caldeira-Leggett equation. By\ncalculating the standard deviation and the kurtosis for the particle's position\nobservable, we can detect deviations of the quantum-mechanical behavior from\nthe classical counterpart, which bases the efficient market hypothesis. By\nvarying the damping factor, temperature as well as the oscillator's frequency,\nwe are able to provide interpretation of different economic scenarios and\npossible situations that are not normally recognized by the efficient market\nhypothesis. Hence we recognize the quantum Brownian oscillator as a possibly\nuseful model for the realistic behavior of stock prices.\n"
    },
    {
        "paper_id": 1901.10552,
        "authors": "Revathi Anil Kumar and Mark Chamness",
        "title": "Stochastic Estimated Risk for Storage Capacity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing data storage growth is of crucial importance to businesses. Poor\npractices can lead to large data and financial losses. Access to storage\ninformation along with timely action, or capacity forecasting, are essential to\navoid these losses. In addition, ensuring high accuracy of capacity forecast\nestimates along with ease of interpretability plays an important role for any\ncustomer facing tool. In this paper, we introduce Stochastic Estimated Risk\n(SER), a tool developed at Nutanix that has been in production. SER shifts the\nfocus from forecasting a single estimate for date of attaining full capacity to\npredicting the risk associated with running out of storage capacity. Using a\nBrownian motion with drift model, SER estimates the probability that a system\nwill run out of capacity within a specific time frame. Our results showed that\na probabilistic approach is more accurate and credible, for systems with\nnon-linear patterns, compared to a regression or ensemble forecasting models.\n"
    },
    {
        "paper_id": 1901.10556,
        "authors": "Irina Georgescu",
        "title": "Possibilistic investment models with background risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the study of investment problem, aside from the investment risk the\nbackground risk appears. Both the investment risk and the background risk are\nprobabilistically described by random variables. This paper starts from the\nhypothesis that the two types of risk can be represented both probabilistically\n(by random variables) and possibilistically (by fuzzy numbers). We will study\nthree models in which the investment risk and the background risk can be: fuzzy\nnumbers, a random variabl-a fuzzy number and a fuzzy number-a random variable.\nA portfolio problem is formulated for each model and an approximate calculation\nformula of the optimal solution is proved.\n"
    },
    {
        "paper_id": 1901.10581,
        "authors": "Morteza Taiebat, Austin L. Brown, Hannah R. Safford, Shen Qu, Ming Xu",
        "title": "A Review on Energy, Environmental, and Sustainability Implications of\n  Connected and Automated Vehicles",
        "comments": null,
        "journal-ref": "Environmental Science & Technology, 2018, 52(20), 11449-11465",
        "doi": "10.1021/acs.est.8b00127",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Connected and automated vehicles (CAVs) are poised to reshape transportation\nand mobility by replacing humans as the driver and service provider. While the\nprimary stated motivation for vehicle automation is to improve safety and\nconvenience of road mobility, this transformation also provides a valuable\nopportunity to improve vehicle energy efficiency and reduce emissions in the\ntransportation sector. Progress in vehicle efficiency and functionality,\nhowever, does not necessarily translate to net positive environmental outcomes.\nHere we examine the interactions between CAV technology and the environment at\nfour levels of increasing complexity: vehicle, transportation system, urban\nsystem, and society. We find that environmental impacts come from\nCAV-facilitated transformations at all four levels, rather than from CAV\ntechnology directly. We anticipate net positive environmental impacts at the\nvehicle, transportation system, and urban system levels, but expect greater\nvehicle utilization and shifts in travel patterns at the society level to\noffset some of these benefits. Focusing on the vehicle-level improvements\nassociated with CAV technology is likely to yield excessively optimistic\nestimates of environmental benefits. Future research and policy efforts should\nstrive to clarify the extent and possible synergetic effects from a systems\nlevel in order to envisage and address concerns regarding the short- and\nlong-term sustainable adoption of CAV technology.\n"
    },
    {
        "paper_id": 1901.10771,
        "authors": "Takashi Shinzato",
        "title": "Minimal Investment Risk with Cost and Return Constraints: A Replica\n  Analysis",
        "comments": "14 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.7566/JPSJ.88.064804",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous studies into the budget constraint of portfolio optimization\nproblems based on statistical mechanical informatics have not considered that\nthe purchase cost per unit of each asset is distinct. Moreover, the fact that\nthe optimal investment allocation differs depending on the size of investable\nfunds has also been neglected. In this paper, we approach the problem of\ninvestment risk minimization using replica analysis. This problem imposes cost\nand return constraints. We also derive the macroscopic theory indicated by the\noptimal solution and confirm the validity of our proposed method through\nnumerical experiments.\n"
    },
    {
        "paper_id": 1901.1086,
        "authors": "Karlson Pfannschmidt, Pritha Gupta, Bj\\\"orn Haddenhorst, Eyke\n  H\\\"ullermeier",
        "title": "Learning Context-Dependent Choice Functions",
        "comments": "45 pages, 21 figures",
        "journal-ref": "International Journal of Approximate Reasoning 140 (2022) 116-155",
        "doi": "10.1016/j.ijar.2021.10.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Choice functions accept a set of alternatives as input and produce a\npreferred subset of these alternatives as output. We study the problem of\nlearning such functions under conditions of context-dependence of preferences,\nwhich means that the preference in favor of a certain choice alternative may\ndepend on what other options are also available. In spite of its practical\nrelevance, this kind of context-dependence has received little attention in\npreference learning so far. We propose a suitable model based on\ncontext-dependent (latent) utility functions, thereby reducing the problem to\nthe task of learning such utility functions. Practically, this comes with a\nnumber of challenges. For example, the set of alternatives provided as input to\na choice function can be of any size, and the output of the function should not\ndepend on the order in which the alternatives are presented. To meet these\nrequirements, we propose two general approaches based on two representations of\ncontext-dependent utility functions, as well as instantiations in the form of\nappropriate end-to-end trainable neural network architectures. Moreover, to\ndemonstrate the performance of both networks, we present extensive empirical\nevaluations on both synthetic and real-world datasets.\n"
    },
    {
        "paper_id": 1901.10989,
        "authors": "Martin Herdegen, Johannes Muhle-Karbe, Dylan Possama\\\"i",
        "title": "Equilibrium Asset Pricing with Transaction Costs",
        "comments": "32 pages, forthcoming in 'Finance and Stochastics'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study risk-sharing economies where heterogenous agents trade subject to\nquadratic transaction costs. The corresponding equilibrium asset prices and\ntrading strategies are characterised by a system of nonlinear, fully-coupled\nforward-backward stochastic differential equations. We show that a unique\nsolution generally exists provided that the agents' preferences are\nsufficiently similar. In a benchmark specification with linear state dynamics,\nthe illiquidity discounts and liquidity premia observed empirically correspond\nto a positive relationship between transaction costs and volatility.\n"
    },
    {
        "paper_id": 1901.11013,
        "authors": "Kartikay Gupta and Niladri Chatterjee",
        "title": "Top performing stocks recommendation strategy for portfolio",
        "comments": "20 pages, 9 Tables, 3 figures. Comments are invited. In the last\n  version, Methodological details corrected at one point, results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock return forecasting is of utmost importance in the business world. This\nhas been the favourite topic of research for many academicians since decades.\nRecently, regularization techniques have reported to tremendously increase the\nforecast accuracy of the simple regression model. Still, this model cannot\nincorporate the effect of things like a major natural disaster, large foreign\ninfluence, etc. in its prediction. Such things affect the whole stock market\nand are very unpredictable. Thus, it is more important to recommend top stocks\nrather than predicting exact stock returns. The present paper modifies the\nregression task to output value for each stock which is more suitable for\nranking the stocks by expected returns. Two large datasets consisting of\naltogether 1205 companies listed at Indian exchanges were used for\nexperimentation. Five different metrics were used for evaluating the different\nmodels. Results were also analysed subjectively through plots. The results\nshowed the superiority of the proposed techniques.\n"
    },
    {
        "paper_id": 1901.11081,
        "authors": "St\\'ephane Cr\\'epey and Matthew Dixon",
        "title": "Gaussian Process Regression for Derivative Portfolio Modeling and\n  Application to CVA Computations",
        "comments": "36 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling counterparty risk is computationally challenging because it requires\nthe simultaneous evaluation of all the trades with each counterparty under both\nmarket and credit risk. We present a multi-Gaussian process regression\napproach, which is well suited for OTC derivative portfolio valuation involved\nin CVA computation. Our approach avoids nested simulation or simulation and\nregression of cash flows by learning a Gaussian metamodel for the\nmark-to-market cube of a derivative portfolio. We model the joint posterior of\nthe derivatives as a Gaussian process over function space, with the spatial\ncovariance structure imposed on the risk factors. Monte-Carlo simulation is\nthen used to simulate the dynamics of the risk factors. The uncertainty in\nportfolio valuation arising from the Gaussian process approximation is\nquantified numerically. Numerical experiments demonstrate the accuracy and\nconvergence properties of our approach for CVA computations, including a\ncounterparty portfolio of interest rate swaps.\n"
    },
    {
        "paper_id": 1901.11123,
        "authors": "Alexander Budzier, Bent Flyvbjerg, Andi Garavaglia, and Andreas Leed",
        "title": "Quantitative Cost and Schedule Risk Analysis of Nuclear Waste Storage",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1901.03698",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study provides an independent, outside-in estimate of the cost and\nschedule risks of nuclear waste storage projects. Based on a reference class of\n216 past, comparable projects, risk of cost overrun was found to be 202% or\nless, with 80% certainty, i.e., 20% risk of an overrun above 202%. Based on a\nreference class of 200 past, comparable projects, risk of schedule overrun was\nfound to be 104% or less, with 80% certainty, i.e., 20% risk of overrun above\n104%. Cost risk and schedule risk are both substantial for nuclear waste\nstorage projects.\n"
    },
    {
        "paper_id": 1901.11296,
        "authors": "Andrea Molent",
        "title": "Taxation of a GMWB Variable Annuity in a Stochastic Interest Rate Model",
        "comments": null,
        "journal-ref": "ASTIN Bull. 50 (2020) 1001-1035",
        "doi": "10.1017/asb.2020.29",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling taxation of Variable Annuities has been frequently neglected but\naccounting for it can significantly improve the explanation of the withdrawal\ndynamics and lead to a better modeling of the financial cost of these insurance\nproducts. The importance of including a model for taxation has first been\nobserved by Moenig and Bauer (2016) while considering a GMWB Variable Annuity.\nIn particular, they consider the simple Black-Scholes dynamics to describe the\nunderlying security. Nevertheless, GMWB are long term products and thus\naccounting for stochastic interest rate has relevant effects on both the\nfinancial evaluation and the policy holder behavior, as observed by Gouden\\`ege\net al. (2018). In this paper we investigate the outcomes of these two elements\ntogether on GMWB evaluation. To this aim, we develop a numerical framework\nwhich allows one to efficiently compute the fair value of a policy. Numerical\nresults show that accounting for both taxation and stochastic interest rate has\na determinant impact on the withdrawal strategy and on the cost of GMWB\ncontracts. In addition, it can explain why these products are so popular with\npeople looking for a protected form of investment for retirement.\n"
    },
    {
        "paper_id": 1901.11491,
        "authors": "Darjus Hosszejni and Gregor Kastner",
        "title": "Approaches Toward the Bayesian Estimation of the Stochastic Volatility\n  Model with Leverage",
        "comments": null,
        "journal-ref": "In R. Argiento, D. Durante, and S. Wade, editors, Bayesian\n  Statistics and New Generations - Selected Contributions from BAYSM 2018,\n  volume 296 of Springer Proceedings in Mathematics & Statistics, pages 75-83,\n  Cham, 2019. Springer",
        "doi": "10.1007/978-3-030-30611-3_8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The sampling efficiency of MCMC methods in Bayesian inference for stochastic\nvolatility (SV) models is known to highly depend on the actual parameter\nvalues, and the effectiveness of samplers based on different parameterizations\nvaries significantly. We derive novel algorithms for the centered and the\nnon-centered parameterizations of the practically highly relevant SV model with\nleverage, where the return process and innovations of the volatility process\nare allowed to correlate. Moreover, based on the idea of\nancillarity-sufficiency interweaving (ASIS), we combine the resulting samplers\nin order to guarantee stable sampling efficiency irrespective of the baseline\nparameterization.We carry out an extensive comparison to already existing\nsampling methods for this model using simulated as well as real world data.\n"
    },
    {
        "paper_id": 1901.11493,
        "authors": "Kei Nakagawa, Tomoki Ito, Masaya Abe, Kiyoshi Izumi",
        "title": "Deep Recurrent Factor Model: Interpretable Non-Linear and Time-Varying\n  Multi-Factor Model",
        "comments": "In AAAI-19 Workshop on Network Interpretability for Deep Learning",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A linear multi-factor model is one of the most important tools in equity\nportfolio management. The linear multi-factor models are widely used because\nthey can be easily interpreted. However, financial markets are not linear and\ntheir accuracy is limited. Recently, deep learning methods were proposed to\npredict stock return in terms of the multi-factor model. Although these methods\nperform quite well, they have significant disadvantages such as a lack of\ntransparency and limitations in the interpretability of the prediction. It is\nthus difficult for institutional investors to use black-box-type machine\nlearning techniques in actual investment practice because they should show\naccountability to their customers. Consequently, the solution we propose is\nbased on LSTM with LRP. Specifically, we extend the linear multi-factor model\nto be non-linear and time-varying with LSTM. Then, we approximate and linearize\nthe learned LSTM models by LRP. We call this LSTM+LRP model a deep recurrent\nfactor model. Finally, we perform an empirical analysis of the Japanese stock\nmarket and show that our recurrent model has better predictive capability than\nthe traditional linear model and fully-connected deep learning methods.\n"
    },
    {
        "paper_id": 1902.00382,
        "authors": "Morteza Taiebat, Samuel Stolper, Ming Xu",
        "title": "Forecasting the Impact of Connected and Automated Vehicles on Energy Use\n  A Microeconomic Study of Induced Travel and Energy Rebound",
        "comments": null,
        "journal-ref": "Applied Energy, 2019, 247, 297-308",
        "doi": "10.1016/j.apenergy.2019.03.174",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Connected and automated vehicles (CAVs) are expected to yield significant\nimprovements in safety, energy efficiency, and time utilization. However, their\nnet effect on energy and environmental outcomes is unclear. Higher fuel economy\nreduces the energy required per mile of travel, but it also reduces the fuel\ncost of travel, incentivizing more travel and causing an energy \"rebound\neffect.\" Moreover, CAVs are predicted to vastly reduce the time cost of travel,\ninducing further increases in travel and energy use. In this paper, we forecast\nthe induced travel and rebound from CAVs using data on existing travel\nbehavior. We develop a microeconomic model of vehicle miles traveled (VMT)\nchoice under income and time constraints; then we use it to estimate\nelasticities of VMT demand with respect to fuel and time costs, with fuel cost\ndata from the 2017 United States National Household Travel Survey (NHTS) and\nwage-derived predictions of travel time cost. Our central estimate of the\ncombined price elasticity of VMT demand is -0.4, which differs substantially\nfrom previous estimates. We also find evidence that wealthier households have\nmore elastic demand, and that households at all income levels are more\nsensitive to time costs than to fuel costs. We use our estimated elasticities\nto simulate VMT and energy use impacts of full, private CAV adoption under a\nrange of possible changes to the fuel and time costs of travel. We forecast a\n2-47% increase in travel demand for an average household. Our results indicate\nthat backfire - i.e., a net rise in energy use - is a possibility, especially\nin higher income groups. This presents a stiff challenge to policy goals for\nreductions in not only energy use but also traffic congestion and local and\nglobal air pollution, as CAV use increases.\n"
    },
    {
        "paper_id": 1902.00428,
        "authors": "Omar A. Guerrero and Gonzalo Casta\\~neda",
        "title": "Does Better Governance Guarantee Less Corruption? Evidence of Loss in\n  Effectiveness of the Rule of Law",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corruption is an endemic societal problem with profound implications in the\ndevelopment of nations. In combating this issue, cross-national evidence\nsupporting the effectiveness of the rule of law seems at odds with poorly\nrealized outcomes from reforms inspired in such literature. This paper provides\nan explanation for such contradiction. By taking a computational approach, we\ndevelop two methodological novelties into the empirical study of corruption:\n(1) generating large within-country variation by means of simulation (instead\nof cross-national data pooling), and (2) accounting for interactions between\ncovariates through a spillover network. The latter (the network), seems\nresponsible for a significant reduction in the effectiveness of the rule of\nlaw; especially among the least developed countries. We also find that\neffectiveness can be boosted by improving complementary policy issues that may\nlie beyond the governance agenda. Moreover, our simulations suggest that\nimprovements to the rule of law are a necessary yet not sufficient condition to\ncurve corruption.\n"
    },
    {
        "paper_id": 1902.00429,
        "authors": "Gonzalo Casta\\~eda and Omar A. Guerrero",
        "title": "The Importance of Social and Government Learning in Ex Ante Policy\n  Evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide two methodological insights on \\emph{ex ante} policy evaluation\nfor macro models of economic development. First, we show that the problems of\nparameter instability and lack of behavioral constancy can be overcome by\nconsidering learning dynamics. Hence, instead of defining social constructs as\nfixed exogenous parameters, we represent them through stable functional\nrelationships such as social norms. Second, we demonstrate how agent computing\ncan be used for this purpose. By deploying a model of policy prioritization\nwith endogenous government behavior, we estimate the performance of different\npolicy regimes. We find that, while strictly adhering to policy recommendations\nincreases efficiency, the nature of such recipes has a bigger effect. In other\nwords, while it is true that lack of discipline is detrimental to prescription\noutcomes (a common defense of failed recommendations), it is more important\nthat such prescriptions consider the systemic and adaptive nature of the\npolicymaking process (something neglected by traditional technocratic advice).\n"
    },
    {
        "paper_id": 1902.0043,
        "authors": "Omar A. Guerrero and Gonzalo Casta\\~neda",
        "title": "Quantifying the Coherence of Development Policy Priorities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the last 30 years, the concept of policy coherence for development has\nreceived especial attention among academics, practitioners and international\norganizations. However, its quantification and measurement remain elusive. To\naddress this challenge, we develop a theoretical and empirical framework to\nmeasure the coherence of policy priorities for development. Our procedure takes\ninto account the country-specific constraints that governments face when trying\nto reach specific development goals. Hence, we put forward a new definition of\npolicy coherence where context-specific efficient resource allocations are\nemployed as the baseline to construct an index. To demonstrate the usefulness\nand validity of our index, we analyze the cases of Mexico, Korea and Estonia,\nthree developing countries that, arguably, joined the OECD with the aim of\ncoherently establishing policies that could enable a catch-up process. We find\nthat Korea shows significant signs of policy coherence, Estonia seems to be in\nthe process of achieving it, and Mexico has unequivocally failed. Furthermore,\nour results highlight the limitations of assessing coherence in terms of naive\nbenchmark comparisons using development-indicator data. Altogether, our\nframework sheds new light in a promising direction to develop bespoke analytic\ntools to meet the 2030 agenda.\n"
    },
    {
        "paper_id": 1902.00432,
        "authors": "Omar A. Guerrero, Gonzalo Casta\\~neda and Florian Ch\\'avez-Ju\\'arez",
        "title": "How do governments determine policy priorities? Studying development\n  strategies through spillover networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Determining policy priorities is a challenging task for any government\nbecause there may be, for example, a multiplicity of objectives to be\nsimultaneously attained, a multidimensional policy space to be explored,\ninefficiencies in the implementation of public policies, interdependencies\nbetween policy issues, etc. Altogether, these factor s generate a complex\nlandscape that governments need to navigate in order to reach their goals. To\naddress this problem, we develop a framework to model the evolution of\ndevelopment indicators as a political economy game on a network. Our approach\naccounts for the --recently documented-- network of spillovers between policy\nissues, as well as the well-known political economy problem arising from budget\nassignment. This allows us to infer not only policy priorities, but also the\neffective use of resources in each policy issue. Using development indicators\ndata from more than 100 countries over 11 years, we show that the\ncountry-specific context is a central determinant of the effectiveness of\npolicy priorities. In addition, our model explains well-known aggregate facts\nabout the relationship between corruption and development. Finally, this\nframework provides a new analytic tool to generate bespoke advice on\ndevelopment strategies.\n"
    },
    {
        "paper_id": 1902.00678,
        "authors": "Mathias Kloss and Thomas Kirschstein and Steffen Liebscher and Martin\n  Petrick",
        "title": "Robust Productivity Analysis: An application to German FADN data",
        "comments": "27 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sources of bias in empirical studies can be separated in those coming from\nthe modelling domain (e.g. multicollinearity) and those coming from outliers.\nWe propose a two-step approach to counter both issues. First, by\ndecontaminating data with a multivariate outlier detection procedure and\nsecond, by consistently estimating parameters of the production function. We\napply this approach to a panel of German field crop data. Results show that the\ndecontamination procedure detects multivariate outliers. In general,\nmultivariate outlier control delivers more reasonable results with a higher\nprecision in the estimation of some parameters and seems to mitigate the\neffects of multicollinearity.\n"
    },
    {
        "paper_id": 1902.00691,
        "authors": "Guglielmo D'Amico, Filippo Petroni, Philippe Regnault, Stefania\n  Scocchera, Loriano Storchi",
        "title": "A copula based Markov Reward approach to the credit spread in European\n  Union",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a methodology based on piece-wise homogeneous\nMarkov chain for credit ratings and a multivariate model of the credit spreads\nto evaluate the financial risk in European Union (EU). Two main aspects are\nconsidered: how the financial risk is distributed among the European countries\nand how large is the value of the total risk. The first aspect is evaluated by\nmeans of the expected value of a dynamic entropy measure. The second one is\nsolved by computing the evolution of the total credit spread over time.\nMoreover, the covariance between countries' total spread allows understand any\ncontagions in EU. The methodology is applied to real data of 24 countries for\nthe three major agencies: Moody's, Standard and Poor's, and Fitch. Obtained\nresults suggest that both the financial risk inequality and the value of the\ntotal risk increase over time at a different rate depending on the rating\nagency and that the dependence structure is characterized by a strong\ncorrelation between most of European countries.\n"
    },
    {
        "paper_id": 1902.00706,
        "authors": "Asaf Cohen and Virginia R. Young",
        "title": "Rate of Convergence of the Probability of Ruin in the Cram\\'er-Lundberg\n  Model to its Diffusion Approximation",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics, Volume 93, July, 2020,\n  333--340",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the probability of ruin for the {\\it scaled} classical\nCram\\'er-Lundberg (CL) risk process and the corresponding diffusion\napproximation. The scaling, introduced by Iglehart \\cite{I1969} to the\nactuarial literature, amounts to multiplying the Poisson rate $\\la$ by $n$,\ndividing the claim severity by $\\sqrtn$, and adjusting the premium rate so that\nnet premium income remains constant. %Therefore, we think of the associated\ndiffusion approximation as being \"asymptotic for large values of $\\la$.\"\n  We are the first to use a comparison method to prove convergence of the\nprobability of ruin for the scaled CL process and to derive the rate of\nconvergence. Specifically, we prove a comparison lemma for the corresponding\nintegro-differential equation and use this comparison lemma to prove that the\nprobability of ruin for the scaled CL process converges to the probability of\nruin for the limiting diffusion process. Moreover, we show that the rate of\nconvergence for the ruin probability is of order $\\mO\\big(n^{-1/2}\\big)$, and\nwe show that the convergence is {\\it uniform} with respect to the surplus. To\nthe best of our knowledge, this is the first rate of convergence achieved for\nthese ruin probabilities, and we show that it is the tightest one in the\ngeneral case. For the case of exponentially-distributed claims, we are able to\nimprove the approximation arising from the diffusion, attaining a uniform\n$\\mO\\big(n^{-k/2}\\big)$ rate of convergence for arbitrary $k \\in \\N$. We also\ninclude two examples that illustrate our results.\n"
    },
    {
        "paper_id": 1902.00766,
        "authors": "Andreas Haier and Ilya Molchanov",
        "title": "Multivariate risk measures in the non-convex setting",
        "comments": "14 pages. Minor revision",
        "journal-ref": "Statistics and Risk Analysis, 2019, 36, 25-35",
        "doi": "10.1515/strm-2019-0002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The family of admissible positions in a transaction costs model is a random\nclosed set, which is convex in case of proportional transaction costs. However,\nthe convexity fails, e.g. in case of fixed transaction costs or when only a\nfinite number of transfers are possible. The paper presents an approach to\nmeasure risks of such positions based on the idea of considering all selections\nof the portfolio and checking if one of them is acceptable. Properties and\nbasic examples of risk measures of non-convex portfolios are presented.\n"
    },
    {
        "paper_id": 1902.00786,
        "authors": "Joseph Attia",
        "title": "The Applications of Graph Theory to Investing",
        "comments": "53 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How can graph theory be applied to investing in the stock market? The answer\nmay help investors realize the true risks of their investments, help prevent\nrecessions like that of 2008, and increase financial literacy amongst students.\nUsing several original Python programs, we take a correlation matrix with\ncorrelations between the stock prices and then transform that into a graphable\nbinary adjacency matrix. From this graph, we take a graph in which each edge\nrepresents weak correlations between two stocks. Finding the largest complete\ngraph will produce a diversified portfolio. Numerous trials have shown that\ndiversified portfolios consistently outperform the market during times of\neconomic stability, but undiversified portfolios prove to be riskier and more\nunpredictable, either producing huge profits or even larger losses.\nFurthermore, once deciding among which stocks our portfolio would consist of,\nhow do we know when to invest in each stock to maximize profits? Can taking\nstock price data and shifting values help predict how a stock will perform\ntoday if another stock performs a certain way n days prior? It was found that\nthis method of predicting the optimal time to investment failed to improve\nreturns when based solely on correlations. Although a trial with random stocks\nwith varied correlations produced more profits than continuously investing.\n"
    },
    {
        "paper_id": 1902.00924,
        "authors": "Aleksejus Kononovicius, Vygintas Gontis",
        "title": "Approximation of the first passage time distribution for the birth-death\n  processes",
        "comments": "12 pages, 6 figures",
        "journal-ref": "Journal of Statistical Mechanics 2019: 073402 (2019)",
        "doi": "10.1088/1742-5468/ab2709",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a general method to obtain approximation of the first passage time\ndistribution for the birth-death processes. We rely on the general properties\nof birth-death processes, Keilson's theorem and the concept of Riemann sum to\nobtain closed-form expressions. We apply the method to the three selected\nbirth-death processes and the sophisticated order-book model exhibiting\nlong-range memory. We discuss how our approach contributes to the competition\nbetween spurious and true long-range memory models.\n"
    },
    {
        "paper_id": 1902.01157,
        "authors": "Diego Zabaljauregui and Luciano Campi",
        "title": "Optimal market making under partial information with general intensities",
        "comments": "37 pages, 4 figures",
        "journal-ref": "Applied Mathematical Finance (2020)",
        "doi": "10.1080/1350486X.2020.1758587",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Starting from the Avellaneda-Stoikov framework, we consider a market maker\nwho wants to optimally set bid/ask quotes over a finite time horizon, to\nmaximize her expected utility. The intensities of the orders she receives\ndepend not only on the spreads she quotes, but also on unobservable factors\nmodelled by a hidden Markov chain. We tackle this stochastic control problem\nunder partial information with a model that unifies and generalizes many\nexisting ones under full information, combining several risk metrics and\nconstraints, and using general decreasing intensity functionals. We use\nstochastic filtering, control and piecewise-deterministic Markov processes\ntheory, to reduce the dimensionality of the problem and characterize the\nreduced value function as the unique continuous viscosity solution of its\ndynamic programming equation. We then solve the analogous full information\nproblem and compare the results numerically through a concrete example. We show\nthat the optimal full information spreads are biased when the exact market\nregime is unknown, and the market maker needs to adjust for additional regime\nuncertainty in terms of P&L sensitivity and observed order flow volatility.\nThis effect becomes higher, the longer the waiting time in between orders.\n"
    },
    {
        "paper_id": 1902.01265,
        "authors": "Joshua B. Miller, Adam Sanjurjo",
        "title": "Surprised by the Hot Hand Fallacy? A Truth in the Law of Small Numbers",
        "comments": null,
        "journal-ref": "Econometrica, Vol. 86, No. 6 (November 2018), pp. 2019-2047",
        "doi": "10.3982/ECTA14943",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that a subtle but substantial bias exists in a common measure of the\nconditional dependence of present outcomes on streaks of past outcomes in\nsequential data. The magnitude of this streak selection bias generally\ndecreases as the sequence gets longer, but increases in streak length, and\nremains substantial for a range of sequence lengths often used in empirical\nwork. We observe that the canonical study in the influential hot hand fallacy\nliterature, along with replications, are vulnerable to the bias. Upon\ncorrecting for the bias we find that the long-standing conclusions of the\ncanonical study are reversed.\n"
    },
    {
        "paper_id": 1902.01398,
        "authors": "Jennifer Hinton and Donnie Maclurcan",
        "title": "How on Earth: Flourishing in a Not-for-Profit World by 2050",
        "comments": "Working draft from August 2016 (still a work-in-progress), 266 pages,\n  41 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this book, we outline a model of a non-capitalist market economy based on\nnot-for-profit forms of business. This work presents both a critique of the\ncurrent economic system and a vision of a more socially, economically, and\necologically sustainable economy. The point of departure is the purpose and\nprofit-orientation embedded in the legal forms used by businesses (e.g.,\nfor-profit or not-for-profit) and the ramifications of this for global\nsustainability challenges such as environmental pollution, resource use,\nclimate change, and economic inequality. We document the rapid rise of\nnot-for-profit forms of business in the global economy and offer a conceptual\nframework and an analytical lens through which to view these relatively new\neconomic actors and their potential for transforming the economy. The book\nexplores how a market consisting of only or mostly not-for-profit forms of\nbusiness might lead to better financial circulation, economic equality, social\nwell-being, and environmental regeneration as compared to for-profit markets.\n"
    },
    {
        "paper_id": 1902.01471,
        "authors": "Philipp Harms",
        "title": "Strong convergence rates for Markovian representations of fractional\n  processes",
        "comments": "improved presentation and correction of some minor mistakes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many fractional processes can be represented as an integral over a family of\nOrnstein-Uhlenbeck processes. This representation naturally lends itself to\nnumerical discretizations, which are shown in this paper to have strong\nconvergence rates of arbitrarily high polynomial order. This explains the\npotential, but also some limitations of such representations as the basis of\nMonte Carlo schemes for fractional volatility models such as the rough Bergomi\nmodel.\n"
    },
    {
        "paper_id": 1902.01673,
        "authors": "Ryan McCrickerd",
        "title": "On spatially irregular ordinary differential equations and a pathwise\n  volatility modelling framework",
        "comments": "Accepted PhD thesis. Minor revision of v3. 215 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This thesis develops a new framework for modelling price processes in\nfinance, such as an equity price or foreign exchange rate. This can be related\nto the conventional Ito calculus-based framework through the time integral of a\nprice's squared volatility, or `cumulative variance'. In the new framework,\ncorresponding processes are strictly increasing, solve random ordinary\ndifferential equations (ODEs), and are composed with geometric Brownian motion.\nThe new framework has no dependence on stochastic calculus, so processes can be\nstudied on a pathwise basis using probability-free ODE techniques and\nfunctional analysis.\n  The ODEs considered depend on continuous driving functions which are\n`spatially irregular', meaning they need not have any spatial regularity\nproperties such as Holder continuity. They are however strictly increasing in\ntime, thus temporally asymmetric. When sensible initial values are chosen,\ninitial value problem (IVP) solutions are also strictly increasing, and the\nsolution set of such IVPs is shown to contain all differentiable bijections on\nthe non-negative reals. This enables the modelling of any non-negative\nvolatility path which is not zero over intervals, via the time derivative of\nsolutions. Despite this generality, new well-posedness results establish the\nuniqueness of solutions going forwards in time.\n  Motivation to explore this framework comes from its connection with a\ntime-changed Heston volatility model. The framework shows how Heston price\nprocesses can converge to a generalisation of the NIG Levy process, and reveals\na deeper relationship between integrated CIR processes and the IG process.\nWithin this framework, a `Riemann-Liouville-Heston' martingale model is defined\nwhich generalises these relationships to fractional counterparts. This model's\nimplied volatilities are simulated, and exhibit features characteristic of\nleading volatility models.\n"
    },
    {
        "paper_id": 1902.01802,
        "authors": "Adam Rej, Philip Seager and Jean-Philippe Bouchaud",
        "title": "How should you discount your backtest PnL?",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In-sample overfitting is a drawback of any backtest-based investment\nstrategy. It is thus of paramount importance to have an understanding of why\nand how the in-sample overfitting occurs. In this article we propose a simple\nframework that allows one to model and quantify in-sample PnL overfitting. This\nallows us to compute the factor appropriate for discounting PnLs of in-sample\ninvestment strategies.\n"
    },
    {
        "paper_id": 1902.01941,
        "authors": "Weili Chen, Jun Wu, Zibin Zheng, Chuan Chen, and Yuren Zhou",
        "title": "Market Manipulation of Bitcoin: Evidence from Mining the Mt. Gox\n  Transaction Network",
        "comments": "This paper has been accepted by 2019 IEEE International Conference on\n  Computer Communications(Infocom2019)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cryptocurrency market is a very huge market without effective\nsupervision. It is of great importance for investors and regulators to\nrecognize whether there are market manipulation and its manipulation patterns.\nThis paper proposes an approach to mine the transaction networks of exchanges\nfor answering this question.By taking the leaked transaction history of Mt. Gox\nBitcoin exchange as a sample,we first divide the accounts into three categories\naccording to its characteristic and then construct the transaction history into\nthree graphs. Many observations and findings are obtained via analyzing the\nconstructed graphs. To evaluate the influence of the accounts' transaction\nbehavior on the Bitcoin exchange price,the graphs are reconstructed into series\nand reshaped as matrices. By using singular value decomposition (SVD) on the\nmatrices, we identify many base networks which have a great correlation with\nthe price fluctuation. When further analyzing the most important accounts in\nthe base networks, plenty of market manipulation patterns are found. According\nto these findings, we conclude that there was serious market manipulation in\nMt. Gox exchange and the cryptocurrency market must strengthen the supervision.\n"
    },
    {
        "paper_id": 1902.01986,
        "authors": "Ali Ardeshiri and Akshay Vij",
        "title": "A lifestyle-based model of household neighbourhood location and\n  individual travel mode choice behaviours",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Issues such as urban sprawl, congestion, oil dependence, climate change and\npublic health, are prompting urban and transportation planners to turn to land\nuse and urban design to rein in automobile use. One of the implicit beliefs in\nthis effort is that the right land-use policies will, in fact, help to reduce\nautomobile use and increase the use of alternative modes of transportation.\nThus, planners and transport engineers are increasingly viewing land use\npolicies and lifestyle patterns as a way to manage transportation demand. While\na substantial body of work has looked at the relationship between the built\nenvironment and travel behaviour, as well as the influence of lifestyles and\nlifestyle-related decisions on using different travel modes and activity\nbehaviours, limited work has been done in capturing these effects\nsimultaneously and also in exploring the effect of intra-household interaction\non individual attitudes and beliefs towards travel and activity behavior, and\ntheir subsequent influence on lifestyles and modality styles. Therefore, for\nthis study we proposed a framework that captures the concurrent influence of\nlifestyles and modality styles on both household-level decisions, such as\nneighbourhood location, and individual-level decisions, such as travel mode\nchoices using a hierarchical Latent Class Choice Model.\n"
    },
    {
        "paper_id": 1902.0204,
        "authors": "Kei Katahira, Yu Chen, Gaku Hashimoto, Hiroshi Okuda",
        "title": "Development of an agent-based speculation game for higher\n  reproducibility of financial stylized facts",
        "comments": "37 pages, 15 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.04.157",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Simultaneous reproduction of all financial stylized facts is so difficult\nthat most existing stochastic process-based and agent-based models are unable\nto achieve the goal. In this study, by extending the decision-making structure\nof Minority Game, we propose a novel agent-based model called \"Speculation\nGame,\" for a better reproducibility of the stylized facts. The new model has\nthree distinct characteristics comparing with preceding agent-based adaptive\nmodels for the financial market: the enabling of nonuniform holding and idling\nperiods, the inclusion of magnitude information of price change in history, and\nthe implementation of a cognitive world for the evaluation of investment\nstrategies with capital gains and losses. With these features, Speculation Game\nsucceeds in reproducing 10 out of the currently well studied 11 stylized facts\nunder a single parameter setting.\n"
    },
    {
        "paper_id": 1902.02418,
        "authors": "Ali Ardeshiri, Roya Etminani Ghasrodashti, Taha Hossein Rashidi,\n  Mahyar Ardeshiri, Ken Willis",
        "title": "Conservation or deterioration in heritage sites? Estimating willingness\n  to pay for preservation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A significant part of the United Nations World Heritage Sites (WHSs) is\nlocated in developing countries. These sites attract an increasing number of\ntourist and income to these countries. Unfortunately, many of these WHSs are in\na poor condition due to climatic and environmental impacts; war and tourism\npressure, requiring the urgent need for restoration and preservation (Tuan &\nNavrud, 2007). In this study, we characterise residents from Shiraz city\n(visitors and non-visitors) willingness to invest in the management of the\nheritage sites through models for the preservation of heritage and development\nof tourism as a local resource. The research looks at different categories of\nheritage sites within Shiraz city, Iran. The measurement instrument is a stated\npreference referendum task administered state-wide to a sample of 489\nrespondents, with the payment mechanism defined as a purpose-specific\nincremental levy of a fixed amount over a set period of years. A Latent Class\nBinary Logit model, using parametric constraints is used innovatively to deal\nwith any strategic voting such as Yea-sayers and Nay-sayers, as well as\nrevealing the latent heterogeneity among sample members. Results indicate that\nalmost 14% of the sampled population is unwilling to be levied any amount\n(Nay-sayers) to preserve any heritage sites. Not recognizing the presence of\nnay-sayers in the data or recognizing them but eliminating them from the\nestimation will result in biased Willingness to Pay (WTP) results and,\nconsequently, biased policy propositions by authorities. Moreover, it is found\nthat the type of heritage site is a driver of WTP. The results from this study\nprovide insights into the WTP of heritage site visitors and non-visitors with\nrespect to avoiding the impacts of future erosion and destruction and\ncontributing to heritage management and maintenance policies.\n"
    },
    {
        "paper_id": 1902.02419,
        "authors": "Ali Ardeshiri, Spring Sampson, Joffre Swait",
        "title": "Seasonality Effects on Consumers Preferences Over Quality Attributes of\n  Different Beef Products",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.meatsci.2019.06.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using discrete choice modelling, the study investigates 946 American\nconsumers willingness-to-pay and preferences for diverse beef products. A novel\nexperiment was used to elicit the number of beef products that each consumer\nwould purchase. The range of products explored in this study included ground,\ndiced, roast, and six cuts of steaks (sirloin, tenderloin, flank, flap, New\nYork and cowboy or rib-eye). The outcome of the study suggests that US\nconsumers vary in their preferences for beef products by season. The presence\nof a USDA certification logo is by far the most important factor affecting\nconsumers willingness to pay for all beef cuts, which is also heavily dependent\non season. In relation to packaging, US consumers have mixed preference for\ndifferent beef products by season. The results from a scaled adjusted ordered\nlogit model showed that after price, safety-related attributes such as\ncertification logos, types of packaging, and antibiotic free and organic\nproducts are a stronger influence on American consumers choice. Furthermore, US\nconsumers on average purchase diced and roast products more often in winter\nslow cooking season, than in summer, whereas New York strip and flank steak are\nmore popular in the summer grilling season. This study provides valuable\ninsights for businesses as well as policymakers to make inform decisions while\nconsidering how consumers relatively value among different labelling and\nproduct attributes by season and better address any ethical, safety and\naesthetic concerns that consumers might have.\n"
    },
    {
        "paper_id": 1902.0248,
        "authors": "Sounman Hong, Jungmin Ryu",
        "title": "Crowdfunding Public Projects: Collaborative Governance for Achieving\n  Citizen Co-funding of Public Goods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study explores the potential of crowdfunding as a tool for achieving\ncitizen co-funding of public projects. Focusing on philanthropic crowdfunding,\nwe examine whether collaborative projects between public and private\norganizations are more successful in fundraising than projects initiated solely\nby private organizations. We argue that government involvement in crowdfunding\nprovides some type of accreditation or certification that attests to a project\naim to achieve public rather than private goals, thereby mitigating information\nasymmetry and improving mutual trust between creators (i.e., private sector\norganizations) and funders (i.e., crowd). To support this argument, we show\nthat crowdfunding projects with government involvement achieved a greater\nsuccess rate and attracted a greater amount of funding than comparable projects\nwithout government involvement. This evidence shows that governments may take\nadvantage of crowdfunding to co-fund public projects with the citizenry for\naddressing the complex challenges that we face in the twenty-first century.\n"
    },
    {
        "paper_id": 1902.02659,
        "authors": "Nneka Ene (Kings College London)",
        "title": "Implementation of a Port-graph Model for Finance",
        "comments": "In Proceedings TERMGRAPH 2018, arXiv:1902.01510",
        "journal-ref": "EPTCS 288, 2019, pp. 14-25",
        "doi": "10.4204/EPTCS.288.2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we examine the process involved in the design and\nimplementation of a port-graph model to be used for the analysis of an\nagent-based rational negligence model. Rational negligence describes the\nphenomenon that occurred during the financial crisis of 2008 whereby investors\nchose to trade asset-backed securities without performing independent\nevaluations of the underlying assets. This has contributed to motivating the\nsearch for more effective and transparent tools in the modelling of the capital\nmarkets.\n  This paper shall contain the details of a proposal for the use of a visual\ndeclarative language, based on strategic port-graph rewriting, as a visual\nmodelling tool to analyse an asset-backed securitisation market.\n"
    },
    {
        "paper_id": 1902.02854,
        "authors": "Svetlana Boyarchenko and Sergei Levendorskii",
        "title": "Static and semi-static hedging as contrarian or conformist bets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we argue that, once the costs of maintaining the hedging\nportfolio are properly taken into account, semi-static portfolios should more\nproperly be thought of as separate classes of derivatives, with non-trivial,\nmodel-dependent payoff structures. We derive new integral representations for\npayoffs of exotic European options in terms of payoffs of vanillas, different\nfrom Carr-Madan representation, and suggest approximations of the idealized\nstatic hedging/replicating portfolio using vanillas available in the market. We\nstudy the dependence of the hedging error on a model used for pricing and show\nthat the variance of the hedging errors of static hedging portfolios can be\nsizably larger than the errors of variance-minimizing portfolios. We explain\nwhy the exact semi-static hedging of barrier options is impossible for\nprocesses with jumps, and derive general formulas for variance-minimizing\nsemi-static portfolio. We show that hedging using vanillas only leads to larger\nerrors than hedging using vanillas and first touch digitals. In all cases,\nefficient calculations of the weights of the hedging portfolios are in the dual\nspace using new efficient numerical methods for calculation of the Wiener-Hopf\nfactors and Laplace-Fourier inversion.\n"
    },
    {
        "paper_id": 1902.02869,
        "authors": "Mohsen Khorasany, Yateendra Mishra, Gerard Ledwich",
        "title": "Two-Step market clearing for local energy trading in feeder-based\n  markets",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": "10.1049/joe.2018.9312",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent innovations in Information and Communication Technologies (ICT)\nprovide new opportunities and challenges for integration of distributed energy\nresources (DERs) into the energy supply system as active market players. By\nincreasing integration of DERs, novel market platform should be designed for\nthese new market players. The designed electricity market should maximize\nmarket surplus for consumers and suppliers and provide correct incentives for\nthem to join the market and follow market rules. In this paper, a feeder-based\nmarket is proposed for local energy trading among prosumers and consumers in\nthe distribution system. In this market, market players are allowed to share\nenergy with other players in the local market and with neighborhood areas. A\nTwo-StepMarket Clearing (2SMC) mechanism is proposed for market clearing, in\nwhich in the first step, each local market is cleared independently to\ndetermine the market clearing price and in the second step, players can trade\nenergy with neighborhood areas. In comparison to a centralized market, the\nproposed method is scalable and reduces computation overheads, because instead\nof clearing market for a large number of players, the market is cleared for a\nfewer number of players. Also, by applying distributed method and Lagrangian\nmultipliers for market clearing, there is no need for a central computation\ncentre and private information of market players. Case studies demonstrate the\nefficiency and effectiveness of the proposed market clearing method in\nincreasing social welfare and reducing computation time.\n"
    },
    {
        "paper_id": 1902.03041,
        "authors": "Oliver Kley, Claudia Kl\\\"uppelberg and Sandra Paterlini",
        "title": "Modelling Extremal Dependence for Operational Risk by a Bipartite Graph",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a statistical model for operational losses based on heavy-tailed\ndistributions and bipartite graphs, which captures the event type and business\nline structure of operational risk data. The model explicitly takes into\naccount the Pareto tails of losses and the heterogeneous dependence structures\nbetween them. We then derive estimators for individual as well as aggregated\ntail risk, measured in terms of Value-at-Risk and Conditional-Tail-Expectation\nfor very high confidence levels, and provide also an asymptotically full\ncapital allocation method. Estimation methods for such tail risk measures and\ncapital allocations are also proposed and tested on simulated data. Finally, by\nhaving access to real-world operational risk losses from the Italian banking\nsystem, we show that even with a small number of observations, the proposed\nestimation methods produce reliable estimates, and that quantifying dependence\nby means of the empirical network has a big impact on estimates at both\nindividual and aggregate level, as well as for capital allocations.\n"
    },
    {
        "paper_id": 1902.03125,
        "authors": "Chariton Chalvatzis, Dimitrios Hristu-Varsakelis",
        "title": "High-performance stock index trading: making effective use of a deep\n  LSTM neural network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a deep long short-term memory (LSTM)-based neural network for\npredicting asset prices, together with a successful trading strategy for\ngenerating profits based on the model's predictions. Our work is motivated by\nthe fact that the effectiveness of any prediction model is inherently coupled\nto the trading strategy it is used with, and vise versa. This highlights the\ndifficulty in developing models and strategies which are jointly optimal, but\nalso points to avenues of investigation which are broader than prevailing\napproaches. Our LSTM model is structurally simple and generates predictions\nbased on price observations over a modest number of past trading days. The\nmodel's architecture is tuned to promote profitability, as opposed to accuracy,\nunder a strategy that does not trade simply based on whether the price is\npredicted to rise or fall, but rather takes advantage of the distribution of\npredicted returns, and the fact that a prediction's position within that\ndistribution carries useful information about the expected profitability of a\ntrade. The proposed model and trading strategy were tested on the S&P 500, Dow\nJones Industrial Average (DJIA), NASDAQ and Russel 2000 stock indices, and\nachieved cumulative returns of 340%, 185%, 371% and 360%, respectively, over\n2010-2018, far outperforming the benchmark buy-and-hold strategy as well as\nother recent efforts.\n"
    },
    {
        "paper_id": 1902.0331,
        "authors": "Ali Ardeshiri, Joffre Swait, Elizabeth C. Heagney, Mladen Kovac",
        "title": "Preserve or retreat? Willingness-to-pay for Coastline Protection in New\n  South Wales",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1902.02418",
        "journal-ref": null,
        "doi": "10.1016/j.ocecoaman.2019.05.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Coastal erosion is a global and pervasive phenomenon that predicates a need\nfor a strategic approach to the future management of coastal values and assets\n(both built and natural), should we invest in protective structures like\nseawalls that aim to preserve specific coastal features, or allow natural\ncoastline retreat to preserve sandy beaches and other coastal ecosystems.\nDetermining the most suitable management approach in a specific context\nrequires a better understanding of the full suite of economic values the\npopulations holds for coastal assets, including non-market values. In this\nstudy, we characterise New South Wales residents willingness to pay to maintain\nsandy beaches (width and length). We use an innovative application of a Latent\nClass Binary Logit model to deal with Yea-sayers and Nay-sayers, as well as\nrevealing the latent heterogeneity among sample members. We find that 65% of\nthe population would be willing to pay some amount of levy, dependent on the\npolicy setting. In most cases, there is no effect of degree of beach\ndeterioration characterised as loss of width and length of sandy beaches of\nbetween 5% and 100% on respondents willingness to pay for a management levy.\nThis suggests that respondents who agreed to pay a management levy were\nmotivated to preserve sandy beaches in their current state irrespective of the\nseverity of sand loss likely to occur as a result of coastal erosion.\nWillingness to pay also varies according to beach type (amongst Iconic, Main,\nBay and Surf beaches) a finding that can assist with spatial prioritisation of\ncoastal management. Not recognizing the presence of nay-sayers in the data or\nrecognizing them but eliminating them from the estimation will result in biased\nWTP results and, consequently, biased policy propositions by coastal managers.\n"
    },
    {
        "paper_id": 1902.0335,
        "authors": "Nick James, Roman Marchant, Richard Gerlach, Sally Cripps",
        "title": "Bayesian Nonparametric Adaptive Spectral Density Estimation for\n  Financial Time Series",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Discrimination between non-stationarity and long-range dependency is a\ndifficult and long-standing issue in modelling financial time series. This\npaper uses an adaptive spectral technique which jointly models the\nnon-stationarity and dependency of financial time series in a non-parametric\nfashion assuming that the time series consists of a finite, but unknown number,\nof locally stationary processes, the locations of which are also unknown. The\nmodel allows a non-parametric estimate of the dependency structure by modelling\nthe auto-covariance function in the spectral domain. All our estimates are made\nwithin a Bayesian framework where we use aReversible Jump Markov Chain Monte\nCarlo algorithm for inference. We study the frequentist properties of our\nestimates via a simulation study, and present a novel way of generating time\nseries data from a nonparametric spectrum. Results indicate that our techniques\nperform well across a range of data generating processes. We apply our method\nto a number of real examples and our results indicate that several financial\ntime series exhibit both long-range dependency and non-stationarity.\n"
    },
    {
        "paper_id": 1902.03457,
        "authors": "Fr\\'ed\\'eric Bucci, Fabrizio Lillo, Jean-Philippe Bouchaud and Michael\n  Benzaquen",
        "title": "Are trading invariants really invariant? Trading costs matter",
        "comments": "13 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the trading invariance hypothesis recently proposed by Kyle and\nObizhaeva by empirically investigating a large dataset of bets, or metaorders,\nprovided by ANcerno. The hypothesis predicts that the quantity\n$I:=\\ri/N^{3/2}$, where $\\ri$ is the exchanged risk (volatility $\\times$ volume\n$\\times$ price) and $N$ is the number of bets, is invariant. We find that the\n$3/2$ scaling between $\\ri$ and $N$ works well and is robust against changes of\nyear, market capitalisation and economic sector. However our analysis clearly\nshows that $I$ is not invariant. We find a very high correlation $R^2>0.8$\nbetween $I$ and the total trading cost (spread and market impact) of the bet.\nWe propose new invariants defined as a ratio of $I$ and costs and find a large\ndecrease in variance. We show that the small dispersion of the new invariants\nis mainly driven by (i) the scaling of the spread with the volatility per\ntransaction, (ii) the near invariance of the distribution of metaorder size and\nof the volume and number fractions of bets across stocks.\n"
    },
    {
        "paper_id": 1902.0361,
        "authors": "Luca Capriotti and Ruggero Vaia",
        "title": "Physics and Derivatives: Effective-Potential Path-Integral\n  Approximations of Arrow-Debreu Densities",
        "comments": "12 pages, 4 figures",
        "journal-ref": "Journal of Derivatives 28, 8-25 (2020)",
        "doi": "10.3905/jod.2020.1.107",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how effective-potential path-integrals methods, stemming on a simple\nand nice idea originally due to Feynman and successfully employed in Physics\nfor a variety of quantum thermodynamics applications, can be used to develop an\naccurate and easy-to-compute semi-analytical approximation of transition\nprobabilities and Arrow-Debreu densities for arbitrary diffusions. We\nillustrate the accuracy of the method by presenting results for the\nBlack-Karasinski and the GARCH linear models, for which the proposed\napproximation provides remarkably accurate results, even in regimes of high\nvolatility, and for multi-year time horizons. The accuracy and the\ncomputational efficiency of the proposed approximation makes it a viable\nalternative to fully numerical schemes for a variety of derivatives pricing\napplications.\n"
    },
    {
        "paper_id": 1902.03714,
        "authors": "Achraf Bahamou, Maud Doumergue, Philippe Donnat",
        "title": "Hawkes processes for credit indices time series analysis: How random are\n  trades arrival times?",
        "comments": "ITISE 2018 International Conference on Time Series and Forecasting\n  accepted paper",
        "journal-ref": "Proceedings - International Conference on Time Series and\n  Forecasting, ITISE 2018. Granada: University of Granada, pp. 1178-1192",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Targeting a better understanding of credit market dynamics, the authors have\nstudied a stochastic model named the Hawkes process. Describing trades arrival\ntimes, this kind of model allows for the capture of self-excitement and mutual\ninteractions phenomena. The authors propose here a simple yet conclusive method\nfor fitting multidimensional Hawkes processes with exponential kernels, based\non a maximum likelihood non-convex optimization. The method was successfully\ntested on simulated data, then used on new publicly available real trading data\nfor three European credit indices, thus enabling quantification of\nself-excitement as well as volume impacts or cross indices influences.\n"
    },
    {
        "paper_id": 1902.03797,
        "authors": "Masato Hisakado and Shintaro Mori",
        "title": "Phase transition in the Bayesian estimation of the default portfolio",
        "comments": "24 pages, 9 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications Volume 544,\n  15 April 2020, 123480",
        "doi": "10.1016/j.physa.2019.123480",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The probability of default (PD) estimation is an important process for\nfinancial institutions. The difficulty of the estimation depends on the\ncorrelations between borrowers. In this paper, we introduce a hierarchical\nBayesian estimation method using the beta binomial distribution and consider a\nmulti-year case with a temporal correlation. A phase transition occurs when the\ntemporal correlation decays by power decay. When the power index is less than\none, the PD estimator does not converge. It is difficult to estimate the PD\nwith limited historical data. Conversely, when the power index is greater than\none, the convergence is the same as that of the binomial distribution. We\nprovide a condition for the estimation of the PD and discuss the universality\nclass of the phase transition. We investigate the empirical default data\nhistory of rating agencies and their Fourier transformations to confirm the\nform of the correlation decay. The power spectrum of the decay history seems to\nbe 1/f, which corresponds to a long memory. But the estimated power index is\nmuch greater than one. If we collect adequate historical data,the parameters\ncan be estimated correctly.\n"
    },
    {
        "paper_id": 1902.03982,
        "authors": "Marco Bottone and Mauro Bernardi and Lea Petrella",
        "title": "Unified Bayesian Conditional Autoregressive Risk Measures using the Skew\n  Exponential Power Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conditional Autoregressive Value-at-Risk and Conditional Autoregressive\nExpectile have become two popular approaches for direct measurement of market\nrisk. Since their introduction several improvements both in the Bayesian and in\nthe classical framework have been proposed to better account for asymmetry and\nlocal non-linearity. Here we propose a unified Bayesian Conditional\nAutoregressive Risk Measures approach by using the Skew Exponential Power\ndistribution. Further, we extend the proposed models using a semiparametric\nP-spline approximation answering for a flexible way to consider the presence of\nnon-linearity. To make the statistical inference we adapt the MCMC algorithm\nproposed in Bernardi et al. (2018) to our case. The effectiveness of the whole\napproach is demonstrated using real data on daily return of five stock market\nindices.\n"
    },
    {
        "paper_id": 1902.04367,
        "authors": "Kathrin Glau, Daniel Kressner, Francesco Statti",
        "title": "Low-rank tensor approximation for Chebyshev interpolation in parametric\n  option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Treating high dimensionality is one of the main challenges in the development\nof computational methods for solving problems arising in finance, where tasks\nsuch as pricing, calibration, and risk assessment need to be performed\naccurately and in real-time. Among the growing literature addressing this\nproblem, Gass et al. [14] propose a complexity reduction technique for\nparametric option pricing based on Chebyshev interpolation. As the number of\nparameters increases, however, this method is affected by the curse of\ndimensionality. In this article, we extend this approach to treat\nhigh-dimensional problems: Additionally exploiting low-rank structures allows\nus to consider parameter spaces of high dimensions. The core of our method is\nto express the tensorized interpolation in tensor train (TT) format and to\ndevelop an efficient way, based on tensor completion, to approximate the\ninterpolation coefficients. We apply the new method to two model problems:\nAmerican option pricing in the Heston model and European basket option pricing\nin the multi-dimensional Black-Scholes model. In these examples we treat\nparameter spaces of dimensions up to 25. The numerical results confirm the\nlow-rank structure of these problems and the effectiveness of our method\ncompared to advanced techniques.\n"
    },
    {
        "paper_id": 1902.04437,
        "authors": "Hai-Chuan Xu, Gao-Feng Gu and Wei-Xing Zhou (ECUST)",
        "title": "Direct determination approach for the multifractal detrending moving\n  average analysis",
        "comments": "12 pages including 8 figures",
        "journal-ref": "Physical Review E 96 (5), 052201 (2017)",
        "doi": "10.1103/PhysRevE.96.052201",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the canonical framework, we propose an alternative approach for the\nmultifractal analysis based on the detrending moving average method (MF-DMA).\nWe define a canonical measure such that the multifractal mass exponent\n$\\tau(q)$ is related to the partition function and the multifractal spectrum\n$f(\\alpha)$ can be directly determined. The performances of the direct\ndetermination approach and the traditional approach of the MF-DMA are compared\nbased on three synthetic multifractal and monofractal measures generated from\nthe one-dimensional $p$-model, the two-dimensional $p$-model and the fractional\nBrownian motions. We find that both approaches have comparable performances to\nunveil the fractal and multifractal nature. In other words, without loss of\naccuracy, the multifractal spectrum $f(\\alpha)$ can be directly determined\nusing the new approach with less computation cost. We also apply the new MF-DMA\napproach to the volatility time series of stock prices and confirm the presence\nof multifractality.\n"
    },
    {
        "paper_id": 1902.04456,
        "authors": "Hadrien De March (ECOLE POLYTECHNIQUE, QANTEV), Pierre Henry-Labordere\n  (SOCIETE GENERALE)",
        "title": "Building arbitrage-free implied volatility: Sinkhorn's algorithm and\n  variants",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the classical problem of building an arbitrage-free implied\nvolatility surface from bid-ask quotes. We design a fast numerical procedure,\nfor which we prove the convergence, based on the Sinkhorn algorithm that has\nbeen recently used to solve efficiently (martingale) optimal transport\nproblems.\n"
    },
    {
        "paper_id": 1902.04489,
        "authors": "Tobias Fissler and Johanna F. Ziegel",
        "title": "Evaluating Range Value at Risk Forecasts",
        "comments": "25 pages, 2 figures An earlier version of this paper was circulated\n  under the name 'Elicitability of Range Value at Risk'. The presentation has\n  been made more concise and minor errors have been corrected. Statistics &\n  Risk Modeling, 2021",
        "journal-ref": "Statistics & Risk Modeling, vol. 38, no. 1-2, 2021, pp. 25-46",
        "doi": "10.1515/strm-2020-0037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The debate of what quantitative risk measure to choose in practice has mainly\nfocused on the dichotomy between Value at Risk (VaR) -- a quantile -- and\nExpected Shortfall (ES) -- a tail expectation. Range Value at Risk (RVaR) is a\nnatural interpolation between these two prominent risk measures, which\nconstitutes a tradeoff between the sensitivity of the latter and the robustness\nof the former, turning it into a practically relevant risk measure on its own.\nAs such, there is a need to statistically validate RVaR forecasts and to\ncompare and rank the performance of different RVaR models, tasks subsumed under\nthe term 'backtesting' in finance. The predictive performance is best evaluated\nand compared in terms of strictly consistent loss or scoring functions. That\nis, functions which are minimised in expectation by the correct RVaR forecast.\nMuch like ES, it has been shown recently that RVaR does not admit strictly\nconsistent scoring functions, i.e., it is not elicitable. Mitigating this\nnegative result, this paper shows that a triplet of RVaR with two VaR\ncomponents at different levels is elicitable. We characterise the class of\nstrictly consistent scoring functions for this triplet. Additional properties\nof these scoring functions are examined, including the diagnostic tool of\nMurphy diagrams. The results are illustrated with a simulation study, and we\nput our approach in perspective with respect to the classical approach of\ntrimmed least squares in robust regression.\n"
    },
    {
        "paper_id": 1902.04517,
        "authors": "Abeer ElBahrawy, Laura Alessandretti, Andrea Baronchelli",
        "title": "Wikipedia and Digital Currencies: Interplay Between Collective Attention\n  and Market Performance",
        "comments": null,
        "journal-ref": "Frontiers in Blockchain 2:12 (2019)",
        "doi": "10.3389/fbloc.2019.00012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The production and consumption of information about Bitcoin and other\ndigital-, or 'crypto'-, currencies have grown together with their market\ncapitalisation. However, a systematic investigation of the relationship between\nonline attention and market dynamics, across multiple digital currencies, is\nstill lacking. Here, we quantify the interplay between the attention towards\ndigital currencies in Wikipedia and their market performance. We consider the\nentire edit history of currency-related pages, and their view history from July\n2015. First, we quantify the evolution of the cryptocurrency presence in\nWikipedia by analysing the editorial activity and the network of co-edited\npages. We find that a small community of tightly connected editors is\nresponsible for most of the production of information about cryptocurrencies in\nWikipedia. Then, we show that a simple trading strategy informed by Wikipedia\nviews performs better, in terms of returns on investment, than classic baseline\nstrategies for most of the covered period. Our results contribute to the recent\nliterature on the interplay between online information and investment markets,\nand we anticipate it will be of interest for researchers as well as investors.\n"
    },
    {
        "paper_id": 1902.04613,
        "authors": "Jaehyuk Park, Ian Wood, Elise Jing, Azadeh Nematzadeh, Souvik Ghosh,\n  Michael Conover, and Yong-Yeol Ahn",
        "title": "Global labor flow network reveals the hierarchical organization and\n  dynamics of geo-industrial clusters in the world economy",
        "comments": null,
        "journal-ref": "Nature Communicationsvolume 10, Article number: 3449 (2019)",
        "doi": "10.1038/s41467-019-11380-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Groups of firms often achieve a competitive advantage through the formation\nof geo-industrial clusters. Although many exemplary clusters, such as Hollywood\nor Silicon Valley, have been frequently studied, systematic approaches to\nidentify and analyze the hierarchical structure of the geo-industrial clusters\nat the global scale are rare. In this work, we use LinkedIn's employment\nhistories of more than 500 million users over 25 years to construct a labor\nflow network of over 4 million firms across the world and apply a recursive\nnetwork community detection algorithm to reveal the hierarchical structure of\ngeo-industrial clusters. We show that the resulting geo-industrial clusters\nexhibit a stronger association between the influx of educated-workers and\nfinancial performance, compared to existing aggregation units. Furthermore, our\nadditional analysis of the skill sets of educated-workers supplements the\nrelationship between the labor flow of educated-workers and productivity\ngrowth. We argue that geo-industrial clusters defined by labor flow provide\nbetter insights into the growth and the decline of the economy than other\ncommon economic units.\n"
    },
    {
        "paper_id": 1902.0469,
        "authors": "Brian F. Tivnan, David Rushing Dewhurst, Colin M. Van Oort, John H.\n  Ring IV, Tyler J. Gray, Brendan F. Tivnan, Matthew T. K. Koehler, Matthew T.\n  McMahon, David Slater, Jason Veneman, Christopher M. Danforth",
        "title": "Fragmentation and inefficiencies in US equity markets: Evidence from the\n  Dow 30",
        "comments": "30 pages, 9 tables, 10 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0226968",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the most comprehensive source of commercially available data on the US\nNational Market System, we analyze all quotes and trades associated with Dow 30\nstocks in 2016 from the vantage point of a single and fixed frame of reference.\nWe find that inefficiencies created in part by the fragmentation of the equity\nmarketplace are relatively common and persist for longer than what physical\nconstraints may suggest. Information feeds reported different prices for the\nsame equity more than 120 million times, with almost 64 million dislocation\nsegments featuring meaningfully longer duration and higher magnitude. During\nthis period, roughly 22% of all trades occurred while the SIP and aggregated\ndirect feeds were dislocated. The current market configuration resulted in a\nrealized opportunity cost totaling over $160 million when compared with a\nsingle feed, single exchange alternative---a conservative estimate that does\nnot take into account intra-day offsetting events.\n"
    },
    {
        "paper_id": 1902.04691,
        "authors": "John H. Ring IV, Colin M. Van Oort, David R. Dewhurst, Tyler J. Gray,\n  Christopher M. Danforth, and Brian F. Tivnan",
        "title": "Scaling of inefficiencies in the U.S. equity markets: Evidence from\n  three market indices and more than 2900 securities",
        "comments": "35 pages, 16 figures, 24 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the most comprehensive, commercially-available dataset of trading\nactivity in U.S. equity markets, we catalog and analyze quote dislocations\nbetween the SIP National Best Bid and Offer (NBBO) and a synthetic BBO\nconstructed from direct feeds. We observe a total of over 3.1 billion\ndislocation segments in the Russell 3000 during trading in 2016, roughly 525\nper second of trading. However, these dislocations do not occur uniformly\nthroughout the trading day. We identify a characteristic structure that\nfeatures more dislocations near the open and close. Additionally, around 23% of\nobserved trades executed during dislocations. These trades may have been\nimpacted by stale information, leading to estimated opportunity costs on the\norder of $ 2 billion USD. A subset of the constituents of the S&P 500 index\nexperience the greatest amount of opportunity cost and appear to drive\ninefficiencies in other stocks. These results quantify impacts of the physical\nstructure of the U.S. National Market System.\n"
    },
    {
        "paper_id": 1902.0494,
        "authors": "Didier Sornette, Spencer Wheatley and Peter Cauwels",
        "title": "The fair reward problem: the illusion of success and how to solve it",
        "comments": "41 pages, 12 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Humanity has been fascinated by the pursuit of fortune since time immemorial,\nand many successful outcomes benefit from strokes of luck. But success is\nsubject to complexity, uncertainty, and change - and at times becoming\nincreasingly unequally distributed. This leads to tension and confusion over to\nwhat extent people actually get what they deserve (i.e., fairness/meritocracy).\nMoreover, in many fields, humans are over-confident and pervasively confuse\nluck for skill (I win, it's skill; I lose, it's bad luck). In some fields,\nthere is too much risk taking; in others, not enough. Where success derives in\nlarge part from luck - and especially where bailouts skew the incentives\n(heads, I win; tails, you lose) - it follows that luck is rewarded too much.\nThis incentivizes a culture of gambling, while downplaying the importance of\nproductive effort. And, short term success is often rewarded, irrespective, and\npotentially at the detriment, of the long-term system fitness. However, much\nsuccess is truly meritocratic, and the problem is to discern and reward based\non merit. We call this the fair reward problem. To address this, we propose\nthree different measures to assess merit: (i) raw outcome; (ii) risk adjusted\noutcome, and (iii) prospective. We emphasize the need, in many cases, for the\ndeductive prospective approach, which considers the potential of a system to\nadapt and mutate in novel futures. This is formalized within an evolutionary\nsystem, comprised of five processes, inter alia handling the\nexploration-exploitation trade-off. Several human endeavors - including\nfinance, politics, and science -are analyzed through these lenses, and concrete\nsolutions are proposed to support a prosperous and meritocratic society.\n"
    },
    {
        "paper_id": 1902.04954,
        "authors": "Yan Wang, Xuelei Sherry Ni",
        "title": "Risk Prediction of Peer-to-Peer Lending Market by a LSTM Model with\n  Macroeconomic Factor",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3374135.3385287",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the peer to peer (P2P) lending platform, investors hope to maximize their\nreturn while minimizing the risk through a comprehensive understanding of the\nP2P market. A low and stable average default rate across all the borrowers\ndenotes a healthy P2P market and provides investors more confidence in a\npromising investment. Therefore, having a powerful model to describe the trend\nof the default rate in the P2P market is crucial. Different from previous\nstudies that focus on modeling the default rate at the individual level, in\nthis paper, we are the first to comprehensively explore the monthly trend of\nthe default rate at the aggregative level for the P2P data from October 2007 to\nJanuary 2016 in the US. We use the long short term memory (LSTM) approach to\nsequentially predict the default risk of the borrowers in Lending Club, which\nis the largest P2P lending platform in the US. Although being first applied in\nmodeling the P2P sequential data, the LSTM approach shows its great potential\nby outperforming traditionally utilized time series models in our experiments.\nFurthermore, incorporating the macroeconomic feature \\textit{unemp\\_rate}\n(i.e., unemployment rate) can improve the LSTM performance by decreasing RMSE\non both the training and the testing datasets. Our study can broaden the\napplications of the LSTM algorithm by using it on the sequential P2P data and\nguide the investors in making investment strategies.\n"
    },
    {
        "paper_id": 1902.05287,
        "authors": "Simon F\\'ecamp, Joseph Mikael, Xavier Warin",
        "title": "Risk management with machine-learning-based algorithms",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose some machine-learning-based algorithms to solve hedging problems\nin incomplete markets. Sources of incompleteness cover illiquidity, untradable\nrisk factors, discrete hedging dates and transaction costs. The proposed\nalgorithms resulting strategies are compared to classical stochastic control\ntechniques on several payoffs using a variance criterion. One of the proposed\nalgorithm is flexible enough to be used with several existing risk criteria. We\nfurthermore propose a new moment-based risk criteria.\n"
    },
    {
        "paper_id": 1902.05418,
        "authors": "Emilio Said, Ahmed Bel Hadj Ayed, Damien Thillou, Jean-Jacques\n  Rabeyrin, Fr\\'ed\\'eric Abergel",
        "title": "Market Impact: A Systematic Study of the High Frequency Options Market",
        "comments": "arXiv admin note: text overlap with arXiv:1802.08502",
        "journal-ref": "Quantitative Finance, 2021, vol. 21, no 1, p. 69-84",
        "doi": "10.1080/14697688.2020.1791948",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with a fundamental subject that has seldom been addressed in\nrecent years, that of market impact in the options market. Our analysis is\nbased on a proprietary database of metaorders-large orders that are split into\nsmaller pieces before being sent to the market on one of the main Asian\nmarkets. In line with our previous work on the equity market [Said et al.,\n2018], we propose an algorithmic approach to identify metaorders, based on some\nimplied volatility parameters, the at the money forward volatility and at the\nmoney forward skew. In both cases, we obtain results similar to the now well\nunderstood equity market: Square-root law, Fair Pricing Condition and Market\nImpact Dynamics.\n"
    },
    {
        "paper_id": 1902.0571,
        "authors": "Jean-Charles Richard, Thierry Roncalli",
        "title": "Constrained Risk Budgeting Portfolios: Theory, Algorithms, Applications\n  & Puzzles",
        "comments": "36 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article develops the theory of risk budgeting portfolios, when we would\nlike to impose weight constraints. It appears that the mathematical problem is\nmore complex than the traditional risk budgeting problem. The formulation of\nthe optimization program is particularly critical in order to determine the\nright risk budgeting portfolio. We also show that numerical solutions can be\nfound using methods that are used in large-scale machine learning problems.\nIndeed, we develop an algorithm that mixes the method of cyclical coordinate\ndescent (CCD), alternating direction method of multipliers (ADMM), proximal\noperators and Dykstra's algorithm. This theoretical body is then applied to\nsome investment problems. In particular, we show how to dynamically control the\nturnover of a risk parity portfolio and how to build smart beta portfolios\nbased on the ERC approach by improving the liquidity of the portfolio or\nreducing the small cap bias. Finally, we highlight the importance of the\nhomogeneity property of risk measures and discuss the related scaling puzzle.\n"
    },
    {
        "paper_id": 1902.0581,
        "authors": "Ali Hirsa, Tugce Karatas, Amir Oskoui",
        "title": "Supervised Deep Neural Networks (DNNs) for Pricing/Calibration of\n  Vanilla/Exotic Options Under Various Different Processes",
        "comments": "17 pages, 28 figures and tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply supervised deep neural networks (DNNs) for pricing and calibration\nof both vanilla and exotic options under both diffusion and pure jump processes\nwith and without stochastic volatility. We train our neural network models\nunder different number of layers, neurons per layer, and various different\nactivation functions in order to find which combinations work better\nempirically. For training, we consider various different loss functions and\noptimization routines. We demonstrate that deep neural networks exponentially\nexpedite option pricing compared to commonly used option pricing methods which\nconsequently make calibration and parameter estimation super fast.\n"
    },
    {
        "paper_id": 1902.05938,
        "authors": "Donovan Platt",
        "title": "A Comparison of Economic Agent-Based Model Calibration Methods",
        "comments": "32 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interest in agent-based models of financial markets and the wider economy has\nincreased consistently over the last few decades, in no small part due to their\nability to reproduce a number of empirically-observed stylised facts that are\nnot easily recovered by more traditional modelling approaches. Nevertheless,\nthe agent-based modelling paradigm faces mounting criticism, focused\nparticularly on the rigour of current validation and calibration practices,\nmost of which remain qualitative and stylised fact-driven. While the literature\non quantitative and data-driven approaches has seen significant expansion in\nrecent years, most studies have focused on the introduction of new calibration\nmethods that are neither benchmarked against existing alternatives nor\nrigorously tested in terms of the quality of the estimates they produce. We\ntherefore compare a number of prominent ABM calibration methods, both\nestablished and novel, through a series of computational experiments in an\nattempt to determine the respective strengths and weaknesses of each approach\nand the overall quality of the resultant parameter estimates. We find that\nBayesian estimation, though less popular in the literature, consistently\noutperforms frequentist, objective function-based approaches and results in\nreasonable parameter estimates in many contexts. Despite this, we also find\nthat agent-based model calibration techniques require further development in\norder to definitively calibrate large-scale models. We therefore make\nsuggestions for future research.\n"
    },
    {
        "paper_id": 1902.06053,
        "authors": "Vassilis Polimenis and Ioannis Neokosmidis",
        "title": "Non-Stationary Dividend-Price Ratios",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1057/s41260-019-00143-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dividend yields have been widely used in previous research to relate stock\nmarket valuations to cash flow fundamentals. However, this approach relies on\nthe assumption that dividend yields are stationary. Due to the failure to\nreject the hypothesis of a unit root in the classical dividend-price ratio for\nthe US stock market, Polimenis and Neokosmidis (2016) proposed the use of a\nmodified dividend price ratio (mdp) as the deviation between d and p from their\nlong run equilibrium, and showed that mdp provides substantially improved\nforecasting results over the classical dp ratio. Here, we extend that paper by\nperforming multivariate regressions based on the Campbell-Shiller\napproximation, by utilizing a dynamic econometric procedure to estimate the\nmodified dp, and by testing the modified ratios against reinvested\ndividend-yields. By comparing the performance of mdp and dp in the period after\n1965, we are not only able to enhance the robustness of the findings, but also\nto debunk a possible false explanation that the enhanced mdp performance in\npredicting future returns comes from a capacity to predict the risk-free return\ncomponent. Depending on whether one uses the recursive or population\nmethodology to measure the performance of mdp, the Out-of-Sample performance\ngain is between 30% to 50%.\n"
    },
    {
        "paper_id": 1902.06175,
        "authors": "Jason S. Anquandah and Leonid V. Bogachev",
        "title": "Optimal Stopping and Utility in a Simple Model of Unemployment Insurance",
        "comments": "45 pages, 8 figures",
        "journal-ref": "Risks, vol. 7 (2019), issue 3, paper #94, pages 1-41; Special\n  Issue \"Applications of Stochastic Optimal Control to Economics and Finance\"",
        "doi": "10.3390/risks7030094",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing unemployment is one of the key issues in social policies.\nUnemployment insurance schemes are designed to cushion the financial and morale\nblow of loss of job but also to encourage the unemployed to seek new jobs more\npro-actively due to the continuous reduction of benefit payments. In the\npresent paper, a simple model of unemployment insurance is proposed with a\nfocus on optimality of the individual's entry to the scheme. The corresponding\noptimal stopping problem is solved, and its similarity and differences with the\nperpetual American call option are discussed. Beyond a purely financial point\nof view, we argue that in the actuarial context the optimal decisions should\ntake into account other possible preferences through a suitable utility\nfunction. Some examples in this direction are worked out.\n"
    },
    {
        "paper_id": 1902.06294,
        "authors": "Kristoffer Lindensj\\\"o and Filip Lindskog",
        "title": "Optimal dividends and capital injection under dividend restrictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a singular stochastic control problem faced by the owner of an\ninsurance company that dynamically pays dividends and raises capital in the\npresence of the restriction that the surplus process must be above a given\ndividend payout barrier in order for dividend payments to be allowed.\nBankruptcy occurs if the surplus process becomes negative and there are\nproportional costs for capital injection. We show that one of the following\nstrategies is optimal: (i) Pay dividends and inject capital in order to reflect\nthe surplus process at an upper barrier and at 0, implying bankruptcy never\noccurs. (ii) Pay dividends in order to reflect the surplus process at an upper\nbarrier and never inject capital --- corresponding to absorption at 0 ---\nimplying bankruptcy occurs the first time the surplus reaches zero. We show\nthat if the costs of capital injection are low, then a sufficiently high\ndividend payout barrier will change the optimal strategy from type (i) (without\nbankruptcy) to type (ii) (with bankruptcy). Moreover, if the costs are high,\nthen the optimal strategy is of type (ii) regardless of the dividend payout\nbarrier. The uncontrolled surplus process is a Wiener process with drift.\n"
    },
    {
        "paper_id": 1902.06483,
        "authors": "Lasko Basnarkov, Viktor Stojkoski, Zoran Utkovski and Ljupco Kocarev",
        "title": "Correlation Patterns in Foreign Exchange Markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.04.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The value of an asset in a financial market is given in terms of another\nasset known as numeraire. The dynamics of the value is non-stationary and\nhence, to quantify the relationships between different assets, one requires\nconvenient measures such as the means and covariances of the respective log\nreturns. Here, we develop transformation equations for these means and\ncovariances when one changes the numeraire. The results are verified by a\nthorough empirical analysis capturing the dynamics of numerous assets in a\nforeign exchange market. We show that the partial correlations between pairs of\nassets are invariant under the change of the numeraire. This observable\nquantifies the relationship between two assets, while the influence of the rest\nis removed. As such the partial correlations uncover intriguing observations\nwhich may not be easily noticed in the ordinary correlation analysis.\n"
    },
    {
        "paper_id": 1902.06505,
        "authors": "L. Di Persio, I. Oliva. K. Wallbaum",
        "title": "Options on CPPI with guaranteed minimum equity exposure",
        "comments": "19 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper we provide a two-step principal protection strategy\nobtained by combining a modification of the Constant Proportion Portfolio\nInsurance (CPPI) algorithm and a classical Option Based Portfolio Insurance\n(OBPI) mechanism. Such a novel approach consists in assuming that the\npercentage of wealth invested in stocks cannot go under a fixed level, called\nguaranteed minimum equity exposure, and using such an adjusted CPPI portfolio\nas the underlying of an option. The first stage ensures to overcome the so\ncalled cash-in risk, typically related to a standard CPPI technique, while the\nsecond one guarantees the equity market participation. To show the\neffectiveness of our proposal we provide a detailed computational analysis\nwithin the Heston-Vasicek framework, numerically comparing the evaluation of\nthe price of European plain vanilla options when the underlying is either a\npurely risky asset, a standard CPPI portfolio and a CPPI with guaranteed\nminimum equity exposure.\n"
    },
    {
        "paper_id": 1902.06549,
        "authors": "Aleksandra Alori\\'c, Peter Sollich",
        "title": "Market fragmentation and market consolidation: Multiple steady states in\n  systems of adaptive traders choosing where to trade",
        "comments": "25 pages, 17 figures",
        "journal-ref": "Phys. Rev. E 99, 062309 (2019)",
        "doi": "10.1103/PhysRevE.99.062309",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological progress is leading to proliferation and diversification of\ntrading venues, thus increasing the relevance of the long-standing question of\nmarket fragmentation versus consolidation. To address this issue\nquantitatively, we analyse systems of adaptive traders that choose where to\ntrade based on their previous experience. We demonstrate that only based on\naggregate parameters about trading venues, such as the demand to supply ratio,\nwe can assess whether a population of traders will prefer fragmentation or\nspecialization towards a single venue. We investigate what conditions lead to\nmarket fragmentation for populations with a long memory and analyse the\nstability and other properties of both fragmented and consolidated steady\nstates. Finally we investigate the dynamics of populations with finite memory;\nwhen this memory is long the true long-time steady states are consolidated but\nfragmented states are strongly metastable, dominating the behaviour out to long\ntimes.\n"
    },
    {
        "paper_id": 1902.06623,
        "authors": "Roberto Baviera, Giulia Bianchi",
        "title": "Model risk in mean-variance portfolio selection: an analytic solution to\n  the worst-case approach",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the worst-case model risk approach described in\nGlasserman and Xu (2014). Portfolio selection with model risk can be a\nchallenging operational research problem. In particular, it presents an\nadditional optimisation compared to the classical one. We find the analytical\nsolution for the optimal mean-variance portfolio selection in the worst-case\nscenario approach. In the minimum-variance case, we prove that the analytical\nsolution is significantly different from the one found numerically by\nGlasserman and Xu (2014) and that model risk reduces to an estimation risk. A\ndetailed numerical example is provided.\n"
    },
    {
        "paper_id": 1902.06883,
        "authors": "Jean-Pierre Fouque, Ruimeng Hu",
        "title": "Multiscale Asymptotic Analysis for Portfolio Optimization under\n  Stochastic Environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical studies indicate the presence of multi-scales in the volatility of\nunderlying assets: a fast-scale on the order of days and a slow-scale on the\norder of months. In our previous works, we have studied the portfolio\noptimization problem in a Markovian setting under each single scale, the slow\none in [Fouque and Hu, SIAM J. Control Optim., 55 (2017), 1990-2023], and the\nfast one in [Hu, Proceedings of IEEE CDC 2018, accepted]. This paper is\ndedicated to the analysis when the two scales coexist in a Markovian setting.\nWe study the terminal wealth utility maximization problem when the volatility\nis driven by both fast- and slow-scale factors. We first propose a zeroth-order\nstrategy, and rigorously establish the first order approximation of the\nassociated problem value. This is done by analyzing the corresponding linear\npartial differential equation (PDE) via regular and singular perturbation\ntechniques, as in the single-scale cases. Then, we show the asymptotic\noptimality of our proposed strategy within a specific family of admissible\ncontrols. Interestingly, we highlight that a pure PDE approach does not work in\nthe multi-scale case and, instead, we use the so-called epsilon-martingale\ndecomposition. This completes the analysis of portfolio optimization in both\nfast mean-reverting and slowly-varying Markovian stochastic environments.\n"
    },
    {
        "paper_id": 1902.06941,
        "authors": "Nicole B\\\"auerle and Tomer Shushi",
        "title": "Risk Management with Tail Quasi-Linear Means",
        "comments": null,
        "journal-ref": "Ann. actuar. sci. 14 (2020) 170-187",
        "doi": "10.1017/S1748499519000113",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize Quasi-Linear Means by restricting to the tail of the risk\ndistribution and show that this can be a useful quantity in risk management\nsince it comprises in its general form the Value at Risk, the Tail Value at\nRisk and the Entropic Risk Measure in a unified way. We then investigate the\nfundamental properties of the proposed measure and show its unique features and\nimplications in the risk measurement process. Furthermore, we derive formulas\nfor truncated elliptical models of losses and provide formulas for selected\nmembers of such models.\n"
    },
    {
        "paper_id": 1902.07355,
        "authors": "Avidit Acharya, Kirk Bansak, Jens Hainmueller",
        "title": "Combining Outcome-Based and Preference-Based Matching: A Constrained\n  Priority Mechanism",
        "comments": "This manuscript has been accepted for publication by Political\n  Analysis and will appear in a revised form subject to peer review and/or\n  input from the journal's editor. End-users of this manuscript may only make\n  use of it for private research and study and may not distribute it further",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a constrained priority mechanism that combines outcome-based\nmatching from machine-learning with preference-based allocation schemes common\nin market design. Using real-world data, we illustrate how our mechanism could\nbe applied to the assignment of refugee families to host country locations, and\nkindergarteners to schools. Our mechanism allows a planner to first specify a\nthreshold $\\bar g$ for the minimum acceptable average outcome score that should\nbe achieved by the assignment. In the refugee matching context, this score\ncorresponds to the predicted probability of employment, while in the student\nassignment context it corresponds to standardized test scores. The mechanism is\na priority mechanism that considers both outcomes and preferences by assigning\nagents (refugee families, students) based on their preferences, but subject to\nmeeting the planner's specified threshold. The mechanism is both strategy-proof\nand constrained efficient in that it always generates a matching that is not\nPareto dominated by any other matching that respects the planner's threshold.\n"
    },
    {
        "paper_id": 1902.07449,
        "authors": "Thibault Bourgeron, Edmond Lezmi, Thierry Roncalli",
        "title": "Robust Asset Allocation for Robo-Advisors",
        "comments": "67 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the last few years, the financial advisory industry has been impacted by\nthe emergence of digitalization and robo-advisors. This phenomenon affects\nmajor financial services, including wealth management, employee savings plans,\nasset managers, etc. Since the robo-advisory model is in its early stages, we\nestimate that robo-advisors will help to manage around $1 trillion of assets in\n2020 (OECD, 2017). And this trend is not going to stop with future generations,\nwho will live in a technology-driven and social media-based world. In the\ninvestment industry, robo-advisors face different challenges: client profiling,\ncustomization, asset pooling, liability constraints, etc. In its primary sense,\nrobo-advisory is a term for defining automated portfolio management. This\nincludes automated trading and rebalancing, but also automated portfolio\nallocation. And this last issue is certainly the most important challenge for\nrobo-advisory over the next five years. Today, in many robo-advisors, asset\nallocation is rather human-based and very far from being computer-based. The\nreason is that portfolio optimization is a very difficult task, and can lead to\noptimized mathematical solutions that are not optimal from a financial point of\nview (Michaud, 1989). The big challenge for robo-advisors is therefore to be\nable to optimize and rebalance hundreds of optimal portfolios without human\nintervention. In this paper, we show that the mean-variance optimization\napproach is mainly driven by arbitrage factors that are related to the concept\nof hedging portfolios. This is why regularization and sparsity are necessary to\ndefine robust asset allocation. However, this mathematical framework is more\ncomplex and requires understanding how norm penalties impacts portfolio\noptimization. From a numerical point of view, it also requires the\nimplementation of non-traditional algorithms based on ADMM methods.\n"
    },
    {
        "paper_id": 1902.07481,
        "authors": "Birte Ewers, Jonathan F. Donges, Jobst Heitzig, Sonja Peterson",
        "title": "Divestment may burst the carbon bubble if investors' beliefs tip to\n  anticipating strong future climate policy",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To achieve the ambitious aims of the Paris climate agreement, the majority of\nfossil-fuel reserves needs to remain underground. As current national\ngovernment commitments to mitigate greenhouse gas emissions are insufficient by\nfar, actors such as institutional and private investors and the social movement\non divestment from fossil fuels could play an important role in putting\npressure on national governments on the road to decarbonization. Using a\nstochastic agent-based model of co-evolving financial market and investors'\nbeliefs about future climate policy on an adaptive social network, here we find\nthat the dynamics of divestment from fossil fuels shows potential for social\ntipping away from a fossil-fuel based economy. Our results further suggest that\nsocially responsible investors have leverage: a small share of 10--20\\,\\% of\nsuch moral investors is sufficient to initiate the burst of the carbon bubble,\nconsistent with the Pareto Principle. These findings demonstrate that\ndivestment has potential for contributing to decarbonization alongside other\nsocial movements and policy instruments, particularly given the credible\nimminence of strong international climate policy. Our analysis also indicates\nthe possible existence of a carbon bubble with potentially destabilizing\neffects to the economy.\n"
    },
    {
        "paper_id": 1902.07855,
        "authors": "Avinash Barnwal, Hari Pad Bharti, Aasim Ali, and Vishal Singh",
        "title": "Stacking with Neural network for Cryptocurrency investment",
        "comments": "20 pages,7 figues",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the direction of assets have been an active area of study and a\ndifficult task. Machine learning models have been used to build robust models\nto model the above task. Ensemble methods is one of them showing results better\nthan a single supervised method. In this paper, we have used generative and\ndiscriminative classifiers to create the stack, particularly 3 generative and 6\ndiscriminative classifiers and optimized over one-layer Neural Network to model\nthe direction of price cryptocurrencies. Features used are technical indicators\nused are not limited to trend, momentum, volume, volatility indicators, and\nsentiment analysis has also been used to gain useful insight combined with the\nabove features. For Cross-validation, Purged Walk forward cross-validation has\nbeen used. In terms of accuracy, we have done a comparative analysis of the\nperformance of Ensemble method with Stacking and Ensemble method with blending.\nWe have also developed a methodology for combined features importance for the\nstacked model. Important indicators are also identified based on feature\nimportance.\n"
    },
    {
        "paper_id": 1902.07892,
        "authors": "Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis",
        "title": "Deep Adaptive Input Normalization for Time Series Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep Learning (DL) models can be used to tackle time series analysis tasks\nwith great success. However, the performance of DL models can degenerate\nrapidly if the data are not appropriately normalized. This issue is even more\napparent when DL is used for financial time series forecasting tasks, where the\nnon-stationary and multimodal nature of the data pose significant challenges\nand severely affect the performance of DL models. In this work, a simple, yet\neffective, neural layer, that is capable of adaptively normalizing the input\ntime series, while taking into account the distribution of the data, is\nproposed. The proposed layer is trained in an end-to-end fashion using\nback-propagation and leads to significant performance improvements compared to\nother evaluated normalization schemes. The proposed method differs from\ntraditional normalization methods since it learns how to perform normalization\nfor a given task instead of using a fixed normalization scheme. At the same\ntime, it can be directly applied to any new time series without requiring\nre-training. The effectiveness of the proposed method is demonstrated using a\nlarge-scale limit order book dataset, as well as a load forecasting dataset.\n"
    },
    {
        "paper_id": 1902.0792,
        "authors": "Denis Demidov, Klaus M. Frahm, Dima L. Shepelyansky",
        "title": "What is the central bank of Wikipedia?",
        "comments": null,
        "journal-ref": "Physica A 542, 123199 (2020)",
        "doi": "10.1016/j.physa.2019.123199",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the influence and interactions of 60 largest world banks for 195\nworld countries using the reduced Google matrix algorithm for the English\nWikipedia network with 5 416 537 articles. While the top asset rank positions\nare taken by the banks of China, with China Industrial and Commercial Bank of\nChina at the first place, we show that the network influence is dominated by\nUSA banks with Goldman Sachs being the central bank. We determine the network\nstructure of interactions of banks and countries and PageRank sensitivity of\ncountries to selected banks. We also present GPU oriented code which\nsignificantly accelerates the numerical computations of reduced Google matrix.\n"
    },
    {
        "paper_id": 1902.08405,
        "authors": "Mourad Berrahoui, Othmane Islah, Chris Kenyon",
        "title": "Revising SA-CCR",
        "comments": "20 pages, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  From SA-CCR to RSA-CCR: making SA-CCR self-consistent and appropriately\nrisk-sensitive by cashflow decomposition in a 3-Factor Gaussian Market Model\n"
    },
    {
        "paper_id": 1902.08483,
        "authors": "Sebastian M. Krause, Hrvoje \\v{S}tefan\\v{c}i\\'c, Vinko Zlati\\'c, Guido\n  Caldarelli",
        "title": "Controlling systemic risk - network structures that minimize it and node\n  properties to calculate it",
        "comments": "10 pages. 4 figures",
        "journal-ref": "Phys. Rev. E 103, 042304 (2021)",
        "doi": "10.1103/PhysRevE.103.042304",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Evaluation of systemic risk in networks of financial institutions in general\nrequires information of inter-institution financial exposures. In the framework\nof Debt Rank algorithm, we introduce an approximate method of systemic risk\nevaluation which requires only node properties, such as total assets and\nliabilities, as inputs. We demonstrate that this approximation captures a large\nportion of systemic risk measured by Debt Rank. Furthermore, using Monte Carlo\nsimulations, we investigate network structures that can amplify systemic risk.\nIndeed, while no topology in general sense is {\\em a priori} more stable if the\nmarket is liquid [1], a larger complexity is detrimental for the overall\nstability [2]. Here we find that the measure of scalar assortativity correlates\nwell with level of systemic risk. In particular, network structures with high\nsystemic risk are scalar assortative, meaning that risky banks are mostly\nexposed to other risky banks. Network structures with low systemic risk are\nscalar disassortative, with interactions of risky banks with stable banks.\n"
    },
    {
        "paper_id": 1902.08681,
        "authors": "Tho V. Le and Satish V. Ukkusuri",
        "title": "Influencing factors that determine the usage of the crowd-shipping\n  services",
        "comments": "32 pages",
        "journal-ref": "TRR, 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this study is to understand how senders choose shipping\nservices for different products, given the availability of both emerging\ncrowd-shipping (CS) and traditional carriers in a logistics market. Using data\ncollected from a US survey, Random Utility Maximization (RUM) and Random Regret\nMinimization (RRM) models have been employed to reveal factors that influence\nthe diversity of decisions made by senders. Shipping costs, along with\nadditional real-time services such as courier reputations, tracking info,\ne-notifications, and customized delivery time and location, have been found to\nhave remarkable impacts on senders' choices. Interestingly, potential senders\nwere willing to pay more to ship grocery items such as food, beverages, and\nmedicines by CS services. Moreover, the real-time services have low\nelasticities, meaning that only a slight change in those services will lead to\na change in sender-behavior. Finally, data-science techniques were used to\nassess the performance of the RUM and RRM models and found to have similar\naccuracies. The findings from this research will help logistics firms address\npotential market segments, prepare service configurations to fulfill senders'\nexpectations, and develop effective business operations strategies.\n"
    },
    {
        "paper_id": 1902.08684,
        "authors": "Marko Po\\v{z}enel and Dejan Lavbi\\v{c}",
        "title": "Discovering Language of the Stocks",
        "comments": "15 pages, 2 figures, 5 tables",
        "journal-ref": "Frontiers in Artificial Intelligence and Applications - Databases\n  and Information Systems X 315 2019",
        "doi": "10.3233/978-1-61499-941-6-243",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock prediction has always been attractive area for researchers and\ninvestors since the financial gains can be substantial. However, stock\nprediction can be a challenging task since stocks are influenced by a multitude\nof factors whose influence vary rapidly through time. This paper proposes a\nnovel approach (Word2Vec) for stock trend prediction combining NLP and Japanese\ncandlesticks. First, we create a simple language of Japanese candlesticks from\nthe source OHLC data. Then, sentences of words are used to train the NLP\nWord2Vec model where training data classification also takes into account\ntrading commissions. Finally, the model is used to predict trading actions. The\nproposed approach was compared to three trading models Buy & Hold, MA and MACD\naccording to the yield achieved. We first evaluated Word2Vec on three shares of\nApple, Microsoft and Coca-Cola where it outperformed the comparative models.\nNext we evaluated Word2Vec on stocks from Russell Top 50 Index where our\nWord2Vec method was also very successful in test phase and only fall behind the\nBuy & Hold method in validation phase. Word2Vec achieved positive results in\nall scenarios while the average yields of MA and MACD were still lower compared\nto Word2Vec.\n"
    },
    {
        "paper_id": 1902.08821,
        "authors": "Luca Di Persio, Luca Prezioso, Kai Wallbaum",
        "title": "Closed-End Formula for options linked to Target Volatility Strategies",
        "comments": "21 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent years have seen an emerging class of structured financial products\nbased on options linked to dynamic asset allocation strategies. One of the most\nchosen approach is the so-called target volatility mechanism. It shifts between\nrisky and riskless assets to control the volatility of the overall portfolio.\nEven if a series of articles have been already devoted to the analysis of\noptions linked to the target volatility mechanism, this paper is the first, to\nthe best of our knowledge, that tries to develop closed-end formulas for\nVolTarget options. In particular, we develop closed-end formulas for option\nprices and some key hedging parameters within a Black and Scholes setting,\nassuming the underlying follows a target volatility mechanism.\n"
    },
    {
        "paper_id": 1902.08938,
        "authors": "Quanxi Wang",
        "title": "Working Paper: Improved Stock Price Forecasting Algorithm based on\n  Feature-weighed Support Vector Regression by using Grey Correlation Degree",
        "comments": "22 pages, 8 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the widespread engineering applications ranging from artificial\nintelligence and big data decision-making, originally a lot of tedious\nfinancial data processing, processing and analysis have become more and more\nconvenient and effective. This paper aims to improve the accuracy of stock\nprice forecasting. It improves the support vector machine regression algorithm\nby using grey correlation analysis (GCA) and improves the accuracy of stock\nprediction. This article first divides the factors affecting the stock price\nmovement into behavioral factors and technical factors. The behavioral factors\nmainly include weather indicators and emotional indicators. The technical\nfactors mainly include the daily closing data and the HS 300 Index, and then\nmeasure relation through the method of grey correlation analysis. The\nrelationship between the stock price and its impact factors during the trading\nday, and this relationship is transformed into the characteristic weight of\neach impact factor. The weight of the impact factors of all trading days is\nweighted by the feature weight, and finally the support vector regression (SVR)\nis used. The forecast of the revised stock trading data was compared based on\nthe forecast results of technical indicators (MSE, MAE, SCC, and DS) and\nunmodified transaction data, and it was found that the forecast results were\nsignificantly improved.\n"
    },
    {
        "paper_id": 1902.09167,
        "authors": "Alje van Dam",
        "title": "Diversity and its decomposition into variety, balance and disparity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Diversity is a central concept in many fields. Despite its importance, there\nis no unified methodological framework to measure diversity and its three\ncomponents of variety, balance and disparity. Current approaches take into\naccount disparity of the types by considering their pairwise similarities.\nPairwise similarities between types do not adequately capture total disparity,\nsince they fail to take into account in which way pairs are similar. Hence,\npairwise similarities do not discriminate between similarity of types in terms\nof the same feature and similarity of types in terms of different features.\nThis paper presents an alternative approach which is based similarities of\nfeatures between types over the whole set. The proposed measure of diversity\nproperly takes into account the aspects of variety, balance and disparity, and\nwithout having to set an arbitrary weight for each aspect of diversity. Based\non this measure, the 'ABC decomposition' is introduced, which provides separate\nmeasures for the variety, balance and disparity, allowing them to enter\nanalysis separately. The method is illustrated by analyzing the industrial\ndiversity from 1850 to present while taking into account the overlap in\noccupations they employ. Finally, the framework is extended to take into\naccount disparity considering multiple features, providing a helpful tool in\nanalysis of high-dimensional data.\n"
    },
    {
        "paper_id": 1902.09204,
        "authors": "Fernando M. Arag\\'on (1), Francisco Oteiza (2) and Juan Pablo Rud (3)\n  ((1) Department of Economics, Simon Fraser University, (2) Department of\n  Social Science, UCL Institute of Education, (3) Department of Economics,\n  Royal Holloway, University of London and Institute of Fiscal Studies)",
        "title": "Climate Change and Agriculture: Subsistence Farmers' Response to Extreme\n  Heat",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper examines how subsistence farmers respond to extreme heat. Using\nmicro-data from Peruvian households, we find that high temperatures reduce\nagricultural productivity, increase area planted, and change crop mix. These\nfindings are consistent with farmers using input adjustments as a short-term\nmechanism to attenuate the effect of extreme heat on output. This response\nseems to complement other coping strategies, such as selling livestock, but\nexacerbates the drop in yields, a standard measure of agricultural\nproductivity. Using our estimates, we show that accounting for land adjustments\nis important to quantify damages associated with climate change.\n"
    },
    {
        "paper_id": 1902.09205,
        "authors": "Chiara Lattanzi and Manuele Leonelli",
        "title": "A changepoint approach for the identification of financial extreme\n  regimes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inference over tails is usually performed by fitting an appropriate limiting\ndistribution over observations that exceed a fixed threshold. However, the\nchoice of such threshold is critical and can affect the inferential results.\nExtreme value mixture models have been defined to estimate the threshold using\nthe full dataset and to give accurate tail estimates. Such models assume that\nthe tail behavior is constant for all observations. However, the extreme\nbehavior of financial returns often changes considerably in time and such\nchanges occur by sudden shocks of the market. Here we extend the extreme value\nmixture model class to formally take into account distributional extreme\nchangepoints, by allowing for the presence of regime-dependent parameters\nmodelling the tail of the distribution. This extension formally uses the full\ndataset to both estimate the thresholds and the extreme changepoint locations,\ngiving uncertainty measures for both quantities. Estimation of functions of\ninterest in extreme value analyses is performed via MCMC algorithms. Our\napproach is evaluated through a series of simulations, applied to real data\nsets and assessed against competing approaches. Evidence demonstrates that the\ninclusion of different extreme regimes outperforms both static and dynamic\ncompeting approaches in financial applications.\n"
    },
    {
        "paper_id": 1902.09253,
        "authors": "Tetsuya Takaishi and Takanori Adachi",
        "title": "Market efficiency, liquidity, and multifractality of Bitcoin: A dynamic\n  study",
        "comments": "10 pages, 6 figures",
        "journal-ref": "Asia-Pacific Financial Markets 27 (2020) 145-154",
        "doi": "10.1007/s10690-019-09286-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This letter investigates the dynamic relationship between market efficiency,\nliquidity, and multifractality of Bitcoin. We find that before 2013 liquidity\nis low and the Hurst exponent is less than 0.5, indicating that the Bitcoin\ntime series is anti-persistent. After 2013, as liquidity increased, the Hurst\nexponent rose to approximately 0.5, improving market efficiency. For several\nperiods, however, the Hurst exponent was found to be significantly less than\n0.5, making the time series anti-persistent during those periods. We also\ninvestigate the multifractal degree of the Bitcoin time series using the\ngeneralized Hurst exponent and find that the multifractal degree is related to\nmarket efficiency in a non-linear manner.\n"
    },
    {
        "paper_id": 1902.09425,
        "authors": "Alexander Jurisch",
        "title": "Statistical mechanics and time-series analysis by L\\'evy-parameters with\n  the possibility of real-time application",
        "comments": "15 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a method that relates the truncated cumulant-function of the\nfourth order with the L\\'evian cumulant-function. This gives us explicit\nformulas for the L\\'evy-parameters, which allow a real-time analysis of the\nstate of a random-motion. Cumbersome procedures like maximum-likelihood or\nleast-square methods are unnecessary. Furthermore, we treat the L\\'evy-system\nin terms of statistical mechanics and work out it's thermodynamic properties.\nThis also includes a discussion of the fractal nature of relativistic\ncorrections. As examples for a time-series analysis, we apply our results on\nthe time-series of the German DAX and the American S\\&P-500\\,.\n"
    },
    {
        "paper_id": 1902.09606,
        "authors": "Charles-Albert Lehalle and Charafeddine Mouzouni",
        "title": "A Mean Field Game of Portfolio Trading and Its Consequences On Perceived\n  Correlations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper goes beyond the optimal trading Mean Field Game model introduced\nby Pierre Cardaliaguet and Charles-Albert Lehalle in [Cardaliaguet, P. and\nLehalle, C.-A., Mean field game of controls and an application to trade\ncrowding, Mathematics and Financial Economics (2018)]. It starts by extending\nit to portfolios of correlated instruments. This leads to several original\ncontributions: first that hedging strategies naturally stem from optimal\nliquidation schemes on portfolios. Second we show the influence of trading\nflows on naive estimates of intraday volatility and correlations. Focussing on\nthis important relation, we exhibit a closed form formula expressing standard\nestimates of correlations as a function of the underlying correlations and the\ninitial imbalance of large orders, via the optimal flows of our mean field game\nbetween traders. To support our theoretical findings, we use a real dataset of\n176 US stocks from January to December 2014 sampled every 5 minutes to analyze\nthe influence of the daily flows on the observed correlations. Finally, we\npropose a toy model based approach to calibrate our MFG model on data.\n"
    },
    {
        "paper_id": 1902.09999,
        "authors": "Zsolt Bihary and Attila Andr\\'as V\\'ig",
        "title": "Analytic solutions in a continuous-time financial market model",
        "comments": "Journal of Economic Literature (JEL) code: G11, G17",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a heterogeneous agent market model (HAM) in continuous time. The\nmarket is populated by fundamental traders and chartists, who both use simple\nlinear trading rules. Most of the related literature explores stability, price\ndynamics and profitability either within deterministic models or by simulation.\nOur novel formulation lends itself to analytic treatment even in the stochastic\ncase. We prove conditions for the (stochastic) stability of the price process,\nand also for the price to mean-revert to the fundamental value. Assuming\nstability, we derive analytic formulae on how the population ratios influence\nprice dynamics and the profitability of the strategies. Our results suggest\nthat whichever trader type is more present in the market will achieve higher\nreturns.\n"
    },
    {
        "paper_id": 1902.10015,
        "authors": "John Armstrong, Damiano Brigo",
        "title": "The ineffectiveness of coherent risk measures",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that coherent risk measures are ineffective in curbing the behaviour\nof investors with limited liability or excessive tail-risk seeking behaviour if\nthe market admits statistical arbitrage opportunities which we term\n$\\rho$-arbitrage for a risk measure $\\rho$. We show how to determine\nanalytically whether such $\\rho$-arbitrage portfolios exist in complete markets\nand in the Markowitz model. We also consider realistic numerical examples of\nincomplete markets and determine whether expected shortfall constraints are\nineffective in these markets. We find that the answer depends heavily upon the\nprobability model selected by the risk manager but that it is certainly\npossible for expected shortfall constraints to be ineffective in realistic\nmarkets. Since value at risk constraints are weaker than expected shortfall\nconstraints, our results can be applied to value at risk. By contrast, we show\nthat reasonable expected utility constraints are effective in any\narbitrage-free market.\n"
    },
    {
        "paper_id": 1902.10021,
        "authors": "Zsolt Bihary, P\\'eter Cs\\'oka, P\\'eter Ker\\'enyi and Alexander\n  Szimayer",
        "title": "Self-respecting worker in the precarious gig economy: A dynamic\n  principal-agent model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3866721",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a dynamic principal-agent model to understand the nature of\ncontracts between an employer and an independent gig worker. We model the\nworker's self-respect with an endogenous backward-looking participation\nconstraint; he accepts a job offer if and only if its utility is at least as\nlarge as his reference value, which is based on the average of previously\nrealized wages. If the dynamically changing reference value capturing the\nworker's demand is too high, then no contract is struck until the reference\nvalue hits a threshold. Below the threshold, contracts are offered and\naccepted, and the worker's wage demand follows a stochastic process. We apply\nour model to perfectly competitive and monopsonistic labor market structures\nand investigate first-best and second-best solutions. We show that a\nfar-sighted employer with market power may sacrifice instantaneous profit to\nregulate the agent's demand. Moreover, the far-sighted employer implements\nincreasing and path-dependent effort levels. Our model captures the worker's\nbargaining power by a vulnerability parameter that measures the rate at which\nhis wage demand decreases when unemployed. With a low vulnerability parameter,\nthe worker can afford to go unemployed and need not take a job at all costs.\nConversely, a worker with high vulnerability can be exploited by the employer,\nand in this case our model also exhibits self-exploitation.\n"
    },
    {
        "paper_id": 1902.10044,
        "authors": "Tomasz R. Bielecki, Igor Cialenco, Marcin Pitera, Thorsten Schmidt",
        "title": "Fair Estimation of Capital Risk Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a novel methodology for estimation of risk capital\nallocation. The methodology is rooted in the theory of risk measures. We work\nwithin a general, but tractable class of law-invariant coherent risk measures,\nwith a particular focus on expected shortfall. We introduce the concept of fair\ncapital allocations and provide explicit formulae for fair capital allocations\nin case when the constituents of the risky portfolio are jointly normally\ndistributed. The main focus of the paper is on the problem of approximating\nfair portfolio allocations in the case of not fully known law of the portfolio\nconstituents. We define and study the concepts of fair allocation estimators\nand asymptotically fair allocation estimators. A substantial part of our study\nis devoted to the problem of estimating fair risk allocations for expected\nshortfall. We study this problem under normality as well as in a nonparametric\nsetup. We derive several estimators, and prove their fairness and/or asymptotic\nfairness. Last, but not least, we propose two backtesting methodologies that\nare oriented at assessing the performance of the allocation estimation\nprocedure. The paper closes with a substantial numerical study of the subject.\n"
    },
    {
        "paper_id": 1902.1008,
        "authors": "Hannu S. Laine, Jyri Salpakari, Erin E. Looney, Hele Savin, Ian Marius\n  Peters and Tonio Buonassisi",
        "title": "Meeting Global Cooling Demand with Photovoltaics during the 21st Century",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1039/C9EE00002J",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Space conditioning, and cooling in particular, is a key factor in human\nproductivity and well-being across the globe. During the 21st century, global\ncooling demand is expected to grow significantly due to the increase in wealth\nand population in sunny nations across the globe and the advance of global\nwarming. The same locations that see high demand for cooling are also ideal for\nelectricity generation via photovoltaics (PV). Despite the apparent synergy\nbetween cooling demand and PV generation, the potential of the cooling sector\nto sustain PV generation has not been assessed on a global scale. Here, we\nperform a global assessment of increased PV electricity adoption enabled by the\nresidential cooling sector during the 21st century. Already today, utilizing PV\nproduction for cooling could facilitate an additional installed PV capacity of\napproximately 540 GW, more than the global PV capacity of today. Using\nestablished scenarios of population and income growth, as well as accounting\nfor future global warming, we further project that the global residential\ncooling sector could sustain an added PV capacity between 20-200 GW each year\nfor most of the 21st century, on par with the current global manufacturing\ncapacity of 100 GW. Furthermore, we find that without storage, PV could\ndirectly power approximately 50% of cooling demand, and that this fraction is\nset to increase from 49% to 56% during the 21st century, as cooling demand\ngrows in locations where PV and cooling have a higher synergy. With this\ngeographic shift in demand, the potential of distributed storage also grows. We\nsimulate that with a 1 m$^3$ water-based latent thermal storage per household,\nthe fraction of cooling demand met with PV would increase from 55% to 70%\nduring the century. These results show that the synergy between cooling and PV\nis notable and could significantly accelerate the growth of the global PV\nindustry.\n"
    },
    {
        "paper_id": 1902.10405,
        "authors": "Romuald Elie, Emma Hubert, Thibaut Mastrolia, Dylan Possama\\\"i",
        "title": "Mean-field moral hazard for optimal energy demand response management",
        "comments": "54 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of demand response contracts in electricity markets by\nquantifying the impact of considering a mean-field of consumers, whose\nconsumption is impacted by a common noise. We formulate the problem as a\nPrincipal-Agent problem with moral hazard in which the Principal - she - is an\nelectricity producer who observes continuously the consumption of a continuum\nof risk-averse consumers, and designs contracts in order to reduce her\nproduction costs. More precisely, the producer incentivises the consumers to\nreduce the average and the volatility of their consumption in different usages,\nwithout observing the efforts they make. We prove that the producer can benefit\nfrom considering the mean-field of consumers by indexing contracts on the\nconsumption of one Agent and aggregate consumption statistics from the\ndistribution of the entire population of consumers. In the case of linear\nenergy valuation, we provide closed-form expression for this new type of\noptimal contracts that maximises the utility of the producer. In most cases, we\nshow that this new type of contracts allows the Principal to choose the risks\nshe wants to bear, and to reduce the problem at hand to an uncorrelated one.\n"
    },
    {
        "paper_id": 1902.10492,
        "authors": "Kristina Rognlien Dahl",
        "title": "A convex duality approach for pricing contingent claims under partial\n  information and short selling constraints",
        "comments": null,
        "journal-ref": "Stochastic Analysis and Applications Volume 35, 2017 - Issue 2,\n  pp. 317-333",
        "doi": "10.1080/07362994.2016.1255147",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the pricing problem facing a seller of a contingent claim. We\nassume that this seller has some general level of partial information, and that\nhe is not allowed to sell short in certain assets. This pricing problem, which\nis our primal problem, is a constrained stochastic optimization problem. We\nderive a dual to this problem by using the conjugate duality theory introduced\nby Rockafellar. Furthermore, we give conditions for strong duality to hold.\nThis gives a characterization of the price of the claim involving martingale-\nand super-martingale conditions on the optional projection of the price\nprocesses.\n"
    },
    {
        "paper_id": 1902.105,
        "authors": "Alonso-Marroquin Fernando, Arias-Calluari Karina, Harre Michael,\n  Najafi Morteza N. and Herrmann Hans J",
        "title": "Q-Gaussian diffusion in stock markets",
        "comments": "Field of study: Condensed-matter physics, 5 pages and 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the Standard & Poor's 500 stock market index from the last 22\nyears. The probability density function of price returns exhibits two\nwell-distinguished regimes with self-similar structure: the first one displays\nstrong super-diffusion together with short-time correlations, and the second\none corresponds to weak super-diffusion with weak time correlations. Both\nregimes are well-described by q-Gaussian distributions. The porous media\nequation is used to derive the governing equation for these regimes, and the\nBlack-Scholes diffusion coefficient is explicitly obtained from the governing\nequation.\n"
    },
    {
        "paper_id": 1902.10502,
        "authors": "J. L. Subias",
        "title": "Quantum model for price forecasting in financial markets",
        "comments": "21 pages, 4 figures, bilingual English-Spanish version",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.32897.92006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper describes a practical example in which the probability\ndistribution of the prices of a stock market blue chip is calculated as the\nwave function of a quantum particle confined in a potential well. This model\nmay naturally explain the operation of several empirical rules used by\ntechnical analysts. Models based on the movement of a Brownian particle do not\naccount for fundamental aspects of financial markets. This is due to the fact\nthat the Brownian particle is a classical particle, while stock market prices\nbehave more like quantum particles. When a classical particle meets an obstacle\nor a potential barrier, it may either bounce or overcome the obstacle, yet not\nboth at a time. Only a quantum particle can simultaneously reflect and transmit\nitself on a potential barrier. This is precisely what prices in a stock market\nimitate when they find a resistance level: they partially bounce against and\npartially overcome it. This can only be explained by admitting that prices\nbehave as quantum rather than as classic particles. The proposed quantum model\nfinds natural justification not only for the aforementioned facts but also for\nother empirically well-known facts such as sudden changes in volatility,\nnon-Gaussian distribution in prices, among others.\n"
    },
    {
        "paper_id": 1902.10743,
        "authors": "Weibing Huang, Mathieu Rosenbaum and Pamela Saliba",
        "title": "From Glosten-Milgrom to the whole limit order book and applications to\n  financial regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build an agent-based model for the order book with three types of market\nparticipants: informed trader, noise trader and competitive market makers.\nUsing a Glosten-Milgrom like approach, we are able to deduce the whole limit\norder book (bid-ask spread and volume available at each price) from the\ninteractions between the different agents. More precisely, we obtain a link\nbetween efficient price dynamic, proportion of trades due to the noise trader,\ntraded volume, bid-ask spread and equilibrium limit order book state. With this\nmodel, we provide a relevant tool for regulators and market platforms. We show\nfor example that it allows us to forecast consequences of a tick size change on\nthe microstructure of an asset. It also enables us to value quantitatively the\nqueue position of a limit order in the book.\n"
    },
    {
        "paper_id": 1902.1079,
        "authors": "L\\'aszl\\'o Csat\\'o and D\\'ora Gr\\'eta Petr\\'oczy",
        "title": "On the monotonicity of the eigenvector method",
        "comments": "16 pages, 2 figures, 2 tables",
        "journal-ref": "European Journal of Operational Research, 292(1): 230-237, 2021",
        "doi": "10.1016/j.ejor.2020.10.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pairwise comparisons are used in a wide variety of decision situations where\nthe importance of alternatives should be measured on a numerical scale. One\npopular method to derive the priorities is based on the right eigenvector of a\nmultiplicative pairwise comparison matrix. We consider two monotonicity axioms\nin this setting. First, increasing an arbitrary entry of a pairwise comparison\nmatrix is not allowed to result in a counter-intuitive rank reversal, that is,\nthe favoured alternative in the corresponding row cannot be ranked lower than\nany other alternative if this was not the case before the change (rank\nmonotonicity). Second, the same modification should not decrease the normalised\nweight of the favoured alternative (weight monotonicity). Both properties are\nsatisfied by the geometric mean method but violated by the eigenvector method.\nThe axioms do not uniquely determine the geometric mean. The relationship\nbetween the two monotonicity properties and the Saaty inconsistency index are\ninvestigated for the eigenvector method via simulations. Even though their\nviolation turns out not to be a usual problem even for heavily inconsistent\nmatrices, all decision-makers should be informed about the possible occurrence\nof such unexpected consequences of increasing a matrix entry.\n"
    },
    {
        "paper_id": 1902.108,
        "authors": "Naji Massad and J{\\o}rgen Vitting Andersen",
        "title": "Three Different Ways Synchronization Can Cause Contagion in Financial\n  Markets",
        "comments": "13 pages, 8 figures",
        "journal-ref": "Risks (2018), 6(4), 104",
        "doi": "10.3390/risks6040104",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce tools to capture the dynamics of three different pathways, in\nwhich the synchronization of human decision-making could lead to turbulent\nperiods and contagion phenomena in financial markets. The first pathway is\ncaused when stock market indices, seen as a set of coupled integrate-and-fire\noscillators, synchronize in frequency. The integrate-and-fire dynamics happens\ndue to change blindness, a trait in human decision-making where people have the\ntendency to ignore small changes, but take action when a large change happens.\nThe second pathway happens due to feedback mechanisms between market\nperformance and the use of certain (decoupled) trading strategies. The third\npathway occurs through the effects of communication and its impact on human\ndecision-making. A model is introduced in which financial market performance\nhas an impact on decision-making through communication between people.\nConversely, the sentiment created via communication has an impact on financial\nmarket performance. The methodologies used are: agent based modeling, models of\nintegrate-and-fire oscillators, and communication models of human\ndecision-making\n"
    },
    {
        "paper_id": 1902.10849,
        "authors": "Elizabeth Fons, Paula Dawson, Jeffrey Yau, Xiao-jun Zeng and John\n  Keane",
        "title": "A novel dynamic asset allocation system using Feature Saliency Hidden\n  Markov models for smart beta investing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial crisis of 2008 generated interest in more transparent,\nrules-based strategies for portfolio construction, with Smart beta strategies\nemerging as a trend among institutional investors. While they perform well in\nthe long run, these strategies often suffer from severe short-term drawdown\n(peak-to-trough decline) with fluctuating performance across cycles. To address\ncyclicality and underperformance, we build a dynamic asset allocation system\nusing Hidden Markov Models (HMMs). We test our system across multiple\ncombinations of smart beta strategies and the resulting portfolios show an\nimprovement in risk-adjusted returns, especially on more return oriented\nportfolios (up to 50$\\%$ in excess of market annually). In addition, we propose\na novel smart beta allocation system based on the Feature Saliency HMM (FSHMM)\nalgorithm that performs feature selection simultaneously with the training of\nthe HMM, to improve regime identification. We evaluate our systematic trading\nsystem with real life assets using MSCI indices; further, the results (up to\n60$\\%$ in excess of market annually) show model performance improvement with\nrespect to portfolios built using full feature HMMs.\n"
    },
    {
        "paper_id": 1902.10877,
        "authors": "Sangyeon Kim, Myungjoo Kang",
        "title": "Financial series prediction using Attention LSTM",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time series prediction, especially with machine learning\ntechniques, is an extensive field of study. In recent times, deep learning\nmethods (especially time series analysis) have performed outstandingly for\nvarious industrial problems, with better prediction than machine learning\nmethods. Moreover, many researchers have used deep learning methods to predict\nfinancial time series with various models in recent years. In this paper, we\nwill compare various deep learning models, such as multilayer perceptron (MLP),\none-dimensional convolutional neural networks (1D CNN), stacked long short-term\nmemory (stacked LSTM), attention networks, and weighted attention networks for\nfinancial time series prediction. In particular, attention LSTM is not only\nused for prediction, but also for visualizing intermediate outputs to analyze\nthe reason of prediction; therefore, we will show an example for understanding\nthe model prediction intuitively with attention vectors. In addition, we focus\non time and factors, which lead to an easy understanding of why certain trends\nare predicted when accessing a given time series table. We also modify the loss\nfunctions of the attention models with weighted categorical cross entropy; our\nproposed model produces a 0.76 hit ratio, which is superior to those of other\nmethods for predicting the trends of the KOSPI 200.\n"
    },
    {
        "paper_id": 1902.10948,
        "authors": "Jinho Lee, Raehyun Kim, Yookyung Koh, and Jaewoo Kang",
        "title": "Global Stock Market Prediction Based on Stock Chart Images Using Deep\n  Q-Network",
        "comments": "12 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1109/ACCESS.2019.2953542",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We applied Deep Q-Network with a Convolutional Neural Network function\napproximator, which takes stock chart images as input, for making global stock\nmarket predictions. Our model not only yields profit in the stock market of the\ncountry where it was trained but generally yields profit in global stock\nmarkets. We trained our model only in the US market and tested it in 31\ndifferent countries over 12 years. The portfolios constructed based on our\nmodel's output generally yield about 0.1 to 1.0 percent return per transaction\nprior to transaction costs in 31 countries. The results show that there are\nsome patterns on stock chart image, that tend to predict the same future stock\nprice movements across global stock markets. Moreover, the results show that\nfuture stock prices can be predicted even if the training and testing\nprocedures are done in different countries. Training procedure could be done in\nrelatively large and liquid markets (e.g., USA) and tested in small markets.\nThis result demonstrates that artificial intelligence based stock price\nforecasting models can be used in relatively small markets (emerging countries)\neven though they do not have a sufficient amount of data for training.\n"
    },
    {
        "paper_id": 1902.11228,
        "authors": "Cyril B\\'en\\'ezet, Jean-Fran\\c{c}ois Chassagneux, Christoph Reisinger",
        "title": "A numerical scheme for the quantile hedging problem",
        "comments": "47 pages, 6 figures",
        "journal-ref": "SIAM J. Finan. Math. 12-1 (2021), pp. 110-157",
        "doi": "10.1137/19M1267477",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the numerical approximation of the quantile hedging price in a\nnon-linear market. In a Markovian framework, we propose a numerical method\nbased on a Piecewise Constant Policy Timestepping (PCPT) scheme coupled with a\nmonotone finite difference approximation. We prove the convergence of our\nalgorithm combining BSDE arguments with the Barles & Jakobsen and Barles &\nSouganidis approaches for non-linear equations. In a numerical section, we\nillustrate the efficiency of our scheme by considering a financial example in a\nmarket with imperfections.\n"
    },
    {
        "paper_id": 1903.00067,
        "authors": "Christian Fries, Peter Kohl-Landgraf, Bj\\\"orn Paffen, Stefanie\n  Weddigen, Luca Del Re, Wilfried Sch\\\"utte, David Bacher, Rebecca Declara,\n  Daniel Eichsteller, Florian Weichand, Michael Streubel",
        "title": "Implementing a financial derivative as smart contract",
        "comments": "51 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this note we describe the application of existing smart contract\ntechnologies with the aim to construct a new digital representation of a\nfinancial derivative contract. We compare several existing DLT based\ntechnologies. We provide a detailed description of two separate prototypes\nwhich are able to be executed on a centralized and on a DLT platform\nrespectively. Beyond that we highlight some insights on legal aspects as well\nas on common integration challenges regarding existing process and system\nlandscapes. For a further introductory note and motivation on the theoretical\nconcept we refer to\nhttps://www.law.ox.ac.uk/business-law-blog/blog/2018/12/smart-derivative-contract-constructing-digital-financial-derivative\n. A very detailed methodological overview of the concept of a smart derivative\ncontract can be found in doi:10.2139/ssrn.3163074.\n"
    },
    {
        "paper_id": 1903.00261,
        "authors": "Dmitry I. Ivanov and Alexander S. Nesterov",
        "title": "Stealed-bid Auctions: Detecting Bid Leakage via Semi-Supervised Learning",
        "comments": "This is long version of short paper 'Identifying Bid Leakage in\n  Procurement Auctions: Machine Learning Approach', published at EC'19:\n  https://dl.acm.org/doi/abs/10.1145/3328526.3329642",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bid leakage is a corrupt scheme in a first-price sealed-bid auction in which\nthe procurer leaks the opponents' bids to a favoured participant. The rational\nbehaviour of such participant is to bid close to the deadline in order to\nreceive all bids, which allows him to ensure his win at the best price\npossible. While such behaviour does leave detectable traces in the data, the\nabsence of bid leakage labels makes supervised classification impossible.\nInstead, we reduce the problem of the bid leakage detection to a\npositive-unlabeled classification. The key idea is to regard the losing\nparticipants as fair and the winners as possibly corrupted. This allows us to\nestimate the prior probability of bid leakage in the sample, as well as the\nposterior probability of bid leakage for each specific auction.\n  We extract and analyze the data on 600,000 Russian procurement auctions\nbetween 2014 and 2018. We find that around 9% of the auctions are exposed to\nbid leakage, which results in an overall 1.5% price increase. The predicted\nprobability of bid leakage is higher for auctions with a higher reserve price,\nwith too low or too high number of participants, and if the winner has met the\nauctioneer in earlier auctions.\n"
    },
    {
        "paper_id": 1903.00313,
        "authors": "Mahendra K. Verma",
        "title": "Hierarchical financial structures with money cascade",
        "comments": "to appear as a chapter in \"New Perspectives and Challenges in\n  Econophysics and Sociophysics\", 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show similarities between turbulence and financial systems.\nMotivated by similarities between the two systems, we construct a multiscale\nmodel for hierarchical financial structures that exhibits a constant cascade of\nwealth from large financial entities to small financial entities. According to\nour model, large and intermediate scale financial institutions have a power law\ndistribution. However, the wealth distribution is Maxwellian at individual\nscales.\n"
    },
    {
        "paper_id": 1903.00369,
        "authors": "Ludovic Gouden\\`ege and Andrea Molent and Antonino Zanette",
        "title": "Gaussian Process Regression for Pricing Variable Annuities with\n  Stochastic Volatility and Interest Rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate price and Greeks computation of a Guaranteed\nMinimum Withdrawal Benefit (GMWB) Variable Annuity (VA) when both stochastic\nvolatility and stochastic interest rate are considered together in the Heston\nHull-White model. We consider a numerical method the solves the dynamic control\nproblem due to the computing of the optimal withdrawal. Moreover, in order to\nspeed up the computation, we employ Gaussian Process Regression (GPR). Starting\nfrom observed prices previously computed for some known combinations of model\nparameters, it is possible to approximate the whole price function on a defined\ndomain. The regression algorithm consists of algorithm training and evaluation.\nThe first step is the most time demanding, but it needs to be performed only\nonce, while the latter is very fast and it requires to be performed only when\npredicting the target function. The developed method, as well as for the\ncalculation of prices and Greeks, can also be employed to compute the\nno-arbitrage fee, which is a common practice in the Variable Annuities sector.\nNumerical experiments show that the accuracy of the values estimated by GPR is\nhigh with very low computational cost. Finally, we stress out that the analysis\nis carried out for a GMWB annuity but it could be generalized to other\ninsurance products.\n"
    },
    {
        "paper_id": 1903.00472,
        "authors": "Tomaso Aste",
        "title": "Cryptocurrency market structure: connecting emotions and economics",
        "comments": "17 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the dependency and causality structure of the cryptocurrency market\ninvestigating collective movements of both prices and social sentiment related\nto almost two thousand cryptocurrencies traded during the first six months of\n2018. This is the first study of the whole cryptocurrency market structure. It\nintroduces several rigorous innovative methodologies applicable to this and to\nseveral other complex systems where a large number of variables interact in a\nnon-linear way, which is a distinctive feature of the digital economy. The\nanalysis of the dependency structure reveals that prices are significantly\ncorrelated with sentiment. The major, most capitalised cryptocurrencies, such\nas bitcoin, have a central role in the price correlation network but only a\nmarginal role in the sentiment network and in the network describing the\ninteractions between the two. The study of the causality structure reveals a\ncausality network that is consistently related with the correlation structures\nand shows that both prices cause sentiment and sentiment cause prices across\ncurrencies with the latter being stronger in size but smaller in number of\nsignificative interactions. Overall our study uncovers a complex and rich\nstructure of interrelations where prices and sentiment influence each other\nboth instantaneously and with lead-lag causal relations. A major finding is\nthat minor currencies, with small capitalisation, play a crucial role in\nshaping the overall dependency and causality structure. Despite the high level\nof noise and the short time-series we verified that these networks are\nsignificant with all links statistically validated and with a structural\norganisation consistently reproduced across all networks.\n"
    },
    {
        "paper_id": 1903.0059,
        "authors": "Yu Feng",
        "title": "Non-Parametric Robust Model Risk Measurement with Path-Dependent Loss\n  Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding and measuring model risk is important to financial\npractitioners. However, there lacks a non-parametric approach to model risk\nquantification in a dynamic setting and with path-dependent losses. We propose\na complete theory generalizing the relative-entropic approach by Glasserman and\nXu to the dynamic case under any $f$-divergence. It provides an unified\ntreatment for measuring both the worst-case risk and the $f$-divergence budget\nthat originate from the model uncertainty of an underlying state process.\n"
    },
    {
        "paper_id": 1903.00631,
        "authors": "Jin Sun, Ryle S. Perera and Pavel V. Shevchenko",
        "title": "Optimal Investment-Consumption-Insurance with Durable and Perishable\n  Consumption Goods in a Jump Diffusion Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate an optimal investment-consumption and optimal level of\ninsurance on durable consumption goods with a positive loading in a\ncontinuous-time economy. We assume that the economic agent invests in the\nfinancial market and in durable as well as perishable consumption goods to\nderive utilities from consumption over time in a jump-diffusion market.\nAssuming that the financial assets and durable consumption goods can be traded\nwithout transaction costs, we provide a semi-explicit solution for the optimal\ninsurance coverage for durable goods and financial asset. With transaction\ncosts for trading the durable good proportional to the total value of the\ndurable good, we formulate the agent's optimization problem as a combined\nstochastic and impulse control problem, with an implicit intervention value\nfunction. We solve this problem numerically using stopping time iteration, and\nanalyze the numerical results using illustrative examples.\n"
    },
    {
        "paper_id": 1903.0069,
        "authors": "Sara Moricz",
        "title": "Using Artificial Intelligence to Recapture Norms: Did #metoo change\n  gender norms in Sweden?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Norms are challenging to define and measure, but this paper takes advantage\nof text data and the recent development in machine learning to create an\nencompassing measure of norms. An LSTM neural network is trained to detect\ngendered language. The network functions as a tool to create a measure on how\ngender norms changes in relation to the Metoo movement on Swedish Twitter. This\npaper shows that gender norms on average are less salient half a year after the\ndate of the first appearance of the hashtag #Metoo. Previous literature\nsuggests that gender norms change over generations, but the current result\nsuggests that norms can change in the short run.\n"
    },
    {
        "paper_id": 1903.00829,
        "authors": "Alex Garivaltis",
        "title": "Cover's Rebalancing Option With Discrete Hindsight Optimization",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study T. Cover's rebalancing option (Ordentlich and Cover 1998) under\ndiscrete hindsight optimization in continuous time. The payoff in question is\nequal to the final wealth that would have accrued to a $\\$1$ deposit into the\nbest of some finite set of (perhaps levered) rebalancing rules determined in\nhindsight. A rebalancing rule (or fixed-fraction betting scheme) amounts to\nfixing an asset allocation (i.e. $200\\%$ stocks and $-100\\%$ bonds) and then\ncontinuously executing rebalancing trades to counteract allocation drift.\nRestricting the hindsight optimization to a small number of rebalancing rules\n(i.e. 2) has some advantages over the pioneering approach taken by Cover $\\&$\nCompany in their brilliant theory of universal portfolios (1986, 1991, 1996,\n1998), where one's on-line trading performance is benchmarked relative to the\nfinal wealth of the best unlevered rebalancing rule of any kind in hindsight.\nOur approach lets practitioners express an a priori view that one of the\nfavored asset allocations (\"bets\") $b\\in\\{b_1,...,b_n\\}$ will turn out to have\nperformed spectacularly well in hindsight. In limiting our robustness to some\ndiscrete set of asset allocations (rather than all possible asset allocations)\nwe reduce the price of the rebalancing option and guarantee to achieve a\ncorrespondingly higher percentage of the hindsight-optimized wealth at the end\nof the planning period. A practitioner who lives to delta-hedge this variant of\nCover's rebalancing option through several decades is guaranteed to see the day\nthat his realized compound-annual capital growth rate is very close to that of\nthe best $b_i$ in hindsight. Hence the point of the rock-bottom option price.\n"
    },
    {
        "paper_id": 1903.00937,
        "authors": "Fazlollah Soleymani and Andrey Itkin",
        "title": "Pricing foreign exchange options under stochastic volatility and\n  interest rates using an RBF--FD method",
        "comments": "24 pages, 7 tables, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a numerical method for pricing foreign exchange (FX)\noptions in a model which deals with stochastic interest rates and stochastic\nvolatility of the FX rate. The model considers four stochastic drivers, each\nrepresented by an It\\^{o}'s diffusion with time--dependent drift, and with a\nfull matrix of correlations. It is known that prices of FX options in this\nmodel can be found by solving an associated backward partial differential\nequation (PDE). However, it contains non--affine terms, which makes its\ndifficult to solve it analytically. Also, a standard approach of solving it\nnumerically by using traditional finite--difference (FD) or finite elements\n(FE) methods suffers from the high computational burden. Therefore, in this\npaper a flavor of a localized radial basis functions (RBFs) method, RBF--FD, is\ndeveloped which allows for a good accuracy at a relatively low computational\ncost. Results of numerical simulations are presented which demonstrate\nefficiency of such an approach in terms of both performance and accuracy for\npricing FX options and computation of the associated Greeks.\n"
    },
    {
        "paper_id": 1903.00952,
        "authors": "D. S. Quevedo and C. J. Quimbay",
        "title": "Piketty's second fundamental law of capitalism as an emergent property\n  in a kinetic wealth-exchange model of economic growth",
        "comments": "16 pages, 4 figures, some types corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose in this work a kinetic wealth-exchange model of economic growth by\nintroducing saving as a non consumed fraction of production. In this new model,\nwhich starts also from microeconomic arguments, it is found that economic\ntransactions between pairs of agents leads the system to a macroscopic behavior\nwhere total wealth is not conserved and it is possible to have an economic\ngrowth which is assumed as the increasing of total production in time. This\nlast macroeconomic result, that we find both numerically through a Monte Carlo\nbased simulation method and analytically in the framework of a mean field\napproximation, corresponds to the economic growth scenario described by the\nwell known Solow model developed in the economic neoclassical theory. If\nadditionally to the income related with production due to return on individual\ncapital, it is also included the individual labor income in the model, then the\nThomas Piketty's second fundamental law of capitalism is found as a emergent\nproperty of the system. We consider that the results obtained in this paper\nshows how Econophysics can help to understand the connection between\nmacroeconomics and microeconomics.\n"
    },
    {
        "paper_id": 1903.00954,
        "authors": "Jonas Rothfuss, Fabio Ferreira, Simon Walther, Maxim Ulrich",
        "title": "Conditional Density Estimation with Neural Networks: Best Practices and\n  Benchmarks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given a set of empirical observations, conditional density estimation aims to\ncapture the statistical relationship between a conditional variable\n$\\mathbf{x}$ and a dependent variable $\\mathbf{y}$ by modeling their\nconditional probability $p(\\mathbf{y}|\\mathbf{x})$. The paper develops best\npractices for conditional density estimation for finance applications with\nneural networks, grounded on mathematical insights and empirical evaluations.\nIn particular, we introduce a noise regularization and data normalization\nscheme, alleviating problems with over-fitting, initialization and\nhyper-parameter sensitivity of such estimators. We compare our proposed\nmethodology with popular semi- and non-parametric density estimators, underpin\nits effectiveness in various benchmarks on simulated and Euro Stoxx 50 data and\nshow its superior performance. Our methodology allows to obtain high-quality\nestimators for statistical expectations of higher moments, quantiles and\nnon-linear return transformations, with very little assumptions about the\nreturn dynamic.\n"
    },
    {
        "paper_id": 1903.00955,
        "authors": "Hadi NekoeiQachkanloo, Benyamin Ghojogh, Ali Saheb Pasand, Mark\n  Crowley",
        "title": "Artificial Counselor System for Stock Investment",
        "comments": "7 pages, 8 figures, 1 table",
        "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  vol. 33, no. 01, pp. 9558-9564, 2019",
        "doi": "10.1609/aaai.v33i01.33019558",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel trading system which plays the role of an\nartificial counselor for stock investment. In this paper, the stock future\nprices (technical features) are predicted using Support Vector Regression.\nThereafter, the predicted prices are used to recommend which portions of the\nbudget an investor should invest in different existing stocks to have an\noptimum expected profit considering their level of risk tolerance. Two\ndifferent methods are used for suggesting best portions, which are Markowitz\nportfolio theory and fuzzy investment counselor. The first approach is an\noptimization-based method which considers merely technical features, while the\nsecond approach is based on Fuzzy Logic taking into account both technical and\nfundamental features of the stock market. The experimental results on New York\nStock Exchange (NYSE) show the effectiveness of the proposed system.\n"
    },
    {
        "paper_id": 1903.01655,
        "authors": "Fenghua Wen (CSU), Yujie Yuan (CSU), Wei-Xing Zhou (ECUST)",
        "title": "Cross-shareholding networks and stock price synchronicity: Evidence from\n  China",
        "comments": "24 pages including 12 tables",
        "journal-ref": "International Journal of Finance & Economics 26 (1), 914-948\n  (2021)",
        "doi": "10.1002/ijfe.1828",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the effect of cross-shareholding on stock price\nsynchronicity, as a measure of price informativeness, of the listed firms in\nthe Chinese stock market. We gauge firms' levels of cross-shareholdings in\nterms of centrality in the cross-shareholding network. It is confirmed that it\nis through a noise-reducing process that cross-shareholding promotes price\nsynchronicity and reduces price delay. More importantly, this effect on price\ninformativeness is pronounced for large firms and in the periods of market\ndownturns. Overall, our analyses provide insights into the relation between the\nownership structure and price informativeness.\n"
    },
    {
        "paper_id": 1903.01744,
        "authors": "Jamshid Ardalankia, Mohammad Osoolian, Emmanuel Haven, G.Reza Jafari",
        "title": "Scaling Features of Price-Volume Cross-Correlation",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.124111",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Price without transaction makes no sense. Trading volume authenticates its\ncorresponding price, so there exist mutual information and correlation between\nprice and trading volume. We are curious about fractal features of this\ncorrelation and need to know how structures in different scales translate\ninformation. To explore the influence of investment size (trading volume),\nprice-wise (gain/loss), and time-scale effects, we analyzed the price and\ntrading volume and their coupling by applying the MF-DXA method. Our results\nimply that price, trading volume, and price-volume coupling exhibit a power law\nand are also multifractal. Meanwhile, considering developed markets, the\nprice-volume couplings are significantly negatively correlated. However, in\nemerging markets, the price has less of a contribution to price-volume\ncoupling. In emerging markets in comparison with the developed markets, trading\nvolume and price are more independent.\n"
    },
    {
        "paper_id": 1903.0182,
        "authors": "C\\'elestin Coquid\\'e, Leonardo Ermann, Jos\\'e Lages, D.L. Shepelyansky",
        "title": "Influence of petroleum and gas trade on EU economies from the reduced\n  Google matrix analysis of UN COMTRADE data",
        "comments": "13 pages, 11 figures, 2 tables",
        "journal-ref": "Eur. Phys. J. B (2019) 92: 171",
        "doi": "10.1140/epjb/e2019-100132-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the United Nations COMTRADE database we apply the reduced Google matrix\n(REGOMAX) algorithm to analyze the multiproduct world trade in years 2004-2016.\nOur approach allows to determine the trade balance sensitivity of a group of\ncountries to a specific product price increase from a specific exporting\ncountry taking into account all direct and indirect trade pathways via all\nworld countries exchanging 61 UN COMTRADE identified trade products. On the\nbasis of this approach we present the influence of trade in petroleum and gas\nproducts from Russia, USA, Saudi Arabia and Norway determining the sensitivity\nof each EU country. We show that the REGOMAX approach provides a new and more\ndetailed analysis of trade influence propagation comparing to the usual\napproach based on export and import flows.\n"
    },
    {
        "paper_id": 1903.01861,
        "authors": "Marit Hinnosaar, Toomas Hinnosaar, Michael Kummer, Olga Slivko",
        "title": "Externalities in Knowledge Production: Evidence from a Randomized Field\n  Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Are there positive or negative externalities in knowledge production? Do\ncurrent contributions to knowledge production increase or decrease the future\ngrowth of knowledge? We use a randomized field experiment, which added relevant\ncontent to some pages in Wikipedia while leaving similar pages unchanged. We\nfind that the addition of content has a negligible impact on the subsequent\nlong-run growth of content. Our results have implications for information\nseeding and incentivizing contributions, implying that additional content does\nnot generate sizable externalities by inspiring nor discouraging future\ncontributions.\n"
    },
    {
        "paper_id": 1903.01954,
        "authors": "Remy J.-C. Pages, Dylan J. Lukes, Drew H. Bailey, and Greg J. Duncan",
        "title": "Elusive Longer-Run Impacts of Head Start: Replications Within and Across\n  Cohorts",
        "comments": "Remy J.-C. Pages and Dylan J. Lukes are co-equal first authors.\n  Before collaborating, both individuals separately and contemporaneously\n  worked on individual papers which substantially overlapped. After careful\n  consideration all parties agreed to collaborate and produce a single unified\n  paper. Online appendix included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using an additional decade of CNLSY data, this study replicated and extended\nDeming's (2009) evaluation of Head Start's life-cycle skill formation impacts\nin three ways. Extending the measurement interval for Deming's adulthood\noutcomes, we found no statistically significant impacts on earnings and mixed\nevidence of impacts on other adult outcomes. Applying Deming's sibling\ncomparison framework to more recent birth cohorts born to CNLSY mothers\nrevealed mostly negative Head Start impacts. Combining all cohorts shows\ngenerally null impacts on school-age and early adulthood outcomes.\n"
    },
    {
        "paper_id": 1903.02043,
        "authors": "Mariia Belaia",
        "title": "Optimal Climate Strategy with Mitigation, Carbon Removal, and Solar\n  Geoengineering",
        "comments": "25 pages + Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Until recently, analysis of optimal global climate policy has focused on\nmitigation. Exploration of policies to meet the 1.5{\\deg}C target have brought\ncarbon dioxide removal (CDR), a second instrument, into the climate policy\nmainstream. Far less agreement exists regarding the role of solar\ngeoengineering (SG), a third instrument to limit global climate risk.\nIntegrated assessment modelling (IAM) studies offer little guidance on\ntrade-offs between these three instruments because they have dealt with CDR and\nSG in isolation. Here, I extend the Dynamic Integrated model of Climate and\nEconomy (DICE) to include both CDR and SG to explore the temporal ordering of\nthe three instruments. Contrary to implicit assumptions that SG would be\nemployed only after mitigation and CDR are exhausted, I find that SG is\nintroduced parallel to mitigation temporary reducing climate risks during the\nera of peak CO2 concentrations. CDR reduces concentrations after mitigation is\nexhausted, enabling SG phasing out.\n"
    },
    {
        "paper_id": 1903.02228,
        "authors": "Nicholas Murphy and Tim Gebbie",
        "title": "Learning the dynamics of technical trading strategies",
        "comments": "35 pages, 7 figures",
        "journal-ref": "Quantitative Finance (2021)",
        "doi": "10.1080/14697688.2020.1869292",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use an adversarial expert based online learning algorithm to learn the\noptimal parameters required to maximise wealth trading zero-cost portfolio\nstrategies. The learning algorithm is used to determine the relative population\ndynamics of technical trading strategies that can survive historical\nback-testing as well as form an overall aggregated portfolio trading strategy\nfrom the set of underlying trading strategies implemented on daily and intraday\nJohannesburg Stock Exchange data. The resulting population time-series are\ninvestigated using unsupervised learning for dimensionality reduction and\nvisualisation. A key contribution is that the overall aggregated trading\nstrategies are tested for statistical arbitrage using a novel hypothesis test\nproposed by Jarrow et al. (2012) on both daily sampled and intraday\ntime-scales. The (low frequency) daily sampled strategies fail the arbitrage\ntests after costs, while the (high frequency) intraday sampled strategies are\nnot falsified as statistical arbitrages after costs. The estimates of trading\nstrategy success, cost of trading and slippage are considered along with an\nonline benchmark portfolio algorithm for performance comparison. In addition,\nthe algorithms generalisation error is analysed by recovering a probability of\nback-test overfitting estimate using a nonparametric procedure introduced by\nBailey et al. (2016). The work aims to explore and better understand the\ninterplay between different technical trading strategies from a data-informed\nperspective.\n"
    },
    {
        "paper_id": 1903.02383,
        "authors": "Philip Protter, Aditi Dandapani",
        "title": "Strict Local Martingales and the Khasminskii test for Explosions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We exhibit sufficient conditions such that components of a multidimensional\nSDE giving rise to a local martingale $M$ are strict local martingales or\nmartingales. We assume that the equations have diffusion coefficients of the\nform $\\sigma(M_t,v_t),$ with $v_t$ being a stochastic volatility term.\n"
    },
    {
        "paper_id": 1903.02833,
        "authors": "Chloe Lacombe, Aitor Muguruza and Henry Stone",
        "title": "Asymptotics for volatility derivatives in multi-factor rough volatility\n  models",
        "comments": "28 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present small-time implied volatility asymptotics for Realised Variance\n(RV) and VIX options for a number of (rough) stochastic volatility models via\nlarge deviations principle. We provide numerical results along with efficient\nand robust numerical recipes to compute the rate function; the backbone of our\ntheoretical framework. Based on our results, we further develop approximation\nschemes for the density of RV, which in turn allows to express the volatility\nswap in close-form. Lastly, we investigate different constructions of\nmulti-factor models and how each of them affects the convexity of the implied\nvolatility smile. Interestingly, we identify the class of models that generate\nnon-linear smiles around-the-money.\n"
    },
    {
        "paper_id": 1903.02924,
        "authors": "David S. Lucas and Christopher J. Boudreaux",
        "title": "The Interdependence of Hierarchical Institutions: Federal Regulation,\n  Job Creation, and the Moderating Effect of State Economic Freedom",
        "comments": "46 pages. 1 Figure, 5 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regulation is commonly viewed as a hindrance to entrepreneurship, but\nheterogeneity in the effects of regulation is rarely explored. We focus on\nregional variation in the effects of national-level regulations by developing a\ntheory of hierarchical institutional interdependence. Using the political\nscience theory of market-preserving federalism, we argue that regional economic\nfreedom attenuates the negative influence of national regulation on net job\ncreation. Using U.S. data, we find that regulation destroys jobs on net, but\nregional economic freedom moderates this effect. In regions with average\neconomic freedom, a one percent increase in regulation results in 14 fewer jobs\ncreated on net. However, a standard deviation increase in economic freedom\nattenuates this relationship by four fewer jobs. Interestingly, this moderation\naccrues strictly to older firms; regulation usually harms young firm job\ncreation, and economic freedom does not attenuate this relationship.\n"
    },
    {
        "paper_id": 1903.02934,
        "authors": "Christopher J. Boudreaux",
        "title": "Entrepreneurship, Institutions, and Economic Growth: Does the Level of\n  Development Matter?",
        "comments": "30 pages, 5 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Entrepreneurship is often touted for its ability to generate economic growth.\nThrough the creative-destructive process, entrepreneurs are often able to\ninnovate and outperform incumbent organizations, all of which is supposed to\nlead to higher employment and economic growth. Although some empirical evidence\nsupports this logic, it has also been the subject of recent criticisms.\nSpecifically, entrepreneurship does not lead to growth in developing countries;\nit only does in more developed countries with higher income levels. Using\nGlobal Entrepreneurship Monitor data for a panel of 83 countries from 2002 to\n2014, we examine the contribution of entrepreneurship towards economic growth.\nOur evidence validates earlier studies findings but also exposes previously\nundiscovered findings. That is, we find that entrepreneurship encourages\neconomic growth but not in developing countries. In addition, our evidence\nfinds that the institutional environment of the country, as measured by GEM\nEntrepreneurial Framework Conditions, only contributes to economic growth in\nmore developed countries but not in developing countries. These findings have\nimportant policy implications. Namely, our evidence contradicts policy\nproposals that suggest entrepreneurship and the adoption of pro-market\ninstitutions that support it to encourage economic growth in developing\ncountries. Our evidence suggests these policy proposals will be unlikely to\ngenerate the economic growth desired.\n"
    },
    {
        "paper_id": 1903.03201,
        "authors": "Junqing Tang, Hans R. Heinimann",
        "title": "Quantitative evaluation of consecutive resilience cycles in stock market\n  performance: A systems-oriented approach",
        "comments": "27 pages, 9 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.121794",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets can be seen as complex systems that are constantly evolving\nand sensitive to external disturbance, such as systemic risks and economic\ninstabilities. Analysis of resilient market performance, therefore, becomes\nuseful for investors. From a systems perspective, this paper proposes a novel\nfunction-based resilience metric that considers the effect of two\nfault-tolerance thresholds: the Robustness Range (RR) and the Elasticity\nThreshold (ET). We examined the consecutive resilience cycles and their\ndynamics in the performance of two stock markets, NASDAQ and SSE. The proposed\nmetric was also compared with three well-documented resilience models. The\nresults showed that this new metric could satisfactorily quantify the\ntime-varying resilience cycles in the multi-cycle volatile performance of stock\nmarkets while also being more feasible in comparative analysis. Furthermore,\nanalysis of dynamics revealed that those consecutive resilience cycles in\nmarket performance were distributed non-linearly, following a power-law\nbehavior in the upper tail. Finally, sensitivity tests demonstrated the\nlarge-value resilience cycles were relatively sensitive to changes in RR. In\npractice, RR could indicate investors' psychological capability to withstand\ndownturns. It supports the observation that perception on the market's\nresilient responses may vary among investors. This study provides a new tool\nand valuable insight for researchers, practitioners, and investors when\nevaluating market performance.\n"
    },
    {
        "paper_id": 1903.03202,
        "authors": "Alexander James, Yaser S. Abu-Mostafa, Xiao Qiao",
        "title": "Nowcasting Recessions using the SVM Machine Learning Algorithm",
        "comments": "My company policy about sharing research papers has been changed. As\n  a result, I would like to withdraw the paper, with the full understanding\n  that previous version will remain accessible. Thank you very much",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel application of Support Vector Machines (SVM), an\nimportant Machine Learning algorithm, to determine the beginning and end of\nrecessions in real time. Nowcasting, \"forecasting\" a condition about the\npresent time because the full information about it is not available until\nlater, is key for recessions, which are only determined months after the fact.\nWe show that SVM has excellent predictive performance for this task, and we\nprovide implementation details to facilitate its use in similar problems in\neconomics and finance.\n"
    },
    {
        "paper_id": 1903.03203,
        "authors": "Peter Klimek, Sebastian Poledna, Stefan Thurner",
        "title": "Economic resilience from input-output susceptibility improves\n  predictions of economic growth and recovery",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41467-019-09357-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern macroeconomic theories were unable to foresee the last Great Recession\nand could neither predict its prolonged duration nor the recovery rate. They\nare based on supply-demand equilibria that do not exist during recessionary\nshocks. Here we focus on resilience as a nonequilibrium property of networked\nproduction systems and develop a linear response theory for input-output\neconomics. By calibrating the framework to data from 56 industrial sectors in\n43 countries between 2000 and 2014, we find that the susceptibility of\nindividual industrial sectors to economic shocks varies greatly across\ncountries, sectors, and time. We show that susceptibility-based predictions\nthat take sector- and country-specific recovery into account, outperform--by\nfar--standard econometric growth-models. Our results are analytically rigorous,\nempirically testable, and flexible enough to address policy-relevant scenarios.\nWe illustrate the latter by estimating the impact of recently imposed tariffs\non US imports (steel and aluminum) on specific sectors across European\ncountries.\n"
    },
    {
        "paper_id": 1903.03304,
        "authors": "Suparna Biswas and Rituparna Sen",
        "title": "Kernel Based Estimation of Spectral Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spectral risk measures (SRMs) belong to the family of coherent risk measures.\nA natural estimator for the class of SRMs has the form of L-statistics. Various\nauthors have studied and derived the asymptotic properties of the empirical\nestimator of SRM. We propose a kernel based estimator of SRM. We investigate\nthe large sample properties of general L-statistics based on i.i.d and\ndependent observations and apply them to our estimator. We prove that it is\nstrongly consistent and asymptotically normal. We compare the finite sample\nperformance of our proposed kernel estimator with that of several existing\nestimators for different SRMs using Monte Carlo simulation. We observe that our\nproposed kernel estimator outperforms all the estimators. Based on our\nsimulation study we have estimated the exponential SRM of four future\nindices-that is Nikkei 225, Dax, FTSE 100, and Hang Seng. We also discuss the\nuse of SRM in setting initial margin requirements of clearinghouses. Finally we\nperform a backtesting exercise of SRM.\n"
    },
    {
        "paper_id": 1903.03407,
        "authors": "Charu Sharma and Amber Habib",
        "title": "Uncovering networks amongst stocks returns by studying nonlinear\n  interactions in high frequency data of the Indian Stock Market using mutual\n  information",
        "comments": "32 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore the detection of clusters of stocks that are in\nsynergy in the Indian Stock Market and understand their behaviour in different\ncircumstances. We have based our study on high frequency data for the year\n2014. This was a year when general elections were held in India, keeping this\nin mind our data set was divided into 3 subsets, pre-election period: Jan-Feb\n2014; election period: Mar-May 2014 and :post-election period: Jun-Dec 2014. On\nanalysing the spectrum of the correlation matrix, quite a few deviations were\nobserved from RMT indicating a correlation across all the stocks. We then used\nmutual information to capture the non-linearity of the data and compared our\nresults with widely used correlation technique using minimum spanning tree\nmethod. With a larger value of power law exponent {\\alpha}, corresponding to\ndistribution of degrees in a network, the nonlinear method of mutual\ninformation succeeds in establishing effective network in comparison to the\ncorrelation method. Of the two prominent clusters detected by our analysis, one\ncorresponds to the financial sector and another to the energy sector. The\nfinancial sector emerged as an isolated, standalone cluster, which remain\nunaffected even during the election periods.\n"
    },
    {
        "paper_id": 1903.03721,
        "authors": "David Landriault, Bin Li and Mohamed Amine Lkabous",
        "title": "On occupation times in the red of L\\'evy risk models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we obtain analytical expression for the distribution of the\noccupation time in the red (below level $0$) up to an (independent) exponential\nhorizon for spectrally negative L\\'{e}vy risk processes and refracted\nspectrally negative L\\'{e}vy risk processes. This result improves the existing\nliterature in which only the Laplace transforms are known. Due to the close\nconnection between occupation time and many other quantities, we provide a few\napplications of our results including future drawdown, inverse occupation time,\nParisian ruin with exponential delay, and the last time at running maximum. By\na further Laplace inversion to our results, we obtain the distribution of the\noccupation time up to a finite time horizon for refracted Brownian motion risk\nprocess and refracted Cram\\'{e}r-Lundberg risk model with exponential claims.\n"
    },
    {
        "paper_id": 1903.03887,
        "authors": "Mathias Beiglb\\\"ock, Marcel Nutz, Florian Stebegg",
        "title": "Fine Properties of the Optimal Skorokhod Embedding Problem",
        "comments": "Forthcoming in 'Journal of the European Mathematical Society (JEMS)'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of stopping a Brownian motion at a given distribution\n$\\nu$ while optimizing a reward function that depends on the (possibly\nrandomized) stopping time and the Brownian motion. Our first result establishes\nthat the set $\\mathcal{T}(\\nu)$ of stopping times embedding $\\nu$ is weakly\ndense in the set $\\mathcal{R}(\\nu)$ of randomized embeddings. In particular,\nthe optimal Skorokhod embedding problem over $\\mathcal{T}(\\nu)$ has the same\nvalue as the relaxed one over $\\mathcal{R}(\\nu)$ when the reward function is\nsemicontinuous, which parallels a fundamental result about Monge maps and\nKantorovich couplings in optimal transport. A second part studies the dual\noptimization in the sense of linear programming. While existence of a dual\nsolution failed in previous formulations, we introduce a relaxation of the dual\nproblem that exploits a novel compactness property and yields existence of\nsolutions as well as absence of a duality gap, even for irregular reward\nfunctions. This leads to a monotonicity principle which complements the key\ntheorem of Beiglb\\\"ock, Cox and Huesmann [Optimal transport and Skorokhod\nembedding, Invent. Math., 208:327-400, 2017]. We show that these results can be\napplied to characterize the geometry of optimal embeddings through a\nvariational condition.\n"
    },
    {
        "paper_id": 1903.03925,
        "authors": "Tatsushi Oka and Ken Yamada",
        "title": "Heterogeneous Impact of the Minimum Wage: Implications for Changes in\n  Between- and Within-group Inequality",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3368/jhr.58.3.0719-10339R1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Workers who earn at or below the minimum wage in the United States are mostly\neither less educated, young, or female. Little is known, however, concerning\nthe extent to which the minimum wage influences wage differentials among\nworkers with different observed characteristics and among workers with the same\nobserved characteristics. This paper shows that changes in the real value of\nthe minimum wage over recent decades have affected the relationship of hourly\nwages with education, experience, and gender. The results suggest that changes\nin the real value of the minimum wage account in part for the patterns of\nchanges in education, experience, and gender wage differentials and mostly for\nthe patterns of changes in within-group wage differentials among female workers\nwith lower levels of experience.\n"
    },
    {
        "paper_id": 1903.03969,
        "authors": "Marcel Br\\\"autigam, Michel Dacorogna, and Marie Kratz",
        "title": "Pro-Cyclicality of Traditional Risk Measurements: Quantifying and\n  Highlighting Factors at its Source",
        "comments": "49 pages, 9 figures, 15 tables. Changes to previous version:\n  Restructured the introduction; added missing regression lines in Figures 5\n  and 7; added an appendix with further material",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the introduction of risk-based solvency regulation, pro-cyclicality has\nbeen a subject of concerns from all market participants. Here, we lay down a\nmethodology to evaluate the amount of pro-cyclicality in the way finnancial\ninstitutions measure risk, and identify factors explaining this pro-cyclical\nbehavior. We introduce a new indicator based on the Sample Quantile Process\n(SQP, a dynamic generalization of Value-at-Risk), conditioned on realized\nvolatility to quantify the pro-cyclicality, and evaluate its amount in the\nmarkets, considering 11 stock indices as realizations of the SQP. Then we\ndetermine two main factors explaining the pro-cyclicality: the clustering and\nreturn-to-the-mean of volatility, as it could have been anticipated but not\nquantified before, and, more surprisingly, the very way risk is measured,\nindependently of this return-to-the-mean effect.\n"
    },
    {
        "paper_id": 1903.04035,
        "authors": "George Liberopoulos and Isidoros Tsikis",
        "title": "Retailer response to wholesale stockouts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to identify the immediate and future retailer\nresponse to wholesale stockouts. We perform a statistical analysis of\nhistorical customer order and delivery data of a local tool wholesaler and\ndistributor, whose customers are retailers, over a period of four years. We\ninvestigate the effect of customer service on the order fill rate and the rate\nof future demand, where the customer service is defined in terms of timely\ndelivery and the fill rate is defined as the fraction of the order that is\neventually materialized, i.e., is not cancelled following a stockout.We find\nthat for customers who order frequently, stockouts have an adverse effect on\nthe fill rate of their orders and on the frequency but not the value of their\nfuture demand; however, this latter effect seems to be more short-term than\nlong-term. Practically all studies on the effects of stockouts measure\nimmediate reported/intended consumer purchase incidence and choice decision\nbehavior in response to stockouts in retail environments, mostly based on\nsurveys. This study looks at how stockouts affect future demand in a wholesale\nenvironment, based on historical behavioral data analysis.\n"
    },
    {
        "paper_id": 1903.0406,
        "authors": "Toomas Hinnosaar",
        "title": "Stackelberg Independence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The standard model of sequential capacity choices is the Stackelberg quantity\nleadership model with linear demand. I show that under the standard\nassumptions, leaders' actions are informative about market conditions and\nindependent of leaders' beliefs about the arrivals of followers. However, this\nStackelberg independence property relies on all standard assumptions being\nsatisfied. It fails to hold whenever the demand function is non-linear,\nmarginal cost is not constant, goods are differentiated, firms are\nnon-identical, or there are any externalities. I show that small deviations\nfrom the linear demand assumption may make the leaders' choices completely\nuninformative.\n"
    },
    {
        "paper_id": 1903.04106,
        "authors": "Hyong-Chol O, Dae-Sung Choe",
        "title": "Pricing Formulae of Power Binary and Normal Distribution Standard\n  Options and Applications",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the Buchen's pricing formulae of (higher order) asset and bond\nbinary options are incorporated into the pricing formula of power binary\noptions and a pricing formula of \"the normal distribution standard options\"\nwith the maturity payoff related to a power function and the density function\nof normal distribution is derived. And as their applications, pricing formulae\nof savings plans that provide a choice of indexing and discrete geometric\naverage Asian options are derived and the fact that the price of discrete\ngeometric average Asian option converges to the price of continuous geometric\naverage Asian option when the largest distance between neighboring monitoring\ntimes goes to zero is proved.\n"
    },
    {
        "paper_id": 1903.04211,
        "authors": "Cheikh Mbaye and Fr\\'ed\\'eric Vrins",
        "title": "Affine term structure models : a time-changed approach with perfect fit\n  to market curves",
        "comments": "44 pages, figures and tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the so-called calibration problem which consists of fitting in a\ntractable way a given model to a specified term structure like, e.g., yield or\ndefault probability curves. Time-homogeneous jump-diffusions like Vasicek or\nCox-Ingersoll-Ross (possibly coupled with compounded Poisson jumps, JCIR), are\ntractable processes but have limited flexibility; they fail to replicate actual\nmarket curves. The deterministic shift extension of the latter (Hull-White or\nJCIR++) is a simple but yet efficient solution that is widely used by both\nacademics and practitioners. However, the shift approach is often not\nappropriate when positivity is required, which is a common constraint when\ndealing with credit spreads or default intensities. In this paper, we tackle\nthis problem by adopting a time change approach. On the top of providing an\nelegant solution to the calibration problem under positivity constraint, our\nmodel features additional interesting properties in terms of implied\nvolatilities. It is compared to the shift extension on various credit risk\napplications such as credit default swap, credit default swaption and credit\nvaluation adjustment under wrong-way risk. The time change approach is able to\ngenerate much larger volatility and covariance effects under the positivity\nconstraint. Our model offers an appealing alternative to the shift in such\ncases.\n"
    },
    {
        "paper_id": 1903.04257,
        "authors": "Yue Yang and Xiang Yu",
        "title": "Optimal Entry and Consumption under Habit Formation",
        "comments": "Final version, forthcoming in Advances in Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a composite problem involving the decision making of the\noptimal entry time and dynamic consumption afterwards. In stage-1, the investor\nhas access to full market information subjecting to some information costs and\nneeds to choose an optimal stopping time to initiate stage-2; in stage-2, the\ninvestor terminates the costly full information acquisition and starts dynamic\ninvestment and consumption under partial observations of free public stock\nprices. The habit formation preference is employed, in which the past\nconsumption affects the investor's current decisions. By using the stochastic\nPerron's method, the value function of the composite problem is proved to be\nthe unique viscosity solution of some variational inequalities.\n"
    },
    {
        "paper_id": 1903.04305,
        "authors": "Baogui Xin, Wei Peng, Yekyung Kwon",
        "title": "A fractional-order difference Cournot duopoly game with long memory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reconsider the Cournot duopoly problem in light of the theory for long\nmemory. We introduce the Caputo fractional-order difference calculus to\nclassical duopoly theory to propose a fractional-order discrete Cournot duopoly\ngame model, which allows participants to make decisions while making full use\nof their historical information. Then we discuss Nash equilibria and local\nstability by using linear approximation. Finally, we detect the chaos of the\nmodel by employing a 0-1 test algorithm.\n"
    },
    {
        "paper_id": 1903.04841,
        "authors": "Joan Gonzalvez, Edmond Lezmi, Thierry Roncalli, Jiali Xu",
        "title": "Financial Applications of Gaussian Processes and Bayesian Optimization",
        "comments": "42 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the last five years, the financial industry has been impacted by the\nemergence of digitalization and machine learning. In this article, we explore\ntwo methods that have undergone rapid development in recent years: Gaussian\nprocesses and Bayesian optimization. Gaussian processes can be seen as a\ngeneralization of Gaussian random vectors and are associated with the\ndevelopment of kernel methods. Bayesian optimization is an approach for\nperforming derivative-free global optimization in a small dimension, and uses\nGaussian processes to locate the global maximum of a black-box function. The\nfirst part of the article reviews these two tools and shows how they are\nconnected. In particular, we focus on the Gaussian process regression, which is\nthe core of Bayesian machine learning, and the issue of hyperparameter\nselection. The second part is dedicated to two financial applications. We first\nconsider the modeling of the term structure of interest rates. More precisely,\nwe test the fitting method and compare the GP prediction and the random walk\nmodel. The second application is the construction of trend-following\nstrategies, in particular the online estimation of trend and covariance\nwindows.\n"
    },
    {
        "paper_id": 1903.04901,
        "authors": "Ilya Molchanov and Anja M\\\"uhlemann",
        "title": "Nonlinear expectations of random sets",
        "comments": "35 pages, 1 figure",
        "journal-ref": "Finance and Stochastics, 2021, 25, 5-41 (open access)",
        "doi": "10.1007/s00780-020-00442-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sublinear functionals of random variables are known as sublinear\nexpectations; they are convex homogeneous functionals on infinite-dimensional\nlinear spaces. We extend this concept for set-valued functionals defined on\nmeasurable set-valued functions (which form a nonlinear space), equivalently,\non random closed sets. This calls for a separate study of sublinear and\nsuperlinear expectations, since a change of sign does not convert one to the\nother in the set-valued setting. We identify the extremal expectations as those\narising from the primal and dual representations of them. Several general\nconstruction methods for nonlinear expectations are presented and the\ncorresponding duality representation results are obtained. On the application\nside, sublinear expectations are naturally related to depth trimming of\nmultivariate samples, while superlinear ones can be used to assess utilities of\nmultiasset portfolios.\n"
    },
    {
        "paper_id": 1903.04954,
        "authors": "Robert L. Axtell, Omar A. Guerrero and Eduardo L\\'opez",
        "title": "Frictional Unemployment on Labor Flow Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an alternative theory to the aggregate matching function in which\nworkers search for jobs through a network of firms: the labor flow network. The\nlack of an edge between two companies indicates the impossibility of labor\nflows between them due to high frictions. In equilibrium, firms' hiring\nbehavior correlates through the network, generating highly disaggregated local\nunemployment. Hence, aggregation depends on the topology of the network in\nnon-trivial ways. This theory provides new micro-foundations for the Beveridge\ncurve, wage dispersion, and the employer-size premium. We apply our model to\nemployer-employee matched records and find that network topologies with\nPareto-distributed connections cause disproportionately large changes on\naggregate unemployment under high labor supply elasticity.\n"
    },
    {
        "paper_id": 1903.0502,
        "authors": "Felix Poege, Dietmar Harhoff, Fabian Gaessler, Stefano Baruffaldi",
        "title": "Science Quality and the Value of Inventions",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": "10.1126/sciadv.aay7323",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite decades of research, the relationship between the quality of science\nand the value of inventions has remained unclear. We present the result of a\nlarge-scale matching exercise between 4.8 million patent families and 43\nmillion publication records. We find a strong positive relationship between\nquality of scientific contributions referenced in patents and the value of the\nrespective inventions. We rank patents by the quality of the science they are\nlinked to. Strikingly, high-rank patents are twice as valuable as low-rank\npatents, which in turn are about as valuable as patents without direct science\nlink. We show this core result for various science quality and patent value\nmeasures. The effect of science quality on patent value remains relevant even\nwhen science is linked indirectly through other patents. Our findings imply\nthat what is considered \"excellent\" within the science sector also leads to\noutstanding outcomes in the technological or commercial realm.\n"
    },
    {
        "paper_id": 1903.05189,
        "authors": "Hyong-chol O, Song-San Jo",
        "title": "Variational inequality for perpetual American option price and\n  convergence to the solution of the difference equation",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A variational inequality for pricing the perpetual American option and the\ncorresponding difference equation are considered. First, the maximum principle\nand uniqueness of the solution to variational inequality for pricing the\nperpetual American option are proved. Then the maximum principle, the existence\nand uniqueness of the solution to the difference equation corresponding to the\nvariational inequality for pricing the perpetual American option and the\nsolution representation are provided and the fact that the solution to the\ndifference equation converges to the viscosity solution to the variational\ninequality is proved. It is shown that the limits of the prices of variational\ninequality and BTM models for American Option when the maturity goes to\ninfinity do not depend on time and they become the prices of the perpetual\nAmerican option.\n"
    },
    {
        "paper_id": 1903.05322,
        "authors": "Rituparna Sen and Manavthi S",
        "title": "Stylized facts of the Indian Stock Market",
        "comments": null,
        "journal-ref": "2019 Asia-Pacific Financial Markets, 26(4): 479-493",
        "doi": "10.1007/s10690-019-09275-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical daily data for eleven years of the fifty constituent stocks of the\nNIFTY index traded on the National Stock Exchange have been analyzed to check\nfor the stylized facts in the Indian market. It is observed that while some\nstylized facts of other markets are also observed in Indian market, there are\nsignificant deviations in three main aspects, namely leverage, asymmetry and\nautocorrelation. Leverage and asymmetry are both reversed making this a more\npromising market to invest in. While significant autocorrelation observed in\nthe returns points towards market inefficiency, the increased predictive power\nis better for investors.\n"
    },
    {
        "paper_id": 1903.05747,
        "authors": "Axel A. Araneda",
        "title": "The fractional and mixed-fractional CEV model",
        "comments": "The final version of the paper, after the referee process",
        "journal-ref": "Journal of computational and applied mathematics, 2019",
        "doi": "10.1016/j.cam.2019.06.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The continuous observation of the financial markets has identified some\nstylized facts which challenge the conventional assumptions, promoting the born\nof new approaches. On the one hand, the long-range dependence has been faced\nreplacing the traditional Gauss-Wiener process (Brownian motion), characterized\nby stationary independent increments, by a fractional version. On the other\nhand, the CEV model addresses the Leverage effect and smile-skew phenomena,\nefficiently. In this paper, these two insights are merging and both the\nfractional and mixed-fractional extensions for the CEV model, are developed.\nUsing the fractional versions of both the Ito's calculus and the Fokker-Planck\nequation, the transition probability density function of the asset price is\nobtained as the solution of a non-stationary Feller process with time-varying\ncoefficients, getting an analytical valuation formula for a European Call\noption. Besides, the Greeks are computed and compared with the standard case.\n"
    },
    {
        "paper_id": 1903.05753,
        "authors": "Enzo Busseti",
        "title": "Derivative of a Conic Problem with a Unique Solution",
        "comments": "The article is being withdrawn because it is not finished",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We view a conic optimization problem that has a unique solution as a map from\nits data to its solution. If sufficient regularity conditions hold at a\nsolution point, namely that the implicit function theorem applies to the\nnormalized residual function of [Busseti et al., 2018], the problem solution\nmap is differentiable. We obtain the derivative, in the form of an abstract\nlinear operator. This applies to any convex optimization problem in conic form,\nwhile a previous result [Amos et al., 2016] studied strictly convex quadratic\nprograms. Such differentiable problems can be used, for example, in machine\nlearning, control, and related areas, as a layer in an end-to-end learning and\ncontrol procedure, for backpropagation. We accompany this note with a\nlightweight Python implementation which can handle problems with the cone\nconstraints commonly used in practice.\n"
    },
    {
        "paper_id": 1903.05781,
        "authors": "Huong Dinh, Manannan Donoghoe, Neal Hughes and Tim Goesch",
        "title": "A micro-simulation model of irrigation farms in the southern\n  Murray-Darling Basin",
        "comments": "44 pages, 18 figures, 18 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a farm level irrigation microsimulation model of the\nsouthern Murray-Darling Basin. The model leverages detailed ABARES survey data\nto estimate a series of input demand and output supply equations, derived from\na normalised quadratic profit function. The parameters from this estimation are\nthen used to simulate the impact on total cost, revenue and profit of a\nhypothetical 30 per cent increase in the price of water. The model is still\nunder development, with several potential improvements suggested in the\nconclusion. This is a working paper, provided for the purpose of receiving\nfeedback on the analytical approach to improve future iterations of the\nmicrosimulation model.\n"
    },
    {
        "paper_id": 1903.0599,
        "authors": "Thomas Bernhardt and Catherine Donnelly",
        "title": "Modern tontine with bequest: innovation in pooled annuity products",
        "comments": "40 pages, 23 pictures. Insurance: Mathematics and Economics(2019)",
        "journal-ref": "Insurance: Mathematics and Economics 86 (2019) 168-188",
        "doi": "10.1016/j.insmatheco.2019.03.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new pension product that offers retirees the opportunity for a\nlifelong income and a bequest for their estate. Based on a tontine mechanism,\nthe product divides pension savings between a tontine account and a bequest\naccount. The tontine account is given up to a tontine pool upon death while the\nbequest account value is paid to the retiree's estate. The values of these two\naccounts are continuously re-balanced to the same proportion, which is the key\nfeature of our new product. Our main research question about the new product is\nwhat proportion of pension savings should a retiree allocate to the tontine\naccount. Under a power utility function, we show that more risk averse retirees\nallocate a fairly stable proportion of their pension savings to the tontine\naccount, regardless of the strength of their bequest motive. The proportion\ndeclines as the retiree becomes less risk averse for a while. However, for the\nleast risk averse retirees, a high proportion of their pension savings is\noptimally allocated to the tontine account. This surprising result is explained\nby the least risk averse retirees seeking the potentially high value of the\nbequest account at very old ages.\n"
    },
    {
        "paper_id": 1903.06033,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Altcoin-Bitcoin Arbitrage",
        "comments": "26 pages; a few trivial typos corrected. arXiv admin note:\n  substantial text overlap with arXiv:1811.07860",
        "journal-ref": "Bulletin of Applied Economics 6(1) (2019) 87-110",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an algorithm and source code for a cryptoasset statistical arbitrage\nalpha based on a mean-reversion effect driven by the leading momentum factor in\ncryptoasset returns discussed in https://ssrn.com/abstract=3245641. Using\nempirical data, we identify the cross-section of cryptoassets for which this\naltcoin-Bitcoin arbitrage alpha is significant and discuss it in the context of\nliquidity considerations as well as its implications for cryptoasset trading.\n"
    },
    {
        "paper_id": 1903.06042,
        "authors": "Francesco Cordoni, Luca Di Persio and Luca Prezioso",
        "title": "A lending scheme for a system of interconnected banks with probabilistic\n  constraints of failure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a closed form solution for an optimal control problem related to an\ninterbank lending schemes subject to terminal probability constraints on the\nfailure of banks which are interconnected through a financial network. The\nderived solution applies to a real banks network by obtaining a general\nsolution when the aforementioned probability constraints are assumed for all\nthe banks. We also present a direct method to compute the systemic relevance\nparameter for each bank within the network.\n"
    },
    {
        "paper_id": 1903.0623,
        "authors": "Nicholas Moehle and Enzo Busseti and Stephen Boyd and Matt Wytock",
        "title": "Dynamic Energy Management",
        "comments": "63 pages, 15 figures, accompanying open source library",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a unified method, based on convex optimization, for managing the\npower produced and consumed by a network of devices over time. We start with\nthe simple setting of optimizing power flows in a static network, and then\nproceed to the case of optimizing dynamic power flows, i.e., power flows that\nchange with time over a horizon. We leverage this to develop a real-time\ncontrol strategy, model predictive control, which at each time step solves a\ndynamic power flow optimization problem, using forecasts of future quantities\nsuch as demands, capacities, or prices, to choose the current power flow\nvalues. Finally, we consider a useful extension of model predictive control\nthat explicitly accounts for uncertainty in the forecasts. We mirror our\nframework with an object-oriented software implementation, an open-source\nPython library for planning and controlling power flows at any scale. We\ndemonstrate our method with various examples. Appendices give more detail about\nthe package, and describe some basic but very effective methods for\nconstructing forecasts from historical data.\n"
    },
    {
        "paper_id": 1903.06334,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Machine Learning Risk Models",
        "comments": "26 pages; a few trivial typos corrected",
        "journal-ref": "Journal of Risk & Control 6(1) (2019) 37-64",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit algorithm and source code for constructing risk models\nbased on machine learning techniques. The resultant covariance matrices are not\nfactor models. Based on empirical backtests, we compare the performance of\nthese machine learning risk models to other constructions, including\nstatistical risk models, risk models based on fundamental industry\nclassifications, and also those utilizing multilevel clustering based industry\nclassifications.\n"
    },
    {
        "paper_id": 1903.06346,
        "authors": "Rongju Zhang and Mark Aarons and Gregoire Loeper",
        "title": "Optimal FX Hedge Tenor with Liquidity Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an optimal currency hedging strategy for fund managers who own\nforeign assets to choose the hedge tenors that maximize their FX carry returns\nwithin a liquidity risk constraint. The strategy assumes that the offshore\nassets are fully hedged with FX forwards. The chosen liquidity risk metric is\nCash Flow at Risk (CFaR). The strategy involves time-dispersing the total\nnominal hedge value into future time buckets to maximize (minimize) the\nexpected FX carry benefit (cost), given the constraint that the CFaRs in all\nthe future time buckets do not breach a predetermined liquidity budget. We\ndemonstrate the methodology via an illustrative example where shorter-dated\nforwards are assumed to deliver higher carry trade returns (motivated by the\nhistorical experience where AUD is the domestic currency and USD is the foreign\ncurrency). We also introduce a tenor-ranking method which is useful when this\nassumption fails. We show by Monte Carlo simulation and by backtesting that our\nhedging strategy successfully operates within the liquidity budget. We provide\npractical insights on when and why fund managers should choose short-dated or\nlong-dated tenors.\n"
    },
    {
        "paper_id": 1903.06478,
        "authors": "Sang Il Lee and Seong Joon Yoo",
        "title": "Multimodal Deep Learning for Finance: Integrating and Forecasting\n  International Stock Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In today's increasingly international economy, return and volatility\nspillover effects across international equity markets are major macroeconomic\ndrivers of stock dynamics. Thus, information regarding foreign markets is one\nof the most important factors in forecasting domestic stock prices. However,\nthe cross-correlation between domestic and foreign markets is highly complex.\nHence, it is extremely difficult to explicitly express this cross-correlation\nwith a dynamical equation. In this study, we develop stock return prediction\nmodels that can jointly consider international markets, using multimodal deep\nlearning. Our contributions are three-fold: (1) we visualize the transfer\ninformation between South Korea and US stock markets by using scatter plots;\n(2) we incorporate the information into the stock prediction models with the\nhelp of multimodal deep learning; (3) we conclusively demonstrate that the\nearly and intermediate fusion models achieve a significant performance boost in\ncomparison with the late fusion and single modality models. Our study indicates\nthat jointly considering international stock markets can improve the prediction\naccuracy and deep neural networks are highly effective for such tasks.\n"
    },
    {
        "paper_id": 1903.06632,
        "authors": "Masoud Fekri, Babak Barazandeh",
        "title": "Designing an Optimal Portfolio for Iran's Stock Market with Genetic\n  Algorithm using Neural Network Prediction of Risk and Return Stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal capital allocation between different assets is an important financial\nproblem, which is generally framed as the portfolio optimization problem.\nGeneral models include the single-period and multi-period cases. The\ntraditional Mean-Variance model introduced by Harry Markowitz has been the\nbasis of many models used to solve the portfolio optimization problem. The\noverall goal is to achieve the highest return and lowest risk in portfolio\noptimization problems. In this paper, we will present an optimal portfolio\nbased the Markowitz Mean-Variance-Skewness with weight constraints model for\nshort-term investment opportunities in Iran's stock market. We will use a\nneural network based predictor to predict the stock returns and measure the\nrisk of stocks based on the prediction errors in the neural network. We will\nperform a series of experiments on our portfolio optimization model with the\nreal data from Iran's stock market indices including Bank, Insurance,\nInvestment, Petroleum Products and Chemicals indices. Finally, 8 different\nportfolios with low, medium and high risks for different type of investors\n(risk-averse or risk taker) using genetic algorithm will be designed and\nanalyzed.\n"
    },
    {
        "paper_id": 1903.06668,
        "authors": "Ekaterina Abramova and Derek Bunn",
        "title": "Estimating Dynamic Conditional Spread Densities to Optimise Daily\n  Storage Trading of Electricity",
        "comments": "59 pages, 37 figures, CEMA 2019, POM Special Issue",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper formulates dynamic density functions, based upon skewed-t and\nsimilar representations, to model and forecast electricity price spreads\nbetween different hours of the day. This supports an optimal day ahead storage\nand discharge schedule, and thereby facilitates a bidding strategy for a\nmerchant arbitrage facility into the day-ahead auctions for wholesale\nelectricity. The four latent moments of the density functions are dynamic and\nconditional upon exogenous drivers, thereby permitting the mean, variance,\nskewness and kurtosis of the densities to respond hourly to such factors as\nweather and demand forecasts. The best specification for each spread is\nselected based on the Pinball Loss function, following the closed form\nanalytical solutions of the cumulative density functions. Those analytical\nproperties also allow the calculation of risk associated with the spread\narbitrages. From these spread densities, the optimal daily operation of a\nbattery storage facility is determined.\n"
    },
    {
        "paper_id": 1903.06751,
        "authors": "Dat Thanh Tran, Juho Kanniainen, Moncef Gabbouj, Alexandros Iosifidis",
        "title": "Data-driven Neural Architecture Learning For Financial Time-series\n  Forecasting",
        "comments": "Accepted in DISP2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasting based on financial time-series is a challenging task since most\nreal-world data exhibits nonstationary property and nonlinear dependencies. In\naddition, different data modalities often embed different nonlinear\nrelationships which are difficult to capture by human-designed models. To\ntackle the supervised learning task in financial time-series prediction, we\npropose the application of a recently formulated algorithm that adaptively\nlearns a mapping function, realized by a heterogeneous neural architecture\ncomposing of Generalized Operational Perceptron, given a set of labeled data.\nWith a modified objective function, the proposed algorithm can accommodate the\nfrequently observed imbalanced data distribution problem. Experiments on a\nlarge-scale Limit Order Book dataset demonstrate that the proposed algorithm\noutperforms related algorithms, including tensor-based methods which have\naccess to a broader set of input information.\n"
    },
    {
        "paper_id": 1903.06912,
        "authors": "Ale\\v{s} \\v{C}ern\\'y",
        "title": "Semimartingale theory of monotone mean--variance portfolio allocation",
        "comments": "(v4) updated bibliography",
        "journal-ref": "Mathematical Finance 30(3), 1168-1178, 2020",
        "doi": "10.1111/mafi.12241",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study dynamic optimal portfolio allocation for monotone mean--variance\npreferences in a general semimartingale model. Armed with new results in this\narea we revisit the work of Cui, Li, Wang and Zhu (2012, MAFI) and fully\ncharacterize the circumstances under which one can set aside a non-negative\ncash flow while simultaneously improving the mean--variance efficiency of the\nleft-over wealth. The paper analyzes, for the first time, the monotone hull of\nthe Sharpe ratio and highlights its relevance to the problem at hand.\n"
    },
    {
        "paper_id": 1903.06928,
        "authors": "Ali Al-Aradi and Sebastian Jaimungal",
        "title": "Active and Passive Portfolio Management with Latent Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address a portfolio selection problem that combines active\n(outperformance) and passive (tracking) objectives using techniques from convex\nanalysis. We assume a general semimartingale market model where the assets'\ngrowth rate processes are driven by a latent factor. Using techniques from\nconvex analysis we obtain a closed-form solution for the optimal portfolio and\nprovide a theorem establishing its uniqueness. The motivation for incorporating\nlatent factors is to achieve improved growth rate estimation, an otherwise\nnotoriously difficult task. To this end, we focus on a model where growth rates\nare driven by an unobservable Markov chain. The solution in this case requires\na filtering step to obtain posterior probabilities for the state of the Markov\nchain from asset price information, which are subsequently used to find the\noptimal allocation. We show the optimal strategy is the posterior average of\nthe optimal strategies the investor would have held in each state assuming the\nMarkov chain remains in that state. Finally, we implement a number of\nhistorical backtests to demonstrate the performance of the optimal portfolio.\n"
    },
    {
        "paper_id": 1903.07222,
        "authors": "Baron Law and Frederi Viens",
        "title": "Market Making under a Weakly Consistent Limit Order Book Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new market-making model, from the ground up, which is tailored\ntowards high-frequency trading under a limit order book (LOB), based on the\nwell-known classification of order types in market microstructure. Our flexible\nframework allows arbitrary order volume, price jump, and bid-ask spread\ndistributions as well as the use of market orders. It also honors the\nconsistency of price movements upon arrivals of different order types. For\nexample, it is apparent that prices should never go down on buy market orders.\nIn addition, it respects the price-time priority of LOB. In contrast to the\napproach of regular control on diffusion as in the classical Avellaneda and\nStoikov [1] market-making framework, we exploit the techniques of optimal\nswitching and impulse control on marked point processes, which have proven to\nbe very effective in modeling the order-book features. The\nHamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI) associated with\nthe control problem can be solved numerically via finite-difference method. We\nillustrate our optimal trading strategy with a full numerical analysis,\ncalibrated to the order-book statistics of a popular Exchanged-Traded Fund\n(ETF). Our simulation shows that the profit of market-making can be severely\noverstated under LOBs with inconsistent price movements.\n"
    },
    {
        "paper_id": 1903.07519,
        "authors": "Weilong Fu and Ali Hirsa",
        "title": "A fast method for pricing American options under the variance gamma\n  model",
        "comments": "16 pages, 1 Figure, 4 Tables",
        "journal-ref": "Journal of Computational Finance, 2021",
        "doi": "10.21314/JCF.2021.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate methods for pricing American options under the variance gamma\nmodel. The variance gamma process is a pure jump process which is constructed\nby replacing the calendar time by the gamma time in a Brownian motion with\ndrift, which makes it a time-changed Brownian motion. In general, the finite\ndifference method and the simulation method can be used for pricing under this\nmodel, but their speed is not satisfactory. So there is a need for fast but\naccurate approximation methods. In the case of Black-Merton-Scholes model,\nthere are fast approximation methods, but they cannot be utilized for the\nvariance gamma model. We develop a new fast method inspired by the quadratic\napproximation method, while reducing the error by making use of a machine\nlearning technique on pre-calculated quantities. We compare the performance of\nour proposed method with those of the existing methods and show that this\nmethod is efficient and accurate for practical use.\n"
    },
    {
        "paper_id": 1903.07615,
        "authors": "Uygar Ozesmi",
        "title": "The Prosumer Economy -- Being Like a Forest",
        "comments": "29 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Planetary life support systems are collapsing due to climate change and the\nbiodiversity crisis. The root cause is the existing consumer economy, coupled\nwith profit maximisation based on ecological and social externalities. Trends\ncan be reversed, civilisation may be saved by transforming the profit\nmaximising consumer economy into an ecologically and socially just economy,\nwhich we call the prosumer economy. Prosumer economy is a macro scale circular\neconomy with minimum negative or positive ecological and social impact, an\necosystem of producers and prosumers, who have synergistic and circular\nrelationships with deepened circular supply chains, networks, where leakage of\nwealth out of the system is minimised. In a prosumer economy there is no waste,\nno lasting negative impacts on the ecology and no social exploitation. The\nprosumer economy is like a lake or a forest, an economic ecosystem that is\nproductive and supportive of the planet. We are already planting this forest\nthrough Good4Trust.org, started in Turkey. Good4Trust is a community platform\nbringing together ecologically and socially just producers and prosumers.\nProsumers come together around a basic ethical tenet the golden rule and share\non the platform their good deeds. The relationship are already deepening and\ncircularity is forming to create a prosumer economy. The platforms software to\nstructure the economy is open source, and is available to be licenced to start\nGood4Trust anywhere on the planet. Complexity theory tells us that if enough\nagents in a given system adopt simple rules which they all follow, the system\nmay shift. The shift from a consumer economy to a prosumer economy has already\nstarted, the future is either ecologically and socially just or bust.\n"
    },
    {
        "paper_id": 1903.07737,
        "authors": "Enzo Busseti",
        "title": "Risk and Return models for Equity Markets and Implied Equity Risk\n  Premium",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Equity risk premium is a central component of every risk and return model in\nfinance and a key input to estimate costs of equity and capital in both\ncorporate finance and valuation. An article by Damodaran examines three broad\napproaches for estimating the equity risk premium. The first is survey based,\nit consists in asking common investors or big players like pension fund\nmanagers what they require as a premium to invest in equity. The second is to\nlook at the premia earned historically by investing in stocks, as opposed to\nrisk-free investments. The third method tries to extrapolate a market-consensus\non equity risk premium (Implied Equity Risk Premium) by analysing equity prices\non the market today. After having introduced some basic concepts and models,\nI'll briefly explain the pluses and minuses of the first two methods, and\nanalyse more deeply the third. In the end I'll show the results of my\nestimation of ERP on real data, using variants of the Implied ERP (third)\nmethod.\n"
    },
    {
        "paper_id": 1903.07809,
        "authors": "Carlos Arturo Soto Campos, Leopoldo S\\'anchez Cant\\'u and Zeus\n  Hern\\'andez Veleros",
        "title": "Dynamic Hurst Exponent in Time Series",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The market efficiency hypothesis has been proposed to explain the behavior of\ntime series of stock markets. The Black-Scholes model (B-S) for example, is\nbased on the assumption that markets are efficient. As a consequence, it is\nimpossible, at least in principle, to \"predict\" how a market behaves, whatever\nthe circumstances. Recently we have found evidence which shows that it is\npossible to find self-organized behavior in the prices of assets in financial\nmarkets during deep falls of those prices. Through a kurtosis analysis we have\nidentified a critical point that separates time series from stock markets in\ntwo different regimes: the mesokurtic segment compatible with a random walk\nregime and the leptokurtic one that allegedly follows a power law behavior. In\nthis paper we provide some evidence, showing that the Hurst exponent is a good\nestimator of the regime in which the market is operating. Finally, we propose\nthat the Hurst exponent can be considered as a critical variable in just the\nsame way as magnetization, for example, can be used to distinguish the phase of\na magnetic system in physics.\n"
    },
    {
        "paper_id": 1903.07866,
        "authors": "Lan Chu Khanh",
        "title": "The effects of institutional quality on formal and informal borrowing\n  across high-, middle-, and low-income countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the effects of institutional quality on financing choice\nof individual using a large dataset of 137,160 people from 131 countries. We\nclassify borrowing activities into three categories, including formal,\nconstructive informal, and underground borrowing. Although the result shows\nthat better institutions aids the uses of formal borrowing, the impact of\ninstitutions on constructive informal and underground borrowing among three\ncountry sub-groups differs. Higher institutional quality improves constructive\ninformal borrowing in middle-income countries but reduces the use of\nunderground borrowing in high- and low-income countries.\n"
    },
    {
        "paper_id": 1903.07875,
        "authors": "Marek Capinski",
        "title": "Non-traded call's volatility smiles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Real life hedging in the Black-Scholes model must be imperfect and if the\nstock's drift is higher than the risk free rate, leads to a profit on average.\nHence the option price is examined as a fair game agreement between the\nparties, based on expected payoffs and a simple measure of risk. The resulting\nprices result in the volatility smile.\n"
    },
    {
        "paper_id": 1903.07997,
        "authors": "Alje van Dam and Koen Frenken",
        "title": "Variety, Complexity and Economic Development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a combinatorial model of economic development. An economy develops\nby acquiring new capabilities allowing for the production of an ever greater\nvariety of products of increasingly complex products. Taking into account that\neconomies abandon the least complex products as they develop over time, we show\nthat variety first increases and then decreases in the course of economic\ndevelopment. This is consistent with the empirical pattern known as 'the hump'.\nOur results question the common association of variety with complexity. We\nfurther discuss the implications of our model for future research.\n"
    },
    {
        "paper_id": 1903.08028,
        "authors": "Jason Poulos",
        "title": "State-Building through Public Land Disposal? An Application of Matrix\n  Completion for Counterfactual Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines how homestead policies, which opened vast frontier lands\nfor settlement, influenced the development of American frontier states. It uses\na treatment propensity-weighted matrix completion model to estimate the\ncounterfactual size of these states without homesteading. In simulation\nstudies, the method shows lower bias and variance than other estimators,\nparticularly in higher complexity scenarios. The empirical analysis reveals\nthat homestead policies significantly and persistently reduced state government\nexpenditure and revenue. These findings align with continuous\ndifference-in-differences estimates using 1.46 million land patent records.\nThis study's extension of the matrix completion method to include propensity\nscore weighting for causal effect estimation in panel data, especially in\nstaggered treatment contexts, enhances policy evaluation by improving the\nprecision of long-term policy impact assessments.\n"
    },
    {
        "paper_id": 1903.08076,
        "authors": "Jamal Bouoiyour (CATT, IRMAPE), Refk Selmi (CATT, IRMAPE)",
        "title": "The Changing Geopolitics in the Arab World: Implications of the 2017\n  Gulf Crisis for Business",
        "comments": null,
        "journal-ref": "ERF 25th Annual Conference, Mar 2019, Kuwait City, Kuwait",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The international community was caught by surprise on 5 June 2017 when Saudi\nArabia, the United Arab Emirates (UAE), Bahrain and Egypt severed diplomatic\nties with Qatar, accusing it of destabilizing the region. More than one year\nafter this diplomatic rift, several questions remain unaddressed. This study\nfocuses on the regional business costs of the year-long blockade on Qatar. We\nsplit the sample to compare the stock market performances of Qatar and its\nMiddle Eastern neighbors before and after the Saudi-led Qatar boycott. We focus\nour attention on the conditional volatility process of stock market returns and\nrisks related to financial interconnectedness. We show that the Gulf crisis had\nthe most adverse impact on Qatar together with Saudi Arabia and the UAE.\nAlthough not to the same degree as these three countries, Bahrain and Egypt\nwere also harmfully affected. But shocks to the volatility process tend to have\nshort-lasting effects. Moreover, the total volatility spillovers to and from\nothers increase but moderately after the blockade. Overall, the quartet\nlobbying efforts did not achieve the intended result. Our findings underscore\nQatar's economic vulnerability but also the successful resilience strategy of\nthis tiny state. The coordinated diplomatic efforts of Qatar have been able to\nfight the economic and political embargo.\n"
    },
    {
        "paper_id": 1903.08156,
        "authors": "Huy N. Chau, Miklos Rasonyi",
        "title": "Behavioural investors in conic market models",
        "comments": "arXiv admin note: text overlap with arXiv:1606.07311",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We treat a fairly broad class of financial models which includes markets with\nproportional transaction costs. We consider an investor with cumulative\nprospect theory preferences and a non-negativity constraint on portfolio\nwealth. The existence of an optimal strategy is shown in this context in a\nclass of generalized strategies.\n"
    },
    {
        "paper_id": 1903.08255,
        "authors": "Vincent Huang and James Unwin",
        "title": "Markov Chain Models of Refugee Migration Data",
        "comments": "21 Pages, 7 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The application of Markov chains to modelling refugee crises is explored,\nfocusing on local migration of individuals at the level of cities and days. As\nan explicit example we apply the Markov chains migration model developed here\nto UNHCR data on the Burundi refugee crisis. We compare our method to a\nstate-of-the-art `agent-based' model of Burundi refugee movements, and\nhighlight that Markov chain approaches presented here can improve the match to\ndata while simultaneously being more algorithmically efficient.\n"
    },
    {
        "paper_id": 1903.08307,
        "authors": "Nima Khodakarami",
        "title": "The Impact of Sex Education on Sexual Activity, Pregnancy, and Abortion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this study is to find a relation between sex education and\nabortion in the United States. Accordingly, multivariate logistic regression is\nemployed to study the relation between abortion and frequency of sex,\npre-marriage sex, and pregnancy by rape. The finding shows the odds of abortion\namong those who have had premarital sex, more frequent sex before marriage, and\nbeen the victim of rape is higher than those who have not experienced any of\nthese incidents. The output identified with one unit increase in pre-marriage\nsex the log-odds of abortion increases by 0.47. Similarly, it shows by one unit\nincrease in the frequency of sex, the log-odds of abortion increases by 0.39.\nAlso, for every additional pregnancy by rape, there is an expectation of a 3.17\nincrease in the log-odds of abortion. The findings of this study also suggests\nabortion is associated with sex education. Despite previous findings, this\nstudy shows the factors of age, having children, and social standing is not\nconsidered a burden to parents and thereby do not have a causal relation to\nabortion.\n"
    },
    {
        "paper_id": 1903.08367,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Nurtai Meimanjan",
        "title": "Computation of systemic risk measures: a mixed-integer programming\n  approach",
        "comments": "62 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk is concerned with the instability of a financial system whose\nmembers are interdependent in the sense that the failure of a few institutions\nmay trigger a chain of defaults throughout the system. Recently, several\nsystemic risk measures have been proposed in the literature that are used to\ndetermine capital requirements for the members subject to joint risk\nconsiderations. We address the problem of computing systemic risk measures for\nsystems with sophisticated clearing mechanisms. In particular, we consider an\nextension of the Rogers-Veraart network model where the operating cash flows\nare unrestricted in sign. We propose a mixed-integer programming problem that\ncan be used to compute clearing vectors in this model. Due to the binary\nvariables in this problem, the corresponding (set-valued) systemic risk measure\nfails to have convex values in general. We associate nonconvex vector\noptimization problems with the systemic risk measure and provide theoretical\nresults related to the weighted-sum and Pascoletti-Serafini scalarizations of\nthis problem. Finally, we test the proposed formulations on computational\nexamples and perform sensitivity analyses with respect to some model-specific\nand structural parameters.\n"
    },
    {
        "paper_id": 1903.08782,
        "authors": "Joshua Aurand, Yu-Jui Huang",
        "title": "Epstein-Zin Utility Maximization on a Random Horizon",
        "comments": null,
        "journal-ref": "Mathematical Finance, Vol. 33 (2023), Issue 4, pp. 1370-1411",
        "doi": "10.1111/mafi.12404",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper solves the consumption-investment problem under Epstein-Zin\npreferences on a random horizon. In an incomplete market, we take the random\nhorizon to be a stopping time adapted to the market filtration, generated by\nall observable, but not necessarily tradable, state processes. Contrary to\nprior studies, we do not impose any fixed upper bound for the random horizon,\nallowing for truly unbounded ones. Focusing on the empirically relevant case\nwhere the risk aversion and the elasticity of intertemporal substitution are\nboth larger than one, we characterize the optimal consumption and investment\nstrategies using backward stochastic differential equations with superlinear\ngrowth on unbounded random horizons. This characterization, compared with the\nclassical fixed-horizon result, involves an additional stochastic process that\nserves to capture the randomness of the horizon. As demonstrated in two\nconcrete examples, changing from a fixed horizon to a random one drastically\nalters the optimal strategies.\n"
    },
    {
        "paper_id": 1903.08957,
        "authors": "Hiroaki Hata, Shuenn-Jyi Sheu, Li-Hsien Sun",
        "title": "Expected exponential utility maximization of insurers with a general\n  diffusion factor model : The complete market case",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of optimal investment by an insurer.\nThe insurer invests in a market consisting of a bank account and $m$ risky\nassets. The mean returns and volatilities of the risky assets depend\nnonlinearly on economic factors that are formulated as the solutions of general\nstochastic differential equations. The wealth of the insurer is described by a\nCram\\'er--Lundberg process, and the insurer preferences are exponential.\nAdapting a dynamic programming approach, we derive Hamilton--Jacobi--Bellman\n(HJB) equation. And, we prove the unique solvability of HJB equation. In\naddition, the optimal strategy is also obtained using the coupled forward and\nbackward stochastic differential equations (FBSDEs). Finally, proving the\nverification theorem, we construct the optimal strategy.\n"
    },
    {
        "paper_id": 1903.0914,
        "authors": "Xin Guo, Charles-Albert Lehalle and Renyuan Xu",
        "title": "Transaction Cost Analytics for Corporate Bonds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The electronic platform has been increasingly popular for executing large\ncorporate bond orders by asset managers, who in turn have to assess the quality\nof their executions via Transaction Cost Analysis (TCA). One of the challenges\nin TCA is to build a realistic benchmark for the expected transaction cost and\nto characterize the price impact of each individual trade with given bond\ncharacteristics and market conditions.\n  Taking the viewpoint of retail investors, this paper presents an analytical\nmethodology for TCA of corporate bond trading. Our analysis is based on the\nTRACE Enhanced dataset; and starts with estimating the initiator of a bond\ntransaction, followed by estimating the bid-ask spread and the mid-price\ndynamics. With these estimations, the first part of our study is to identify\nkey features for corporate bonds and to compute the expected average trading\ncost. This part is on the time scale of weekly transactions, and is by applying\nand comparing several regularized regression models. The second part of our\nstudy is using the estimated mid-price dynamics to investigate the amplitude of\nits price impact and the decay pattern of individual bond transaction. This\npart is on the time scale of each transaction of liquid corporate bonds, and is\nby applying a transient impact model to estimate the price impact kernel using\na non-parametric method.\n  Our benchmark model allows for identifying abnormal transactions and for\nenhancing counter-party selections. A key discovery of our study is the price\nimpact asymmetry between customer-buy orders and consumer-sell orders.\n"
    },
    {
        "paper_id": 1903.09279,
        "authors": "Neave O'Clery and Samuel Heroy and Francois Hulot and Mariano\n  Beguerisse-D\\'iaz",
        "title": "Unravelling the forces underlying urban industrial agglomeration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As early as the 1920's Marshall suggested that firms co-locate in cities to\nreduce the costs of moving goods, people, and ideas. These 'forces of\nagglomeration' have given rise, for example, to the high tech clusters of San\nFrancisco and Boston, and the automobile cluster in Detroit. Yet, despite its\nimportance for city planners and industrial policy-makers, until recently there\nhas been little success in estimating the relative importance of each\nMarshallian channel to the location decisions of firms.\n  Here we explore a burgeoning literature that aims to exploit the co-location\npatterns of industries in cities in order to disentangle the relationship\nbetween industry co-agglomeration and customer/supplier, labour and idea\nsharing. Building on previous approaches that focus on across- and\nbetween-industry estimates, we propose a network-based method to estimate the\nrelative importance of each Marshallian channel at a meso scale. Specifically,\nwe use a community detection technique to construct a hierarchical\ndecomposition of the full set of industries into clusters based on\nco-agglomeration patterns, and show that these industry clusters exhibit\ndistinct patterns in terms of their relative reliance on individual Marshallian\nchannels.\n"
    },
    {
        "paper_id": 1903.09536,
        "authors": "Daniel Poh, Stephen Roberts, Martin Tegn\\'er",
        "title": "A Machine Learning approach to Risk Minimisation in Electricity Markets\n  with Coregionalized Sparse Gaussian Processes",
        "comments": "24 pages, 4 figures, journal submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The non-storability of electricity makes it unique among commodity assets,\nand it is an important driver of its price behaviour in secondary financial\nmarkets. The instantaneous and continuous matching of power supply with demand\nis a key factor explaining its volatility. During periods of high demand,\ncostlier generation capabilities are utilised since electricity cannot be\nstored and this has the impact of driving prices up very quickly. Furthermore,\nthe non-storability also complicates physical hedging. Owing to these, the\nproblem of joint price-quantity risk in electricity markets is a commonly\nstudied theme.\n  We propose using Gaussian Processes (GPs) to tackle this problem since GPs\nprovide a versatile and elegant non-parametric approach for regression and\ntime-series modelling. However, GPs scale poorly with the amount of training\ndata due to a cubic complexity. These considerations suggest that knowledge\ntransfer between price and load is vital for effective hedging, and that a\ncomputationally efficient method is required. To this end, we use the\ncoregionalized (or multi-task) sparse GPs which addresses the aforementioned\nissues.\n  To gauge the performance of our model, we use an average-load strategy as\ncomparator. The latter is a robust approach commonly used by industry. If the\nspot and load are uncorrelated and Gaussian, then hedging with the expected\nload will result in the minimum variance position.\n  Our main contributions are twofold. Firstly, in developing a coregionalized\nsparse GP-based approach for hedging. Secondly, in demonstrating that our\nmodel-based strategy outperforms the comparator, and can thus be employed for\neffective hedging in electricity markets.\n"
    },
    {
        "paper_id": 1903.09641,
        "authors": "Sergei Kulakov and Florian Ziel",
        "title": "The Impact of Renewable Energy Forecasts on Intraday Electricity Prices",
        "comments": null,
        "journal-ref": "Economics of Energy and Environmental Policy, 10(1) 2021",
        "doi": "10.5547/2160-5890.10.1.skul",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the impact of errors in wind and solar power forecasts\non intraday electricity prices. We develop a novel econometric model which is\nbased on day-ahead wholesale auction curves data and errors in wind and solar\npower forecasts. The model shifts day-ahead supply curves to calculate intraday\nprices. We apply our model to the German EPEX SPOT SE data. Our model\noutperforms both linear and non-linear benchmarks. Our study allows us to\nconclude that errors in renewable energy forecasts exert a non-linear impact on\nintraday prices. We demonstrate that additional wind and solar power capacities\ninduce non-linear changes in the intraday price volatility. Finally, we comment\non economical and policy implications of our findings.\n"
    },
    {
        "paper_id": 1903.09683,
        "authors": "Corry Bedwell, Ryan Guttridge",
        "title": "Modern Asset Theory: A Framework for Successful Active Management",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Active management is a term that has many meanings and we have found the\ndefining characteristics needed for success as an \"active manager\" elusive\nwithin the literature. In this paper we offer a set of criteria that defines an\nactive manager and his success. In order to facilitate this, we introduce\nseveral definitions, which lead to a logically coherent evaluation framework.\nWe expand on the definitions of these six key concepts: Introduce a specific\nconcept of Ruin, Assets, Risk, Discount rate, Margin of safety, and\nOptimization.\n  Through these definitions a strong defense of active management emerges.\nFurthermore, to the extent one chooses to limit the definitions we offer, our\nframework reduces to that of Modern Portfolio Theory. Overall, we have aimed to\nconstruct a robust expansion of a framework for active management, one that has\nbeen found wanting in the current literature.\n"
    },
    {
        "paper_id": 1903.09898,
        "authors": "Nicholas CL Beale, Richard M Gunton, Kutlwano L Bashe, Heather S\n  Battey, Robert S MacKay",
        "title": "Dynamics of Value-Tracking in Financial Markets",
        "comments": "revised to take into account reviewers' comments",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The efficiency of a modern economy depends on what we call the Value-Tracking\nHypothesis: that market prices of key assets broadly track some underlying\nvalue. This can be expected if a sufficient weight of market participants are\nvaluation-based traders, buying and selling an asset when its price is,\nrespectively, below and above their well-informed private valuations. Such\ntracking will never be perfect, and we propose a natural unit of tracking\nerror, the 'deciblack'. We then use a simple discrete-time model to show how\nlarge tracking errors can arise if enough market participants are not\nvaluation-based traders, regardless of how much information the valuation-based\ntraders have. We find a threshold above which value-tracking breaks down\nwithout any changes in the underlying value of the asset. Because financial\nmarkets are increasingly dominated by non-valuation-based traders, assessing\nhow much valuation-based investing is required for reasonable value tracking is\nof urgent practical interest.\n"
    },
    {
        "paper_id": 1903.10065,
        "authors": "Sona Kilianova, Daniel Sevcovic",
        "title": "Dynamic intertemporal utility optimization by means of Riccati\n  transformation of Hamilton-Jacobi Bellman equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate a dynamic stochastic portfolio optimization\nproblem involving both the expected terminal utility and intertemporal utility\nmaximization. We solve the problem by means of a solution to a fully nonlinear\nevolutionary Hamilton-Jacobi-Bellman (HJB) equation. We propose the so-called\nRiccati method for transformation of the fully nonlinear HJB equation into a\nquasi-linear parabolic equation with non-local terms involving the\nintertemporal utility function. As a numerical method we propose a\nsemi-implicit scheme in time based on a finite volume approximation in the\nspatial variable. By analyzing an explicit traveling wave solution we show that\nthe numerical method is of the second experimental order of convergence. As a\npractical application we compute optimal strategies for a portfolio investment\nproblem motivated by market financial data of German DAX 30 Index and show the\neffect of considering intertemporal utility on optimal portfolio selection.\n"
    },
    {
        "paper_id": 1903.10454,
        "authors": "Tahsin Deniz Akt\\\"urk, \\c{C}a\\u{g}{\\i}n Ararat",
        "title": "Portfolio optimization with two coherent risk measures",
        "comments": "29 pages",
        "journal-ref": "Journal of Global Optimization 78 (3), 597-626, (2020)",
        "doi": "10.1007/s10898-020-00922-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide analytical results for a static portfolio optimization problem\nwith two coherent risk measures. The use of two risk measures is motivated by\njoint decision-making for portfolio selection where the risk perception of the\nportfolio manager is of primary concern, hence, it appears in the objective\nfunction, and the risk perception of an external authority needs to be taken\ninto account as well, which appears in the form of a risk constraint. The\nproblem covers the risk minimization problem with an expected return constraint\nand the expected return maximization problem with a risk constraint, as special\ncases. For the general case of an arbitrary joint distribution for the asset\nreturns, under certain conditions, we characterize the optimal portfolio as the\noptimal Lagrange multiplier associated to an equality-constrained dual problem.\nThen, we consider the special case of Gaussian returns for which it is possible\nto identify all cases where an optimal solution exists and to give an explicit\nformula for the optimal portfolio whenever it exists.\n"
    },
    {
        "paper_id": 1903.10795,
        "authors": "Antoine Jacquier, Emma R. Malone, Mugad Oumgari",
        "title": "Stacked Monte Carlo for option pricing",
        "comments": "12 pages, 13 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a stacking version of the Monte Carlo algorithm in the context\nof option pricing. Introduced recently for aeronautic computations, this simple\ntechnique, in the spirit of current machine learning ideas, learns control\nvariates by approximating Monte Carlo draws with some specified function. We\ndescribe the method from first principles and suggest appropriate fits, and\nshow its efficiency to evaluate European and Asian Call options in constant and\nstochastic volatility models.\n"
    },
    {
        "paper_id": 1903.10855,
        "authors": "Adrien Ehrhardt, Christophe Biernacki, Vincent Vandewalle, Philippe\n  Heinrich, S\\'ebastien Beben",
        "title": "R\\'eint\\'egration des refus\\'es en Credit Scoring",
        "comments": "In French, 6 pages, 1 table, 1 figure; long abstract for the 49th\n  'Journ\\'ees de Statistiques' conference, May 2017, Avignon, France",
        "journal-ref": "49e Journ\\'ees de Statistique, May 2017, Avignon, France",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The granting process of all credit institutions rejects applicants who seem\nrisky regarding the repayment of their debt. A credit score is calculated and\nassociated with a cut-off value beneath which an applicant is rejected.\nDeveloping a new score implies having a learning dataset in which the response\nvariable good/bad borrower is known, so that rejects are de facto excluded from\nthe learning process. We first introduce the context and some useful notations.\nThen we formalize if this particular sampling has consequences on the score's\nrelevance. Finally, we elaborate on methods that use not-financed clients'\ncharacteristics and conclude that none of these methods are satisfactory in\npractice using data from Cr\\'edit Agricole Consumer Finance.\n  -----\n  Un syst\\`eme d'octroi de cr\\'edit peut refuser des demandes de pr\\^et\njug\\'ees trop risqu\\'ees. Au sein de ce syst\\`eme, le score de cr\\'edit fournit\nune valeur mesurant un risque de d\\'efaut, valeur qui est compar\\'ee \\`a un\nseuil d'acceptabilit\\'e. Ce score est construit exclusivement sur des donn\\'ees\nde clients financ\\'es, contenant en particulier l'information `bon ou mauvais\npayeur', alors qu'il est par la suite appliqu\\'e \\`a l'ensemble des demandes.\nUn tel score est-il statistiquement pertinent ? Dans cette note, nous\npr\\'ecisons et formalisons cette question et \\'etudions l'effet de l'absence\ndes non-financ\\'es sur les scores \\'elabor\\'es. Nous pr\\'esentons ensuite des\nm\\'ethodes pour r\\'eint\\'egrer les non-financ\\'es et concluons sur leur\ninefficacit\\'e en pratique, \\`a partir de donn\\'ees issues de Cr\\'edit Agricole\nConsumer Finance.\n"
    },
    {
        "paper_id": 1903.10965,
        "authors": "Liyang Han, Thomas Morstyn, Constance Crozier, Malcolm McCulloch",
        "title": "Improving the Scalability of a Prosumer Cooperative Game with K-Means\n  Clustering",
        "comments": "6 pages, 4 figures, 2 tables. Accepted to the 13th IEEE PES PowerTech\n  Conference, 23-27 June 2019, Milano, Italy",
        "journal-ref": "2019 IEEE Milan PowerTech, Milan, Italy, 2019, pp. 1-6",
        "doi": "10.1109/PTC.2019.8810558",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Among the various market structures under peer-to-peer energy sharing, one\nmodel based on cooperative game theory provides clear incentives for prosumers\nto collaboratively schedule their energy resources. The computational\ncomplexity of this model, however, increases exponentially with the number of\nparticipants. To address this issue, this paper proposes the application of\nK-means clustering to the energy profiles following the grand coalition\noptimization. The cooperative model is run with the \"clustered players\" to\ncompute their payoff allocations, which are then further distributed among the\nprosumers within each cluster. Case studies show that the proposed method can\nsignificantly improve the scalability of the cooperative scheme while\nmaintaining a high level of financial incentives for the prosumers.\n"
    },
    {
        "paper_id": 1903.11047,
        "authors": "Liyang Han, Thomas Morstyn, Malcolm McCulloch",
        "title": "Estimation of the Shapley Value of a Peer-to-Peer Energy Sharing Game\n  using Coalitional Stratified Random Sampling",
        "comments": "6 pages, 5 figures, 1 table. Accepted to IFAC Workshop on Control of\n  Smart Grid and Renewable Energy Systems (CSGRES), June 10-12, 2019, Jeju,\n  Korea",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Various peer-to-peer energy markets have emerged in recent years in an\nattempt to manage distributed energy resources in a more efficient way. One of\nthe main challenges these models face is how to create and allocate incentives\nto participants. Cooperative game theory offers a methodology to financially\nreward prosumers based on their contributions made to the local energy\ncoalition using the Shapley value, but its high computational complexity limits\nthe size of the game. This paper explores a stratified sampling method proposed\nin existing literature for Shapley value estimation, and modifies the method\nfor a peer-to-peer cooperative game to improve its scalability. Finally,\nselected case studies verify the effectiveness of the proposed coalitional\nstratified random sampling method and demonstrate results from large games.\n"
    },
    {
        "paper_id": 1903.11183,
        "authors": "Curtis Atkisson, Piotr J. G\\'orski, Matthew O. Jackson, Janusz A.\n  Ho{\\l}yst, Raissa M. D'Souza",
        "title": "Why understanding multiplex social network structuring processes will\n  help us better understand the evolution of human behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social scientists have long appreciated that relationships between\nindividuals cannot be described from observing a single domain, and that the\nstructure across domains of interaction can have important effects on outcomes\nof interest (e.g., cooperation).1 One debate explicitly about this surrounds\nfood sharing. Some argue that failing to find reciprocal food sharing means\nthat some process other than reciprocity must be occurring, whereas others\nargue for models that allow reciprocity to span domains in the form of trade.2\nMultilayer networks, high-dimensional networks that allow us to consider\nmultiple sets of relationships at the same time, are ubiquitous and have\nconsequences, so processes giving rise to them are important social phenomena.\nThe analysis of multi-dimensional social networks has recently garnered the\nattention of the network science community.3 Recent models of these processes\nshow how ignoring layer interdependencies can lead one to miss why a layer\nformed the way it did, and/or draw erroneous conclusions.6 Understanding the\nstructuring processes that underlie multiplex networks will help understand\nincreasingly rich datasets, giving more accurate and complete pictures of\nsocial interactions.\n"
    },
    {
        "paper_id": 1903.11198,
        "authors": "Caio Waisman and Navdeep S. Sahni and Harikesh S. Nair and Xiliang Lin",
        "title": "Parallel Experimentation and Competitive Interference on Online\n  Advertising Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the measurement of advertising effects on online platforms\nwhen parallel experimentation occurs, that is, when multiple advertisers\nexperiment concurrently. It provides a framework that makes precise how\nparallel experimentation affects the experiment's value: while ignoring\nparallel experimentation yields an estimate of the average effect of\nadvertising in-place, which has limited value in decision-making in an\nenvironment with variable advertising competition, accounting for parallel\nexperimentation captures the actual uncertainty advertisers face due to\ncompetitive actions. It then implements an experimental design that enables the\nestimation of these effects on JD.com, a large e-commerce platform that is also\na publisher of digital ads. Using traditional and kernel-based estimators, it\nshows that not accounting for competitive actions can result in the advertiser\ninaccurately estimating the advertising lift by a factor of two or higher,\nwhich can be consequential for decision-making.\n"
    },
    {
        "paper_id": 1903.11275,
        "authors": "Ludovic Gouden\\`ege, Andrea Molent, Antonino Zanette",
        "title": "Variance Reduction Applied to Machine Learning for Pricing\n  Bermudan/American Options in High Dimension",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose an efficient method to compute the price of\nmulti-asset American options, based on Machine Learning, Monte Carlo\nsimulations and variance reduction technique. Specifically, the options we\nconsider are written on a basket of assets, each of them following a\nBlack-Scholes dynamics. In the wake of Ludkovski's approach (2018), we\nimplement here a backward dynamic programming algorithm which considers a\nfinite number of uniformly distributed exercise dates. On these dates, the\noption value is computed as the maximum between the exercise value and the\ncontinuation value, which is obtained by means of Gaussian process regression\ntechnique and Monte Carlo simulations. Such a method performs well for low\ndimension baskets but it is not accurate for very high dimension baskets. In\norder to improve the dimension range, we employ the European option price as a\ncontrol variate, which allows us to treat very large baskets and moreover to\nreduce the variance of price estimators. Numerical tests show that the proposed\nalgorithm is fast and reliable, and it can handle also American options on very\nlarge baskets of assets, overcoming the problem of the curse of dimensionality.\n"
    },
    {
        "paper_id": 1903.11383,
        "authors": "Sergei Kulakov and Florian Ziel",
        "title": "Determining Fundamental Supply and Demand Curves in a Wholesale\n  Electricity Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a novel method of wholesale electricity market\nmodeling. Our optimization-based model decomposes wholesale supply and demand\ncurves into buy and sell orders of individual market participants. In doing so,\nthe model detects and removes arbitrage orders. As a result, we construct an\ninnovative fundamental model of a wholesale electricity market. First, our\nfundamental demand curve has a unique composition. The demand curve lies in\nbetween the wholesale demand curve and a perfectly inelastic demand curve.\nSecond, our fundamental supply and demand curves contain only actual (i.e.\nnon-arbitrage) transactions with physical assets on buy and sell sides. Third,\nthese transactions are designated to one of the three groups of wholesale\nelectricity market participants: retailers, suppliers, or utility companies. To\nevaluate the performance of our model, we use the German wholesale market data.\nOur fundamental model yields a more precise approximation of the actual load\nvalues than a model with perfectly inelastic demand. Moreover, we conduct a\nstudy of wholesale demand elasticities. The obtained conclusions regarding\nwholesale demand elasticity are consistent with the existing academic\nliterature.\n"
    },
    {
        "paper_id": 1903.11469,
        "authors": "Jacopo Arpetti, Antonio Iovanella",
        "title": "Towards more effective consumer steering via network analysis",
        "comments": null,
        "journal-ref": "European Journal of Law and Economics (2019)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Increased data gathering capacity, together with the spread of data analytics\ntechniques, has prompted an unprecedented concentration of information related\nto the individuals' preferences in the hands of a few gatekeepers. In the\npresent paper, we show how platforms' performances still appear astonishing in\nrelation to some unexplored data and networks properties, capable to enhance\nthe platforms' capacity to implement steering practices by means of an\nincreased ability to estimate individuals' preferences. To this end, we rely on\nnetwork science whose analytical tools allow data representations capable of\nhighlighting relationships between subjects and/or items, extracting a great\namount of information. We therefore propose a measure called Network\nInformation Patrimony, considering the amount of information available within\nthe system and we look into how platforms could exploit data stemming from\nconnected profiles within a network, with a view to obtaining competitive\nadvantages. Our measure takes into account the quality of the connections among\nnodes as the one of a hypothetical user in relation to its neighbourhood,\ndetecting how users with a good neighbourhood -- hence of a superior\nconnections set -- obtain better information. We tested our measures on\nAmazons' instances, obtaining evidence which confirm the relevance of\ninformation extracted from nodes' neighbourhood in order to steer targeted\nusers.\n"
    },
    {
        "paper_id": 1903.1153,
        "authors": "Vladislav Gennadievich Malyshkin",
        "title": "Market Dynamics: On Directional Information Derived From (Time,\n  Execution Price, Shares Traded) Transaction Sequences",
        "comments": "Non--local price change as $p^{[IH]}$ change per tick subject to\n  positive $\\lambda_I^{[IH]}$ per tick change is added as an important feature.\n  Conditional optimization cleanup",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A new approach to obtaining market--directional information, based on a\nnon-stationary solution to the dynamic equation \"future price tends to the\nvalue that maximizes the number of shares traded per unit time\" [1] is\npresented. In our previous work[2], we established that it is the share\nexecution flow ($I=dV/dt$) and not the share trading volume ($V$) that is the\ndriving force of the market, and that asset prices are much more sensitive to\nthe execution flow $I$ (the dynamic impact) than to the traded volume $V$ (the\nregular impact). In this paper, an important advancement is achieved: we define\nthe \"scalp-price\" ${\\cal P}$ as the sum of only those price moves that are\nrelevant to market dynamics; the criterion of relevance is a high $I$. Thus,\nonly \"follow the market\" (and not \"little bounce\") events are included in\n${\\cal P}$. Changes in the scalp-price defined this way indicate a market trend\nchange - not a bear market rally or a bull market sell-off; the approach can be\nfurther extended to non-local price change. The software calculating the\nscalp--price given market observations triples (time, execution price, shares\ntraded) is available from the authors.\n"
    },
    {
        "paper_id": 1903.11642,
        "authors": "Ansari Saleh Ahmar",
        "title": "Sutte Indicator: an approach to predict the direction of stock market\n  movements",
        "comments": null,
        "journal-ref": "Songklanakarin J. Sci. Technol. 40 (5), 1228-1231, Sep. - Oct.\n  2018",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The purpose of this research is to apply technical analysis of Sutte\nIndicator in stock trading which will assist in the investment decision making\nprocess i.e. buying or selling shares. This research takes data of \"A\" on the\nIndonesia Stock Exchange(IDX or BEI) 29 November 2006 until 20 September 2016\nperiod. To see the performance of Sutte Indicator, other technical analysis are\nused as a comparison, Simple Moving Average (SMA) and Moving Average\nConvergence/Divergence (MACD). To see a comparison of the level of reliability\nprediction, the stock data were compared using the mean absolute deviation\n(MAD), mean of square error (MSE), and mean absolute percentage error (MAPE).\nThe result of this research is that Sutte Indicator can be used as a reference\nin predicting stock movements, and if it is compared to other indicator methods\n(SMA and MACD) via MAD, MSE, and MAPE, the Sutte Indicator has a better level\nof reliability.\n"
    },
    {
        "paper_id": 1903.11686,
        "authors": "Bernardo D'Auria and Eduardo Garc\\'ia-Portugu\\'es and Abel Guada",
        "title": "Discounted optimal stopping of a Brownian bridge, with application to\n  American options under pinning",
        "comments": "29 pages, 9 figures. Supplementary material: 5 R scripts, 4 RData\n  files",
        "journal-ref": "Mathematics, 8:1159, 2020",
        "doi": "10.3390/math8071159",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Mathematically, the execution of an American-style financial derivative is\ncommonly reduced to solving an optimal stopping problem. Breaking the general\nassumption that the knowledge of the holder is restricted to the price history\nof the underlying asset, we allow for the disclosure of future information\nabout the terminal price of the asset by modeling it as a Brownian bridge. This\nmodel may be used under special market conditions, in particular we focus on\nwhat in the literature is known as the \"pinning effect\", that is, when the\nprice of the asset approaches the strike price of a highly-traded option close\nto its expiration date. Our main mathematical contribution is in characterizing\nthe solution to the optimal stopping problem when the gain function includes\nthe discount factor. We show how to numerically compute the solution and we\nanalyze the effect of the volatility estimation on the strategy by computing\nthe confidence curves around the optimal stopping boundary. Finally, we compare\nour method with the optimal exercise time based on a geometric Brownian motion\nby using real data exhibiting pinning.\n"
    },
    {
        "paper_id": 1903.11804,
        "authors": "Kristoffer Glover and Hardy Hulley",
        "title": "Short Selling with Margin Risk and Recall Risk",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Short sales are regarded as negative purchases in textbook asset pricing\ntheory. In reality, however, the symmetry between purchases and short sales is\nbroken by a variety of costs and risks peculiar to the latter. We formulate an\noptimal stopping model in which the decision to cover a short position is\naffected by two short sale-specific frictions---margin risk and recall risk.\nMargin risk refers to the fact that short sales are collateralised\ntransactions, which means that short sellers may be forced to close out their\npositions involuntarily if they cannot fund margin calls. Recall risk refers to\na peculiarity of the stock lending market, which permits lenders to recall\nborrowed stock at any time, once again triggering involuntary close-outs. We\nexamine the effect of these frictions on the optimal close-out strategy and\nquantify the loss of value resulting from each. Our results show that realistic\nshort selling constraints have a dramatic impact on the optimal behaviour of a\nshort seller, and are responsible for a substantial loss of value relative to\nthe first-best situation without them. This has implications for many familiar\nno-arbitrage identities, which are predicated on the assumption of unfettered\nshort selling.\n"
    },
    {
        "paper_id": 1903.12258,
        "authors": "Rosdyana Mangir Irawan Kusuma, Trang-Thi Ho, Wei-Chun Kao, Yu-Yen Ou\n  and Kai-Lung Hua",
        "title": "Using Deep Learning Neural Networks and Candlestick Chart Representation\n  to Predict Stock Market",
        "comments": "conference,13 pages,3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market prediction is still a challenging problem because there are many\nfactors effect to the stock market price such as company news and performance,\nindustry performance, investor sentiment, social media sentiment and economic\nfactors. This work explores the predictability in the stock market using Deep\nConvolutional Network and candlestick charts. The outcome is utilized to design\na decision support framework that can be used by traders to provide suggested\nindications of future stock price direction. We perform this work using various\ntypes of neural networks like convolutional neural network, residual network\nand visual geometry group network. From stock market historical data, we\nconverted it to candlestick charts. Finally, these candlestick charts will be\nfeed as input for training a Convolutional Neural Network model. This\nConvolutional Neural Network model will help us to analyze the patterns inside\nthe candlestick chart and predict the future movements of stock market. The\neffectiveness of our method is evaluated in stock market prediction with a\npromising results 92.2% and 92.1% accuracy for Taiwan and Indonesian stock\nmarket dataset respectively. The constructed model have been implemented as a\nweb-based system freely available at http://140.138.155.216/deepcandle/ for\npredicting stock market using candlestick chart and deep learning neural\nnetworks.\n"
    },
    {
        "paper_id": 1903.12267,
        "authors": "Baogui Xin, Wei Peng, Yekyung Kwon, Yanqin Liu",
        "title": "Modeling, discretization, and hyperchaos detection of conformable\n  derivative approach to a financial system with market confidence and ethics\n  risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1186/s13662-019-2074-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new chaotic financial system is proposed by considering ethics involvement\nin a four-dimensional financial system with market confidence. A\nfive-dimensional conformable derivative financial system is presented by\nintroducing conformable fractional calculus to the integer-order system. A\ndiscretization scheme is proposed to calculate numerical solutions of\nconformable derivative systems. The scheme is illustrated by testing hyperchaos\nfor the system.\n"
    },
    {
        "paper_id": 1903.12426,
        "authors": "Matteo Brachetta and Hanspeter Schmidli",
        "title": "Optimal Reinsurance and Investment in a Diffusion Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a diffusion approximation to an insurance risk model where an\nexternal driver models a stochastic environment. The insurer can buy\nreinsurance. Moreover, investment in a financial market is possible. The\nfinancial market is also driven by the environmental process. Our goal is to\nmaximise terminal expected utility. In particular, we consider the case of\nSAHARA utility functions. In the case of proportional and excess-of-loss\nreinsurance, we obtain explicit results.\n"
    },
    {
        "paper_id": 1903.12458,
        "authors": "Vasilios Mavroudis",
        "title": "Market Manipulation as a Security Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Order matching systems form the backbone of modern equity exchanges, used by\nmillions of investors daily. Thus, their operation is strictly controlled\nthrough numerous regulatory directives to ensure that markets are fair and\ntransparent. Despite these efforts, market manipulation remains an open\nproblem.\n  In this work, we focus on a class of market manipulation techniques that\nexploit technical details and glitches in the operation of the exchanges (i.e.,\nmechanical arbitrage). Such techniques are used by predatory traders with deep\nknowledge of the exchange's structure to gain an advantage over the other\nmarket participants. We argue that technical solutions to the problem of\nmechanical arbitrage have the potential to significantly thwart these\npractices. Our work provides the first overview of the threat landscape, models\nfair markets and their security assumptions, and discusses various mitigation\nmeasures.\n"
    },
    {
        "paper_id": 1904.00029,
        "authors": "Olena Kostylenko, Helena Sofia Rodrigues, Delfim F. M. Torres",
        "title": "Parametric identification of the dynamics of inter-sectoral balance:\n  modelling and forecasting",
        "comments": "This is a preprint of a paper accepted for publication 29-March-2019\n  as a book chapter in 'Advances in Intelligent Systems and Computing'\n  [https://www.springer.com/series/11156], Springer",
        "journal-ref": "Studies in Systems, Decision and Control 243 (2020), 133--143",
        "doi": "10.1007/978-3-030-26149-8_11",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is devoted to modelling and identification of the dynamics of the\ninter-sectoral balance of a macroeconomic system. An approach to the problem of\nspecification and identification of a weakly formalized dynamical system is\ndeveloped. A matching procedure for parameters of a linear stationary Cauchy\nproblem with a decomposition of its upshot trend and a periodic component, is\nproposed. Moreover, an approach for detection of significant harmonic waves,\nwhich are inherent to real macroeconomic dynamical systems, is developed.\n"
    },
    {
        "paper_id": 1904.00075,
        "authors": "Tiziano De Angelis and Alessandro Milazzo",
        "title": "Optimal stopping for the exponential of a Brownian bridge",
        "comments": "22 pages, 6 figures",
        "journal-ref": "J. Appl. Probab. 57 (2020) 361-384",
        "doi": "10.1017/jpr.2019.98",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the problem of stopping a Brownian bridge $X$ in order\nto maximise the expected value of an exponential gain function. In particular,\nwe solve the stopping problem $$\\sup_{0\\le \\tau\\le\n1}\\mathsf{E}[\\mathrm{e}^{X_\\tau}]$$ which was posed by Ernst and Shepp in their\npaper [Commun. Stoch. Anal., 9 (3), 2015, pp. 419--423] and was motivated by\nbond selling with non-negative prices.\n  Due to the non-linear structure of the exponential gain, we cannot rely on\nmethods used in the literature to find closed-form solutions to other problems\ninvolving the Brownian bridge. Instead, we develop techniques that use pathwise\nproperties of the Brownian bridge and martingale methods of optimal stopping\ntheory in order to find the optimal stopping rule and to show regularity of the\nvalue function.\n"
    },
    {
        "paper_id": 1904.00151,
        "authors": "Yu Feng",
        "title": "A Thermodynamic Picture of Financial Market and Model Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By treating the financial market as a thermodynamic system, we establish a\none-to-one correspondence between thermodynamic variables and economic\nquantities. Measured by the expected loss under the worst-case scenario,\nfinancial risk caused by model uncertainty is regarded as a result of the\ninteraction between financial market and external information sources. This\nforms a thermodynamic picture in which a closed system interacts with an\nexternal reservoir, reaching its equilibrium at the worst-case scenario. The\nseverity of the worst-case scenario depends on the rate of heat dissipation,\ncaused by information sources reducing the entropy of the system. This\nthermodynamic picture leads to simple and natural derivation of the\ncharacterization rules of the worst-case risk, and gives its Lagrangian and\nHamiltonian forms. With its help financial practitioners may evaluate risks\nutilizing both equilibrium and non-equilibrium thermodynamics.\n"
    },
    {
        "paper_id": 1904.00267,
        "authors": "Carey Caginalp, Gunduz Caginalp",
        "title": "Price equations with symmetric supply/demand; implications for fat tails",
        "comments": null,
        "journal-ref": "Economics Letters 176 (2019): 79-82",
        "doi": "10.1016/j.econlet.2018.12.037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Implementing a set of microeconomic criteria, we develop price dynamics\nequations using a function of demand/supply with key symmetry properties. The\nfunction of demand/supply can be linear or nonlinear. The type of function\ndetermines the nature of the tail of the distribution based on the randomness\nin the supply and demand. For example, if supply and demand are normally\ndistributed, and the function is assumed to be linear, then the density of\nrelative price change has behavior $x^{-2}$ for large $x$ (i.e., large\ndeviations). The exponent approaches $-1$ if the function of supply and demand\ninvolves a large exponent. The falloff is exponential, i.e., $e^{-x}$, if the\nfunction of supply and demand is logarithmic.\n"
    },
    {
        "paper_id": 1904.005,
        "authors": "H. Dharma Kwon",
        "title": "Game of Variable Contributions to the Common Good under Uncertainty",
        "comments": "Forthcoming in Operations Research",
        "journal-ref": "Operations Research (2022) 70(3):1359-1370",
        "doi": "10.1287/opre.2019.1879",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic game of contribution to the common good in which the\nplayers have continuous control over the degree of contribution, and we examine\nthe gradualism arising from the free rider effect. This game belongs to the\nclass of variable concession games which generalize wars of attrition.\nPreviously known examples of variable concession games in the literature yield\nequilibria characterized by singular control strategies without any delay of\nconcession. However, these no-delay equilibria are in contrast to mixed\nstrategy equilibria of canonical wars of attrition in which each player delays\nconcession by a randomized time. We find that a variable contribution game with\na single state variable, which extends the Nerlove-Arrow model, possesses an\nequilibrium characterized by regular control strategies that result in a\ngradual concession. This equilibrium naturally generalizes the mixed strategy\nequilibria from the canonical wars of attrition. Stochasticity of the problem\naccentuates the qualitative difference between a singular control solution and\na regular control equilibrium solution. We also find that asymmetry between the\nplayers can mitigate the inefficiency caused by the gradualism.\n"
    },
    {
        "paper_id": 1904.00745,
        "authors": "Luyang Chen, Markus Pelger and Jason Zhu",
        "title": "Deep Learning in Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use deep neural networks to estimate an asset pricing model for individual\nstock returns that takes advantage of the vast amount of conditioning\ninformation, while keeping a fully flexible form and accounting for\ntime-variation. The key innovations are to use the fundamental no-arbitrage\ncondition as criterion function, to construct the most informative test assets\nwith an adversarial approach and to extract the states of the economy from many\nmacroeconomic time series. Our asset pricing model outperforms out-of-sample\nall benchmark approaches in terms of Sharpe ratio, explained variation and\npricing errors and identifies the key factors that drive asset prices.\n"
    },
    {
        "paper_id": 1904.00749,
        "authors": "Novy Ann M. Etac and Roel F. Ceballos",
        "title": "Forecasting the Volatilities of Philippine Stock Exchange Composite\n  Index Using the Generalized Autoregressive Conditional Heteroskedasticity\n  Modeling",
        "comments": null,
        "journal-ref": "International Journal of Statistics and Economics, 19(3), 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study was conducted to find an appropriate statistical model to forecast\nthe volatilities of PSEi using the model Generalized Autoregressive Conditional\nHeteroskedasticity (GARCH). Using the R software, the log returns of PSEi is\nmodeled using various ARIMA models and with the presence of heteroskedasticity,\nthe log returns was modeled using GARCH. Based on the analysis, GARCH models\nare the most appropriate to use for the log returns of PSEi. Among the selected\nGARCH models, GARCH (1,2) has the lowest AIC value and also has the highest LL\nvalue implying that GARCH (1,2) is the best model for the log returns of PSEi.\n"
    },
    {
        "paper_id": 1904.0089,
        "authors": "Stjepan Begu\\v{s}i\\'c, Zvonko Kostanj\\v{c}ar",
        "title": "Momentum and liquidity in cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to explore the relationship between momentum\neffects and liquidity in cryptocurrency markets. Portfolios based on\nmomentum-liquidity bivariate sorts are formed and rebalanced on a varying\nnumber of cryptocurrencies through time. We find a strong momentum effect in\nthe most liquid cryptocurrencies, which supports the theories of investor\nherding behavior. Moreover, we propose two profitable long-only strategies: the\nilliquid losers and liquid winners, which exhibit improved risk adjusted\nperformance over the market capitalization weighted portfolio.\n"
    },
    {
        "paper_id": 1904.01412,
        "authors": "Vladimir Markov, Olga Vilenskaia and Vlad Rashkovich",
        "title": "Quintet Volume Projection",
        "comments": null,
        "journal-ref": "Automated Trader Magazine, Issue 44, Q1, 2018",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a set of models relevant for predicting various aspects of\nintra-day trading volume for equities and showcase them as an ensemble that\nprojects volume in unison. We introduce econometric methods for predicting\ntotal and remaining daily volume, intra-day volume profile (u-curve), close\nauction volume and special day seasonalities and emphasize a need for a unified\napproach where all sub-models work consistently with one another. Historical\nand current inputs are combined using Bayesian methods, which have the\nadvantage of providing adaptive and parameterless estimations of volume for a\nbroad range of equities while automatically taking into account uncertainty of\nthe model input components. The shortcomings of traditional statistical error\nmetrics for calibrating volume prediction are also discussed and we introduce\nAsymmetrical Logarithmic Error (ALE) to overweight an overestimation risk.\n"
    },
    {
        "paper_id": 1904.01566,
        "authors": "Vladimir Markov",
        "title": "Bayesian Trading Cost Analysis and Ranking of Broker Algorithms",
        "comments": "Submitted to the Econometrics Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a formulation of the transaction cost analysis (TCA) in the\nBayesian framework for the primary purpose of comparing broker algorithms using\nstandardized benchmarks. Our formulation allows effective calculation of the\nexpected value of trading benchmarks with only a finite sample of data relevant\nto practical applications. We discuss the nature of distribution of\nimplementation shortfall, volume-weighted average price, participation-weighted\nprice and short-term reversion benchmarks. Our model takes into account fat\ntails, skewness of the distributions and heteroscedasticity of benchmarks. The\nproposed framework allows the use of hierarchical models to transfer\napproximate knowledge from a large aggregated sample of observations to a\nsmaller sample of a particular algorithm.\n"
    },
    {
        "paper_id": 1904.01745,
        "authors": "Xue Dong He and Moris S. Strub and Thaleia Zariphopoulou",
        "title": "Forward Rank-Dependent Performance Criteria: Time-Consistent Investment\n  Under Probability Distortion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the concept of forward rank-dependent performance processes,\nextending the original notion to forward criteria that incorporate probability\ndistortions. A fundamental challenge is how to reconcile the time-consistent\nnature of forward performance criteria with the time-inconsistency stemming\nfrom probability distortions. For this, we first propose two distinct\ndefinitions, one based on the preservation of performance value and the other\non the time-consistency of policies and, in turn, establish their equivalence.\nWe then fully characterize the viable class of probability distortion\nprocesses, providing a bifurcation-type result. Specifically, it is either the\ncase that the probability distortions are degenerate in the sense that the\ninvestor would never invest in the risky assets, or the marginal probability\ndistortion equals to a normalized power of the quantile function of the pricing\nkernel. We also characterize the optimal wealth process, whose structure\nmotivates the introduction of a new, distorted measure and a related market. We\nthen build a striking correspondence between the forward rank-dependent\ncriteria in the original market and forward criteria without probability\ndistortions in the auxiliary market. This connection also provides a direct\nconstruction method for forward rank-dependent criteria. A byproduct of our\nwork are some new results on the so-called dynamic utilities and on\ntime-inconsistent problems in the classical (backward) setting.\n"
    },
    {
        "paper_id": 1904.01889,
        "authors": "Damiano Brigo",
        "title": "Probability-free models in option pricing: statistically\n  indistinguishable dynamics and historical vs implied volatility",
        "comments": "Paper presented at the conference \"Options: 45 Years after the\n  publication of the Black-Scholes-Merton Model\", Jerusalem, 4-5 December 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate whether it is possible to formulate option pricing and hedging\nmodels without using probability. We present a model that is consistent with\ntwo notions of volatility: a historical volatility consistent with statistical\nanalysis, and an implied volatility consistent with options priced with the\nmodel. The latter will be also the quadratic variation of the model, a pathwise\nproperty. This first result, originally presented in Brigo and Mercurio (1998,\n2000), is then connected with the recent work of Armstrong et al (2018, 2021),\nwhere using rough paths theory it is shown that implied volatility is\nassociated with a purely pathwise lift of the stock dynamics involving no\nprobability and no semimartingale theory in particular, leading to option\nmodels without probability. Finally, an intermediate result by Bender et al.\n(2008) is recalled. Using semimartingale theory, Bender et al. showed that one\ncould obtain option prices based only on the semimartingale quadratic variation\nof the model, a pathwise property, and highlighted the difference between\nhistorical and implied volatility. All three works confirm the idea that while\nhistorical volatility is a statistical quantity, implied volatility is a\npathwise one. This leads to a 20 years mini-anniversary of pathwise pricing\nthrough 1998, 2008 and 2018, which is rather fitting for a talk presented at\nthe conference for the 45 years of the Black, Scholes and Merton option pricing\nparadigm.\n"
    },
    {
        "paper_id": 1904.02058,
        "authors": "Sung J. Choi, M. Eric Johnson",
        "title": "Do Hospital Data Breaches Reduce Patient Care Quality?",
        "comments": "32 pages, 6 figures, 4 tables, presented at the Workshop on the\n  Economics of Information Security 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Objective: To estimate the relationship between a hospital data breach and\nhospital quality outcome\n  Materials and Methods: Hospital data breaches reported to the U.S. Department\nof Health and Human Services breach portal and the Privacy Rights Clearinghouse\ndatabase were merged with the Medicare Hospital Compare data to assemble a\npanel of non-federal acutecare inpatient hospitals for years 2011 to 2015. The\nstudy panel included 2,619 hospitals. Changes in 30-day AMI mortality rate\nfollowing a hospital data breach were estimated using a multivariate regression\nmodel based on a difference-in-differences approach.\n  Results: A data breach was associated with a 0.338[95% CI, 0.101-0.576]\npercentage point increase in the 30-day AMI mortality rate in the year\nfollowing the breach and a 0.446[95% CI, 0.164-0.729] percentage point increase\ntwo years after the breach. For comparison, the median 30-day AMI mortality\nrate has been decreasing about 0.4 percentage points annually since 2011 due to\nprogress in care. The magnitude of the breach impact on hospitals' AMI\nmortality rates was comparable to a year's worth historical progress in\nreducing AMI mortality rates.\n  Conclusion: Hospital data breaches significantly increased the 30-day\nmortality rate for AMI. Data breaches may disrupt the processes of care that\nrely on health information technology. Financial costs to repair a breach may\nalso divert resources away from patient care. Thus breached hospitals should\ncarefully focus investments in security procedures, processes, and health\ninformation technology that jointly lead to better data security and improved\npatient outcomes.\n"
    },
    {
        "paper_id": 1904.02412,
        "authors": "Hao Liao, Xiao-Min Huang, Xing-Tong Wu, Ming-Kai Liu, Alexandre\n  Vidmer, Mingyang Zhou, Yi-Cheng Zhang",
        "title": "Enhancing countries' fitness with recommender systems on the\n  international trade network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction is one of the major challenges in complex systems. The prediction\nmethods have shown to be effective predictors of the evolution of networks.\nThese methods can help policy makers to solve practical problems successfully\nand make better strategy for the future. In this work, we focus on exporting\ncountries' data of the international trading network. A recommendation system\nis then used to identify the products corresponding to the production capacity\nof each individual country, but are somehow overlook by the country. Then, we\nsimulate the evolution of the country's fitness if it would have followed the\nrecommendations. The result of this work is the combination combine these two\nmethods to provide insights to countries on how to enhance the diversification\nof their exported products in a scientific way and improve national\ncompetitiveness significantly, especially for developing countries.\n"
    },
    {
        "paper_id": 1904.02567,
        "authors": "Cheoljun Eom, Taisei Kaizoji, Enrico Scalas",
        "title": "Fat Tails in Financial Return Distributions Revisited: Evidence from the\n  Korean Stock Market",
        "comments": "24 pages, 4 Tables, 3 Figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.121055",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study empirically re-examines fat tails in stock return distributions by\napplying statistical methods to an extensive dataset taken from the Korean\nstock market. The tails of the return distributions are shown to be much fatter\nin recent periods than in past periods and much fatter for small-capitalization\nstocks than for large-capitalization stocks. After controlling for the 1997\nKorean foreign currency crisis and using the GARCH filter models to control for\nvolatility clustering in the returns, the fat tails in the distribution of\nresiduals are found to persist. We show that market crashes and volatility\nclustering may not sufficiently account for the existence of fat tails in\nreturn distributions. These findings are robust regardless of period or type of\nstock group.\n"
    },
    {
        "paper_id": 1904.0293,
        "authors": "Julian H\\\"olzermann",
        "title": "Term Structure Modeling under Volatility Uncertainty",
        "comments": "rewrote/restructured the paper while main results remain unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study term structure movements in the spirit of Heath,\nJarrow, and Morton [Econometrica 60(1), 77-105] under volatility uncertainty.\nWe model the instantaneous forward rate as a diffusion process driven by a\nG-Brownian motion. The G-Brownian motion represents the uncertainty about the\nvolatility. Within this framework, we derive a sufficient condition for the\nabsence of arbitrage, known as the drift condition. In contrast to the\ntraditional model, the drift condition consists of several equations and\nseveral market prices, termed market price of risk and market prices of\nuncertainty, respectively. The drift condition is still consistent with the\nclassical one if there is no volatility uncertainty. Similar to the traditional\nmodel, the risk-neutral dynamics of the forward rate are completely determined\nby its diffusion term. The drift condition allows to construct arbitrage-free\nterm structure models that are completely robust with respect to the\nvolatility. In particular, we obtain robust versions of classical term\nstructure models.\n"
    },
    {
        "paper_id": 1904.03053,
        "authors": "Martine J Barons and Willy Aspinall",
        "title": "Anticipated impacts of Brexit scenarios on UK food prices and\n  implications for policies on poverty and health: a structured expert\n  judgement approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Food insecurity is associated with increased risk for several health\nconditions and with poor chronic disease management. Key determinants for\nhousehold food insecurity are income and food costs. Whereas short-term\nhousehold incomes are likely to remain static, increased food prices would be a\nsignificant driver of food insecurity. To investigate food price drivers for\nhousehold food security and its health consequences in the UK under scenarios\nof Deal and No deal for Brexit . To estimate the 5\\% and 95\\% quantiles of the\nprojected price distributions. Structured expert judgement elicitation, a\nwell-established method for quantifying uncertainty, using experts. In July\n2018, each expert estimated the median, 5\\% and 95\\% quantiles of changes in\nprice for ten food categories under Brexit Deal and No-deal to June 2020\nassuming Brexit had taken place on 29th March 2019. These were aggregated based\non the accuracy and informativeness of the experts on calibration questions.\nTen specialists in food procurement, retail, agriculture, economics, statistics\nand household food security. Results: when combined in proportions used to\ncalculate Consumer Prices Index food basket costs, median food price change for\nBrexit with a Deal is expected to be +6.1\\% [90\\% credible interval:-3\\%,\n+17\\%] and with No deal +22.5\\% [+1\\%, +52\\%]. The number of households\nexperiencing food insecurity and its severity are likely to increase because of\nexpected sizeable increases in median food prices after Brexit. Higher\nincreases are more likely than lower rises and towards the upper limits, these\nwould entail severe impacts. Research showing a low food budget leads to\nincreasingly poor diet suggests that demand for health services in both the\nshort and longer term is likely to increase due to the effects of food\ninsecurity on the incidence and management of diet-sensitive conditions.\n"
    },
    {
        "paper_id": 1904.03058,
        "authors": "Rama Cont and Marvin S. Mueller",
        "title": "A stochastic partial differential equation model for limit order book\n  dynamics",
        "comments": "40 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an analytically tractable class of models for the dynamics of a\nlimit order book, described through a stochastic partial differential equation\n(SPDE) with multiplicative noise for the order book centered at the mid-price,\nalong with stochastic dynamics for the mid-price which is consistent with the\norder flow dynamics. We provide conditions under which the model admits a\nfinite dimensional realization driven by a (low-dimensional) Markov process,\nleading to efficient estimation and computation methods. We study two examples\nof parsimonious models in this class: a two-factor model and a model with\nmean-reverting order book depth. For each model we analyze in detail the role\nof different parameters, the dynamics of the price, order book depth, volume\nand order imbalance, provide an intuitive financial interpretation of the\nvariables involved and show how the model reproduces statistical properties of\nprice changes, market depth and order flow in limit order markets.\n"
    },
    {
        "paper_id": 1904.03356,
        "authors": "Zbigniew Palmowski, Jos\\'e Luis P\\'erez, Budhi Arta Surya, Kazutoshi\n  Yamazaki",
        "title": "The Leland-Toft optimal capital structure model under Poisson\n  observations",
        "comments": "Forthcoming in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the optimal capital structure model with endogenous bankruptcy\nfirst studied by Leland \\cite{Leland94} and Leland and Toft \\cite{Leland96}.\nDifferently from the standard case, where shareholders observe continuously the\nasset value and bankruptcy is executed instantaneously without delay, we assume\nthat the information of the asset value is updated only at intervals, modeled\nby the jump times of an independent Poisson process. Under the spectrally\nnegative L\\'evy model, we obtain the optimal bankruptcy strategy and the\ncorresponding capital structure. A series of numerical studies are given to\nanalyze the sensitivity of observation frequency on the optimal solutions, the\noptimal leverage and the credit spreads.\n"
    },
    {
        "paper_id": 1904.03488,
        "authors": "Giuseppe Pernagallo and Benedetto Torrisi",
        "title": "Blindfolded monkeys or financial analysts: who is worth your money? New\n  evidence on informational inefficiencies in the U.S. stock market",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 2019",
        "doi": "10.1016/j.physa.2019.122900",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The efficient market hypothesis has been considered one of the most\ncontroversial arguments in finance, with the academia divided between who\nclaims the impossibility of beating the market and who believes that it is\npossible to gain over the average profits. If the hypothesis holds, it means,\nas suggested by Burton Malkiel, that a blindfolded monkey selecting stocks by\nthrowing darts at a newspaper's financial pages could perform as well as a\nfinancial analyst, or even better. In this paper we use a novel approach, based\non confidence intervals for proportions, to assess the degree of inefficiency\nin the S&P 500 Index components concluding that several stocks are inefficient:\nwe estimated the proportion of inefficient stocks in the index to be between\n12.13% and 27.87%. This supports other studies proving that a financial\nanalyst, probably, is a better investor than a blindfolded monkey.\n"
    },
    {
        "paper_id": 1904.03726,
        "authors": "Giuseppe Pernagallo and Benedetto Torrisi",
        "title": "A Theory of Information overload applied to perfectly efficient\n  financial markets",
        "comments": "21 pages, 3 figures",
        "journal-ref": "Review of Behavioral Finance, 2020",
        "doi": "10.1108/RBF-07-2019-0088",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Before the massive spread of computer technology, information was far from\ncomplex. The development of technology shifted the paradigm: from individuals\nwho faced scarce and costly information to individuals who face massive amounts\nof information accessible at low costs. Nowadays we are living in the era of\nbig data and investors deal every day with a huge flow of information. In the\nspirit of the modern idea that economic agents have limited computational\ncapacity, we propose an original model using information overload to show how\ntoo much information could cause financial markets to depart from the\ntraditional assumption of informational efficiency. We show that when\ninformation tends to infinite, the efficient market hypothesis ceases to be\ntrue. This happens also for lower levels of information, when the use of the\nmaximum amount of information is not optimal for investors. The present work\ncan be a stimulus to consider more realistic economic models and it can be\nfurther deepened including other realistic features present in financial\nmarkets, such as information asymmetry or noise in the transmission of\ninformation.\n"
    },
    {
        "paper_id": 1904.04171,
        "authors": "Julio Backhoff-Veraguas, Gudmund Pammer",
        "title": "Stability of martingale optimal transport and weak optimal transport",
        "comments": "Arguments and proofs have been significantly expanded. An appendix\n  has been added containing technical lemmas. An example with a figure have\n  been added for illustration",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under mild regularity assumptions, the transport problem is stable in the\nfollowing sense: if a sequence of optimal transport plans $\\pi_1, \\pi_2,\n\\ldots$ converges weakly to a transport plan $\\pi$, then $\\pi$ is also optimal\n(between its marginals).\n  Alfonsi, Corbetta and Jourdain asked whether the same property is true for\nthe martingale transport problem. This question seems particularly pressing\nsince martingale transport is motivated by robust finance where data is\nnaturally noisy. On a technical level, stability in the martingale case appears\nmore intricate than for classical transport since optimal transport plans $\\pi$\nare not characterized by a `monotonicity'-property of their support.\n  In this paper we give a positive answer and establish stability of the\nmartingale transport problem. As a particular case, this recovers the stability\nof the left curtain coupling established by Juillet. An important auxiliary\ntool is an unconventional topology which takes the temporal structure of\nmartingales into account. Our techniques also apply to the the weak transport\nproblem introduced by Gozlan, Roberto, Samson and Tetali.\n"
    },
    {
        "paper_id": 1904.04192,
        "authors": "Manlio De Domenico, Andrea Baronchelli",
        "title": "The fragility of decentralised trustless socio-technical systems",
        "comments": "Commentary published in EPJ Data Science",
        "journal-ref": "EPJ Data Science 8:2 (2019)",
        "doi": "10.1140/epjds/s13688-018-0180-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The blockchain technology promises to transform finance, money and even\ngovernments. However, analyses of blockchain applicability and robustness\ntypically focus on isolated systems whose actors contribute mainly by running\nthe consensus algorithm. Here, we highlight the importance of considering\ntrustless platforms within the broader ecosystem that includes social and\ncommunication networks. As an example, we analyse the flash-crash observed on\n21st June 2017 in the Ethereum platform and show that a major phenomenon of\nsocial coordination led to a catastrophic cascade of events across several\ninterconnected systems. We propose the concept of ``emergent centralisation''\nto describe situations where a single system becomes critically important for\nthe functioning of the whole ecosystem, and argue that such situations are\nlikely to become more and more frequent in interconnected socio-technical\nsystems. We anticipate that the systemic approach we propose will have\nimplications for future assessments of trustless systems and call for the\nattention of policy-makers on the fragility of our interconnected and rapidly\nchanging world.\n"
    },
    {
        "paper_id": 1904.04225,
        "authors": "Mateus A. Cavaliere, Sergio Granville, Gerson C. Oliveira, Mario V.F.\n  Pereira",
        "title": "A Forward Electricity Contract Price Projection: A Market Equilibrium\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents a methodology for forward electricity contract price\nprojection based on market equilibrium and social welfare optimization. In the\nmethodology supply and demand for forward contracts are produced in such a way\nthat each agent (generator/load/trader) optimizes a risk adjusted expected\nvalue of its revenue/cost. When uncertainties are represented by a discrete\nnumber of scenarios, a key result in the paper is that contract price\ncorresponds to the dual variable of the equilibrium constraints in the linear\nprogramming problem associated to the optimization of total agents' welfare.\nBesides computing an equilibrium contract price for a given year, the\nmethodology can also be used to compute the evolution of the probability\ndistribution associated to a contract price with a future delivery period; this\nan import issue in quantifying forward contract risks. Examples of the\nmethodology application are presented and discussed\n"
    },
    {
        "paper_id": 1904.04422,
        "authors": "Takuya Okabe and Jin Yoshimura",
        "title": "A long-term alternative formula for a stochastic stock price model",
        "comments": null,
        "journal-ref": "SN Applied Sciences 4 (2022) 292",
        "doi": "10.1007/s42452-022-05176-9",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study presents a long-term alternative formula for stock price variation\ndescribed by a geometric Brownian motion on the basis of median instead of mean\nor expected values. The proposed method is motivated by the observation made in\nremote fields, where optimality of bet-hedging or diversification strategies is\nexplained based on a measure different from expected value, like geometric\nmean. When the probability distribution of possible outcomes is significantly\nskewed, it is generally known that expected value leads to an erroneous picture\nowing to its sensitivity to outliers, extreme values of rare occurrence. Since\ngeometric mean, or its counterpart median for the log-normal distribution, does\nnot suffer from this drawback, it provides us with a more appropriate measure\nespecially for evaluating long-term outcomes dominated by outliers. Thus, the\npresent formula makes a more realistic prediction for long-term outcomes of a\nlarge volatility, for which the probability distribution becomes conspicuously\nheavy-tailed.\n"
    },
    {
        "paper_id": 1904.04546,
        "authors": "Pierre Henry-Labordere",
        "title": "(Martingale) Optimal Transport And Anomaly Detection With Neural\n  Networks: A Primal-dual Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a primal-dual algorithm for solving (martingale)\noptimal transportation problems, with cost functions satisfying the twist\ncondition, close to the one that has been used recently for training generative\nadversarial networks. As some additional applications, we consider anomaly\ndetection and automatic generation of financial data.\n"
    },
    {
        "paper_id": 1904.04554,
        "authors": "Pierre Henry-Labordere (SOCIETE GENERALE)",
        "title": "From (Martingale) Schrodinger bridges to a new class of Stochastic\n  Volatility Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following closely the construction of the Schrodinger bridge, we build a new\nclass of Stochastic Volatility Models exactly calibrated to market instruments\nsuch as for example Vanillas, options on realized variance or VIX options.\nThese models differ strongly from the well-known local stochastic volatility\nmodels, in particular the instantaneous volatility-of-volatility of the\nassociated naked SVMs is not modified, once calibrated to market instruments.\nThey can be interpreted as a martingale version of the Schrodinger bridge. The\nnumerical calibration is performed using a dynamic-like version of the Sinkhorn\nalgorithm. We finally highlight a striking relation with Dyson non-colliding\nBrownian motions.\n"
    },
    {
        "paper_id": 1904.04644,
        "authors": "Patrick Cheridito, Matti Kiiski, David J. Pr\\\"omel and H. Mete Soner",
        "title": "Martingale optimal transport duality",
        "comments": "29 pages",
        "journal-ref": "Math. Ann. 379, 1685--1712 (2021)",
        "doi": "10.1007/s00208-019-01952-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain a dual representation of the Kantorovich functional defined for\nfunctions on the Skorokhod space using quotient sets. Our representation takes\nthe form of a Choquet capacity generated by martingale measures satisfying\nadditional constraints to ensure compatibility with the quotient sets. These\nsets contain stochastic integrals defined pathwise and two such definitions\nstarting with simple integrands are given. Another important ingredient of our\nanalysis is a regularized version of Jakubowski's $S$-topology on the Skorokhod\nspace.\n"
    },
    {
        "paper_id": 1904.04911,
        "authors": "Brahim Gaies, Khaled Guesmi, St\\'ephane Goutte (LED)",
        "title": "FDI, banking crisis and growth: direct and spill over effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study suggests a new decomposition of the effect of Foreign Direct\nInvestment (FDI) on long-term growth in developing countries. It reveals that\nFDI not only have a positive direct effect on growth, but also increase the\nlatter by reducing the recessionary effect resulting from a banking crisis.\nEven more, they reduce its occurrence. JEL: F65, F36, G01, G15\n"
    },
    {
        "paper_id": 1904.04912,
        "authors": "Bryan Lim, Stefan Zohren, Stephen Roberts",
        "title": "Enhancing Time Series Momentum Strategies Using Deep Neural Networks",
        "comments": null,
        "journal-ref": "The Journal of Financial Data Science, Fall 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While time series momentum is a well-studied phenomenon in finance, common\nstrategies require the explicit definition of both a trend estimator and a\nposition sizing rule. In this paper, we introduce Deep Momentum Networks -- a\nhybrid approach which injects deep learning based trading rules into the\nvolatility scaling framework of time series momentum. The model also\nsimultaneously learns both trend estimation and position sizing in a\ndata-driven manner, with networks directly trained by optimising the Sharpe\nratio of the signal. Backtesting on a portfolio of 88 continuous futures\ncontracts, we demonstrate that the Sharpe-optimised LSTM improved traditional\nmethods by more than two times in the absence of transactions costs, and\ncontinue outperforming when considering transaction costs up to 2-3 basis\npoints. To account for more illiquid assets, we also propose a turnover\nregularisation term which trains the network to factor in costs at run-time.\n"
    },
    {
        "paper_id": 1904.04951,
        "authors": "Maximilian Beikirch, Simon Cramer, Martin Frank, Philipp Otte, Emma\n  Pabich, Torsten Trimborn",
        "title": "Robust Mathematical Formulation and Probabilistic Description of\n  Agent-Based Computational Economic Market Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In science and especially in economics, agent-based modeling has become a\nwidely used modeling approach. These models are often formulated as a large\nsystem of difference equations. In this study, we discuss two aspects,\nnumerical modeling and the probabilistic description for two agent-based\ncomputational economic market models: the Levy-Levy-Solomon model and the\nFranke-Westerhoff model. We derive time-continuous formulations of both models,\nand in particular we discuss the impact of the time-scaling on the model\nbehavior for the Levy-Levy-Solomon model. For the Franke-Westerhoff model, we\nproof that a constraint required in the original model is not necessary for\nstability of the time-continuous model. It is shown that a semi-implicit\ndiscretization of the time-continuous system preserves this unconditional\nstability. In addition, this semi-implicit discretization can be computed at\ncost comparable to the original model. Furthermore, we discuss possible\nprobabilistic descriptions of time continuous agent-based computational\neconomic market models. Especially, we present the potential advantages of\nkinetic theory in order to derive mesoscopic desciptions of agent-based models.\nExemplified, we show two probabilistic descriptions of the Levy-Levy-Solomon\nand Franke-Westerhoff model.\n"
    },
    {
        "paper_id": 1904.04973,
        "authors": "Yoshiharu Sato",
        "title": "Model-Free Reinforcement Learning for Financial Portfolios: A Brief\n  Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial portfolio management is one of the problems that are most\nfrequently encountered in the investment industry. Nevertheless, it is not\nwidely recognized that both Kelly Criterion and Risk Parity collapse into Mean\nVariance under some conditions, which implies that a universal solution to the\nportfolio optimization problem could potentially exist. In fact, the process of\nsequential computation of optimal component weights that maximize the\nportfolio's expected return subject to a certain risk budget can be\nreformulated as a discrete-time Markov Decision Process (MDP) and hence as a\nstochastic optimal control, where the system being controlled is a portfolio\nconsisting of multiple investment components, and the control is its component\nweights. Consequently, the problem could be solved using model-free\nReinforcement Learning (RL) without knowing specific component dynamics. By\nexamining existing methods of both value-based and policy-based model-free RL\nfor the portfolio optimization problem, we identify some of the key unresolved\nquestions and difficulties facing today's portfolio managers of applying\nmodel-free RL to their investment portfolios.\n"
    },
    {
        "paper_id": 1904.05028,
        "authors": "Zhiyong Tu (Peking University HSBC Business School University Town,\n  Shenzhen, China), Lan Ju (Peking University HSBC Business School University\n  Town, Shenzhen, China)",
        "title": "A Normative Dual-value Theory for Bitcoin and other Cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin as well as other cryptocurrencies are all plagued by the impact from\nbifurcation. Since the marginal cost of bifurcation is theoretically zero, it\ncauses the coin holders to doubt on the existence of the coin's intrinsic\nvalue. This paper suggests a normative dual-value theory to assess the\nfundamental value of Bitcoin. We draw on the experience from the art market,\nwhere similar replication problems are prevalent. The idea is to decompose the\ntotal value of a cryptocurrency into two parts: one is its art value and the\nother is its use value. The tradeoff between these two values is also analyzed,\nwhich enlightens our proposal of an image coin for Bitcoin so as to elevate its\nuse value without sacrificing its art value. To show the general validity of\nthe dual-value theory, we also apply it to evaluate the prospects of four major\ncryptocurrencies. We find this framework is helpful for both the investors and\nthe exchanges to examine a new coin's value when it first appears in the\nmarket.\n"
    },
    {
        "paper_id": 1904.05312,
        "authors": "Angelos Alexopoulos, Petros Dellaportas, Omiros Papaspiliopoulos",
        "title": "Bayesian prediction of jumps in large panels of time series data",
        "comments": "49 pages, 27 figures, 4 tables",
        "journal-ref": null,
        "doi": "10.1214/21-BA1268",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We take a new look at the problem of disentangling the volatility and jumps\nprocesses of daily stock returns. We first provide a computational framework\nfor the univariate stochastic volatility model with Poisson-driven jumps that\noffers a competitive inference alternative to the existing tools. This\nmethodology is then extended to a large set of stocks for which we assume that\ntheir unobserved jump intensities co-evolve in time through a dynamic factor\nmodel. To evaluate the proposed modelling approach we conduct out-of-sample\nforecasts and we compare the posterior predictive distributions obtained from\nthe different models. We provide evidence that joint modelling of jumps\nimproves the predictive ability of the stochastic volatility models.\n"
    },
    {
        "paper_id": 1904.05315,
        "authors": "Amin Azari",
        "title": "Bitcoin Price Prediction: An ARIMA Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin is considered the most valuable currency in the world. Besides being\nhighly valuable, its value has also experienced a steep increase, from around 1\ndollar in 2010 to around 18000 in 2017. Then, in recent years, it has attracted\nconsiderable attention in a diverse set of fields, including economics and\ncomputer science. The former mainly focuses on studying how it affects the\nmarket, determining reasons behinds its price fluctuations, and predicting its\nfuture prices. The latter mainly focuses on its vulnerabilities, scalability,\nand other techno-crypto-economic issues. Here, we aim at revealing the\nusefulness of traditional autoregressive integrative moving average (ARIMA)\nmodel in predicting the future value of bitcoin by analyzing the price time\nseries in a 3-years-long time period. On the one hand, our empirical studies\nreveal that this simple scheme is efficient in sub-periods in which the\nbehavior of the time-series is almost unchanged, especially when it is used for\nshort-term prediction, e.g. 1-day. On the other hand, when we try to train the\nARIMA model to a 3-years-long period, during which the bitcoin price has\nexperienced different behaviors, or when we try to use it for a long-term\nprediction, we observe that it introduces large prediction errors. Especially,\nthe ARIMA model is unable to capture the sharp fluctuations in the price, e.g.\nthe volatility at the end of 2017. Then, it calls for more features to be\nextracted and used along with the price for a more accurate prediction of the\nprice. We have further investigated the bitcoin price prediction using an ARIMA\nmodel, trained over a large dataset, and a limited test window of the bitcoin\nprice, with length $w$, as inputs. Our study sheds lights on the interaction of\nthe prediction accuracy, choice of ($p,q,d$), and window size $w$.\n"
    },
    {
        "paper_id": 1904.05317,
        "authors": "Abhibasu Sen, Prof. Karabi Dutta Chaudhury",
        "title": "On the Co-movement of Crude, Gold Prices and Stock Index in Indian\n  Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This non-linear relationship in the joint time-frequency domain has been\nstudied for the Indian National Stock Exchange (NSE) with the international\nGold price and WTI Crude Price being converted from Dollar to Indian National\nRupee based on that week's closing exchange rate. Though a good correlation was\nobtained during some period, but as a whole no such cointegration relation can\nbe found out. Using the \\textit{Discrete Wavelet Analysis}, the data was\ndecomposed and the presence of Granger Causal relations was tested.\nUnfortunately no significant relationships are being found. We then studied the\n\\textit{Wavelet Coherence} of the two pairs viz. NSE-Nifty \\& Gold and\nNSE-Nifty \\& Crude. For different frequencies, the coherence between the pairs\nhave been studied. At lower frequencies, some relatively good coherence have\nbeen found. In this paper, we report for the first time the co-movements\nbetween Crude Oil, Gold and Indian Stock Market Index using Wavelet Analysis\n(both Discrete and Continuous), a technique which is most sophisticated and\nrecent in market analysis. Thus for long term traders they can include gold\nand/or crude in their portfolio along with NSE-Nifty index in order to decrease\nthe risk(volatility) of the portfolio for Indian Market. But for short term\ntraders, it will not be effective, not to include all the three in their\nportfolio.\n"
    },
    {
        "paper_id": 1904.05384,
        "authors": "Adamantios Ntakaris, Giorgio Mirone, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis",
        "title": "Feature Engineering for Mid-Price Prediction with Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mid-price movement prediction based on limit order book (LOB) data is a\nchallenging task due to the complexity and dynamics of the LOB. So far, there\nhave been very limited attempts for extracting relevant features based on LOB\ndata. In this paper, we address this problem by designing a new set of\nhandcrafted features and performing an extensive experimental evaluation on\nboth liquid and illiquid stocks. More specifically, we implement a new set of\neconometrical features that capture statistical properties of the underlying\nsecurities for the task of mid-price prediction. Moreover, we develop a new\nexperimental protocol for online learning that treats the task as a\nmulti-objective optimization problem and predicts i) the direction of the next\nprice movement and ii) the number of order book events that occur until the\nchange takes place. In order to predict the mid-price movement, the features\nare fed into nine different deep learning models based on multi-layer\nperceptrons (MLP), convolutional neural networks (CNN) and long short-term\nmemory (LSTM) neural networks. The performance of the proposed method is then\nevaluated on liquid and illiquid stocks, which are based on TotalView-ITCH US\nand Nordic stocks, respectively. For some stocks, results suggest that the\ncorrect choice of a feature set and a model can lead to the successful\nprediction of how long it takes to have a stock price movement.\n"
    },
    {
        "paper_id": 1904.05422,
        "authors": "Matteo Brachetta and Claudia Ceci",
        "title": "Optimal excess-of-loss reinsurance for stochastic factor risk models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal excess-of-loss reinsurance problem when both the\nintensity of the claims arrival process and the claim size distribution are\ninfluenced by an exogenous stochastic factor. We assume that the insurer's\nsurplus is governed by a marked point process with dual-predictable projection\naffected by an environmental factor and that the insurance company can borrow\nand invest money at a constant real-valued risk-free interest rate $r$. Our\nmodel allows for stochastic risk premia, which take into account risk\nfluctuations. Using stochastic control theory based on the\nHamilton-Jacobi-Bellman equation, we analyze the optimal reinsurance strategy\nunder the criterion of maximizing the expected exponential utility of the\nterminal wealth. A verification theorem for the value function in terms of\nclassical solutions of a backward partial differential equation is provided.\nFinally, some numerical results are discussed.\n"
    },
    {
        "paper_id": 1904.05472,
        "authors": "Dorje C. Brody, Lane P. Hughston and Bernhard K. Meister",
        "title": "Theory of Cryptocurrency Interest Rates",
        "comments": "21 pages, 3 figures, version to appear in SIAM Journal on Financial\n  Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A term structure model in which the short rate is zero is developed as a\ncandidate for a theory of cryptocurrency interest rates. The price processes of\ncrypto discount bonds are worked out, along with expressions for the\ninstantaneous forward rates and the prices of interest-rate derivatives. The\nmodel admits functional degrees of freedom that can be calibrated to the\ninitial yield curve and other market data. Our analysis suggests that strict\nlocal martingales can be used for modelling the pricing kernels associated with\nvirtual currencies based on distributed ledger technologies.\n"
    },
    {
        "paper_id": 1904.05481,
        "authors": "Bar Light",
        "title": "Stochastic Comparative Statics in Markov Decision Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In multi-period stochastic optimization problems, the future optimal decision\nis a random variable whose distribution depends on the parameters of the\noptimization problem. We analyze how the expected value of this random variable\nchanges as a function of the dynamic optimization parameters in the context of\nMarkov decision processes. We call this analysis \\emph{stochastic comparative\nstatics}. We derive both \\emph{comparative statics} results and\n\\emph{stochastic comparative statics} results showing how the current and\nfuture optimal decisions change in response to changes in the single-period\npayoff function, the discount factor, the initial state of the system, and the\ntransition probability function. We apply our results to various models from\nthe economics and operations research literature, including investment theory,\ndynamic pricing models, controlled random walks, and comparisons of stationary\ndistributions.\n"
    },
    {
        "paper_id": 1904.05554,
        "authors": "Akshay Vij",
        "title": "Understanding consumer demand for new transport technologies and\n  services, and implications for the future of mobility",
        "comments": "15 pages, 0 figures, book chapter",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The transport sector is witnessing unprecedented levels of disruption.\nPrivately owned cars that operate on internal combustion engines have been the\ndominant modes of passenger transport for much of the last century. However,\nrecent advances in transport technologies and services, such as the development\nof autonomous vehicles, the emergence of shared mobility services, and the\ncommercialization of alternative fuel vehicle technologies, promise to\nrevolutionise how humans travel. The implications are profound: some have\npredicted the end of private car dependent Western societies, others have\nportended greater suburbanization than has ever been observed before. If\ntransport systems are to fulfil current and future needs of different\nsubpopulations, and satisfy short and long-term societal objectives, it is\nimperative that we comprehend the many factors that shape individual behaviour.\nThis chapter introduces the technologies and services most likely to disrupt\nprevailing practices in the transport sector. We review past studies that have\nexamined current and future demand for these new technologies and services, and\ntheir likely short and long-term impacts on extant mobility patterns. We\nconclude with a summary of what these new technologies and services might mean\nfor the future of mobility.\n"
    },
    {
        "paper_id": 1904.05656,
        "authors": "Erik Eyster and Kristof Madarasz and Pascal Michaillat",
        "title": "Pricing under Fairness Concerns",
        "comments": null,
        "journal-ref": "Journal of the European Economic Association 19 (2021) 1853-1898",
        "doi": "10.1093/jeea/jvaa041",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a theory of pricing premised upon the assumptions that\ncustomers dislike unfair prices---those marked up steeply over cost---and that\nfirms take these concerns into account when setting prices. Since they do not\nobserve firms' costs, customers must extract costs from prices. The theory\nassumes that customers infer less than rationally: when a price rises due to a\ncost increase, customers partially misattribute the higher price to a higher\nmarkup---which they find unfair. Firms anticipate this response and trim their\nprice increases, which drives the passthrough of costs into prices below one:\nprices are somewhat rigid. Embedded in a New Keynesian model as a replacement\nfor the usual pricing frictions, our theory produces monetary nonneutrality:\nwhen monetary policy loosens and inflation rises, customers misperceive markups\nas higher and feel unfairly treated; firms mitigate this perceived unfairness\nby reducing their markups; in general equilibrium, employment rises. The theory\nalso features a hybrid short-run Phillips curve, realistic impulse responses of\noutput and employment to monetary and technology shocks, and an upward-sloping\nlong-run Phillips curve.\n"
    },
    {
        "paper_id": 1904.05921,
        "authors": "Bing Yu, Xiaojing Xing, and Agus Sudjianto",
        "title": "Deep-learning based numerical BSDE method for barrier options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As is known, an option price is a solution to a certain partial differential\nequation (PDE) with terminal conditions (payoff functions). There is a close\nassociation between the solution of PDE and the solution of a backward\nstochastic differential equation (BSDE). We can either solve the PDE to obtain\noption prices or solve its associated BSDE. Recently a deep learning technique\nhas been applied to solve option prices using the BSDE approach. In this\napproach, deep learning is used to learn some deterministic functions, which\nare used in solving the BSDE with terminal conditions. In this paper, we extend\nthe deep-learning technique to solve a PDE with both terminal and boundary\nconditions. In particular, we will employ the technique to solve barrier\noptions using Brownian motion bridges.\n"
    },
    {
        "paper_id": 1904.05931,
        "authors": "Anshul Verma and Pierpaolo Vivo and Tiziana Di Matteo",
        "title": "A memory-based method to select the number of relevant components in\n  Principal Component Analysis",
        "comments": "29 pages, published",
        "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment, 2019",
        "doi": "10.1088/1742-5468/ab3bc4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new data-driven method to select the optimal number of relevant\ncomponents in Principal Component Analysis (PCA). This new method applies to\ncorrelation matrices whose time autocorrelation function decays more slowly\nthan an exponential, giving rise to long memory effects. In comparison with\nother available methods present in the literature, our procedure does not rely\non subjective evaluations and is computationally inexpensive. The underlying\nbasic idea is to use a suitable factor model to analyse the residual memory\nafter sequentially removing more and more components, and stopping the process\nwhen the maximum amount of memory has been accounted for by the retained\ncomponents. We validate our methodology on both synthetic and real financial\ndata, and find in all cases a clear and computationally superior answer\nentirely compatible with available heuristic criteria, such as cumulative\nvariance and cross-validation.\n"
    },
    {
        "paper_id": 1904.06007,
        "authors": "Seyed Soheil Hosseini, Nick Wormald, and Tianhai Tian",
        "title": "A Weight-based Information Filtration Algorithm for Stock-Correlation\n  Networks",
        "comments": "7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several algorithms have been proposed to filter information on a complete\ngraph of correlations across stocks to build a stock-correlation network. Among\nthem the planar maximally filtered graph (PMFG) algorithm uses $3n-6$ edges to\nbuild a graph whose features include a high frequency of small cliques and a\ngood clustering of stocks. We propose a new algorithm which we call\nproportional degree (PD) to filter information on the complete graph of\nnormalised mutual information (NMI) across stocks. Our results show that the PD\nalgorithm produces a network showing better homogeneity with respect to\ncliques, as compared to economic sectoral classification than its PMFG\ncounterpart. We also show that the partition of the PD network obtained through\nnormalised spectral clustering (NSC) agrees better with the NSC of the complete\ngraph than the corresponding one obtained from PMFG. Finally, we show that the\nclusters in the PD network are more robust with respect to the removal of\nrandom sets of edges than those in the PMFG network.\n"
    },
    {
        "paper_id": 1904.06298,
        "authors": "O. A. Malafeyev, I. I. Pavlov",
        "title": "Dynamic investment model of the life cycle of a company under the\n  influence of factors in a competitive environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modelling all possible life cycles of a company in a highly competitive\neconomic environment gives a significant advantage to the owner in his business\ninvestment activities. This article proposes and analyses a dynamic model of a\ncompany's life cycle with known action costs and transition probabilities, that\ncan be affected by an outside influence. For this task, the Markov model was\nutilized. The proposed model is illustrated on a task of determining an\nadvertising policy for a car dealership, that would increase the stock equity\nof a company. The result demonstrates the usefulness of a model for use in\ndetermining future actions of a company. We also review multiple models of the\ninfluence of outside factors on a company's total capitalization.\n"
    },
    {
        "paper_id": 1904.06337,
        "authors": "Arvind Shrivats, Sebastian Jaimungal",
        "title": "Optimal Behaviour in Solar Renewable Energy Certificate (SREC) Markets",
        "comments": "29 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  SREC markets are a relatively novel market-based system to incentivize the\nproduction of energy from solar means. A regulator imposes a floor on the\namount of energy each regulated firm must generate from solar power in a given\nperiod and provides them with certificates for each generated MWh. Firms offset\nthese certificates against the floor and pay a penalty for any lacking\ncertificates. Certificates are tradable assets, allowing firms to purchase/sell\nthem freely. In this work, we formulate a stochastic control problem for\ngenerating and trading in SREC markets from a regulated firm's perspective. We\naccount for generation and trading costs, the impact both have on SREC prices,\nprovide a characterization of the optimal strategy, and develop a numerical\nalgorithm to solve this control problem. Through numerical experiments, we\nexplore how a firm who acts optimally behaves under various conditions. We find\nthat an optimal firm's generation and trading behaviour can be separated into\nvarious regimes, based on the marginal benefit of obtaining an additional SREC,\nand validate our theoretical characterization of the optimal strategy. We also\nconduct parameter sensitivity experiments and conduct comparisons of the\noptimal strategy to other candidate strategies.\n"
    },
    {
        "paper_id": 1904.0652,
        "authors": "Jamie Hentall MacCuish",
        "title": "Costly Attention and Retirement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most people are mistaken about the details of their pensions. Mistaken\nbeliefs about financially important policies imply significant informational\nfrictions. This paper incorporates informational friction, specifically a cost\nof attention to an uncertain pension policy, into a life-cycle model of\nretirement. This entails solving a dynamic rational inattention model with\nendogenous heterogeneous beliefs: a significant methodological contribution in\nitself. Resulting endogenous mistaken beliefs help explain a puzzle, namely\nlabour market exits concentrate at official retirement ages despite weak\nincentives to do so. The context of the study is the UK female state pension\nage (SPA) reform. I find most women are mistaken about their SPA, mistakes are\npredictive of behaviour, and mistakes decrease with age. I estimate the model\nusing simulated method of moments. Costly attention significantly improves\nmodel predictions of the labour supply response to the SPA whilst accommodating\nthe observed learning about the individual's SPA. An extension addresses\nanother retirement puzzle, the extremely low take-up of actuarially\nadvantageous deferral options. Introducing costly attention into a model with\nclaiming significantly increase the number of people claiming early when the\noption to defer appears actuarially advantageous.\n"
    },
    {
        "paper_id": 1904.06628,
        "authors": "Alex Garivaltis",
        "title": "Nash Bargaining Over Margin Loans to Kelly Gamblers",
        "comments": "24 pages, 2 figures",
        "journal-ref": "Risks, 7(3), 93 (2019)",
        "doi": "10.13140/RG.2.2.29080.65286",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I derive practical formulas for optimal arrangements between sophisticated\nstock market investors (namely, continuous-time Kelly gamblers or, more\ngenerally, CRRA investors) and the brokers who lend them cash for leveraged\nbets on a high Sharpe asset (i.e. the market portfolio). Rather than, say, the\nbroker posting a monopoly price for margin loans, the gambler agrees to use a\ngreater quantity of margin debt than he otherwise would in exchange for an\ninterest rate that is lower than the broker would otherwise post. The gambler\nthereby attains a higher asymptotic capital growth rate and the broker enjoys a\ngreater rate of intermediation profit than would obtain under non-cooperation.\nIf the threat point represents a vicious breakdown of negotiations (resulting\nin zero margin loans), then we get an elegant rule of thumb:\n$r_L^*=(3/4)r+(1/4)(\\nu-\\sigma^2/2)$, where $r$ is the broker's cost of funds,\n$\\nu$ is the compound-annual growth rate of the market index, and $\\sigma$ is\nthe annual volatility. We show that, regardless of the particular threat point,\nthe gambler will negotiate to size his bets as if he himself could borrow at\nthe broker's call rate.\n"
    },
    {
        "paper_id": 1904.06695,
        "authors": "Prateek Bansal, Akanksha Sinha, Rubal Dua, Ricardo Daziano",
        "title": "Eliciting Preferences of Ridehailing Users and Drivers: Evidence from\n  the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transportation Network Companies (TNCs) are changing the transportation\necosystem, but micro-decisions of drivers and users need to be better\nunderstood to assess the system-level impacts of TNCs. In this regard, we\ncontribute to the literature by estimating a) individuals' preferences of being\na rider, a driver, or a non-user of TNC services; b) preferences of ridehailing\nusers for ridepooling; c) TNC drivers' choice to switch to vehicles with better\nfuel economy, and also d) the drivers' decision to buy, rent or lease new\nvehicles with driving for TNCs being a major consideration. Elicitation of\ndrivers' preferences using a unique sample (N=11,902) of the U.S. population\nresiding in TNC-served areas is the key feature of this study. The statistical\nanalysis indicates that ridehailing services are mainly attracting personal\nvehicle users as riders, without substantially affecting demand for transit.\nMoreover, around 10% of ridehailing users reported postponing the purchase of a\nnew car due to the availability of TNC services. The model estimation results\nindicate that the likelihood of being a TNC user increases with the increase in\nage for someone younger than 44 years, but the pattern is reversed post 44\nyears. This change in direction of the marginal effect of age is insightful as\nthe previous studies have reported a negative association. We also find that\npostgraduate drivers who live in metropolitan regions are more likely to switch\nto fuel-efficient vehicles. These findings would inform transportation planners\nand TNCs in developing policies to improve the fuel economy of the fleet.\n"
    },
    {
        "paper_id": 1904.06722,
        "authors": "Snehalkumar (Neil) S. Gaikwad, Durim Morina, Adam Ginzberg, Catherine\n  Mullings, Shirish Goyal, Dilrukshi Gamage, Christopher Diemert, Mathias\n  Burton, Sharon Zhou, Mark Whiting, Karolina Ziulkoski, Alipta Ballav, Aaron\n  Gilbee, Senadhipathige S. Niranga, Vibhor Sehgal, Jasmine Lin, Leonardy\n  Kristianto, Angela Richmond-Fuller, Jeff Regino, Nalin Chhibber, Dinesh\n  Majeti, Sachin Sharma, Kamila Mananova, Dinesh Dhakal, William Dai, Victoria\n  Purynova, Samarth Sandeep, Varshine Chandrakanthan, Tejas Sarma, Sekandar\n  Matin, Ahmed Nasser, Rohit Nistala, Alexander Stolzoff, Kristy Milland,\n  Vinayak Mathur, Rajan Vaish, Michael S. Bernstein",
        "title": "Boomerang: Rebounding the Consequences of Reputation Feedback on\n  Crowdsourcing Platforms",
        "comments": null,
        "journal-ref": "Proceedings of the 29th Annual Symposium on User Interface\n  Software and Technology, 2016",
        "doi": "10.1145/2984511.2984542",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Paid crowdsourcing platforms suffer from low-quality work and unfair\nrejections, but paradoxically, most workers and requesters have high reputation\nscores. These inflated scores, which make high-quality work and workers\ndifficult to find, stem from social pressure to avoid giving negative feedback.\nWe introduce Boomerang, a reputation system for crowdsourcing that elicits more\naccurate feedback by rebounding the consequences of feedback directly back onto\nthe person who gave it. With Boomerang, requesters find that their highly-rated\nworkers gain earliest access to their future tasks, and workers find tasks from\ntheir highly-rated requesters at the top of their task feed. Field experiments\nverify that Boomerang causes both workers and requesters to provide feedback\nthat is more closely aligned with their private opinions. Inspired by a\ngame-theoretic notion of incentive-compatibility, Boomerang opens opportunities\nfor interaction design to incentivize honest reporting over strategic\ndishonesty.\n"
    },
    {
        "paper_id": 1904.06757,
        "authors": "Toomas Hinnosaar",
        "title": "Price Setting on a Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most products are produced and sold by supply chain networks, where an\ninterconnected network of producers and intermediaries set prices to maximize\ntheir profits. I show that there exists a unique equilibrium in a price-setting\ngame on a network. The key distortion reducing both total profits and social\nwelfare is multiple-marginalization, which is magnified by strategic\ninteractions. Individual profits are proportional to influentiality, which is a\nnew measure of network centrality defined by the equilibrium characterization.\nThe results emphasize the importance of the network structure when considering\npolicy questions such as mergers or trade tariffs.\n"
    },
    {
        "paper_id": 1904.06824,
        "authors": "Bikramjit Das, Vicky Fasen-Hartmann and Claudia Kl\\\"uppelberg",
        "title": "Tail probabilities of random linear functions of regularly varying\n  random vectors",
        "comments": "27 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a new extension of Breiman's Theorem on computing tail\nprobabilities of a product of random variables to a multivariate setting. In\nparticular, we give a complete characterization of regular variation on cones\nin $[0,\\infty)^d$ under random linear transformations. This allows us to\ncompute probabilities of a variety of tail events, which classical multivariate\nregularly varying models would report to be asymptotically negligible. We\nillustrate our findings with applications to risk assessment in financial\nsystems and reinsurance markets under a bipartite network structure.\n"
    },
    {
        "paper_id": 1904.07226,
        "authors": "Muhammad Naqeeb, Amjad hussain",
        "title": "From multi-dimensional black scholes to Hamilton jacobi",
        "comments": "9 pages, dynamical systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The first widely used financial model is linked to dynamical Hamilton jacobi\nmodel\n"
    },
    {
        "paper_id": 1904.07583,
        "authors": "M. Derksen, B. Kleijn and R. de Vilder",
        "title": "Clearing price distributions in call auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for price formation in financial markets based on clearing\nof a standard call auction with random orders, and verify its validity for\nprediction of the daily closing price distribution statistically. The model\nconsiders random buy and sell orders, placed following demand- and supply-side\nvaluation distributions; an equilibrium equation then leads to a distribution\nfor clearing price and transacted volume. Bid and ask volumes are left as free\nparameters, permitting possibly heavy-tailed or very skewed order flow\nconditions. In highly liquid auctions, the clearing price distribution\nconverges to an asymptotically normal central limit, with mean and variance in\nterms of supply/demand-valuation distributions and order flow imbalance. By\nmeans of simulations, we illustrate the influence of variations in order flow\nand valuation distributions on price/volume, noting a distinction between high-\nand low-volume auction price variance. To verify the validity of the model\nstatistically, we predict a year's worth of daily closing price distributions\nfor 5 constituents of the Eurostoxx 50 index; Kolmogorov-Smirnov statistics and\nQQ-plots demonstrate with ample statistical significance that the model\npredicts closing price distributions accurately, and compares favourably with\nalternative methods of prediction.\n"
    },
    {
        "paper_id": 1904.07644,
        "authors": "V. Bhaskar and Nikita Roketskiy",
        "title": "Consumer Privacy and Serial Monopoly",
        "comments": "Keywords: consumer privacy, dynamic demand, endogenous screening,\n  nonlinear pricing. JEL Codes: D11, D43, L13",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the implications of consumer privacy when preferences today depend\nupon past consumption choices, and consumers shop from different sellers in\neach period. Although consumers are ex ante identical, their initial\nconsumption choices cannot be deterministic. Thus ex post heterogeneity in\npreferences arises endogenously. Consumer privacy improves social welfare,\nconsumer surplus and the profits of the second-period seller, while reducing\nthe profits of the first period seller, relative to the situation where\nconsumption choices are observed by the later seller.\n"
    },
    {
        "paper_id": 1904.0766,
        "authors": "Giuseppe Toscani, Andrea Tosin, Mattia Zanella",
        "title": "Multiple-interaction kinetic modelling of a virtual-item gambling\n  economy",
        "comments": null,
        "journal-ref": "Phys. Rev. E 100, 012308 (2019)",
        "doi": "10.1103/PhysRevE.100.012308",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, there has been a proliferation of online gambling sites,\nwhich made gambling more accessible with a consequent rise in related problems,\nsuch as addiction. Hence, the analysis of the gambling behaviour at both the\nindividual and the aggregate levels has become the object of several\ninvestigations. In this paper, resorting to classical methods of the kinetic\ntheory, we describe the behaviour of a multi-agent system of gamblers\nparticipating in lottery-type games on a virtual-item gambling market. The\ncomparison with previous, often empirical, results highlights the ability of\nthe kinetic approach to explain how the simple microscopic rules of a\ngambling-type game produce complex collective trends, which might be difficult\nto interpret precisely by looking only at the available data.\n"
    },
    {
        "paper_id": 1904.07987,
        "authors": "Prateek Bansal, Yang Liu, Ricardo Daziano, Samitha Samaranayake",
        "title": "Can Mobility-on-Demand services do better after discerning reliability\n  preferences of riders?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formalize one aspect of reliability in the context of Mobility-on-Demand\n(MoD) systems by acknowledging the uncertainty in the pick-up time of these\nservices. This study answers two key questions: i) how the difference between\nthe stated and actual pick-up times affect the propensity of a passenger to\nchoose an MoD service? ii) how an MoD service provider can leverage this\ninformation to increase its ridership? We conduct a discrete choice experiment\nin New York to answer the former question and adopt a micro-simulation-based\noptimization method to answer the latter question. In our experiments, the\nridership of an MoD service could be increased by up to 10\\% via displaying the\npredicted wait time strategically.\n"
    },
    {
        "paper_id": 1904.08029,
        "authors": "Wenyuan Wang, Zhimin Zhang",
        "title": "Optimal loss-carry-forward taxation for L\\'{e}vy risk processes stopped\n  at general draw-down time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by Kyprianou and Zhou (2009), Wang and Hu (2012), Avram et al.\n(2017), Li et al. (2017) and Wang and Zhou (2018), we consider in this paper\nthe problem of maximizing the expected accumulated discounted tax payments of\nan insurance company, whose reserve process (before taxes are deducted) evolves\nas a spectrally negative L\\'{e}vy process with the usual exclusion of negative\nsubordinator or deterministic drift. Tax payments are collected according to\nthe very general loss-carry-forward tax system introduced in Kyprianou and Zhou\n(2009). To achieve a balance between taxation optimization and solvency, we\nconsider an interesting modified objective function by considering the expected\naccumulated discounted tax payments of the company until the general draw-down\ntime, instead of until the classical ruin time. The optimal tax return function\ntogether with the optimal tax strategy is derived, and some numerical examples\nare also provided.\n"
    },
    {
        "paper_id": 1904.08131,
        "authors": "Ionel Popescu and Tushar Vaidya",
        "title": "Averaging plus Learning Models and Their Asymptotics",
        "comments": "34 pages, 8 figures",
        "journal-ref": "Proc. R. Soc. A 2023",
        "doi": "10.1098/rspa.2022.0681",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop original models to study interacting agents in financial markets\nand in social networks. Within these models randomness is vital as a form of\nshock or news that decays with time. Agents learn from their observations and\nlearning ability to interpret news or private information in time-varying\nnetworks. Under general assumption on the noise, a limit theorem is developed\nfor the generalised DeGroot framework for certain type of conditions governing\nthe learning. In this context, the agents beliefs (properly scaled) converge in\ndistribution that is not necessarily normal. Fresh insights are gained not only\nfrom proposing a new setting for social learning models but also from using\ndifferent techniques to study discrete time random linear dynamical systems.\n"
    },
    {
        "paper_id": 1904.08136,
        "authors": "Yong Shi, Bo Li and Wen Long",
        "title": "A Pyramid Scheme Model Based on \"Consumption Rebate\" Frauds",
        "comments": "17 pages, 10 figures",
        "journal-ref": "Chin. Phys. B, 2019, Vol. 28(7): 078901",
        "doi": "10.1088/1674-1056/28/7/078901",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are various types of pyramid schemes which have inflicted or are\ninflicting losses on many people in the world. We propose a pyramid scheme\nmodel which has the principal characters of many pyramid schemes appeared in\nrecent years: promising high returns, rewarding the participants recruiting the\nnext generation of participants, and the organizer will take all the money away\nwhen he finds the money from the new participants is not enough to pay the\nprevious participants interest and rewards. We assume the pyramid scheme\ncarries on in the tree network, ER random network, SW small-world network or BA\nscale-free network respectively, then give the analytical results of how many\ngenerations the pyramid scheme can last in these cases. We also use our model\nto analyse a pyramid scheme in the real world and we find the connections\nbetween participants in the pyramid scheme may constitute a SW small-world\nnetwork.\n"
    },
    {
        "paper_id": 1904.08153,
        "authors": "Th\\'eophile Griveau-Billion, Ben Calderhead",
        "title": "A Dynamic Bayesian Model for Interpretable Decompositions of Market\n  Behaviour",
        "comments": "59 pages, 14 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a heterogeneous simultaneous graphical dynamic linear model\n(H-SGDLM), which extends the standard SGDLM framework to incorporate a\nheterogeneous autoregressive realised volatility (HAR-RV) model. This novel\napproach creates a GPU-scalable multivariate volatility estimator, which\ndecomposes multiple time series into economically-meaningful variables to\nexplain the endogenous and exogenous factors driving the underlying\nvariability. This unique decomposition goes beyond the classic one step ahead\nprediction; indeed, we investigate inferences up to one month into the future\nusing stocks, FX futures and ETF futures, demonstrating its superior\nperformance according to accuracy of large moves, longer-term prediction and\nconsistency over time.\n"
    },
    {
        "paper_id": 1904.08459,
        "authors": "Hieu Quang Nguyen, Abdul Hasib Rahimyar, Xiaodi Wang",
        "title": "Stock Forecasting using M-Band Wavelet-Based SVR and RNN-LSTMs Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The task of predicting future stock values has always been one that is\nheavily desired albeit very difficult. This difficulty arises from stocks with\nnon-stationary behavior, and without any explicit form. Hence, predictions are\nbest made through analysis of financial stock data. To handle big data sets,\ncurrent convention involves the use of the Moving Average. However, by\nutilizing the Wavelet Transform in place of the Moving Average to denoise stock\nsignals, financial data can be smoothened and more accurately broken down. This\nnewly transformed, denoised, and more stable stock data can be followed up by\nnon-parametric statistical methods, such as Support Vector Regression (SVR) and\nRecurrent Neural Network (RNN) based Long Short-Term Memory (LSTM) networks to\npredict future stock prices. Through the implementation of these methods, one\nis left with a more accurate stock forecast, and in turn, increased profits.\n"
    },
    {
        "paper_id": 1904.08522,
        "authors": "Vitor Possebom",
        "title": "Sharp Bounds for the Marginal Treatment Effect with Sample Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I analyze treatment effects in situations when agents endogenously select\ninto the treatment group and into the observed sample. As a theoretical\ncontribution, I propose pointwise sharp bounds for the marginal treatment\neffect (MTE) of interest within the always-observed subpopulation under\nmonotonicity assumptions. Moreover, I impose an extra mean dominance assumption\nto tighten the previous bounds. I further discuss how to identify those bounds\nwhen the support of the propensity score is either continuous or discrete.\nUsing these results, I estimate bounds for the MTE of the Job Corps Training\nProgram on hourly wages for the always-employed subpopulation and find that it\nis decreasing in the likelihood of attending the program within the\nNon-Hispanic group. For example, the Average Treatment Effect on the Treated is\nbetween \\$.33 and \\$.99 while the Average Treatment Effect on the Untreated is\nbetween \\$.71 and \\$3.00.\n"
    },
    {
        "paper_id": 1904.0878,
        "authors": "Romain Blanchard and Laurence Carassus",
        "title": "No-arbitrage with multiple-priors in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a discrete time and multiple-priors setting, we propose a new\ncharacterisation of the condition of quasi-sure no-arbitrage which has become a\nstandard assumption. This characterisation shows that it is indeed a\nwell-chosen condition being equivalent to several previously used alternative\nnotions of no-arbitrage and allowing the proof of important results in\nmathematical finance. We also revisit the so-called geometric and quantitative\nno-arbitrage conditions and explicit two important examples where all these\nconcepts are illustrated.\n"
    },
    {
        "paper_id": 1904.08829,
        "authors": "Xiaochuan Deng and Fei Sun",
        "title": "Regulator-based risk statistics for portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk statistic is a critical factor not only for risk analysis but also for\nfinancial application. However, the traditional risk statistics may fail to\ndescribe the characteristics of regulator-based risk. In this paper, we\nconsider the regulator-based risk statistics for portfolios. By further\ndeveloping the properties related to regulator-based risk statistics, we are\nable to derive dual representation for such risk.\n"
    },
    {
        "paper_id": 1904.08925,
        "authors": "Johannes Ruf and Kangjianan Xie",
        "title": "The impact of proportional transaction costs on systematically generated\n  portfolios",
        "comments": "25 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The effect of proportional transaction costs on systematically generated\nportfolios is studied empirically. The performance of several portfolios (the\nindex tracking portfolio, the equally-weighted portfolio, the entropy-weighted\nportfolio, and the diversity-weighted portfolio) in the presence of dividends\nand transaction costs is examined under different configurations involving the\ntrading frequency, constituent list size, and renewing frequency. Moreover, a\nmethod to smooth transaction costs is proposed.\n"
    },
    {
        "paper_id": 1904.09088,
        "authors": "Junyao Chen, Tony Sit and Hoi Ying Wong",
        "title": "Simulation-based Value-at-Risk for Nonlinear Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Value-at-risk (VaR) has been playing the role of a standard risk measure\nsince its introduction. In practice, the delta-normal approach is usually\nadopted to approximate the VaR of portfolios with option positions. Its\neffectiveness, however, substantially diminishes when the portfolios concerned\ninvolve a high dimension of derivative positions with nonlinear payoffs; lack\nof closed form pricing solution for these potentially highly correlated,\nAmerican-style derivatives further complicates the problem. This paper proposes\na generic simulation-based algorithm for VaR estimation that can be easily\napplied to any existing procedures. Our proposal leverages cross-sectional\ninformation and applies variable selection techniques to simplify the existing\nsimulation framework. Asymptotic properties of the new approach demonstrate\nfaster convergence due to the additional model selection component introduced.\nWe have also performed sets of numerical results that verify the effectiveness\nof our approach in comparison with some existing strategies.\n"
    },
    {
        "paper_id": 1904.09214,
        "authors": "Tarcisio M. Rocha Filho and Paulo M. M. Rocha",
        "title": "Inefficiency of the Brazilian Stock Market: the IBOVESPA Future\n  Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.123200",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present some indications of inefficiency of the Brazilian stock market\nbased on the existence of strong long-time cross-correlations with foreign\nmarkets and indices. Our results show a strong dependence on foreign markets\nindices as the S\\&P 500 and CAC 40, but not to the Shanghai SSE 180, indicating\nan intricate interdependence. We also show that the distribution of log-returns\nof the Brazilian BOVESPA index has a discrete fat tail in the time scale of a\nday, which is also a deviation of what is expected of an efficient equilibrated\nmarket. As a final argument of the inefficiency of the Brazilian stock market,\nwe use a neural network approach to forecast the direction of movement of the\nvalue of the IBOVESPA future contracts, with an accuracy allowing financial\nreturns over passive strategies.\n"
    },
    {
        "paper_id": 1904.0924,
        "authors": "Peter Carr and Andrey Itkin",
        "title": "ADOL - Markovian approximation of rough lognormal model",
        "comments": "19 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we apply Markovian approximation of the fractional Brownian\nmotion (BM), known as the Dobric-Ojeda (DO) process, to the fractional\nstochastic volatility model where the instantaneous variance is modelled by a\nlognormal process with drift and fractional diffusion. Since the DO process is\na semi-martingale, it can be represented as an \\Ito diffusion. It turns out\nthat in this framework the process for the spot price $S_t$ is a geometric BM\nwith stochastic instantaneous volatility $\\sigma_t$, the process for $\\sigma_t$\nis also a geometric BM with stochastic speed of mean reversion and\ntime-dependent colatility of volatility, and the supplementary process\n$\\calV_t$ is the Ornstein-Uhlenbeck process with time-dependent coefficients,\nand is also a function of the Hurst exponent. We also introduce an adjusted DO\nprocess which provides a uniformly good approximation of the fractional BM for\nall Hurst exponents $H \\in [0,1]$ but requires a complex measure. Finally, the\ncharacteristic function (CF) of $\\log S_t$ in our model can be found in closed\nform by using asymptotic expansion. Therefore, pricing options and variance\nswaps (by using a forward CF) can be done via FFT, which is much easier than in\nrough volatility models.\n"
    },
    {
        "paper_id": 1904.09379,
        "authors": "Qian Lin, Xianming Sun, Chao Zhou",
        "title": "Horizon-unbiased Investment with Ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the presence of ambiguity on the driving force of market randomness, we\nconsider the dynamic portfolio choice without any predetermined investment\nhorizon. The investment criteria is formulated as a robust forward performance\nprocess, reflecting an investor's dynamic preference. We show that the market\nrisk premium and the utility risk premium jointly determine the investors'\ntrading direction and the worst-case scenarios of the risky asset's mean return\nand volatility. The closed-form formulas for the optimal investment strategies\nare given in the special settings of the CRRA preference.\n"
    },
    {
        "paper_id": 1904.09403,
        "authors": "Akihiko Noda",
        "title": "On the Evolution of Cryptocurrency Market Efficiency",
        "comments": "10 pages, 3 figures, and 1 table",
        "journal-ref": null,
        "doi": "10.1080/13504851.2020.1758617",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines whether the efficiency of cryptocurrency markets (Bitcoin\nand Ethereum) evolve over time based on Lo's (2004) adaptive market hypothesis\n(AMH). In particular, we measure the degree of market efficiency using a\ngeneralized least squares-based time-varying model that does not depend on\nsample size, unlike previous studies that used conventional methods. The\nempirical results show that (1) the degree of market efficiency varies with\ntime in the markets, (2) Bitcoin's market efficiency level is higher than that\nof Ethereum over most periods, and (3) a market with high market liquidity has\nbeen evolving. We conclude that the results support the AMH for the most\nestablished cryptocurrency market.\n"
    },
    {
        "paper_id": 1904.09456,
        "authors": "Birgit Rudloff, Firdevs Ulus",
        "title": "Certainty Equivalent and Utility Indifference Pricing for Incomplete\n  Preferences via Convex Vector Optimization",
        "comments": null,
        "journal-ref": "Mathematics and Financial Economics 15(2), 397-430 (2021)",
        "doi": "10.1007/s11579-020-00282-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For incomplete preference relations that are represented by multiple priors\nand/or multiple -- possibly multivariate -- utility functions, we define a\ncertainty equivalent as well as the utility buy and sell prices and\nindifference price bounds as set-valued functions of the claim. Furthermore, we\nmotivate and introduce the notion of a weak and a strong certainty equivalent.\nWe will show that our definitions contain as special cases some definitions\nfound in the literature so far on complete or special incomplete preferences.\nWe prove monotonicity and convexity properties of utility buy and sell prices\nthat hold in total analogy to the properties of the scalar indifference prices\nfor complete preferences. We show how the (weak and strong) set-valued\ncertainty equivalent as well as the indifference price bounds can be computed\nor approximated by solving convex vector optimization problems. Numerical\nexamples and their economic interpretations are given for the univariate as\nwell as for the multivariate case.\n"
    },
    {
        "paper_id": 1904.09799,
        "authors": "Tommi Sottinen and Lauri Viitasaari",
        "title": "Prediction Law of Mixed Gaussian Volterra Processes",
        "comments": "60G15; 60G25",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the regular conditional law of mixed Gaussian Volterra processes\nunder the influence of model disturbances. More precisely, we study prediction\nof Gaussian Volterra processes driven by a Brownian motion in a case where the\nBrownian motion is not observable, but only a noisy version is observed. As an\napplication, we discuss how our result can be applied to variance reduction in\nthe presence of measurement errors.\n"
    },
    {
        "paper_id": 1904.09857,
        "authors": "Hiroya Taniguchi and Ken Yamada",
        "title": "ICT Capital-Skill Complementarity and Wage Inequality: Evidence from\n  OECD Countries",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.labeco.2022.102151",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although wage inequality has evolved in advanced countries over recent\ndecades, it remains unknown the extent to which changes in wage inequality and\ntheir differences across countries are attributable to specific capital and\nlabor quantities. We examine this issue by estimating a sector-level production\nfunction extended to allow for capital-skill complementarity and factor-biased\ntechnological change using cross-country and cross-industry panel data. Our\nresults indicate that most of the changes in the skill premium are attributable\nto the relative quantities of ICT equipment, skilled labor, and unskilled labor\nin the goods and service sectors of the majority of advanced countries.\n"
    },
    {
        "paper_id": 1904.09967,
        "authors": "Brendan Badia, Randall Berry, Ermin Wei",
        "title": "Investment in EV charging spots for parking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As demand for electric vehicles (EVs) is expanding, meeting the need for\ncharging infrastructure, especially in urban areas, has become a critical\nissue. One method of adding charging stations is to install them at parking\nspots. This increases the value of these spots to EV drivers needing to charge\ntheir vehicles. However, there is a cost to constructing these spots and such\nspots may preclude drivers not needing to charge from using them, reducing the\nparking options for such drivers\\color{black}. We look at two models for how\ndecisions surrounding investment in charging stations on existing parking spots\nmay be undertaken. First, we analyze two firms who compete over installing\nstations under government set mandates or subsidies. Given the cost of\nconstructing spots and the competitiveness of the markets, we find it is\nambiguous whether setting higher mandates or higher subsidies for spot\nconstruction leads to better aggregate outcomes. Second, we look at a system\noperator who faces uncertainty on the size of the EV market. If they are risk\nneutral, we find a relatively small change in the uncertainty of the EV market\ncan lead to large changes in the optimal charging capacity.\n"
    },
    {
        "paper_id": 1904.10063,
        "authors": "Zbigniew Palmowski and Budhi Surya",
        "title": "Optimal valuation of American callable credit default swaps under\n  drawdown of L\\'evy insurance risk process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the valuation of credit default swaps, where default is\nannounced when the reference asset price has gone below certain level from the\nlast record maximum, also known as the high-water mark or drawdown. We assume\nthat the protection buyer pays premium at fixed rate when the asset price is\nabove a pre-specified level and continuously pays whenever the price increases.\nThis payment scheme is in favour of the buyer as she only pays the premium when\nthe market is in good condition for the protection against financial downturn.\nUnder this framework, we look at an embedded option which gives the issuer an\nopportunity to call back the contract to a new one with reduced premium payment\nrate and slightly lower default coverage subject to paying a certain cost. We\nassume that the buyer is risk neutral investor trying to maximize the expected\nmonetary value of the option over a class of stopping time. We discuss optimal\nsolution to the stopping problem when the source of uncertainty of the asset\nprice is modelled by L\\'evy process with only downward jumps. Using recent\ndevelopment in excursion theory of L\\'evy process, the results are given\nexplicitly in terms of scale function of the L\\'evy process. Furthermore, the\nvalue function of the stopping problem is shown to satisfy continuous and\nsmooth pasting conditions regardless of regularity of the sample paths of the\nL\\'evy process. Optimality and uniqueness of the solution are established using\nmartingale approach for drawdown process and convexity of the scale function\nunder Esscher transform of measure. Some numerical examples are discussed to\nillustrate the main results.\n"
    },
    {
        "paper_id": 1904.10182,
        "authors": "Arnab Chakrabarti and Rituparna Sen",
        "title": "Copula estimation for nonsynchronous financial data",
        "comments": null,
        "journal-ref": "2022 Sankhya B, 85 (Suppl 1): 116-149",
        "doi": "10.1007/s13571-022-00276-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copula is a powerful tool to model multivariate data. We propose the\nmodelling of intraday financial returns of multiple assets through copula. The\nproblem originates due to the asynchronous nature of intraday financial data.\nWe propose a consistent estimator of the correlation coefficient in case of\nElliptical copula and show that the plug-in copula estimator is uniformly\nconvergent. For non-elliptical copulas, we capture the dependence through\nKendall's Tau. We demonstrate underestimation of the copula parameter and use a\nquadratic model to propose an improved estimator. In simulations, the proposed\nestimator reduces the bias significantly for a general class of copulas. We\napply the proposed methods to real data of several stock prices.\n"
    },
    {
        "paper_id": 1904.10229,
        "authors": "Ankush Agarwal, Christian-Oliver Ewald and Yongjie Wang",
        "title": "Hedging longevity risk in defined contribution pension schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pension schemes all over the world are under increasing pressure to\nefficiently hedge the longevity risk posed by ageing populations. In this work,\nwe study an optimal investment problem for a defined contribution pension\nscheme which decides to hedge the longevity risk using a mortality-linked\nsecurity, typically a longevity bond. The pension scheme invests in the risky\nassets available in the market, including the longevity bond, by using the\ncontributions from a representative scheme member to ensure a minimum guarantee\nsuch that the member is able to purchase a lifetime annuity upon retirement. We\ntransform this constrained optimal investment problem into an unconstrained\nproblem by replicating a self-financing portfolio of future contributions from\nthe member and the minimum guarantee provided by the scheme. We solve the\nresulting optimisation problem using the dynamic programming principle and\nthrough a series of numerical studies reveal that the longevity risk has an\nimportant impact on the performance of investment strategies. Our results\nprovide mathematical evidence supporting the use of mortality-linked securities\nfor efficient hedging of the longevity risk.\n"
    },
    {
        "paper_id": 1904.1025,
        "authors": "Jaros{\\l}aw Gruszka, Janusz Szwabi\\'nski",
        "title": "Best Portfolio Management Strategies For Synthetic and Real Assets",
        "comments": "19 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.122938",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing investment portfolios is an old and well know problem in multiple\nfields including financial mathematics and financial engineering as well as\neconometrics and econophysics. Multiple different concepts and theories were\nused so far to describe methods of handling with financial assets, including\ndifferential equations, stochastic calculus and advanced statistics. In this\npaper, using a set of tools from the probability theory, various strategies of\nbuilding financial portfolios are analysed in different market conditions. A\nspecial attention is given to several realisations of a so called balanced\nportfolio, which is rooted in the natural \"buy-low-sell-high\" principle.\nResults show that there is no universal strategy, because they perform\ndifferently in different circumstances (e.g. for varying transaction costs).\nMoreover, the planned time of investment may also have a significant impact on\nthe profitability of certain strategies. All methods have been tested with both\nsimulated trajectories and real data from the Polish stock market.\n"
    },
    {
        "paper_id": 1904.10523,
        "authors": "Shuaiqiang Liu, Anastasia Borovykh, Lech A. Grzelak and Cornelis W.\n  Oosterlee",
        "title": "A neural network-based framework for financial model calibration",
        "comments": "34 pages, 9 figures, 11 tables",
        "journal-ref": "J.Math.Industry 9, 9 (2019)",
        "doi": "10.1186/s13362-019-0066-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A data-driven approach called CaNN (Calibration Neural Network) is proposed\nto calibrate financial asset price models using an Artificial Neural Network\n(ANN). Determining optimal values of the model parameters is formulated as\ntraining hidden neurons within a machine learning framework, based on available\nfinancial option prices. The framework consists of two parts: a forward pass in\nwhich we train the weights of the ANN off-line, valuing options under many\ndifferent asset model parameter settings; and a backward pass, in which we\nevaluate the trained ANN-solver on-line, aiming to find the weights of the\nneurons in the input layer. The rapid on-line learning of implied volatility by\nANNs, in combination with the use of an adapted parallel global optimization\nmethod, tackles the computation bottleneck and provides a fast and reliable\ntechnique for calibrating model parameters while avoiding, as much as possible,\ngetting stuck in local minima. Numerical experiments confirm that this\nmachine-learning framework can be employed to calibrate parameters of\nhigh-dimensional stochastic volatility models efficiently and accurately.\n"
    },
    {
        "paper_id": 1904.10554,
        "authors": "Philippe Casgrain, Brian Ning, Sebastian Jaimungal",
        "title": "Deep Q-Learning for Nash Equilibria: Nash-DQN",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Model-free learning for multi-agent stochastic games is an active area of\nresearch. Existing reinforcement learning algorithms, however, are often\nrestricted to zero-sum games, and are applicable only in small state-action\nspaces or other simplified settings. Here, we develop a new data efficient\nDeep-Q-learning methodology for model-free learning of Nash equilibria for\ngeneral-sum stochastic games. The algorithm uses a local linear-quadratic\nexpansion of the stochastic game, which leads to analytically solvable optimal\nactions. The expansion is parametrized by deep neural networks to give it\nsufficient flexibility to learn the environment without the need to experience\nall state-action pairs. We study symmetry properties of the algorithm stemming\nfrom label-invariant stochastic games and as a proof of concept, apply our\nalgorithm to learning optimal trading strategies in competitive electronic\nmarkets.\n"
    },
    {
        "paper_id": 1904.10625,
        "authors": "Mohammad Bahrami, Narges Chinichian, Ali Hosseiny, Gholamreza Jafari,\n  and Marcel Ausloos",
        "title": "Optimization of the post-crisis recovery plans in scale-free networks",
        "comments": "to be appeared in Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.123203",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  General Motors or a local business, which one is better to be stimulated in\npost-crisis recessions, where government stimulation is meant to overcome\nrecessions? Due to the budget constraints, it is quite relevant to ask how one\ncan increase the chance of economic recovery. One of the key elements to answer\nthis question is to understand metastable features of the economic networks.\nIsing model has been suggested for studying such features in the literature. In\nthe homogenous networks one needs at least a minimum activation, forcing an\nIsing network to switch its local equilibria, where such minimum is independent\nof the nodes characteristics. In the scale free networks however, when one aims\nto push the network to switch its vacuum, she faces the question of which nodes\nare better to be stimulated to minimize the cost. In the paper it has been\nshown that stimulation of the high degree nodes costs less in general. Despite\nregular networks, in the scale free networks, the stimulation cost depends on\nthe networks features such as assortativity. Though we have utilized the Ising\nmodel to tackle a problem in economics, our analysis shed lights on many other\nproblems concerning stimulations of socio-economic systems.\n"
    },
    {
        "paper_id": 1904.10744,
        "authors": "Dominik Grafenhofer, Wolfgang Kuhle",
        "title": "Observing Actions in Bayesian Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study Bayesian coordination games where agents receive noisy private\ninformation over the game's payoff structure, and over each others' actions. If\nprivate information over actions is precise, we find that agents can coordinate\non multiple equilibria. If private information over actions is of low quality,\nequilibrium uniqueness obtains like in a standard global games setting. The\ncurrent model, with its flexible information structure, can thus be used to\nstudy phenomena such as bank-runs, currency crises, recessions, riots, and\nrevolutions, where agents rely on information over each others' actions.\n"
    },
    {
        "paper_id": 1904.11032,
        "authors": "Xiaochuan Deng, Fei Sun",
        "title": "Regulator-based risk statistics with scenario analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As regulators pay more attentions to losses rather than gains, we are able to\nderive a new class of risk statistics, named regulator-based risk statistics\nwith scenario analysis in this paper. This new class of risk statistics can be\nconsidered as a kind of risk extension of risk statistics introduced by Kou et\nal. \\cite{11}, and also data-based versions of loss-based risk measures\nintroduced by Cont et al. \\cite{5} and Sun et al. \\cite{12}.\n"
    },
    {
        "paper_id": 1904.11145,
        "authors": "Ali Habibnia (1) and Esfandiar Maasoumi (2) ((1) Virginia Tech, (2)\n  Emory University)",
        "title": "Forecasting in Big Data Environments: an Adaptable and Automated\n  Shrinkage Estimation of Neural Networks (AAShNet)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers improved forecasting in possibly nonlinear dynamic\nsettings, with high-dimension predictors (\"big data\" environments). To overcome\nthe curse of dimensionality and manage data and model complexity, we examine\nshrinkage estimation of a back-propagation algorithm of a deep neural net with\nskip-layer connections. We expressly include both linear and nonlinear\ncomponents. This is a high-dimensional learning approach including both\nsparsity L1 and smoothness L2 penalties, allowing high-dimensionality and\nnonlinearity to be accommodated in one step. This approach selects significant\npredictors as well as the topology of the neural network. We estimate optimal\nvalues of shrinkage hyperparameters by incorporating a gradient-based\noptimization technique resulting in robust predictions with improved\nreproducibility. The latter has been an issue in some approaches. This is\nstatistically interpretable and unravels some network structure, commonly left\nto a black box. An additional advantage is that the nonlinear part tends to get\npruned if the underlying process is linear. In an application to forecasting\nequity returns, the proposed approach captures nonlinear dynamics between\nequities to enhance forecast performance. It offers an appreciable improvement\nover current univariate and multivariate models by RMSE and actual portfolio\nperformance.\n"
    },
    {
        "paper_id": 1904.11252,
        "authors": "Laurence Carassus and Miklos Rasonyi",
        "title": "Risk-neutral pricing for APT",
        "comments": null,
        "journal-ref": "Journal of Optimization Theory and Applications 186 2020 248 263",
        "doi": "10.1007/s10957-020-01699-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider infinite dimensional optimization problems motivated by the\nfinancial model called Arbitrage Pricing Theory. Using probabilistic and\nfunctional analytic tools, we provide a dual characterization of the\nsuper-replication cost. Then, we show the existence of optimal strategies for\ninvestors maximizing their expected utility and the convergence of their\nreservation prices to the super-replication cost as their risk-aversion tends\nto infinity.\n"
    },
    {
        "paper_id": 1904.11376,
        "authors": "Rogelio A. Mancisidor, Michael Kampffmeyer, Kjersti Aas, Robert\n  Jenssen",
        "title": "Deep Generative Models for Reject Inference in Credit Scoring",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit scoring models based on accepted applications may be biased and their\nconsequences can have a statistical and economic impact. Reject inference is\nthe process of attempting to infer the creditworthiness status of the rejected\napplications. In this research, we use deep generative models to develop two\nnew semi-supervised Bayesian models for reject inference in credit scoring, in\nwhich we model the data generating process to be dependent on a Gaussian\nmixture. The goal is to improve the classification accuracy in credit scoring\nmodels by adding reject applications. Our proposed models infer the unknown\ncreditworthiness of the rejected applications by exact enumeration of the two\npossible outcomes of the loan (default or non-default). The efficient\nstochastic gradient optimization technique used in deep generative models makes\nour models suitable for large data sets. Finally, the experiments in this\nresearch show that our proposed models perform better than classical and\nalternative machine learning models for reject inference in credit scoring.\n"
    },
    {
        "paper_id": 1904.11377,
        "authors": "Pingyu Jiang, Pulin Li",
        "title": "Shared factory: a new production node for social manufacturing in the\n  context of sharing economy",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1177/0954405419863220",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Manufacturing industry is heading towards socialization, interconnection, and\nplatformization. Motivated by the infiltration of sharing economy usage in\nmanufacturing, this paper addresses a new factory model -- shared factory --\nand provides a theoretical architecture and some actual cases for manufacturing\nsharing. Concepts related to three kinds of shared factories which deal\nrespectively with sharing production-orders, manufacturing-resources and\nmanufacturing-capabilities, are defined accordingly. These three kinds of\nshared factory modes can be used for building correspondent sharing\nmanufacturing ecosystems. On the basis of sharing economic analysis, we\nidentify feasible key enabled technologies for configuring and running a shared\nfactory. At the same time, opportunities and challenges of enabling the shared\nfactory are also analyzed in detail. In fact, shared factory, as a new\nproduction node, enhances the sharing nature of social manufacturing paradigm,\nfits the needs of light assets and gives us a new chance to use socialized\nmanufacturing resources. It can be drawn that implementing a shared factory\nwould reach a win-win way through production value-added transformation and\nsocial innovation.\n"
    },
    {
        "paper_id": 1904.11392,
        "authors": "Haoran Wang, Xun Yu Zhou",
        "title": "Continuous-Time Mean-Variance Portfolio Selection: A Reinforcement\n  Learning Framework",
        "comments": "39 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We approach the continuous-time mean-variance (MV) portfolio selection with\nreinforcement learning (RL). The problem is to achieve the best tradeoff\nbetween exploration and exploitation, and is formulated as an\nentropy-regularized, relaxed stochastic control problem. We prove that the\noptimal feedback policy for this problem must be Gaussian, with time-decaying\nvariance. We then establish connections between the entropy-regularized MV and\nthe classical MV, including the solvability equivalence and the convergence as\nexploration weighting parameter decays to zero. Finally, we prove a policy\nimprovement theorem, based on which we devise an implementable RL algorithm. We\nfind that our algorithm outperforms both an adaptive control based method and a\ndeep neural networks based algorithm by a large margin in our simulations.\n"
    },
    {
        "paper_id": 1904.11496,
        "authors": "Marc Ringel and Roufaida Laidi and Djamel Djenouri",
        "title": "Multiple Benefits through Smart Home Energy Management Solutions -- A\n  Simulation-Based Case Study of a Single-Family House in Algeria and Germany",
        "comments": null,
        "journal-ref": "Energies 2019, 12(8), 1537",
        "doi": "10.3390/en12081537",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  From both global and local perspectives, there are strong reasons to promote\nenergy efficiency. These reasons have prompted leaders in the European Union\n(EU) and countries of the Middle East and North Africa (MENA) to adopt policies\nto move their citizenry toward more efficient energy consumption. Energy\nefficiency policy is typically framed at the national, or transnational level.\nPolicy makers then aim to incentivize microeconomic actors to align their\ndecisions with macroeconomic policy. We suggest another path towards greater\nenergy efficiency: Highlighting individual benefits at microeconomic level. By\nsimulating lighting, heating and cooling operations in a model single-family\nhome equipped with modest automation, we show that individual actors can be led\nto pursue energy efficiency out of enlightened self-interest. We apply\nsimple-to-use, easily, scalable impact indicators that can be made available to\nhomeowners and serve as intrinsic economic, environmental and social motivators\nfor pursuing energy efficiency. The indicators reveal tangible homeowner\nbenefits realizable under both the market-based pricing structure for energy in\nGermany and the state-subsidized pricing structure in Algeria. Benefits accrue\nunder both the continental climate regime of Germany and the Mediterranean\nregime of Algeria, notably in the case that cooling energy needs are\nconsidered. Our findings show that smart home technology provides an attractive\npath for advancing energy efficiency goals. The indicators we assemble can help\npolicy makers both to promote tangible benefits of energy efficiency to\nindividual homeowners, and to identify those investments of public funds that\nbest support individual pursuit of national and transnational energy goals.\n"
    },
    {
        "paper_id": 1904.11565,
        "authors": "Simone Farinelli and Hideyuki Takada",
        "title": "The Black-Scholes Equation in Presence of Arbitrage",
        "comments": "The assumptions of Proposition 23 were corrected after Claudio\n  Fontana provided us with a counterexample for the previous version of this\n  proposition. arXiv admin note: substantial text overlap with\n  arXiv:1509.03264, arXiv:1906.07164, arXiv:1406.6805, arXiv:0910.1671",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply Geometric Arbitrage Theory to obtain results in Mathematical\nFinance, which do not need stochastic differential geometry in their\nformulation. First, for a generic market dynamics given by a multidimensional\nIt\\^o's process we specify and prove the equivalence between (NFLVR) and\nexpected utility maximization. As a by-product we provide a geometric\ncharacterization of the (NUPBR) condition given by the zero curvature (ZC)\ncondition. Finally, we extend the Black-Scholes PDE to markets allowing\narbitrage.\n"
    },
    {
        "paper_id": 1904.11604,
        "authors": "Allison Koenecke",
        "title": "A Game Theoretic Setting of Capitation Versus Fee-For-Service Payment\n  Systems",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0223672",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We aim to determine whether a game-theoretic model between an insurer and a\nhealthcare practice yields a predictive equilibrium that incentivizes either\nplayer to deviate from a fee-for-service to capitation payment system. Using\nUnited States data from various primary care surveys, we find that non-extreme\nequilibria (i.e., shares of patients, or shares of patient visits, seen under a\nfee-for-service payment system) can be derived from a Stackelberg game if\ninsurers award a non-linear bonus to practices based on performance. Overall,\nboth insurers and practices can be incentivized to embrace capitation payments\nsomewhat, but potentially at the expense of practice performance.\n"
    },
    {
        "paper_id": 1904.11911,
        "authors": "M\\'onica B. Carvajal Pinto and Kees van Schaik",
        "title": "Optimally stopping at a given distance from the ultimate supremum of a\n  spectrally negative L\\'evy process",
        "comments": "Minor revision and typo's",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal prediction problem of stopping a spectrally negative\nL\\'evy process as close as possible to a given distance $b \\geq 0$ from its\nultimate supremum, under a squared error penalty function. Under some mild\nconditions, the solution is fully and explicitly characterised in terms of\nscale functions. We find that the solution has an interesting non-trivial\nstructure: if $b$ is larger than a certain threshold then it is optimal to stop\nas soon as the difference between the running supremum and the position of the\nprocess exceeds a certain level (less than $b$), while if $b$ is smaller than\nthis threshold then it is optimal to stop immediately (independent of the\nrunning supremum and position of the process). We also present some examples.\n"
    },
    {
        "paper_id": 1904.12051,
        "authors": "Alok Raj, J Ajith Kumar, Prateek Bansal",
        "title": "A Multicriteria Decision Making Approach to Study the Barriers to the\n  Adoption of Autonomous Vehicles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The automation technology is emerging, but the adoption rate of autonomous\nvehicles (AV) will largely depend upon how policymakers and the government\naddress various challenges such as public acceptance and infrastructure\ndevelopment. This study proposes a five-step method to understand these\nbarriers to AV adoption. First, based on a literature review followed by\ndiscussions with experts, ten barriers are identified. Second, the opinions of\neighteen experts from industry and academia regarding inter-relations between\nthese barriers are recorded. Third, a multicriteria decision making (MCDM)\ntechnique, the grey-based Decision-making Trial and Evaluation Laboratory\n(Grey-DEMATEL), is applied to characterize the structure of relationships\nbetween the barriers. Fourth, robustness of the results is tested using\nsensitivity analysis. Fifth, the key results are depicted in a causal loop\ndiagram (CLD), a systems thinking approach, to comprehend cause-and-effect\nrelationships between the barriers. The results indicate that the lack of\ncustomer acceptance (LCA) is the most prominent barrier, the one which should\nbe addressed at the highest priority. The CLD suggests that LCA can be rather\nmitigated by addressing two other prominent, yet more tangible, barriers --\nlack of industry standards and the absence of regulations and certifications.\nThe study's overarching contribution thus lies in bringing to fore multiple\nbarriers to AV adoption and their potential influences on each other. Moreover,\nthe insights from this study can help associations related to AVs prioritize\ntheir endeavors to expedite AV adoption. From the methodological perspective,\nthis is the first study in transportation literature that integrates\nGrey-DEMATEL with systems thinking.\n"
    },
    {
        "paper_id": 1904.12113,
        "authors": "Ingo Hoffmann and Christoph J. B\\\"orner",
        "title": "Tail models and the statistical limit of accuracy in risk assessment",
        "comments": "3 figures",
        "journal-ref": "Journal of Risk Finance 2020",
        "doi": "10.1108/JRF-11-2019-0217",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In risk management, tail risks are of crucial importance. The assessment of\nrisks should be carried out in accordance with the regulatory authority's\nrequirement at high quantiles. In general, the underlying distribution function\nis unknown, the database is sparse, and therefore special tail models are used.\nVery often, the generalized Pareto distribution is employed as a basic model,\nand its parameters are determined with data from the tail area. With the\ndetermined tail model, statisticians then calculate the required high\nquantiles. In this context, we consider the possible accuracy of the\ncalculation of the quantiles and determine the finite sample distribution\nfunction of the quantile estimator, depending on the confidence level and the\nparameters of the tail model, and then calculate the finite sample bias and the\nfinite sample variance of the quantile estimator. Finally, we present an impact\nanalysis on the quantiles of an unknown distribution function.\n"
    },
    {
        "paper_id": 1904.12134,
        "authors": "Otello Ardovino, Jacopo Arpetti, Marco Delmastro",
        "title": "Regulating AI: do we need new tools?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Artificial Intelligence paradigm (hereinafter referred to as \"AI\") builds\non the analysis of data able, among other things, to snap pictures of the\nindividuals' behaviors and preferences. Such data represent the most valuable\ncurrency in the digital ecosystem, where their value derives from their being a\nfundamental asset in order to train machines with a view to developing AI\napplications. In this environment, online providers attract users by offering\nthem services for free and getting in exchange data generated right through the\nusage of such services. This swap, characterized by an implicit nature,\nconstitutes the focus of the present paper, in the light of the disequilibria,\nas well as market failures, that it may bring about. We use mobile apps and the\nrelated permission system as an ideal environment to explore, via econometric\ntools, those issues. The results, stemming from a dataset of over one million\nobservations, show that both buyers and sellers are aware that access to\ndigital services implicitly implies an exchange of data, although this does not\nhave a considerable impact neither on the level of downloads (demand), nor on\nthe level of the prices (supply). In other words, the implicit nature of this\nexchange does not allow market indicators to work efficiently. We conclude that\ncurrent policies (e.g. transparency rules) may be inherently biased and we put\nforward suggestions for a new approach.\n"
    },
    {
        "paper_id": 1904.1226,
        "authors": "Takuji Arai",
        "title": "Pricing and hedging of VIX options for Barndorff-Nielsen and Shephard\n  models",
        "comments": "20 pages, 4 fugures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The VIX call options for the Barndorff-Nielsen and Shephard models will be\ndiscussed. Derivatives written on the VIX, which is the most popular volatility\nmeasurement, have been traded actively very much. In this paper, we give\nrepresentations of the VIX call option price for the Barndorff-Nielsen and\nShephard models: non-Gaussian Ornstein--Uhlenbeck type stochastic volatility\nmodels. Moreover, we provide representations of the locally risk-minimizing\nstrategy constructed by a combination of the underlying riskless and risky\nassets. Remark that the representations obtained in this paper are efficient to\ndevelop a numerical method using the fast Fourier transform. Thus, numerical\nexperiments will be implemented in the last section of this paper.\n"
    },
    {
        "paper_id": 1904.12346,
        "authors": "Tetsuya Takaishi",
        "title": "Rough volatility of Bitcoin",
        "comments": "12 pages, 8 figures",
        "journal-ref": "Finance Research Letters 32 (2020) 101379",
        "doi": "10.1016/j.frl.2019.101379",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent studies have found that the log-volatility of asset returns exhibit\nroughness. This study investigates roughness or the anti-persistence of Bitcoin\nvolatility. Using the multifractal detrended fluctuation analysis, we obtain\nthe generalized Hurst exponent of the log-volatility increments and find that\nthe generalized Hurst exponent is less than $1/2$, which indicates\nlog-volatility increments that are rough. Furthermore, we find that the\ngeneralized Hurst exponent is not constant. This observation indicates that the\nlog-volatility has multifractal property. Using shuffled time series of the\nlog-volatility increments, we infer that the source of multifractality partly\ncomes from the distributional property.\n"
    },
    {
        "paper_id": 1904.12397,
        "authors": "Tembo Nakamoto, Abhijit Chakraborty, Yuichi Ikeda",
        "title": "Identification of Key Companies for International Profit Shifting in the\n  Global Ownership Network",
        "comments": null,
        "journal-ref": "Applied Network Science volume 4, Article number: 58 (2019)",
        "doi": "10.1007/s41109-019-0158-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the global economy, the intermediate companies owned by multinational\ncorporations are becoming an important policy issue as they are likely to cause\ninternational profit shifting and diversion of foreign direct investments. The\npurpose of this analysis is to call the intermediate companies with high risk\nof international profit shifting as key firms and to identifying and clarify\nthem. For this aim, we propose a model that focuses on each affiliate's\nposition on the ownership structure of each multinational corporation. Based on\nthe information contained in the Orbis database, we constructed the Global\nOwnership Network, reflecting the relationship that can give significant\ninfluence to a firm, and analyzed for large multinational corporations listed\nin Fortune Global 500. In this analysis, first, we confirmed the validity of\nthis model by identifying affiliates playing an important role in international\ntax avoidance at a certain degree. Secondly, intermediate companies are mainly\nfound in the Netherlands and the United Kingdom, etc., and tended to be located\nin jurisdictions favorable to treaty shopping. And it was found that such key\nfirms are concentrated on the IN component of the bow-tie structure that the\ngiant weakly connected component of the Global Ownership Network consist of.\nTherefore, it clarifies that the key firms are geographically located in\nspecific jurisdictions, and concentrates on specific components in the Global\nOwnership Network. The location of key firms are related with the ease of\ntreaty shopping, and there is a difference in the jurisdiction where key firms\nare located depending on the location of the multinational corporations.\n"
    },
    {
        "paper_id": 1904.12442,
        "authors": "Bingyan Han and Hoi Ying Wong",
        "title": "Mean-variance portfolio selection under Volterra Heston model",
        "comments": "Final version, 22 pages, 5 figures, to appear in Applied Mathematics\n  & Optimization",
        "journal-ref": null,
        "doi": "10.1007/s00245-020-09658-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by empirical evidence for rough volatility models, this paper\ninvestigates continuous-time mean-variance (MV) portfolio selection under the\nVolterra Heston model. Due to the non-Markovian and non-semimartingale nature\nof the model, classic stochastic optimal control frameworks are not directly\napplicable to the associated optimization problem. By constructing an auxiliary\nstochastic process, we obtain the optimal investment strategy, which depends on\nthe solution to a Riccati-Volterra equation. The MV efficient frontier is shown\nto maintain a quadratic curve. Numerical studies show that both roughness and\nvolatility of volatility materially affect the optimal strategy.\n"
    },
    {
        "paper_id": 1904.12526,
        "authors": "Paolo Di Caro, Giuseppe Pernagallo, Antonino Damiano Rossello and\n  Benedetto Torrisi",
        "title": "Empirical facts characterizing banking crises: an analysis via binary\n  time series",
        "comments": "14 pages, 2 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Various works have already showed that common shocks and cross-country\nfinancial linkages caused the banking systems of several countries to be highly\ninterconnected with the result that during bad times, banking crises may arise\nsimultaneously in different countries. Our aim is to provide further evidence\non the topic using a dataset made by dichotomous banking crises time series for\n66 countries from 1800 to 2014. Via the use of heatmap matrices we show that\nseveral countries exhibit pairwise correlation, which means that banking crises\ntend to occur in the same year. Clustering analysis suggests that developed\ncountries (for the most European ones) are highly similar in terms of the path\nof events. An analysis of the events that followed the Great Depression and the\nGreat Recession shows that after the crisis of 2008, banking crises tend to\ncharacterize countries tied by financial links whereas before 2008 contagion\nseems to affect countries in the same geographical area. Clustering analysis\nshows also that after financial liberalization, crises affected countries with\nsimilar economic structures and growth. Further researches should enlighten the\norigin of these linkages investigating how the process of contagion eventually\nhappens.\n"
    },
    {
        "paper_id": 1904.12614,
        "authors": "Dorje C Brody",
        "title": "Modelling election dynamics and the impact of disinformation",
        "comments": "20 pages, 5 figures",
        "journal-ref": "Information Geometry, 2019",
        "doi": "10.1007/s41884-019-00021-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex dynamical systems driven by the unravelling of information can be\nmodelled effectively by treating the underlying flow of information as the\nmodel input. Complicated dynamical behaviour of the system is then derived as\nan output. Such an information-based approach is in sharp contrast to the\nconventional mathematical modelling of information-driven systems whereby one\nattempts to come up with essentially {\\it ad hoc} models for the outputs. Here,\ndynamics of electoral competition is modelled by the specification of the flow\nof information relevant to election. The seemingly random evolution of the\nelection poll statistics are then derived as model outputs, which in turn are\nused to study election prediction, impact of disinformation, and the optimal\nstrategy for information management in an election campaign.\n"
    },
    {
        "paper_id": 1904.12834,
        "authors": "Yu Zheng and Yongxin Yang and Bowei Chen",
        "title": "Incorporating prior financial domain knowledge into neural networks for\n  implied volatility surface prediction",
        "comments": "8 pages, SIGKDD 2021",
        "journal-ref": null,
        "doi": "10.1145/3447548.3467115",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a novel neural network model for predicting implied\nvolatility surface. Prior financial domain knowledge is taken into account. A\nnew activation function that incorporates volatility smile is proposed, which\nis used for the hidden nodes that process the underlying asset price. In\naddition, financial conditions, such as the absence of arbitrage, the\nboundaries and the asymptotic slope, are embedded into the loss function. This\nis one of the very first studies which discuss a methodological framework that\nincorporates prior financial domain knowledge into neural network architecture\ndesign and model training. The proposed model outperforms the benchmarked\nmodels with the option data on the S&P 500 index over 20 years. More\nimportantly, the domain knowledge is satisfied empirically, showing the model\nis consistent with the existing financial theories and conditions related to\nimplied volatility surface.\n"
    },
    {
        "paper_id": 1904.12887,
        "authors": "Allison Koenecke and Amita Gajewar",
        "title": "Curriculum Learning in Deep Neural Networks for Financial Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-030-37720-5_2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For any financial organization, computing accurate quarterly forecasts for\nvarious products is one of the most critical operations. As the granularity at\nwhich forecasts are needed increases, traditional statistical time series\nmodels may not scale well. We apply deep neural networks in the forecasting\ndomain by experimenting with techniques from Natural Language Processing\n(Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as\nincorporating transfer learning. A novel contribution of this paper is the\napplication of curriculum learning to neural network models built for time\nseries forecasting. We illustrate the performance of our models using\nMicrosoft's revenue data corresponding to Enterprise, and Small, Medium &\nCorporate products, spanning approximately 60 regions across the globe for 8\ndifferent business segments, and totaling in the order of tens of billions of\nUSD. We compare our models' performance to the ensemble model of traditional\nstatistics and machine learning techniques currently used by Microsoft Finance.\nWith this in-production model as a baseline, our experiments yield an\napproximately 30% improvement in overall accuracy on test data. We find that\nour curriculum learning LSTM-based model performs best, showing that it is\nreasonable to implement our proposed methods without overfitting on\nmedium-sized data.\n"
    },
    {
        "paper_id": 1904.13064,
        "authors": "Vygintas Gontis, Aleksejus Kononovicius",
        "title": "Bessel-like birth-death process",
        "comments": "11 pages, 3 figures",
        "journal-ref": "Physica A 540: 123119 (2020)",
        "doi": "10.1016/j.physa.2019.123119",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider models of the population or opinion dynamics which result in the\nnon-linear stochastic differential equations (SDEs) exhibiting the spurious\nlong-range memory. In this context, the correspondence between the description\nof the birth-death processes as the continuous-time Markov chains and the\ncontinuous SDEs is of high importance for the alternatives of modeling. We\npropose and generalize the Bessel-like birth-death process having clear\nrepresentation by the SDEs. The new process helps to integrate the alternatives\nof description and to derive the equations for the probability density function\n(PDF) of the burst and inter-burst duration of the proposed continuous time\nbirth-death processes.\n"
    },
    {
        "paper_id": 1904.13257,
        "authors": "Alessandro Calvia and Emanuela Rosazza Gianin",
        "title": "Risk measures and progressive enlargement of filtration: a BSDE approach",
        "comments": "30 pages, 2 figures",
        "journal-ref": "SIAM J. Financial Math., 11 (2020), pp. 815-848",
        "doi": "10.1137/19M1259134",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider dynamic risk measures induced by Backward Stochastic Differential\nEquations (BSDEs) in enlargement of filtration setting. On a fixed probability\nspace, we are given a standard Brownian motion and a pair of random variables\n$(\\tau, \\zeta) \\in (0,+\\infty) \\times E$, with $E \\subset \\mathbb{R}^m$, that\nenlarge the reference filtration, i.e., the one generated by the Brownian\nmotion. These random variables can be interpreted financially as a default time\nand an associated mark. After introducing a BSDE driven by the Brownian motion\nand the random measure associated to $(\\tau, \\zeta)$, we define the dynamic\nrisk measure $(\\rho_t)_{t \\in [0,T]}$, for a fixed time $T > 0$, induced by its\nsolution. We prove that $(\\rho_t)_{t \\in [0,T]}$ can be decomposed in a pair of\nrisk measures, acting before and after $\\tau$ and we characterize its\nproperties giving suitable assumptions on the driver of the BSDE. Furthermore,\nwe prove an inequality satisfied by the penalty term associated to the robust\nrepresentation of $(\\rho_t)_{t \\in [0,T]}$ and we discuss the dynamic entropic\nrisk measure case, providing examples where it is possible to write explicitly\nits decomposition and simulate it numerically.\n"
    },
    {
        "paper_id": 1904.13329,
        "authors": "John A. Clithero and Jae Joon Lee and Joshua Tasoff",
        "title": "Supervised Machine Learning for Eliciting Individual Demand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Direct elicitation, guided by theory, is the standard method for eliciting\nlatent preferences. The canonical direct-elicitation approach for measuring\nindividuals' valuations for goods is the Becker-DeGroot-Marschak procedure,\nwhich generates willingness-to-pay (WTP) values that are imprecise and\nsystematically biased by understating valuations. We show that enhancing\nelicited WTP values with supervised machine learning (SML) can substantially\nimprove estimates of peoples' out-of-sample purchase behavior. Furthermore,\nswapping WTP data with choice data generated from a simple task,\ntwo-alternative forced choice, leads to comparable performance. Combining all\nthe data with the best-performing SML methods yields large improvements in\npredicting out-of-sample purchases. We quantify the benefit of using various\nSML methods in conjunction with using different types of data. Our results\nsuggest that prices set by SML would increase revenue by 28% over using the\nstated WTP, with the same data.\n"
    },
    {
        "paper_id": 1905.00107,
        "authors": "Alessandro Balata and Michael Ludkovski and Aditya Maheshwari and Jan\n  Palczewski",
        "title": "Statistical Learning for Probability-Constrained Stochastic Optimal\n  Control",
        "comments": "Updated literature review and additional discussion on results",
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2020.08.041",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate Monte Carlo based algorithms for solving stochastic control\nproblems with probabilistic constraints. Our motivation comes from microgrid\nmanagement, where the controller tries to optimally dispatch a diesel generator\nwhile maintaining low probability of blackouts. The key question we investigate\nare empirical simulation procedures for learning the admissible control set\nthat is specified implicitly through a probability constraint on the system\nstate. We propose a variety of relevant statistical tools including logistic\nregression, Gaussian process regression, quantile regression and support vector\nmachines, which we then incorporate into an overall Regression Monte Carlo\n(RMC) framework for approximate dynamic programming. Our results indicate that\nusing logistic or Gaussian process regression to estimate the admissibility\nprobability outperforms the other options. Our algorithms offer an efficient\nand reliable extension of RMC to probability-constrained control. We illustrate\nour findings with two case studies for the microgrid problem.\n"
    },
    {
        "paper_id": 1905.00238,
        "authors": "Kathrin Glau, Ricardo Pachon and Christian P\\\"otz",
        "title": "Fast Calculation of Credit Exposures for Barrier and Bermudan options\n  using Chebyshev interpolation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new method to calculate the credit exposure of Bermudan,\ndiscretely monitored barrier and European options. Core of the approach is the\napplication of the dynamic Chebyshev method of Glau et al. (2019). The dynamic\nChebyshev method delivers a closed form approximation of the option prices\nalong the paths together with the options' delta and gamma. Key advantage is\nthe polynomial structure of the approximation, which allows us a highly\nefficient evaluation of the credit exposures, even for a large number of\nsimulated paths. The approach is highly flexible in the model choice, payoff\nprofiles and asset classes. We compute the exposure profiles for Bermudan and\nbarrier options in three different equity models and compare them to the\nprofiles of European options. The analysis reveals potential shortcomings of\ncommon simplifications in the exposure calculation. The proposed method is\nsufficiently simple and efficient to avoid such risk-bearing simplifications.\n"
    },
    {
        "paper_id": 1905.00486,
        "authors": "Fei Sun, Xiaozhi Fan, Weitao Liu",
        "title": "Set-valued risk statistics with the time value of money",
        "comments": "arXiv admin note: text overlap with arXiv:1904.11032,\n  arXiv:1904.08829",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The time value of money is a critical factor not only in risk analysis, but\nalso in insurance and financial applications. In this paper, we consider a\nspecial class of set-valued risk statistics by introducing the time value of\nmoney. In fact, the risk statistics established by this method is closer to\nfinancial reality than traditional ones. Moreover, this new risk statistic can\nbe uesd for the quantification of portfolio risk. By further developing the\nproperties related to these risk statistics, we are able to derive\nrepresentation results for such risk.\n"
    },
    {
        "paper_id": 1905.00545,
        "authors": "Andr\\'es Garc\\'ia Medina and Graciela Gonz\\'alez-Far\\'ias",
        "title": "Determining the number of factors in a forecast model by a random matrix\n  test: cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the number of statistically significant factors in a forecast\nmodel using a random matrices test. The applied forecast model is of the type\nof Reduced Rank Regression (RRR), in particular, we chose a flavor which can be\nseen as the Canonical Correlation Analysis (CCA). As empirical data, we use\ncryptocurrencies at hour frequency, where the variable selection was made by a\ncriterion from information theory. The results are consistent with the usual\nvisual inspection, with the advantage that the subjective element is avoided.\nFurthermore, the computational cost is minimal compared to the cross-validation\napproach.\n"
    },
    {
        "paper_id": 1905.00556,
        "authors": "Velibor V. Mi\\v{s}i\\'c, Georgia Perakis",
        "title": "Data Analytics in Operations Management: A Review",
        "comments": "Forthcoming in Manufacturing & Services Operations Management",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research in operations management has traditionally focused on models for\nunderstanding, mostly at a strategic level, how firms should operate. Spurred\nby the growing availability of data and recent advances in machine learning and\noptimization methodologies, there has been an increasing application of data\nanalytics to problems in operations management. In this paper, we review recent\napplications of data analytics to operations management, in three major areas\n-- supply chain management, revenue management and healthcare operations -- and\nhighlight some exciting directions for the future.\n"
    },
    {
        "paper_id": 1905.00711,
        "authors": "Terry Lyons and Sina Nejad and Imanol Perez Arribas",
        "title": "Nonparametric pricing and hedging of exotic derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the spirit of Arrow-Debreu, we introduce a family of financial derivatives\nthat act as primitive securities in that exotic derivatives can be approximated\nby their linear combinations. We call these financial derivatives signature\npayoffs. We show that signature payoffs can be used to nonparametrically price\nand hedge exotic derivatives in the scenario where one has access to price data\nfor other exotic payoffs. The methodology leads to a computationally tractable\nand accurate algorithm for pricing and hedging using market prices of a basket\nof exotic derivatives that has been tested on real and simulated market prices,\nobtaining good results.\n"
    },
    {
        "paper_id": 1905.00728,
        "authors": "Jasdeep Kalsi and Terry Lyons and Imanol Perez Arribas",
        "title": "Optimal execution with rough path signatures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method for obtaining approximate solutions to the problem of\noptimal execution, based on a signature method. The framework is general, only\nrequiring that the price process is a geometric rough path and the price impact\nfunction is a continuous function of the trading speed. Following an\napproximation of the optimisation problem, we are able to calculate an optimal\nsolution for the trading speed in the space of linear functions on a truncation\nof the signature of the price process. We provide strong numerical evidence\nillustrating the accuracy and flexibility of the approach. Our numerical\ninvestigation both examines cases where exact solutions are known,\ndemonstrating that the method accurately approximates these solutions, and\nmodels where exact solutions are not known. In the latter case, we obtain\nfavourable comparisons with standard execution strategies.\n"
    },
    {
        "paper_id": 1905.01018,
        "authors": "Lyudmyla Kirichenko and Vitalii Bulakh and Tamara Radivilova",
        "title": "Fractal Time Series Analysis of Social Network Activities",
        "comments": "4",
        "journal-ref": "2017 4th International Scientific-Practical Conference Problems of\n  Infocommunications. Science and Technology (PIC S&T), Kharkov, Ukraine, 2017,\n  pp. 456-459",
        "doi": "10.1109/INFOCOMMST.2017.8246438",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the work, a comparative correlation and fractal analysis of time series of\nBitcoin crypto currency rate and community activities in social networks\nassociated with Bitcoin was conducted. A significant correlation between the\nBitcoin rate and the community activities was detected. Time series fractal\nanalysis indicated the presence of self-similar and multifractal properties.\nThe results of researches showed that the series having a strong correlation\ndependence have a similar multifractal structure.\n"
    },
    {
        "paper_id": 1905.01099,
        "authors": "M.C. Calvo-Garrido, S. Diop, A. Pascucci, C. V\\'azquez",
        "title": "PDE models for the valuation of a non callable defaultable coupon bond\n  under an extended JDCEV model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a two-factor model for the valuation of a non callable\ndefaultable bond which pays coupons at certain given dates. The model under\nconsideration is the Jump to Default Constant Elasticity of Variance (JDCEV)\nmodel. The JDCEV model is an improvement of the reduced form approach, which\nunifies credit and equity models into a single framework allowing for\nstochastic and possible negative interest rates. From the mathematical point of\nview, the valuation involves two partial differential equation (PDE) problems\nfor each coupon. First, we obtain the existence of solution for these PDE\nproblems. In order to solve them, we propose appropriate numerical schemes\nbased on a Crank-Nicolson semi-Lagrangian method for time discretization\ncombined with bi-quadratic Lagrange finite elements for space discretization.\nOnce the numerical solutions of the PDEs are obtained, a post-processing\nprocedure is carried out in order to achieve the value of the bond. This\npost-processing includes the computation of an integral term which is\napproximated by using the composite trapezoidal rule. Finally, we present some\nnumerical results for real market bonds issued by different firms in order to\nillustrate the proper behaviour of the numerical schemes. Moreover, we obtain\nan agreement between the numerical results from the PDE approach and those ones\nobtained by applying a Monte Carlo technique and an asymptotic approximation\nmethod.\n"
    },
    {
        "paper_id": 1905.01327,
        "authors": "Ilai Bistritz, Nasimeh Heydaribeni and Achilleas Anastasopoulos",
        "title": "Do Informational Cascades Happen with Non-myopic Agents?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an environment where players need to decide whether to buy a\ncertain product (or adopt a technology) or not. The product is either good or\nbad, but its true value is unknown to the players. Instead, each player has her\nown private information on its quality. Each player can observe the previous\nactions of other players and estimate the quality of the product. A classic\nresult in the literature shows that in similar settings informational cascades\noccur where learning stops for the whole network and players repeat the actions\nof their predecessors. In contrast to this literature, in this work, players\nget more than one opportunity to act. In each turn, a player is chosen\nuniformly at random from all players and can decide to buy the product and\nleave the market or wait. Her utility is the total expected discounted reward,\nand thus myopic strategies may not constitute equilibria. We provide a\ncharacterization of perfect Bayesian equilibria (PBE) with forward-looking\nstrategies through a fixed-point equation of dimensionality that grows only\nquadratically with the number of players. Using this tractable fixed-point\nequation, we show the existence of a PBE and characterize PBE with threshold\nstrategies. Based on this characterization we study informational cascades in\ntwo regimes. First, we show that for a discount factor {\\delta} strictly\nsmaller than one, informational cascades happen with high probability as the\nnumber of players N increases. Furthermore, only a small portion of the total\ninformation in the system is revealed before a cascade occurs ...\n"
    },
    {
        "paper_id": 1905.01541,
        "authors": "Jozef Barunik and Pavel Fiser",
        "title": "Co-jumping of Treasury Yield Curve Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the role of co-jumps in the interest rate futures markets. To\ndisentangle continuous part of quadratic covariation from co-jumps, we localize\nthe co-jumps precisely through wavelet coefficients and identify statistically\nsignificant ones. Using high frequency data about U.S. and European yield\ncurves we quantify the effect of co-jumps on their correlation structure.\nEmpirical findings reveal much stronger co-jumping behavior of the U.S. yield\ncurves in comparison to the European one. Further, we connect co-jumping\nbehavior to the monetary policy announcements, and study effect of 103 FOMC and\n119 ECB announcements on the identified co-jumps during the period from January\n2007 to December 2017.\n"
    },
    {
        "paper_id": 1905.01617,
        "authors": "Marcel Ausloos, Ali Eskandary, Parmjit Kaur, Gurjeet Dhesi",
        "title": "Evidence for Gross Domestic Product growth time delay dependence over\n  Foreign Direct Investment. A time-lag dependent correlation study",
        "comments": "76 references, 5 tables,2 figures, 22 pages,prepared for Physica A",
        "journal-ref": "Physica A 527 (2019) 121181",
        "doi": "10.1016/j.physa.2019.121181",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers an often forgotten relationship, the time delay between\na cause and its effect in economies and finance. We treat the case of Foreign\nDirect Investment (FDI) and economic growth, - measured through a country Gross\nDomestic Product (GDP). The pertinent data refers to 43 countries, over\n1970-2015, - for a total of 4278 observations. When countries are grouped\n  according to the Inequality-Adjusted Human Development Index (IHDI), it is\nfound that a time lag dependence effect exists in FDI-GDP correlations.\n  This is established through a time-dependent Pearson 's product-moment\ncorrelation coefficient matrix.\n  Moreover, such a Pearson correlation coefficient is observed to evolve from\npositive\n  to negative values depending on the IHDI, from low to high. It is\n\"politically and policy\n  \"relevant\" that\n  the correlation is statistically significant providing the time lag is less\nthan 3 years. A \"rank-size\" law is demonstrated.\n  It is recommended to reconsider such a time lag effect when discussing\nprevious analyses whence conclusions on international business, and thereafter\non forecasting.\n"
    },
    {
        "paper_id": 1905.01706,
        "authors": "Anastasia Borovykh, Andrea Pascucci, Cornelis W. Oosterlee",
        "title": "Efficient Computation of Various Valuation Adjustments Under Local\n  L\\'evy Models",
        "comments": null,
        "journal-ref": "SIAM Journal on Financial Mathematics 9 (1), 251-273, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Various valuation adjustments, or XVAs, can be written in terms of non-linear\nPIDEs equivalent to FBSDEs. In this paper we develop a Fourier-based method for\nsolving FBSDEs in order to efficiently and accurately price Bermudan\nderivatives, including options and swaptions, with XVA under the flexible\ndynamics of a local L\\'evy model: this framework includes a local volatility\nfunction and a local jump measure. Due to the unavailability of the\ncharacteristic function for such processes, we use an asymptotic approximation\nbased on the adjoint formulation of the problem.\n"
    },
    {
        "paper_id": 1905.0172,
        "authors": "Terry Lyons and Sina Nejad and Imanol Perez Arribas",
        "title": "Numerical method for model-free pricing of exotic derivatives using\n  rough path signatures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We estimate prices of exotic options in a discrete-time model-free setting\nwhen the trader has access to market prices of a rich enough class of exotic\nand vanilla options. This is achieved by estimating an unobservable quantity\ncalled \"implied expected signature\" from such market prices, which are used to\nprice other exotic derivatives. The implied expected signature is an object\nthat characterises the market dynamics.\n"
    },
    {
        "paper_id": 1905.01805,
        "authors": "Eric Bax",
        "title": "Computing a Data Dividend",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quality data is a fundamental contributor to success in statistics and\nmachine learning. If a statistical assessment or machine learning leads to\ndecisions that create value, data contributors may want a share of that value.\nThis paper presents methods to assess the value of individual data samples, and\nof sets of samples, to apportion value among different data contributors. We\nuse Shapley values for individual samples and Owen values for combined samples,\nand show that these values can be computed in polynomial time in spite of their\ndefinitions having numbers of terms that are exponential in the number of\nsamples.\n"
    },
    {
        "paper_id": 1905.01859,
        "authors": "Martin Brown and Tomasz Zastawniak",
        "title": "Fundamental Theorem of Asset Pricing under fixed and proportional\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the lack of arbitrage in a model with both fixed and\nproportional transaction costs is equivalent to the existence of a family of\nabsolutely continuous single-step probability measures, together with an\nadapted process with values between the bid-ask spreads that satisfies the\nmartingale property with respect to each of the measures. This extends Harrison\nand Pliska's classical Fundamental Theorem of Asset Pricing to the case of\ncombined fixed and proportional transaction costs.\n"
    },
    {
        "paper_id": 1905.01894,
        "authors": "Takanori Adachi, Katsushi Nakajima, Yoshihiro Ryu",
        "title": "A Binomial Asset Pricing Model in a Categorical Setting",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adachi and Ryu introduced a category Prob of probability spaces whose objects\nare all probability spaces and whose arrows correspond to measurable functions\nsatisfying an absolutely continuous requirement in [Adachi and Ryu, 2019]. In\nthis paper, we develop a binomial asset pricing model based on Prob. We\nintroduce generalized filtrations with which we can represent situations such\nas some agents forget information at some specific time. We investigate the\nvaluations of financial claims along this type of non-standard filtrations.\n"
    },
    {
        "paper_id": 1905.02092,
        "authors": "Neha Soni, Enakshi Khular Sharma, Narotam Singh, Amita Kapoor",
        "title": "Impact of Artificial Intelligence on Businesses: from Research,\n  Innovation, Market Deployment to Future Shifts in Business Models",
        "comments": "38 pages, 10 figures, 3 tables. A part of this work has been\n  presented in DIGITS 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fast pace of artificial intelligence (AI) and automation is propelling\nstrategists to reshape their business models. This is fostering the integration\nof AI in the business processes but the consequences of this adoption are\nunderexplored and need attention. This paper focuses on the overall impact of\nAI on businesses - from research, innovation, market deployment to future\nshifts in business models. To access this overall impact, we design a\nthree-dimensional research model, based upon the Neo-Schumpeterian economics\nand its three forces viz. innovation, knowledge, and entrepreneurship. The\nfirst dimension deals with research and innovation in AI. In the second\ndimension, we explore the influence of AI on the global market and the\nstrategic objectives of the businesses and finally, the third dimension\nexamines how AI is shaping business contexts. Additionally, the paper explores\nAI implications on actors and its dark sides.\n"
    },
    {
        "paper_id": 1905.0265,
        "authors": "Katia Colaneri and Tiziano De Angelis",
        "title": "A class of recursive optimal stopping problems with applications to\n  stock trading",
        "comments": "36 pages, 2 figures. In this version, we provide a general analysis\n  of a class of recursive optimal stopping problems with both finite-time and\n  infinite-time horizon. We also discuss other applications and provide a\n  comparison with a non-recursive model",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce and solve a class of optimal stopping problems of\nrecursive type. In particular, the stopping payoff depends directly on the\nvalue function of the problem itself. In a multi-dimensional Markovian setting\nwe show that the problem is well posed, in the sense that the value is indeed\nthe unique solution to a fixed point problem in a suitable space of continuous\nfunctions, and an optimal stopping time exists. We then apply our class of\nproblems to a model for stock trading in two different market venues and we\ndetermine the optimal stopping rule in that case.\n"
    },
    {
        "paper_id": 1905.02674,
        "authors": "Alec Biehl, Ying Chen, Karla Sanabria-Veaz, David Uttal, Amanda\n  Stathopoulos",
        "title": "Where does active travel fit within local community narratives of\n  mobility space and place?",
        "comments": null,
        "journal-ref": "Transportation Research Part A: Policy and Practice, Volume 123,\n  2019, Pages 269-287",
        "doi": "10.1016/j.tra.2018.10.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Encouraging sustainable mobility patterns is at the forefront of policymaking\nat all scales of governance as the collective consciousness surrounding climate\nchange continues to expand. Not every community, however, possesses the\nnecessary economic or socio-cultural capital to encourage modal shifts away\nfrom private motorized vehicles towards active modes. The current literature on\n`soft' policy emphasizes the importance of tailoring behavior change campaigns\nto individual or geographic context. Yet, there is a lack of insight and\nappropriate tools to promote active mobility and overcome transport\ndisadvantage from the local community perspective. The current study\ninvestigates the promotion of walking and cycling adoption using a series of\nfocus groups with local residents in two geographic communities, namely\nChicago's (1) Humboldt Park neighborhood and (2) suburb of Evanston. The\nresearch approach combines traditional qualitative discourse analysis with\nquantitative text-mining tools, namely topic modeling and sentiment analysis.\nThe analysis uncovers the local mobility culture, embedded norms and values\nassociated with acceptance of active travel modes in different communities. We\nobserve that underserved populations within diverse communities view active\nmobility simultaneously as a necessity and as a symbol of privilege that is\nsometimes at odds with the local culture. The mixed methods approach to\nanalyzing community member discourses is translated into policy findings that\nare either tailored to local context or broadly applicable to curbing\nautomobile dominance. Overall, residents of both Humboldt Park and Evanston\nenvision a society in which multimodalism replaces car-centrism, but\ndifferences in the local physical and social environments would and should\ninfluence the manner in which overarching policy objectives are met.\n"
    },
    {
        "paper_id": 1905.0281,
        "authors": "Kai Feng, Han Hong, Ke Tang, Jingyuan Wang",
        "title": "Decision Making with Machine Learning and ROC Curves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Receiver Operating Characteristic (ROC) curve is a representation of the\nstatistical information discovered in binary classification problems and is a\nkey concept in machine learning and data science. This paper studies the\nstatistical properties of ROC curves and its implication on model selection. We\nanalyze the implications of different models of incentive heterogeneity and\ninformation asymmetry on the relation between human decisions and the ROC\ncurves. Our theoretical discussion is illustrated in the context of a large\ndata set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy\nCheckups of reproductive age couples in Henan Province provided by the Chinese\nMinistry of Health.\n"
    },
    {
        "paper_id": 1905.02875,
        "authors": "Xiaojun Hu, Ronald Rousseau, Sandra Rousseau",
        "title": "Does Environmental Economics lead to patentable research?",
        "comments": "10 pages, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this feasibility study, the impact of academic research from social\nsciences and humanities on technological innovation is explored through a study\nof citations patterns of journal articles in patents. Specifically we focus on\ncitations of journals from the field of environmental economics in patents\nincluded in an American patent database (USPTO). Three decades of patents have\nled to a small set of journal articles (85) that are being cited from the field\nof environmental economics. While this route of measuring how academic research\nis validated through its role in stimulating technological progress may be\nrather limited (based on this first exploration), it may still point to a\nvaluable and interesting topic for further research.\n"
    },
    {
        "paper_id": 1905.02956,
        "authors": "Daniel Seligson and Anne McCants",
        "title": "Economic Performance Through Time: A Dynamical Theory",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The central problems of Development Economics are the explanation of the\ngross disparities in the global distribution, $\\cal{D}$, of economic\nperformance, $\\cal{E}$, and the persistence, $\\cal{P}$, of said distribution.\nDouglass North argued, epigrammatically, that institutions, $\\cal{I}$, are the\nrules of the game, meaning that $\\cal{I}$ determines or at least constrains\n$\\cal{E}$. This promised to explain $\\cal{D}$. 65,000 citations later, the\ncentral problems remain unsolved. North's institutions are informal, slowly\nchanging cultural norms as well as roads, guilds, and formal legislation that\nmay change overnight. This definition, mixing the static and the dynamic, is\nunsuited for use in a necessarily time dependent theory of developing\neconomies. We offer here a suitably precise definition of $\\cal{I}$, a\ndynamical theory of economic development, a new measure of the economy, an\nexplanation of $\\cal{P}$, a bivariate model that explains half of $\\cal{D}$,\nand a critical reconsideration of North's epigram.\n"
    },
    {
        "paper_id": 1905.03002,
        "authors": "Jaume Benseny, Juuso T\\\"oyli, Heikki H\\\"amm\\\"ainen, Andr\\'es\n  Arcia-Moret",
        "title": "The mitigating role of regulation on the concentric patterns of\n  broadband diffusion. The case of Finland",
        "comments": "Improved version of the accepted manuscript by Telematics and\n  Informatics",
        "journal-ref": "Telematics and Informatics, 41, pp.139-155 (2019)",
        "doi": "10.1016/j.tele.2019.04.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article analyzes the role of Finnish regulation in achieving the\nbroadband penetration goals defined by the National Regulatory Authority. It is\nwell known that in the absence of regulatory mitigation the population density\nhas a positive effect on broadband diffusion. Hence, we measure the effect of\nthe population density on the determinants of broadband diffusion throughout\nthe postal codes of Finland via Geographically Weighted Regression. We suggest\nthat the main determinants of broadband diffusion and the population density\nfollow a spatial pattern that is either concentric with a weak/medium/strong\nstrength or non-concentric convex/concave. Based on 10 patterns, we argue that\nthe Finnish spectrum policy encouraged Mobile Network Operators to satisfy\nambitious Universal Service Obligations without the need for a Universal\nService Fund. Spectrum auctions facilitated infrastructure-based competition\nvia equitable spectrum allocation and coverage obligation delivery via low-fee\nlicenses. However, state subsidies for fiber deployment did not attract\ninvestment from nationwide operators due to mobile preference. These subsidies\nencouraged demand-driven investment, leading to the emergence of fiber consumer\ncooperatives. To explain this emergence, we show that when population density\ndecreases, the level of mobile service quality decreases and community\ncommitment increases. Hence, we recommend regulators implementing market-driven\nstrategies for 5G to stimulate local investment. For example, by allocating the\n3.5 GHz and higher bands partly through local light licensing.\n"
    },
    {
        "paper_id": 1905.03108,
        "authors": "Jeff Yan",
        "title": "From Sicilian mafia to Chinese \"scam villages\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by Gambetta's theory on the origins of the mafia in Sicily, we\nreport a geo-concentrating phenomenon of scams in China, and propose a novel\neconomic explanation. Our analysis has some policy implications.\n"
    },
    {
        "paper_id": 1905.03189,
        "authors": "Junran Wu, Ke Xu and Jichang Zhao",
        "title": "Online reviews can predict long-term returns of individual stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online reviews are feedback voluntarily posted by consumers about their\nconsumption experiences. This feedback indicates customer attitudes such as\naffection, awareness and faith towards a brand or a firm and demonstrates\ninherent connections with a company's future sales, cash flow and stock\npricing. However, the predicting power of online reviews for long-term returns\non stocks, especially at the individual level, has received little research\nattention, making a comprehensive exploration necessary to resolve existing\ndebates. In this paper, which is based exclusively on online reviews, a\nmethodology framework for predicting long-term returns of individual stocks\nwith competent performance is established. Specifically, 6,246 features of 13\ncategories inferred from more than 18 million product reviews are selected to\nbuild the prediction models. With the best classifier selected from\ncross-validation tests, a satisfactory increase in accuracy, 13.94%, was\nachieved compared to the cutting-edge solution with 10 technical indicators\nbeing features, representing an 18.28% improvement relative to the random\nvalue. The robustness of our model is further evaluated and testified in\nrealistic scenarios. It is thus confirmed for the first time that long-term\nreturns of individual stocks can be predicted by online reviews. This study\nprovides new opportunities for investors with respect to long-term investments\nin individual stocks.\n"
    },
    {
        "paper_id": 1905.03211,
        "authors": "C. R. da Cunha, R. da Silva",
        "title": "Relevant Stylized Facts About Bitcoin: Fluctuations, First Return\n  Probability, and Natural Phenomena",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124155",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin is a digital financial asset that is devoid of a central authority.\nThis makes it distinct from traditional financial assets in a number of ways.\nFor instance, the total number of tokens is limited and it has not explicit use\nvalue. Nonetheless, little is know whether it obeys the same stylized facts\nfound in traditional financial assets. Here we test bitcoin for a set of these\nstylized facts and conclude that it behaves statistically as most of other\nassets. For instance, it exhibits aggregational Gaussianity and fluctuation\nscaling. Moreover, we show by an analogy with natural occurring quakes that\nbitcoin obeys both the Omori and Gutenberg-Richter laws. Finally, we show that\nthe global persistence, originally defined for spin systems, presents a power\nlaw behavior with exponent similar to that found in stock markets.\n"
    },
    {
        "paper_id": 1905.03273,
        "authors": "Anna Denkowska, Stanis{\\l}aw Wanat",
        "title": "Dependencies and systemic risk in the European insurance sector: Some\n  new evidence based on copula-DCC-GARCH model and selected clustering methods",
        "comments": "JEL: G22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The subject of the present article is the study of correlations between large\ninsurance companies and their contribution to systemic risk in the insurance\nsector. Our main goal is to analyze the conditional structure of the\ncorrelation on the European insurance market and to compare systemic risk in\ndifferent regimes of this market. These regimes are identified by monitoring\nthe weekly rates of returns of eight of the largest insurers (five from Europe\nand the biggest insurers from the USA, Canada and China) during the period\nJanuary 2005 to December 2018. To this aim we use statistical clustering\nmethods for time units (weeks) to which we assigned the conditional variances\nobtained from the estimated copula-DCC-GARCH model. The advantage of such an\napproach is that there is no need to assume a priori a number of market\nregimes, since this number has been identified by means of clustering quality\nvalidation. In each of the identified market regimes we determined the commonly\nnow used CoVaR systemic risk measure. From the performed analysis we conclude\nthat all the considered insurance companies are positively correlated and this\ncorrelation is stronger in times of turbulences on global markets which shows\nan increased exposure of the European insurance sector to systemic risk during\ncrisis. Moreover, in times of turbulences on global markets the value level of\nthe CoVaR systemic risk index is much higher than in \"normal conditions\".\n"
    },
    {
        "paper_id": 1905.03316,
        "authors": "Paul McCloud",
        "title": "Repo convexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is an observed basis between repo discounting, implied from market repo\nrates, and bond discounting, stripped from the market prices of the underlying\nbonds. Here, this basis is explained as a convexity effect arising from the\ndecorrelation between the discount rates for derivatives and bonds.\n  Using a Hull-White model for the discount basis, expressions are derived that\ncan be used to interpolate the repo rates of bonds with different maturities\nand to extrapolate the repo curve for discounting bond-collateralised\nderivatives.\n"
    },
    {
        "paper_id": 1905.03338,
        "authors": "Mich\\`ele Friend",
        "title": "A Policy Compass for Ecological Economics",
        "comments": "35 pages, 17 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.30052.01924",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A policy compass indicates the direction in which an institution is going in\nterms of three general qualities. The three qualities are: suppression, harmony\nand passion. Any formal institution can develop a policy compass to examine the\ndiscrepancy between what the institution would like to do (suggested in its\nmandate) and the actual performance and situation it finds itself in. The\nlatter is determined through an aggregation of statistical data and facts.\nThese are made robust and stable using meta-requirements of convergence. Here,\nI present a version of the compass adapted to embed the central ideas of\necological economics: that society is dependent on the environment, and that\neconomic activity is dependent on society; that we live in a world subject to\nat least the first two laws of thermodynamics; that the planet we live on is\nlimited in space and resources; that some of our practices have harmful and\nirreversible consequences on the natural environment; that there are values\nother than value in exchange, such as intrinsic value and use value. In this\npaper, I explain how to construct a policy compass in general. This is followed\nby the adaptation for ecological economics. The policy compass is original, and\nso is the adaptation. The compass is inspired by the work of Anthony Friend,\nRob Hoffman, Satish Kumar, Georgescu-Roegen, Stanislav Schmelev, Peter\nS\\\"oderbaum and Arild Vatn. In the conclusion, I discuss the accompanying\nconception of sustainability.\n"
    },
    {
        "paper_id": 1905.03339,
        "authors": "Rigoberto P\\'erez Ram\\'irez",
        "title": "Privatizaciones, fusiones y adquisiciones: las grandes empresas en\n  M\\'exico",
        "comments": "27 pages, in Spanish, 4 charts",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present work has as principal objective analyze the evolution of the\nprocess of privatization, mergers and acquisitions of the big companies in the\ncountry in the last decades, to understand the conductive threads that formed\nthe structural changes of the economy, in order world oligop\\'olicas to insert\nit to the global market characterized by formations of strategic alliances,\nacross the mergers and acquisitions that they favour to the transnational\ncompanies.\n"
    },
    {
        "paper_id": 1905.0334,
        "authors": "St\\'ephane Bl\\'emus (UP1), Dominique Guegan (CES, UP1)",
        "title": "Initial Crypto-asset Offerings (ICOs), tokenization and corporate\n  governance",
        "comments": "URL des Documents de travail :\n  https://centredeconomiesorbonne.univ-paris1.fr/documents-de-travail-du-ces/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the potential impacts of the so-called `initial coin\nofferings', and of several developments based on distributed ledger technology\n(`DLT'), on corporate governance. While many academic papers focus mainly on\nthe legal qualification of DLT and crypto-assets, and most notably in relation\nto the potential definition of the latter as securities/financial instruments,\nthe authors analyze some of the use cases based on DLT technology and their\npotential for significant changes of the corporate governance analyses. This\narticle studies the consequences due to the emergence of new kinds of firm\nstakeholders, i.e. the crypto-assets holders, on the governance of small and\nmedium-sized enterprises (`SMEs') as well as of publicly traded companies.\nSince early 2016, a new way of raising funds has rapidly emerged as a major\nissue for FinTech founders and financial regulators. Frequently referred to as\ninitial coin offerings, Initial Token Offerings (`ITO'), Token Generation\nEvents (`TGE') or simply `token sales', we use in our paper the terminology\nInitial Crypto-asset Offerings (`ICO'), as it describes more effectively than\n`initial coin offerings' the vast diversity of assets that could be created and\nwhich goes far beyond the payment instrument issue.\n"
    },
    {
        "paper_id": 1905.03463,
        "authors": "Jaap H. Abbring and Tim Salimans",
        "title": "The Likelihood of Mixed Hitting Times",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": "10.1016/j.jeconom.2019.08.017",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a method for computing the likelihood of a mixed hitting-time\nmodel that specifies durations as the first time a latent L\\'evy process\ncrosses a heterogeneous threshold. This likelihood is not generally known in\nclosed form, but its Laplace transform is. Our approach to its computation\nrelies on numerical methods for inverting Laplace transforms that exploit\nspecial properties of the first passage times of L\\'evy processes. We use our\nmethod to implement a maximum likelihood estimator of the mixed hitting-time\nmodel in MATLAB. We illustrate the application of this estimator with an\nanalysis of Kennan's (1985) strike data.\n"
    },
    {
        "paper_id": 1905.03876,
        "authors": "Alexander L. Brown and Rodrigo A. Velez",
        "title": "Empirical bias and efficiency of alpha-auctions: experimental evidence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We experimentally evaluate the comparative performance of the winner-bid,\naverage-bid, and loser-bid auctions for the dissolution of a partnership. The\nanalysis of these auctions based on the empirical equilibrium refinement of\nVelez and Brown (2020) arXiv:1907.12408 reveals that as long as behavior\nsatisfies weak payoff monotonicity, winner-bid and loser-bid auctions\nnecessarily exhibit a form of bias when empirical distributions of play\napproximate best responses (Velez and Brown, 2020 arXiv:1905.08234). We find\nsupport for both weak payoff monotonicity and the form of bias predicted by the\ntheory for these two auctions. Consistently with the theory, the average-bid\nauction does not exhibit this form of bias. It has lower efficiency that the\nwinner-bid auction, however.\n"
    },
    {
        "paper_id": 1905.04028,
        "authors": "Debopam Bhattacharya, Pascaline Dupas, Shin Kanaya",
        "title": "Demand and Welfare Analysis in Discrete Choice Models with Social\n  Interactions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many real-life settings of consumer-choice involve social interactions,\ncausing targeted policies to have spillover-effects. This paper develops novel\nempirical tools for analyzing demand and welfare-effects of\npolicy-interventions in binary choice settings with social interactions.\nExamples include subsidies for health-product adoption and vouchers for\nattending a high-achieving school. We establish the connection between\neconometrics of large games and Brock-Durlauf-type interaction models, under\nboth I.I.D. and spatially correlated unobservables. We develop new convergence\nresults for associated beliefs and estimates of preference-parameters under\nincreasing-domain spatial asymptotics. Next, we show that even with fully\nparametric specifications and unique equilibrium, choice data, that are\nsufficient for counterfactual demand-prediction under interactions, are\ninsufficient for welfare-calculations. This is because distinct underlying\nmechanisms producing the same interaction coefficient can imply different\nwelfare-effects and deadweight-loss from a policy-intervention. Standard\nindex-restrictions imply distribution-free bounds on welfare. We illustrate our\nresults using experimental data on mosquito-net adoption in rural Kenya.\n"
    },
    {
        "paper_id": 1905.04137,
        "authors": "Rene Carmona and Kevin Webster",
        "title": "Applications of a New Self-Financing Equation",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1312.2302",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this note is to illustrate the impact of a self-financing\ncondition recently introduced by the authors. We present the analyses of two\nspecific applications usually considered in more traditional models in\nfinancial mathematics. They include hedging European options with limit orders\nand the optimal behavior of market makers.\n"
    },
    {
        "paper_id": 1905.0437,
        "authors": "Bernardo J. Zubillaga and Andr\\'e L. M. Vilela and Chao Wang and\n  Kenric P. Nelson and H. Eugene Stanley",
        "title": "A Three-state Opinion Formation Model for Financial Markets",
        "comments": "10 pages, 11 figures, regular paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a three-state microscopic opinion formation model for the purpose\nof simulating the dynamics of financial markets. In order to mimic the\nheterogeneous composition of the mass of investors in a market, the agent-based\nmodel considers two different types of traders: noise traders and contrarians.\nAgents are represented as nodes in a network of interactions and they can\nassume any of three distinct possible states (e.g. buy, sell or remain\ninactive). The time evolution of the state of an agent is dictated by\nprobabilistic dynamics that include both local and global influences. A noise\ntrader is subject to local interactions, tending to assume the majority state\nof its nearest neighbors, whilst a contrarian is subject to a global\ninteraction with the behavior of the market as a whole, tending to assume the\nstate of the global minority of the market. The model exhibits the typical\nqualitative and quantitative features of real financial time series, including\ndistributions of returns with heavy tails, volatility clustering and long-time\nmemory for the absolute values of the returns. The distributions of returns are\nfitted by means of coupled Gaussian distributions, quantitatively revealing\ntransitions between leptokurtic, mesokurtic and platykurtic regimes in terms of\na non-linear statistical coupling which describes the complexity of the system.\n"
    },
    {
        "paper_id": 1905.04397,
        "authors": "Ben Hambly, Nikolaos Kolliopoulos",
        "title": "ERRATUM: Stochastic evolution equations for large portfolios of\n  stochastic volatility models",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": "10.1137/19M1260980",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the article \"Stochastic evolution equations for large portfolios of\nStochastic Volatility models\" (Arxiv:1701.05640) there is a mistake in the\nproof of Theorem 3.1. In this erratum we establish a weaker version of this\nTheorem and then we redevelop the regularity theory for our problem\naccordingly. This means that most of our regularity results are replaced by\nslightly weaker ones. We also clarify a point in the proof of a correct result.\n"
    },
    {
        "paper_id": 1905.04417,
        "authors": "Tatsuki Inoue, Nana Nunokawa, Daisuke Kurisu and Kota Ogasawara",
        "title": "Particulate Air Pollution, Birth Outcomes, and Infant Mortality:\n  Evidence from Japan's Automobile Emission Control Law of 1992",
        "comments": "12 pages, 2 figures, 2 tables, and 7-page Appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the impacts of the Automobile NOx Law of 1992 on\nambient air pollutants and fetal and infant health outcomes in Japan. Using\npanel data taken from more than 1,500 monitoring stations between 1987 and\n1997, we find that NOx and SO2 levels reduced by 87% and 52%, respectively in\nregulated areas following the 1992 regulation. In addition, using a\nmunicipal-level Vital Statistics panel dataset and adopting the regression\ndifferences-in-differences method, we find that the enactment of the regulation\nexplained most of the improvements in the fetal death rate between 1991 and\n1993. This study is the first to provide evidence on the positive impacts of\nthis large-scale automobile regulation policy on fetal health.\n"
    },
    {
        "paper_id": 1905.04419,
        "authors": "Tatsuki Inoue",
        "title": "The role of pawnshops in risk coping in early twentieth-century Japan",
        "comments": "24 pages, 2 figures, 4 tables, and appendix",
        "journal-ref": "Financial History Review 28 (2021) 319-343",
        "doi": "10.1017/S0968565021000111",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the role of pawnshops as a risk-coping device in prewar\nJapan. Using data on pawnshop loans for more than 250 municipalities and\nexploiting the 1918-1920 influenza pandemic as a natural experiment, we find\nthat the adverse health shock increased the total amount of loans from\npawnshops. This is because those who regularly relied on pawnshops borrowed\nmore money from them than usual to cope with the adverse health shock, and not\nbecause the number of people who used pawnshops increased.\n"
    },
    {
        "paper_id": 1905.04569,
        "authors": "Fr\\'ed\\'eric Bucci and Iacopo Mastromatteo and Michael Benzaquen and\n  Jean-Philippe Bouchaud",
        "title": "Impact is not just volatility",
        "comments": "5 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The notion of market impact is subtle and sometimes misinterpreted. Here we\nargue that impact should not be misconstrued as volatility. In particular, the\nso-called ``square-root impact law'', which states that impact grows as the\nsquare-root of traded volume, has nothing to do with price diffusion, i.e. that\ntypical price changes grow as the square-root of time. We rationalise empirical\nfindings on impact and volatility by introducing a simple scaling argument and\nconfronting it to data.\n"
    },
    {
        "paper_id": 1905.04603,
        "authors": "Andrey Sarantsev",
        "title": "A New Stock Market Valuation Measure with Applications to Retirement\n  Planning",
        "comments": "20 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We generalize the classic Shiller cyclically adjusted price-earnings ratio\n(CAPE) used for prediction of future total returns of the stock market. We\ntreat earnings growth as exogenous. The difference between log wealth and log\nearnings is modeled as an autoregression of order 1 with linear trend 4.5\\% and\nGaussian innovations. Detrending gives us a new valuation measure. This\nautoregression is significantly different from the random walk. Therefore, our\nresults disprove the Efficient Market Hypothesis. Therefore, long-run total\nreturns equal long-run earnings growth plus 4.5\\%. We apply results to\nretirement planning. A withdrawal process governs how a retired capital owner\nwithdraws a certain fraction of wealth annually. The fraction can vary from\nyear to year. We study the long-term behavior of such processes.\n"
    },
    {
        "paper_id": 1905.04821,
        "authors": "Matt Emschwiller, Benjamin Petit, Jean-Philippe Bouchaud",
        "title": "Optimal multi-asset trading with linear costs: a mean-field approach",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal multi-asset trading with Markovian predictors is well understood in\nthe case of quadratic transaction costs, but remains intractable when these\ncosts are $L_1$. We present a mean-field approach that reduces the multi-asset\nproblem to a single-asset problem, with an effective predictor that includes a\nrisk averse component. We obtain a simple approximate solution in the case of\nOrnstein-Uhlenbeck predictors and maximum position constraints. The optimal\nstrategy is of the \"bang-bang\" type similar to that obtained in [de Lataillade\net al., 2012]. When the risk aversion parameter is small, we find that the\ntrading threshold is an affine function of the instantaneous global position,\nwith a slope coefficient that we compute exactly. We relate the risk aversion\nparameter to the desired target risk and provide numerical simulations that\nsupport our analytical results.\n"
    },
    {
        "paper_id": 1905.04842,
        "authors": "Jessie Sun",
        "title": "A Stock Selection Method Based on Earning Yield Forecast Using Sequence\n  Prediction Models",
        "comments": "10 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long-term investors, different from short-term traders, focus on examining\nthe underlying forces that affect the well-being of a company. They rely on\nfundamental analysis which attempts to measure the intrinsic value an equity.\nQuantitative investment researchers have identified some value factors to\ndetermine the cost of investment for a stock and compare different stocks. This\npaper proposes using sequence prediction models to forecast a value factor-the\nearning yield (EBIT/EV) of a company for stock selection. Two advanced sequence\nprediction models-Long Short-term Memory (LSTM) and Gated Recurrent Unit (GRU)\nnetworks are studied. These two models can overcome the inherent problems of a\nstandard Recurrent Neural Network, i.e., vanishing and exploding gradients.\nThis paper firstly introduces the theories of the networks. And then elaborates\nthe workflow of stock pool creation, feature selection, data structuring, model\nsetup and model evaluation. The LSTM and GRU models demonstrate superior\nperformance of forecast accuracy over a traditional Feedforward Neural Network\nmodel. The GRU model slightly outperformed the LSTM model.\n"
    },
    {
        "paper_id": 1905.04852,
        "authors": "Masaaki Fukasawa, Tetsuya Takabatake, Rebecca Westphal",
        "title": "Is Volatility Rough ?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough volatility models are continuous time stochastic volatility models\nwhere the volatility process is driven by a fractional Brownian motion with the\nHurst parameter smaller than half, and have attracted much attention since a\nseminal paper titled \"Volatility is rough\" was posted on SSRN in 2014 showing\nthat the log realized volatility time series of major stock indices have the\nsame scaling property as such a rough fractional Brownian motion has. We\nhowever find by simulations that the impressive approach tends to suggest the\nsame roughness irrespectively whether the volatility is actually rough or not;\nan overlooked estimation error of latent volatility often results in an\nillusive scaling property. Motivated by this preliminary finding, here we\ndevelop a statistical theory for a continuous time fractional stochastic\nvolatility model to examine whether the Hurst parameter is indeed estimated\nsmaller than half, that is, whether the volatility is really rough. We\nconstruct a quasi-likelihood estimator and apply it to realized volatility time\nseries. Our quasi-likelihood is based on the error distribution of the realized\nvolatility and a Whittle-type approximation to the auto-covariance of the\nlog-volatility process. We prove the consistency of our estimator under high\nfrequency asymptotics, and examine by simulations its finite sample\nperformance. Our empirical study suggests that the volatility is indeed rough;\nactually it is even rougher than considered in the literature.\n"
    },
    {
        "paper_id": 1905.05023,
        "authors": "Adriano Koshiyama and Nick Firoozye",
        "title": "Avoiding Backtesting Overfitting by Covariance-Penalties: an empirical\n  investigation of the ordinary and total least squares cases",
        "comments": "Large portions of this work appeared previously in a replacement of\n  arXiv:1901.01751 (version 2) which was uploaded there by mistake",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systematic trading strategies are rule-based procedures which choose\nportfolios and allocate assets. In order to attain certain desired return\nprofiles, quantitative strategists must determine a large array of trading\nparameters. Backtesting, the attempt to identify the appropriate parameters\nusing historical data available, has been highly criticized due to the\nabundance of misleading results. Hence, there is an increasing interest in\ndevising procedures for the assessment and comparison of strategies, that is,\ndevising schemes for preventing what is known as backtesting overfitting. So\nfar, many financial researchers have proposed different ways to tackle this\nproblem that can be broadly categorised in three types: Data Snooping,\nOverestimated Performance, and Cross-Validation Evaluation. In this paper, we\npropose a new approach to dealing with financial overfitting, a\nCovariance-Penalty Correction, in which a risk metric is lowered given the\nnumber of parameters and data used to underpins a trading strategy. We outlined\nthe foundation and main results behind the Covariance-Penalty correction for\ntrading strategies. After that, we pursue an empirical investigation, comparing\nits performance with some other approaches in the realm of Covariance-Penalties\nacross more than 1300 assets, using Ordinary and Total Least Squares. Our\nresults suggest that Covariance-Penalties are a suitable procedure to avoid\nBacktesting Overfitting, and Total Least Squares provides superior performance\nwhen compared to Ordinary Least Squares.\n"
    },
    {
        "paper_id": 1905.05027,
        "authors": "Lukas Gonon, Johannes Muhle-Karbe, Xiaofei Shi",
        "title": "Asset Pricing with General Transaction Costs: Theory and Numerics",
        "comments": "45 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study risk-sharing equilibria with general convex costs on the agents'\ntrading rates. For an infinite-horizon model with linear state dynamics and\nexogenous volatilities, we prove that the equilibrium returns mean-revert\naround their frictionless counterparts - the deviation has Ornstein-Uhlenbeck\ndynamics for quadratic costs whereas it follows a doubly-reflected Brownian\nmotion if costs are proportional. More general models with arbitrary state\ndynamics and endogenous volatilities lead to multidimensional systems of\nnonlinear, fully-coupled forward-backward SDEs. These fall outside the scope of\nknown wellposedness results, but can be solved numerically using the\nsimulation-based deep-learning approach of Han, Jentzen and E (2018). In a\ncalibration to time series of prices and trading volume, realistic liquidity\npremia are accompanied by a moderate increase in volatility. The effects of\ndifferent cost specifications are rather similar, justifying the use of\nquadratic costs as a proxy for other less tractable specifications.\n"
    },
    {
        "paper_id": 1905.05237,
        "authors": "Lisa R. Goldberg and Saad Mouti",
        "title": "Sustainable Investing and the Cross-Section of Returns and Maximum\n  Drawdown",
        "comments": null,
        "journal-ref": "The Journal of Finance and Data Science, Volume 8, November 2022,\n  Pages 353-387",
        "doi": "10.1016/j.jfds.2022.11.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use supervised learning to identify factors that predict the cross-section\nof returns and maximum drawdown for stocks in the US equity market. Our data\nrun from January 1970 to December 2019 and our analysis includes ordinary least\nsquares, penalized linear regressions, tree-based models, and neural networks.\nWe find that the most important predictors tended to be consistent across\nmodels, and that non-linear models had better predictive power than linear\nmodels. Predictive power was higher in calm periods than in stressed periods.\nEnvironmental, social, and governance indicators marginally impacted the\npredictive power of non-linear models in our data, despite their negative\ncorrelation with maximum drawdown and positive correlation with returns. Upon\nexploring whether ESG variables are captured by some models, we find that ESG\ndata contribute to the prediction nonetheless.\n"
    },
    {
        "paper_id": 1905.0531,
        "authors": "Federico Graceffa, Damiano Brigo, Andrea Pallavicini",
        "title": "On the consistency of jump-diffusion dynamics for FX rates under\n  inversion",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we investigate the consistency under inversion of jump diffusion\nprocesses in the Foreign Exchange (FX) market. In other terms, if the EUR/USD\nFX rate follows a given type of dynamics, under which conditions will USD/EUR\nfollow the same type of dynamics? In order to give a numerical description of\nthis property, we first calibrate a Heston model and a SABR model to market\ndata, plotting their smiles together with the smiles of the reciprocal\nprocesses. Secondly, we determine a suitable local volatility structure\nensuring consistency. We subsequently introduce jumps and analyze both constant\njump size (Poisson process) and random jump size (compound Poisson process). In\nthe first scenario, we find that consistency is automatically satisfied, for\nthe jump size of the inverted process is a constant as well. The second case is\nmore delicate, since we need to make sure that the distribution of jumps in the\ndomestic measure is the same as the distribution of jumps in the foreign\nmeasure. We determine a fairly general class of admissible densities for the\njump size in the domestic measure satisfying the condition.\n"
    },
    {
        "paper_id": 1905.05371,
        "authors": "Bingyan Han and Hoi Ying Wong",
        "title": "Merton's portfolio problem under Volterra Heston model",
        "comments": "14 pages, 3 figures, exponential utility added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates Merton's portfolio problem in a rough stochastic\nenvironment described by Volterra Heston model. The model has a non-Markovian\nand non-semimartingale structure. By considering an auxiliary random process,\nwe solve the portfolio optimization problem with the martingale optimality\nprinciple. Optimal strategies for power and exponential utilities are derived\nin semi-closed form solutions depending on the respective Riccati-Volterra\nequations. We numerically examine the relationship between investment demand\nand volatility roughness.\n"
    },
    {
        "paper_id": 1905.05429,
        "authors": "S\\\"oren Christensen, Luis H. R. Alvarez E",
        "title": "A Solvable Two-dimensional Optimal Stopping Problem in the Presence of\n  Ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  According to conventional wisdom, ambiguity accelerates optimal timing by\ndecreasing the value of waiting in comparison with the unambiguous benchmark\ncase. We study this mechanism in a multidimensional setting and show that in a\nmultifactor model ambiguity does not only influence the rate at which the\nunderlying processes are expected to grow, it also affects the rate at which\nthe problem is discounted. This mechanism where nature also selects the rate at\nwhich the problem is discounted cannot appear in a one-dimensional setting and\nas such we identify an indirect way of how ambiguity affects optimal timing.\n"
    },
    {
        "paper_id": 1905.05663,
        "authors": "Aur\\'elien Alfonsi and Rafa\\\"el Coyaud and Virginie Ehrlacher and\n  Damiano Lombardi",
        "title": "Approximation of Optimal Transport problems with marginal moments\n  constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal Transport (OT) problems arise in a wide range of applications, from\nphysics to economics. Getting numerical approximate solution of these problems\nis a challenging issue of practical importance. In this work, we investigate\nthe relaxation of the OT problem when the marginal constraints are replaced by\nsome moment constraints. Using Tchakaloff's theorem, we show that the Moment\nConstrained Optimal Transport problem (MCOT) is achieved by a finite discrete\nmeasure. Interestingly, for multimarginal OT problems, the number of points\nweighted by this measure scales linearly with the number of marginal laws,\nwhich is encouraging to bypass the curse of dimension. This approximation\nmethod is also relevant for Martingale OT problems. We show the convergence of\nthe MCOT problem toward the corresponding OT problem. In some fundamental\ncases, we obtain rates of convergence in $O(1/n)$ or $O(1/n^2)$ where $n$ is\nthe number of moments, which illustrates the role of the moment functions.\nLast, we present algorithms exploiting the fact that the MCOT is reached by a\nfinite discrete measure and provide numerical examples of approximations.\n"
    },
    {
        "paper_id": 1905.0573,
        "authors": "Johannes Muhle-Karbe, Marcel Nutz, Xiaowei Tan",
        "title": "Asset Pricing with Heterogeneous Beliefs and Illiquidity",
        "comments": "37 pages, 2 figures, forthcoming in 'Mathematical Finance'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the equilibrium price of an asset that is traded in\ncontinuous time between N agents who have heterogeneous beliefs about the state\nprocess underlying the asset's payoff. We propose a tractable model where\nagents maximize expected returns under quadratic costs on inventories and\ntrading rates. The unique equilibrium price is characterized by a weakly\ncoupled system of linear parabolic equations which shows that holding and\nliquidity costs play dual roles. We derive the leading-order asymptotics for\nsmall transaction and holding costs which give further insight into the\nequilibrium and the consequences of illiquidity.\n"
    },
    {
        "paper_id": 1905.05813,
        "authors": "Ivan Arraut, Alan Au, Alan Ching-biu Tse and Carlos Segovia",
        "title": "The connection between multiple prices of an Option at a given time with\n  single prices defined at different times: The concept of weak-value in\n  quantum finance",
        "comments": "31 pages, 5 figures, version published in Physica A",
        "journal-ref": "Phys. A, Vol. 526 (2019), 1-19",
        "doi": "10.1016/j.physa.2019.04.264",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new tool for predicting the evolution of an option for the\ncases where at some specific time, there is a high-degree of uncertainty for\nidentifying its price. We work over the special case where we can predict the\nevolution of the system by joining a single price for the Option, defined at\nsome specific time with a pair of prices defined at another instant. This is\nachieved by describing the evolution of the system through a financial\nHamiltonian. The extension to the case of multiple prices at a given instant is\nstraightforward. We also explain how to apply these results in real situations.\n"
    },
    {
        "paper_id": 1905.05841,
        "authors": "Th\\'eophile Griveau-Billion and Ben Calderhead",
        "title": "Efficient computation of mean reverting portfolios using cyclical\n  coordinate descent",
        "comments": "17 pages, 7 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The econometric challenge of finding sparse mean reverting portfolios based\non a subset of a large number of assets is well known. Many current\nstate-of-the-art approaches fall into the field of co-integration theory, where\nthe problem is phrased in terms of an eigenvector problem with sparsity\nconstraint. Although a number of approximate solutions have been proposed to\nsolve this NP-hard problem, all are based on relatively simple models and are\nlimited in their scalability. In this paper we leverage information obtained\nfrom a heterogeneous simultaneous graphical dynamic linear model (H-SGDLM) and\npropose a novel formulation of the mean reversion problem, which is phrased in\nterms of a quasi-convex minimisation with a normalisation constraint. This new\nformulation allows us to employ a cyclical coordinate descent algorithm for\nefficiently computing an exact sparse solution, even in a large universe of\nassets, while the use of H-SGDLM data allows us to easily control the required\nlevel of sparsity. We demonstrate the flexibility, speed and scalability of the\nproposed approach on S\\&P$500$, FX and ETF futures data.\n"
    },
    {
        "paper_id": 1905.05911,
        "authors": "Yadong Li, Dimitri Offengenden, Jan Burgy",
        "title": "Reduced Form Capital Optimization",
        "comments": "12 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.15535.18080",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate banks' capital optimization problem as a classic mean variance\noptimization, by leveraging an accurate linear approximation to the Shapely or\nConstrained Aumann-Shapley (CAS) allocation of max or nested max cost\nfunctions. This reduced form formulation admits an analytical solution, to the\noptimal leveraged balance sheet (LBS) and risk weighted assets (RWA) target of\nbanks' business units for achieving the best return on capital.\n"
    },
    {
        "paper_id": 1905.05931,
        "authors": "Christian Diem, Anton Pichler, Stefan Thurner",
        "title": "What is the Minimal Systemic Risk in Financial Exposure Networks?",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Management of systemic risk in financial markets is traditionally associated\nwith setting (higher) capital requirements for market participants. There are\nindications that while equity ratios have been increased massively since the\nfinancial crisis, systemic risk levels might not have lowered, but even\nincreased. It has been shown that systemic risk is to a large extent related to\nthe underlying network topology of financial exposures. A natural question\narising is how much systemic risk can be eliminated by optimally rearranging\nthese networks and without increasing capital requirements. Overlapping\nportfolios with minimized systemic risk which provide the same market\nfunctionality as empirical ones have been studied by [pichler2018]. Here we\npropose a similar method for direct exposure networks, and apply it to\ncross-sectional interbank loan networks, consisting of 10 quarterly\nobservations of the Austrian interbank market. We show that the suggested\nframework rearranges the network topology, such that systemic risk is reduced\nby a factor of approximately 3.5, and leaves the relevant economic features of\nthe optimized network and its agents unchanged. The presented optimization\nprocedure is not intended to actually re-configure interbank markets, but to\ndemonstrate the huge potential for systemic risk management through rearranging\nexposure networks, in contrast to increasing capital requirements that were\nshown to have only marginal effects on systemic risk [poledna2017]. Ways to\nactually incentivize a self-organized formation toward optimal network\nconfigurations were introduced in [thurner2013] and [poledna2016]. For\nregulatory policies concerning financial market stability the knowledge of\nminimal systemic risk for a given economic environment can serve as a benchmark\nfor monitoring actual systemic risk in markets.\n"
    },
    {
        "paper_id": 1905.06166,
        "authors": "Jian Gao, Yi-Cheng Zhang, Tao Zhou",
        "title": "Computational Socioeconomics",
        "comments": "122 pages, 49 figures",
        "journal-ref": "Physics Reports 817 (2019) 1-104",
        "doi": "10.1016/j.physrep.2019.05.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Uncovering the structure of socioeconomic systems and timely estimation of\nsocioeconomic status are significant for economic development. The\nunderstanding of socioeconomic processes provides foundations to quantify\nglobal economic development, to map regional industrial structure, and to infer\nindividual socioeconomic status. In this review, we will make a brief manifesto\nabout a new interdisciplinary research field named Computational\nSocioeconomics, followed by detailed introduction about data resources,\ncomputational tools, data-driven methods, theoretical models and novel\napplications at multiple resolutions, including the quantification of global\neconomic inequality and complexity, the map of regional industrial structure\nand urban perception, the estimation of individual socioeconomic status and\ndemographic, and the real-time monitoring of emergent events. This review,\ntogether with pioneering works we have highlighted, will draw increasing\ninterdisciplinary attentions and induce a methodological shift in future\nsocioeconomic studies.\n"
    },
    {
        "paper_id": 1905.06213,
        "authors": "Daniel Lacker, Mykhaylo Shkolnikov, Jiacheng Zhang",
        "title": "Inverting the Markovian projection, with an application to local\n  stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study two-dimensional stochastic differential equations (SDEs) of\nMcKean--Vlasov type in which the conditional distribution of the second\ncomponent of the solution given the first enters the equation for the first\ncomponent of the solution. Such SDEs arise when one tries to invert the\nMarkovian projection developed by Gy\\\"ongy (1986), typically to produce an\nIt\\^o process with the fixed-time marginal distributions of a given\none-dimensional diffusion but richer dynamical features. We prove the strong\nexistence of stationary solutions for these SDEs, as well as their strong\nuniqueness in an important special case. Variants of the SDEs discussed in this\npaper enjoy frequent application in the calibration of local stochastic\nvolatility models in finance, despite the very limited theoretical\nunderstanding.\n"
    },
    {
        "paper_id": 1905.06315,
        "authors": "Archil Gulisashvili, Ra\\'ul Merino, Marc Lagunas and Josep Vives",
        "title": "Higher order approximation of call option prices under stochastic\n  volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, a decomposition formula for the call price due to\nAl\\`{o}s is transformed into a Taylor type formula containing an infinite\nseries with stochastic terms. The new decomposition may be considered as an\nalternative to the decomposition of the call price found in a recent paper of\nAl\\`{o}s, Gatheral and Radoi\\v{c}i\\'{c}. We use the new decomposition to obtain\nvarious approximations to the call price in the Heston model with sharper\nestimates of the error term than in the previously known approximations. One of\nthe formulas obtained in the present paper has five significant terms and an\nerror estimate of the form $O(\\nu^{3}(\\left|\\rho\\right|+\\nu))$, where $\\nu$ is\nthe vol-vol parameter, and $\\rho$ is the correlation coefficient between the\nprice and the volatility in the Heston model. Another approximation formula\ncontains seven more terms and the error estimate is of the form\n$O(\\nu^4(1+|\\rho|)$. For the uncorrelated Hestom model ($\\rho=0$), we obtain a\nformula with four significant terms and an error estimate $O(\\nu^6)$. Numerical\nexperiments show that the new approximations to the call price perform\nespecially well in the high volatility mode.\n"
    },
    {
        "paper_id": 1905.06364,
        "authors": "Oleg Malafeyev, Eduard Abramyan, Andrey Shulga",
        "title": "Dynamic model of firms competitive interaction on the market with\n  taxation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article three models of firms interaction on the market are\ndescribed. One of these models is described by using a differential equation\nand by Lotka-Volterra model, where the equation has a different form. Also,\nthere are models of non-competing and competing firms. The article presents an\nalgorithm for solving the interaction of competing firms in taxation and the\ncalculation of a compromise point. Besides, the article presents a compromise\nbetween the interests of a state and an enterprise.\n"
    },
    {
        "paper_id": 1905.06489,
        "authors": "C\\'elestin Coquid\\'e, Jos\\'e Lages, Dima L. Shepelyansky",
        "title": "Interdependence of sectors of economic activities for world countries\n  from the reduced Google matrix analysis of WTO data",
        "comments": "20 pages, 10 figures, 1 additional figure in Appendix; minor\n  corrections",
        "journal-ref": "Entropy 2020, 22(12), 1407",
        "doi": "10.3390/e22121407",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply the recently developed reduced Google matrix algorithm for the\nanalysis of the OECD-WTO world network of economic activities. This approach\nallows to determine interdependences and interactions of economy sectors of\nseveral countries, including China, Russia and USA, properly taking into\naccount the influence of all other world countries and their economic\nactivities. Within this analysis we also obtain the sensitivity of economy\nsectors and EU countries to petroleum activity sector. We show that this\napproach takes into account multiplicity of network links with economy\ninteractions between countries and activity sectors thus providing more rich\ninformation compared to the usual export-import analysis.\n"
    },
    {
        "paper_id": 1905.06536,
        "authors": "Takashi Yamashita and Ryozo Miura",
        "title": "Improving Regression-based Event Study Analysis Using a Topological\n  Machine-learning Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new correction scheme to a conventional\nregression-based event study method: a topological machine-learning approach\nwith a self-organizing map (SOM).We use this new scheme to analyze a major\nmarket event in Japan and find that the factors of abnormal stock returns can\nbe easily can be easily identified and the event-cluster can be depicted.We\nalso find that a conventional event study method involves an empirical analysis\nmechanism that tends to derive bias due to its mechanism, typically in an\nevent-clustered market situation. We explain our new correction scheme and\napply it to an event in the Japanese market --- the holding disclosure of the\nGovernment Pension Investment Fund (GPIF) on July 31, 2015.\n"
    },
    {
        "paper_id": 1905.06564,
        "authors": "Tiziano De Angelis and Erik Ekstr\\\"om",
        "title": "Playing with ghosts in a Dynkin game",
        "comments": "20 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a class of optimal stopping games (Dynkin games) of preemption type,\nwith uncertainty about the existence of competitors. The set-up is well-suited\nto model, for example, real options in the context of investors who do not want\nto publicly reveal their interest in a certain business opportunity. We show\nthat there exists a Nash equilibrium in randomized stopping times which is\ndescribed explicitly in terms of the corresponding one-player game.\n"
    },
    {
        "paper_id": 1905.06721,
        "authors": "Oliver James Scholten and Peter Cowling and Kenneth A. Hawick and\n  James Alfred Walker",
        "title": "Unconventional Exchange: Methods for Statistical Analysis of Virtual\n  Goods",
        "comments": "7 pages, 6 figures, 2 tables, accepted to IEEE CoG 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hyperinflation and price volatility in virtual economies has the potential to\nreduce player satisfaction and decrease developer revenue. This paper describes\nintuitive analytical methods for monitoring volatility and inflation in virtual\neconomies, with worked examples on the increasingly popular multiplayer game\nOld School Runescape. Analytical methods drawn from mainstream financial\nliterature are outlined and applied in order to present a high level overview\nof virtual economic activity of 3467 price series over 180 trading days.\nSix-monthly volume data for the top 100 most traded items is also used both for\nmonitoring and value estimation, giving a conservative estimate of exchange\ntrading volume of over {\\pounds}60m in real value. Our worked examples show\nresults from a well functioning virtual economy to act as a benchmark for\nfuture work. This work contributes to the growing field of virtual economics\nand game development, describing how data transformations and statistical tests\ncan be used to improve virtual economic design and analysis, with applications\nin real-time monitoring systems.\n"
    },
    {
        "paper_id": 1905.06722,
        "authors": "Andrea Berdondini",
        "title": "The professional trader's paradox",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, I will present a paradox whose purpose is to draw your\nattention to an important topic in finance, concerning the non-independence of\nthe financial returns (non-ergodic hypothesis). In this paradox, we have two\npeople sitting at a table separated by a black sheet so that they cannot see\neach other and are playing the following game: the person we call A flip a coin\nand the person we'll call B tries to guess the outcome of the coin flip. At the\nend of the game, both people are asked to estimate the compound probability of\nthe result obtained. The two people give two different answers, one estimates\nthe events as independent and the other one considers the events as dependent\ntherefore they calculate the conditional probability differently. This paradox\nshow how the erroneous estimation of conditional probability implies a strong\ndistortion of the forecasting skill that can lead us to bear excessive risks.\n"
    },
    {
        "paper_id": 1905.06733,
        "authors": "Reason Machete",
        "title": "Options to Receive Retirement Gratuity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Retirement gratuity is the money companies typically pay their employees at\nthe end of their contracts or at the time of leaving the company. It is a\ndefined benefit plan and is often given as an alternative to a pension plan. In\nBotswana, there is now a new pattern whereby companies give their employees the\noption to receive their gratuity at various stages before the end of their\ncontracts. In particular, some companies give their employees an option to\nreceive their gratuity on a monthly basis rather than having them wait for a\nyear or more. Many employees find this option attractive, but is it\neconomically sound? This paper sheds light on this question by quantifying the\neconomic benefits of the tax relief provided by government relative to\ninvesting the monthly-received funds in a risk-free savings account or helping\nrepay a loan. The principles and methods used herein can be adapted and applied\nto different taxation systems.\n"
    },
    {
        "paper_id": 1905.07048,
        "authors": "Jaap H. Abbring and {\\O}ystein Daljord",
        "title": "A Comment on \"Estimating Dynamic Discrete Choice Models with Hyperbolic\n  Discounting\" by Hanming Fang and Yang Wang",
        "comments": "11 pages, version submitted to the International Econonomic Review on\n  May 24",
        "journal-ref": "International Economic Review 61(2), 565-571 (May 2020)",
        "doi": "10.1111/iere.12434",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent literature often cites Fang and Wang (2015) for analyzing the\nidentification of time preferences in dynamic discrete choice under exclusion\nrestrictions (e.g. Yao et al., 2012; Lee, 2013; Ching et al., 2013; Norets and\nTang, 2014; Dub\\'e et al., 2014; Gordon and Sun, 2015; Bajari et al., 2016;\nChan, 2017; Gayle et al., 2018). Fang and Wang's Proposition 2 claims generic\nidentification of a dynamic discrete choice model with hyperbolic discounting.\nThis claim uses a definition of \"generic\" that does not preclude the\npossibility that a generically identified model is nowhere identified. To\nillustrate this point, we provide two simple examples of models that are\ngenerically identified in Fang and Wang's sense, but that are, respectively,\neverywhere and nowhere identified. We conclude that Proposition 2 is void: It\nhas no implications for identification of the dynamic discrete choice model. We\nshow that its proof is incorrect and incomplete and suggest alternative\napproaches to identification.\n"
    },
    {
        "paper_id": 1905.07081,
        "authors": "Simon Clinet and Yoann Potiron",
        "title": "Cointegration in high frequency data",
        "comments": "68 pages, 3 figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a framework adapting the notion of cointegration\nwhen two asset prices are generated by a driftless It\\^{o}-semimartingale\nfeaturing jumps with infinite activity, observed regularly and synchronously at\nhigh frequency. We develop a regression based estimation of the cointegrated\nrelations method and show the related consistency and central limit theory when\nthere is cointegration within that framework. We also provide a Dickey-Fuller\ntype residual based test for the null of no cointegration against the\nalternative of cointegration, along with its limit theory. Under no\ncointegration, the asymptotic limit is the same as that of the original\nDickey-Fuller residual based test, so that critical values can be easily\ntabulated in the same way. Finite sample indicates adequate size and good power\nproperties in a variety of realistic configurations, outperforming original\nDickey-Fuller and Phillips-Perron type residual based tests, whose sizes are\ndistorted by non ergodic time-varying variance and power is altered by price\njumps. Two empirical examples consolidate the Monte-Carlo evidence that the\nadapted tests can be rejected while the original tests are not, and vice versa.\n"
    },
    {
        "paper_id": 1905.07257,
        "authors": "Will Hicks",
        "title": "A Nonlocal Approach to The Quantum Kolmogorov Backward Equation and\n  Links to Noncommutative Geometry",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Accardi-Boukas quantum Black-Scholes equation can be used as an\nalternative to the classical approach to finance, and has been found to have a\nnumber of useful benefits. The quantum Kolmogorov backward equations, and\nassociated quantum Fokker-Planck equations, that arise from this general\nframework, are derived using the Hudson-Parthasarathy quantum stochastic\ncalculus. In this paper we show how these equations can be derived using a\nnonlocal approach to quantum mechanics. We show how nonlocal diffusions, and\nquantum stochastic processes can be linked, and discuss how moment matching can\nbe used for deriving solutions.\n"
    },
    {
        "paper_id": 1905.07544,
        "authors": "Nikhil Garg, Hamid Nazerzadeh",
        "title": "Driver Surge Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ride-hailing marketplaces like Uber and Lyft use dynamic pricing, often\ncalled surge, to balance the supply of available drivers with the demand for\nrides. We study driver-side payment mechanisms for such marketplaces,\npresenting the theoretical foundation that has informed the design of Uber's\nnew additive driver surge mechanism. We present a dynamic stochastic model to\ncapture the impact of surge pricing on driver earnings and their strategies to\nmaximize such earnings. In this setting, some time periods (surge) are more\nvaluable than others (non-surge), and so trips of different time lengths vary\nin the induced driver opportunity cost.\n  First, we show that multiplicative surge, historically the standard on\nride-hailing platforms, is not incentive compatible in a dynamic setting. We\nthen propose a structured, incentive-compatible pricing mechanism. This\nclosed-form mechanism has a simple form and is well-approximated by Uber's new\nadditive surge mechanism. Finally, through both numerical analysis and real\ndata from a ride-hailing marketplace, we show that additive surge is more\nincentive compatible in practice than is multiplicative surge.\n"
    },
    {
        "paper_id": 1905.07546,
        "authors": "Samuel Asante Gyamerah, Philip Ngare, and Dennis Ikpe",
        "title": "Hedging crop yields against weather uncertainties -- a weather\n  derivative perspective",
        "comments": "28 pages, 6 figures",
        "journal-ref": "Mathematical and Computational Applications, 24(3) (2019)",
        "doi": "10.3390/mca24030071",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The effects of weather on agriculture in recent years have become a major\nglobal concern. Hence, the need for an effective weather risk management tool\n(i.e., weather derivatives) that can hedge crop yields against weather\nuncertainties. However, most smallholder farmers and agricultural stakeholders\nare unwilling to pay for the price of weather derivatives (WD) because of the\npresence of basis risks (product-design and geographical) in the pricing\nmodels. To eliminate product-design basis risks, a machine learning ensemble\ntechnique was used to determine the relationship between maize yield and\nweather variables. The results revealed that the most significant weather\nvariable that affected the yield of maize was average temperature. A\nmean-reverting model with a time-varying speed of mean reversion, seasonal\nmean, and local volatility that depended on the local average temperature was\nthen proposed. The model was extended to a multi-dimensional model for\ndifferent but correlated locations. Based on these average temperature models,\npricing models for futures, options on futures, and basket futures for\ncumulative average temperature and growing degree-days are presented. Pricing\nfutures on baskets reduces geographical basis risk, as buyers have the\nopportunity to select the most appropriate weather stations with their desired\nweight preference. With these pricing models, farmers and agricultural\nstakeholders can hedge their crops against the perils of extreme weather.\n"
    },
    {
        "paper_id": 1905.07581,
        "authors": "Shangeth Rajaa, Jajati Keshari Sahoo",
        "title": "Convolutional Feature Extraction and Neural Arithmetic Logic Units for\n  Stock Prediction",
        "comments": "Accepted at ICACDS 2019 - Springer CCIS",
        "journal-ref": null,
        "doi": "10.1007/978-981-13-9939-8_31",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock prediction is a topic undergoing intense study for many years. Finance\nexperts and mathematicians have been working on a way to predict the future\nstock price so as to decide to buy the stock or sell it to make profit. Stock\nexperts or economists, usually analyze on the previous stock values using\ntechnical indicators, sentiment analysis etc to predict the future stock price.\nIn recent years, many researches have extensively used machine learning for\npredicting the stock behaviour. In this paper we propose data driven deep\nlearning approach to predict the future stock value with the previous price\nwith the feature extraction property of convolutional neural network and to use\nNeural Arithmetic Logic Units with it.\n"
    },
    {
        "paper_id": 1905.07716,
        "authors": "Mohammed Berkhouch, Ghizlane Lakhnati and Marcelo Brutti Righi",
        "title": "Spectral risk measures and uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk assessment under different possible scenarios is a source of uncertainty\nthat may lead to concerning financial losses. We address this issue, first, by\nadapting a robust framework to the class of spectral risk measures. Second, we\npropose a Deviation-based approach to quantify uncertainty. Furthermore, the\ntheory is illustrated with a practical case study from NASDAQ index.\n"
    },
    {
        "paper_id": 1905.07886,
        "authors": "Christopher Kath and Florian Ziel",
        "title": "Conformal Prediction Interval Estimations with an Application to\n  Day-Ahead and Intraday Power Markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.ijforecast.2020.09.006",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We discuss a concept denoted as Conformal Prediction (CP) in this paper.\nWhile initially stemming from the world of machine learning, it was never\napplied or analyzed in the context of short-term electricity price forecasting.\nTherefore, we elaborate the aspects that render Conformal Prediction worthwhile\nto know and explain why its simple yet very efficient idea has worked in other\nfields of application and why its characteristics are promising for short-term\npower applications as well. We compare its performance with different\nstate-of-the-art electricity price forecasting models such as quantile\nregression averaging (QRA) in an empirical out-of-sample study for three\nshort-term electricity time series. We combine Conformal Prediction with\nvarious underlying point forecast models to demonstrate its versatility and\nbehavior under changing conditions. Our findings suggest that Conformal\nPrediction yields sharp and reliable prediction intervals in short-term power\nmarkets. We further inspect the effect each of Conformal Prediction's model\ncomponents has and provide a path-based guideline on how to find the best CP\nmodel for each market.\n"
    },
    {
        "paper_id": 1905.08004,
        "authors": "Lijun Bo, Huafu Liao, Xiang Yu",
        "title": "Risk-Sensitive Credit Portfolio Optimization under Partial Information\n  and Contagion Risk",
        "comments": "Final version, forthcoming in the Annals of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the finite horizon risk-sensitive portfolio\noptimization in a regime-switching credit market with physical and\ninformation-induced default contagion. It is assumed that the underlying\nregime-switching process has countable states and is unobservable. The\nstochastic control problem is formulated under partial observations of asset\nprices and sequential default events. By establishing a martingale\nrepresentation theorem based on incomplete and phasing out filtration, we\nconnect the control problem to a quadratic BSDE with jumps, in which the driver\nterm is non-standard and carries the conditional filter as an\ninfinite-dimensional parameter. By proposing some truncation techniques and\nproving a uniform a priori estimates, we obtain the existence of a solution to\nthe BSDE using the convergence of solutions associated to some truncated BSDEs.\nThe verification theorem can be concluded with the aid of our BSDE results,\nwhich in turn yields the uniqueness of the solution to the BSDE.\n"
    },
    {
        "paper_id": 1905.08042,
        "authors": "Eric Benhamou, David Saltiel, Beatrice Guez, Nicolas Paris",
        "title": "Testing Sharpe ratio: luck or skill?",
        "comments": "56 pages, 44 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sharpe ratio (sometimes also referred to as information ratio) is widely used\nin asset management to compare and benchmark funds and asset managers. It\ncomputes the ratio of the (excess) net return over the strategy standard\ndeviation. However, the elements to compute the Sharpe ratio, namely, the\nexpected returns and the volatilities are unknown numbers and need to be\nestimated statistically. This means that the Sharpe ratio used by funds is\nlikely to be error prone because of statistical estimation errors. In this\npaper, we provide various tests to measure the quality of the Sharpe ratios. By\nquality, we are aiming at measuring whether a manager was indeed lucky of\nskillful. The test assesses this through the statistical significance of the\nSharpe ratio. We not only look at the traditional Sharpe ratio but also compute\na modified Sharpe insensitive to used Capital. We provide various statistical\ntests that can be used to precisely quantify the fact that the Sharpe is\nstatistically significant. We illustrate in particular the number of trades for\na given Sharpe level that provides statistical significance as well as the\nimpact of auto-correlation by providing reference tables that provides the\nminimum required Sharpe ratio for a given time period and correlation. We also\nprovide for a Sharpe ratio of 0.5, 1.0, 1.5 and 2.0 the skill percentage given\nthe auto-correlation level.\n"
    },
    {
        "paper_id": 1905.08444,
        "authors": "Reaz Chowdhury, M. Arifur Rahman, M. Sohel Rahman, M.R.C. Mahdy",
        "title": "Predicting and Forecasting the Price of Constituents and Index of\n  Cryptocurrency Using Machine Learning",
        "comments": "main article along with the supplement article at the end",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124569",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  At present, cryptocurrencies have become a global phenomenon in financial\nsectors as it is one of the most traded financial instruments worldwide.\nCryptocurrency is not only one of the most complicated and abstruse fields\namong financial instruments, but it is also deemed as a perplexing problem in\nfinance due to its high volatility. This paper makes an attempt to apply\nmachine learning techniques on the index and constituents of cryptocurrency\nwith a goal to predict and forecast prices thereof. In particular, the purpose\nof this paper is to predict and forecast the close (closing) price of the\ncryptocurrency index 30 and nine constituents of cryptocurrencies using machine\nlearning algorithms and models so that, it becomes easier for people to trade\nthese currencies. We have used several machine learning techniques and\nalgorithms and compared the models with each other to get the best output. We\nbelieve that our work will help reduce the challenges and difficulties faced by\npeople, who invest in cryptocurrencies. Moreover, the obtained results can play\na major role in cryptocurrency portfolio management and in observing the\nfluctuations in the prices of constituents of cryptocurrency market. We have\nalso compared our approach with similar state of the art works from the\nliterature, where machine learning approaches are considered for predicting and\nforecasting the prices of these currencies. In the sequel, we have found that\nour best approach presents better and competitive results than the best works\nfrom the literature thereby advancing the state of the art. Using such\nprediction and forecasting methods, people can easily understand the trend and\nit would be even easier for them to trade in a difficult and challenging\nfinancial instrument like cryptocurrency.\n"
    },
    {
        "paper_id": 1905.0887,
        "authors": "Claude Kl\\\"ockl, Katharina Gruber, Peter Regner, Sebastian Wehrle,\n  Johannes Schmidt",
        "title": "The perils of automated fitting of datasets: the case of a wind turbine\n  cost model",
        "comments": "Updated for Examples and Counterexamples Submission, In response to\n  referee feedback we have extensively revised and integrated new data\n  (the-windpower.net), updated all figures and made sure that the given wind\n  turbine examples are more numerous, named explicitly and show more clearly\n  unplausible behavior",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rinne et al. conduct an interesting analysis of the impact of wind turbine\ntechnology and land-use on wind power potentials, which allows profound\ninsights into each factors contribution to overall potentials. The paper\npresents a detailed model of site-specific wind turbine investment cost (i.e.\nroad- and grid access costs) complemented by a model used to estimate\nsite-independent costs. We believe that propose a cutting edge model of\nsite-specific investment costs. However, the site-independent cost model is\nflawed in our opinion. This flaw most likely does not impact the results\npresented in the paper, although we expect a considerable generalization error.\nThus the application of the wind turbine cost model in other contexts may lead\nto unreasonable results. More generally, the derivation of the wind turbine\ncost model serves as an example of how applications of automated regression\nanalysis can go wrong.\n"
    },
    {
        "paper_id": 1905.09474,
        "authors": "Ludovic Gouden\\`ege, Andrea Molent, Antonino Zanette",
        "title": "Machine Learning for Pricing American Options in High-Dimensional\n  Markovian and non-Markovian models",
        "comments": "arXiv admin note: text overlap with arXiv:1903.11275",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose two efficient techniques which allow one to compute\nthe price of American basket options. In particular, we consider a basket of\nassets that follow a multi-dimensional Black-Scholes dynamics. The proposed\ntechniques, called GPR Tree (GRP-Tree) and GPR Exact Integration (GPR-EI), are\nboth based on Machine Learning, exploited together with binomial trees or with\na closed formula for integration. Moreover, these two methods solve the\nbackward dynamic programming problem considering a Bermudan approximation of\nthe American option. On the exercise dates, the value of the option is first\ncomputed as the maximum between the exercise value and the continuation value\nand then approximated by means of Gaussian Process Regression. The two methods\nmainly differ in the approach used to compute the continuation value: a single\nstep of binomial tree or integration according to the probability density of\nthe process. Numerical results show that these two methods are accurate and\nreliable in handling American options on very large baskets of assets. Moreover\nwe also consider the rough Bergomi model, which provides stochastic volatility\nwith memory. Despite this model is only bidimensional, the whole history of the\nprocess impacts on the price, and handling all this information is not obvious\nat all. To this aim, we present how to adapt the GPR-Tree and GPR-EI methods\nand we focus on pricing American options in this non-Markovian framework.\n"
    },
    {
        "paper_id": 1905.09552,
        "authors": "Andrea Bastianin and Paolo Castelnovo and Massimo Florio and Anna\n  Giunta",
        "title": "Technological Learning and Innovation Gestation Lags at the Frontier of\n  Science: from CERN Procurement to Patent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper contributes to the literature on the impact of Big Science Centres\non technological innovation. We exploit a unique dataset with information on\nCERN's procurement orders to study the collaborative innovation process between\nCERN and its industrial partners. After a qualitative discussion of case\nstudies, survival and count data models are estimated; the impact of CERN\nprocurement on suppliers' innovation is captured by the number of patent\napplications. The fact that firms in our sample received their first order over\na long time span (1995-2008) delivers a natural partition of industrial\npartners into \"suppliers\" and \"not yet suppliers\". This allows estimating the\nimpact of CERN on the hazard to file a patent for the first time and on the\nnumber of patent applications, as well as the time needed for these effects to\nshow up. We find that a \"CERN effect\" does exist: being an industrial partner\nof CERN is associated with an increase in the hazard to file a patent for the\nfirst time and in the number of patent applications. These effects require a\nsignificant \"gestation lag\" in the range of five to eight years, pointing to a\nrelatively slow process of absorption of new ideas.\n"
    },
    {
        "paper_id": 1905.09596,
        "authors": "Laura Ballotta, Ernst Eberlein, Thorsten Schmidt, and Raghid\n  Zeineddine",
        "title": "Variable annuities in a L\\'evy-based hybrid model with surrender risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a market consistent valuation framework for variable\nannuities with guaranteed minimum accumulation benefit, death benefit and\nsurrender benefit features. The setup is based on a hybrid model for the\nfinancial market and uses time-inhomogeneous L\\'evy processes as risk drivers.\nFurther, we allow for dependence between financial and surrender risks. Our\nmodel leads to explicit analytical formulas for the quantities of interest, and\npractical and efficient numerical procedures for the evaluation of these\nformulas. We illustrate the tractability of this approach by means of a\ndetailed sensitivity analysis of the price of the variable annuity and its\ncomponents with respect to the model parameters. The results highlight the role\nplayed by the surrender behaviour and the importance of its appropriate\nmodelling.\n"
    },
    {
        "paper_id": 1905.09633,
        "authors": "Min Shu, Wei Zhu",
        "title": "Diagnosis and Prediction of the 2015 Chinese Stock Market Bubble",
        "comments": "20 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we perform a novel analysis of the 2015 financial bubble in\nthe Chinese stock market by calibrating the Log Periodic Power Law Singularity\n(LPPLS) model to two important Chinese stock indices, SSEC and SZSC, from early\n2014 to June 2015. The back tests of the 2015 Chinese stock market bubbles\nindicates that the LPPLS model can readily detect the bubble behavior of the\nfaster-than-exponential increase corrected by the accelerating\nlogarithm-periodic oscillations in the 2015 Chinese Stock market. The existence\nof log-periodicity is detected by applying the Lomb spectral analysis on the\ndetrended residuals. The Ornstein-Uhlenbeck property and the stationarity of\nthe LPPLS fitting residuals are confirmed by the two Unit-root tests\n(Philips-Perron test and Dickery-Fuller test). According to our analysis, the\nactual critical day t_c can be well predicted by the LPPLS model as far back as\ntwo months before the actual bubble crash. Compared to the traditional\noptimization method used in the LPPLS model, we find the covariance matrix\nadaptation evolution strategy (CMA-ES) to have a significantly lower\ncomputation cost, and thus recommend this as a better alternative algorithm for\nLPPLS model fit. Furthermore, in the LPPLS fitting with expanding windows, the\ngap (tc -t2) shows a significant decrease when the end day t2 approaches the\nactual bubble crash time. The change rate of the gap (tc-t2) may be used as an\nadditional indicator besides the key indicator tc to improve the prediction of\nbubble burst.\n"
    },
    {
        "paper_id": 1905.0964,
        "authors": "Min Shu, Wei Zhu",
        "title": "Detection of Chinese Stock Market Bubbles with LPPLS Confidence\n  Indicator",
        "comments": "20 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124892",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an advance bubble detection methodology based on the Log Periodic\nPower Law Singularity (LPPLS) confidence indicator for the early causal\nidentification of positive and negative bubbles in the Chinese stock market\nusing the daily data on the Shanghai Shenzhen CSI 300 stock market index from\nJanuary 2002 through April 2018. We account for the damping condition of LPPLS\nmodel in the search space and implement the stricter filter conditions for the\nqualification of the valid LPPLS fits by taking account of the maximum relative\nerror, performing the Lomb log-periodic test of the detrended residual, and\nunit-root tests of the logarithmic residual based on both the Phillips-Perron\ntest and Dickey-Fuller test to improve the performance of LPPLS confidence\nindicator. Our analysis shows that the LPPLS detection strategy diagnoses the\npositive bubbles and negative bubbles corresponding to well-known historical\nevents, implying the detection strategy based on the LPPLS confidence indicator\nhas an outstanding performance to identify the bubbles in advance. We find that\nthe probability density distribution of the estimated beginning time of bubbles\nappears to be skewed and the mass of the distribution is concentrated on the\narea where the price starts to have an obvious super-exponentially growth. This\nstudy is the first work in the literature that identifies the existence of\nbubbles in the Chinese stock market using the daily data of CSI 300 index with\nthe advance bubble detection methodology of LPPLS confidence indicator. We have\nshown that it is possible to detect the potential positive and negative bubbles\nand crashes ahead of time, which in turn limits the bubble sizes and eventually\nminimizes the damages from the bubble crash.\n"
    },
    {
        "paper_id": 1905.09647,
        "authors": "Min Shu, Wei Zhu",
        "title": "Real-time Prediction of Bitcoin Bubble Crashes",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124477",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the past decade, Bitcoin as an emerging asset class has gained widespread\npublic attention because of their extraordinary returns in phases of extreme\nprice growth and their unpredictable massive crashes. We apply the log-periodic\npower law singularity (LPPLS) confidence indicator as a diagnostic tool for\nidentifying bubbles using the daily data on Bitcoin price in the past two\nyears. We find that the LPPLS confidence indicator based on the daily Bitcoin\nprice data fails to provide effective warnings for detecting the bubbles when\nthe Bitcoin price suffers from a large fluctuation in a short time, especially\nfor positive bubbles. In order to diagnose the existence of bubbles and\naccurately predict the bubble crashes in the cryptocurrency market, this study\nproposes an adaptive multilevel time series detection methodology based on the\nLPPLS model and finer (than daily) timescale for the Bitcoin price data. We\nadopt two levels of time series, 1 hour and 30 minutes, to demonstrate the\nadaptive multilevel time series detection methodology. The results show that\nthe LPPLS confidence indicator based on this new method is an outstanding\ninstrument to effectively detect the bubbles and accurately forecast the bubble\ncrashes, even if a bubble exists in a short time. In addition, we discover that\nthe short-term LPPLS confidence indicator highly sensitive to the extreme\nfluctuations of Bitcoin price can provide some useful insights into the bubble\nstatus on a shorter time scale - on a day to week scale, and the long-term\nLPPLS confidence indicator has a stable performance in terms of effectively\nmonitoring the bubble status on a longer time scale - on a week to month scale.\nThe adaptive multilevel time series detection methodology can provide real-time\ndetection of bubbles and advanced forecast of crashes to warn of the imminent\nrisk.\n"
    },
    {
        "paper_id": 1905.10164,
        "authors": "David G Maher",
        "title": "How big should a Stress Shock be?",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stress shocks are often calculated as multiples of the standard deviation of\na history set. This paper investigates how many standard deviations are\nrequired to guarantee that this shock exceeds any observation within the\nhistory set, given the additional constraint of kurtosis. The results of this\nanalysis are then used to validate the shocks produced by some stress test\nmodels, in particular that of Brace-Lauer-Rado. A secondary application of our\nresults is to investigate three known extensions of Chebyshev's Inequality\nwhere the kurtosis is known. It is found that our results give a tighter bound\nthan the well-known inequalities.\n"
    },
    {
        "paper_id": 1905.10737,
        "authors": "Ranjiva Munasinghe and Leslie Kanthan and Pathum Kossinna",
        "title": "Revisiting Feller Diffusion: Derivation and Simulation",
        "comments": "18 pages, 3 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simpler derivation of the probability density function of Feller\nDiffusion using the Fourier Transform and solving the resulting equation via\nthe Method of Characteristics. We also discuss simulation algorithms and\nconfirm key properties related to hitting time probabilities via the\nsimulation.\n"
    },
    {
        "paper_id": 1905.10806,
        "authors": "Domenico Di Gangi, Giacomo Bormetti, Fabrizio Lillo",
        "title": "Score-Driven Exponential Random Graphs: A New Class of Time-Varying\n  Parameter Models for Dynamical Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the increasing abundance of data describing real-world networks\nthat exhibit dynamical features, we propose an extension of the Exponential\nRandomGraph Models (ERGMs) that accommodates the time variation of its\nparameters. Inspired by the fast growing literature on Dynamic Conditional\nScore-driven models each parameter evolves according to an updating rule driven\nby the score of the ERGM distribution. We demonstrate the flexibility of the\nscore-driven ERGMs (SD-ERGMs), both as data generating processes and as\nfilters, and we show the advantages of the dynamic version with respect to the\nstatic one. We discuss two applications to time-varying networks from financial\nand political systems. First, we consider the prediction of future links in the\nItalian inter-bank credit network. Second, we show that the SD-ERGM allows to\ndiscriminate between static or time-varying parameters when used to model the\ndynamics of the US congress co-voting network.\n"
    },
    {
        "paper_id": 1905.11004,
        "authors": "Toomas Hinnosaar",
        "title": "Contest Architecture with Public Disclosures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study optimal disclosure policies in sequential contests. A contest\ndesigner chooses at which periods to publicly disclose the efforts of previous\ncontestants. I provide results for a wide range of possible objectives for the\ncontest designer. While different objectives involve different trade-offs, I\nshow that under many circumstances the optimal contest is one of the three\nbasic contest structures widely studied in the literature: simultaneous,\nfirst-mover, or sequential contest.\n"
    },
    {
        "paper_id": 1905.11328,
        "authors": "Francesca Biagini and Alessandro Gnoatto and Immacolata Oliva",
        "title": "A unified approach to xVA with CSA discounting and initial margin",
        "comments": "37 pages. Structure of the paper revised",
        "journal-ref": "SIAM Journal on Financial Mathematics (2021) forthcoming",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend the existing literature on xVA along three\ndirections. First, we enhance current BSDE-based xVA frameworks to include\ninitial margin in presence of defaults. Next, we solve the consistency problem\nthat arises when the front-office desk of the bank uses trade-specific discount\ncurves (CSA discounting) which differ from the discount rate adopted by the xVA\ndesk. Finally, we clarify the impact of aggregation of several sub-portfolios\nof trades on the xVA-valuation of the resulting global portfolio and study\nrelated non-linearity effects.\n"
    },
    {
        "paper_id": 1905.11486,
        "authors": "Rico Krueger and Taha H. Rashidi and Vinayak V. Dixit",
        "title": "Autonomous Driving and Residential Location Preferences: Evidence from a\n  Stated Choice Survey",
        "comments": null,
        "journal-ref": "Transportation Research Part C: Emerging Technologies, Volume 108,\n  November 2019, Pages 255-268",
        "doi": "10.1016/j.trc.2019.09.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The literature suggests that autonomous vehicles (AVs) may drastically change\nthe user experience of private automobile travel by allowing users to engage in\nproductive or relaxing activities while travelling. As a consequence, the\ngeneralised cost of car travel may decrease, and car users may become less\nsensitive to travel time. By facilitating private motorised mobility, AVs may\neventually impact land use and households' residential location choices. This\npaper seeks to advance the understanding of the potential impacts of AVs on\ntravel behaviour and land use by investigating stated preferences for\ncombinations of residential locations and travel options for the commute in the\ncontext of autonomous automobile travel. Our analysis draws from a stated\npreference survey, which was completed by 512 commuters from the Sydney\nmetropolitan area in Australia and provides insights into travel time\nvaluations in a long-term decision-making context. For the analysis of the\nstated choice data, mixed logit models are estimated. Based on the empirical\nresults, no changes in the valuation of travel time due to the advent of AVs\nshould be expected. However, given the hypothetical nature of the stated\npreference survey, the results may be affected by methodological limitations.\n"
    },
    {
        "paper_id": 1905.11606,
        "authors": "Milad Ghasri, Ali Ardeshiri, Taha Rashidi",
        "title": "Perceived Advantage in Perspective Application of Integrated Choice and\n  Latent Variable Model to Capture Electric Vehicles Perceived Advantage from\n  Consumers Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Relative advantage, or the degree to which a new technology is perceived to\nbe better over the existing technology it supersedes, has a significant impact\non individuals decision of adopting to the new technology. This paper\ninvestigates the impact of electric vehicles perceived advantage over the\nconventional internal combustion engine vehicles, from consumers perspective,\non their decision to select electric vehicles. Data is obtained from a stated\npreference survey from 1176 residents in New South Wales, Australia. The\ncollected data is used to estimate an integrated choice and latent variable\nmodel of electric vehicle choice, which incorporates the perceived advantage of\nelectric vehicles in the form of latent variables in the utility function. The\ndesign of the electric vehicle, impact on the environment, and safety are three\nidentified advantages from consumers point of view. The model is used to\nsimulate the effectiveness of various policies to promote electric vehicles on\ndifferent cohorts. Rebate on the purchase price is found to be the most\neffective strategy to promote electric vehicles adoption.\n"
    },
    {
        "paper_id": 1905.11782,
        "authors": "Daniel Lacker, Agathe Soret",
        "title": "Many-player games of optimal consumption and investment under relative\n  performance criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a portfolio optimization problem for competitive agents with CRRA\nutilities and a common finite time horizon. The utility of an agent depends not\nonly on her absolute wealth and consumption but also on her relative wealth and\nconsumption when compared to the averages among the other agents. We derive a\nclosed form solution for the $n$-player game and the corresponding mean field\ngame. This solution is unique in the class of equilibria with constant\ninvestment and continuous time-dependent consumption, both independent of the\nwealth of the agent. Compared to the classical Merton problem with one agent,\nthe competitive model exhibits a wide range of highly nonlinear and\nnon-monotone dependence on the agents' risk tolerance and competitiveness\nparameters. Counter-intuitively, competitive agents with high risk tolerance\nmay behave like non-competitive agents with low risk tolerance.\n"
    },
    {
        "paper_id": 1905.11842,
        "authors": "C\\'ecile Bastidon, Antoine Parent, Pablo Jensen, Patrice Abry, Pierre\n  Borgnat",
        "title": "Graph-based era segmentation of international financial integration",
        "comments": "18 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.122877",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assessing world-wide financial integration constitutes a recurrent challenge\nin macroeconometrics, often addressed by visual inspections searching for data\npatterns. Econophysics literature enables us to build complementary,\ndata-driven measures of financial integration using graphs. The present\ncontribution investigates the potential and interests of a novel 3-step\napproach that combines several state-of-the-art procedures to i) compute\ngraph-based representations of the multivariate dependence structure of asset\nprices time series representing the financial states of 32 countries world-wide\n(1955-2015); ii) compute time series of 5 graph-based indices that characterize\nthe time evolution of the topologies of the graph; iii) segment these time\nevolutions in piece-wise constant eras, using an optimization framework\nconstructed on a multivariate multi-norm total variation penalized functional.\nThe method shows first that it is possible to find endogenous stable eras of\nworld-wide financial integration. Then, our results suggest that the most\nrelevant globalization eras would be based on the historical patterns of global\ncapital flows, while the major regulatory events of the 1970s would only appear\nas a cause of sub-segmentation.\n"
    },
    {
        "paper_id": 1905.11905,
        "authors": "Ingolf G.A. Pernice, Sebastian Henningsen, Roman Proskalovich, Martin\n  Florian, Hermann Elendner, Bj\\\"orn Scheuermann",
        "title": "Monetary Stabilization in Cryptocurrencies - Design Approaches and Open\n  Questions",
        "comments": "Accepted at IEEE Crypto Valley Conference on Blockchain Technology\n  (CVCBT) 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price volatility of cryptocurrencies is often cited as a major hindrance\nto their wide-scale adoption. Consequently, during the last two years, multiple\nso called stablecoins have surfaced---cryptocurrencies focused on maintaining\nstable exchange rates. In this paper, we systematically explore and analyze the\nstablecoin landscape. Based on a survey of 24 specific stablecoin projects, we\ngo beyond individual coins for extracting general concepts and approaches. We\ncombine our findings with learnings from classical monetary policy, resulting\nin a comprehensive taxonomy of cryptocurrency stabilization. We use our\ntaxonomy to highlight the current state of development from different\nperspectives and show blank spots. For instance, while over 91% of projects\npromote 1-to-1 stabilization targets to external assets, monetary policy\nliterature suggests that the smoothing of short term volatility is often a more\nsustainable alternative. Our taxonomy bridges computer science and economics,\nfostering the transfer of expertise. For example, we find that 38% of the\nreviewed projects use a combination of exchange rate targeting and specific\nstabilization techniques that can render them vulnerable to speculative\neconomic attacks - an avoidable design flaw.\n"
    },
    {
        "paper_id": 1905.12104,
        "authors": "Jaelle Scheuerman, Jason L. Harman, Nicholas Mattei, and K. Brent\n  Venable",
        "title": "Heuristics in Multi-Winner Approval Voting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many real world situations, collective decisions are made using voting.\nMoreover, scenarios such as committee or board elections require voting rules\nthat return multiple winners. In multi-winner approval voting (AV), an agent\nmay vote for as many candidates as they wish. Winners are chosen by tallying up\nthe votes and choosing the top-$k$ candidates receiving the most votes. An\nagent may manipulate the vote to achieve a better outcome by voting in a way\nthat does not reflect their true preferences. In complex and uncertain\nsituations, agents may use heuristics to strategize, instead of incurring the\nadditional effort required to compute the manipulation which most favors them.\nIn this paper, we examine voting behavior in multi-winner approval voting\nscenarios with complete information. We show that people generally manipulate\ntheir vote to obtain a better outcome, but often do not identify the optimal\nmanipulation. Instead, voters tend to prioritize the candidates with the\nhighest utilities. Using simulations, we demonstrate the effectiveness of these\nheuristics in situations where agents only have access to partial information.\n"
    },
    {
        "paper_id": 1905.12431,
        "authors": "Lorella Fatone and Francesca Mariani",
        "title": "An assets-liabilities dynamical model of banking system and systemic\n  risk governance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of governing systemic risk in an assets-liabilities\ndynamical model of banking system. In the model considered each bank is\nrepresented by its assets and its liabilities.The capital reserves of a bank\nare the difference between assets and liabilities of the bank. A bank is\nsolvent when its capital reserves are greater or equal to zero otherwise the\nbank is failed.The banking system dynamics is defined by an initial value\nproblem for a system of stochastic differential equations whose independent\nvariable is time and whose dependent variables are the assets and the\nliabilities of the banks.The banking system model presented generalizes those\ndiscussed in [4],[3] and describes a homogeneous population of banks. The main\nfeatures of the model are a cooperation mechanism among banks and the\npossibility of the (direct) intervention of the monetary authority in the\nbanking system dynamics. We call systemic risk or systemic event in a bounded\ntime interval the fact that in that time interval at least a given fraction of\nthe banks fails. The probability of systemic risk in a bounded time interval is\nevaluated using statistical simulation. The systemic risk governance pursues\nthe goal of keeping the probability of systemic risk in a bounded time interval\nbetween two given thresholds.The monetary authority is responsible for the\nsystemic risk governance.The governance consists in the choice of the assets\nand of the liabilities of a kind of \"ideal bank\" as functions of time and in\nthe choice of the rules that regulate the cooperation mechanism among\nbanks.These rules are obtained solving an optimal control problem for the\npseudo mean field approximation of the banking system model. The governance\ninduces the banks of the system to behave like the \"ideal bank\". Shocks acting\non the assets or on the liabilities of the banks are simulated.\n"
    },
    {
        "paper_id": 1905.12705,
        "authors": "Salvatore Corrente and Ana Garcia-Bernabeu and Salvatore Greco and\n  Teemu Makkonen",
        "title": "Robust measurement of innovation performances in Europe with a hierarchy\n  of interacting composite indicators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For long time the measurement of innovation has been in the forefront of\npolicy makers' and researchers' agenda worldwide. Therefore, there is an\nongoing debate about which indicators should be used to measure innovation.\nRecent approaches have favoured the use of composite innovation indicators.\nHowever, there is no consensus about the appropriate methodology to aggregate\nthe varying dimensions of innovation into a single summary indicator. One of\nthe best known examples of composite innovation indicators is the European\nInnovation Scoreboard (EIS). It is a relevant tool for benchmarking innovation\nin Europe. Still, the EIS lacks a proper scheme for weighting the included\nindicators according to their relative importance. In this context, we propose\nan appraisal methodology permitting to take into consideration the interaction\nof criteria and robustness concerns related to the elicitation of the weights\nassigned to the elementary indicators. With this aim, we apply the\nhierarchical-SMAA-Choquet integral approach. This integrated multicriteria\ndecision making (MCDM) method helps the users to rank and benchmark countries'\ninnovation performance taking into account the importance and interaction of\ncriteria assigned by themselves, rather than equal weights or weights\nexogenously fixed by external experts.\n"
    },
    {
        "paper_id": 1905.13281,
        "authors": "Pedro Cavalcante Oliveira and Daniel Duque",
        "title": "Labor Market Outcomes and Early Schooling: Evidence from School Entry\n  Policies Using Exact Date of Birth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a rich, census-like Brazilian dataset containing information on\nspatial mobility, schooling, and income in which we can link children to\nparents to assess the impact of early education on several labor market\noutcomes. Brazilian public primary schools admit children up to one year\nyounger than the national minimum age to enter school if their birthday is\nbefore an arbitrary threshold, causing an exogenous variation in schooling at\nadulthood. Using a Regression Discontinuity Design, we estimate one additional\nyear of schooling increases labor income in 25.8% - almost twice as large as\nestimated using mincerian models. Around this cutoff there is also a gap of\n9.6% on the probability of holding a college degree in adulthood, with which we\nestimate the college premium and find a 201% increase in labor income. We test\nthe robustness of our estimates using placebo variables, alternative model\nspecifcations and McCrary Density Tests.\n"
    },
    {
        "paper_id": 1905.13407,
        "authors": "Min Huang, Guo Luo",
        "title": "A simple and efficient numerical method for pricing discretely monitored\n  early-exercise options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple, fast, and accurate method for pricing a variety of\ndiscretely monitored options in the Black-Scholes framework, including\nautocallable structured products, single and double barrier options, and\nBermudan options. The method is based on a quadrature technique, and it employs\nonly elementary calculations and a fixed one-dimensional uniform grid. The\nconvergence rate is $O(1/N^4)$ and the complexity is $O(MN\\log N)$, where $N$\nis the number of grid points and $M$ is the number of observation dates.\n"
    },
    {
        "paper_id": 1905.13425,
        "authors": "Xing Yan, Qi Wu, Wen Zhang",
        "title": "Cross-sectional Learning of Extremal Dependence among Financial Assets",
        "comments": null,
        "journal-ref": "Advances in Neural Information Processing Systems, pages\n  3852-3862, 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel probabilistic model to facilitate the learning of\nmultivariate tail dependence of multiple financial assets. Our method allows\none to construct from known random vectors, e.g., standard normal,\nsophisticated joint heavy-tailed random vectors featuring not only distinct\nmarginal tail heaviness, but also flexible tail dependence structure. The\nnovelty lies in that pairwise tail dependence between any two dimensions is\nmodeled separately from their correlation, and can vary respectively according\nto its own parameter rather than the correlation parameter, which is an\nessential advantage over many commonly used methods such as multivariate $t$ or\nelliptical distribution. It is also intuitive to interpret, easy to track, and\nsimple to sample comparing to the copula approach. We show its flexible tail\ndependence structure through simulation. Coupled with a GARCH model to\neliminate serial dependence of each individual asset return series, we use this\nnovel method to model and forecast multivariate conditional distribution of\nstock returns, and obtain notable performance improvements in multi-dimensional\ncoverage tests. Besides, our empirical finding about the asymmetry of tails of\nthe idiosyncratic component as well as the market component is interesting and\nworth to be well studied in the future.\n"
    },
    {
        "paper_id": 1905.13508,
        "authors": "Margarita Baltakien\\.e, K\\k{e}stutis Baltakys, Juho Kanniainen, Dino\n  Pedreschi and Fabrizio Lillo",
        "title": "Clusters of investors around Initial Public Offering",
        "comments": null,
        "journal-ref": "Palgrave Commun 5, 129 (2019)",
        "doi": "10.1057/s41599-019-0342-6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The complex networks approach has been gaining popularity in analysing\ninvestor behaviour and stock markets, but within this approach, initial public\nofferings (IPO) have barely been explored. We fill this gap in the literature\nby analysing investor clusters in the first two years after the IPO filing in\nthe Helsinki Stock Exchange by using a statistically validated network method\nto infer investor links based on the co-occurrences of investors' trade timing\nfor 69 IPO stocks. Our findings show that a rather large part of statistically\nsimilar network structures form in different securities and persist in time for\nmature and IPO companies. We also find evidence of institutional herding.\n"
    },
    {
        "paper_id": 1905.13645,
        "authors": "Pascal Michaillat, Emmanuel Saez",
        "title": "Resolving New Keynesian Anomalies with Wealth in the Utility Function",
        "comments": null,
        "journal-ref": "Review of Economics and Statistics 103 (2021) 197-215",
        "doi": "10.1162/rest_a_00893",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  At the zero lower bound, the New Keynesian model predicts that output and\ninflation collapse to implausibly low levels, and that government spending and\nforward guidance have implausibly large effects. To resolve these anomalies, we\nintroduce wealth into the utility function; the justification is that wealth is\na marker of social status, and people value status. Since people partly save to\naccrue social status, the Euler equation is modified. As a result, when the\nmarginal utility of wealth is sufficiently large, the dynamical system\nrepresenting the zero-lower-bound equilibrium transforms from a saddle to a\nsource---which resolves all the anomalies.\n"
    },
    {
        "paper_id": 1905.1366,
        "authors": "Dmitry Arkhangelsky, Vasily Korovkin",
        "title": "On Policy Evaluation with Aggregate Time-Series Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an estimator for applications where the variable of interest is\nendogenous and researchers have access to aggregate instruments. Our method\naddresses the critical identification challenge -- unobserved confounding,\nwhich renders conventional estimators invalid. Our proposal relies on a new\ndata-driven aggregation scheme that eliminates the unobserved confounders. We\nillustrate the advantages of our algorithm using data from Nakamura and\nSteinsson (2014) study of local fiscal multipliers. We introduce a finite\npopulation model with aggregate uncertainty to analyze our estimator. We\nestablish conditions for consistency and asymptotic normality and show how to\nuse our estimator to conduct valid inference.\n"
    },
    {
        "paper_id": 1905.13711,
        "authors": "Davide Cellai and Trevor Fitzpatrick",
        "title": "The Network Effect in Credit Concentration Risk",
        "comments": "22 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Measurement and management of credit concentration risk is critical for banks\nand relevant for micro-prudential requirements. While several methods exist for\nmeasuring credit concentration risk within institutions, the systemic effect of\ndifferent institutions' exposures to the same counterparties has been less\nexplored so far. In this paper, we propose a measure of the systemic credit\nconcentration risk that arises because of common exposures between different\ninstitutions within a financial system. This approach is based on a network\nmodel that describes the effect of overlapping portfolios. This network metric\nis applied to synthetic and real world data to illustrate that the effect of\ncommon exposures is not fully reflected in single portfolio concentration\nmeasures. It also allows to quantify several aspects of the interplay between\ninterconnectedness and credit risk. Using this network measure, we formulate an\nanalytical approximation for the additional capital requirement corresponding\nto the systemic risk arising from credit concentration interconnectedness. Our\nmethodology also avoids double counting between the granularity adjustment and\nthe common exposure adjustment. Although approximated, this common exposure\nadjustment is able to capture, with only two parameters, an aspect of systemic\nrisk that can extend single portfolios view to a system-wide one.\n"
    },
    {
        "paper_id": 1906.00059,
        "authors": "Jozef Barunik and Cathy Yi-Hsuan Chen and Jan Vecer",
        "title": "Sentiment-Driven Stochastic Volatility Model: A High-Frequency Textual\n  Tool for Economists",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose how to quantify high-frequency market sentiment using\nhigh-frequency news from NASDAQ news platform and support vector machine\nclassifiers. News arrive at markets randomly and the resulting news sentiment\nbehaves like a stochastic process. To characterize the joint evolution of\nsentiment, price, and volatility, we introduce a unified continuous-time\nsentiment-driven stochastic volatility model. We provide closed-form formulas\nfor moments of the volatility and news sentiment processes and study the news\nimpact. Further, we implement a simulation-based method to calibrate the\nparameters. Empirically, we document that news sentiment raises the threshold\nof volatility reversion, sustaining high market volatility.\n"
    },
    {
        "paper_id": 1906.00553,
        "authors": "Martin Obschonka, David B. Audretsch",
        "title": "Artificial Intelligence and Big Data in Entrepreneurship: A New Era Has\n  Begun",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11187-019-00202-4",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While the disruptive potential of artificial intelligence (AI) and Big Data\nhas been receiving growing attention and concern in a variety of research and\napplication fields over the last few years, it has not received much scrutiny\nin contemporary entrepreneurship research so far. Here we present some\nreflections and a collection of papers on the role of AI and Big Data for this\nemerging area in the study and application of entrepreneurship research. While\nbeing mindful of the potentially overwhelming nature of the rapid progress in\nmachine intelligence and other Big Data technologies for contemporary\nstructures in entrepreneurship research, we put an emphasis on the reciprocity\nof the co-evolving fields of entrepreneurship research and practice. How can AI\nand Big Data contribute to a productive transformation of the research field\nand the real-world phenomena (e.g., 'smart entrepreneurship')? We also discuss,\nhowever, ethical issues as well as challenges around a potential contradiction\nbetween entrepreneurial uncertainty and rule-driven AI rationality. The\neditorial gives researchers and practitioners orientation and showcases avenues\nand examples for concrete research in this field. At the same time, however, it\nis not unlikely that we will encounter unforeseeable and currently inexplicable\ndevelopments in the field soon. We call on entrepreneurship scholars,\neducators, and practitioners to proactively prepare for future scenarios.\n"
    },
    {
        "paper_id": 1906.00573,
        "authors": "Steven E. Pav",
        "title": "Conditional inference on the asset with maximum Sharpe ratio",
        "comments": "code and latex source available from github repo,\n  github.com/shabbychef/maxsharpe",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We apply the procedure of Lee et al. to the problem of performing inference\non the signal-noise ratio of the asset which displays maximum sample Sharpe\nratio over a set of possibly correlated assets. We find a multivariate analogue\nof the commonly used approximate standard error of the Sharpe ratio to use in\nthis conditional estimation procedure. We also consider several alternative\nprocedures, including the simple Bonferroni correction for multiple hypothesis\ntesting, which we fix for the case of positive common correlation among assets,\nthe chi-bar square test against one-sided alternatives, Follman's test, and\nHansen's asymptotic adjustments.\n  Testing indicates the conditional inference procedure achieves nominal type I\nrate, and does not appear to suffer from non-normality of returns. The\nconditional estimation test has low power under the alternative where there is\nlittle spread in the signal-noise ratios of the assets, and high power under\nthe alternative where a single asset has high signal-noise ratio. Unlike the\nalternative procedures, it appears to enjoy rejection probabilities montonic in\nthe signal-noise ratio of the selected asset.\n"
    },
    {
        "paper_id": 1906.0092,
        "authors": "Mathias Barkhagen, Brian Fleming, Sergio Garcia Quiles, Jacek Gondzio,\n  Joerg Kalcsics, Jens Kroeske, Sotirios Sabanis and Arne Staal",
        "title": "Optimising portfolio diversification and dimensionality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new framework for portfolio diversification is introduced which goes beyond\nthe classical mean-variance approach and portfolio allocation strategies such\nas risk parity. It is based on a novel concept called portfolio dimensionality\nthat connects diversification to the non-Gaussianity of portfolio returns and\ncan typically be defined in terms of the ratio of risk measures which are\nhomogenous functions of equal degree. The latter arises naturally due to our\nrequirement that diversification measures should be leverage invariant. We\nintroduce this new framework and argue the benefits relative to existing\nmeasures of diversification in the literature, before addressing the question\nof optimizing diversification or, equivalently, dimensionality. Maximising\nportfolio dimensionality leads to highly non-trivial optimization problems with\nobjective functions which are typically non-convex and potentially have\nmultiple local optima. Two complementary global optimization algorithms are\nthus presented. For problems of moderate size and more akin to asset allocation\nproblems, a deterministic Branch and Bound algorithm is developed, whereas for\nproblems of larger size a stochastic global optimization algorithm based on\nGradient Langevin Dynamics is given. We demonstrate analytically and through\nnumerical experiments that the framework reflects the desired properties often\ndiscussed in the literature.\n"
    },
    {
        "paper_id": 1906.00946,
        "authors": "Alex Garivaltis",
        "title": "The Laws of Motion of the Broker Call Rate in the United States",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": "International Journal of Financial Studies, 7(4), 56 (2019)",
        "doi": "10.3390/ijfs7040056",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, which is the third installment of the author's trilogy on\nmargin loan pricing, we analyze $1,367$ monthly observations of the U.S. broker\ncall money rate, which is the interest rate at which stock brokers can borrow\nto fund their margin loans to retail clients. We describe the basic features\nand mean-reverting behavior of this series and juxtapose the\nempirically-derived laws of motion with the author's prior theories of margin\nloan pricing (Garivaltis 2019a-b). This allows us to derive stochastic\ndifferential equations that govern the evolution of the margin loan interest\nrate and the leverage ratios of sophisticated brokerage clients (namely,\ncontinuous time Kelly gamblers). Finally, we apply Merton's (1974) arbitrage\ntheory of corporate liability pricing to study theoretical constraints on the\nrisk premia that could be generated in the market for call money. Apparently,\nif there is no arbitrage in the U.S. financial markets, the implication is that\nthe total volume of call loans must constitute north of $70\\%$ of the value of\nall leveraged portfolios.\n"
    },
    {
        "paper_id": 1906.0096,
        "authors": "Ravi Kashyap",
        "title": "For Whom the Bell (Curve) Tolls: A to F, Trade Your Grade Based on the\n  Net Present Value of Friendships with Financial Incentives",
        "comments": null,
        "journal-ref": "June 2019, The Journal of Private Equity, 22(3), 64-81",
        "doi": "10.3905/jpe.2019.22.3.064",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a possible solution to an unintended consequence of having grades,\ncertificates, rankings and other diversions in the act of transferring\nknowledge; and zoom in specifically to the topic of having grades, on a curve.\nWe conduct a thought experiment, taking a chapter (and some more?) from the\nfinancial markets, (where we trade pollution and what not?), to create a\nmarketplace, where we can trade our grade, similar in structure to the interest\nrate swap. We connect this to broader problems that are creeping up,\nunintentionally, due to artificial labels we are attaching, to ourselves. The\npolicy and philosophical implications of our arguments are to suggest that all\ntrophies that we collect (including certificates, grades, medals etc.) should\nbe viewed as personal equity or private equity (borrowing another widely used\nterm in finance) and we should not use them to determine the outcomes in any\nselection criteria except have a cutoff point: either for jobs, higher studies,\nor, financial scholarships, other than for entertainment or spectator sports.\nWe suggest alternate methods for grading and performance assessment and put\nforth tests for teaching and learning similar to the Turing Test for\nintelligence.\n"
    },
    {
        "paper_id": 1906.01025,
        "authors": "Alex Garivaltis",
        "title": "Two Resolutions of the Margin Loan Pricing Puzzle",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": "Research in Economics (73)2, pp.199-207 (2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper supplies two possible resolutions of Fortune's (2000) margin-loan\npricing puzzle. Fortune (2000) noted that the margin loan interest rates\ncharged by stock brokers are very high in relation to the actual (low) credit\nrisk and the cost of funds. If we live in the Black-Scholes world, the brokers\nare presumably making arbitrage profits by shorting dynamically precise amounts\nof their clients' portfolios. First, we extend Fortune's (2000) application of\nMerton's (1974) no-arbitrage approach to allow for brokers that can only revise\ntheir hedges finitely many times during the term of the loan. We show that\nextremely small differences in the revision frequency can easily explain the\nobserved variation in margin loan pricing. In fact, four additional revisions\nper three-day period serve to explain all of the currently observed\nheterogeneity. Second, we study monopolistic (or oligopolistic) margin loan\npricing by brokers whose clients are continuous-time Kelly gamblers. The broker\nsolves a general stochastic control problem that yields simple and pleasant\nformulas for the optimal interest rate and the net interest margin. If the\nauthor owned a brokerage, he would charge an interest rate of\n$(r+\\nu)/2-\\sigma^2/4$, where $r$ is the cost of funds, $\\nu$ is the\ncompound-annual growth rate of the S&P 500 index, and $\\sigma$ is the\nvolatility.\n"
    },
    {
        "paper_id": 1906.01232,
        "authors": "Yu-Jui Huang, Xiang Yu",
        "title": "Optimal Stopping under Model Ambiguity: a Time-Consistent Equilibrium\n  Approach",
        "comments": null,
        "journal-ref": "Mathematical Finance, Vol. 31 (2021), Issue 3, pp. 979-1012",
        "doi": "10.1111/mafi.12312",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An unconventional approach for optimal stopping under model ambiguity is\nintroduced. Besides ambiguity itself, we take into account how ambiguity-averse\nan agent is. This inclusion of ambiguity attitude, via an $\\alpha$-maxmin\nnonlinear expectation, renders the stopping problem time-inconsistent. We look\nfor subgame perfect equilibrium stopping policies, formulated as fixed points\nof an operator. For a one-dimensional diffusion with drift and volatility\nuncertainty, we show that every equilibrium can be obtained through a\nfixed-point iteration. This allows us to capture much more diverse behavior,\ndepending on an agent's ambiguity attitude, beyond the standard worst-case (or\nbest-case) analysis. In a concrete example of real options valuation under\nvolatility uncertainty, all equilibrium stopping policies, as well as the best\none among them, are fully characterized. It demonstrates explicitly the effect\nof ambiguity attitude on decision making: the more ambiguity-averse, the more\neager to stop -- so as to withdraw from the uncertain environment. The main\nresult hinges on a delicate analysis of continuous sample paths in the\ncanonical space and the capacity theory. To resolve measurability issues, a\ngeneralized measurable projection theorem, new to the literature, is also\nestablished.\n"
    },
    {
        "paper_id": 1906.01241,
        "authors": "Evandro Luquini and Nizam Omar",
        "title": "Kinetic Market Model: An Evolutionary Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research proposes the econophysics kinetic market model as an\nevolutionary algorithm's instance. The immediate results from this proposal is\na new replacement rule for family competition genetic algorithms. It also\nrepresents a starting point to adding evolvable entities to kinetic market\nmodels.\n"
    },
    {
        "paper_id": 1906.01293,
        "authors": "C\\'elestin Coquid\\'e, Jos\\'e Lages, and Dima L. Shepelyansky",
        "title": "Contagion in Bitcoin networks",
        "comments": "12 pages, 6 figures. Paper accepted in 2nd Workshop on Blockchain and\n  Smart Contract Technologies (BSCT 2019), workshop satellite of 22nd\n  International Conference on Business Information Systems (BIS 2019)",
        "journal-ref": "Business Information Systems Workshops. BIS 2019. Lecture Notes in\n  Business Information Processing, vol 373. Springer, Cham",
        "doi": "10.1007/978-3-030-36691-9_18",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct the Google matrices of bitcoin transactions for all year\nquarters during the period of January 11, 2009 till April 10, 2013. During the\nlast quarters the network size contains about 6 million users (nodes) with\nabout 150 million transactions. From PageRank and CheiRank probabilities,\nanalogous to trade import and export, we determine the dimensionless trade\nbalance of each user and model the contagion propagation on the network\nassuming that a user goes bankrupt if its balance exceeds a certain\ndimensionless threshold $\\kappa$. We find that the phase transition takes place\nfor $\\kappa<\\kappa_c\\approx0.1$ with almost all users going bankrupt. For\n$\\kappa>0.55$ almost all users remain safe. We find that even on a distance\nfrom the critical threshold $\\kappa_c$ the top PageRank and CheiRank users, as\na house of cards, rapidly drop to the bankruptcy. We attribute this effect to\nstrong interconnections between these top users which we determine with the\nreduced Google matrix algorithm. This algorithm allows to establish efficiently\nthe direct and indirect interactions between top PageRank users. We argue that\nthis study models the contagion on real financial networks.\n"
    },
    {
        "paper_id": 1906.0132,
        "authors": "Jin Sun, Kevin Fergusson, Eckhard Platen, Pavel V. Shevchenko",
        "title": "Fair Pricing of Variable Annuities with Guarantees under the Benchmark\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the pricing of variable annuities (VAs) with\nguaranteed minimum withdrawal benefits. We consider two pricing approaches, the\nclassical risk-neutral approach and the benchmark approach, and we examine the\nassociated static and optimal behaviors of both the investor and insurer. The\nfirst model considered is the so-called minimal market model, where pricing is\nachieved using the benchmark approach. The benchmark approach was introduced by\nPlaten in 2001 and has received wide acceptance in the finance community. Under\nthis approach, valuing an asset involves determining the minimum-valued\nreplicating portfolio, with reference to the growth optimal portfolio under the\nreal-world probability measure, and it both subsumes classical risk-neutral\npricing as a particular case and extends it to situations where risk-neutral\npricing is impossible. The second model is the Black-Scholes model for the\nequity index, where the pricing of contracts is performed within the\nrisk-neutral framework. Crucially, we demonstrate that when the insurer prices\nand reserves using the Black-Scholes model, while the insured employs a dynamic\nwithdrawal strategy based on the minimal market model, the insurer may be\nunderestimating the value and associated reserves of the contract.\n"
    },
    {
        "paper_id": 1906.01427,
        "authors": "Nick Firoozye and Adriano Koshiyama",
        "title": "Optimal Dynamic Strategies on Gaussian Returns",
        "comments": "Accepted by Journal of Investment Strategies. arXiv admin note: text\n  overlap with arXiv:1905.05023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic trading strategies, in the spirit of trend-following or\nmean-reversion, represent an only partly understood but lucrative and pervasive\narea of modern finance. Assuming Gaussian returns and Gaussian dynamic weights\nor signals, (e.g., linear filters of past returns, such as simple moving\naverages, exponential weighted moving averages, forecasts from ARIMA models),\nwe are able to derive closed-form expressions for the first four moments of the\nstrategy's returns, in terms of correlations between the random signals and\nunknown future returns. By allowing for randomness in the asset-allocation and\nmodelling the interaction of strategy weights with returns, we demonstrate that\npositive skewness and excess kurtosis are essential components of all positive\nSharpe dynamic strategies, which is generally observed empirically; demonstrate\nthat total least squares (TLS) or orthogonal least squares is more appropriate\nthan OLS for maximizing the Sharpe ratio, while canonical correlation analysis\n(CCA) is similarly appropriate for the multi-asset case; derive standard errors\non Sharpe ratios which are tighter than the commonly used standard errors from\nLo; and derive standard errors on the skewness and kurtosis of strategies,\napparently new results. We demonstrate these results are applicable\nasymptotically for a wide range of stationary time-series.\n"
    },
    {
        "paper_id": 1906.01449,
        "authors": "Wenyuan Wang, Ping Chen, Shuanming Li",
        "title": "Generalized Expected Discounted Penalty Function at General Drawdown for\n  L\\'{e}vy Risk Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers an insurance surplus process modeled by a spectrally\nnegative L\\'{e}vy process. Instead of the time of ruin in the traditional\nsetting, we apply the time of drawdown as the risk indicator in this paper. We\nstudy the joint distribution of the time of drawdown, the running maximum at\ndrawdown, the last minimum before drawdown, the surplus before drawdown and the\nsurplus at drawdown (may not be deficit in this case), which generalizes the\nknown results on the classical expected discounted penalty function in Gerber\nand Shiu (1998). The results have semi-explicit expressions in terms of the\n$q$-scale functions and the L\\'{e}vy measure associated with the L\\'{e}vy\nprocess. As applications, the obtained result is applied to recover results in\nthe literature and to obtain new results for the Gerber-Shiu function at ruin\nfor risk processes embedded with a loss-carry-forward taxation system or a\nbarrier dividend strategy. Moreover, numerical examples are provided to\nillustrate the results.\n"
    },
    {
        "paper_id": 1906.01531,
        "authors": "Felipe M. Cardoso, Carlos Gracia-Lazaro, Frederic Moisan, Sanjeev\n  Goyal, Angel Sanchez, and Yamir Moreno",
        "title": "Trading in Complex Networks",
        "comments": "20 pages including a SI file describing the experiment and additional\n  statistical analyses",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global supply networks in agriculture, manufacturing, and services are a\ndefining feature of the modern world. The efficiency and the distribution of\nsurpluses across different parts of these networks depend on choices of\nintermediaries. This paper conducts price formation experiments with human\nsubjects located in large complex networks to develop a better understanding of\nthe principles governing behavior. Our first finding is that prices are larger\nand that trade is significantly less efficient in small-world networks as\ncompared to random networks. Our second finding is that location within a\nnetwork is not an important determinant of pricing. An examination of the price\ndynamics suggests that traders on cheapest -- and hence active -- paths raise\nprices while those off these paths lower them. We construct an agent-based\nmodel (ABM) that embodies this rule of thumb. Simulations of this ABM yield\nmacroscopic patterns consistent with the experimental findings. Finally, we\nextrapolate the ABM on to significantly larger random and small world networks\nand find that network topology remains a key determinant of pricing and\nefficiency.\n"
    },
    {
        "paper_id": 1906.01713,
        "authors": "Paul Jusselin, Thibaut Mastrolia, Mathieu Rosenbaum",
        "title": "Optimal auction duration: A price formation viewpoint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an auction market in which market makers fill the order book\nduring a given time period while some other investors send market orders. We\ndefine the clearing price of the auction as the price maximizing the exchanged\nvolume at the clearing time according to the supply and demand of each market\nparticipants. Then we derive in a semi-explicit form the error made between\nthis clearing price and the efficient price as a function of the auction\nduration. We study the impact of the behavior of market takers on this error.\nTo do so we consider the case of naive market takers and that of rational\nmarket takers playing a Nash equilibrium to minimize their transaction costs.\nWe compute the optimal duration of the auctions for 77 stocks traded on\nEuronext and compare the quality of price formation process under this optimal\nvalue to the case of a continuous limit order book. Continuous limit order\nbooks are found to be usually sub-optimal. However, in term of our metric, they\nonly moderately impair the quality of price formation process. Order of\nmagnitude of optimal auction durations is from 2 to 10 minutes.\n"
    },
    {
        "paper_id": 1906.01923,
        "authors": "Di Wang, Qi Wu, Wen Zhang",
        "title": "Neural Learning of Online Consumer Credit Risk",
        "comments": "49 pages, 11 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper takes a deep learning approach to understand consumer credit risk\nwhen e-commerce platforms issue unsecured credit to finance customers'\npurchase. The \"NeuCredit\" model can capture both serial dependences in\nmulti-dimensional time series data when event frequencies in each dimension\ndiffer. It also captures nonlinear cross-sectional interactions among different\ntime-evolving features. Also, the predicted default probability is designed to\nbe interpretable such that risks can be decomposed into three components: the\nsubjective risk indicating the consumers' willingness to repay, the objective\nrisk indicating their ability to repay, and the behavioral risk indicating\nconsumers' behavioral differences. Using a unique dataset from one of the\nlargest global e-commerce platforms, we show that the inclusion of shopping\nbehavioral data, besides conventional payment records, requires a deep learning\napproach to extract the information content of these data, which turns out\nsignificantly enhancing forecasting performance than the traditional machine\nlearning methods.\n"
    },
    {
        "paper_id": 1906.0198,
        "authors": "Theophile Carniel, Clement Gastaud, Jean-Michel Dalle",
        "title": "The temporal evolution of venture investment strategies in sector space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the sectoral dynamics of startup venture financing. Based on a\ndataset of 52000 start-ups and 110000 funding rounds in the United States from\n2000 to 2017, and by applying both Principal Component Analysis (PCA) and\nTensor Component Analysis (TCA) in sector space, we visualize and measure the\nevolution of the investment strategies of different classes of investors across\nsectors and over time. During the past decade, we observe a coherent evolution\nof early stage investments towards a lower-tech area in sector space,\nassociated with a marked increase in the concentration of investments and with\nthe emergence of a newer class of investors called accelerators. We provide\nevidence for a more recent shift of start-up venture financing away from the\nprevious one.\n"
    },
    {
        "paper_id": 1906.01981,
        "authors": "Qi Wu, Shumin Ma, Cheuk Hang Leung, Wei Liu and Nanbo Peng",
        "title": "Understanding Distributional Ambiguity via Non-robust Chance Constraint",
        "comments": "8 pages, 3 figures, Accepted for publication in ICAIF 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a non-robust interpretation of the distributionally\nrobust optimization (DRO) problem by relating the distributional uncertainties\nto the chance probabilities. Our analysis allows a decision-maker to interpret\nthe size of the ambiguity set, which is often lack of business meaning, through\nthe chance parameters constraining the objective function. We first show that,\nfor general $\\phi$-divergences, a DRO problem is asymptotically equivalent to a\nclass of mean-deviation problems. These mean-deviation problems are not subject\nto uncertain distributions, and the ambiguity radius in the original DRO\nproblem now plays the role of controlling the risk preference of the\ndecision-maker. We then demonstrate that a DRO problem can be cast as a\nchance-constrained optimization (CCO) problem when a boundedness constraint is\nadded to the decision variables. Without the boundedness constraint, the CCO\nproblem is shown to perform uniformly better than the DRO problem, irrespective\nof the radius of the ambiguity set, the choice of the divergence measure, or\nthe tail heaviness of the center distribution. Thanks to our high-order\nexpansion result, a notable feature of our analysis is that it applies to\ndivergence measures that accommodate well heavy tail distributions such as the\nstudent $t$-distribution and the lognormal distribution, besides the\nwidely-used Kullback-Leibler (KL) divergence, which requires the distribution\nof the objective function to be exponentially bounded. Using the portfolio\nselection problem as an example, our comprehensive testings on multivariate\nheavy-tail datasets, both synthetic and real-world, shows that this\nbusiness-interpretation approach is indeed useful and insightful.\n"
    },
    {
        "paper_id": 1906.02152,
        "authors": "Ariah Klages-Mundt, Andreea Minca",
        "title": "(In)Stability for the Blockchain: Deleveraging Spirals and Stablecoin\n  Attacks",
        "comments": "To be published in Cryptoeconomic Systems 2021",
        "journal-ref": null,
        "doi": "10.21428/58320208.e46b7b81",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model of stable assets, including non-custodial stablecoins\nbacked by cryptocurrencies. Such stablecoins are popular methods for\nbootstrapping price stability within public blockchain settings. We derive\nfundamental results about dynamics and liquidity in stablecoin markets,\ndemonstrate that these markets face deleveraging feedback effects that cause\nilliquidity during crises and exacerbate collateral drawdown, and characterize\nstable dynamics of the system under particular conditions. The possibility of\nsuch `deleveraging spirals' was first predicted in the initial release of our\npaper in 2019 and later directly observed during the `Black Thursday' crisis in\nDai in 2020. From these insights, we suggest design improvements that aim to\nimprove long-term stability. We also introduce new attacks that exploit\narbitrage-like opportunities around stablecoin liquidations. Using our model,\nwe demonstrate that these can be profitable. These attacks may induce\nvolatility in the `stable' asset and cause perverse incentives for miners,\nposing risks to blockchain consensus. A variant of such attacks also later\noccurred during Black Thursday, taking the form of mempool manipulation to\nclear Dai liquidation auctions at near zero prices, costing $8m.\n"
    },
    {
        "paper_id": 1906.02216,
        "authors": "Alex Garivaltis",
        "title": "Game-Theoretic Optimal Portfolios in Continuous Time",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": "Economic Theory Bulletin, pp.1-9 (2018)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a two-person trading game in continuous time whereby each player\nchooses a constant rebalancing rule $b$ that he must adhere to over $[0,t]$. If\n$V_t(b)$ denotes the final wealth of the rebalancing rule $b$, then Player 1\n(the `numerator player') picks $b$ so as to maximize\n$\\mathbb{E}[V_t(b)/V_t(c)]$, while Player 2 (the `denominator player') picks\n$c$ so as to minimize it. In the unique Nash equilibrium, both players use the\ncontinuous-time Kelly rule $b^*=c^*=\\Sigma^{-1}(\\mu-r\\textbf{1})$, where\n$\\Sigma$ is the covariance of instantaneous returns per unit time, $\\mu$ is the\ndrift vector of the stock market, and $\\textbf{1}$ is a vector of ones. Thus,\neven over very short intervals of time $[0,t]$, the desire to perform well\nrelative to other traders leads one to adopt the Kelly rule, which is\nordinarily derived by maximizing the asymptotic exponential growth rate of\nwealth. Hence, we find agreement with Bell and Cover's (1988) result in\ndiscrete time.\n"
    },
    {
        "paper_id": 1906.02223,
        "authors": "Olivier Walther and Denis Retaille",
        "title": "Mapping the Sahelian Space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This chapter examines the geographical meaning of the Sahel, its fluid\nboundaries, and its spatial dynamics. Unlike other approaches that define the\nSahel as a bioclimatic zone or as an ungoverned area, it shows that the Sahel\nis primarily a space of circulation in which uncertainty has historically been\novercome by mobility. The first part of the paper discusses how pre-colonial\nempires relied on a network of markets and cities that facilitated trade and\nsocial relationships across the region and beyond. The second part explores\nchanging regional mobility patterns precipitated by colonial powers and the new\napproach they developed to control networks and flows. The third part discusses\nthe contradiction between the mobile strategies adopted by local herders,\nfarmers and traders in the Sahel and the territorial development initiatives of\nmodern states and international donors. Particular attention is paid in the\nlast section to how the Sahel was progressively redefined through a security\nlens.\n"
    },
    {
        "paper_id": 1906.02306,
        "authors": "M. Dashti Moghaddam, Jiong Liu and R. A. Serota",
        "title": "Implied and Realized Volatility: A Study of Distributions and the\n  Distribution of Difference",
        "comments": "16 pages, 11 figures, 10 tables",
        "journal-ref": "IJFE 26 (2), 2581-2594 (2021)",
        "doi": "10.1002/ijfe.1922",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study distributions of realized variance (squared realized volatility) and\nsquared implied volatility, as represented by VIX and VXO indices. We find that\nGeneralized Beta distribution provide the best fits. These fits are much more\naccurate for realized variance than for squared VIX and VXO -- possibly another\nindicator that the latter have deficiencies in predicting the former. We also\nshow that there are noticeable differences between the distributions of the\n1970-2017 realized variance and its 1990-2017 portion, for which VIX and VXO\nbecame available. This may be indicative of a feedback effect that implied\nvolatility has on realized volatility. We also discuss the distribution of the\ndifference between squared implied volatility and realized variance and show\nthat, at the basic level, it is consistent with Pearson's correlations obtained\nfrom linear regression.\n"
    },
    {
        "paper_id": 1906.02312,
        "authors": "Svitlana Vyetrenko, Shaojie Xu",
        "title": "Risk-Sensitive Compact Decision Trees for Autonomous Execution in\n  Presence of Simulated Market Response",
        "comments": "Proceedings of the 36th International Conference on Machine\n  Learning,Long Beach, California, 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate an application of risk-sensitive reinforcement learning to\noptimizing execution in limit order book markets. We represent taking order\nexecution decisions based on limit order book knowledge by a Markov Decision\nProcess; and train a trading agent in a market simulator, which emulates\nmulti-agent interaction by synthesizing market response to our agent's\nexecution decisions from historical data. Due to market impact, executing high\nvolume orders can incur significant cost. We learn trading signals from market\nmicrostructure in presence of simulated market response and derive explainable\ndecision-tree-based execution policies using risk-sensitive Q-learning to\nminimize execution cost subject to constraints on cost variance.\n"
    },
    {
        "paper_id": 1906.02455,
        "authors": "Clement Gastaud, Theophile Carniel, Jean-Michel Dalle",
        "title": "The emerging sectoral diversity of startup ecosystems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thanks to the recent availability of comprehensive and detailed online\ndatabases of startup companies, it has become possible to more directly\ninvestigate startup ecosystems i.e. startup populations in specific regions. In\nthis paper, we analyze the emergence of 20+ such ecosystems in Europe and the\nUSA, with a specific focus on their sectoral diversity. Analyzing the sectoral\nlandscapes of these ecosystems using a new visualization tool indeed highlights\nmarked differences in terms of diversity, which we characterize using metrics\nderived from ecological sciences. Numerical simulations suggest that the\nemerging diversity of startup ecosystems can be explained using a simple\npreferential attachment model based on sectoral funding.\n"
    },
    {
        "paper_id": 1906.02486,
        "authors": "Thiparat Chotibut, Fryderyk Falniowski, Micha{\\l} Misiurewicz,\n  Georgios Piliouras",
        "title": "The route to chaos in routing games: When is Price of Anarchy too\n  optimistic?",
        "comments": "51 pages, 12 figures",
        "journal-ref": "Advances in Neural Information Processing Systems, Vol 33,\n  p.766-777, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Routing games are amongst the most studied classes of games. Their two most\nwell-known properties are that learning dynamics converge to equilibria and\nthat all equilibria are approximately optimal. In this work, we perform a\nstress test for these classic results by studying the ubiquitous dynamics,\nMultiplicative Weights Update, in different classes of congestion games,\nuncovering intricate non-equilibrium phenomena. As the system demand increases,\nthe learning dynamics go through period-doubling bifurcations, leading to\ninstabilities, chaos and large inefficiencies even in the simplest case of\nnon-atomic routing games with two paths of linear cost where the Price of\nAnarchy is equal to one.\n  Starting with this simple class, we show that every system has a carrying\ncapacity, above which it becomes unstable. If the equilibrium flow is a\nsymmetric $50-50\\%$ split, the system exhibits one period-doubling bifurcation.\nA single periodic attractor of period two replaces the attracting fixed point.\nAlthough the Price of Anarchy is equal to one, in the large population limit\nthe time-average social cost for all but a zero measure set of initial\nconditions converges to its worst possible value. For asymmetric equilibrium\nflows, increasing the demand eventually forces the system into Li-Yorke chaos\nwith positive topological entropy and periodic orbits of all possible periods.\nRemarkably, in all non-equilibrating regimes, the time-average flows on the\npaths converge exactly to the equilibrium flows, a property akin to no-regret\nlearning in zero-sum games. These results are robust. We extend them to routing\ngames with arbitrarily many strategies, polynomial cost functions, non-atomic\nas well as atomic routing games and heteregenous users. Our results are also\napplicable to any sequence of shrinking learning rates, e.g., $1/\\sqrt{T}$, by\nallowing for a dynamically increasing population size.\n"
    },
    {
        "paper_id": 1906.02551,
        "authors": "Antoine Jacquier and Mugad Oumgari",
        "title": "Deep Curve-dependent PDEs for affine rough volatility",
        "comments": "22 pages, 10 figures, 4 tables -- Revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new deep-learning based algorithm to evaluate options in\naffine rough stochastic volatility models. Viewing the pricing function as the\nsolution to a curve-dependent PDE (CPDE), depending on forward curves rather\nthan the whole path of the process, for which we develop a numerical scheme\nbased on deep learning techniques. Numerical simulations suggest that the\nlatter is a promising alternative to classical Monte Carlo simulations.\n"
    },
    {
        "paper_id": 1906.02561,
        "authors": "Stefania Gabrielli, Andrea Pallavicini and Stefano Scoleri",
        "title": "Funding Adjustments in Equity Linear Products",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Valuation adjustments are nowadays a common practice to include credit and\nliquidity effects in option pricing. Funding costs arising from collateral\nprocedures, hedging strategies and taxes are added to option prices to take\ninto account the production cost of financial contracts so that a profitability\nanalysis can be reliably assessed. In particular, when dealing with linear\nproducts, we need a precise evaluation of such contributions since bid-ask\nspreads may be very tight. In this paper we start from a general pricing\nframework inclusive of valuation adjustments to derive simple evaluation\nformulae for the relevant case of total return equity swaps when stock lending\nand borrowing is adopted as hedging strategy.\n"
    },
    {
        "paper_id": 1906.02818,
        "authors": "Francois Belletti, Davis King, Kun Yang, Roland Nelet, Yusef Shafi,\n  Yi-Fan Chen, John Anderson",
        "title": "Tensor Processing Units for Financial Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Monte Carlo methods are critical to many routines in quantitative finance\nsuch as derivatives pricing, hedging and risk metrics. Unfortunately, Monte\nCarlo methods are very computationally expensive when it comes to running\nsimulations in high-dimensional state spaces where they are still a method of\nchoice in the financial industry. Recently, Tensor Processing Units (TPUs) have\nprovided considerable speedups and decreased the cost of running Stochastic\nGradient Descent (SGD) in Deep Learning. After highlighting computational\nsimilarities between training neural networks with SGD and simulating\nstochastic processes, we ask in the present paper whether TPUs are accurate,\nfast and simple enough to use for financial Monte Carlo. Through a theoretical\nreminder of the key properties of such methods and thorough empirical\nexperiments we examine the fitness of TPUs for option pricing, hedging and risk\nmetrics computation. In particular we demonstrate that, in spite of the use of\nmixed precision, TPUs still provide accurate estimators which are fast to\ncompute when compared to GPUs. We also show that the Tensorflow programming\nmodel for TPUs is elegant, expressive and simplifies automated differentiation.\n"
    },
    {
        "paper_id": 1906.02904,
        "authors": "Yanfang Mo, Wei Chen, Li Qiu, Pravin Varaiya",
        "title": "Market Implementation of Multiple-Arrival Multiple-Deadline\n  Differentiated Energy Services",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1016/j.automatica.2020.108933",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  An increasing concern in power systems is how to elicit flexibilities in\ndemand, which leads to nontraditional electricity products for accommodating\nloads of different flexibility levels. We have proposed Multiple-Arrival\nMultiple-Deadline (MAMD) differentiated energy services for the flexible loads\nwhich require constant power for specified durations. Such loads are\nindifferent to the actual power delivery time as long as the duration\nrequirements are satisfied between the specified arrival times and deadlines.\nThe focus of this paper is the market implementation of such services. In a\nforward market, we establish the existence of an efficient competitive\nequilibrium to verify the economic feasibility, which implies that selfish\nmarket participants can attain the maximum social welfare in a distributed\nmanner. We also show the strengths of the MAMD services by simulation.\n"
    },
    {
        "paper_id": 1906.03044,
        "authors": "Michael Allan Ribers and Hannes Ullrich",
        "title": "Battling Antibiotic Resistance: Can Machine Learning Improve\n  Prescribing?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Antibiotic resistance constitutes a major health threat. Predicting bacterial\ncauses of infections is key to reducing antibiotic misuse, a leading driver of\nantibiotic resistance. We train a machine learning algorithm on administrative\nand microbiological laboratory data from Denmark to predict diagnostic test\noutcomes for urinary tract infections. Based on predictions, we develop\npolicies to improve prescribing in primary care, highlighting the relevance of\nphysician expertise and policy implementation when patient distributions vary\nover time. The proposed policies delay antibiotic prescriptions for some\npatients until test results are known and give them instantly to others. We\nfind that machine learning can reduce antibiotic use by 7.42 percent without\nreducing the number of treated bacterial infections. As Denmark is one of the\nmost conservative countries in terms of antibiotic use, this result is likely\nto be a lower bound of what can be achieved elsewhere.\n"
    },
    {
        "paper_id": 1906.03119,
        "authors": "Martin Keller-Ressel and Assad Majid",
        "title": "A comparison principle between rough and non-rough Heston models - with\n  applications to the volatility surface",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a number of related comparison results, which allow to compare\nmoment explosion times, moment generating functions and critical moments\nbetween rough and non-rough Heston models of stochastic volatility. All results\nare based on a comparison principle for certain non-linear Volterra integral\nequations. Our upper bound for the moment explosion time is different from the\nbound introduced by Gerhold, Gerstenecker and Pinter (2018) and tighter for\ntypical parameter values. The results can be directly transferred to a\ncomparison principle for the asymptotic slope of implied volatility between\nrough and non-rough Heston models. This principle shows that the ratio of\nimplied volatility slopes in the rough vs. the non-rough Heston model increases\nat least with power-law behavior for small maturities.\n"
    },
    {
        "paper_id": 1906.03201,
        "authors": "Matthias Feiler and Thibaut Ajdler",
        "title": "Learning from Others in the Financial Market",
        "comments": "16 pages, V2: added examples, results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction problems in finance go beyond estimating the unknown parameters of\na model (e.g. of expected returns). This is because such a model would have to\ninclude parameters governing the market participants' propensity to change\ntheir opinions on the validity of that model. This leads to a well--known\ncircular situation characteristic of financial markets, where participants\ncollectively create the future they wish to estimate. In this paper, we\nintroduce a framework for organizing multiple expectation models and study the\nconditions under which they are adopted by a majority of market participants.\n"
    },
    {
        "paper_id": 1906.0321,
        "authors": "Clement Gastaud, Theophile Carniel, Jean-Michel Dalle",
        "title": "The varying importance of extrinsic factors in the success of startup\n  fundraising: competition at early-stage and networks at growth-stage",
        "comments": "14 pages, 4 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the issue of the factors driving startup success in raising funds.\nUsing the popular and public startup database Crunchbase, we explicitly take\ninto account two extrinsic characteristics of startups: the competition that\nthe companies face, using similarity measures derived from the Word2Vec\nalgorithm, as well as the position of investors in the investment network,\npioneering the use of Graph Neural Networks (GNN), a recent deep learning\ntechnique that enables the handling of graphs as such and as a whole. We show\nthat the different stages of fundraising, early- and growth-stage, are\nassociated with different success factors. Our results suggest a marked\nrelevance of startup competition for early stage while growth-stage fundraising\nis influenced by network features. Both of these factors tend to average out in\nglobal models, which could lead to the false impression that startup success in\nfundraising would mostly if not only be influenced by its intrinsic\ncharacteristics, notably those of their founders.\n"
    },
    {
        "paper_id": 1906.03232,
        "authors": "Brandon Da Silva and Sylvie Shang Shi",
        "title": "Style Transfer with Time Series: Generating Synthetic Financial Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Training deep learning models that generalize well to live deployment is a\nchallenging problem in the financial markets. The challenge arises because of\nhigh dimensionality, limited observations, changing data distributions, and a\nlow signal-to-noise ratio. High dimensionality can be dealt with using robust\nfeature selection or dimensionality reduction, but limited observations often\nresult in a model that overfits due to the large parameter space of most deep\nneural networks. We propose a generative model for financial time series, which\nallows us to train deep learning models on millions of simulated paths. We show\nthat our generative model is able to create realistic paths that embed the\nunderlying structure of the markets in a way stochastic processes cannot.\n"
    },
    {
        "paper_id": 1906.03305,
        "authors": "Xin Qian, Yudong Chen, Andreea Minca",
        "title": "Clustering Degree-Corrected Stochastic Block Model with Outliers",
        "comments": "32 pages, 8 Fig",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the degree corrected stochastic block model in the presence of arbitrary\nor even adversarial outliers, we develop a convex-optimization-based clustering\nalgorithm that includes a penalization term depending on the positive deviation\nof a node from the expected number of edges to other inliers. We prove that\nunder mild conditions, this method achieves exact recovery of the underlying\nclusters. Our synthetic experiments show that our algorithm performs well on\nheterogeneous networks, and in particular those with Pareto degree\ndistributions, for which outliers have a broad range of possible degrees that\nmay enhance their adversarial power. We also demonstrate that our method allows\nfor recovery with significantly lower error rates compared to existing\nalgorithms.\n"
    },
    {
        "paper_id": 1906.03306,
        "authors": "Sandra Johnson, Peter Robinson, Kishore Atreya, Claudio Lisco",
        "title": "Invoice Financing of Supply Chains with Blockchain technology and\n  Artificial Intelligence",
        "comments": "12 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Supply chains lend themselves to blockchain technology, but certain\nchallenges remain, especially around invoice financing. For example, the\nfurther a supplier is removed from the final consumer product, the more\ndifficult it is to get their invoices financed. Moreover, for competitive\nreasons, retailers and manufacturers do not want to disclose their supply\nchains. However, upstream suppliers need to prove that they are part of a\n`stable' supply chain to get their invoices financed, which presents the\nupstream suppliers with huge, and often unsurmountable, obstacles to get the\nnecessary finance to fulfil the next order, or to expand their business. Using\na fictitious supply chain use case, which is based on a real world use case, we\ndemonstrate how these challenges have the potential to be solved by combining\nmore advanced and specialised blockchain technologies with other technologies\nsuch as Artificial Intelligence. We describe how atomic crosschain\nfunctionality can be utilised across private blockchains to retrieve the\ninformation required for an invoice financier to make informed decisions under\nuncertainty, and consider the effect this decision has on the overall stability\nof the supply chain.\n"
    },
    {
        "paper_id": 1906.03309,
        "authors": "Dmitry Kramkov and Yan Xu",
        "title": "An optimal transport problem with backward martingale constraints\n  motivated by insider trading",
        "comments": "46 pages",
        "journal-ref": "Annals of Applied Probability 2022, Vol. 32, No. 1, 294-326",
        "doi": "10.1214/21-AAP1678",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We study a single-period optimal transport problem on $\\mathbb{R}^2$ with a\ncovariance-type cost function $c(x,y) = (x_1-y_1)(x_2-y_2)$ and a backward\nmartingale constraint. We show that a transport plan $\\gamma$ is optimal if and\nonly if there is a maximal monotone set $G$ that supports the $x$-marginal of\n$\\gamma$ and such that $c(x,y) = \\min_{z\\in G}c(z,y)$ for every $(x,y)$ in the\nsupport of $\\gamma$. We obtain sharp regularity conditions for the uniqueness\nof an optimal plan and for its representation in terms of a map. Our study is\nmotivated by a variant of the classical Kyle model of insider trading from\nRochet and Vila (1994).\n"
    },
    {
        "paper_id": 1906.0343,
        "authors": "Wonse Kim, Junseok Lee, Kyungwon Kang",
        "title": "The Effects of the Introduction of Bitcoin Futures on the Volatility of\n  Bitcoin Returns",
        "comments": "Accepted in Finance Research Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the effects of the launch of Bitcoin futures on the\nintraday volatility of Bitcoin. Based on one-minute price data collected from\nfour cryptocurrency exchanges, we first examine the change in realized\nvolatility after the introduction of Bitcoin futures to investigate their\naggregate effects on the intraday volatility of Bitcoin. We then analyze the\neffects in more detail utilizing the discrete Fourier transform. We show that\nalthough the Bitcoin market became more volatile immediately after the\nintroduction of Bitcoin futures, over time it has become more stable than it\nwas before the introduction.\n"
    },
    {
        "paper_id": 1906.03507,
        "authors": "A Itkin",
        "title": "Deep learning calibration of option pricing models: some pitfalls and\n  solutions",
        "comments": "16 pages, 15 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent progress in the field of artificial intelligence, machine learning and\nalso in computer industry resulted in the ongoing boom of using these\ntechniques as applied to solving complex tasks in both science and industry.\nSame is, of course, true for the financial industry and mathematical finance.\nIn this paper we consider a classical problem of mathematical finance -\ncalibration of option pricing models to market data, as it was recently drawn\nsome attention of the financial society in the context of deep learning and\nartificial neural networks. We highlight some pitfalls in the existing\napproaches and propose resolutions that improve both performance and accuracy\nof calibration. We also address a problem of no-arbitrage pricing when using a\ntrained neural net, that is currently ignored in the literature.\n"
    },
    {
        "paper_id": 1906.03562,
        "authors": "Tim Leung, Yang Zhou",
        "title": "A Top-Down Approach for the Multiple Exercises and Valuation of Employee\n  Stock Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new framework to value employee stock options (ESOs) that\ncaptures multiple exercises of different quantities over time. We also model\nthe ESO holder's job termination risk and incorporate its impact on the payoffs\nof both vested and unvested ESOs. Numerical methods based on Fourier transform\nand finite differences are developed and implemented to solve the associated\nsystems of PDEs. In addition, we introduce a new valuation method based on\nmaturity randomization that yields analytic formulae for vested and unvested\nESO costs. We examine the cost impact of job termination risk, exercise\nintensity, and various contractual features.\n"
    },
    {
        "paper_id": 1906.0369,
        "authors": "Hyungbin Park and Stephan Sturm",
        "title": "A sensitivity analysis of the long-term expected utility of optimal\n  portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the sensitivity of the long-term expected utility of\noptimal portfolios for an investor with constant relative risk aversion. Under\nan incomplete market given by a factor model, we consider the utility\nmaximization problem with long-time horizon. The main purpose is to find the\nlong-term sensitivity, that is, the extent how much the optimal expected\nutility is affected in the long run for small changes of the underlying factor\nmodel. The factor model induces a specific eigenpair of an operator, and this\neigenpair does not only characterize the long-term behavior of the optimal\nexpected utility but also provides an explicit representation of the expected\nutility on a finite time horizon. We conclude that this eigenpair therefore\ndetermines the long-term sensitivity. As examples, explicit results for several\nmarket models such as the Kim--Omberg model for stochastic excess returns and\nthe Heston stochastic volatility model are presented.\n"
    },
    {
        "paper_id": 1906.03726,
        "authors": "Lotfi Boudabsa, Damir Filipovic",
        "title": "Machine learning with kernels for portfolio valuation and risk\n  management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a simulation method for dynamic portfolio valuation and risk\nmanagement building on machine learning with kernels. We learn the dynamic\nvalue process of a portfolio from a finite sample of its cumulative cash flow.\nThe learned value process is given in closed form thanks to a suitable choice\nof the kernel. We show asymptotic consistency and derive finite sample error\nbounds under conditions that are suitable for finance applications. Numerical\nexperiments show good results in large dimensions for a moderate training\nsample size.\n"
    },
    {
        "paper_id": 1906.03935,
        "authors": "Rukmal Weerawarana, Yiyi Zhu, Yuzhen He",
        "title": "Learned Sectors: A fundamentals-driven sector reclassification project",
        "comments": "Supervised by Thomas Lonon <tlonon@stevens.edu>, Ionut Florescu\n  <ifloresc@stevens.edu>, Papa Ndiaye <pndiaye@stevens.edu>, and Dragos Bozdog\n  <dbozdog@stevens.edu>",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Market sectors play a key role in the efficient flow of capital through the\nmodern Global economy. We analyze existing sectorization heuristics, and\nobserve that the most popular - the GICS (which informs the S&P 500), and the\nNAICS (published by the U.S. Government) - are not entirely quantitatively\ndriven, but rather appear to be highly subjective and rooted in dogma. Building\non inferences from analysis of the capital structure irrelevance principle and\nthe Modigliani-Miller theoretic universe conditions, we postulate that\ncorporation fundamentals - particularly those components specific to the\nModigliani-Miller universe conditions - would be optimal descriptors of the\ntrue economic domain of operation of a company. We generate a set of potential\ncandidate learned sector universes by varying the linkage method of a\nhierarchical clustering algorithm, and the number of resulting sectors derived\nfrom the model (ranging from 5 to 19), resulting in a total of 60 candidate\nlearned sector universes. We then introduce reIndexer, a backtest-driven sector\nuniverse evaluation research tool, to rank the candidate sector universes\nproduced by our learned sector classification heuristic. This rank was utilized\nto identify the risk-adjusted return optimal learned sector universe as being\nthe universe generated under CLINK (i.e. complete linkage), with 17 sectors.\nThe optimal learned sector universe was tested against the benchmark GICS\nclassification universe with reIndexer, outperforming on both absolute\nportfolio value, and risk-adjusted return over the backtest period. We conclude\nthat our fundamentals-driven Learned Sector classification heuristic provides a\nsuperior risk-diversification profile than the status quo classification\nheuristic.\n"
    },
    {
        "paper_id": 1906.04086,
        "authors": "R. Maria del Rio-Chanona, Penny Mealy, Mariano Beguerisse-D\\'iaz,\n  Francois Lafond and J. Doyne Farmer",
        "title": "Automation and occupational mobility: A data-driven network model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The potential impact of automation on the labor market is a topic that has\ngenerated significant interest and concern amongst scholars, policymakers, and\nthe broader public. A number of studies have estimated occupation-specific risk\nprofiles by examining the automatability of associated skills and tasks.\nHowever, relatively little work has sought to take a more holistic view on the\nprocess of labor reallocation and how employment prospects are impacted as\ndisplaced workers transition into new jobs. In this paper, we develop a new\ndata-driven model to analyze how workers move through an empirically derived\noccupational mobility network in response to automation scenarios which\nincrease labor demand for some occupations and decrease it for others. At the\nmacro level, our model reproduces a key stylized fact in the labor market known\nas the Beveridge curve and provides new insights for explaining the curve's\ncounter-clockwise cyclicality. At the micro level, our model provides\noccupation-specific estimates of changes in short and long-term unemployment\ncorresponding to a given automation shock. We find that the network structure\nplays an important role in determining unemployment levels, with occupations in\nparticular areas of the network having very few job transition opportunities.\nSuch insights could be fruitfully applied to help design more efficient and\neffective policies aimed at helping workers adapt to the changing nature of the\nlabor market.\n"
    },
    {
        "paper_id": 1906.04322,
        "authors": "Jean-Fran\\c{c}ois B\\'egin and Mathieu Boudreault",
        "title": "Likelihood Evaluation of Jump-Diffusion Models Using Deterministic\n  Nonlinear Filters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we develop a deterministic nonlinear filtering algorithm based\non a high-dimensional version of Kitagawa (1987) to evaluate the likelihood\nfunction of models that allow for stochastic volatility and jumps whose arrival\nintensity is also stochastic. We show numerically that the deterministic\nfiltering method is precise and much faster than the particle filter, in\naddition to yielding a smooth function over the parameter space. We then find\nthe maximum likelihood estimates of various models that include stochastic\nvolatility, jumps in the returns and variance, and also stochastic jump arrival\nintensity with the S&P 500 daily returns. During the Great Recession, the jump\narrival intensity increases significantly and contributes to the clustering of\nvolatility and negative returns.\n"
    },
    {
        "paper_id": 1906.04404,
        "authors": "Zihao Zhang, Stefan Zohren, Stephen Roberts",
        "title": "Extending Deep Learning Models for Limit Order Books to Quantile\n  Regression",
        "comments": "5 pages, 4 figures, Time Series Workshop of the ICML (2019)",
        "journal-ref": "Proceedings of Time Series Workshop of the 36 th International\n  Conference on Machine Learning, Long Beach, California, PMLR 97, 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We showcase how Quantile Regression (QR) can be applied to forecast financial\nreturns using Limit Order Books (LOBs), the canonical data source of\nhigh-frequency financial time-series. We develop a deep learning architecture\nthat simultaneously models the return quantiles for both buy and sell\npositions. We test our model over millions of LOB updates across multiple\ndifferent instruments on the London Stock Exchange. Our results suggest that\nthe proposed network not only delivers excellent performance but also provides\nimproved prediction robustness by combining quantile estimates.\n"
    },
    {
        "paper_id": 1906.04522,
        "authors": "Donovan Platt",
        "title": "Bayesian Estimation of Economic Simulation Models using Neural Networks",
        "comments": "26 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in computing power and the potential to make more realistic\nassumptions due to increased flexibility have led to the increased prevalence\nof simulation models in economics. While models of this class, and particularly\nagent-based models, are able to replicate a number of empirically-observed\nstylised facts not easily recovered by more traditional alternatives, such\nmodels remain notoriously difficult to estimate due to their lack of tractable\nlikelihood functions. While the estimation literature continues to grow,\nexisting attempts have approached the problem primarily from a frequentist\nperspective, with the Bayesian estimation literature remaining comparatively\nless developed. For this reason, we introduce a Bayesian estimation protocol\nthat makes use of deep neural networks to construct an approximation to the\nlikelihood, which we then benchmark against a prominent alternative from the\nexisting literature. Overall, we find that our proposed methodology\nconsistently results in more accurate estimates in a variety of settings,\nincluding the estimation of financial heterogeneous agent models and the\nidentification of changes in dynamics occurring in models incorporating\nstructural breaks.\n"
    },
    {
        "paper_id": 1906.04652,
        "authors": "David Meder, Finn Rabe, Tobias Morville, Kristoffer H. Madsen, Magnus\n  T. Koudahl, Ray J. Dolan, Hartwig R. Siebner, Oliver J. Hulme",
        "title": "Ergodicity-breaking reveals time optimal decision making in humans",
        "comments": "43 pages including supplementary methods & materials",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ergodicity describes an equivalence between the expectation value and the\ntime average of observables. Applied to human behaviour, ergodic theories of\ndecision-making reveal how individuals should tolerate risk in different\nenvironments. To optimise wealth over time, agents should adapt their utility\nfunction according to the dynamical setting they face. Linear utility is\noptimal for additive dynamics, whereas logarithmic utility is optimal for\nmultiplicative dynamics. Whether humans approximate time optimal behavior\nacross different dynamics is unknown. Here we compare the effects of additive\nversus multiplicative gamble dynamics on risky choice. We show that utility\nfunctions are modulated by gamble dynamics in ways not explained by prevailing\ndecision theory. Instead, as predicted by time optimality, risk aversion\nincreases under multiplicative dynamics, distributing close to the values that\nmaximise the time average growth of wealth. We suggest that our findings\nmotivate a need for explicitly grounding theories of decision-making on ergodic\nconsiderations.\n"
    },
    {
        "paper_id": 1906.04711,
        "authors": "Matias Barenstein",
        "title": "ProPublica's COMPAS Data Revisited",
        "comments": "28 pages, 13 figures (v3); fixed figure and footnote formatting;\n  edited writing, organization, references, and appendix, results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I examine the COMPAS recidivism risk score and criminal history data\ncollected by ProPublica in 2016 that fueled intense debate and research in the\nnascent field of 'algorithmic fairness'. ProPublica's COMPAS data is used in an\nincreasing number of studies to test various definitions of algorithmic\nfairness. This paper takes a closer look at the actual datasets put together by\nProPublica. In particular, the sub-datasets built to study the likelihood of\nrecidivism within two years of a defendant's original COMPAS survey screening\ndate. I take a new yet simple approach to visualize these data, by analyzing\nthe distribution of defendants across COMPAS screening dates. I find that\nProPublica made an important data processing error when it created these\ndatasets, failing to implement a two-year sample cutoff rule for recidivists in\nsuch datasets (whereas it implemented a two-year sample cutoff rule for\nnon-recidivists). When I implement a simple two-year COMPAS screen date cutoff\nrule for recidivists, I estimate that in the two-year general recidivism\ndataset ProPublica kept over 40% more recidivists than it should have. This\nfundamental problem in dataset construction affects some statistics more than\nothers. It obviously has a substantial impact on the recidivism rate;\nartificially inflating it. For the two-year general recidivism dataset created\nby ProPublica, the two-year recidivism rate is 45.1%, whereas, with the simple\nCOMPAS screen date cutoff correction I implement, it is 36.2%. Thus, the\ntwo-year recidivism rate in ProPublica's dataset is inflated by over 24%. This\nalso affects the positive and negative predictive values. On the other hand,\nthis data processing error has little impact on some of the other key\nstatistical measures, which are less susceptible to changes in the relative\nshare of recidivists, such as the false positive and false negative rates, and\nthe overall accuracy.\n"
    },
    {
        "paper_id": 1906.04813,
        "authors": "Jacobo Roa-Vicens, Cyrine Chtourou, Angelos Filos, Francisco Rullan,\n  Yarin Gal, Ricardo Silva",
        "title": "Towards Inverse Reinforcement Learning for Limit Order Book Dynamics",
        "comments": "Published as a workshop paper on AI in Finance: Applications and\n  Infrastructure for Multi-Agent Learning at the 36th International Conference\n  on Machine Learning (ICML), Long Beach, California, PMLR97, 2019. Copyright\n  2019 by the author(s)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-agent learning is a promising method to simulate aggregate competitive\nbehaviour in finance. Learning expert agents' reward functions through their\nexternal demonstrations is hence particularly relevant for subsequent design of\nrealistic agent-based simulations. Inverse Reinforcement Learning (IRL) aims at\nacquiring such reward functions through inference, allowing to generalize the\nresulting policy to states not observed in the past. This paper investigates\nwhether IRL can infer such rewards from agents within real financial stochastic\nenvironments: limit order books (LOB). We introduce a simple one-level LOB,\nwhere the interactions of a number of stochastic agents and an expert trading\nagent are modelled as a Markov decision process. We consider two cases for the\nexpert's reward: either a simple linear function of state features; or a\ncomplex, more realistic non-linear function. Given the expert agent's\ndemonstrations, we attempt to discover their strategy by modelling their latent\nreward function using linear and Gaussian process (GP) regressors from previous\nliterature, and our own approach through Bayesian neural networks (BNN). While\nthe three methods can learn the linear case, only the GP-based and our proposed\nBNN methods are able to discover the non-linear reward case. Our BNN IRL\nalgorithm outperforms the other two approaches as the number of samples\nincreases. These results illustrate that complex behaviours, induced by\nnon-linear reward functions amid agent-based stochastic scenarios, can be\ndeduced through inference, encouraging the use of inverse reinforcement\nlearning for opponent-modelling in multi-agent systems.\n"
    },
    {
        "paper_id": 1906.04822,
        "authors": "M. Dashti Moghaddam, Jeffrey Mills and R. A. Serota",
        "title": "Generalized Beta Prime Distribution: Stochastic Model of Economic\n  Exchange and Properties of Inequality Indices",
        "comments": "19 pages, 14 figures, 16 Tables",
        "journal-ref": "Physica A 559, 125047 (2020)",
        "doi": "10.1016/j.physa.2020.125047",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue that a stochastic model of economic exchange, whose steady-state\ndistribution is a Generalized Beta Prime (also known as GB2), and some unique\nproperties of the latter, are the reason for GB2's success in describing\nwealth/income distributions. We use housing sale prices as a proxy to\nwealth/income distribution to numerically illustrate this point. We also\nexplore parametric limits of the distribution to do so analytically. We discuss\nparametric properties of the inequality indices -- Gini, Hoover, Theil T and\nTheil L -- vis-a-vis those of GB2 and introduce a new inequality index, which\nserves a similar purpose. We argue that Hoover and Theil L are more appropriate\nmeasures for distributions with power-law dependencies, especially fat tails,\nsuch as GB2.\n"
    },
    {
        "paper_id": 1906.05057,
        "authors": "Kartikay Gupta, Niladri Chatterjee",
        "title": "Selecting stock pairs for pairs trading while incorporating lead-lag\n  relationship",
        "comments": "Better updated version in lots of ways to be uploaded soon",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.124103",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pairs Trading is carried out in the financial market to earn huge profits\nfrom known equilibrium relation between pairs of stock. In financial markets,\nseldom it is seen that stock pairs are correlated at particular lead or lag.\nThis lead-lag relationship has been empirically studied in various financial\nmarkets. Earlier research works have suggested various measures for identifying\nthe best pairs for pairs trading, but they do not consider this lead-lag\neffect. The present study proposes a new distance measure which incorporates\nthe lead-lag relationship between the stocks while selecting the best pairs for\npairs trading. Further, the lead-lag value between the stocks is allowed to\nvary continuously over time. The proposed measures importance has been\nshow-cased through experimentation on two different datasets, one corresponding\nto Indian companies and another corresponding to American companies. When the\nproposed measure is clubbed with SSD measure, i.e., when pairs are identified\nthrough optimising both these measures, then the selected pairs consistently\ngenerate the best profit, as compared to all other measures. Finally, possible\ngeneralisation and extension of the proposed distance measure have been\ndiscussed.\n"
    },
    {
        "paper_id": 1906.05065,
        "authors": "Damien Ackerer, Natasa Tagasovska, Thibault Vatter",
        "title": "Deep Smoothing of the Implied Volatility Surface",
        "comments": "forthcoming NeurIPS 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a neural network (NN) approach to fit and predict implied\nvolatility surfaces (IVSs). Atypically to standard NN applications, financial\nindustry practitioners use such models equally to replicate market prices and\nto value other financial instruments. In other words, low training losses are\nas important as generalization capabilities. Importantly, IVS models need to\ngenerate realistic arbitrage-free option prices, meaning that no portfolio can\nlead to risk-free profits. We propose an approach guaranteeing the absence of\narbitrage opportunities by penalizing the loss using soft constraints.\nFurthermore, our method can be combined with standard IVS models in\nquantitative finance, thus providing a NN-based correction when such models\nfail at replicating observed market prices. This lets practitioners use our\napproach as a plug-in on top of classical methods. Empirical results show that\nthis approach is particularly useful when only sparse or erroneous data are\navailable. We also quantify the uncertainty of the model predictions in regions\nwith few or no observations. We further explore how deeper NNs improve over\nshallower ones, as well as other properties of the network architecture. We\nbenchmark our method against standard IVS models. By evaluating our method on\nboth training sets, and testing sets, namely, we highlight both their capacity\nto reproduce observed prices and predict new ones.\n"
    },
    {
        "paper_id": 1906.05187,
        "authors": "Pierre-Alain Reigneron, Vincent Nguyen, Stefano Ciliberti, Philip\n  Seager, Jean-Philippe Bouchaud",
        "title": "The Case for Long-Only Agnostic Allocation Portfolios",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We advocate the use of Agnostic Allocation for the construction of long-only\nportfolios of stocks. We show that Agnostic Allocation Portfolios (AAPs) are a\nspecial member of a family of risk-based portfolios that are able to mitigate\ncertain extreme features (excess concentration, high turnover, strong exposure\nto low-risk factors) of classical portfolio construction methods, while\nachieving similar performance. AAPs thus represent a very attractive\nalternative risk-based portfolio construction framework that can be implemented\nin different situations, with or without an active trading signal.\n"
    },
    {
        "paper_id": 1906.05269,
        "authors": "Seyyedmilad Talebzadehhosseini, Steven R. Scheinert, and Ivan Garibay",
        "title": "Growing green: the role of path dependency and structural jumps in the\n  green economy expansion",
        "comments": "31 Pages, 2 figures, working paper and will be submitted to a journal\n  for peer review soon",
        "journal-ref": "https://www.ipsonet.org/publications/open-access/policy-and-complex-systems/policy-and-complex-systems-volume-6-number-1-spring-2020",
        "doi": "10.18278/jpcs.6.1.2",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Existing research argues that countries increase their production basket by\nadding products which require similar capabilities to those they already\nproduce, a process referred to as path dependency. Green economic growth is a\nglobal movement that seeks to achieve economic expansion while at the same time\nmitigating environmental risks. We postulate that countries engaging in green\neconomic growth are motivated to invest strategically to develop new\ncapabilities that will help them transition to a green economy. As a result,\nthey could potentially increase their production baskets not only by a path\ndependent process but also by the non path dependent process we term, high\ninvestment structural jumps. The main objective of this research is to\ndetermine whether countries increase their green production basket mainly by a\nprocess of path dependency, or alternatively, by a process of structural jumps.\nWe analyze data from 65 countries and over a period from years 2007 to 2017. We\nfocus on China as our main case study. The results of this research show that\ncountries not only increase their green production baskets based on their\navailable capabilities, following path dependency, but also expand to products\nthat path dependency does not predict by investing in innovating and developing\nnew environmental related technologies.\n"
    },
    {
        "paper_id": 1906.05327,
        "authors": "Yuxuan Huang, Luiz Fernando Capretz, Danny Ho",
        "title": "Neural Network Models for Stock Selection Based on Fundamental Analysis",
        "comments": "4 pages",
        "journal-ref": "32nd Canadian Conference on Electrical & Computer Engineering,\n  Edmonton, Canada, 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Application of neural network architectures for financial prediction has been\nactively studied in recent years. This paper presents a comparative study that\ninvestigates and compares feed-forward neural network (FNN) and adaptive neural\nfuzzy inference system (ANFIS) on stock prediction using fundamental financial\nratios. The study is designed to evaluate the performance of each architecture\nbased on the relative return of the selected portfolios with respect to the\nbenchmark stock index. The results show that both architectures possess the\nability to separate winners and losers from a sample universe of stocks, and\nthe selected portfolios outperform the benchmark. Our study argues that FNN\nshows superior performance over ANFIS.\n"
    },
    {
        "paper_id": 1906.0542,
        "authors": "Othmane Mounjid, Mathieu Rosenbaum, Pamela Saliba",
        "title": "From asymptotic properties of general point processes to the ranking of\n  financial agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a general non-linear order book model that is built from the\nindividual behaviours of the agents. Our framework encompasses Markovian and\nHawkes based models. Under mild assumptions, we prove original results on the\nergodicity and diffusivity of such system. Then we provide closed form formulas\nfor various quantities of interest: stationary distribution of the best bid and\nask quantities, spread, liquidity fluctuations and price volatility. These\nformulas are expressed in terms of individual order flows of market\nparticipants. Our approach enables us to establish a ranking methodology for\nthe market makers with respect to the quality of their trading.\n"
    },
    {
        "paper_id": 1906.05494,
        "authors": "Ajit Mahata and Md Nurujjaman",
        "title": "Time scales in stock markets",
        "comments": null,
        "journal-ref": "Front. Phys., 12 November 2020",
        "doi": "10.3389/fphy.2020.590623",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Different investment strategies are adopted in short-term and long-term\ndepending on the time scales, even though time scales are adhoc in nature.\nEmpirical mode decomposition based Hurst exponent analysis and variance\ntechnique have been applied to identify the time scales for short-term and\nlong-term investment from the decomposed intrinsic mode functions(IMF). Hurst\nexponent ($H$) is around 0.5 for the IMFs with time scales from few days to 3\nmonths, and $H\\geq0.75$ for the IMFs with the time scales $\\geq5$ months. Short\nterm time series [$X_{ST}(t)$] with time scales from few days to 3 months and\n$H~0.5$ and long term time series [$X_{LT}(t)$] with time scales $\\geq5$ and\n$H\\geq0.75$, which represent the dynamics of the market, are constructed from\nthe IMFs. The $X_{ST}(t)$ and $X_{LT}(t)$ show that the market is random in\nshort-term and correlated in long term. The study also show that the\n$X_{LT}(t)$ is correlated with fundamentals of the company. The analysis will\nbe useful for investors to design the investment and trading strategy.\n"
    },
    {
        "paper_id": 1906.05545,
        "authors": "Maurizio Daniele, Winfried Pohlmeier, Aygul Zagidullina",
        "title": "Sparse Approximate Factor Estimation for High-Dimensional Covariance\n  Matrices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel estimation approach for the covariance matrix based on the\n$l_1$-regularized approximate factor model. Our sparse approximate factor (SAF)\ncovariance estimator allows for the existence of weak factors and hence relaxes\nthe pervasiveness assumption generally adopted for the standard approximate\nfactor model. We prove consistency of the covariance matrix estimator under the\nFrobenius norm as well as the consistency of the factor loadings and the\nfactors.\n  Our Monte Carlo simulations reveal that the SAF covariance estimator has\nsuperior properties in finite samples for low and high dimensions and different\ndesigns of the covariance matrix. Moreover, in an out-of-sample portfolio\nforecasting application the estimator uniformly outperforms alternative\nportfolio strategies based on alternative covariance estimation approaches and\nmodeling strategies including the $1/N$-strategy.\n"
    },
    {
        "paper_id": 1906.0574,
        "authors": "Z. Keskin and T. Aste",
        "title": "Information-theoretic measures for non-linear causality detection:\n  application to social media sentiment and cryptocurrency prices",
        "comments": "12 pages, 7 figures, 1 table",
        "journal-ref": "Royal Society open science (2020), 7(9), 200863",
        "doi": "10.1098/rsos.200863",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information transfer between time series is calculated by using the\nasymmetric information-theoretic measure known as transfer entropy. Geweke's\nautoregressive formulation of Granger causality is used to find linear transfer\nentropy, and Schreiber's general, non-parametric, information-theoretic\nformulation is used to detect non-linear transfer entropy.\n  We first validate these measures against synthetic data. Then we apply these\nmeasures to detect causality between social sentiment and cryptocurrency\nprices. We perform significance tests by comparing the information transfer\nagainst a null hypothesis, determined via shuffled time series, and calculate\nthe Z-score. We also investigate different approaches for partitioning in\nnonparametric density estimation which can improve the significance of results.\n  Using these techniques on sentiment and price data over a 48-month period to\nAugust 2018, for four major cryptocurrencies, namely bitcoin (BTC), ripple\n(XRP), litecoin (LTC) and ethereum (ETH), we detect significant information\ntransfer, on hourly timescales, in directions of both sentiment to price and of\nprice to sentiment. We report the scale of non-linear causality to be an order\nof magnitude greater than linear causality.\n"
    },
    {
        "paper_id": 1906.05898,
        "authors": "Ben Hambly and Nikolaos Kolliopoulos",
        "title": "Stochastic PDEs for large portfolios with general mean-reverting\n  volatility processes",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a structural stochastic volatility model for the loss from a\nlarge portfolio of credit risky assets. Both the asset value and the volatility\nprocesses are correlated through systemic Brownian motions, with default\ndetermined by the asset value reaching a lower boundary. We prove that if our\nvolatility models are picked from a class of mean-reverting diffusions, the\nsystem converges as the portfolio becomes large and, when the vol-of-vol\nfunction satisfies certain regularity and boundedness conditions, the limit of\nthe empirical measure process has a density given in terms of a solution to a\nstochastic initial-boundary value problem on a half-space. The problem is\ndefined in a special weighted Sobolev space. Regularity results are established\nfor solutions to this problem, and then we show that there exists a unique\nsolution. In contrast to the CIR volatility setting covered by the existing\nliterature, our results hold even when the systemic Brownian motions are taken\nto be correlated.\n"
    },
    {
        "paper_id": 1906.06,
        "authors": "Takanobu Mizuta",
        "title": "An agent-based model for designing a financial market that works well",
        "comments": null,
        "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)",
        "doi": "10.1109/SSCI47803.2020.9308376",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing a financial market that works well is very important for developing\nand maintaining an advanced economy, but is not easy because changing detailed\nrules, even ones that seem trivial, sometimes causes unexpected large impacts\nand side effects. A computer simulation using an agent-based model can directly\ntreat and clearly explain such complex systems where micro processes and macro\nphenomena interact. Many effective agent-based models investigating human\nbehavior have already been developed. Recently, an artificial market model,\nwhich is an agent-based model for a financial market, has started to contribute\nto discussions on rules and regulations of actual financial markets. I\nintroduce an artificial market model to design financial markets that work well\nand describe a previous study investigating tick size reduction. I hope that\nmore artificial market models will contribute to designing financial markets\nthat work well to further develop and maintain advanced economies.\n"
    },
    {
        "paper_id": 1906.06092,
        "authors": "Chengyuan Han, Dirk Witthaut, Marc Timme, and Malte Schr\\\"oder",
        "title": "The winner takes it all -- How to win network globalization",
        "comments": null,
        "journal-ref": "PLoS ONE 14, e0225346 (2019)",
        "doi": "10.1371/journal.pone.0225346",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantifying the importance and power of individual nodes depending on their\nposition in socio-economic networks constitutes a problem across a variety of\napplications. Examples include the reach of individuals in (online) social\nnetworks, the importance of individual banks or loans in financial networks,\nthe relevance of individual companies in supply networks, and the role of\ntraffic hubs in transport networks. Which features characterize the importance\nof a node in a trade network during the emergence of a globalized, connected\nmarket? Here we analyze a model that maps the evolution of trade networks to a\npercolation problem. In particular, we focus on the influence of topological\nfeatures of the node within the trade network. Our results reveal that an\nadvantageous position with respect to different length scales determines the\nsuccess of a node at different stages of globalization and depending on the\nspeed of globalization.\n"
    },
    {
        "paper_id": 1906.06164,
        "authors": "Roberto Fontana, Elisa Luciano, Patrizia Semeraro",
        "title": "Model Risk in Credit Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The issue of model risk in default modeling has been known since inception of\nthe Academic literature in the field. However, a rigorous treatment requires a\ndescription of all the possible models, and a measure of the distance between a\nsingle model and the alternatives, consistent with the applications. This is\nthe purpose of the current paper. We first analytically describe all possible\njoint models for default, in the class of finite sequences of exchangeable\nBernoulli random variables. We then measure how the model risk of choosing or\ncalibrating one of them affects the portfolio loss from default, using two\npopular and economically sensible metrics, Value-at-Risk (VaR) and Expected\nShortfall (ES).\n"
    },
    {
        "paper_id": 1906.06248,
        "authors": "Simon Schn\\\"urch and Andreas Wagner",
        "title": "Machine Learning on EPEX Order Books: Insights and Forecasts",
        "comments": "14 pages, 5 figures",
        "journal-ref": "Applied Mathematical Finance 27 (2020) 189-206",
        "doi": "10.1080/1350486X.2020.1805337",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper employs machine learning algorithms to forecast German electricity\nspot market prices. The forecasts utilize in particular bid and ask order book\ndata from the spot market but also fundamental market data like renewable\ninfeed and expected demand. Appropriate feature extraction for the order book\ndata is developed. Using cross-validation to optimise hyperparameters, neural\nnetworks and random forests are proposed and compared to statistical reference\nmodels. The machine learning models outperform traditional approaches.\n"
    },
    {
        "paper_id": 1906.06363,
        "authors": "Thomas Viehmann",
        "title": "Variants of the Smith-Wilson method with a view towards applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose two variants of the Smith-Wilson method for practical application\nin the insurance industry. Our first variant relaxes the Smith-Wilson energy\nand can be used to incorporate less reliable market data with a certain weight\nrather than disregarding it completely. This is particularly useful for\nderiving yield curves in the IFRS 17 accounting regime, where there is a\nmandate to incorporate all available market data.\n  A second variant incorporates the requirement to reach the ultimate forward\nrate at a prescribed term into the problem formulation. This provides a natural\nway to fulfil the Solvency II convergence requirement and is more elegant than\nthe current methodology adapting the term-scale parameter to control\nconvergence.\n"
    },
    {
        "paper_id": 1906.06389,
        "authors": "Marcin Pitera, {\\L}ukasz Stettner",
        "title": "Long-run risk sensitive dyadic impulse control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper long-run risk sensitive optimisation problem is studied with\ndyadic impulse control applied to continuous-time Feller-Markov process. In\ncontrast to the existing literature, focus is put on unbounded and\nnon-uniformly ergodic case by adapting the weight norm approach. In particular,\nit is shown how to combine geometric drift with local minorisation property in\norder to extend local span-contraction approach when the process as well as the\nlinked reward/cost functions are unbounded. For any predefined risk-aversion\nparameter, the existence of solution to suitable Bellman equation is shown and\nlinked to the underlying stochastic control problem. For completeness, examples\nof uncontrolled processes that satisfy the geometric drift assumption are\nprovided.\n"
    },
    {
        "paper_id": 1906.06478,
        "authors": "Ivan Guo, Gregoire Loeper, Shiyi Wang",
        "title": "Calibration of Local-Stochastic Volatility Models by Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a semi-martingale optimal transport problem and its\napplication to the calibration of Local-Stochastic Volatility (LSV) models.\nRather than considering the classical constraints on marginal distributions at\ninitial and final time, we optimise our cost function given the prices of a\nfinite number of European options. We formulate the problem as a convex\noptimisation problem, for which we provide a PDE formulation along with its\ndual counterpart. Then we solve numerically the dual problem, which involves a\nfully non-linear Hamilton-Jacobi-Bellman equation. The method is tested by\ncalibrating a Heston-like LSV model with simulated data and foreign exchange\nmarket data.\n"
    },
    {
        "paper_id": 1906.06483,
        "authors": "Wei-Cheng Chen, Wei-Ho Chung",
        "title": "Option Pricing via Multi-path Autoregressive Monte Carlo Approach",
        "comments": "5 pages, 61st Meeting of EURO Working Group for Commodities and\n  Financial Modeling (EWGCFM), Financial derivative pricing, Autoregressive\n  process, Monte carlo simulation, Short-term option pricing",
        "journal-ref": "61st Meeting of EURO Working Group for Commodities and Financial\n  Modeling, 16-18 MAY 2018, Kaunas, Lithuania",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The pricing of financial derivatives, which requires massive calculations and\nclose-to-real-time operations under many trading and arbitrage scenarios, were\nlargely infeasible in the past. However, with the advancement of modern\ncomputing, the efficiency has substantially improved. In this work, we propose\nand design a multi-path option pricing approach via autoregression (AR) process\nand Monte Carlo Simulations (MCS). Our approach learns and incorporates the\nprice characteristics into AR process, and re-generates the price paths for\noptions. We apply our approach to price weekly options underlying Taiwan Stock\nExchange Capitalization Weighted Stock Index (TAIEX) and compare the results\nwith prior practiced models, e.g., Black-Scholes-Merton and Binomial Tree. The\nresults show that our approach is comparable with prior practiced models.\n"
    },
    {
        "paper_id": 1906.06648,
        "authors": "Takuji Arai and Ryoichi Suzuki",
        "title": "A Clark-Ocone type formula via Ito calculus and its application to\n  finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An explicit martingale representation for random variables described as a\nfunctional of a Levy process will be given. The Clark-Ocone theorem shows that\nintegrands appeared in a martingale representation are given by conditional\nexpectations of Malliavin derivatives. Our goal is to extend it to random\nvariables which are not Malliavin differentiable. To this end, we make use of\nIto's formula, instead of Malliavin calculus. As an application to mathematical\nfinance, we shall give an explicit representation of locally risk-minimizing\nstrategy of digital options for exponential Levy models. Since the payoff of\ndigital options is described by an indicator function, we also discuss the\nMalliavin differentiability of indicator functions with respect to Levy\nprocesses.\n"
    },
    {
        "paper_id": 1906.06711,
        "authors": "Graham Elliott, Nikolay Kudrin, Kaspar Wuthrich",
        "title": "Detecting p-hacking",
        "comments": null,
        "journal-ref": "Econometrica, Volume 90, Issue 2 (March 2022)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We theoretically analyze the problem of testing for $p$-hacking based on\ndistributions of $p$-values across multiple studies. We provide general results\nfor when such distributions have testable restrictions (are non-increasing)\nunder the null of no $p$-hacking. We find novel additional testable\nrestrictions for $p$-values based on $t$-tests. Specifically, the shape of the\npower functions results in both complete monotonicity as well as bounds on the\ndistribution of $p$-values. These testable restrictions result in more powerful\ntests for the null hypothesis of no $p$-hacking. When there is also publication\nbias, our tests are joint tests for $p$-hacking and publication bias. A\nreanalysis of two prominent datasets shows the usefulness of our new tests.\n"
    },
    {
        "paper_id": 1906.0693,
        "authors": "Raul Merino, Jan Posp\\'i\\v{s}il, Tom\\'a\\v{s} Sobotka and Josep Vives",
        "title": "Decomposition formula for jump diffusion models",
        "comments": null,
        "journal-ref": "Int. J. Theor. Appl. Finance 21(8), 1850052, 2018",
        "doi": "10.1142/S0219024918500528",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive a generic decomposition of the option pricing formula\nfor models with finite activity jumps in the underlying asset price process\n(SVJ models). This is an extension of the well-known result by Alos (2012) for\nHeston (1993) SV model. Moreover, explicit approximation formulas for option\nprices are introduced for a popular class of SVJ models - models utilizing a\nvariance process postulated by Heston (1993). In particular, we inspect in\ndetail the approximation formula for the Bates (1996) model with log-normal\njump sizes and we provide a numerical comparison with the industry standard -\nFourier transform pricing methodology. For this model, we also reformulate the\napproximation formula in terms of implied volatilities. The main advantages of\nthe introduced pricing approximations are twofold. Firstly, we are able to\nsignificantly improve computation efficiency (while preserving reasonable\napproximation errors) and secondly, the formula can provide an intuition on the\nvolatility smile behaviour under a specific SVJ model.\n"
    },
    {
        "paper_id": 1906.07101,
        "authors": "Raul Merino, Jan Posp\\'i\\v{s}il, Tom\\'a\\v{s} Sobotka, Tommi Sottinen\n  and Josep Vives",
        "title": "Decomposition formula for rough Volterra stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The research presented in this article provides an alternative option pricing\napproach for a class of rough fractional stochastic volatility models. These\nmodels are increasingly popular between academics and practitioners due to\ntheir surprising consistency with financial markets. However, they bring\nseveral challenges alongside. Most noticeably, even simple non-linear financial\nderivatives as vanilla European options are typically priced by means of\nMonte-Carlo (MC) simulations which are more computationally demanding than\nsimilar MC schemes for standard stochastic volatility models.\n  In this paper, we provide a proof of the prediction law for general Gaussian\nVolterra processes. The prediction law is then utilized to obtain an adapted\nprojection of the future squared volatility -- a cornerstone of the proposed\npricing approximation. Firstly, a decomposition formula for European option\nprices under general Volterra volatility models is introduced. Then we focus on\nparticular models with rough fractional volatility and we derive an explicit\nsemi-closed approximation formula. Numerical properties of the approximation\nfor a popular model -- the rBergomi model -- are studied and we propose a\nhybrid calibration scheme which combines the approximation formula alongside MC\nsimulations. This scheme can significantly speed up the calibration to\nfinancial markets as illustrated on a set of AAPL options.\n"
    },
    {
        "paper_id": 1906.07164,
        "authors": "Simone Farinelli and Hideyuki Takada",
        "title": "When Risks and Uncertainties Collide: Mathematical Finance for Arbitrage\n  Markets in a Quantum Mechanical View",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1509.03264,\n  arXiv:1904.11565, arXiv:1406.6805, arXiv:0910.1671",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric arbitrage theory reformulates a generic asset model possibly\nallowing for arbitrage by packaging all asset and their forward dynamics into a\nstochastic principal fibre bundle, with a connection whose parallel transport\nencodes discounting and portfolio rebalancing, and whose curvature measures, in\nthis geometric language, the instantaneous arbitrage capability generated by\nthe market itself. The asset and market portfolio dynamics have a quantum\nmechanical description, which is constructed by quantizing the deterministic\nversion of the stochastic Lagrangian system describing a market allowing for\narbitrage. Results, obtained by solving the Schroedinger equation, coincide\nwith those obtained by solving the stochastic Euler Lagrange equations derived\nby a variational principle and providing therefore consistency.\n"
    },
    {
        "paper_id": 1906.07491,
        "authors": "Robert G\\k{e}barowski, Pawe{\\l} O\\'swi\\k{e}cimka, Marcin W\\k{a}torek,\n  Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Detecting correlations and triangular arbitrage opportunities in the\n  Forex by means of multifractal detrended cross-correlations analysis",
        "comments": "accepted in Nonlinear Dynamics",
        "journal-ref": "Nonlinear Dynamics 98, 2349-2364 (2019)",
        "doi": "10.1007/s11071-019-05335-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multifractal detrended cross-correlation methodology is described and applied\nto Foreign exchange (Forex) market time series. Fluctuations of high frequency\nexchange rates of eight major world currencies over 2010-2018 period are used\nto study cross-correlations. The study is motivated by fundamental questions in\ncomplex systems' response to significant environmental changes and by potential\napplications in investment strategies, including detecting triangular arbitrage\nopportunities. Dominant multiscale cross-correlations between the exchange\nrates are found to typically occur at smaller fluctuation levels. However\nhierarchical organization of ties expressed in terms of dendrograms, with a\nnovel application of the multiscale cross-correlation coefficient, are more\npronounced at large fluctuations. The cross-correlations are quantified to be\nstronger on average between those exchange rate pairs that are bound within\ntriangular relations. Some pairs from outside triangular relations are however\nidentified to be exceptionally strongly correlated as compared to the average\nstrength of triangular correlations.This in particular applies to those\nexchange rates that involve Australian and New Zealand dollars and reflects\ntheir economic relations. Significant events with impact on the Forex are shown\nto induce triangular arbitrage opportunities which at the same time reduce\ncross--correlations on the smallest time scales and act destructively on the\nmultiscale organization of correlations. In 2010--2018 such instances took\nplace in connection with the Swiss National Bank intervention and the weakening\nof British pound sterling accompanying the initiation of Brexit procedure. The\nmethodology could be applicable to temporal and multiscale pattern detection in\nany time series.\n"
    },
    {
        "paper_id": 1906.07533,
        "authors": "Luis H. R. Alvarez E. and S\\\"oren Christensen",
        "title": "The Impact of Ambiguity on the Optimal Exercise Timing of Integral\n  Option Contracts",
        "comments": "23 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the impact of ambiguity on the optimal timing of a class of\ntwo-dimensional integral option contracts when the exercise payoff is a\npositively homogeneous measurable function. Hence, the considered class of\nexercise payoffs includes discontinuous functions as well. We identify a\nparameterized family of excessive functions generating an appropriate class of\nsupermartingales for the considered problems and then express the value of the\noptimal policy as well as the worst case measure in terms of these processes.\nThe advantage of our approach is that it reduces the analysis of the\nmultidimensional problem to the analysis of an ordinary one-dimensional static\noptimization problem. In that way it simplifies earlier treatments of the\nproblem without ambiguity considerably. We also illustrate our findings in\nexplicitly parameterized examples.\n"
    },
    {
        "paper_id": 1906.07786,
        "authors": "Lukas Ryll and Sebastian Seidens",
        "title": "Evaluating the Performance of Machine Learning Algorithms in Financial\n  Market Forecasting: A Comprehensive Survey",
        "comments": "22 pages, 1 figure, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  With increasing competition and pace in the financial markets, robust\nforecasting methods are becoming more and more valuable to investors. While\nmachine learning algorithms offer a proven way of modeling non-linearities in\ntime series, their advantages against common stochastic models in the domain of\nfinancial market prediction are largely based on limited empirical results. The\nsame holds true for determining advantages of certain machine learning\narchitectures against others. This study surveys more than 150 related articles\non applying machine learning to financial market forecasting. Based on a\ncomprehensive literature review, we build a table across seven main parameters\ndescribing the experiments conducted in these studies. Through listing and\nclassifying different algorithms, we also introduce a simple, standardized\nsyntax for textually representing machine learning algorithms. Based on\nperformance metrics gathered from papers included in the survey, we further\nconduct rank analyses to assess the comparative performance of different\nalgorithm classes. Our analysis shows that machine learning algorithms tend to\noutperform most traditional stochastic methods in financial market forecasting.\nWe further find evidence that, on average, recurrent neural networks outperform\nfeed forward neural networks as well as support vector machines which implies\nthe existence of exploitable temporal dependencies in financial time series\nacross multiple asset classes and geographies.\n"
    },
    {
        "paper_id": 1906.07834,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Ludovico Minati, Pawe{\\l} O\\'swi\\k{e}cimka,\n  Marek Stanuszek, Marcin W\\k{a}torek",
        "title": "Signatures of crypto-currency market decoupling from the Forex",
        "comments": null,
        "journal-ref": "Future Internet 11(7), 154 (2019)",
        "doi": "10.3390/fi11070154",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the high-frequency recordings from Kraken, a cryptocurrency exchange\nand professional trading platform that aims to bring Bitcoin and other\ncryptocurrencies into the mainstream, the multiscale cross-correlations\ninvolving the Bitcoin (BTC), Ethereum (ETH), Euro (EUR) and US dollar (USD) are\nstudied over the period between July 1, 2016 and December 31, 2018. It is shown\nthat the multiscaling characteristics of the exchange rate fluctuations related\nto the cryptocurrency market approach those of the Forex. This, in particular,\napplies to the BTC/ETH exchange rate, whose Hurst exponent by the end of 2018\nstarted approaching the value of 0.5, which is characteristic of the mature\nworld markets. Furthermore, the BTC/ETH direct exchange rate has already\ndeveloped multifractality, which manifests itself via broad singularity\nspectra. A particularly significant result is that the measures applied for\ndetecting cross-correlations between the dynamics of the BTC/ETH and EUR/USD\nexchange rates do not show any noticeable relationships. This may be taken as\nan indication that the cryptocurrency market has begun decoupling itself from\nthe Forex.\n"
    },
    {
        "paper_id": 1906.08088,
        "authors": "Xue Guo, Hu Zhang, Tianhai Tian",
        "title": "Multi-Likelihood Methods for Developing Stock Relationship Networks\n  Using Financial Big Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Development of stock networks is an important approach to explore the\nrelationship between different stocks in the era of big-data. Although a number\nof methods have been designed to construct the stock correlation networks, it\nis still a challenge to balance the selection of prominent correlations and\nconnectivity of networks. To address this issue, we propose a new approach to\nselect essential edges in stock networks and also maintain the connectivity of\nestablished networks. This approach uses different threshold values for\nchoosing the edges connecting to a particular stock, rather than employing a\nsingle threshold value in the existing asset-value method. The innovation of\nour algorithm includes the multiple distributions in a maximum likelihood\nestimator for selecting the threshold value rather than the single distribution\nestimator in the existing methods. Using the Chinese Shanghai security market\ndata of 151 stocks, we develop a stock relationship network and analyze the\ntopological properties of the developed network. Our results suggest that the\nproposed method is able to develop networks that maintain appropriate\nconnectivities in the type of assets threshold methods.\n"
    },
    {
        "paper_id": 1906.08244,
        "authors": "Abdul Rahman Shaikh and Hamed Alhoori",
        "title": "Predicting Patent Citations to measure Economic Impact of Scholarly\n  Research",
        "comments": "2 Pages, 1 figure, JCDL conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A crucial goal of funding research and development has always been to advance\neconomic development. On this basis, a consider-able body of research\nundertaken with the purpose of determining what exactly constitutes economic\nimpact and how to accurately measure that impact has been published. Numerous\nindicators have been used to measure economic impact, although no single\nindicator has been widely adapted. Based on patent data collected from\nAltmetric we predict patent citations through various social media features\nusing several classification models. Patents citing a research paper implies\nthe potential it has for direct application inits field. These predictions can\nbe utilized by researchers in deter-mining the practical applications for their\nwork when applying for patents.\n"
    },
    {
        "paper_id": 1906.0841,
        "authors": "Shihao Zhu, Jingtao Shi",
        "title": "Optimal Reinsurance and Investment Strategies under Mean-Variance\n  Criteria: Partial and Full Information",
        "comments": "22 pages, 5 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with an optimal reinsurance and investment problem\nfor an insurance firm under the criterion of mean-variance. The driving\nBrownian motion and the rate in return of the risky asset price dynamic\nequation cannot be directly observed. And the short-selling of stocks is\nprohibited. The problem is formulated as a stochastic linear-quadratic (LQ)\noptimal control problem where the control variables are constrained. Based on\nthe separation principle and stochastic filtering theory, the partial\ninformation problem is solved. Efficient strategies and efficient frontier are\npresented in closed forms via solutions to two extended stochastic Riccati\nequations. As a comparison, the efficient strategies and efficient frontier are\ngiven by the viscosity solution for the Hamilton-Jacobi-Bellman (HJB) equation\nin the full information case. Some numerical illustrations are also provided.\n"
    },
    {
        "paper_id": 1906.08617,
        "authors": "Marnix Van Soom, Milan van den Heuvel, Jan Ryckebusch, Koen Schoors",
        "title": "Loan maturity aggregation in interbank lending networks obscures\n  mesoscale structure and economic functions",
        "comments": null,
        "journal-ref": "Sci Rep, vol. 9, Aug. 2019",
        "doi": "10.1038/s41598-019-48924-5",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since the 2007-2009 financial crisis, substantial academic effort has been\ndedicated to improving our understanding of interbank lending networks (ILNs).\nBecause of data limitations or by choice, the literature largely lacks multiple\nloan maturities. We employ a complete interbank loan contract dataset to\ninvestigate whether maturity details are informative of the network structure.\nApplying the layered stochastic block model of Peixoto (2015) and other tools\nfrom network science on a time series of bilateral loans with multiple maturity\nlayers in the Russian ILN, we find that collapsing all such layers consistently\nobscures mesoscale structure. The optimal maturity granularity lies between\ncompletely collapsing and completely separating the maturity layers and depends\non the development phase of the interbank market, with a more developed market\nrequiring more layers for optimal description. Closer inspection of the\ninferred maturity bins associated with the optimal maturity granularity reveals\nspecific economic functions, from liquidity intermediation to financing.\nCollapsing a network with multiple underlying maturity layers or extracting one\nsuch layer, common in economic research, is therefore not only an incomplete\nrepresentation of the ILN's mesoscale structure, but also conceals existing\neconomic functions. This holds important insights and opportunities for\ntheoretical and empirical studies on interbank market functioning, contagion,\nstability, and on the desirable level of regulatory data disclosure.\n"
    },
    {
        "paper_id": 1906.08636,
        "authors": "Shanka Subhra Mondal, Sharada Prasanna Mohanty, Benjamin Harlander,\n  Mehmet Koseoglu, Lance Rane, Kirill Romanov, Wei-Kai Liu, Pranoot Hatwar,\n  Marcel Salathe, Joe Byrum",
        "title": "Investment Ranking Challenge: Identifying the best performing stocks\n  based on their semi-annual returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the IEEE Investment ranking challenge 2018, participants were asked to\nbuild a model which would identify the best performing stocks based on their\nreturns over a forward six months window. Anonymized financial predictors and\nsemi-annual returns were provided for a group of anonymized stocks from 1996 to\n2017, which were divided into 42 non-overlapping six months period. The second\nhalf of 2017 was used as an out-of-sample test of the model's performance.\nMetrics used were Spearman's Rank Correlation Coefficient and Normalized\nDiscounted Cumulative Gain (NDCG) of the top 20% of a model's predicted\nrankings. The top six participants were invited to describe their approach. The\nsolutions used were varied and were based on selecting a subset of data to\ntrain, combination of deep and shallow neural networks, different boosting\nalgorithms, different models with different sets of features, linear support\nvector machine, combination of convoltional neural network (CNN) and Long short\nterm memory (LSTM).\n"
    },
    {
        "paper_id": 1906.08667,
        "authors": "Johannes Wachs, J\\'anos Kert\\'esz",
        "title": "A network approach to cartel detection in public auction markets",
        "comments": null,
        "journal-ref": "Scientific Reports, 2019",
        "doi": "10.1038/s41598-019-47198-1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Competing firms can increase profits by setting prices collectively, imposing\nsignificant costs on consumers. Such groups of firms are known as cartels and\nbecause this behavior is illegal, their operations are secretive and difficult\nto detect. Cartels feel a significant internal obstacle: members feel short-run\nincentives to cheat. Here we present a network-based framework to detect\npotential cartels in bidding markets based on the idea that the chance a group\nof firms can overcome this obstacle and sustain cooperation depends on the\npatterns of its interactions. We create a network of firms based on their\nco-bidding behavior, detect interacting groups, and measure their cohesion and\nexclusivity, two group-level features of their collective behavior. Applied to\na market for school milk, our method detects a known cartel and calculates that\nit has high cohesion and exclusivity. In a comprehensive set of nearly 150,000\npublic contracts awarded by the Republic of Georgia from 2011 to 2016, detected\ngroups with high cohesion and exclusivity are significantly more likely to\ndisplay traditional markers of cartel behavior. We replicate this relationship\nbetween group topology and the emergence of cooperation in a simulation model.\nOur method presents a scalable, unsupervised method to find groups of firms in\nbidding markets ideally positioned to form lasting cartels.\n"
    },
    {
        "paper_id": 1906.08872,
        "authors": "Brendan Hoover, Richard S. Middleton, and Sean Yaw",
        "title": "CostMAP: An open-source software package for developing cost surfaces",
        "comments": "Pre-print, 17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cost Surfaces are a quantitative means of assigning social, environmental,\nand engineering costs that impact movement across landscapes. Cost surfaces are\na crucial aspect of route optimization and least cost path (LCP) calculations\nand are used in a wide range of disciplines including computer science,\nlandscape ecology, and energy infrastructure modeling. Linear features present\na key weakness to traditional routing calculations along costs surfaces because\nthey cannot identify whether moving from a cell to its adjacent neighbors\nconstitutes crossing a linear barrier (increased cost) or following a corridor\n(reduced cost). Following and avoiding linear features can drastically change\npredicted routes. In this paper, we introduce an approach to address this\n\"adjacency\" issue using a search kernel that identifies these critical barriers\nand corridors. We have built this approach into a new Java-based open-source\nsoftware package called CostMAP (cost surface multi-layer aggregation program),\nwhich calculates cost surfaces and cost networks using the search kernel.\nCostMAP not only includes the new adjacency capability, it is also a versatile\nmulti-platform package that allows users to input multiple GIS data layers and\nto set weights and rules for developing a weighted-cost network. We compare\nCostMAP performance with traditional cost surface approaches and show\nsignificant performance gains, both following corridors and avoiding barriers,\nusing examples in a movement ecology framework and pipeline routing for carbon\ncapture, and storage (CCS). We also demonstrate that the new software can\nstraightforwardly calculate cost surfaces on a national scale.\n"
    },
    {
        "paper_id": 1906.08892,
        "authors": "Ippei Suzuki and Takashi Shinzato",
        "title": "Macroscopic theorem of the portfolio optimization problem with a\n  risk-free asset",
        "comments": "22 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The investment risk minimization problem with budget and return constraints\nhas been the subject of research using replica analysis but there are\nshortcomings in the extant literature. With respect to Tobin's separation\ntheorem and the capital asset pricing model, it is necessary to investigate the\nimplications of a risk-free asset and examine its influence on the optimal\nportfolio. Accordingly, in this work, we explore the investment risk\nminimization problem in the presence of a risk-free asset with budget and\nreturn constraints. Moreover, we discuss opportunity loss, the Pythagorean\ntheorem of the Sharpe ratio, and Tobin's separation theorem.\n"
    },
    {
        "paper_id": 1906.08933,
        "authors": "Ignasi Merediz-Sol\\`a, Aurelio F. Bariviera",
        "title": "A bibliometric analysis of Bitcoin scientific production",
        "comments": null,
        "journal-ref": "Research in International Business and Finance, 50, 294-305 (2019)",
        "doi": "10.1016/J.RIBAF.2019.06.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain technology, and more specifically Bitcoin (one of its foremost\napplications), have been receiving increasing attention in the scientific\ncommunity. The first publications with Bitcoin as a topic, can be traced back\nto 2012. In spite of this short time span, the production magnitude (1162\npapers) makes it necessary to make a bibliometric study in order to observe\nresearch clusters, emerging topics, and leading scholars. Our paper is aimed at\nstudying the scientific production only around bitcoin, excluding other\nblockchain applications. Thus, we restricted our search to papers indexed in\nthe Web of Science Core Collection, whose topic is \"bitcoin\". This database is\nsuitable for such diverse disciplines such as economics, engineering,\nmathematics, and computer science. This bibliometric study draws the landscape\nof the current state and trends of Bitcoin-related research in different\nscientific disciplines.\n"
    },
    {
        "paper_id": 1906.09024,
        "authors": "Joshua Zoen Git Hiew, Xin Huang, Hao Mou, Duan Li, Qi Wu, Yabo Xu",
        "title": "BERT-based Financial Sentiment Index and LSTM-based Stock Return\n  Predictability",
        "comments": "Manuscript",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional sentiment construction in finance relies heavily on the\ndictionary-based approach, with a few exceptions using simple machine learning\ntechniques such as Naive Bayes classifier. While the current literature has not\nyet invoked the rapid advancement in the natural language processing, we\nconstruct in this research a textual-based sentiment index using a well-known\npre-trained model BERT developed by Google, especially for three actively\ntrading individual stocks in Hong Kong market with at the same time the hot\ndiscussion on Weibo.com. On the one hand, we demonstrate a significant\nenhancement of applying BERT in financial sentiment analysis when compared with\nthe existing models. On the other hand, by combining with the other two\ncommonly-used methods when it comes to building the sentiment index in the\nfinancial literature, i.e., the option-implied and the market-implied\napproaches, we propose a more general and comprehensive framework for the\nfinancial sentiment analysis, and further provide convincing outcomes for the\npredictability of individual stock return by combining LSTM (with a feature of\na nonlinear mapping). It is significantly distinct with the dominating\neconometric methods in sentiment influence analysis which are all of a nature\nof linear regression.\n"
    },
    {
        "paper_id": 1906.09034,
        "authors": "Martin Forde, Stefan Gerhold, Benjamin Smith",
        "title": "Small-time, large-time and $H\\to 0$ asymptotics for the Rough Heston\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize the behaviour of the Rough Heston model introduced by\nJaisson\\&Rosenbaum \\cite{JR16} in the small-time, large-time and $\\alpha \\to\n1/2$ (i.e. $H\\to 0$) limits. We show that the short-maturity smile scales in\nqualitatively the same way as a general rough stochastic volatility model (cf.\\\n\\cite{FZ17}, \\cite{FGP18a} et al.), and the rate function is equal to the\nFenchel-Legendre transform of a simple transformation of the solution to the\nsame Volterra integral equation (VIE) that appears in \\cite{ER19}, but with the\ndrift and mean reversion terms removed. The solution to this VIE satisfies a\nspace-time scaling property which means we only need to solve this equation for\nthe moment values of $p=1$ and $p=-1$ so the rate function can be efficiently\ncomputed using an Adams scheme or a power series, and we compute a power series\nin the log-moneyness variable for the asymptotic implied volatility which\nyields tractable expressions for the implied vol skew and convexity. The\nlimiting asymptotic smile in the large-maturity regime is obtained via a\nstability analysis of the fixed points of the VIE, and is the same as for the\nstandard Heston model in \\cite{FJ11}. Finally, using L\\'{e}vy's convergence\ntheorem, we show that the log stock price $X_t$ tends weakly to a non-symmetric\nrandom variable $X^{(1/2)}_t$ as $\\alpha \\to 1/2$ (i.e. $H\\to 0$) whose mgf is\nalso the solution to the Rough Heston VIE with $\\alpha=1/2$, and we show that\n$X^{(1/2)}_t/\\sqrt{t}$ tends weakly to a non-symmetric random variable as $t\\to\n0$, which leads to a non-flat non-symmetric asymptotic smile in the Edgeworth\nregime. We also show that the third moment of the log stock price tends to a\nfinite constant as $H\\to 0$ (in contrast to the Rough Bergomi model discussed\nin \\cite{FFGS20} where the skew flattens or blows up) and the $V$ process\nconverges on pathspace to a random tempered distribution.\n"
    },
    {
        "paper_id": 1906.09431,
        "authors": "D. Belomestny, M. Kaledin and J. Schoenmakers",
        "title": "Semi-tractability of optimal stopping problems via a weighted stochastic\n  mesh algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we propose a Weighted Stochastic Mesh (WSM) Algorithm for\napproximating the value of a discrete and continuous time optimal stopping\nproblem. We prove that in the discrete case the WSM algorithm leads to\n  semi-tractability of the corresponding optimal problems in the sense that its\ncomplexity is bounded in order by $\\varepsilon^{-4}\\log^{d+2}(1/\\varepsilon)$\nwith $d$ being the dimension of the underlying Markov chain. Furthermore we\nstudy the WSM approach in the context of continuous time optimal stopping\nproblems and derive the corresponding complexity bounds. Although we can not\nprove semi-tractability in this case, our bounds turn out to be the tightest\nones among the bounds known for the existing algorithms in the literature. We\nillustrate our theoretical findings by a numerical example.\n"
    },
    {
        "paper_id": 1906.09632,
        "authors": "Silvia Bartolucci and Andrei Kirilenko",
        "title": "A Model of the Optimal Selection of Crypto Assets",
        "comments": "19 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a modelling framework for the optimal selection of crypto assets.\nCrypto assets differ by two essential features: security (technological) and\nstability (governance). Investors make choices over crypto assets similarly to\nhow they make choices by using a recommender app: the app presents each\ninvestor with a pair of crypto assets with certain security-stability\ncharacteristics to be compared. Each investor submits its preference for\nadopting one of the two assets to the app. The app, in turn, provides a\nrecommendation on whether the proposed adoption is sensible given the assets'\nessential features, information about the adoption choices of all other\ninvestors, and expected future economic benefits of adoption. Investors\ncontinue making their adoption choices over all pairs of crypto assets until\ntheir expected future economic benefits can no longer be improved upon. This\nconstitutes an optimal selection decision. We simulate optimal selection\ndecisions considering the behaviour of different types of investors, driven by\ntheir attitudes towards assets' features. We find a variety of possible\nemergent outcomes for the investments in the crypto-ecosystem and the future\nadoption of the crypto assets.\n"
    },
    {
        "paper_id": 1906.09694,
        "authors": "Haodong Bai and Frank Z. Xing and Erik Cambria and Win-Bin Huang",
        "title": "Business Taxonomy Construction Using Concept-Level Hierarchical\n  Clustering",
        "comments": "Accepted to The First Workshop on Financial Technology and Natural\n  Language Processing (FinNLP@IJCAI-19)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Business taxonomies are indispensable tools for investors to do equity\nresearch and make professional decisions. However, to identify the structure of\nindustry sectors in an emerging market is challenging for two reasons. First,\nexisting taxonomies are designed for mature markets, which may not be the\nappropriate classification for small companies with innovative business models.\nSecond, emerging markets are fast-developing, thus the static business\ntaxonomies cannot promptly reflect the new features. In this article, we\npropose a new method to construct business taxonomies automatically from the\ncontent of corporate annual reports. Extracted concepts are hierarchically\nclustered using greedy affinity propagation. Our method requires less\nsupervision and is able to discover new terms. Experiments and evaluation on\nthe Chinese National Equities Exchange and Quotations (NEEQ) market show\nseveral advantages of the business taxonomy we build. Our results provide an\neffective tool for understanding and investing in the new growth companies.\n"
    },
    {
        "paper_id": 1906.09698,
        "authors": "Yuan Yuan, Tracy Liu, Chenhao Tan, Qian Chen, Alex Pentland, Jie Tang",
        "title": "Gift Contagion in Online Groups: Evidence From Virtual Red Packets",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gifts are important instruments for forming bonds in interpersonal\nrelationships. Our study analyzes the phenomenon of gift contagion in online\ngroups. Gift contagion encourages social bonds by prompting further gifts; it\nmay also promote group interaction and solidarity. Using data on 36 million\nonline red packet gifts on a large social site in East Asia, we leverage a\nnatural experimental design to identify the social contagion of gift giving in\nonline groups. Our natural experiment is enabled by the randomization of the\ngift amount allocation algorithm on the platform, which addresses the common\nchallenge of causal identifications in observational data. Our study provides\nevidence of gift contagion: on average, receiving one additional dollar causes\na recipient to send 18 cents back to the group within the subsequent 24 hours.\nDecomposing this effect, we find that it is mainly driven by the extensive\nmargin -- more recipients are triggered to send red packets. Moreover, we find\nthat this effect is stronger for \"luckiest draw\" recipients, suggesting the\npresence of a group norm regarding the next red packet sender. Finally, we\ninvestigate the moderating effects of group- and individual-level social\nnetwork characteristics on gift contagion as well as the causal impact of\nreceiving gifts on group network structure. Our study has implications for\npromoting group dynamics and designing marketing strategies for product\nadoption.\n"
    },
    {
        "paper_id": 1906.09729,
        "authors": "Samuel Drapeau and Mekonnen Tadese",
        "title": "Relative Bound and Asymptotic Comparison of Expectile with Respect to\n  Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expectile bears some interesting properties in comparison to the industry\nwide expected shortfall in terms of assessment of tail risk. We study the\nrelationship between expectile and expected shortfall using duality results and\nthe link to optimized certainty equivalent. Lower and upper bounds of expectile\nare derived in terms of expected shortfall as well as a characterization of\nexpectile in terms of expected shortfall. Further, we study the asymptotic\nbehavior of expectile with respect to expected shortfall as the confidence\nlevel goes to $1$ in terms of extreme value distributions. We use concentration\ninequalities to illustrate that the estimation of value at risk requires larger\nsample size than expected shortfall and expectile for heavy tail distributions\nwhen $\\alpha$ is close to $1$. Illustrating the formulation of expectile in\nterms of expected shortfall, we also provide explicit or semi-explicit\nexpressions of expectile and some simulation results for some classical\ndistributions.\n"
    },
    {
        "paper_id": 1906.09961,
        "authors": "Chao Wang, Richard Gerlach",
        "title": "Semi-parametric Realized Nonlinear Conditional Autoregressive Expectile\n  and Expected Shortfall",
        "comments": "41 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1805.08653, arXiv:1807.02422, arXiv:1612.08488",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A joint conditional autoregressive expectile and Expected Shortfall framework\nis proposed. The framework is extended through incorporating a measurement\nequation which models the contemporaneous dependence between the realized\nmeasures and the latent conditional expectile. Nonlinear threshold\nspecification is further incorporated into the proposed framework. A Bayesian\nMarkov Chain Monte Carlo method is adapted for estimation, whose properties are\nassessed and compared with maximum likelihood via a simulation study.\nOne-day-ahead VaR and ES forecasting studies, with seven market indices,\nprovide empirical support to the proposed models.\n"
    },
    {
        "paper_id": 1906.1003,
        "authors": "Yan Yang",
        "title": "A New Solution to Market Definition: An Approach Based on\n  Multi-dimensional Substitutability Statistics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market definition is an important component in the premerger investigation,\nbut the models used in the market definition have not developed much in the\npast three decades since the Critical Loss Analysis (CLA) was proposed in 1989.\nThe CLA helps the Hypothetical Monopolist Test to determine whether the\nhypothetical monopolist is going to profit from the small but significant and\nnon-transitory increase in price (SSNIP). However, the CLA has long been\ncriticized by academic scholars for its tendency to conclude a narrow market.\nAlthough the CLA was adopted by the 2010 Horizontal Merger Guidelines (the 2010\nGuidelines), the criticisms are likely still valid. In this dissertation, we\ndiscussed the mathematical deduction of CLA, the data used, and the SSNIP\ndefined by the Agencies. Based on our research, we concluded that the narrow\nmarket conclusion was due to the incorrect implementation of the CLA; not the\nmodel itself. On the other hand, there are other unresolvable problems in the\nCLA and the Hypothetical Monopolist Test. The SSNIP test and the CLA are bright\nresolutions for market definition problem during their time, but we have more\nadvanced tools to solve the task nowadays. In this dissertation, we propose a\nmodel which is based directly on the multi-dimensional substitutability between\nthe products and is capable of maximizing the substitutability of product\nfeatures within each group. Since the 2010 Guidelines does not exclude the use\nof models other than the ones mentioned by the Guidelines, our method can\nhopefully supplement the current models to show a better picture of the\nsubstitutive relations and provide a more stable definition of the market.\n"
    },
    {
        "paper_id": 1906.10084,
        "authors": "Alex Garivaltis",
        "title": "Long Run Feedback in the Broker Call Money Market",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I unravel the basic long run dynamics of the broker call money market, which\nis the pile of cash that funds margin loans to retail clients (read: continuous\ntime Kelly gamblers). Call money is assumed to supply itself perfectly\ninelastically, and to continuously reinvest all principal and interest. I show\nthat the relative size of the money market (that is, relative to the Kelly\nbankroll) is a martingale that nonetheless converges in probability to zero.\nThe margin loan interest rate is a submartingale that converges in mean square\nto the choke price $r_\\infty:=\\nu-\\sigma^2/2$, where $\\nu$ is the asymptotic\ncompound growth rate of the stock market and $\\sigma$ is its annual volatility.\nIn this environment, the gambler no longer beats the market asymptotically a.s.\nby an exponential factor (as he would under perfectly elastic supply). Rather,\nhe beats the market asymptotically with very high probability (think 98%) by a\nfactor (say 1.87, or 87% more final wealth) whose mean cannot exceed what the\nleverage ratio was at the start of the model (say, $2:1$). Although the ratio\nof the gambler's wealth to that of an equivalent buy-and-hold investor is a\nsubmartingale (always expected to increase), his realized compound growth rate\nconverges in mean square to $\\nu$. This happens because the equilibrium\nleverage ratio converges to $1:1$ in lockstep with the gradual rise of margin\nloan interest rates.\n"
    },
    {
        "paper_id": 1906.10121,
        "authors": "Bradley J. Pillay and Absalom E. Ezugwu",
        "title": "Metaheuristics optimized feedforward neural networks for efficient stock\n  price prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The prediction of stock prices is an important task in economics, investment\nand making financial decisions. This has, for decades, spurred the interest of\nmany researchers to make focused contributions to the design of accurate stock\nprice predictive models; of which some have been utilized to predict the next\nday opening and closing prices of the stock indices. This paper proposes the\ndesign and implementation of a hybrid symbiotic organisms search trained\nfeedforward neural network model for effective and accurate stock price\nprediction. The symbiotic organisms search algorithm is used as an efficient\noptimization technique to train the feedforward neural networks, while the\nresulting training process is used to build a better stock price prediction\nmodel. Furthermore, the study also presents a comparative performance\nevaluation of three different stock price forecasting models; namely, the\nparticle swarm optimization trained feedforward neural network model, the\ngenetic algorithm trained feedforward neural network model and the well-known\nARIMA model. The system developed in support of this study utilizes sixteen\nstock indices as time series datasets for training and testing purpose. Three\nstatistical evaluation measures are used to compare the results of the\nimplemented models, namely the root mean squared error, the mean absolute\npercentage error and the mean absolution deviation. The computational results\nobtained reveal that the symbiotic organisms search trained feedforward neural\nnetwork model exhibits outstanding predictive performance compared to the other\nmodels. However, the performance study shows that the three metaheuristics\ntrained feedforward neural network models have promising predictive competence\nfor solving problems of high dimensional nonlinear time series data, which are\ndifficult to capture by traditional models.\n"
    },
    {
        "paper_id": 1906.10325,
        "authors": "David Toth, Bruce Jones",
        "title": "Against the Norm: Modeling Daily Stock Returns with the Laplace\n  Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modeling stock returns is not a new task for mathematicians, investors, and\nportfolio managers, but it remains a difficult objective due to the ebb and\nflow of stock markets. One common solution is to approximate the distribution\nof stock returns with a normal distribution. However, normal distributions\nplace infinitesimal probabilities on extreme outliers, but these outliers are\nof particular importance in the practice of investing. In this paper, we\ninvestigate the normality of the distribution of daily returns of major stock\nmarket indices. We find that the normal distribution is not a good model for\nstock returns, even over several years' worth of data. Moreover, we propose\nusing the Laplace distribution as a model for daily stock returns.\n"
    },
    {
        "paper_id": 1906.10372,
        "authors": "Nick Whiteley",
        "title": "Dynamic time series clustering via volatility change-points",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note outlines a method for clustering time series based on a statistical\nmodel in which volatility shifts at unobserved change-points. The model\naccommodates some classical stylized features of returns and its relation to\nGARCH is discussed. Clustering is performed using a probability metric\nevaluated between posterior distributions of the most recent change-point\nassociated with each series. This implies series are grouped together at a\ngiven time if there is evidence the most recent shifts in their respective\nvolatilities were coincident or closely timed. The clustering method is\ndynamic, in that groupings may be updated in an online manner as data arrive.\nNumerical results are given analyzing daily returns of constituents of the S&P\n500.\n"
    },
    {
        "paper_id": 1906.10388,
        "authors": "Lasko Basnarkov, Viktor Stojkoski, Zoran Utkovski and Ljupco Kocarev",
        "title": "Lead-lag Relationships in Foreign Exchange Markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.122986",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lead-lag relationships among assets represent a useful tool for analyzing\nhigh frequency financial data. However, research on these relationships\npredominantly focuses on correlation analyses for the dynamics of stock prices,\nspots and futures on market indexes, whereas foreign exchange data have been\nless explored. To provide a valuable insight on the nature of the lead-lag\nrelationships in foreign exchange markets here we perform a detailed study for\nthe one-minute log returns on exchange rates through three different\napproaches: i) lagged correlations, ii) lagged partial correlations and iii)\nGranger causality. In all studies, we find that even though for most pairs of\nexchange rates lagged effects are absent, there are many pairs which pass\nstatistical significance tests. Out of the statistically significant\nrelationships, we construct directed networks and investigate the influence of\nindividual exchange rates through the PageRank algorithm. The algorithm, in\ngeneral, ranks stock market indexes quoted in their respective currencies, as\nmost influential. In contrast to the claims of the efficient market hypothesis,\nthese findings suggest that all market information does not spread\ninstantaneously.\n"
    },
    {
        "paper_id": 1906.10624,
        "authors": "Christoph J. B\\\"orner, Ingo Hoffmann, Fabian Poetter, Tim Schmitz",
        "title": "On Capital Allocation under Information Constraints",
        "comments": "22 pages, 2 Figure, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Attempts to allocate capital across a selection of different investments are\noften hampered by the fact that investors' decisions are made under limited\ninformation (no historical return data) and during an extremely limited\ntimeframe. Nevertheless, in some cases, rational investors with a certain level\nof experience are able to ordinally rank investment alternatives through\nrelative assessments of the probabilities that investments will be successful.\nHowever, to apply traditional portfolio optimization models, analysts must use\nhistorical (or simulated/expected) return data as the basis for their\ncalculations. This paper develops an alternative portfolio optimization\nframework that is able to handle this kind of information (given by an ordinal\nranking of investment alternatives) and to calculate an optimal capital\nallocation based on a Cobb-Douglas function, which we call the Sorted Weighted\nPortfolio (SWP). Considering risk-neutral investors, we show that the results\nof this portfolio optimization model usually outperform the output generated by\nthe (intuitive) Equally Weighted Portfolio (EWP) of different investment\nalternatives, which is the result of optimization when one is unable to\nincorporate additional data (the ordinal ranking of the alternatives). To\nfurther extend this work, we show that our model can also address risk-averse\ninvestors to capture correlation effects.\n"
    },
    {
        "paper_id": 1906.10865,
        "authors": "Frederico Botafogo",
        "title": "The Syntax of the Accounting Language: A First Step",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review and interpret two basic propositions published by Ellerman (2014).\nThe propositions address the algebraic structure of T accounts and double entry\nbookkeeping (DEB). The paper builds on this previous contribution with the view\nof reconciling the two, apparently dichotomous, perspectives of accounting\nmeasurement: the one that focuses preferably on the stock of wealth and to the\none that focuses preferably on the flow of income. The paper claims that\nT-accounts and DEB have an underlying algebraic structure suitable for\napproaching measurement from either or both perspectives. Accountants\npreferences for stocks or flows can be framed in ways which are mutually\nconsistent. The paper is a first step in addressing this consistency issue. It\navoids the difficult mathematics of abstract algebra by applying the concept of\nsyntax to accounting numbers such that the accounting procedure qualifies as a\nformal language with which accountants convey meaning.\n"
    },
    {
        "paper_id": 1906.10888,
        "authors": "Martin Kegnenlezom, Patrice Takam Soh, Antoine-Marie Bogso, Yves\n  Emvudu Wono",
        "title": "European Option Pricing of electricity under exponential functional of\n  L\\'evy processes with Price-Cap principle",
        "comments": "19 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model for electricity pricing based on the price cap\nprinciple. The particularity of the model is that the asset price is an\nexponential functional of a jump L\\'evy process. This model can capture both\nmean reversion and jumps which are observed in electricity market. It is shown\nthat the value of an European option of this asset is the unique viscosity\nsolution of a partial integro-differential equation (PIDE). A numerical\napproximation of this solution by the finite differences method is provided.\nThe consistency, stability and convergence results of the scheme are given.\nNumerical simulations are performed under a smooth initial condition.\n"
    },
    {
        "paper_id": 1906.10933,
        "authors": "Maria Arduca, Pablo Koch-Medina, Cosimo Munari",
        "title": "Dual representations for systemic risk measures based on acceptance sets",
        "comments": "27 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish dual representations for systemic risk measures based on\nacceptance sets in a general setting. We deal with systemic risk measures of\nboth \"first allocate, then aggregate\" and \"first aggregate, then allocate\"\ntype. In both cases, we provide a detailed analysis of the corresponding\nsystemic acceptance sets and their support functions. The same approach\ndelivers a simple and self-contained proof of the dual representation of\nutility-based risk measures for univariate positions.\n"
    },
    {
        "paper_id": 1906.11023,
        "authors": "Stefano Ugolini (LEREPS)",
        "title": "The Coevolution of Banks and Corporate Securities Markets: The Financing\n  of Belgium's Industrial Take-Off in the 1830s",
        "comments": null,
        "journal-ref": "Business History, Taylor & Francis (Routledge), 2019, pp.1-22",
        "doi": "10.1080/00076791.2019.1621293",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent developments in the literature on financial architecture suggest that\nbanks and markets not only coexist, but also coevolve in ways that are\nnon-neutral from the viewpoint of optimality. This article aims to analyse the\nconcrete mechanisms of this coevolution by focusing on a very relevant case\nstudy: Belgium (the first Continental country to industrialize) at the time of\nthe very first emergence of a modern financial system (the 1830s). The article\nshows that intermediaries played a crucial role in developing secondary\nsecurities markets (as banks acted as securitizers), but market conditions also\nhad a strong feedback on banks' balance sheets and activities (as banks also\nacted as market-makers for the securities they had issued). The findings\nsuggest that not only structural, but also cyclical factors can be important\ndeterminants of changes in financial architecture.\n"
    },
    {
        "paper_id": 1906.11046,
        "authors": "Wenhang Bao, Xiao-yang Liu",
        "title": "Multi-Agent Deep Reinforcement Learning for Liquidation Strategy\n  Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Liquidation is the process of selling a large number of shares of one stock\nsequentially within a given time frame, taking into consideration the costs\narising from market impact and a trader's risk aversion. The main challenge in\noptimizing liquidation is to find an appropriate modeling system that can\nincorporate the complexities of the stock market and generate practical trading\nstrategies. In this paper, we propose to use multi-agent deep reinforcement\nlearning model, which better captures high-level complexities comparing to\nvarious machine learning methods, such that agents can learn how to make the\nbest selling decisions. First, we theoretically analyze the Almgren and Chriss\nmodel and extend its fundamental mechanism so it can be used as the multi-agent\ntrading environment. Our work builds the foundation for future multi-agent\nenvironment trading analysis. Secondly, we analyze the cooperative and\ncompetitive behaviours between agents by adjusting the reward functions for\neach agent, which overcomes the limitation of single-agent reinforcement\nlearning algorithms. Finally, we simulate trading and develop an optimal\ntrading strategy with practical constraints by using a reinforcement learning\nmethod, which shows the capabilities of reinforcement learning methods in\nsolving realistic liquidation problems.\n"
    },
    {
        "paper_id": 1906.11186,
        "authors": "Pascal Traccucci, Luc Dumontier, Guillaume Garchery and Benjamin Jacot",
        "title": "A Triptych Approach for Reverse Stress Testing of Complex Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The quest for diversification has led to an increasing number of complex\nfunds with a high number of strategies and non-linear payoffs. The new\ngeneration of Alternative Risk Premia (ARP) funds are an example that has been\nvery popular in recent years. For complex funds like these, a Reverse Stress\nTest (RST) is regarded by the industry and regulators as a better\nforward-looking risk measure than a Value-at-Risk (VaR). We present an Extended\nRST (ERST) triptych approach with three variables: level of plausibility, level\nof loss and scenario. In our approach, any two of these variables can be\nderived by providing the third as the input. We advocate and demonstrate that\nERST is a powerful tool for both simple linear and complex portfolios and for\nboth risk management as well as day-to-day portfolio management decisions. An\nupdated new version of the Levenberg - Marquardt optimization algorithm is\nintroduced to derive ERST in certain complex cases.\n"
    },
    {
        "paper_id": 1906.1132,
        "authors": "Fred Espen Benth and Silvia Lavagnini",
        "title": "Correlators of Polynomial Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the setting of polynomial jump-diffusion dynamics, we provide an explicit\nformula for computing correlators, namely, cross-moments of the process at\ndifferent time points along its path. The formula appears as a linear\ncombination of exponentials of the generator matrix, extending the well-known\nmoment formula for polynomial processes. The developed framework can, for\nexample, be applied in financial pricing, such as for path-dependent options\nand in a stochastic volatility models context. In applications to options,\nhaving closed and compact formulations is attractive for sensitivity analysis\nand risk management, since Greeks can be derived explicitly.\n"
    },
    {
        "paper_id": 1906.11831,
        "authors": "Irina Georgescu, Louis Aim\\'e Fono",
        "title": "A portfolio choice problem in the framework of expected utility\n  operators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Possibilistic risk theory starts from the hypothesis that risk is modelled by\nfuzzy numbers. In particular, in a possibilistic portfolio choice problem, the\nreturn of a risky asset will be a fuzzy number. The expected utility operators\nhave been introduced in a previous paper to build an abstract theory of\npossibilistic risk aversion. To each expected utility operator one can\nassociate a notion of possibilistic expected utility. Using this notion, we\nwill formulate in this very general context a possibilistic choice problem. The\nmain results of the paper are two approximate calculation formulas for\ncorresponding optimization problem. The first formula approximates the optimal\nallocation with respect to risk aversion and investor's prudence, as well as\nthe first three possibilistic moments. Besides these parameters, in the second\nformula the temperance index of the utility function and the fourth\npossibilistic moment appear.\n"
    },
    {
        "paper_id": 1906.11968,
        "authors": "Matthias Pelster, Bastian Breitmayer and Tim Hasso",
        "title": "Are cryptocurrency traders pioneers or just risk-seekers? Evidence from\n  brokerage accounts",
        "comments": "10 pages",
        "journal-ref": "Economics Letters, 182, 98-100 (2019)",
        "doi": "10.1016/j.econlet.2019.06.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Are cryptocurrency traders driven by a desire to invest in a new asset class\nto diversify their portfolio or are they merely seeking to increase their\nlevels of risk? To answer this question, we use individual-level brokerage data\nand study their behavior in stock trading around the time they engage in their\nfirst cryptocurrency trade. We find that when engaging in cryptocurrency\ntrading investors simultaneously increase their risk-seeking behavior in stock\ntrading as they increase their trading intensity and use of leverage. The\nincrease in risk-seeking in stocks is particularly pronounced when volatility\nin cryptocurrency returns is low, suggesting that their overall behavior is\ndriven by excitement-seeking.\n"
    },
    {
        "paper_id": 1906.1201,
        "authors": "Tucker Hybinette Balch, Mahmoud Mahfouz, Joshua Lockhart, Maria\n  Hybinette, David Byrd",
        "title": "How to Evaluate Trading Strategies: Single Agent Market Replay or\n  Multiple Agent Interactive Simulation?",
        "comments": null,
        "journal-ref": "Presented at the 2019 ICML Workshop on AI in Finance: Applications\n  and Infrastructure for Multi-Agent Learning. Long Beach, CA",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how a multi-agent simulator can support two important but distinct\nmethods for assessing a trading strategy: Market Replay and Interactive\nAgent-Based Simulation (IABS). Our solution is important because each method\noffers strengths and weaknesses that expose or conceal flaws in the subject\nstrategy. A key weakness of Market Replay is that the simulated market does not\nsubstantially adapt to or respond to the presence of the experimental strategy.\nIABS methods provide an artificial market for the experimental strategy using a\npopulation of background trading agents. Because the background agents attend\nto market conditions and current price as part of their strategy, the overall\nmarket is responsive to the presence of the experimental strategy. Even so,\nIABS methods have their own weaknesses, primarily that it is unclear if the\nmarket environment they provide is realistic. We describe our approach in\ndetail, and illustrate its use in an example application: The evaluation of\nmarket impact for various size orders.\n"
    },
    {
        "paper_id": 1906.12123,
        "authors": "Darjus Hosszejni and Gregor Kastner",
        "title": "Modeling Univariate and Multivariate Stochastic Volatility in R with\n  stochvol and factorstochvol",
        "comments": null,
        "journal-ref": "Journal of Statistical Software, 100(12), 1-34 (2021)",
        "doi": "10.18637/jss.v100.i12",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stochastic volatility (SV) models are nonlinear state-space models that enjoy\nincreasing popularity for fitting and predicting heteroskedastic time series.\nHowever, due to the large number of latent quantities, their efficient\nestimation is non-trivial and software that allows to easily fit SV models to\ndata is rare. We aim to alleviate this issue by presenting novel\nimplementations of four SV models delivered in two R packages. Several unique\nfeatures are included and documented. As opposed to previous versions, stochvol\nis now capable of handling linear mean models, heavy-tailed SV, and SV with\nleverage. Moreover, we newly introduce factorstochvol which caters for\nmultivariate SV. Both packages offer a user-friendly interface through the\nconventional R generics and a range of tailor-made methods. Computational\nefficiency is achieved via interfacing R to C++ and doing the heavy work in the\nlatter. In the paper at hand, we provide a detailed discussion on Bayesian SV\nestimation and showcase the use of the new software through various examples.\n"
    },
    {
        "paper_id": 1906.12134,
        "authors": "Gregor Kastner",
        "title": "Dealing with Stochastic Volatility in Time Series Using the R Package\n  stochvol",
        "comments": null,
        "journal-ref": "Journal of Statistical Software, 69(5), 1-30 (2016)",
        "doi": "10.18637/jss.v069.i05",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The R package stochvol provides a fully Bayesian implementation of\nheteroskedasticity modeling within the framework of stochastic volatility. It\nutilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by\nobtaining draws from the posterior distribution of parameters and latent\nvariables which can then be used for predicting future volatilities. The\npackage can straightforwardly be employed as a stand-alone tool; moreover, it\nallows for easy incorporation into other MCMC samplers. The main focus of this\npaper is to show the functionality of stochvol. In addition, it provides a\nbrief mathematical description of the model, an overview of the sampling\nschemes used, and several illustrative examples using exchange rate data.\n"
    },
    {
        "paper_id": 1906.12317,
        "authors": "Thijs Kamma and Antoon Pelsser",
        "title": "Near-Optimal Dynamic Asset Allocation in Financial Markets with Trading\n  Constraints",
        "comments": "46 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a dual-control method for approximating investment strategies in\nincomplete environments that emerge from the presence of trading constraints.\nConvex duality enables the approximate technology to generate lower and upper\nbounds on the optimal value function. The mechanism rests on closed-form\nexpressions pertaining to the portfolio composition, from which we are able to\nderive the near-optimal asset allocation explicitly. In a real financial\nmarket, we illustrate the accuracy of our approximate method on a dual CRRA\nutility function that characterises the preferences of a finite-horizon\ninvestor. Negligible duality gaps and insignificant annual welfare losses\nsubstantiate accuracy of the technique.\n"
    },
    {
        "paper_id": 1907.00149,
        "authors": "Hasan Fallahgoul and Kihun Nam",
        "title": "Time-changed \\levy processes and option pricing: a critical comment",
        "comments": "arXiv admin note: text overlap with arXiv:1808.01852",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Carr and Wu (2004), henceforth CW, developed a framework that encompasses\nalmost all of the continuous-time models proposed in the option pricing\nliterature. Their framework hinges on the stopping time property of the time\nchanges. By analyzing the measurability of the time changes with respect to the\nunderlying filtration, we show that all models CW proposed for the time changes\nfail to satisfy this assumption.\n"
    },
    {
        "paper_id": 1907.00185,
        "authors": "J\\'er\\^ome Adda, Christian Decker, Marco Ottaviani",
        "title": "P-hacking in clinical trials and how incentives shape the distribution\n  of results across phases",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.1919906117",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Clinical research should conform to high standards of ethical and scientific\nintegrity, given that human lives are at stake. However, economic incentives\ncan generate conflicts of interest for investigators, who may be inclined to\nwithhold unfavorable results or even tamper with data in order to achieve\ndesired outcomes. To shed light on the integrity of clinical trial results,\nthis paper systematically analyzes the distribution of p-values of primary\noutcomes for phase II and phase III drug trials reported to the\nClinicalTrials.gov registry. First, we detect no bunching of results just above\nthe classical 5% threshold for statistical significance. Second, a density\ndiscontinuity test reveals an upward jump at the 5% threshold for phase III\nresults by small industry sponsors. Third, we document a larger fraction of\nsignificant results in phase III compared to phase II. Linking trials across\nphases, we find that early favorable results increase the likelihood of\ncontinuing into the next phase. Once we take into account this selective\ncontinuation, we can explain almost completely the excess of significant\nresults in phase III for trials conducted by large industry sponsors. For small\nindustry sponsors, instead, part of the excess remains unexplained.\n"
    },
    {
        "paper_id": 1907.00212,
        "authors": "Fernando F. Ferreira, A. Christian Silva, Ju-Yi Yen",
        "title": "Detailed study of a moving average trading rule",
        "comments": "32 pages, 14 figures, accepted for publication in Quantitative\n  Finance on 12/2017. Result of research initiated in 2013 which generated\n  several conference presentations and working papers. This effort is\n  substantially extended, edited and updated. arXiv admin note: substantial\n  text overlap with arXiv:1402.3030",
        "journal-ref": "Quantitative Finance, 18:9, 1599-1617 (2018)",
        "doi": "10.1080/14697688.2017.1417621",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a detailed study of the performance of a trading rule that uses\nmoving average of past returns to predict future returns on stock indexes. Our\nmain goal is to link performance and the stochastic process of the traded\nasset. Our study reports short, medium and long term effects by looking at the\nSharpe ratio (SR). We calculate the Sharpe ratio of our trading rule as a\nfunction of the probability distribution function of the underlying traded\nasset and compare it with data. We show that if the performance is mainly due\nto presence of autocorrelation in the returns of the traded assets, the SR as a\nfunction of the portfolio formation period (look-back) is very different from\nperformance due to the drift (average return). The SR shows that for look-back\nperiods of a few months the investor is more likely to tap into\nautocorrelation. However, for look-back larger than few months, the drift of\nthe asset becomes progressively more important. Finally, our empirical work\nreports a new long-term effect, namely oscillation of the SR and propose a\nnon-stationary model to account for such oscillations.\n"
    },
    {
        "paper_id": 1907.00219,
        "authors": "Michael A. Kouritzin, Anne MacKay",
        "title": "Branching Particle Pricers with Heston Examples",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of sequential Monte Carlo within simulation for path-dependent option\npricing is proposed and evaluated. Recently, it was shown that explicit\nsolutions and importance sampling are valuable for efficient simulation of spot\nprice and volatility, especially for purposes of path-dependent option pricing.\nThe resulting simulation algorithm is an analog to the weighted particle\nfiltering algorithm that might be improved by resampling or branching. Indeed,\nsome branching algorithms are shown herein to improve pricing performance\nsubstantially while some resampling algorithms are shown to be less suitable in\ncertain cases. A historical property is given and explained as the\ndistinguishing feature between the sequential Monte Carlo algorithms that work\non path-dependent option pricing and those that do not. In particular, it is\nrecommended to use the so-called effective particle branching algorithm within\nimportance-sampling Monte Carlo methods for path-dependent option pricing. All\nrecommendations are based upon numeric comparison of option pricing problems in\nthe Heston model.\n"
    },
    {
        "paper_id": 1907.00293,
        "authors": "Tim Leung, Brian Ward",
        "title": "Tracking VIX with VIX Futures: Portfolio Construction and Performance",
        "comments": "22 pages, book chapter",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a series of static and dynamic portfolios of VIX futures and their\neffectiveness to track the VIX index. We derive each portfolio using\noptimization methods, and evaluate its tracking performance from both empirical\nand theoretical perspectives. Among our results, we show that static portfolios\nof different VIX futures fail to track VIX closely. VIX futures simply do not\nreact quickly enough to movements in the spot VIX. In a discrete-time model, we\ndesign and implement a dynamic trading strategy that adjusts daily to optimally\ntrack VIX. The model is calibrated to historical data and a simulation study is\nperformed to understand the properties exhibited by the strategy. In addition,\ncomparing to the volatility ETN, VXX, we find that our dynamic strategy has a\nsuperior tracking performance.\n"
    },
    {
        "paper_id": 1907.00297,
        "authors": "Grzegorz Krzy\\.zanowski, Marcin Magdziarz, {\\L}ukasz P{\\l}ociniczak",
        "title": "A weighted finite difference method for subdiffusive Black Scholes Model",
        "comments": null,
        "journal-ref": "Computers & Mathematics with Applications 80.5 (2020): 653-670",
        "doi": "10.1016/j.camwa.2020.04.029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we focus on the subdiffusive Black Scholes model. The main part\nof our work consists of the finite difference method as a numerical approach to\nthe option pricing in the considered model. We derive the governing fractional\ndifferential equation and the related weighted numerical scheme being a\ngeneralization of the classical Crank-Nicolson scheme. The proposed method has\n$2-\\alpha$ order of accuracy with respect to time where $\\alpha\\in(0,1)$ is the\nsubdiffusion parameter, and $2$ with respect to space. Further, we provide the\nstability and convergence analysis. Finally, we present some numerical results.\n"
    },
    {
        "paper_id": 1907.00335,
        "authors": "Stefan Tappe",
        "title": "Existence of affine realizations for stochastic partial differential\n  equations driven by L\\'evy processes",
        "comments": "19 pages",
        "journal-ref": "Proceedings of The Royal Society of London. Series A.\n  Mathematical, Physical and Engineering Sciences 471(2178), 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to clarify when a semilinear stochastic partial\ndifferential equation driven by L\\'evy processes admits an affine realization.\nOur results are accompanied by several examples arising in natural sciences and\neconomics.\n"
    },
    {
        "paper_id": 1907.00336,
        "authors": "Stefan Tappe",
        "title": "Affine realizations with affine state processes for stochastic partial\n  differential equations",
        "comments": "27 pages",
        "journal-ref": "Stochastic Processes and Their Applications 126(7):2062-2091, 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to clarify when a stochastic partial differential\nequation with an affine realization admits affine state processes. This\nincludes a characterization of the set of initial points of the realization.\nSeveral examples, as the HJMM equation from mathematical finance, illustrate\nour results.\n"
    },
    {
        "paper_id": 1907.00371,
        "authors": "Abhin Kakkad, Harsh Vasoya and Arnab K. Ray",
        "title": "Regularities in stock markets",
        "comments": "5 pages, ReVTeX",
        "journal-ref": "IJMPC Vol.31, No.10, 2050145 (2020)",
        "doi": "10.1142/S0129183120501454",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  From the stock markets of six countries with high GDP, we study the stock\nindices, S&P 500 (NYSE, USA), SSE Composite (SSE, China), Nikkei (TSE, Japan),\nDAX (FSE, Germany), FTSE 100 (LSE, Britain) and NIFTY (NSE, India). The daily\nmean growth of the stock values is exponential. The daily price fluctuations\nabout the mean growth are Gaussian, but with a non-zero asymptotic convergence.\nThe growth of the monthly average of stock values is statistically self-similar\nto their daily growth. The monthly fluctuations of the price follow a Wiener\nprocess, with a decline of the volatility. The mean growth of the daily volume\nof trade is exponential. These observations are globally applicable and\nunderline regularities across global stock markets.\n"
    },
    {
        "paper_id": 1907.00558,
        "authors": "Maria Glenski, Tim Weninger, and Svitlana Volkova",
        "title": "Improved Forecasting of Cryptocurrency Price using Social Signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social media signals have been successfully used to develop large-scale\npredictive and anticipatory analytics. For example, forecasting stock market\nprices and influenza outbreaks. Recently, social data has been explored to\nforecast price fluctuations of cryptocurrencies, which are a novel disruptive\ntechnology with significant political and economic implications. In this paper\nwe leverage and contrast the predictive power of social signals, specifically\nuser behavior and communication patterns, from multiple social platforms GitHub\nand Reddit to forecast prices for three cyptocurrencies with high developer and\ncommunity interest - Bitcoin, Ethereum, and Monero. We evaluate the performance\nof neural network models that rely on long short-term memory units (LSTMs)\ntrained on historical price data and social data against price only LSTMs and\nbaseline autoregressive integrated moving average (ARIMA) models, commonly used\nto predict stock prices. Our results not only demonstrate that social signals\nreduce error when forecasting daily coin price, but also show that the language\nused in comments within the official communities on Reddit (r/Bitcoin,\nr/Ethereum, and r/Monero) are the best predictors overall. We observe that\nmodels are more accurate in forecasting price one day ahead for Bitcoin (4%\nroot mean squared percent error) compared to Ethereum (7%) and Monero (8%).\n"
    },
    {
        "paper_id": 1907.01056,
        "authors": "Matthew Lorig, Zhou Zhou, Bin Zou",
        "title": "Optimal Bookmaking",
        "comments": "30 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a general framework for continuous-time betting markets, in\nwhich a bookmaker can dynamically control the prices of bets on outcomes of\nrandom events. In turn, the prices set by the bookmaker affect the rate or\nintensity of bets placed by gamblers. The bookmaker seeks a price process that\nmaximizes his expected (utility of) terminal wealth. We obtain explicit\nsolutions or characterizations to the bookmaker's optimal bookmaking problem in\nvarious interesting models.\n"
    },
    {
        "paper_id": 1907.01119,
        "authors": "Peng Wang (ECUST), Jun-Chao Ma (ECUST), Zhi-Qiang Jiang (ECUST),\n  Wei-Xing Zhou (ECUST), and Didier Sornette (ETH Zurich)",
        "title": "Comparative analysis of layered structures in empirical investor\n  networks and cellphone communication networks",
        "comments": "9 pages, 9 figues, 3 tables",
        "journal-ref": "EPJ Data Science 9 (1), 11 (2020)",
        "doi": "10.1140/epjds/s13688-020-00230-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical investor networks (EIN) proposed by\n\\cite{Ozsoylev-Walden-Yavuz-Bildik-2014-RFS} are assumed to capture the\ninformation spreading path among investors. Here, we perform a comparative\nanalysis between the EIN and the cellphone communication networks (CN) to test\nwhether EIN is an information exchanging network from the perspective of the\nlayer structures of ego networks. We employ two clustering algorithms\n($k$-means algorithm and $H/T$ break algorithm) to detect the layer structures\nfor each node in both networks. We find that the nodes in both networks can be\nclustered into two groups, one that has a layer structure similar to the\ntheoretical Dunbar Circle corresponding to that the alters in ego networks\nexhibit a four-layer hierarchical structure with the cumulative number of 5,\n15, 50 and 150 from the inner layer to the outer layer, and the other one\nhaving an additional inner layer with about 2 alters compared with the Dunbar\nCircle. We also find that the scale ratios, which are estimated based on the\nunique parameters in the theoretical model of layer structures\n\\citep{Tamarit-Cuesta-Dunbar-Sanchez-2018-PNAS}, conform to a log-normal\ndistribution for both networks. Our results not only deepen our understanding\non the topological structures of EIN, but also provide empirical evidence of\nthe channels of information diffusion among investors.\n"
    },
    {
        "paper_id": 1907.01189,
        "authors": "Carlo Milana",
        "title": "Solving the Reswitching Paradox in the Sraffian Theory of Capital",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The possibility of re-switching of techniques in Piero Sraffa's intersectoral\nmodel, namely the returning capital-intensive techniques with monotonic changes\nin the profit rate, is traditionally considered as a paradox putting at stake\nthe viability of the neoclassical theory of production. It is argued here that\nthis phenomenon can be rationalized within the neoclassical paradigm. Sectoral\ninterdependencies can give rise to non-monotonic effects of progressive\nvariations in income distribution on relative prices. The re-switching of\ntechniques is, therefore, the result of cost-minimizing technical choices\nfacing returning ranks of relative input prices in full consistency with the\nneoclassical perspective.\n"
    },
    {
        "paper_id": 1907.01225,
        "authors": "Philippe Bergault, Olivier Gu\\'eant",
        "title": "Size matters for OTC market makers: general results and dimensionality\n  reduction techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In most OTC markets, a small number of market makers provide liquidity to\nother market participants. More precisely, for a list of assets, they set\nprices at which they agree to buy and sell. Market makers face therefore an\ninteresting optimization problem: they need to choose bid and ask prices for\nmaking money while mitigating the risk associated with holding inventory in a\nvolatile market. Many market making models have been proposed in the academic\nliterature, most of them dealing with single-asset market making whereas market\nmakers are usually in charge of a long list of assets. The rare models tackling\nmulti-asset market making suffer however from the curse of dimensionality when\nit comes to the numerical approximation of the optimal quotes. The goal of this\npaper is to propose a dimensionality reduction technique to address multi-asset\nmarket making by using a factor model. Moreover, we generalize existing market\nmaking models by the addition of an important feature: the existence of\ndifferent transaction sizes and the possibility for the market makers in OTC\nmarkets to answer different prices to requests with different sizes.\n"
    },
    {
        "paper_id": 1907.01274,
        "authors": "Gian Paolo Clemente and Rosanna Grassi and Asmerilda Hitaj",
        "title": "Smart network based portfolios",
        "comments": null,
        "journal-ref": "Annals of Operations Research 2022",
        "doi": "10.1007/s10479-022-04675-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we deal with the problem of portfolio allocation by enhancing\nnetwork theory tools. We use the dependence structure of the correlations\nnetwork in constructing some well-known risk-based models in which the\nestimation of correlation matrix is a building block in the portfolio\noptimization. We formulate and solve all these portfolio allocation problems\nusing both the standard approach and the network-based approach. Moreover, in\nconstructing the network-based portfolios we propose the use of two different\nestimators for the covariance matrix: the sample estimator and the shrinkage\ntoward constant correlation one. All the strategies under analysis are\nimplemented on two high-dimensional portfolios having different\ncharacteristics, covering the period from January $2001$ to December $2017$. We\nfind that the network-based portfolio consistently better performs and has\nlower risk compared to the corresponding standard portfolio in an out-of-sample\nperspective.\n"
    },
    {
        "paper_id": 1907.01306,
        "authors": "Tobias Fissler, Jana Hlavinov\\'a, Birgit Rudloff",
        "title": "Elicitability and Identifiability of Systemic Risk Measures",
        "comments": "42 pages, 3 figures + supplementary material (6 pages, 2 figures)",
        "journal-ref": "Finance and Stochastics (2021), Volume 25, No. 1, 133-165",
        "doi": "10.1007/s00780-020-00446-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identification and scoring functions are statistical tools to assess the\ncalibration and the relative performance of risk measure estimates, e.g., in\nbacktesting. A risk measures is called identifiable (elicitable) it it admits a\nstrict identification function (strictly consistent scoring function). We\nconsider measures of systemic risk introduced in Feinstein, Rudloff and Weber\n(2017). Since these are set-valued, we work within the theoretical framework of\nFissler, Hlavinov\\'a and Rudloff (2019) for forecast evaluation of set-valued\nfunctionals. We construct oriented selective identification functions, which\ninduce a mixture representation of (strictly) consistent scoring functions.\nTheir applicability is demonstrated with a comprehensive simulation study.\n"
    },
    {
        "paper_id": 1907.01362,
        "authors": "Doron Klunover, John Morgan",
        "title": "A Model of Presidential Debates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Presidential debates are viewed as providing an important public good by\nrevealing information on candidates to voters. We consider an endogenous model\nof presidential debates in which an incumbent and a challenger (who is\nprivately informed about her own quality) publicly announce whether they are\nwilling to participate in a public debate, taking into account that a voter's\nchoice of candidate depends on her beliefs regarding the candidates' qualities\nand on the state of nature.It is found that in equilibrium a debate occurs or\ndoes not occur independently of the challenger's quality and therefore the\ncandidates' announcements are uninformative. This is because opting-out is\nperceived to be worse than losing a debate and therefore the challenger never\nrefuses to participate.\n"
    },
    {
        "paper_id": 1907.01437,
        "authors": "Stefan Tappe",
        "title": "Compact embeddings for spaces of forward rate curves",
        "comments": "9 pages",
        "journal-ref": "Abstract and Applied Analysis, vol. 2013, Article ID 709505, 6\n  pages",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this note is to prove a compact embedding result for spaces of\nforward rate curves. As a consequence of this result, we show that any forward\nrate evolution can be approximated by a sequence of finite dimensional\nprocesses in the larger state space.\n"
    },
    {
        "paper_id": 1907.01503,
        "authors": "Xinyi Li, Yinchuan Li, Yuancheng Zhan, Xiao-Yang Liu",
        "title": "Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement\n  Learning for Stock Portfolio Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio allocation is crucial for investment companies. However, getting\nthe best strategy in a complex and dynamic stock market is challenging. In this\npaper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning\nscheme (Adaptive DDPG) for the portfolio allocation task, which incorporates\noptimistic or pessimistic deep reinforcement learning that is reflected in the\ninfluence from prediction errors. Dow Jones 30 component stocks are selected as\nour trading stocks and their daily prices are used as the training and testing\ndata. We train the Adaptive DDPG agent and obtain a trading strategy. The\nAdaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones\nIndustrial Average index and the traditional min-variance and mean-variance\nportfolio allocation strategies. Adaptive DDPG outperforms the baselines in\nterms of the investment return and the Sharpe ratio.\n"
    },
    {
        "paper_id": 1907.01576,
        "authors": "Aubrey Clayton",
        "title": "Election predictions are arbitrage-free: response to Taleb",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Taleb (2018) claimed a novel approach to evaluating the quality of\nprobabilistic election forecasts via no-arbitrage pricing techniques and argued\nthat popular forecasts of the 2016 U.S. Presidential election had violated\narbitrage boundaries. We show that under mild assumptions all such political\nforecasts are arbitrage-free and that the heuristic that Taleb's argument was\nbased on is false.\n"
    },
    {
        "paper_id": 1907.018,
        "authors": "Jeremy D. Turiel and Tomaso Aste",
        "title": "P2P Loan acceptance and default prediction with Artificial Intelligence",
        "comments": "11 pages, 2 figures, 6 tables, presented as case study for the EC\n  HO2020 FinTech project",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Logistic Regression and Support Vector Machine algorithms, together with\nLinear and Non-Linear Deep Neural Networks, are applied to lending data in\norder to replicate lender acceptance of loans and predict the likelihood of\ndefault of issued loans. A two phase model is proposed; the first phase\npredicts loan rejection, while the second one predicts default risk for\napproved loans. Logistic Regression was found to be the best performer for the\nfirst phase, with test set recall macro score of $77.4 \\%$. Deep Neural\nNetworks were applied to the second phase only, were they achieved best\nperformance, with validation set recall score of $72 \\%$, for defaults. This\nshows that AI can improve current credit risk models reducing the default risk\nof issued loans by as much as $70 \\%$. The models were also applied to loans\ntaken for small businesses alone. The first phase of the model performs\nsignificantly better when trained on the whole dataset. Instead, the second\nphase performs significantly better when trained on the small business subset.\nThis suggests a potential discrepancy between how these loans are screened and\nhow they should be analysed in terms of default prediction.\n"
    },
    {
        "paper_id": 1907.01828,
        "authors": "Yuchao Dong (LASP), J\\'er\\^ome Spielmann (LAREMA, UA)",
        "title": "Weak Limits of Random Coefficient Autoregressive Processes and their\n  Application in Ruin Theory",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics, Elsevier, 2020, 91, pp.1-11",
        "doi": "10.1016/j.insmatheco.2019.12.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that a large class of discrete-time insurance surplus processes\nconverge weakly to a generalized Ornstein-Uhlenbeck process, under a suitable\nre-normalization and when the time-step goes to 0. Motivated by ruin theory, we\nuse this result to obtain approximations for the moments, the ultimate ruin\nprobability and the discounted penalty function of the discrete-time process.\n"
    },
    {
        "paper_id": 1907.01902,
        "authors": "Bernhelm Booss-Bavnbek, Rasmus Kristoffer Pedersen, Ulf R{\\o}rb{\\ae}k\n  Pedersen",
        "title": "Multiplicity of time scales in climate, matter, life, and economy",
        "comments": "55 pages, 34 figures, 4 tables, 117 items in bibliography. Difference\n  to v1: More details on credibility and lack of credibility when treating\n  complex systems with a multiplicity of different characteristic time lengths:\n  sampling problems, feedback loops, scientification of environmental politics\n  and the pitfalls of the related politicisation of sciences. See also\n  https://events.ruc.dk/timescales/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This topic review communicates working experiences regarding interaction of a\nmultiplicity of processes. Our experiences come from climate change modelling,\nmaterials science, cell physiology and public health, and macroeconomic\nmodelling. We look at the astonishing advances of recent years in broad-band\ntemporal frequency sampling, multiscale modelling and fast large-scale\nnumerical simulation of complex systems, but also the continuing uncertainty of\nmany science-based results.\n  We describe and analyse properties that depend on the time scale of the\nmeasurement; structural instability; tipping points; thresholds; hysteresis;\nfeedback mechanisms with runaways or stabilizations or delays. We point to\ngrave disorientation in statistical sampling, the interpretation of\nobservations and the design of control when neglecting the presence or\nemergence of multiple characteristic times.\n  We explain what these working experiences can demonstrate for environmental\nresearch.\n"
    },
    {
        "paper_id": 1907.01917,
        "authors": "Christa Cuchiero and Josef Teichmann",
        "title": "Markovian lifts of positive semidefinite affine Volterra type processes",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1804.10450",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider stochastic partial differential equations appearing as Markovian\nlifts of matrix valued (affine) Volterra type processes from the point of view\nof the generalized Feller property (see e.g., \\cite{doetei:10}). We introduce\nin particular Volterra Wishart processes with fractional kernels and values in\nthe cone of positive semidefinite matrices. They are constructed from matrix\nproducts of infinite dimensional Ornstein Uhlenbeck processes whose state space\nare matrix valued measures. Parallel to that we also consider positive definite\nVolterra pure jump processes, giving rise to multivariate Hawkes type\nprocesses. We apply these affine covariance processes for multivariate (rough)\nvolatility modeling and introduce a (rough) multivariate Volterra Heston type\nmodel.\n"
    },
    {
        "paper_id": 1907.021,
        "authors": "Emir Hrnjic and Nikodem Tomczak",
        "title": "Machine learning and behavioral economics for personalized choice\n  architecture",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Behavioral economics changed the way we think about market participants and\nrevolutionized policy-making by introducing the concept of choice architecture.\nHowever, even though effective on the level of a population, interventions from\nbehavioral economics, nudges, are often characterized by weak generalisation as\nthey struggle on the level of individuals. Recent developments in data science,\nartificial intelligence (AI) and machine learning (ML) have shown ability to\nalleviate some of the problems of weak generalisation by providing tools and\nmethods that result in models with stronger predictive power. This paper aims\nto describe how ML and AI can work with behavioral economics to support and\naugment decision-making and inform policy decisions by designing personalized\ninterventions, assuming that enough personalized traits and psychological\nvariables can be sampled.\n"
    },
    {
        "paper_id": 1907.02101,
        "authors": "Bo Honore, Thomas Jorgensen, Aureo de Paula",
        "title": "The Informativeness of Estimation Moments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces measures for how each moment contributes to the\nprecision of parameter estimates in GMM settings. For example, one of the\nmeasures asks what would happen to the variance of the parameter estimates if a\nparticular moment was dropped from the estimation. The measures are all easy to\ncompute. We illustrate the usefulness of the measures through two simple\nexamples as well as an application to a model of joint retirement planning of\ncouples. We estimate the model using the UK-BHPS, and we find evidence of\ncomplementarities in leisure. Our sensitivity measures illustrate that the\nestimate of the complementarity is primarily informed by the distribution of\ndifferences in planned retirement dates. The estimated econometric model can be\ninterpreted as a bivariate ordered choice model that allows for simultaneity.\nThis makes the model potentially useful in other applications.\n"
    },
    {
        "paper_id": 1907.02155,
        "authors": "Yuki M. Asano, Jakob J. Kolb, Jobst Heitzig and J. Doyne Farmer",
        "title": "Emergent inequality and endogenous dynamics in a simple behavioral\n  macroeconomic model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Standard macroeconomic models assume that households are rational in the\nsense that they are perfect utility maximizers, and explain economic dynamics\nin terms of shocks that drive the economy away from the stead-state. Here we\nbuild on a standard macroeconomic model in which a single rational\nrepresentative household makes a savings decision of how much to consume or\ninvest. In our model households are myopic boundedly rational heterogeneous\nagents embedded in a social network. From time to time each household updates\nits savings rate by copying the savings rate of its neighbor with the highest\nconsumption. If the updating time is short, the economy is stuck in a poverty\ntrap, but for longer updating times economic output approaches its optimal\nvalue, and we observe a critical transition to an economy with irregular\nendogenous oscillations in economic output, resembling a business cycle. In\nthis regime households divide into two groups: Poor households with low savings\nrates and rich households with high savings rates. Thus inequality and economic\ndynamics both occur spontaneously as a consequence of imperfect household\ndecision making. Our work here supports an alternative program of research that\nsubstitutes utility maximization for behaviorally grounded decision making.\n"
    },
    {
        "paper_id": 1907.0232,
        "authors": "Arthur Charpentier, Alfred Galichon, Lucas Vernet",
        "title": "Optimal transport on large networks, a practitioner's guide",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a set of tools for the modeling of a spatial allocation\nproblem in a large geographic market and gives examples of applications. In our\nsettings, the market is described by a network that maps the cost of travel\nbetween each pair of adjacent locations. Two types of agents are located at the\nnodes of this network. The buyers choose the most competitive sellers depending\non their prices and the cost to reach them. Their utility is assumed additive\nin both these quantities. Each seller, taking as given other sellers prices,\nsets her own price to have a demand equal to the one we observed. We give a\nlinear programming formulation for the equilibrium conditions. After formally\nintroducing our model we apply it on two examples: prices offered by petrol\nstations and quality of services provided by maternity wards. These examples\nillustrate the applicability of our model to aggregate demand, rank prices and\nestimate cost structure over the network. We insist on the possibility of\napplications to large scale data sets using modern linear programming solvers\nsuch as Gurobi. In addition to this paper we released a R toolbox to implement\nour results and an online tutorial (http://optimalnetwork.github.io)\n"
    },
    {
        "paper_id": 1907.02363,
        "authors": "Stefan Tappe",
        "title": "Existence of affine realizations for L\\'evy term structure models",
        "comments": "16 pages",
        "journal-ref": "Proceedings of The Royal Society of London. Series A.\n  Mathematical, Physical and Engineering Sciences 468(2147):3685-3704, 2012",
        "doi": "10.1098/rspa.2012.0089",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the existence of affine realizations for term structure models\ndriven by L\\'evy processes. It turns out that we obtain more severe\nrestrictions on the volatility than in the classical diffusion case without\njumps. As special cases, we study constant direction volatilities and the\nexistence of short rate realizations.\n"
    },
    {
        "paper_id": 1907.02457,
        "authors": "Zsolt Nika, Mikl\\'os R\\'asonyi",
        "title": "Learning Threshold-Type Investment Strategies with Stochastic Gradient\n  Method",
        "comments": "11 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In online portfolio optimization the investor makes decisions based on new,\ncontinuously incoming information on financial assets (typically their prices).\nIn our study we consider a learning algorithm, namely the Kiefer--Wolfowitz\nversion of the Stochastic Gradient method, that converges to the log-optimal\nsolution in the threshold-type, buy-and-sell strategy class.\n  The systematic study of this method is novel in the field of portfolio\noptimization; we aim to establish the theory and practice of Stochastic\nGradient algorithm used on parametrized trading strategies.\n  We demonstrate on a wide variety of stock price dynamics (e.g. with\nstochastic volatility and long-memory) that there is an optimal threshold type\nstrategy which can be learned. Subsequently, we numerically show the\nconvergence of the algorithm. Furthermore, we deal with the typically\nproblematic question of how to choose the hyperparameters (the parameters of\nthe algorithm and not the dynamics of the prices) without knowing anything\nabout the price other than a small sample.\n"
    },
    {
        "paper_id": 1907.02666,
        "authors": "Huiling Yuan, Yong Zhou, Zhiyuan Zhang, Xiangyu Cui",
        "title": "Forecasting security's volatility using low-frequency historical data,\n  high-frequency historical data and option-implied volatility",
        "comments": "37 pages,9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Low-frequency historical data, high-frequency historical data and option data\nare three major sources, which can be used to forecast the underlying\nsecurity's volatility. In this paper, we propose two econometric models, which\nintegrate three information sources. In GARCH-It\\^{o}-OI model, we assume that\nthe option-implied volatility can influence the security's future volatility,\nand the option-implied volatility is treated as an observable exogenous\nvariable. In GARCH-It\\^{o}-IV model, we assume that the option-implied\nvolatility can not influence the security's volatility directly, and the\nrelationship between the option-implied volatility and the security's\nvolatility is constructed to extract useful information of the underlying\nsecurity. After providing the quasi-maximum likelihood estimators for the\nparameters and establishing their asymptotic properties, we also conduct a\nseries of simulation analysis and empirical analysis to compare the proposed\nmodels with other popular models in the literature. We find that when the\nsampling interval of the high-frequency data is 5 minutes, the GARCH-It\\^{o}-OI\nmodel and GARCH-It\\^{o}-IV model has better forecasting performance than other\nmodels.\n"
    },
    {
        "paper_id": 1907.03009,
        "authors": "Ajit Mahata, Debi Prasad Bal and Md Nurujjaman",
        "title": "Identification of short-term and long-term time scales in stock markets\n  and effect of structural break",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 545,\n  1 May 2020, 123612",
        "doi": "10.1016/j.physa.2019.123612",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents the comparative study of the nature of stock markets in\nshort-term and long-term time scales with and without structural break in the\nstock data. Structural break point has been identified by applying Zivot and\nAndrews structural trend break model to break the original time series (TSO)\ninto time series before structural break (TSB) and time series after structural\nbreak (TSA). The empirical mode decomposition based Hurst exponent and variance\ntechniques have been applied to the TSO, TSB and TSA to identify the time\nscales in short-term and long-term from the decomposed intrinsic mode\nfunctions. We found that for TSO, TSB and TSA the short-term time scales and\nlong-term time scales are within the range of few days to 3 months and greater\nthan 5 months respectively, which indicates that the short-term and long-term\ntime scales are present in the stock market. The Hurst exponent is $\\sim 0.5$\nand $\\geq 0.75$ for TSO, TSB and TSA in short-term and long-term respectively,\nwhich indicates that the market is random in short-term and strongly correlated\nin long-term. The identification of time scales at short-term and long-term\ninvestment horizon will be useful for investors to design investment and\ntrading strategies.\n"
    },
    {
        "paper_id": 1907.0301,
        "authors": "Fabrice Daniel",
        "title": "Financial Time Series Data Processing for Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.\n"
    },
    {
        "paper_id": 1907.03082,
        "authors": "Li-Hsien Sun",
        "title": "Systemic Risk and Heterogeneous Mean Field Type Interbank Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the system of heterogeneous interbank lending and borrowing based on\nthe relative average of log-capitalization given by the linear combination of\nthe average within groups and the ensemble average and describe the evolution\nof log-capitalization by a system of coupled diffusions. The model incorporates\na game feature with homogeneity within groups and heterogeneity between groups\nwhere banks search for the optimal lending or borrowing strategies through\nminimizing the heterogeneous linear quadratic costs in order to avoid to\napproach the default barrier. Due to the complicity of the lending and\nborrowing system, the closed-loop Nash equilibria and the open-loop Nash\nequilibria are both driven by the coupled Riccati equations. The existence of\nthe equilibria in the two-group case where the number of banks are sufficiently\nlarge is guaranteed by the solvability for the coupled Riccati equations as the\nnumber of banks goes to infinity in each group. The equilibria are consisted of\nthe mean-reverting term identical to the one group game and the group average\nowing to heterogeneity. In addition, the corresponding heterogeneous mean filed\ngame with the arbitrary number of groups is also discussed. The existence of\nthe $\\epsilon$-Nash equilibrium in the general $d$ heterogeneous groups is also\nverified. Finally, in the financial implication, we observe the Nash equilibria\ngoverned by the mean-reverting term and the linear combination of the ensemble\naverages of individual groups and study the influence of the relative\nparameters on the liquidity rate through the numerical analysis.\n"
    },
    {
        "paper_id": 1907.03093,
        "authors": "Xiang Meng",
        "title": "Dynamic Mean-Variance Portfolio Optimisation",
        "comments": "National University of Singapore, Quantitative Finance (Department of\n  Mathematics) Undergraduate Thesis. Advisor: Chao Zhou",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The portfolio optimisation problem, first raised by Harry Markowitz in 1952,\nhas been a fundamental and central topic to understanding the stock market and\nmaking decisions. There has been plenty of works contributing to development of\nthe mean-variance optimisation (MVO) so far. In this paper, one kind of them,\nnamely, dynamic mean-variance optimisation (DMVO) is mainly discussed. One can\napply either precommitment or game-theoritical approach to address\ntime-inconsistency in DMVO. We use the second approach to seek for a\ntime-consistent strategy. After obtaining the optimal strategy, we extend the\nresult to a CEV-driven economy. In order to prove the usefulness of them,\nstrategies are fit into both real market data and simulated data. It turns out\nthat the strategy whose assumptions are close to market conditions generally\ngives a better result. Lastly, a selected strategy is chosen to compare with\nanother strategy came up by deep learning technique.\n"
    },
    {
        "paper_id": 1907.03256,
        "authors": "Stefan Tappe",
        "title": "An alternative approach on the existence of affine realizations for HJM\n  term structure models",
        "comments": null,
        "journal-ref": "Proceedings of The Royal Society of London. Series A.\n  Mathematical, Physical and Engineering Sciences 466(2122):3033-3060, 2010",
        "doi": "10.1098/rspa.2009.0493",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an alternative approach on the existence of affine realizations\nfor HJM interest rate models. It is applicable to a wide class of models, and\nsimultaneously it is conceptually rather comprehensible. We also supplement\nsome known existence results for particular volatility structures and provide\nfurther insights into the geometry of term structure models.\n"
    },
    {
        "paper_id": 1907.03295,
        "authors": "Tianyao Chen, Xue Cheng, Jingping Yang",
        "title": "Common Decomposition of Correlated Brownian Motions and its Financial\n  Applications",
        "comments": "52 pages, 18 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a theory of common decomposition for two correlated\nBrownian motions, in which, by using change of time method, the correlated\nBrownian motions are represented by a triplet of processes, $(X,Y,T)$, where\n$X$ and $Y$ are independent Brownian motions. We show the equivalent conditions\nfor the triplet being independent. We discuss the connection and difference of\nthe common decomposition with the local correlation model. Indicated by the\ndiscussion, we propose a new method for constructing correlated Brownian\nmotions which performs very well in simulation. For applications, we use these\nvery general results for pricing two-factor financial derivatives whose payoffs\nrely very much on the correlations of underlyings. And in addition, with the\nhelp of numerical method, we also make a discussion of the pricing deviation\nwhen substituting a constant correlation model for a general one.\n"
    },
    {
        "paper_id": 1907.03355,
        "authors": "Hung Ba",
        "title": "Improving Detection of Credit Card Fraudulent Transactions using\n  Generative Adversarial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this study, we employ Generative Adversarial Networks as an oversampling\nmethod to generate artificial data to assist with the classification of credit\ncard fraudulent transactions. GANs is a generative model based on the idea of\ngame theory, in which a generator G and a discriminator D are trying to\noutsmart each other. The objective of the generator is to confuse the\ndiscriminator. The objective of the discriminator is to distinguish the\ninstances coming from the generator and the instances coming from the original\ndataset. By training GANs on a set of credit card fraudulent transactions, we\nare able to improve the discriminatory power of classifiers. The experiment\nresults show that the Wasserstein-GAN is more stable in training and produce\nmore realistic fraudulent transactions than the other GANs. On the other hand,\nthe conditional version of GANs in which labels are set by k-means clustering\ndoes not necessarily improve the non-conditional versions of GANs.\n"
    },
    {
        "paper_id": 1907.0337,
        "authors": "Catherine D'Hondt, Rudy De Winne, Eric Ghysels, Steve Raymond",
        "title": "Artificial Intelligence Alter Egos: Who benefits from Robo-investing?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial intelligence, or AI, enhancements are increasingly shaping our\ndaily lives. Financial decision-making is no exception to this. We introduce\nthe notion of AI Alter Egos, which are shadow robo-investors, and use a unique\ndata set covering brokerage accounts for a large cross-section of investors\nover a sample from January 2003 to March 2012, which includes the 2008\nfinancial crisis, to assess the benefits of robo-investing. We have detailed\ninvestor characteristics and records of all trades. Our data set consists of\ninvestors typically targeted for robo-advising. We explore robo-investing\nstrategies commonly used in the industry, including some involving advanced\nmachine learning methods. The man versus machine comparison allows us to shed\nlight on potential benefits the emerging robo-advising industry may provide to\ncertain segments of the population, such as low income and/or high risk averse\ninvestors.\n"
    },
    {
        "paper_id": 1907.03561,
        "authors": "Damir Filipovi\\'c and Stefan Tappe",
        "title": "Existence of L\\'evy term structure models",
        "comments": "26 pages",
        "journal-ref": "Finance and Stochastics 12(1):83-115, 2008",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  L\\'evy driven term structure models have become an important subject in the\nmathematical finance literature. This paper provides a comprehensive analysis\nof the L\\'evy driven Heath-Jarrow-Morton type term structure equation. This\nincludes a full proof of existence and uniqueness in particular, which seems to\nhave been lacking in the finance literature so far.\n"
    },
    {
        "paper_id": 1907.03577,
        "authors": "Alexandre Bovet, Carlo Campajola, Francesco Mottes, Valerio Restocchi,\n  Nicol\\`o Vallarano, Tiziano Squartini, Claudio J. Tessone",
        "title": "The evolving liaisons between the transaction networks of Bitcoin and\n  its price dynamics",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies are distributed systems that allow exchanges of native\ntokens among participants, or the exchange of such tokens for fiat currencies\nin markets external to these public ledgers. The availability of their complete\nhistorical bookkeeping opens up the possibility of understanding the\nrelationship between aggregated users' behaviour and the cryptocurrency pricing\nin exchange markets. This paper analyses the properties of the transaction\nnetwork of Bitcoin. We consider four different representations of it, over a\nperiod of nine years since the Bitcoin creation and involving 16 million users\nand 283 million transactions. By analysing these networks, we show the\nexistence of causal relationships between Bitcoin price movements and changes\nof its transaction network topology. Our results reveal the interplay between\nstructural quantities, indicative of the collective behaviour of Bitcoin users,\nand price movements, showing that, during price drops, the system is\ncharacterised by a larger heterogeneity of nodes activity.\n"
    },
    {
        "paper_id": 1907.03665,
        "authors": "Hyungjun Park, and Min Kyu Sim, and Dong Gu Choi",
        "title": "An intelligent financial portfolio trading strategy using deep\n  Q-learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio traders strive to identify dynamic portfolio allocation schemes so\nthat their total budgets are efficiently allocated through the investment\nhorizon. This study proposes a novel portfolio trading strategy in which an\nintelligent agent is trained to identify an optimal trading action by using\ndeep Q-learning. We formulate a Markov decision process model for the portfolio\ntrading process, and the model adopts a discrete combinatorial action space,\ndetermining the trading direction at prespecified trading size for each asset,\nto ensure practical applicability. Our novel portfolio trading strategy takes\nadvantage of three features to outperform in real-world trading. First, a\nmapping function is devised to handle and transform an initially found but\ninfeasible action into a feasible action closest to the originally proposed\nideal action. Second, by overcoming the dimensionality problem, this study\nestablishes models of agent and Q-network for deriving a multi-asset trading\nstrategy in the predefined action space. Last, this study introduces a\ntechnique that has the advantage of deriving a well-fitted multi-asset trading\nstrategy by designing an agent to simulate all feasible actions in each state.\nTo validate our approach, we conduct backtests for two representative\nportfolios and demonstrate superior results over the benchmark strategies.\n"
    },
    {
        "paper_id": 1907.04046,
        "authors": "Luis H. R. Alvarez E. and S\\\"oren Christensen",
        "title": "A Class of Solvable Multidimensional Stopping Problems in the Presence\n  of Knightian Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the impact of Knightian uncertainty on the optimal timing\npolicy of an ambiguity averse decision maker in the case where the underlying\nfactor dynamics follow a multidimensional Brownian motion and the exercise\npayoff depends on either a linear combination of the factors or the radial part\nof the driving factor dynamics. We present a general characterization of the\nvalue of the optimal timing policy and the worst case measure in terms of a\nfamily of an explicitly identified excessive functions generating an\nappropriate class of supermartingales. In line with previous findings based on\nlinear diffusions, we find that ambiguity accelerates timing in comparison with\nthe unambiguous setting. Somewhat surprisingly, we find that ambiguity may\nresult into stationarity in models which typically do not possess stationary\nbehavior. In this way, our results indicate that ambiguity may act as a\nstabilizing mechanism.\n"
    },
    {
        "paper_id": 1907.0423,
        "authors": "Kristian Buchardt and Christian Furrer and Thomas M{\\o}ller",
        "title": "Tax- and expense-modified risk-minimization for insurance payment\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of determining risk-minimizing investment strategies for\ninsurance payment processes in the presence of taxes and expenses. We consider\nthe situation where taxes and expenses are paid continuously and symmetrically\nand introduce the concept of tax- and expense-modified risk-minimization.\nRisk-minimizing strategies in the presence of taxes and expenses are derived\nand linked to Galtchouk-Kunita-Watanabe decompositions associated with modified\nversions of the original payment processes. Furthermore, we show equivalence to\nan alternative approach involving an artificial market consisting of after-tax\nand after-expense assets, and we establish a type of consistency with classic\nrisk-minimization. Finally, a case study involving classic multi-state life\ninsurance payments in combination with a bond market exemplifies the results.\n"
    },
    {
        "paper_id": 1907.04257,
        "authors": "Francesca Biagini, Alessandro Doldi, Jean-Pierre Fouque, Marco\n  Frittelli, Thilo Meyer-Brandis",
        "title": "Systemic Optimal Risk Transfer Equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel concept of a Systemic Optimal Risk Transfer Equilibrium\n(SORTE), which is inspired by the B\\\"uhlmann's classical notion of an\nEquilibrium Risk Exchange. We provide sufficient general assumptions that\nguarantee existence, uniqueness, and Pareto optimality of such a SORTE. In both\nthe B\\\"uhlmann and the SORTE definition, each agent is behaving rationally by\nmaximizing his/her expected utility given a budget constraint. The two\napproaches differ by the budget constraints. In B\\\"uhlmann's definition the\nvector that assigns the budget constraint is given a priori. On the contrary,\nin the SORTE approach, the vector that assigns the budget constraint is\nendogenously determined by solving a systemic utility maximization. SORTE gives\npriority to the systemic aspects of the problem, in order to optimize the\noverall systemic performance, rather than to individual rationality.\n"
    },
    {
        "paper_id": 1907.04373,
        "authors": "Souradeep Chakraborty",
        "title": "Capturing Financial markets to apply Deep Reinforcement Learning",
        "comments": "17 pages, 3 figures, 3 tables, accepted to be presented at the India\n  Finance Conference, IIM Ahmedabad, December 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we explore the usage of deep reinforcement learning algorithms\nto automatically generate consistently profitable, robust, uncorrelated trading\nsignals in any general financial market. In order to do this, we present a\nnovel Markov decision process (MDP) model to capture the financial trading\nmarkets. We review and propose various modifications to existing approaches and\nexplore different techniques like the usage of technical indicators, to\nsuccinctly capture the market dynamics to model the markets. We then go on to\nuse deep reinforcement learning to enable the agent (the algorithm) to learn\nhow to take profitable trades in any market on its own, while suggesting\nvarious methodology changes and leveraging the unique representation of the\nFMDP (financial MDP) to tackle the primary challenges faced in similar works.\nThrough our experimentation results, we go on to show that our model could be\neasily extended to two very different financial markets and generates a\npositively robust performance in all conducted experiments.\n"
    },
    {
        "paper_id": 1907.04422,
        "authors": "Gunduz Caginalp and Mark DeSantis",
        "title": "Nonlinear price dynamics of S&P 100 stocks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.122067",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The methodology presented provides a quantitative way to characterize\ninvestor behavior and price dynamics within a particular asset class and time\nperiod. The methodology is applied to a data set consisting of over 250,000\ndata points of the S&P 100 stocks during 2004-2018. Using a two-way\nfixed-effects model, we uncover trader motivations including evidence of both\nunder- and overreaction within a unified setting. A nonlinear relationship is\nfound between return and trend suggesting a small, positive trend increases the\nreturn, while a larger one tends to decrease it. The shape parameters of the\nnonlinearity quantify trader motivation to buy into trends or wait for\nbargains. The methodology allows the testing of any behavioral finance bias or\ntechnical analysis concept.\n"
    },
    {
        "paper_id": 1907.04447,
        "authors": "Saannidhya Rawat",
        "title": "Relationships between different Macroeconomic Variables using VECM",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through this paper, an attempt has been made to quantify the underlying\nrelationships between the leading macroeconomic indicators. More clearly, an\neffort has been made in this paper to assess the cointegrating relationships\nand examine the error correction behavior revealed by macroeconomic variables\nusing econometric techniques that were initially developed by Engle and Granger\n(1987), and further explored by various succeeding papers, with the latest\nbeing Tu and Yi (2017). Gross Domestic Product, Discount Rate, Consumer Price\nIndex and population of U.S are representatives of the economy that have been\nused in this study to analyze the relationships between economic indicators and\nunderstand how an adverse change in one of these variables might have\nramifications on the others. This is performed to corroborate and guide the\nbelief that a policy maker with specified intentions cannot ignore the\nspillover effects caused by implementation of a certain policy.\n"
    },
    {
        "paper_id": 1907.04925,
        "authors": "Riccardo Marcaccioli, Giacomo Livan",
        "title": "Maximum Entropy approach to multivariate time series randomization",
        "comments": "20 pages, 6 figures, 5 tables",
        "journal-ref": "Scientific Reports volume 10, Article number: 10656 (2020)",
        "doi": "10.1038/s41598-020-67536-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Natural and social multivariate systems are commonly studied through sets of\nsimultaneous and time-spaced measurements of the observables that drive their\ndynamics, i.e., through sets of time series. Typically, this is done via\nhypothesis testing: the statistical properties of the empirical time series are\ntested against those expected under a suitable null hypothesis. This is a very\nchallenging task in complex interacting systems, where statistical stability is\noften poor due to lack of stationarity and ergodicity. Here, we describe an\nunsupervised, data-driven framework to perform hypothesis testing in such\nsituations. This consists of a statistical mechanical approach - analogous to\nthe configuration model for networked systems - for ensembles of time series\ndesigned to preserve, on average, some of the statistical properties observed\non an empirical set of time series. We showcase its possible applications with\na case study on financial portfolio selection.\n"
    },
    {
        "paper_id": 1907.04937,
        "authors": "Mohammed Kaicer and Abdelilah Kaddar",
        "title": "Mathematical Analysis of Dynamic Risk Default in Microfinance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we will develop a new approach to solve the non repayment\nproblem in microfinance due to the problem of asymmetric information. This\napproach is based on modeling and simulation of ordinary differential systems\nwhere time remains a primordial component, they thus enable microfinance\ninstitutions to manage their risk portfolios by a prediction of numbers of\nsolvent and insolvent borrowers ever a period, in order to define or redefine\nits development strategy, investment and management in an area, where the\npopulation is often poor and in need a mechanism of financial inclusion.\n"
    },
    {
        "paper_id": 1907.05049,
        "authors": "Peng-Fei Dai (TJU), Xiong Xiong (TJU), Wei-Xing Zhou (ECUST)",
        "title": "A global economic policy uncertainty index from principal component\n  analysis",
        "comments": "5 pages, 6 figures",
        "journal-ref": "Finance Research Letters 40, 101686 (2021)",
        "doi": "10.1016/j.frl.2020.101686",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper constructs a global economic policy uncertainty index through the\nprincipal component analysis of the economic policy uncertainty indices for\ntwenty primary economies around the world. We find that the PCA-based global\neconomic policy uncertainty index is a good proxy for the economic policy\nuncertainty on a global scale, which is quite consistent with the GDP-weighted\nglobal economic policy uncertainty index. The PCA-based economic policy\nuncertainty index is found to be positively related with the volatility and\ncorrelation of the global financial market, which indicates that the stocks are\nmore volatile and correlated when the global economic policy uncertainty is\nhigher. The PCA-based global economic policy uncertainty index performs\nslightly better because the relationship between the PCA-based uncertainty and\nmarket volatility and correlation is more significant.\n"
    },
    {
        "paper_id": 1907.05072,
        "authors": "Eckhard Platen and Stefan Tappe",
        "title": "Real-world forward rate dynamics with affine realizations",
        "comments": "31 pages",
        "journal-ref": "Stochastic Analysis and Applications 33(4):573-608, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the existence of affine realizations for L\\'{e}vy driven\ninterest rate term structure models under the real-world probability measure,\nwhich so far has only been studied under an assumed risk-neutral probability\nmeasure. For models driven by Wiener processes, all results obtained under the\nrisk-neutral approach concerning the existence of affine realizations are\ntransferred to the general case. A similar result holds true for models driven\nby compound Poisson processes with finite jump size distributions. However, in\nthe presence of jumps with infinite activity we obtain severe restrictions on\nthe structure of the market price of risk; typically, it must even be constant.\n"
    },
    {
        "paper_id": 1907.05141,
        "authors": "Uwe K\\\"uchler and Stefan Tappe",
        "title": "Tempered stable distributions and processes",
        "comments": "34 pages",
        "journal-ref": "Stochastic Processes and Their Applications 123(12):4256-4293,\n  2013",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the class of tempered stable distributions and their\nassociated processes. Our analysis of tempered stable distributions includes\nlimit distributions, parameter estimation and the study of their densities.\nRegarding tempered stable processes, we deal with density transformations and\ncompute their $p$-variation indices. Exponential stock models driven by\ntempered stable processes are discussed as well.\n"
    },
    {
        "paper_id": 1907.05142,
        "authors": "Uwe K\\\"uchler and Stefan Tappe",
        "title": "Exponential stock models driven by tempered stable processes",
        "comments": "20 pages",
        "journal-ref": "Journal of Econometrics 181(1):53-63, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate exponential stock models driven by tempered stable processes,\nwhich constitute a rich family of purely discontinuous L\\'{e}vy processes. With\na view of option pricing, we provide a systematic analysis of the existence of\nequivalent martingale measures, under which the model remains analytically\ntractable. This includes the existence of Esscher martingale measures and\nmartingale measures having minimal distance to the physical probability\nmeasure. Moreover, we provide pricing formulae for European call options and\nperform a case study.\n"
    },
    {
        "paper_id": 1907.05157,
        "authors": "Stefan Tappe and Stefan Weber",
        "title": "Stochastic mortality models: An infinite dimensional approach",
        "comments": "28 pages",
        "journal-ref": "Finance and Stochastics 18(1):209-248, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Demographic projections of future mortality rates involve a high level of\nuncertainty and require stochastic mortality models. The current paper\ninvestigates forward mortality models driven by a (possibly infinite\ndimensional) Wiener process and a compensated Poisson random measure. A major\ninnovation of the paper is the introduction of a family of processes called\nforward mortality improvements which provide a flexible tool for a simple\nconstruction of stochastic forward mortality models. In practice, the notion of\nmortality improvements are a convenient device for the quantification of\nchanges in mortality rates over time that enables, for example, the detection\nof cohort effects.\n  We show that the forward mortality rates satisfy Heath-Jarrow-Morton-type\nconsistency conditions which translate to the forward mortality improvements.\nWhile the consistency conditions of the forward mortality rates are analogous\nto the classical conditions in the context of bond markets, the conditions of\nthe forward mortality improvements possess a different structure: forward\nmortality models include a cohort parameter besides the time horizon; these two\ndimensions are coupled in the dynamics of consistent models of forwards\nmortality improvements. In order to obtain a unified framework, we transform\nthe systems of It\\^o-processes which describe the forward mortality rates and\nimprovements: in contrast to term-structure models, the corresponding\nstochastic partial differential equations (SPDEs) describe the random dynamics\nof two-dimensional surfaces rather than curves.\n"
    },
    {
        "paper_id": 1907.05348,
        "authors": "M. Dashti Moghaddam, Zhiyuan Liu and R. A. Serota",
        "title": "Distributions of Historic Market Data -- Relaxation and Correlations",
        "comments": "17 pages, 8 figures, 3 tables",
        "journal-ref": "Eur. Phys. J. B 94, 83 (2021)",
        "doi": "10.1140/epjb/s10051-021-00089-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate relaxation and correlations in a class of mean-reverting\nmodels for stochastic variances. We derive closed-form expressions for the\ncorrelation functions and leverage for a general form of the stochastic term.\nWe also discuss correlation functions and leverage for three specific models --\nmultiplicative, Heston (Cox-Ingersoll-Ross) and combined multiplicative-Heston\n-- whose steady-state probability density functions are Gamma, Inverse Gamma\nand Beta Prime respectively, the latter two exhibiting \"fat\" tails. For the\nHeston model, we apply the eigenvalue analysis of the Fokker-Planck equation to\nderive the correlation function -- in agreement with the general analysis --\nand to identify a series of time scales, which are observable in relaxation of\ncumulants on approach to the steady state. We test our findings on a very large\nset of historic financial markets data.\n"
    },
    {
        "paper_id": 1907.05381,
        "authors": "Yuqing Zhang and Neil Walton",
        "title": "Adaptive Pricing in Insurance: Generalized Linear Models and Gaussian\n  Process Regression Approaches",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the application of dynamic pricing to insurance. We view this as an\nonline revenue management problem where the insurance company looks to set\nprices to optimize the long-run revenue from selling a new insurance product.\nWe develop two pricing models: an adaptive Generalized Linear Model (GLM) and\nan adaptive Gaussian Process (GP) regression model. Both balance between\nexploration, where we choose prices in order to learn the distribution of\ndemands & claims for the insurance product, and exploitation, where we\nmyopically choose the best price from the information gathered so far. The\nperformance of the pricing policies is measured in terms of regret: the\nexpected revenue loss caused by not using the optimal price. As is commonplace\nin insurance, we model demand and claims by GLMs. In our adaptive GLM design,\nwe use the maximum quasi-likelihood estimation (MQLE) to estimate the unknown\nparameters. We show that, if prices are chosen with suitably decreasing\nvariability, the MQLE parameters eventually exist and converge to the correct\nvalues, which in turn implies that the sequence of chosen prices will also\nconverge to the optimal price. In the adaptive GP regression model, we sample\ndemand and claims from Gaussian Processes and then choose selling prices by the\nupper confidence bound rule. We also analyze these GLM and GP pricing\nalgorithms with delayed claims. Although similar results exist in other\ndomains, this is among the first works to consider dynamic pricing problems in\nthe field of insurance. We also believe this is the first work to consider\nGaussian Process regression in the context of insurance pricing. These initial\nfindings suggest that online machine learning algorithms could be a fruitful\narea of future investigation and application in insurance.\n"
    },
    {
        "paper_id": 1907.05582,
        "authors": "Michael S. Harr\\'e and Adam Harris and Scott McCallum",
        "title": "Singularities and Catastrophes in Economics: Historical Perspectives and\n  Future Directions",
        "comments": "23 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic theory is a mathematically rich field in which there are\nopportunities for the formal analysis of singularities and catastrophes. This\narticle looks at the historical context of singularities through the work of\ntwo eminent Frenchmen around the late 1960s and 1970s. Ren\\'e Thom (1923-2002)\nwas an acclaimed mathematician having received the Fields Medal in 1958,\nwhereas G\\'erard Debreu (1921-2004) would receive the Nobel Prize in economics\nin 1983. Both were highly influential within their fields and given the\nfundamental nature of their work, the potential for cross-fertilisation would\nseem to be quite promising. This was not to be the case: Debreu knew of Thom's\nwork and cited it in the analysis of his own work, but despite this and other\napplied mathematicians taking catastrophe theory to economics, the theory never\nachieved a lasting following and relatively few results were published. This\narticle reviews Debreu's analysis of the so called ${\\it regular}$ and ${\\it\ncrtitical}$ economies in order to draw some insights into the economic\nperspective of singularities before moving to how singularities arise naturally\nin the Nash equilibria of game theory. Finally a modern treatment of stochastic\ngame theory is covered through recent work on the quantal response equilibrium.\nIn this view the Nash equilibrium is to the quantal response equilibrium what\ndeterministic catastrophe theory is to stochastic catastrophe theory, with some\ncaveats regarding when this analogy breaks down discussed at the end.\n"
    },
    {
        "paper_id": 1907.05593,
        "authors": "Laurence Carassus and Miklos Rasonyi",
        "title": "From small markets to big markets",
        "comments": "Final version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the most famous example of a large financial market: the Arbitrage\nPricing Model, where investors can trade in a one-period setting with countably\nmany assets admitting a factor structure. We consider the problem of maximising\nexpected utility in this setting. Besides establishing the existence of\noptimizers under weaker assumptions than previous papers, we go on studying the\nrelationship between optimal investments in finite market segments and those in\nthe whole market. We show that certain natural (but nontrivial) continuity\nrules hold: maximal satisfaction, reservation prices and (convex combinations\nof) optimizers computed in small markets converge to their respective\ncounterparts in the big market.\n"
    },
    {
        "paper_id": 1907.05689,
        "authors": "Samuel N. Cohen and Tanut Treetanthiploet",
        "title": "Gittins' theorem under uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study dynamic allocation problems for discrete time multi-armed bandits\nunder uncertainty, based on the the theory of nonlinear expectations. We show\nthat, under strong independence of the bandits and with some relaxation in the\ndefinition of optimality, a Gittins allocation index gives optimal choices.\nThis involves studying the interaction of our uncertainty with controls which\ndetermine the filtration. We also run a simple numerical example which\nillustrates the interaction between the willingness to explore and uncertainty\naversion of the agent when making decisions.\n"
    },
    {
        "paper_id": 1907.05697,
        "authors": "J.M. Calabuig, H. Falciani and E.A. S\\'anchez-P\\'erez",
        "title": "Dreaming machine learning: Lipschitz extensions for reinforcement\n  learning on financial markets",
        "comments": "20 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a quasi-metric topological structure for the construction of a\nnew reinforcement learning model in the framework of financial markets. It is\nbased on a Lipschitz type extension of reward functions defined in metric\nspaces. Specifically, the McShane and Whitney extensions are considered for a\nreward function which is defined by the total evaluation of the benefits\nproduced by the investment decision at a given time. We define the metric as a\nlinear combination of a Euclidean distance and an angular metric component. All\ninformation about the evolution of the system from the beginning of the time\ninterval is used to support the extension of the reward function, but in\naddition this data set is enriched by adding some artificially produced states.\nThus, the main novelty of our method is the way we produce more states -- which\nwe call \"dreams\" -- to enrich learning. Using some known states of the\ndynamical system that represents the evolution of the financial market, we use\nour technique to simulate new states by interpolating real states and\nintroducing some random variables. These new states are used to feed a learning\nalgorithm designed to improve the investment strategy by following a typical\nreinforcement learning scheme.\n"
    },
    {
        "paper_id": 1907.05954,
        "authors": "Torsten Heinrich and Juan Sabuco and J. Doyne Farmer",
        "title": "A simulation of the insurance industry: The problem of risk model\n  homogeneity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an agent-based simulation of the catastrophe insurance and\nreinsurance industry and use it to study the problem of risk model homogeneity.\nThe model simulates the balance sheets of insurance firms, who collect premiums\nfrom clients in return for ensuring them against intermittent, heavy-tailed\nrisks. Firms manage their capital and pay dividends to their investors, and use\neither reinsurance contracts or cat bonds to hedge their tail risk. The model\ngenerates plausible time series of profits and losses and recovers stylized\nfacts, such as the insurance cycle and the emergence of asymmetric, long tailed\nfirm size distributions. We use the model to investigate the problem of risk\nmodel homogeneity. Under Solvency II, insurance companies are required to use\nonly certified risk models. This has led to a situation in which only a few\nfirms provide risk models, creating a systemic fragility to the errors in these\nmodels. We demonstrate that using too few models increases the risk of\nnonpayment and default while lowering profits for the industry as a whole. The\npresence of the reinsurance industry ameliorates the problem but does not\nremove it. Our results suggest that it would be valuable for regulators to\nincentivize model diversity. The framework we develop here provides a first\nstep toward a simulation model of the insurance industry for testing policies\nand strategies for better capital management.\n"
    },
    {
        "paper_id": 1907.06118,
        "authors": "Geoff Boeing",
        "title": "Online Rental Housing Market Representation and the Digital Reproduction\n  of Urban Inequality",
        "comments": null,
        "journal-ref": "Environment and Planning A: Economy and Space, 2019",
        "doi": "10.1177/0308518X19869678",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the rental housing market moves online, the Internet offers divergent\npossible futures: either the promise of more-equal access to information for\npreviously marginalized homeseekers, or a reproduction of longstanding\ninformation inequalities. Biases in online listings' representativeness could\nimpact different communities' access to housing search information, reinforcing\ntraditional information segregation patterns through a digital divide. They\ncould also circumscribe housing practitioners' and researchers' ability to draw\nbroad market insights from listings to understand rental supply and\naffordability. This study examines millions of Craigslist rental listings\nacross the US and finds that they spatially concentrate and over-represent\nwhiter, wealthier, and better-educated communities. Other significant\ndemographic differences exist in age, language, college enrollment, rent,\npoverty rate, and household size. Most cities' online housing markets are\ndigitally segregated by race and class, and we discuss various implications for\nresidential mobility, community legibility, gentrification, housing voucher\nutilization, and automated monitoring and analytics in the smart cities\nparadigm. While Craigslist contains valuable crowdsourced data to better\nunderstand affordability and available rental supply in real-time, it does not\nevenly represent all market segments. The Internet promises information\ndemocratization, and online listings can reduce housing search costs and\nincrease choice sets. However, technology access/preferences and information\nchannel segregation can concentrate such information-broadcasting benefits in\nalready-advantaged communities, reproducing traditional inequalities and\nreinforcing residential sorting and segregation dynamics. Technology platforms\nlike Craigslist construct new institutions with the power to shape spatial\neconomies.\n"
    },
    {
        "paper_id": 1907.06151,
        "authors": "Aditi Dandapani, Paul Jusselin, Mathieu Rosenbaum",
        "title": "From quadratic Hawkes processes to super-Heston rough volatility models\n  with Zumbach effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using microscopic price models based on Hawkes processes, it has been shown\nthat under some no-arbitrage condition, the high degree of endogeneity of\nmarkets together with the phenomenon of metaorders splitting generate rough\nHeston-type volatility at the macroscopic scale. One additional important\nfeature of financial dynamics, at the heart of several influential works in\neconophysics, is the so-called feedback or Zumbach effect. This essentially\nmeans that past trends in returns convey significant information on future\nvolatility. A natural way to reproduce this property in microstructure modeling\nis to use quadratic versions of Hawkes processes. We show that after suitable\nrescaling, the long term limits of these processes are refined versions of\nrough Heston models where the volatility coefficient is enhanced compared to\nthe square root characterizing Heston-type dynamics. Furthermore the Zumbach\neffect remains explicit in these limiting rough volatility models.\n"
    },
    {
        "paper_id": 1907.0623,
        "authors": "Ke Xu and Martin D. Gould and Sam D. Howison",
        "title": "Multi-Level Order-Flow Imbalance in a Limit Order Book",
        "comments": "32 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the multi-level order-flow imbalance (MLOFI), which is a vector\nquantity that measures the net flow of buy and sell orders at different price\nlevels in a limit order book (LOB). Using a recent, high-quality data set for 6\nliquid stocks on Nasdaq, we fit a simple, linear relationship between MLOFI and\nthe contemporaneous change in mid-price. For all 6 stocks that we study, we\nfind that the out-of-sample goodness-of-fit of the relationship improves with\neach additional price level that we include in the MLOFI vector. Our results\nunderline how order-flow activity deep into the LOB can influence the\nprice-formation process.\n"
    },
    {
        "paper_id": 1907.06465,
        "authors": "Felix Ritchie and Jim Smith",
        "title": "Confidentiality and linked data",
        "comments": "Paper published as part of The National Statistician's Quality\n  Review. London, December 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data providers such as government statistical agencies perform a balancing\nact: maximising information published to inform decision-making and research,\nwhile simultaneously protecting privacy. The emergence of identified\nadministrative datasets with the potential for sharing (and thus linking)\noffers huge potential benefits but significant additional risks. This article\nintroduces the principles and methods of linking data across different sources\nand points in time, focusing on potential areas of risk. We then consider\nconfidentiality risk, focusing in particular on the \"intruder\" problem central\nto the area, and looking at both risks from data producer outputs and from the\nrelease of micro-data for further analysis. Finally, we briefly consider\npotential solutions to micro-data release, both the statistical solutions\nconsidered in other contributed articles and non-statistical solutions.\n"
    },
    {
        "paper_id": 1907.06474,
        "authors": "Bernard Lapeyre (CERMICS, MATHRISK), J\\'er\\^ome Lelong (DAO)",
        "title": "Neural network regression for Bermudan option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The pricing of Bermudan options amounts to solving a dynamic programming\nprinciple, in which the main difficulty, especially in high dimension, comes\nfrom the conditional expectation involved in the computation of the\ncontinuation value. These conditional expectations are classically computed by\nregression techniques on a finite dimensional vector space. In this work, we\nstudy neural networks approximations of conditional expectations. We prove the\nconvergence of the well-known Longstaff and Schwartz algorithm when the\nstandard least-square regression is replaced by a neural network approximation.\nWe illustrate the numerical efficiency of neural networks as an alternative to\nstandard regression methods for approximating conditional expectations on\nseveral numerical examples.\n"
    },
    {
        "paper_id": 1907.06673,
        "authors": "Magnus Wiese, Robert Knobloch, Ralf Korn, Peter Kretschmer",
        "title": "Quant GANs: Deep Generation of Financial Time Series",
        "comments": "Corrected typos. Added section 2 as an overview of existing\n  literature. Added section 5.3 to clarify the modeling assumptions. Appendix B\n  now contains more details on the neural network architectures used. Changed\n  latex template",
        "journal-ref": "Quantitative Finance, 2020",
        "doi": "10.1080/14697688.2020.1730426",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling financial time series by stochastic processes is a challenging task\nand a central area of research in financial mathematics. As an alternative, we\nintroduce Quant GANs, a data-driven model which is inspired by the recent\nsuccess of generative adversarial networks (GANs). Quant GANs consist of a\ngenerator and discriminator function, which utilize temporal convolutional\nnetworks (TCNs) and thereby achieve to capture long-range dependencies such as\nthe presence of volatility clusters. The generator function is explicitly\nconstructed such that the induced stochastic process allows a transition to its\nrisk-neutral distribution. Our numerical results highlight that distributional\nproperties for small and large lags are in an excellent agreement and\ndependence properties such as volatility clusters, leverage effects, and serial\nautocorrelations can be generated by the generator function of Quant GANs,\ndemonstrably in high fidelity.\n"
    },
    {
        "paper_id": 1907.07101,
        "authors": "Justo Puerto and Moises Rodr\\'iguez-Madrena and Andrea Scozzari",
        "title": "Location and portfolio selection problems: A unified framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a set of assets and an investment capital, the classical portfolio\nselection problem consists in determining the amount of capital to be invested\nin each asset in order to build the most profitable portfolio. The portfolio\noptimization problem is naturally modeled as a mean-risk bi-criteria\noptimization problem where the mean rate of return of the portfolio must be\nmaximized whereas a given risk measure must be minimized. Several mathematical\nprogramming models and techniques have been presented in the literature in\norder to efficiently solve the portfolio problem. A relatively recent promising\nline of research is to exploit clustering information of an assets network in\norder to develop new portfolio optimization paradigms. In this paper we endow\nthe assets network with a metric based on correlation coefficients between\nassets' returns, and show how classical location problems on networks can be\nused for clustering assets. In particular, by adding a new criterion to the\nportfolio selection problem based on an objective function of a classical\nlocation problem, we are able to measure the effect of clustering on the\nselected assets with respect to the non-selected ones. Most papers dealing with\nclustering and portfolio selection models solve these problems in two distinct\nsteps: cluster first and then selection. The innovative contribution of this\npaper is that we propose a Mixed-Integer Linear Programming formulation for\ndealing with this problem in a unified phase. The effectiveness of our approach\nis validated reporting some preliminary computational experiments on some real\nfinancial dataset.\n"
    },
    {
        "paper_id": 1907.07108,
        "authors": "Burin Gumjudpai (IF Naresuan, ThEP Center, Econ NIDA) and Yuthana\n  Sethapramote (Econ NIDA)",
        "title": "Nature of thermodynamics equation of state towards economics equation of\n  state",
        "comments": "Proceedings of the 11th Silpakorn University Research Fair, Silpakorn\n  University, Thailand (2019)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work critics on nature of thermodynamics coordinates and on roles of the\nvariables in the equation of state (EoS). Coordinate variables in the EoS are\nanalyzed so that central concepts are noticed and are used to lay a foundation\nin building of a new EoS or in testing EoS status of a newly constructed\nempirical equation. With these concepts, we classify EoS into two classes. We\nfind that the EoS of market with unitary price demand and linear\nprice-dependent supply function proposed by \\cite{GumjMarket}, is not an EoS\nbecause it has only one degree of freedom.\n"
    },
    {
        "paper_id": 1907.07305,
        "authors": "Peter Carr, Andrey Itkin and Sasha Stoikov",
        "title": "A model-free backward and forward nonlinear PDEs for implied volatility",
        "comments": "31 pages, 9 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a backward and forward nonlinear PDEs that govern the implied\nvolatility of a contingent claim whenever the latter is well-defined. This\nwould include at least any contingent claim written on a positive stock price\nwhose payoff at a possibly random time is convex. We also discuss suitable\ninitial and boundary conditions for those PDEs. Finally, we demonstrate how to\nsolve them numerically by using an iterative finite-difference approach.\n"
    },
    {
        "paper_id": 1907.07425,
        "authors": "Federico Guglielmo Morelli and Michael Benzaquen and Marco Tarzia and\n  Jean-Philippe Bouchaud",
        "title": "Confidence Collapse in a Multi-Household, Self-Reflexive DSGE Model",
        "comments": "6 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a multi-household DSGE model in which past aggregate\nconsumption impacts the confidence, and therefore consumption propensity, of\nindividual households. We find that such a minimal setup is extremely rich, and\nleads to a variety of realistic output dynamics: high output with no crises;\nhigh output with increased volatility and deep, short lived recessions;\nalternation of high and low output states where relatively mild drop in\neconomic conditions can lead to a temporary confidence collapse and steep\ndecline in economic activity. The crisis probability depends exponentially on\nthe parameters of the model, which means that markets cannot efficiently price\nthe associated risk premium. We conclude by stressing that within our\nframework, {\\it narratives} become an important monetary policy tool, that can\nhelp steering the economy back on track.\n"
    },
    {
        "paper_id": 1907.07491,
        "authors": "A. M. B. Araujo, P. R. B. Lustosa",
        "title": "The cyclicality of loan loss provisions under three different accounting\n  models: the United Kingdom, Spain, and Brazil",
        "comments": "17 pages",
        "journal-ref": "Revista Contabilidade e Financas. USP, Sao Paulo, v. 29, n. 76, p.\n  97-113, Jan-Abr 2018",
        "doi": "10.1590/1808-057x201804490",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A controversy involving loan loss provisions in banks concerns their\nrelationship with the business cycle. While international accounting standards\nfor recognizing provisions (incurred loss model) would presumably be\npro-cyclical, accentuating the effects of the current economic cycle, an\nalternative model, the expected loss model, has countercyclical\ncharacteristics, acting as a buffer against economic imbalances caused by\nexpansionary or contractionary phases in the economy. In Brazil, a mixed\naccounting model exists, whose behavior is not known to be pro-cyclical or\ncountercyclical. The aim of this research is to analyze the behavior of these\naccounting models in relation to the business cycle, using an econometric model\nconsisting of financial and macroeconomic variables. The study allowed us to\nidentify the impact of credit risk behavior, earnings management, capital\nmanagement, Gross Domestic Product (GDP) behavior, and the behavior of the\nunemployment rate on provisions in countries that use different accounting\nmodels. Data from commercial banks in the United Kingdom (incurred loss), in\nSpain (expected loss), and in Brazil (mixed model) were used, covering the\nperiod from 2001 to 2012. Despite the accounting models of the three countries\nbeing formed by very different rules regarding possible effects on the business\ncycles, the results revealed a pro-cyclical behavior of provisions in each\ncountry, indicating that when GDP grows, provisions tend to fall and vice\nversa. The results also revealed other factors influencing the behavior of loan\nloss provisions, such as earning management.\n"
    },
    {
        "paper_id": 1907.07514,
        "authors": "Peter Cotton",
        "title": "Self Organizing Supply Chains for Micro-Prediction: Present and Future\n  uses of the ROAR Protocol",
        "comments": "Thirty-sixth International Conference on Machine Learning Workshop on\n  AI in Finance: Applications and Infrastructure for Multi-Agent Learning",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A multi-agent system is trialed as a means of crowd-sourcing inexpensive but\nhigh quality streams of predictions. Each agent is a microservice embodying\nstatistical models and endowed with economic self-interest. The ability to fork\nand modify simple agents is granted to a large number of employees in a firm\nand empirical lessons are reported. We suggest that one plausible trajectory\nfor this project is the creation of a Prediction Web.\n"
    },
    {
        "paper_id": 1907.07858,
        "authors": "Michelle Baddeley",
        "title": "Behavioural Macroeconomic Policy: New perspectives on time inconsistency",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper brings together divergent approaches to time inconsistency from\nmacroeconomic policy and behavioural economics. Behavioural discount functions\nfrom behavioural microeconomics are embedded into a game-theoretic analysis of\ntemptation versus enforcement to construct an encompassing model, nesting\ncombinations of time consistent and time inconsistent preferences. The analysis\npresented in this paper shows that, with hyperbolic/quasihyperbolic\ndiscounting, the enforceable range of inflation targets is narrowed. This\nsuggests limits to the effectiveness of monetary targets, under certain\nconditions. The paper concludes with a discussion of monetary policy\nimplications, explored specifically in the light of current macroeconomic\npolicy debates.\n"
    },
    {
        "paper_id": 1907.07885,
        "authors": "Suneel Sarswat and Abhishek Kr Singh",
        "title": "Formal verification of trading in financial markets",
        "comments": "Preprint of 12 pages in lipicsv2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a formal framework for analyzing trades in financial markets. An\nexchange is where multiple buyers and sellers participate to trade. These days,\nall big exchanges use computer algorithms that implement double sided auctions\nto match buy and sell requests and these algorithms must abide by certain\nregulatory guidelines. For example, market regulators enforce that a matching\nproduced by exchanges should be \\emph{fair}, \\emph{uniform} and\n\\emph{individual rational}. To verify these properties of trades, we first\nformally define these notions in a theorem prover and then give formal proofs\nof relevant results on matchings. Finally, we use this framework to verify\nproperties of two important classes of double sided auctions. All the\ndefinitions and results presented in this paper are completely formalised in\nthe Coq proof assistant without adding any additional axioms to it.\n"
    },
    {
        "paper_id": 1907.07908,
        "authors": "Paolo Bartesaghi, Michele Benzi, Gian Paolo Clemente, Rosanna Grassi\n  and Ernesto Estrada",
        "title": "Risk-dependent centrality in economic and financial networks",
        "comments": "This version is accepted on SIAM journal on Financial Mathematics",
        "journal-ref": "SIAM J. FINANCIAL MATH. 2020",
        "doi": "10.1137/19M13020411",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Node centrality is one of the most important and widely used concepts in the\nstudy of complex networks. Here, we extend the paradigm of node centrality in\nfinancial and economic networks to consider the changes of node \"importance\"\nproduced not only by the variation of the topology of the system but also as a\nconsequence of the external levels of risk to which the network as a whole is\nsubmitted. Starting from the \"Susceptible-Infected\" (SI) model of epidemics and\nits relation to the communicability functions of networks we develop a series\nof risk-dependent centralities for nodes in (financial and economic) networks.\nWe analyze here some of the most important mathematical properties of these\nrisk-dependent centrality measures. In particular, we study the newly observed\nphenomenon of ranking interlacement, by means of which two entities may\ninterlace their ranking positions in terms of risk in the network as a\nconsequence of the change in the external conditions only, i.e., without any\nchange in the topology. We test the risk-dependent centralities by studying two\nreal-world systems: the network generated by collecting assets of the S\\&P 100\nand the corporate board network of the US top companies, according to Forbes in\n1999. We found that a high position in the ranking of the analyzed financial\ncompanies according to their risk-dependent centrality corresponds to companies\nmore sensitive to the external market variations during the periods of crisis.\n"
    },
    {
        "paper_id": 1907.07975,
        "authors": "Beka Dalakishvili, Ana Mikatadze",
        "title": "Powershare Mechanics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes the governance framework of a gamified social network for\ncharity crowdfunding fueled by public computing. It introduces optimal scarce\nresource allocation model, technological configuration of the FIRE consensus\nprotocol, and multi-layer incentivization structure that maximizes value\ncreation within the network.\n"
    },
    {
        "paper_id": 1907.08047,
        "authors": "Mohammed Louriki",
        "title": "Brownian bridge with random length and pinning point for modelling of\n  financial information",
        "comments": "29 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce an extension of a Brownian bridge with a random\nlength by including uncertainty also in the pinning level of the bridge. The\nmain result of this work is that unlike for deterministic pinning point, the\nbridge process fails to be Markovian if the pining point distribution is\nabsolutely continuous with respect to the Lebesgue measure. Further results\ninclude the derivation of formulae to calculate the conditional expectation of\nvarious functions of the random pinning time, the random pinning location, and\nthe future value of the Brownian bridge, given an observation of the underlying\nprocess. For the specific case that the pining point has a two-point\ndistribution, we state further properties of the Brownian bridge, e.g., the\nright continuity of its natural filtration and its semi-martingale\ndecomposition. The newly introduced process can be used to model the flow of\ninformation about the behaviour of a gas storage contract holder; concerning\nwhether to inject or withdraw gas at some random future time.\n"
    },
    {
        "paper_id": 1907.08397,
        "authors": "Dhruv Mahajan, Abhijeet Chandra",
        "title": "Stochastic Spread Pairs Trading in the Indian Commodity Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we applied a stochastic spread pairs trading strategy on the\nIndian commodity market. The complete set of commodities were taken whose spot\nprice was available for the period of January 1st 2010 to December 31st 2018\nincluding energy, metals and the agricultural commodity sector. Spot data was\ntaken from the MCX pooled spot prices for 17 commodities. The data was split\ninto training period (January 1st 2010 to 14th March 2017) and testing\nperiod(15th Match 2017 to 31st December 2018). The splitting was done using a\n80:20 split.Johanssen Cointegration tests were done on training data for pairs\nof commodities to check for long-run relationship and the cointegrated\ncommodities were selected for formation of the trading process. We found a\ntotal of 12 cointegrated pairs out of 136 possible pairs. Cointegration was\nassumed for the testing period. A single-factor stochastic trading approach was\napplied on the logarithmic spread of the cointegrated pairs for both the\ntraining and testing period.The parameters of stochastic spread model were\nestimated using differential evolution algorithm. Also parameters for the\ntrading rule were optimized by backtesting on the training period and assumed\nfor the testing period. The results show a sharpe ratio of above 1.4 for all\nthe commodity cointegrated pairs in the backtesing period.\n"
    },
    {
        "paper_id": 1907.08499,
        "authors": "George Bouzianis, Lane P. Hughston, Sebastian Jaimungal, Leandro\n  S\\'anchez-Betancourt",
        "title": "L\\'evy-Ito Models in Finance",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an overview of the broad class of financial models in which the\nprices of assets are L\\'evy-Ito processes driven by an $n$-dimensional Brownian\nmotion and an independent Poisson random measure. The Poisson random measure is\nassociated with an $n$-dimensional L\\'evy process. Each model consists of a\npricing kernel, a money market account, and one or more risky assets. We show\nhow the excess rate of return above the interest rate can be calculated for\nrisky assets in such models, thus showing the relationship between risk and\nreturn when asset prices have jumps. The framework is applied to a variety of\nasset classes, allowing one to construct new models as well as interesting\ngeneralizations of familiar models.\n"
    },
    {
        "paper_id": 1907.08771,
        "authors": "Chung-Han Hsieh, B. Ross Barmish, John A. Gubner",
        "title": "The Impact of Execution Delay on Kelly-Based Stock Trading:\n  High-Frequency Versus Buy and Hold",
        "comments": "Has been accepted to the IEEE Conference on Decision and Control,\n  2019",
        "journal-ref": "Proceedings of the IEEE Conference of Decision and Control (CDC),\n  pp. 2580-2585, Nice, France, 2019",
        "doi": "10.1109/CDC40024.2019.9029292",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trading based on Kelly's celebrated Expected Logarithmic Growth (ELG)\ncriterion, a well-known prescription for optimal resource allocation, has\nreceived considerable attention in the literature. Using ELG as the performance\nmetric, we compare the impact of trade execution delay on the relative\nperformance of high-frequency trading versus buy and hold. While it is\nintuitively obvious and straightforward to prove that in the presence of\nsufficiently high transaction costs, buy and hold is the better strategy, is it\npossible that with no transaction costs, buy and hold can still be the better\nstrategy? When there is no delay in trade execution, we prove a theorem saying\nthat the answer is ``no.'' However, when there is delay in trade execution, we\npresent simulation results using a binary lattice stock model to show that the\nanswer can be ``yes.'' This is seen to be true whether self-financing is\nimposed or not.\n"
    },
    {
        "paper_id": 1907.08911,
        "authors": "Andrey Sarantsev, Blessing Ofori-Atta, Brandon Flores",
        "title": "A Stock Market Model Based on CAPM and Market Size",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new system of stochastic differential equations which models\ndependence of market beta and unsystematic risk upon size, measured by market\ncapitalization. We fit our model using size deciles data from Kenneth French's\ndata library. This model is somewhat similar to generalized\nvolatility-stabilized models in (Pal, 2011; Pickova, 2013). The novelty of our\nwork is twofold. First, we take into account the difference between price and\ntotal returns (in other words, between market size and wealth processes).\nSecond, we work with actual market data. We study the long-term properties of\nthis system of equations, and reproduce observed linearity of the capital\ndistribution curve. Our model has two modifications: for price returns and for\nequity premium. Somewhat surprisingly, they exhibit the same fit, with very\nsimilar coefficients. In the Appendix, we analyze size-based real-world index\nfunds.\n"
    },
    {
        "paper_id": 1907.09144,
        "authors": "Nicole B\\\"auerle and Daniel Schmithals",
        "title": "Consistent upper price bounds for exotic options given a finite number\n  of call prices and their convergence",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance Vol. 24,\n  No. 02, 2150011 (2021)",
        "doi": "10.1142/S0219024921500114",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of finding a consistent upper price bound for exotic\noptions whose payoff depends on the stock price at two different predetermined\ntime points (e.g. Asian option), given a finite number of observed call prices\nfor these maturities. A model-free approach is used, only taking into account\nthat the (discounted) stock price process is a martingale under the\nno-arbitrage condition. In case the payoff is directionally convex we obtain\nthe worst case marginal pricing measures. The speed of convergence of the upper\nprice bound is determined when the number of observed stock prices increases.\nWe illustrate our findings with some numerical computations.\n"
    },
    {
        "paper_id": 1907.09218,
        "authors": "Christian Rein, Ludger R\\\"uschendorf, Thorsten Schmidt",
        "title": "Generalized statistical arbitrage concepts and related gain strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generalized statistical arbitrage concepts are introduced corresponding to\ntrading strategies which yield positive gains on average in a class of\nscenarios rather than almost surely. The relevant scenarios or market states\nare specified via an information system given by a $\\sigma$-algebra and so this\nnotion contains classical arbitrage as a special case. It also covers the\nnotion of statistical arbitrage introduced in Bondarenko (2003).\n  Relaxing these notions further we introduce generalized profitable strategies\nwhich include also static or semi-static strategies. Under standard\nno-arbitrage there may exist generalized gain strategies yielding positive\ngains on average under the specified scenarios.\n  In the first part of the paper we characterize these generalized statistical\nno-arbitrage notions. In the second part of the paper we construct several\nprofitable generalized strategies with respect to various choices of the\ninformation system. In particular, we consider several forms of embedded\nbinomial strategies and follow-the-trend strategies as well as partition-type\nstrategies. We study and compare their behaviour on simulated data.\nAdditionally, we find good performance on market data of these simple\nstrategies which makes them profitable candidates for real applications.\n"
    },
    {
        "paper_id": 1907.09452,
        "authors": "Adamantios Ntakaris, Juho Kanniainen, Moncef Gabbouj, Alexandros\n  Iosifidis",
        "title": "Mid-price Prediction Based on Machine Learning Methods with Technical\n  and Quantitative Indicators",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0234107",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction is a challenging task, but machine learning methods\nhave recently been used successfully for this purpose. In this paper, we\nextract over 270 hand-crafted features (factors) inspired by technical and\nquantitative analysis and tested their validity on short-term mid-price\nmovement prediction. We focus on a wrapper feature selection method using\nentropy, least-mean squares, and linear discriminant analysis. We also build a\nnew quantitative feature based on adaptive logistic regression for online\nlearning, which is constantly selected first among the majority of the proposed\nfeature selection methods. This study examines the best combination of features\nusing high frequency limit order book data from Nasdaq Nordic. Our results\nsuggest that sorting methods and classifiers can be used in such a way that one\ncan reach the best performance with a combination of only very few advanced\nhand-crafted features.\n"
    },
    {
        "paper_id": 1907.09567,
        "authors": "Naftali Cohen, Tucker Balch, and Manuela Veloso",
        "title": "The Effect of Visual Design in Image Classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial companies continuously analyze the state of the markets to rethink\nand adjust their investment strategies. While the analysis is done on the\ndigital form of data, decisions are often made based on graphical\nrepresentations in white papers or presentation slides. In this study, we\nexamine whether binary decisions are better to be decided based on the numeric\nor the visual representation of the same data. Using two data sets, a matrix of\nnumerical data with spatial dependencies and financial data describing the\nstate of the S&P index, we compare the results of supervised classification\nbased on the original numerical representation and the visual transformation of\nthe same data. We show that, for these data sets, the visual transformation\nresults in higher predictability skill compared to the original form of the\ndata. We suggest thinking of the visual representation of numeric data,\neffectively, as a combination of dimensional reduction and feature engineering\ntechniques. In particular, if the visual layout encapsulates the full\ncomplexity of the data. In this view, thoughtful visual design can guard\nagainst overfitting, or introduce new features -- all of which benefit the\nlearning process, and effectively lead to better recognition of meaningful\npatterns.\n"
    },
    {
        "paper_id": 1907.09639,
        "authors": "Rico Krueger and Taha H. Rashidi and Akshay Vij",
        "title": "Semi-Parametric Hierarchical Bayes Estimates of New Yorkers' Willingness\n  to Pay for Features of Shared Automated Vehicle Services",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we contrast parametric and semi-parametric representations of\nunobserved heterogeneity in hierarchical Bayesian multinomial logit models and\nleverage these methods to infer distributions of willingness to pay for\nfeatures of shared automated vehicle (SAV) services. Specifically, we compare\nthe multivariate normal (MVN), finite mixture of normals (F-MON) and Dirichlet\nprocess mixture of normals (DP-MON) mixing distributions. The latter promises\nto be particularly flexible in respect to the shapes it can assume and unlike\nother semi-parametric approaches does not require that its complexity is fixed\nprior to estimation. However, its properties relative to simpler mixing\ndistributions are not well understood. In this paper, we evaluate the\nperformance of the MVN, F-MON and DP-MON mixing distributions using simulated\ndata and real data sourced from a stated choice study on preferences for SAV\nservices in New York City. Our analysis shows that the DP-MON mixing\ndistribution provides superior fit to the data and performs at least as well as\nthe competing methods at out-of-sample prediction. The DP-MON mixing\ndistribution also offers substantive behavioural insights into the adoption of\nSAVs. We find that preferences for in-vehicle travel time by SAV with\nride-splitting are strongly polarised. Whereas one third of the sample is\nwilling to pay between 10 and 80 USD/h to avoid sharing a vehicle with\nstrangers, the remainder of the sample is either indifferent to ride-splitting\nor even desires it. Moreover, we estimate that new technologies such as vehicle\nautomation and electrification are relatively unimportant to travellers. This\nsuggests that travellers may primarily derive indirect, rather than immediate\nbenefits from these new technologies through increases in operational\nefficiency and lower operating costs.\n"
    },
    {
        "paper_id": 1907.09704,
        "authors": "Alex Garivaltis",
        "title": "A Note on Universal Bilinear Portfolios",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note provides a neat and enjoyable expansion and application of the\nmagnificent Ordentlich-Cover theory of \"universal portfolios.\" I generalize\nCover's benchmark of the best constant-rebalanced portfolio (or 1-linear\ntrading strategy) in hindsight by considering the best bilinear trading\nstrategy determined in hindsight for the realized sequence of asset prices. A\nbilinear trading strategy is a mini two-period active strategy whose final\ncapital growth factor is linear separately in each period's gross return vector\nfor the asset market. I apply Cover's ingenious (1991) performance-weighted\naveraging technique to construct a universal bilinear portfolio that is\nguaranteed (uniformly for all possible market behavior) to compound its money\nat the same asymptotic rate as the best bilinear trading strategy in hindsight.\nThus, the universal bilinear portfolio asymptotically dominates the original\n(1-linear) universal portfolio in the same technical sense that Cover's\nuniversal portfolios asymptotically dominate all constant-rebalanced portfolios\nand all buy-and-hold strategies. In fact, like so many Russian dolls, one can\nget carried away and use these ideas to construct an endless hierarchy of ever\nmore dominant $H$-linear universal portfolios.\n"
    },
    {
        "paper_id": 1907.09753,
        "authors": "Olivier Gu\\'eant, Iuliia Manziuk, Jiang Pu",
        "title": "Accelerated Share Repurchase and other buyback programs: what neural\n  networks can bring",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When firms want to buy back their own shares, they have a choice between\nseveral alternatives. If they often carry out open market repurchase, they also\nincreasingly rely on banks through complex buyback contracts involving option\ncomponents, e.g. accelerated share repurchase contracts, VWAP-minus\nprofit-sharing contracts, etc. The entanglement between the execution problem\nand the option hedging problem makes the management of these contracts a\ndifficult task that should not boil down to simple Greek-based risk hedging,\ncontrary to what happens with classical books of options. In this paper, we\npropose a machine learning method to optimally manage several types of buyback\ncontract. In particular, we recover strategies similar to those obtained in the\nliterature with partial differential equation and recombinant tree methods and\nshow that our new method, which does not suffer from the curse of\ndimensionality, enables to address types of contract that could not be\naddressed with grid or tree methods.\n"
    },
    {
        "paper_id": 1907.09855,
        "authors": "Claudia G\\\"unther and Wolf-Peter Schill and Alexander Zerrahn",
        "title": "Prosumage of solar electricity: tariff design, capacity investments, and\n  power system effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze how tariff design incentivizes households to invest in residential\nphotovoltaic and battery systems, and explore selected power sector effects. To\nthis end, we apply an open-source power system model featuring prosumage agents\nto German 2030 scenarios. Results show that lower feed-in tariffs substantially\nreduce investments in photovoltaics, yet optimal battery sizing and\nself-generation are relatively robust. With increasing fixed parts of retail\ntariffs, optimal battery capacities and self-generation are smaller, and\nhouseholds contribute more to non-energy power sector costs. When choosing\ntariff designs, policy makers should not aim to (dis-)incentivize prosumage as\nsuch, but balance effects on renewable capacity expansion and system cost\ncontribution.\n"
    },
    {
        "paper_id": 1907.09857,
        "authors": "Uwe K\\\"uchler and Stefan Tappe",
        "title": "Bilateral Gamma distributions and processes in financial mathematics",
        "comments": "21 pages, 3 figures",
        "journal-ref": "Stochastic Processes and Their Applications 118(2):261-283, 2008",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a class of L\\'evy processes for modelling financial market\nfluctuations: Bilateral Gamma processes. Our starting point is to explore the\nproperties of bilateral Gamma distributions, and then we turn to their\nassociated L\\'evy processes. We treat exponential L\\'evy stock models with an\nunderlying bilateral Gamma process as well as term structure models driven by\nbilateral Gamma processes and apply our results to a set of real financial data\n(DAX 1996-1998).\n"
    },
    {
        "paper_id": 1907.09862,
        "authors": "Uwe K\\\"uchler and Stefan Tappe",
        "title": "Option pricing in bilateral Gamma stock models",
        "comments": "21 pages, 1 figure",
        "journal-ref": "Statistics and Decisions 27(4):281-307, 2009",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the framework of bilateral Gamma stock models we seek for adequate option\npricing measures, which have an economic interpretation and allow numerical\ncalculations of option prices. Our investigations encompass Esscher transforms,\nminimal entropy martingale measures, $p$-optimal martingale measures, bilateral\nEsscher transforms and the minimal martingale measure. We illustrate our theory\nby a numerical example.\n"
    },
    {
        "paper_id": 1907.09943,
        "authors": "Victor Amelkin, Rakesh Vohra",
        "title": "Yield Uncertainty and Strategic Formation of Supply Chain Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/net.22186",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How does supply uncertainty affect the structure of supply chain networks? To\nanswer this question we consider a setting where retailers and suppliers must\nestablish a costly relationship with each other prior to engaging in trade.\nSuppliers, with uncertain yield, announce wholesale prices, while retailers\nmust decide which suppliers to link to based on their wholesale prices.\nSubsequently, retailers compete with each other in Cournot fashion to sell the\nacquired supply to consumers. We find that in equilibrium retailers concentrate\ntheir links among too few suppliers, i.e., there is insufficient\ndiversification of the supply base. We find that either reduction of supply\nvariance or increase of mean supply, increases a supplier's profit. However,\nthese two ways of improving service have qualitatively different effects on\nwelfare: improvement of the expected supply by a supplier makes everyone better\noff, whereas improvement of supply variance lowers consumer surplus.\n"
    },
    {
        "paper_id": 1907.0999,
        "authors": "Mohamed Amine Lkabous",
        "title": "Poissonian occupation times of spectrally negative L\\'evy processes with\n  applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce the concept of \\emph{Poissonian occupation times}\nbelow level $0$ of spectrally negative L\\'evy processes. In this case,\noccupation time is accumulated only when the process is observed to be negative\nat arrival epochs of an independent Poisson process. Our results extend some\nwell known continuously observed quantities involving occupation times of\nspectrally negative L\\'evy processes. As an application, we establish a link\nbetween Poissonian occupation times and insurance risk models with Parisian\nimplementation delays.\n"
    },
    {
        "paper_id": 1907.09993,
        "authors": "Mohamed Amine Lkabous",
        "title": "A note on Parisian ruin under a hybrid observation scheme",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the concept of Parisian ruin under the hybrid\nobservation scheme model introduced by Li et al. \\cite{binetal2016}. Under this\nmodel, the process is observed at Poisson arrival times whenever the business\nis financially healthy and it is continuously observed when it goes below $0$.\nThe Parisian ruin is then declared when the process stays below zero for a\nconsecutive period of time greater than a fixed delay. We improve the result\noriginally obtained in \\cite{binetal2016} and we compute other fluctuation\nidentities. All identities are given in terms of second-generation scale\nfunctions.\n"
    },
    {
        "paper_id": 1907.10046,
        "authors": "Naftali Cohen, Tucker Balch, and Manuela Veloso",
        "title": "Trading via Image Classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The art of systematic financial trading evolved with an array of approaches,\nranging from simple strategies to complex algorithms all relying, primary, on\naspects of time-series analysis. Recently, after visiting the trading floor of\na leading financial institution, we noticed that traders always execute their\ntrade orders while observing images of financial time-series on their screens.\nIn this work, we built upon the success in image recognition and examine the\nvalue in transforming the traditional time-series analysis to that of image\nclassification. We create a large sample of financial time-series images\nencoded as candlestick (Box and Whisker) charts and label the samples following\nthree algebraically-defined binary trade strategies. Using the images, we train\nover a dozen machine-learning classification models and find that the\nalgorithms are very efficient in recovering the complicated, multiscale\nlabel-generating rules when the data is represented visually. We suggest that\nthe transformation of continuous numeric time-series classification problem to\na vision problem is useful for recovering signals typical of technical\nanalysis.\n"
    },
    {
        "paper_id": 1907.10052,
        "authors": "Saeed Nosratabadi, Amir Mosavi, Shahaboddin Shamshirband, Edmundas\n  Kazimieras Zavadskas, Andry Rakotonirainy and Kwok Wing Chau",
        "title": "Sustainable Business Models: A Review",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/su11061663",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of the sustainable business model describes the rationale of how\nan organization creates, delivers, and captures value, in economic, social,\ncultural, or other contexts, in a sustainable way. The process of sustainable\nbusiness model construction forms an innovative part of a business strategy.\nDifferent industries and businesses have utilized sustainable business models\nconcept to satisfy their economic, environmental, and social goals\nsimultaneously. However, the success, popularity, and progress of sustainable\nbusiness models in different application domains are not clear. To explore this\nissue, this research provides a comprehensive review of sustainable business\nmodels literature in various application areas. Notable sustainable business\nmodels are identified and further classified in fourteen unique categories, and\nin every category, the progress -- either failure or success -- has been\nreviewed, and the research gaps are discussed. Taxonomy of the applications\nincludes innovation, management and marketing, entrepreneurship, energy,\nfashion, healthcare, agri-food, supply chain management, circular economy,\ndeveloping countries, engineering, construction and real estate, mobility and\ntransportation, and hospitality. The key contribution of this study is that it\nprovides an insight into the state of the art of sustainable business models in\nthe various application areas and future research directions. This paper\nconcludes that popularity and the success rate of sustainable business models\nin all application domains have been increased along with the increasing use of\nadvanced technologies.\n"
    },
    {
        "paper_id": 1907.10152,
        "authors": "Michael Weylandt and Yu Han and Katherine B. Ensor",
        "title": "Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\n  Futures Market Realized Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial markets for Liquified Natural Gas (LNG) are an important and\nrapidly-growing segment of commodities markets. Like other commodities markets,\nthere is an inherent spatial structure to LNG markets, with different price\ndynamics for different points of delivery hubs. Certain hubs support highly\nliquid markets, allowing efficient and robust price discovery, while others are\nhighly illiquid, limiting the effectiveness of standard risk management\ntechniques. We propose a joint modeling strategy, which uses high-frequency\ninformation from thickly-traded hubs to improve volatility estimation and risk\nmanagement at thinly traded hubs. The resulting model has superior in- and\nout-of-sample predictive performance, particularly for several commonly used\nrisk management metrics, demonstrating that joint modeling is indeed possible\nand useful. To improve estimation, a Bayesian estimation strategy is employed\nand data-driven weakly informative priors are suggested. Our model is robust to\nsparse data and can be effectively used in any market with similar irregular\npatterns of data availability.\n"
    },
    {
        "paper_id": 1907.10306,
        "authors": "Petr Koldanov",
        "title": "Testing new property of elliptical model for stock returns distribution",
        "comments": "15 pages, 2 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wide class of elliptically contoured distributions is a popular model of\nstock returns distribution. However the important question of adequacy of the\nmodel is open. There are some results which reject and approve such model. Such\nresults are obtained by testing some properties of elliptical model for each\npair of stocks from some markets. New property of equality of $\\tau$ Kendall\ncorrelation coefficient and probability of sign coincidence for any pair of\nrandom variables with elliptically contoured distribution is proved in the\npaper. Distribution free statistical tests for testing this property for any\npair of stocks are constructed. Holm multiple hypotheses testing procedure\nbased on the individual tests is constructed and applied for stock markets data\nfor the concrete year. New procedure of testing the elliptical model for stock\nreturns distribution for all years of observation for some period is proposed.\nThe procedure is applied for the stock markets data of China, USA, Great\nBritain and Germany for the period from 2003 to 2014. It is shown that for USA,\nGreat Britain and Germany stock markets the hypothesis of elliptical model of\nstock returns distribution could be accepted but for Chinese stock market is\nrejected for some cases.\n"
    },
    {
        "paper_id": 1907.10407,
        "authors": "Joseph Attia",
        "title": "Evaluating the Effectiveness of Common Technical Trading Models",
        "comments": "43 pages. arXiv admin note: text overlap with arXiv:1902.00786",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  How effective are the most common trading models? The answer may help\ninvestors realize upsides to using each model, act as a segue for investors\ninto more complex financial analysis and machine learning, and to increase\nfinancial literacy amongst students. Creating original versions of popular\nmodels, like linear regression, K-Nearest Neighbor, and moving average\ncrossovers, we can test how each model performs on the most popular stocks and\nlargest indexes. With the results for each, we can compare the models, and\nunderstand which model reliably increases performance. The trials showed that\nwhile all three models reduced losses on stocks with strong overall downward\ntrends, the two machine learning models did not work as well to increase\nprofits. Moving averages crossovers outperformed a continuous investment every\ntime, although did result in a more volatile investment as well. Furthermore,\nonce finished creating the program that implements moving average crossover,\nwhat are the optimal periods to use? A massive test consisting of 169,880\ntrials, showed the best periods to use to increase investment performance\n(5,10) and to decrease volatility (33,44). In addition, the data showed\nnumerous trends such as a smaller short SMA period is accompanied by higher\nperformance. Plotting volatility against performance shows that the high risk,\nhigh reward saying holds true and shows that for investments, as the volatility\nincreases so does its performance.\n"
    },
    {
        "paper_id": 1907.10578,
        "authors": "Jian Liang and Zhe Xu and Peter Li",
        "title": "Deep Learning-Based Least Square Forward-Backward Stochastic\n  Differential Equation Solver for High-Dimensional Derivative Pricing",
        "comments": "22 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new forward-backward stochastic differential equation solver for\nhigh-dimensional derivatives pricing problems by combining deep learning solver\nwith least square regression technique widely used in the least square Monte\nCarlo method for the valuation of American options. Our numerical experiments\ndemonstrate the efficiency and accuracy of our least square backward deep\nneural network solver and its capability to provide accurate prices for complex\nearly exercise derivatives such as callable yield notes. Our method can serve\nas a generic numerical solver for pricing derivatives across various asset\ngroups, in particular, as an efficient means for pricing high-dimensional\nderivatives with early exercises features.\n"
    },
    {
        "paper_id": 1907.1072,
        "authors": "Michael Brolley, Marius Zoican",
        "title": "Liquid Speed: On-Demand Fast Trading at Distributed Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exchanges acquire excess processing capacity to accommodate trading activity\nsurges associated with zero-sum high-frequency trader (HFT) \"duels.\" The idle\ncapacity's opportunity cost is an externality of low-latency trading. We build\na model of decentralized exchanges (DEX) with flexible capacity. On DEX, HFTs\nacquire speed in real-time from peer-to-peer networks. The price of speed\nsurges during activity bursts, as HFTs simultaneously race to market. Relative\nto centralized exchanges, HFTs acquire more speed on DEX, but for shorter\ntimespans. Low-latency \"sprints\" speed up price discovery without harming\nliquidity. Overall, speed rents decrease and fewer resources are locked-in to\nsupport zero-sum HFT trades.\n"
    },
    {
        "paper_id": 1907.11053,
        "authors": "Bastien Baldacci, Dylan Possama\\\"i, Mathieu Rosenbaum",
        "title": "Optimal make take fees in a multi market maker environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the recent literature on make take fees policies, we consider an\nexchange wishing to set a suitable contract with several market makers in order\nto improve trading quality on its platform. To do so, we use a principal-agent\napproach, where the agents (the market makers) optimise their quotes in a Nash\nequilibrium fashion, providing best response to the contract proposed by the\nprincipal (the exchange). This contract aims at attracting liquidity on the\nplatform. This is because the wealth of the exchange depends on the arrival of\nmarket orders, which is driven by the spread of market makers. We compute the\noptimal contract in quasi explicit form and also derive the optimal spread\npolicies for the market makers. Several new phenomena appears in this multi\nmarket maker setting. In particular we show that it is not necessarily optimal\nto have a large number of market makers in the presence of a contracting\nscheme.\n"
    },
    {
        "paper_id": 1907.11054,
        "authors": "Andrea Berdondini",
        "title": "Resolution of the St. Petersburg paradox using Von Mises axiom of\n  randomness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we will propose a completely new point of view for solving\none of the most important paradoxes concerning game theory. The solution\ndevelop shifts the focus from the result to the strategy s ability to operate\nin a cognitive way by exploiting useful information about the system. In order\nto determine from a mathematical point of view if a strategy is cognitive, we\nuse Von Mises' axiom of randomness. Based on this axiom, the knowledge of\nuseful information consequently generates results that cannot be reproduced\nrandomly. Useful information in this case may be seen as a significant datum\nfor the recipient, for their present or future decision-making process.\nFinally, by resolving the paradox from this new point of view, we will\ndemonstrate that an expected gain that tends toward infinity is not always a\nconsequence of a cognitive and non-random strategy. Therefore, this result\nleads us to define a hierarchy of values in decision-making, where the\ncognitive aspect, whose statistical consequence is a divergence from random\nbehaviour, turns out to be more important than the expected gain.\n"
    },
    {
        "paper_id": 1907.11057,
        "authors": "Andreas Veneris, Andreas Park",
        "title": "Special Drawing Rights in a New Decentralized Century",
        "comments": "4 pages, IMF Georgetown",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unfulfilled expectations from macro-economic initiatives during the Great\nRecession and the massive shift into globalization echo today with political\nupheaval, anti-establishment propaganda, and looming trade/currency wars that\nthreaten domestic and international value chains. Once stable entities like the\nEU now look fragile and political instability in the US presents unprecedented\nchallenges to an International Monetary System (IMS) that predominantly relies\non the USD and EUR as reserve currencies. In this environment, it is critical\nfor an international organization mandated to ensure stability to plan and act\nahead. This paper argues that Decentralized Ledger-based technology (DLT) is\nkey for the International Monetary Fund (IMF) to mitigate some of those risks,\npromote stability and safeguard world prosperity. Over the last two years, DLT\nhas made headline news globally and created a worldwide excitement not seen\nsince the internet entered the mainstream. The rapid adoption and open-to-all\nphilosophy of DLT has already redefined global socioeconomics, promises to\nshake up the world of commerce/finance and challenges the workings of central\ngovernments/regulators. This paper examines DLT core premises and proposes a\ntwo-step approach for the IMF to expand Special Drawing Rights (SDR) into that\nsphere so as to become the originally envisioned numeraire and reserve currency\nfor cross-border transactions in this new decentralized century.\n"
    },
    {
        "paper_id": 1907.11162,
        "authors": "Nassim Nicholas Taleb",
        "title": "On the Statistical Differences between Binary Forecasts and Real World\n  Payoffs",
        "comments": "Minor revisions for the accepted version",
        "journal-ref": "International Journal of Forecasting, April 4, 2020",
        "doi": "10.1016/j.ijforecast.2019.12.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What do binary (or probabilistic) forecasting abilities have to do with\noverall performance? We map the difference between (univariate) binary\npredictions, bets and \"beliefs\" (expressed as a specific \"event\" will\nhappen/will not happen) and real-world continuous payoffs (numerical benefits\nor harm from an event) and show the effect of their conflation and\nmischaracterization in the decision-science literature. We also examine the\ndifferences under thin and fat tails. The effects are:\n  A- Spuriousness of many psychological results particularly those documenting\nthat humans overestimate tail probabilities and rare events, or that they\noverreact to fears of market crashes, ecological calamities, etc. Many\nperceived \"biases\" are just mischaracterizations by psychologists. There is\nalso a misuse of Hayekian arguments in promoting prediction markets.\n  We quantify such conflations with a metric for \"pseudo-overestimation\".\n  B- Being a \"good forecaster\" in binary space doesn't lead to having a good\nactual performance}, and vice versa, especially under nonlinearities. A binary\nforecasting record is likely to be a reverse indicator under some classes of\ndistributions. Deeper uncertainty or more complicated and realistic probability\ndistribution worsen the conflation .\n  C- Machine Learning: Some nonlinear payoff functions, while not lending\nthemselves to verbalistic expressions and \"forecasts\", are well captured by ML\nor expressed in option contracts.\n  D- Fattailedness: The difference is exacerbated in the power law classes of\nprobability distributions.\n"
    },
    {
        "paper_id": 1907.11224,
        "authors": "Milad Mousavian H. (1), Hamed Shakouri G. (1), Alinaghi Mashayekhi\n  (2), Aliyeh Kazemi (3) ((1) School of Industrial and Systems Engineering,\n  University of Tehran, Iran (2) Graduate School of Management and Economics,\n  Sharif University of Technology, Iran (3) Department of Industrial\n  Management, University of Tehran, Iran)",
        "title": "Does the short-term boost of renewable energies guarantee their stable\n  long-term growth? Assessment of the dynamics of feed-in tariff policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Feed in tariff (FiT) is one of the most efficient ways that many governments\nthroughout the world use to stimulate investment in renewable energies (REs)\ntechnology. For governments, financial management of the policy is very\nchallenging as that it needs a considerable amount of budget to support RE\nproducers during the long remuneration period. In this paper, we illuminate\nthat the early growth of REs capacity could be a temporary boost and the system\nelements would backlash the policy if financial circumstances are not handled\nwell. To show this, we chose Iran as the case, which is in the infancy period\nof FiT implementation. Iran started the implementation of FiT policy in 2015\naiming to achieve 5 GW of renewable capacity until 2021. Analyses show that the\nprobable financial crisis will not only lead to inefficient REs development\nafter the target time (2021), but may also cause the existing plants to fail.\nSocial tolerance for paying REs tax and potential investors trust emanated from\nbudget related mechanisms are taken into consideration in the system dynamics\nmodel developed in this research to reflect those financial effects, which have\nrarely been considered in the previous researches. To prevent the financial\ncrisis of the FiT funding and to maintain the stable growth in long term, three\npolicy scenarios are analyzed: continuation of the current program with higher\nFiT rates, adjusting the FiT rates based on the budget status, and adjusting\nthe tax on electricity consumption for the development of REs based on the\nbudget status. The results demonstrate that adjusting the tax on electricity\nconsumption for the development of REs based on budget status leads to the best\npolicy result for a desired installed capacity development without any negative\nsocial effects and financial crises.\n"
    },
    {
        "paper_id": 1907.11378,
        "authors": "Bingyan Han and Hoi Ying Wong",
        "title": "Time-inconsistency with rough volatility",
        "comments": "An earlier version of this paper was circulated and cited under the\n  title \"Time-consistent feedback strategies with Volterra processes\"",
        "journal-ref": "SIAM Journal on Financial Mathematics, 2021",
        "doi": "10.1137/20m136654x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider equilibrium strategies under Volterra processes\nand time-inconsistent preferences embracing mean-variance portfolio selection\n(MVP). Using a functional It\\^o calculus approach, we overcome the\nnon-Markovian and non-semimartingale difficulty in Volterra processes. The\nequilibrium strategy is then characterized by an extended path-dependent\nHamilton-Jacobi-Bellman equation system under a game-theoretic framework. A\nverification theorem is provided. We derive explicit solutions to three\nproblems, including MVP with constant risk aversion, MVP for log returns, and a\nmean-variance objective with a linear controlled Volterra process. We also\nthoroughly examine the effect of volatility roughness on equilibrium\nstrategies. Numerical experiments demonstrate that trading rules with rough\nvolatility outperform the classic counterparts.\n"
    },
    {
        "paper_id": 1907.11424,
        "authors": "David M. Kreps, Walter Schachermayer",
        "title": "Convergence of Optimal Expected Utility for a Sequence of Discrete-Time\n  Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine Kreps' (2019) conjecture that optimal expected utility in the\nclassic Black--Scholes--Merton (BSM) economy is the limit of optimal expected\nutility for a sequence of discrete-time economies that \"approach\" the BSM\neconomy in a natural sense: The $n$th discrete-time economy is generated by a\nscaled $n$-step random walk, based on an unscaled random variable $\\zeta$ with\nmean zero, variance one, and bounded support. We confirm Kreps' conjecture if\nthe consumer's utility function $U$ has asymptotic elasticity strictly less\nthan one, and we provide a counterexample to the conjecture for a utility\nfunction $U$ with asymptotic elasticity equal to 1, for $\\zeta$ such that\n$E[\\zeta^3] > 0.$\n"
    },
    {
        "paper_id": 1907.11622,
        "authors": "Chulwook Park",
        "title": "Network and Agent Dynamics with Evolving Protection against Systemic\n  Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamics of protection processes has been a fundamental challenge in\nsystemic risk analysis. The conceptual principle and methodological techniques\nbehind the mechanisms involved [in such dynamics] have been harder to grasp\nthan researchers understood them to be. In this paper, we show how to construct\na large variety of behaviors by applying a simple algorithm to networked\nagents, which could, conceivably, offer a straightforward way out of the\ncomplexity. The model starts with the probability that systemic risk spreads.\nEven in a very random social structure, the propagation of risk is guaranteed\nby an arbitrary network property of a set of elements. Despite intensive\nsystemic risk, the potential of the absence of failure could also be driven\nwhen there has been a strong investment in protection through a heuristically\nevolved protection level. It is very interesting to discover that many\napplications are still seeking the mechanisms through which networked\nindividuals build many of these protection process or mechanisms based on\nfitness due to evolutionary drift. Our implementation still needs to be\npolished against what happens in the real world, but in general, the approach\ncould be useful for researchers and those who need to use protection dynamics\nto guard against systemic risk under intrinsic randomness in artificial\ncircumstances.\n"
    },
    {
        "paper_id": 1907.11634,
        "authors": "Ke Ren and Avinash Malik",
        "title": "Recommendation Engine for Lower Interest Borrowing on Peer to Peer\n  Lending (P2PL) Platform",
        "comments": "Accepted in Web intelligence 2019, this is a long version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online Peer to Peer Lending (P2PL) systems connect lenders and borrowers\ndirectly, thereby making it convenient to borrow and lend money without\nintermediaries such as banks. Many recommendation systems have been developed\nfor lenders to achieve higher interest rates and avoid defaulting loans.\nHowever, there has not been much research in developing recommendation systems\nto help borrowers make wise decisions. On P2PL platforms, borrowers can either\napply for bidding loans, where the interest rate is determined by lenders\nbidding on a loan or traditional loans where the P2PL platform determines the\ninterest rate. Different borrower grades -- determining the credit worthiness\nof borrowers get different interest rates via these two mechanisms. Hence, it\nis essential to determine which type of loans borrowers should apply for. In\nthis paper, we build a recommendation system that recommends to any new\nborrower the type of loan they should apply for. Using our recommendation\nsystem, any borrower can achieve lowered interest rates with a higher\nlikelihood of getting funded.\n"
    },
    {
        "paper_id": 1907.11718,
        "authors": "Haoran Wang",
        "title": "Large scale continuous-time mean-variance portfolio allocation via\n  reinforcement learning",
        "comments": "15 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1904.11392",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to solve large scale Markowitz mean-variance (MV) portfolio\nallocation problem using reinforcement learning (RL). By adopting the recently\ndeveloped continuous-time exploratory control framework, we formulate the\nexploratory MV problem in high dimensions. We further show the optimality of a\nmultivariate Gaussian feedback policy, with time-decaying variance, in trading\noff exploration and exploitation. Based on a provable policy improvement\ntheorem, we devise a scalable and data-efficient RL algorithm and conduct large\nscale empirical tests using data from the S&P 500 stocks. We found that our\nmethod consistently achieves over 10% annualized returns and it outperforms\neconometric methods and the deep RL method by large margins, for both long and\nmedium terms of investment with monthly and daily trading.\n"
    },
    {
        "paper_id": 1907.11719,
        "authors": "M. S. S. Rosa, P. R. B. Lustosa",
        "title": "Market and Long Term Accounting Operational Performance",
        "comments": "13 pages, in Portuguese",
        "journal-ref": "BASE, 11(1), Jan-Mar 2014, p. 34-46",
        "doi": "10.4013/base.2014.111.03",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the value relevance literature, this study verifies whether the\nmarketplace differentiates companies of high, medium, and low long-term\noperational performance, measured by accounting information on profitability,\nsales variation and indebtedness. The data comprises the Corporate Financial\nStatements disclosed during the period from 1996 to 2009 and stock prices of\ncompanies listed on the Sao Paulo Stock Exchange and Commodities and Futures\nExchange - BM&FBOVESPA. The final sample is composed of 142 non-financial\ncompanies. Five year mobile windows were used, which resulted in ten five-year\nperiods. After checking each company indices, the accounting variables were\nunified in an Index Performance Summary to synthesize the final performance for\neach five-year period, which allowed segregation in operational performance\nlevels. Multiple regressions were performed using panel data techniques, fixed\neffects model and dummies variables, and then hypothesis tests were made.\nRegarding the explanatory power of each individual variable, the results show\nthat not all behaviors are according to the research hypothesis and that the\nBrazilian stock market differentiates companies of high and low long-term\noperational performance. This distinction is not fully perceived between\ncompanies of high and medium operational performance.\n"
    },
    {
        "paper_id": 1907.11855,
        "authors": "Wentao Hu",
        "title": "SlideVaR: a risk measure with variable risk attitudes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To find a trade-off between profitability and prudence, financial\npractitioners need to choose appropriate risk measures. Two key points are:\nFirstly, investors' risk attitudes under uncertainty conditions should be an\nimportant reference for risk measures. Secondly, risk attitudes are not\nabsolute. For different market performance, investors have different risk\nattitudes. We proposed a new risk measure named SlideVaR which sufficiently\nreflects the different subjective attitudes of investors and the impact of\nmarket changes on investors' attitudes. We proposed the concept of risk-tail\nregion and risk-tail sub-additivity and proved that SlideVaR satisfies several\nimportant mathematical properties. Moreover, SlideVaR has a simple and\nintuitive form of expression for practical application. Several simulate and\nempirical computations show that SlideVaR has obvious advantages in markets\nwhere the state changes frequently.\n"
    },
    {
        "paper_id": 1907.11984,
        "authors": "Naser Rostamni and Tarik A. Rashid",
        "title": "Investigating the effect of competitiveness power in estimating the\n  average weighted price in electricity market",
        "comments": "11 pages",
        "journal-ref": "The Electricity Journal, 2019",
        "doi": "10.1016/j.tej.2019.106628",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper evaluates the impact of the power extent on price in the\nelectricity market. The competitiveness extent of the electricity market during\nspecific times in a day is considered to achieve this. Then, the effect of\ncompetitiveness extent on the forecasting precision of the daily power price is\nassessed. A price forecasting model based on multi-layer perception via back\npropagation with the Levenberg-Marquardt mechanism is used. The Residual Supply\nIndex (RSI) and other variables that affect prices are used as inputs to the\nmodel to evaluate the market competitiveness. The results show that using\nmarket power indices as inputs helps to increase forecasting accuracy. Thus,\nthe competitiveness extent of the market power in different daily time periods\nis a notable variable in price formation. Moreover, market players cannot\nignore the explanatory power of market power in price forecasting. In this\nresearch, the real data of the electricity market from 2013 is used and the\nmain source of data is the Grid Management Company in Iran.\n"
    },
    {
        "paper_id": 1907.12025,
        "authors": "Kyungsub Lee, Byoung Ki Seo",
        "title": "Marked Hawkes process modeling of price dynamics and volatility\n  estimation",
        "comments": null,
        "journal-ref": "40, pp.174-220, Journal of Empirical Finance, 2017",
        "doi": "10.1016/j.jempfin.2016.08.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A simple Hawkes model have been developed for the price tick structure\ndynamics incorporating market microstructure noise and trade clustering. In\nthis paper, the model is extended with random mark to deal with more realistic\nprice tick structures of equities. We examine the impact of jump in price\ndynamics to the future movements and dependency between the jump sizes and\nground intensities. We also derive the volatility formula based on stochastic\nand statistical methods and compare with realized volatility in simulation and\nempirical studies. The marked Hawkes model is useful to estimate the intraday\nvolatility similarly in the case of simple Hawkes model.\n"
    },
    {
        "paper_id": 1907.12093,
        "authors": "Shan Huang",
        "title": "Taxable Stock Trading with Deep Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose stock trading based on the average tax basis.\nRecall that when selling stocks, capital gain should be taxed while capital\nloss can earn certain tax rebate. We learn the optimal trading strategies with\nand without considering taxes by reinforcement learning. The result shows that\ntax ignorance could induce more than 62% loss on the average portfolio returns,\nimplying that taxes should be embedded in the environment of continuous stock\ntrading on AI platforms.\n"
    },
    {
        "paper_id": 1907.12179,
        "authors": "Fatima Zahra Azayite, Said Achchab",
        "title": "A hybrid neural network model based on improved PSO and SA for\n  bankruptcy prediction",
        "comments": "13 pages",
        "journal-ref": "International Journal of Computer Science Issues, Vol 16, Issue 1,\n  January 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting firm's failure is one of the most interesting subjects for\ninvestors and decision makers. In this paper, a bankruptcy prediction model is\nproposed based on Artificial Neural networks (ANN). Taking into consideration\nthat the choice of variables to discriminate between bankrupt and non-bankrupt\nfirms influences significantly the model's accuracy and considering the problem\nof local minima, we propose a hybrid ANN based on variables selection\ntechniques. Moreover, we evolve the convergence of Particle Swarm Optimization\n(PSO) by proposing a training algorithm based on an improved PSO and Simulated\nAnnealing. A comparative performance study is reported, and the proposed hybrid\nmodel shows a high performance and convergence in the context of missing data.\n"
    },
    {
        "paper_id": 1907.12289,
        "authors": "Tomoya Mori, Tony E. Smith, Wen-Tai Hsu",
        "title": "Cities and space: Common power laws and spatial fractal structures",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  City size distributions are known to be well approximated by power laws\nacross a wide range of countries. But such distributions are also meaningful at\nother spatial scales, such as within certain regions of a country. Using data\nfrom China, France, Germany, India, Japan, and the US, we first document that\nlarge cities are significantly more spaced out than would be expected by chance\nalone. We next construct spatial hierarchies for countries by first\npartitioning geographic space using a given number of their largest cities as\ncell centers, and then continuing this partitioning procedure within each cell\nrecursively. We find that city size distributions in different parts of these\nspatial hierarchies exhibit power laws that are again far more similar than\nwould be expected by chance alone -- suggesting the existence of a spatial\nfractal structure.\n"
    },
    {
        "paper_id": 1907.12406,
        "authors": "Mario Coccia",
        "title": "Killer Technologies: the destructive creation in the technical change",
        "comments": "37 pages, 7 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Killer technology is a radical innovation, based on new products and/or\nprocesses, that with high technical and/or economic performance destroys the\nusage value of established techniques previously sold and used. Killer\ntechnology is a new concept in economics of innovation that may be useful for\nbringing a new perspective to explain and generalize the behavior and\ncharacteristics of innovations that generate a destructive creation for\nsustaining technical change. To explore the behavior of killer technologies, a\nsimple model is proposed to analyze and predict how killer technologies destroy\nand substitute established technologies. Empirical evidence of this theoretical\nframework is based on historical data on the evolution of some example\ntechnologies. Theoretical framework and empirical evidence hint at general\nproperties of the behavior of killer technologies to explain corporate,\nindustrial, economic and social change and to support best practices for\ntechnology management of firms and innovation policy of nations. Overall, then,\nthe proposed theoretical framework can lay a foundation for the development of\nmore sophisticated concepts to explain the behavior of vital technologies that\ngenerate technological and industrial change in society.\n"
    },
    {
        "paper_id": 1907.12433,
        "authors": "Bastien Baldacci, Philippe Bergault, Olivier Gu\\'eant",
        "title": "Algorithmic market making for options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we tackle the problem of a market maker in charge of a book\nof options on a single liquid underlying asset. By using an approximation of\nthe portfolio in terms of its vega, we show that the seemingly high-dimensional\nstochastic optimal control problem of an option market maker is in fact\ntractable. More precisely, when volatility is modeled using a classical\nstochastic volatility model -- e.g. the Heston model -- the problem faced by an\noption market maker is characterized by a low-dimensional functional equation\nthat can be solved numerically using a Euler scheme along with interpolation\ntechniques, even for large portfolios. In order to illustrate our findings,\nnumerical examples are provided.\n"
    },
    {
        "paper_id": 1907.126,
        "authors": "Abootaleb Shirvani, Svetlozar T. Rachev, Frank J. Fabozzi",
        "title": "Multiple Subordinated Modeling of Asset Returns",
        "comments": "37 pages, 3 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Subordination is an often used stochastic process in modeling asset prices.\nSubordinated Levy price processes and local volatility price processes are now\nthe main tools in modern dynamic asset pricing theory. In this paper, we\nintroduce the theory of multiple internally embedded financial time-clocks\nmotivated by behavioral finance. To be consistent with dynamic asset pricing\ntheory and option pricing, as suggested by behavioral finance, the investors'\nview is considered by introducing an intrinsic time process which we refer to\nas a behavioral subordinator. The process is subordinated to the Brownian\nmotion process in the well-known log-normal model, resulting in a new log-price\nprocess. The number of embedded subordinations results in a new parameter that\nmust be estimated and this parameter is as important as the mean and variance\nof asset returns. We describe new distributions, demonstrating how they can be\napplied to modeling the tail behavior of stock market returns. We apply the\nproposed models to modeling S&P 500 returns, treating the CBOE Volatility Index\nas intrinsic time change and the CBOE Volatility-of-Volatility Index as the\nvolatility subordinator. We find that these volatility indexes are not proper\ntime-change subordinators in modeling the returns of the S&P 500.\n"
    },
    {
        "paper_id": 1907.12615,
        "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers",
        "title": "A procedure for loss-optimising default definitions across simulated\n  credit risk scenarios",
        "comments": "This paper is an old and deprecated version of arXiv:2009.11064",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new procedure is presented for the objective comparison and evaluation of\ndefault definitions. This allows the lender to find a default threshold at\nwhich the financial loss of a loan portfolio is minimised, in accordance with\nBasel II. Alternative delinquency measures, other than simply measuring\npayments in arrears, can also be evaluated using this optimisation procedure.\nFurthermore, a simulation study is performed in testing the procedure from\n`first principles' across a wide range of credit risk scenarios. Specifically,\nthree probabilistic techniques are used to generate cash flows, while the\nparameters of each are varied, as part of the simulation study. The results\nshow that loss minima can exist for a select range of credit risk profiles,\nwhich suggests that the loss optimisation of default thresholds can become a\nviable practice. The default decision is therefore framed anew as an\noptimisation problem in choosing a default threshold that is neither too early\nnor too late in loan life. These results also challenges current practices\nwherein default is pragmatically defined as `90 days past due', with little\nobjective evidence for its overall suitability or financial impact, at least\nbeyond flawed roll rate analyses or a regulator's decree.\n"
    },
    {
        "paper_id": 1907.12806,
        "authors": "Ilaria Nava, Davide Cuccio, Lorenzo Giada and Claudio Nordio",
        "title": "A Simple Factoring Pricing Model",
        "comments": "11 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a simplified setting, we show how to price invoice non-recourse factoring\ntaking into account not only the credit worthiness of the debtor but also the\nassignor's one, together with the default correlation between the two. Indeed,\nthe possible default of the assignor might impact the payoff by means of the\nbankruptcy revocatory, especially in case of undisclosed factoring.\n"
    },
    {
        "paper_id": 1907.12922,
        "authors": "Elisa Alos, Fabio Antonelli, Alessandro Ramponi, Sergio Scarlatti",
        "title": "CVA and vulnerable options in stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we want to provide a general principle to evaluate the CVA\n(Credit Value Adjustment) for a vulnerable option, that is an option subject to\nsome default event, concerning the solvability of the issuer. CVA is needed to\nevaluate correctly the contract and it is particularly important in presence of\nWWR (Wrong Way Risk), when a credit deterioration determines an increase of the\nclaim's price. In particular, we are interested in evaluating the CVA in\nstochastic volatility models for the underlying's price (which often fit quite\nwell the market's prices) when admitting correlation with the default event. By\ncunningly using Ito's calculus, we provide a general representation formula\napplicable to some popular models such as SABR, Hull \\& White and Heston, which\nexplicitly shows the correction in CVA due to the processes correlation. Later,\nwe specialize this formula and construct its approximation for the three\nselected models. Lastly, we run a numerical study to test the formula's\naccuracy, comparing our results with Monte Carlo simulations.\n"
    },
    {
        "paper_id": 1907.13296,
        "authors": "Yuan Cheng, Yanbo Xue, Meng Chang",
        "title": "Career Choice as an Extended Spatial Evolutionary Public Goods Game",
        "comments": "17 pages, 7 figures, submitted to Chaos, Solitons & Fractals,\n  Elsevier, Career Science Lab",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2020.109856",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an extended spatial evolutionary public goods game (SEPGG) model\nto study the dynamics of individual career choice and the corresponding social\noutput. Based on the social value orientation theory, we categorized two\nclasses of work, namely the public work if it serves public interests, and the\nprivate work if it serves personal interests. Under the context of SEPGG,\nchoosing public work is to cooperate and choosing private work is to defect. We\nthen investigate the effects of employee productivity, human capital and\nexternal subsidies on individual career choices of the two work types, as well\nas the overall social welfare. From simulation results, we found that when\nemployee productivity of public work is low, people are more willing to enter\nthe private sector. Although this will make both the effort level and human\ncapital of individuals doing private work higher than those engaging in public\nwork, the total outcome of the private sector is still lower than that of the\npublic sector provided a low level of public subsidies. When the employee\nproductivity is higher for public work, a certain amount of subsidy can greatly\nimprove system output. On the contrary, when the employee productivity of\npublic work is low, provisions of subsidy to the public sector can result in a\ndecline in social output.\n"
    },
    {
        "paper_id": 1908.00054,
        "authors": "Alvaro Cartea, Ryan Donnelly, Sebastian Jaimungal",
        "title": "Hedging Non-Tradable Risks with Transaction Costs and Price Impact",
        "comments": "Originally posted to SSRN April 27, 2018. Forthcoming in Mathematical\n  Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A risk-averse agent hedges her exposure to a non-tradable risk factor $U$\nusing a correlated traded asset $S$ and accounts for the impact of her trades\non both factors. The effect of the agent's trades on $U$ is referred to as\ncross-impact. By solving the agent's stochastic control problem, we obtain a\nclosed-form expression for the optimal strategy when the agent holds a linear\nposition in $U$. When the exposure to the non-tradable risk factor $\\psi(U_T)$\nis non-linear, we provide an approximation to the optimal strategy in\nclosed-form, and prove that the value function is correctly approximated by\nthis strategy when cross-impact and risk-aversion are small. We further prove\nthat when $\\psi(U_T)$ is non-linear, the approximate optimal strategy can be\nwritten in terms of the optimal strategy for a linear exposure with the size of\nthe position changing dynamically according to the exposure's \"Delta\" under a\nparticular probability measure.\n"
    },
    {
        "paper_id": 1908.00141,
        "authors": "Sven Serneels",
        "title": "Projection pursuit based generalized betas accounting for higher order\n  co-moment effects in financial market analysis",
        "comments": null,
        "journal-ref": "In: JSM Proceedings, Business and Economic Statistics Section,\n  2019. Alexandria, VA: American Statistical Association. 3009-3035",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Betas are possibly the most frequently applied tool to analyze how securities\nrelate to the market. While in very widespread use, betas only express dynamics\nderived from second moment statistics. Financial returns data often deviate\nfrom normal assumptions in the sense that they have significant third and\nfourth order moments and contain outliers. This paper targets to introduce a\nway to calculate generalized betas that also account for higher order moment\neffects, while maintaining the conceptual simplicity and interpretability of\nbetas. Thereunto, the co-moment analysis projection index (CAPI) is introduced.\nWhen applied as a projection index in the projection pursuit (PP) framework,\ngeneralized betas are obtained as the directions optimizing the CAPI objective.\nA version of CAPI based on trimmed means is introduced as well, which is more\nstable in the presence of outliers. Simulation results underpin the statistical\nproperties of all projections and a small, yet highly illustrative example is\npresented.\n"
    },
    {
        "paper_id": 1908.00216,
        "authors": "Olga Scrivner, Thuy Nguyen, Kosali Simon, Esm\\'e Middaugh, Bledi\n  Taska, Katy B\\\"orner",
        "title": "Hiring in the substance use disorder treatment related sector during the\n  first five years of Medicaid expansion",
        "comments": "7 figures, 20 pages",
        "journal-ref": "PlosOne, January 30, 2020",
        "doi": "10.1371/journal.pone.0228394",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Effective treatment strategies exist for substance use disorder (SUD),\nhowever severe hurdles remain in ensuring adequacy of the SUD treatment (SUDT)\nworkforce as well as improving SUDT affordability, access and stigma. Although\nevidence shows recent increases in SUD medication access from expanding\nMedicaid availability under the Affordable Care Act, it is yet unknown whether\nthese policies also led to a growth in the changes in the nature of hiring in\nSUDT related workforce, partly due to poor data availability. Our study uses\nnovel data to shed light on recent trends in a fast-evolving and\npolicy-relevant labor market, and contributes to understanding the current SUDT\nrelated workforce and the effect of Medicaid expansion on hiring attempts in\nthis sector. We examine attempts over 2010-2018 at hiring in the SUDT and\nrelated behavioral health sector as background for estimating the causal effect\nof the 2014-and-beyond state Medicaid expansion on these outcomes through\n\"difference-in-difference\" econometric models. We use Burning Glass\nTechnologies (BGT) data covering virtually all U.S. job postings by employers.\nNationally, we find little growth in the sector's hiring attempts in 2010-2018\nrelative to the rest of the economy or to health care as a whole. However, this\nmasks diverging trends in subsectors, which saw reduction in hospital based\nhiring attempts, increases towards outpatient facilities, and changes in\noccupational hiring demand shifting from medical personnel towards counselors\nand social workers. Although Medicaid expansion did not lead to any\nstatistically significant or meaningful change in overall hiring attempts,\nthere was a shift in the hiring landscape.\n"
    },
    {
        "paper_id": 1908.00257,
        "authors": "L. Ponta and A. Carbone",
        "title": "Quantifying horizon dependence of asset prices: a cluster entropy\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market dynamic is quantified in terms of the entropy $S(\\tau,n)$ of the\nclusters formed by the intersections between the series of the prices $p_t$ and\nthe moving average $\\widetilde{p}_{t,n}$. The entropy $S(\\tau,n)$ is defined\naccording to Shannon as $\\sum P(\\tau,n)\\log P(\\tau,n),$ with $P(\\tau,n)$ the\nprobability for the cluster to occur with duration $\\tau$. \\par The\ninvestigation is performed on high-frequency data of the Nasdaq Composite, Dow\nJones Industrial Avg and Standard \\& Poor 500 indexes downloaded from the\nBloomberg terminal. The cluster entropy $S(\\tau,n)$ is analysed in raw and\nsampled data over a broad range of temporal horizons $M$ varying from one to\ntwelve months over the year 2018. The cluster entropy $S(\\tau,n)$ is integrated\nover the cluster duration $\\tau$ to yield the Market Dynamic Index $I(M,n)$, a\nsynthetic figure of price dynamics. A systematic dependence of the cluster\nentropy $S(\\tau,n)$ and the Market Dynamic Index $I(M,n)$ on the temporal\nhorizon $M$ is evidenced. \\par Finally, the Market Horizon Dependence}, defined\nas $H(M,n)=I(M,n)-I(1,n)$, is compared with the horizon dependence of the\npricing kernel with different representative agents obtained via a\nKullback-Leibler entropy approach. The Market Horizon Dependence $H(M,n)$ of\nthe three assets is compared against the values obtained by implementing the\ncluster entropy $S(\\tau,n)$ approach on artificially generated series\n(Fractional Brownian Motion).\n"
    },
    {
        "paper_id": 1908.00734,
        "authors": "Marco Schreyer, Timur Sattarov, Christian Schulze, Bernd Reimer, and\n  Damian Borth",
        "title": "Detection of Accounting Anomalies in the Latent Space using Adversarial\n  Autoencoder Neural Networks",
        "comments": "11 pages, 9 figures, 2nd KDD Workshop on Anomaly Detection in\n  Finance, August 05, 2019, Anchorage, Alaska",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The detection of fraud in accounting data is a long-standing challenge in\nfinancial statement audits. Nowadays, the majority of applied techniques refer\nto handcrafted rules derived from known fraud scenarios. While fairly\nsuccessful, these rules exhibit the drawback that they often fail to generalize\nbeyond known fraud scenarios and fraudsters gradually find ways to circumvent\nthem. In contrast, more advanced approaches inspired by the recent success of\ndeep learning often lack seamless interpretability of the detected results. To\novercome this challenge, we propose the application of adversarial autoencoder\nnetworks. We demonstrate that such artificial neural networks are capable of\nlearning a semantic meaningful representation of real-world journal entries.\nThe learned representation provides a holistic view on a given set of journal\nentries and significantly improves the interpretability of detected accounting\nanomalies. We show that such a representation combined with the networks\nreconstruction error can be utilized as an unsupervised and highly adaptive\nanomaly assessment. Experiments on two datasets and initial feedback received\nby forensic accountants underpinned the effectiveness of the approach.\n"
    },
    {
        "paper_id": 1908.00811,
        "authors": "Aur\\'elien Alfonsi and Adel Cherchali and Jose Arturo Infante Acevedo",
        "title": "A full and synthetic model for Asset-Liability Management in life\n  insurance, and analysis of the SCR with the standard formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to introduce a synthetic ALM model that catches the\nmain specificity of life insurance contracts. First, it keeps track of both\nmarket and book values to apply the regulatory profit sharing rule. Second, it\nintroduces a determination of the crediting rate to policyholders that is close\nto the practice and is a trade-off between the regulatory rate, a competitor\nrate and the available profits. Third, it considers an investment in bonds that\nenables to match a part of the cash outflow due to surrenders, while avoiding\nto store the trading history. We use this model to evaluate the Solvency\nCapital Requirement (SCR) with the standard formula, and show that the choice\nof the interest rate model is important to get a meaningful model after the\nregulatory shocks on the interest rate. We discuss the different values of the\nSCR modules first in a framework with moderate interest rates using the shocks\nof the present legislation, and then we consider a low interest framework with\nthe latest recommandation of the EIOPA on the shocks. In both cases, we\nillustrate the importance of matching cash-flows and its impact on the SCR.\n"
    },
    {
        "paper_id": 1908.00951,
        "authors": "Lionel Yelibi, Tim Gebbie",
        "title": "Agglomerative Likelihood Clustering",
        "comments": "15 pages, 8 figures",
        "journal-ref": "J. Stat. Mech. (2021) 113408",
        "doi": "10.1088/1742-5468/ac3661",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of fast time-series data clustering. Building on\nprevious work modeling the correlation-based Hamiltonian of spin variables we\npresent an updated fast non-expensive Agglomerative Likelihood Clustering\nalgorithm (ALC). The method replaces the optimized genetic algorithm based\napproach (f-SPC) with an agglomerative recursive merging framework inspired by\nprevious work in Econophysics and Community Detection. The method is tested on\nnoisy synthetic correlated time-series data-sets with built-in cluster\nstructure to demonstrate that the algorithm produces meaningful non-trivial\nresults. We apply it to time-series data-sets as large as 20,000 assets and we\nargue that ALC can reduce compute time costs and resource usage cost for large\nscale clustering for time-series applications while being serialized, and hence\nhas no obvious parallelization requirement. The algorithm can be an effective\nchoice for state-detection for online learning in a fast non-linear data\nenvironment because the algorithm requires no prior information about the\nnumber of clusters.\n"
    },
    {
        "paper_id": 1908.00982,
        "authors": "Wentao Hu",
        "title": "calculation worst-case Value-at-Risk prediction using empirical data\n  under model uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantification of risk positions under model uncertainty is of crucial\nimportance from both viewpoints of external regulation and internal management.\nThe concept of model uncertainty, sometimes also referred to as model\nambiguity. Although we know the family of models, we cannot precisely decide\nwhich one to use. Given the set $\\mathcal{P}$, the value of the risk measure\n$\\rho$ varies in a range over the set of all possible models. The largest value\nin such a range is referred to as a worst-case value, and the corresponding\nmodel is called a worst scenario. Value-at-Risk(VaR) has become a very popular\nrisk-measurement tool since it was first proposed. Naturally, WVaR(worst-case\nValue-at-Risk) attracts the attention of many researchers. Although many\nliteratures investigated WVaR, the implications for empirical data analysis\nremain rare. In this paper, we proposed a special model uncertainty market\nmodel to simply the $\\mathcal{P}$ to a set contain finite number of probability\ndistributions. The model has the structure of the two-layer mixed distribution\nmodel. We used change point detection method to divide the returns series and\nthen used EM algorithm to estimate the parameters. Finally, we calculated VaR,\nWVaR(worst-case Value-at-Risk) and BVaR(best-case Value-at-Risk) for four\nfinancial markets and then analyzed their different performance.\n"
    },
    {
        "paper_id": 1908.01112,
        "authors": "Xinyi Li, Yinchuan Li, Xiao-Yang Liu and Christina Dan Wang",
        "title": "Risk Management via Anomaly Circumvent: Mnemonic Deep Learning for\n  Midterm Stock Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Midterm stock price prediction is crucial for value investments in the stock\nmarket. However, most deep learning models are essentially short-term and\napplying them to midterm predictions encounters large cumulative errors because\nthey cannot avoid anomalies. In this paper, we propose a novel deep neural\nnetwork Mid-LSTM for midterm stock prediction, which incorporates the market\ntrend as hidden states. First, based on the autoregressive moving average model\n(ARMA), a midterm ARMA is formulated by taking into consideration both hidden\nstates and the capital asset pricing model. Then, a midterm LSTM-based deep\nneural network is designed, which consists of three components: LSTM, hidden\nMarkov model and linear regression networks. The proposed Mid-LSTM can avoid\nanomalies to reduce large prediction errors, and has good explanatory effects\non the factors affecting stock prices. Extensive experiments on S&P 500 stocks\nshow that (i) the proposed Mid-LSTM achieves 2-4% improvement in prediction\naccuracy, and (ii) in portfolio allocation investment, we achieve up to 120.16%\nannual return and 2.99 average Sharpe ratio.\n"
    },
    {
        "paper_id": 1908.01142,
        "authors": "Anna Denkowska, Stanis{\\l}aw Wanat",
        "title": "Linkages and systemic risk in the European insurance sector: Some new\n  evidence based on dynamic spanning trees",
        "comments": "JEL: G22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is part of the research on the interlinkages between insurers and\ntheir contribution to systemic risk on the insurance market. Its main purpose\nis to present the results of the analysis of linkage dynamics and systemic risk\nin the European insurance sector which are obtained using correlation networks.\nThese networks are based on dynamic dependence structures modelled using a\ncopula. Then, we determine minimum spanning trees (MST). Finally, the linkage\ndynamics is described by means of selected topological network measures.\n"
    },
    {
        "paper_id": 1908.01171,
        "authors": "Yaroslav Drokin, Mikhail Zhitlukhin",
        "title": "Relative growth optimal strategies in an asset market game",
        "comments": "16 pages. Published in \"Annals of Finance\"",
        "journal-ref": null,
        "doi": "10.1007/s10436-020-00360-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a game-theoretic model of a market where investors compete for\npayoffs yielded by several assets. The main result consists in a proof of the\nexistence and uniqueness of a strategy, called relative growth optimal, such\nthat the logarithm of the share of its wealth in the total wealth of the market\nis a submartingale for any strategies of the other investors. It is also shown\nthat this strategy is asymptotically optimal in the sense that it achieves the\nmaximal capital growth rate when compared to competing strategies. Based on the\nresults obtained, we study the asymptotic structure of the market when all the\ninvestors use the relative growth optimal strategy.\n"
    },
    {
        "paper_id": 1908.01256,
        "authors": "Tomoya Mori, Shosei Sakaguchi",
        "title": "Creation of knowledge through exchanges of knowledge: Evidence from\n  Japanese patent data",
        "comments": "18 pages, 3 figures and 1 table in the main text (18 pages of\n  Appendix)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study shows evidence for collaborative knowledge creation among\nindividual researchers through direct exchanges of their mutual differentiated\nknowledge. Using patent application data from Japan, the collaborative output\nis evaluated according to the quality and novelty of the developed patents,\nwhich are measured in terms of forward citations and the order of application\nwithin their primary technological category, respectively. Knowledge exchange\nis shown to raise collaborative productivity more through the extensive margin\n(i.e., the number of patents developed) in the quality dimension, whereas it\ndoes so more through the intensive margin in the novelty dimension (i.e.,\nnovelty of each patent).\n"
    },
    {
        "paper_id": 1908.01602,
        "authors": "Sebastian Becker, Patrick Cheridito, Arnulf Jentzen, and Timo Welti",
        "title": "Solving high-dimensional optimal stopping problems using deep learning",
        "comments": "54 pages, 1 figure",
        "journal-ref": "Eur. J. Appl. Math 32 (2021) 470-514",
        "doi": "10.1017/S0956792521000073",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays many financial derivatives, such as American or Bermudan options,\nare of early exercise type. Often the pricing of early exercise options gives\nrise to high-dimensional optimal stopping problems, since the dimension\ncorresponds to the number of underlying assets. High-dimensional optimal\nstopping problems are, however, notoriously difficult to solve due to the\nwell-known curse of dimensionality. In this work, we propose an algorithm for\nsolving such problems, which is based on deep learning and computes, in the\ncontext of early exercise option pricing, both approximations of an optimal\nexercise strategy and the price of the considered option. The proposed\nalgorithm can also be applied to optimal stopping problems that arise in other\nareas where the underlying stochastic process can be efficiently simulated. We\npresent numerical results for a large number of example problems, which include\nthe pricing of many high-dimensional American and Bermudan options, such as\nBermudan max-call options in up to 5000 dimensions. Most of the obtained\nresults are compared to reference values computed by exploiting the specific\nproblem design or, where available, to reference values from the literature.\nThese numerical results suggest that the proposed algorithm is highly effective\nin the case of many underlyings, in terms of both accuracy and speed.\n"
    },
    {
        "paper_id": 1908.01709,
        "authors": "Jos\\'e Cl\\'audio do Nascimento",
        "title": "Behavioral Biases and Nonadditive Dynamics in Risk Taking: An\n  Experimental Investigation",
        "comments": "12 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the dynamics of gambling and how they can affect\nrisk-taking behavior in regions not explored by Kahneman and Tversky's Prospect\nTheory. Specifically, it questions why extreme outcomes do not fit the theory\nand proposes alternative ways to measure prospects. The paper introduces a\nmeasure of contrast between gambles and conducts an experiment to test the\nhypothesis that individuals prospect gambles with nonadditive dynamics\ndifferently. The results suggest a strong bias towards certain options, which\nchallenges the predictions of Kahneman and Tversky's theory.\n"
    },
    {
        "paper_id": 1908.01714,
        "authors": "Nils Bertschinger and Martin Hoefer and Daniel Schmand",
        "title": "Flow Allocation Games",
        "comments": "44 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a game-theoretic variant of the maximum circulation problem. In a\nflow allocation game, we are given a directed flow network. Each node is a\nrational agent and can strategically allocate any incoming flow to the outgoing\nedges. Given the strategy choices of all agents, a maximal circulation that\nadheres to the chosen allocation strategies evolves in the network. Each agent\nwants to maximize the amount of flow through her node. Flow allocation games\ncan be used to express strategic incentives of clearing in financial networks.\nWe provide a cumulative set of results on the existence and computational\ncomplexity of pure Nash and strong equilibria, as well as tight bounds on the\n(strong) prices of anarchy and stability. Our results show an interesting\ndichotomy: Ranking strategies over individual flow units allow to obtain\noptimal strong equilibria for many objective functions. In contrast, more\nintuitive ranking strategies over edges can give rise to unfavorable incentive\nproperties.\n"
    },
    {
        "paper_id": 1908.01808,
        "authors": "Ariel Soto-Caro, Feng Wu, Zhengfei Guan, Natalia Peres",
        "title": "Evaluating Pest Management Strategies: A Robust Method and its\n  Application to Strawberry Disease Management",
        "comments": "Selected Paper prepared for presentation at the 2019 Agricultural &\n  Applied Economics Association Annual Meeting, Atlanta, GA, July 21-23.\n  Copyright 2019 by Soto-Caro, Wu, Guan, Peres. All rights reserved. Readers\n  may make verbatim copies of this document for non-commercial purposes by any\n  means, provided that this copyright notice appears on all such copies",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Farmers use pesticides to reduce yield losses. The efficacies of pesticide\ntreatments are often evaluated by analyzing the average treatment effects and\nrisks. The stochastic efficiency with respect to a function is often employed\nin such evaluations through ranking the certainty equivalents of each\ntreatment. The main challenge of using this method is gathering an adequate\nnumber of observations to produce results with statistical power. However, in\nmany cases, only a limited number of trials are replicated in field\nexperiments, leaving an inadequate number of observations. In addition, this\nmethod focuses only on the farmer's profit without incorporating the impact of\ndisease pressure on yield and profit. The objective of our study is to propose\na methodology to address the issue of an insufficient number of observations\nusing simulations and take into account the effect of disease pressure on yield\nthrough a quantile regression model. We apply this method to the case of\nstrawberry disease management in Florida.\n"
    },
    {
        "paper_id": 1908.01943,
        "authors": "Chuancun Yin",
        "title": "Stochastic ordering of Gini indexes for multivariate elliptical random\n  variables",
        "comments": "14pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we establish the stochastic ordering of the Gini indexes for\nmultivariate elliptical risks which generalized the corresponding results for\nmultivariate normal risks. It is shown that several conditions on dispersion\nmatrices and the components of dispersion matrices of multivariate normal risks\nfor the monotonicity of the Gini index in the usual stochastic order proposed\nby Samanthi, Wei and Brazauskas (2016) and Kim and Kim (2019) are also suitable\nfor multivariate elliptical risks. We also study the tail probability of Gini\nindex for multivariate elliptical risks and revised a large deviation result\nfor the Gini indexes of multivariate normal risks in Kim and Kim (2019).\n"
    },
    {
        "paper_id": 1908.02101,
        "authors": "Bruno Scalzo Dees",
        "title": "Analysing Global Fixed Income Markets with Tensors",
        "comments": "9 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global fixed income returns span across multiple maturities and economies,\nthat is, they naturally reside on multi-dimensional data structures referred to\nas tensors. In contrast to standard \"flat-view\" multivariate models that are\nagnostic to data structure and only describe linear pairwise relationships, we\nintroduce a tensor-valued approach to model the global risks shared by multiple\ninterest rate curves. In this way, the estimated risk factors can be\nanalytically decomposed into maturity-domain and country-domain constituents,\nwhich allows the investor to devise rigorous and tractable global portfolio\nmanagement and hedging strategies tailored to each risk domain. An empirical\nanalysis confirms the existence of global risk factors shared by eight\ndeveloped economies, and demonstrates their ability to compactly describe the\nglobal macroeconomic environment.\n"
    },
    {
        "paper_id": 1908.02164,
        "authors": "T. N. Li and A. Papanicolaou",
        "title": "Statistical Arbitrage for Multiple Co-Integrated Stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we analyse optimal statistical arbitrage strategies from\nstochastic control and optimisation problems for multiple co-integrated stocks\nwith eigenportfolios being factors. Optimal portfolio weights are found by\nsolving a Hamilton-Jacobi-Bellman (HJB) partial differential equation, which we\nsolve for both an unconstrained portfolio and a portfolio constrained to be\nmarket neutral. Our analyses demonstrate sufficient conditions on the model\nparameters to ensure long-term stability of the HJB solutions and stable growth\nrates for the optimal portfolios. To gauge how these optimal portfolios behave\nin practice, we perform backtests on historical stock prices of the S&P 500\nconstituents from year 2000 through year 2021. These backtests suggest three\nkey conclusions: that the proposed co-integrated model with eigenportfolios\nbeing factors can generate a large number of co-integrated stocks over a long\ntime horizon, that the optimal portfolios are sensitive to parameter\nestimation, and that the statistical arbitrage strategies are more profitable\nin periods when overall market volatilities are high.\n"
    },
    {
        "paper_id": 1908.02228,
        "authors": "Patrice Gaillardetz and Saeb Hachem",
        "title": "Risk-Control Strategies",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the pricing of derivative products that involve\ndynamic hedging strategies and payments within the planning horizon.\nEquity-indexed annuities (EIAs), Guaranteed investment certificate (GIC),\nAmerican and Barrier options are typical examples of these products. Our\nexploration involves evaluation under different assumptions related to the way\nthe risk is tailored by the issuer. The unified constrained discrete stochastic\ndynamic programming framework presented in this paper makes use of sequential\nlocal minimizing strategies related to stochastic transitions. This sequential\nminimizations takes into account all intermediate requirements and involves\nseveral dynamic risk measures modelling. To demonstrate the flexibility of this\nframework we present numerical examples featuring GICs and point-to-point EIAs.\n"
    },
    {
        "paper_id": 1908.02347,
        "authors": "Nassim Nicholas Taleb, Brandon Yarckin, Chitpuneet Mann, Damir Delic,\n  and Mark Spitznagel",
        "title": "Tail Option Pricing Under Power Laws",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build a methodology that takes a given option price in the tails with\nstrike $K$ and extends (for calls, all strikes > $K$, for puts all strikes $<\nK$) assuming the continuation falls into what we define as \"Karamata Constant\"\nover which the strong Pareto law holds. The heuristic produces relative prices\nfor options, with for sole parameter the tail index $\\alpha$, under some mild\narbitrage constraints.\n  Usual restrictions such as finiteness of variance are not required.\n  The methodology allows us to scrutinize the volatility surface and test\nvarious theories of relative tail option overpricing (usually built on thin\ntailed models and minor modifications/fudging of the Black-Scholes formula).\n"
    },
    {
        "paper_id": 1908.0243,
        "authors": "Idha Sudianto",
        "title": "Review of the Plan for Integrating Big Data Analytics Program for the\n  Electronic Marketing System and Customer Relationship Management: A Case\n  Study XYZ Institution",
        "comments": null,
        "journal-ref": "The International Journal of Business Management and Technology,\n  Volume 3 Issue 4 July (2019) 79-87",
        "doi": "10.5281/zenodo.3361854",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This research aims to explore business processes and what the factors have\nmajor influence on electronic marketing and CRM systems? Which data needs to be\nanalyzed and integrated in the system, and how to do that? How effective of\nintegration the electronic marketing and CRM with big data enabled to support\nMarketing and Customer Relation operations. Research based on case studies at\nXYZ Organization: International Language Education Service in Surabaya.\nResearch is studying secondary data which is supported by qualitative research\nmethods. Using purposive sampling technique with observation and interviewing\nseveral respondents who need the system integration. The documentation of\ninterview is coded to keep confidentiality of the informant. Method of\nextending participation, triangulation of data sources, discussions and the\nadequacy of the theory are uses to validate data. Miles and Huberman models is\nuses to do analysis the data interview. Results of the research are expected to\nbecome a holistic approach to fully integrate the Big Data Analytics program\nwith electronic marketing and CRM systems.\n"
    },
    {
        "paper_id": 1908.02545,
        "authors": "Ta-Hsin Li",
        "title": "Quantile-Frequency Analysis and Spectral Divergence Metrics for\n  Diagnostic Checks of Time Series With Nonlinear Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nonlinear dynamic volatility has been observed in many financial time series.\nThe recently proposed quantile periodogram offers an alternative way to examine\nthis phenomena in the frequency domain. The quantile periodogram is constructed\nfrom trigonometric quantile regression of time series data at different\nfrequencies and quantile levels. It is a useful tool for quantile-frequency\nanalysis (QFA) of nonlinear serial dependence. This paper introduces a number\nof spectral divergence metrics based on the quantile periodogram for diagnostic\nchecks of financial time series models and model-based discriminant analysis.\nThe parametric bootstrapping technique is employed to compute the $p$-values of\nthe metrics. The usefulness of the proposed method is demonstrated empirically\nby a case study using the daily log returns of the S\\&P 500 index over three\nperiods of time together with their GARCH-type models. The results show that\nthe QFA method is able to provide additional insights into the goodness of fit\nof these financial time series models that may have been missed by conventional\ntests. The results also show that the QFA method offers a more informative way\nof discriminant analysis for detecting regime changes in time series.\n"
    },
    {
        "paper_id": 1908.02591,
        "authors": "Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I. Weidele,\n  Claudio Bellei, Tom Robinson, Charles E. Leiserson",
        "title": "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional\n  Networks for Financial Forensics",
        "comments": "7 pages, Tutorial in the Anomaly Detection in Finance Workshop at the\n  25th SIGKDD Conference on Knowledge Discovery and Data Mining",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Anti-money laundering (AML) regulations play a critical role in safeguarding\nfinancial systems, but bear high costs for institutions and drive financial\nexclusion for those on the socioeconomic and international margins. The advent\nof cryptocurrency has introduced an intriguing paradox: pseudonymity allows\ncriminals to hide in plain sight, but open data gives more power to\ninvestigators and enables the crowdsourcing of forensic analysis. Meanwhile\nadvances in learning algorithms show great promise for the AML toolkit. In this\nworkshop tutorial, we motivate the opportunity to reconcile the cause of safety\nwith that of financial inclusion. We contribute the Elliptic Data Set, a time\nseries graph of over 200K Bitcoin transactions (nodes), 234K directed payment\nflows (edges), and 166 node features, including ones based on non-public data;\nto our knowledge, this is the largest labelled transaction data set publicly\navailable in any cryptocurrency. We share results from a binary classification\ntask predicting illicit transactions using variations of Logistic Regression\n(LR), Random Forest (RF), Multilayer Perceptrons (MLP), and Graph Convolutional\nNetworks (GCN), with GCN being of special interest as an emergent new method\nfor capturing relational information. The results show the superiority of\nRandom Forest (RF), but also invite algorithmic work to combine the respective\npowers of RF and graph methods. Lastly, we consider visualization for analysis\nand explainability, which is difficult given the size and dynamism of\nreal-world transaction graphs, and we offer a simple prototype capable of\nnavigating the graph and observing model performance on illicit activity over\ntime. With this tutorial and data set, we hope to a) invite feedback in support\nof our ongoing inquiry, and b) inspire others to work on this societally\nimportant challenge.\n"
    },
    {
        "paper_id": 1908.02646,
        "authors": "Jingyuan Wang, Yang Zhang, Ke Tang, Junjie Wu and Zhang Xiong",
        "title": "AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy\n  using Interpretable Deep Reinforcement Attention Networks",
        "comments": "Accepted for POSTER presentation at KDD2019 Applied Data Science\n  Track",
        "journal-ref": null,
        "doi": "10.1145/3292500.3330647",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent years have witnessed the successful marriage of finance innovations\nand AI techniques in various finance applications including quantitative\ntrading (QT). Despite great research efforts devoted to leveraging deep\nlearning (DL) methods for building better QT strategies, existing studies still\nface serious challenges especially from the side of finance, such as the\nbalance of risk and return, the resistance to extreme loss, and the\ninterpretability of strategies, which limit the application of DL-based\nstrategies in real-life financial markets. In this work, we propose AlphaStock,\na novel reinforcement learning (RL) based investment strategy enhanced by\ninterpretable deep attention networks, to address the above challenges. Our\nmain contributions are summarized as follows: i) We integrate deep attention\nnetworks with a Sharpe ratio-oriented reinforcement learning framework to\nachieve a risk-return balanced investment strategy; ii) We suggest modeling\ninterrelationships among assets to avoid selection bias and develop a\ncross-asset attention mechanism; iii) To our best knowledge, this work is among\nthe first to offer an interpretable investment strategy using deep\nreinforcement learning models. The experiments on long-periodic U.S. and\nChinese markets demonstrate the effectiveness and robustness of AlphaStock over\ndiverse market states. It turns out that AlphaStock tends to select the stocks\nas winners with high long-term growth, low volatility, high intrinsic value,\nand being undervalued recently.\n"
    },
    {
        "paper_id": 1908.02793,
        "authors": "David Rushing Dewhurst, Christopher M. Danforth, and Peter Sheridan\n  Dodds",
        "title": "Noncooperative dynamics in election interference",
        "comments": "33 pages (22 body, 11 appendix), 33 figures (15 body, 18 appendix),\n  accompanying code at https://gitlab.com/daviddewhurst/red-blue-game",
        "journal-ref": "Phys. Rev. E 101, 022307 (2020)",
        "doi": "10.1103/PhysRevE.101.022307",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Foreign power interference in domestic elections is an existential threat to\nsocieties. Manifested through myriad methods from war to words, such\ninterference is a timely example of strategic interaction between economic and\npolitical agents. We model this interaction between rational game players as a\ncontinuous-time differential game, constructing an analytical model of this\ncompetition with a variety of payoff structures. All-or-nothing attitudes by\nonly one player regarding the outcome of the game lead to an arms race in which\nboth countries spend increasing amounts on interference and\ncounter-interference operations. We then confront our model with data\npertaining to the Russian interference in the 2016 United States presidential\nelection contest. We introduce and estimate a Bayesian structural time series\nmodel of election polls and social media posts by Russian Twitter troll\naccounts. Our analytical model, while purposefully abstract and simple,\nadequately captures many temporal characteristics of the election and social\nmedia activity. We close with a discussion of our model's shortcomings and\nsuggestions for future research.\n"
    },
    {
        "paper_id": 1908.02847,
        "authors": "Oleh Danyliv, Bruce Bland",
        "title": "An instantaneous market volatility estimation",
        "comments": "16 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Working on different aspects of algorithmic trading we empirically discovered\na new market invariant. It links together the volatility of the instrument with\nits traded volume, the average spread and the volume in the order book. The\ninvariant has been tested on different markets and different asset classes. In\nall cases we did not find significant violation of the invariant. The formula\nfor the invariant was used for the volatility estimation, which we called the\ninstantaneous volatility. Quantitative comparison showed that it reproduces\nrealised volatility better than one-day-ahead GARCH(1,1) prediction. Because of\nthe short-term prediction nature, the instantaneous volatility could be used by\nalgo developers, volatility traders and other market professionals.\n"
    },
    {
        "paper_id": 1908.03007,
        "authors": "Antoine Jacquier and Lorenzo Torricelli",
        "title": "Anomalous diffusions in option prices: connecting trade duration and the\n  volatility term structure",
        "comments": "33 pages, 14 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Anomalous diffusions arise as scaling limits of continuous-time random walks\n(CTRWs) whose innovation times are distributed according to a power law. The\nimpact of a non-exponential waiting time does not vanish with time and leads to\ndifferent distribution spread rates compared to standard models. In financial\nmodelling this has been used to accommodate for random trade duration in the\ntick-by-tick price process. We show here that anomalous diffusions are able to\nreproduce the market behaviour of the implied volatility more consistently than\nusual L\\'evy or stochastic volatility models. We focus on two distinct classes\nof underlying asset models, one with independent price innovations and waiting\ntimes, and one allowing dependence between these two components. These two\nmodels capture the well-known paradigm according to which shorter trade\nduration is associated with higher return impact of individual trades. We fully\ndescribe these processes in a semimartingale setting leading no-arbitrage\npricing formulae, and study their statistical properties. We observe that\nskewness and kurtosis of the asset returns do not tend to zero as time goes by.\nWe also characterize the large-maturity asymptotics of Call option prices, and\nfind that the convergence rate is slower than in standard L\\'evy regimes, which\nin turn yields a declining implied volatility term structure and a slower decay\nof the skew.\n"
    },
    {
        "paper_id": 1908.03137,
        "authors": "Nicola Cufaro Petroni, Piergiacomo Sabino",
        "title": "Fast Pricing of Energy Derivatives with Mean-reverting Jump-diffusion\n  Processes",
        "comments": "22 pages, 5 figures, 8 Tables, preprint",
        "journal-ref": "Applied Mathematical Finance, 2021",
        "doi": "10.1080/1350486X.2021.1909488",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most energy and commodity markets exhibit mean-reversion and occasional\ndistinctive price spikes, which results in demand for derivative products which\nprotect the holder against high prices. To this end, in this paper we present\nexact and fast methodologies for the simulation of the spot price dynamics\nmodeled as the exponential of the sum of an Ornstein-Uhlenbeck and an\nindependent pure jump process, where the latter one is driven by a compound\nPoisson process with (bilateral) exponentially distributed jumps. These\nmethodologies are finally applied to the pricing of Asian options, gas storages\nand swings under different combinations of jump-diffusion market models, and\nthe apparent computational advantages of the proposed procedures are\nemphasized.\n"
    },
    {
        "paper_id": 1908.03206,
        "authors": "Sebastian Frischbier, Mario Paic, Alexander Echler, Christian Roth",
        "title": "Managing the Complexity of Processing Financial Data at Scale -- an\n  Experience Report",
        "comments": "12 pages, 2 figures, to be published in the proceedings of the 10th\n  Complex Systems Design & Management conference (CSD&M'19) by Springer",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-34843-4_2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are extremely data-driven and regulated. Participants rely\non notifications about significant events and background information that meet\ntheir requirements regarding timeliness, accuracy, and completeness. As one of\nEurope's leading providers of financial data and regulatory solutions vwd\nprocesses a daily average of 18 billion notifications from 500+ data sources\nfor 30 million symbols. Our large-scale geo-distributed systems handle daily\npeak rates of 1+ million notifications/sec. In this paper we give practical\ninsights about the different types of complexity we face regarding the data we\nprocess, the systems we operate, and the regulatory constraints we must comply\nwith. We describe the volume, variety, velocity, and veracity of the data we\nprocess, the infrastructure we operate, and the architecture we apply. We\nillustrate the load patterns created by trading and how the markets' attention\nto the Brexit vote and similar events stressed our systems.\n"
    },
    {
        "paper_id": 1908.03233,
        "authors": "Ravi Kashyap",
        "title": "The Economics of Enlightenment: Time Value of Knowledge and the Net\n  Present Value (NPV) of Knowledge Machines, A Proposed Approach Adapted from\n  Finance",
        "comments": null,
        "journal-ref": "The BE Journal of Economic Analysis & Policy, 20(2) (2020)",
        "doi": "10.1515/bejeap-2019-0044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate one methodology to put a value or price on knowledge using well\naccepted techniques from finance. We provide justifications for these finance\nprinciples based on the limitations of the physical world we live in. We start\nwith the intuition for our method to value knowledge and then formalize this\nidea with a series of axioms and models. To the best of our knowledge this is\nthe first recorded attempt to put a numerical value on knowledge. The\nimplications of this valuation exercise, which places a high premium on any\npiece of knowledge, are to ensure that participants in any knowledge system are\nbetter trained to notice the knowledge available from any source. Just because\nsomeone does not see a connection does not mean that there is no connection. We\nneed to try harder and be more open to acknowledging the smallest piece of new\nknowledge that might have been brought to light by anyone from anywhere about\nanything.\n"
    },
    {
        "paper_id": 1908.03281,
        "authors": "\\'Alvaro Cartea, Sebastian Jaimungal, Leandro S\\'anchez-Betancourt",
        "title": "Latency and Liquidity Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Latency (i.e., time delay) in electronic markets affects the efficacy of\nliquidity taking strategies. During the time liquidity takers process\ninformation and send marketable limit orders (MLOs) to the exchange, the limit\norder book (LOB) might undergo updates, so there is no guarantee that MLOs are\nfilled. We develop a latency-optimal trading strategy that improves the\nmarksmanship of liquidity takers. The interaction between the LOB and MLOs is\nmodelled as a marked point process. Each MLO specifies a price limit so the\norder can receive worse prices and quantities than those the liquidity taker\ntargets if the updates in the LOB are against the interest of the trader. In\nour model, the liquidity taker balances the tradeoff between missing trades and\nthe costs of walking the book. We employ techniques of variational analysis to\nobtain the optimal price limit of each MLO the agent sends. The price limit of\na MLO is characterized as the solution to a new class of forward-backward\nstochastic differential equations (FBSDEs) driven by random measures. We prove\nthe existence and uniqueness of the solution to the FBSDE and numerically solve\nit to illustrate the performance of the latency-optimal strategies.\n"
    },
    {
        "paper_id": 1908.03287,
        "authors": "Nate Dwyer, Sandro Claudio Lera, Alex Sandy Pentland",
        "title": "Ordinal Tax To Sustain a Digital Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, the French Senate approved a law that imposes a 3% tax on revenue\ngenerated from digital services by companies above a certain size. While there\nis a lot of political debate about economic consequences of this action, it is\nactually interesting to reverse the question: We consider the long-term\nimplications of an economy with no such digital tax. More generally, we can\nthink of digital services as a special case of products with low or zero cost\nof transportation. With basic economic models we show that a market with no\ntransportation costs is prone to monopolization as minuscule, random\ndifferences in quality are rewarded disproportionally. We then propose a\ndistance-based tax to counter-balance the tendencies of random centralisation.\nUnlike a tax that scales with physical (cardinal) distance, a ranked (ordinal)\ndistance tax leverages the benefits of digitalization while maintaining a\nstable economy.\n"
    },
    {
        "paper_id": 1908.03407,
        "authors": "Aparna B. S, Neelesh S Upadhye",
        "title": "On the Compound Beta-Binomial Risk Model with Delayed Claims and\n  Randomized Dividends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose the discrete time Compound Beta-Binomial Risk Model\nwith by-claims, delayed by-claims and randomized dividends. We then analyze the\nGerber-Shiu function for the cases where the dividend threshold $d=0$ and $d>0$\nunder the assumption that the constant discount rate $\\nu \\in (0,1)$. More\nspecifically, we study the discrete time compound binomial risk model subject\nto the assumption that the probabilities with which the claims, by-claims occur\nand the dividends are issued are not fixed(constant), instead the probabilities\nare random and follow a Beta distribution with parameters $a_{i}$ and $b_{i}$,\n$i = 1, 2, 3$. Recursive expressions for the Gerber-Shiu function corresponding\nto the proposed model are obtained. The recursive relations are further\nutilized to obtain significant ruin related quantities of interest. Recursive\nrelations for probability of ruin, the probability of the deficit at ruin, the\ngenerating function of the deficit at ruin and the probability of surplus at\nruin and for the probability of the claim causing ruin are obtained.\n"
    },
    {
        "paper_id": 1908.03899,
        "authors": "Subhojit Biswas and Diganta Mukherjee",
        "title": "A Proposal for Multi-asset Generalised Variance Swaps",
        "comments": "arXiv admin note: text overlap with arXiv:1205.5565 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes swaps on two important new measures of generalized\nvariance, namely the maximum eigen-value and trace of the covariance matrix of\nthe assets involved. We price these generalized variance swaps for financial\nmarkets with Markov-modulated volatilities. We consider multiple assets in the\nportfolio for theoretical purpose and demonstrate our approach with numerical\nexamples taking three stocks in the portfolio. The resultsobtained in this\npaper have important implications for the commodity sector where such swaps\nwould be useful for hedging risk\n"
    },
    {
        "paper_id": 1908.03905,
        "authors": "Subhojit Biswas, Mrinal K.Ghosh and Diganta Mukherjee",
        "title": "Portfolio Optimization Managing Value at Risk under Heavy Tail Return,\n  using Stochastic Maximum Principle",
        "comments": "Minor revision in Stochastic Analysis and Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor, whose portfolio consists of a single risky asset and\na risk free asset, who wants to maximize his expected utility of the portfolio\nsubject to managing the Value at Risk (VaR) assuming a heavy tailed\ndistribution of the stock prices return. We use a stochastic maximum principle\nto formulate the dynamic optimisation problem. The equations which we obtain\ndoes not have any explicit analytical solution, so we look for accurate\napproximations to estimate the value function and optimal strategy. As our\ncalibration strategy is non-parametric in nature, no prior knowledge on the\nform of the distribution function is needed. We also provide detailed empirical\nillustration using real life data. Our results show close concordance with\nfinancial intuition.We expect that our results will add to the arsenal of the\nhigh frequency traders.\n"
    },
    {
        "paper_id": 1908.03907,
        "authors": "Subhojit Biswas and Diganta Mukherjee",
        "title": "Discrete time portfolio optimisation managing value at risk under heavy\n  tail return distribution",
        "comments": "Published in International Journal Mathematical Modelling and\n  Numerical Optimisation, Vol. 10, No. 4, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor, whose portfolio consists of a single risky asset and\na risk free asset, who wants to maximize his expected utility of the portfolio\nsubject to the Value at Risk assuming a heavy tail distribution of the stock\nprices return. We use Markov Decision Process and dynamic programming principle\nto get the optimal strategies and the value function which maximize the\nexpected utility for parametric as well as non parametric distributions. Due to\nlack of explicit solution in the non parametric case, we use numerical\nintegration for optimization\n"
    },
    {
        "paper_id": 1908.03946,
        "authors": "Constantinos Kardaras",
        "title": "Stochastic integration with respect to arbitrary collections of\n  continuous semimartingales and applications to Mathematical Finance",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic integrals are defined with respect to a collection $P = (P_i; \\, i\n\\in I)$ of continuous semimartingales, imposing no assumptions on the index set\n$I$ and the subspace of $\\mathbb{R}^I$ where $P$ takes values. The integrals\nare constructed though finite-dimensional approximation, identifying the\nappropriate local geometry that allows extension to infinite dimensions. For\nlocal martingale integrators, the resulting space $\\mathsf{S} (P)$ of\nstochastic integrals has an operational characterisation via a corresponding\nset of integrands $\\mathsf{R} (C)$, constructed with only reference the\ncovariation structure $C$ of $P$. This bijection between $\\mathsf{R} (C)$ and\nthe (closed in the semimartingale topology) set $\\mathsf{S} (P)$ extends to\nfamilies of continuous semimartingale integrators for which the drift process\nof $P$ belongs to $\\mathsf{R} (C)$. In the context of infinite-asset models in\nMathematical Finance, the latter structural condition is equivalent to a\ncertain natural form of market viability. The enriched class of wealth\nprocesses via extended stochastic integrals leads to exact analogues of\noptional decomposition and hedging duality as the finite-asset case. A\ncorresponding characterisation of market completeness in this setting is\nprovided.\n"
    },
    {
        "paper_id": 1908.04243,
        "authors": "Taras Bodnar, Holger Dette, Nestor Parolya and Erik Thors\\'en",
        "title": "Sampling Distributions of Optimal Portfolio Weights and Characteristics\n  in Low and Large Dimensions",
        "comments": "40 pages, 4 figures (this version: accepted version manuscript +\n  corrigendum)",
        "journal-ref": "Random Matrices: Theory and Applications, Vol. 11, No. 01, 2250008\n  (2022)",
        "doi": "10.1142/S2010326322500083",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal portfolio selection problems are determined by the (unknown)\nparameters of the data generating process. If an investor wants to realise the\nposition suggested by the optimal portfolios, he/she needs to estimate the\nunknown parameters and to account for the parameter uncertainty in the decision\nprocess. Most often, the parameters of interest are the population mean vector\nand the population covariance matrix of the asset return distribution. In this\npaper, we characterise the exact sampling distribution of the estimated optimal\nportfolio weights and their characteristics. This is done by deriving their\nsampling distribution by its stochastic representation. This approach possesses\nseveral advantages, {e.g.} (i) it determines the sampling distribution of the\nestimated optimal portfolio weights by expressions, which could be used to draw\nsamples from this distribution efficiently; (ii) the application of the derived\nstochastic representation provides an easy way to obtain the asymptotic\napproximation of the sampling distribution. The later property is used to show\nthat the high-dimensional asymptotic distribution of optimal portfolio weights\nis a multivariate normal and to determine its parameters. Moreover, a\nconsistent estimator of optimal portfolio weights and their characteristics is\nderived under the high-dimensional settings. Via an extensive simulation study,\nwe investigate the finite-sample performance of the derived asymptotic\napproximation and study its robustness to the violation of the model\nassumptions used in the derivation of the theoretical results.\n"
    },
    {
        "paper_id": 1908.04333,
        "authors": "Oleh Danyliv, Bruce Bland and Alexandre Argenson",
        "title": "Random walk model from the point of view of algorithmic trading",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the fact that an intraday market price distribution is not normal,\nthe random walk model of price behaviour is as important for the understanding\nof basic principles of the market as the pendulum model is a starting point of\nmany fundamental theories in physics. This model is a good zero order\napproximation for liquid fast moving markets where the queue position is less\nimportant than the price action. In this paper we present an exact solution for\nthe cost of the static passive slice execution. It is shown, that if a price\nhas a random walk behaviour, there is no optimal limit level for an order\nexecution: all levels have the same execution cost as an immediate aggressive\nexecution at the beginning of the slice. Additionally the estimations for the\nrisk of a limit order as well as the probability of a limit order execution as\nfunctions of the slice time and standard deviation of the price are derived.\n"
    },
    {
        "paper_id": 1908.04369,
        "authors": "Fangzhou Xie",
        "title": "Wasserstein Index Generation Model: Automatic Generation of Time-series\n  Index with Application to Economic Policy Uncertainty",
        "comments": "Accepted at Economics Letters, and will be available online soon",
        "journal-ref": null,
        "doi": "10.1016/j.econlet.2019.108874",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I propose a novel method, the Wasserstein Index Generation model (WIG), to\ngenerate a public sentiment index automatically. To test the model`s\neffectiveness, an application to generate Economic Policy Uncertainty (EPU)\nindex is showcased.\n"
    },
    {
        "paper_id": 1908.04401,
        "authors": "Grzegorz Krzy\\.zanowski, Ernesto Mordecki, Andr\\'es Sosa",
        "title": "Zero Black-Derman-Toy interest rate model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a modification of the classical Black-Derman-Toy (BDT) interest\nrate tree model, which includes the possibility of a jump with small\nprobability at each step to a practically zero interest rate. The corresponding\nBDT algorithms are consequently modified to calibrate the tree containing the\nzero interest rate scenarios. This modification is motivated by the recent\n2008-2009 crisis in the United States and it quantifies the risk of a future\ncrises in bond prices and derivatives. The proposed model is useful to price\nderivatives. This exercise also provides a tool to calibrate the probability of\nthis event. A comparison of option prices and implied volatilities on US\nTreasury bonds computed with both the proposed and the classical tree model is\nprovided, in six different scenarios along the different periods comprising the\nyears 2002-2017.\n"
    },
    {
        "paper_id": 1908.04569,
        "authors": "Timo Dimitriadis and Julie Schnaitmann",
        "title": "Forecast Encompassing Tests for the Expected Shortfall",
        "comments": "International Journal of Forecasting (2020+, forthcoming)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce new forecast encompassing tests for the risk measure Expected\nShortfall (ES). The ES currently receives much attention through its\nintroduction into the Basel III Accords, which stipulate its use as the primary\nmarket risk measure for the international banking regulation. We utilize joint\nloss functions for the pair ES and Value at Risk to set up three ES\nencompassing test variants. The tests are built on misspecification robust\nasymptotic theory and we investigate the finite sample properties of the tests\nin an extensive simulation study. We use the encompassing tests to illustrate\nthe potential of forecast combination methods for different financial assets.\n"
    },
    {
        "paper_id": 1908.04667,
        "authors": "Martin Keller-Ressel",
        "title": "The classification of term structure shapes in the two-factor Vasicek\n  model -- a total positivity approach",
        "comments": "major rewrite of the original paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a full classification of all attainable term structure shapes in\nthe two-factor Vasicek model of interest rates. In particular, we show that the\nshapes normal, inverse, humped, dipped and hump-dip are always attainable. In\ncertain parameter regimes up to four additional shapes can be produced. Our\nresults apply to both forward and yield curves and show that the correlation\nand the difference in mean-reversion speeds of the two factor processes play a\nkey role in determining the scope of attainable shapes. The key mathematical\ntool is the theory of total positivity, pioneered by Samuel Karlin and others\nin the 1950ies.\n"
    },
    {
        "paper_id": 1908.04697,
        "authors": "Giovanni Bonaccolto",
        "title": "Critical Decisions for Asset Allocation via Penalized Quantile\n  Regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the analysis of investment strategies derived from penalized\nquantile regression models, introducing alternative approaches to improve\nstate\\textendash of\\textendash art asset allocation rules. First, we use a\npost\\textendash penalization procedure to deal with overshrinking and\nconcentration issues. Second, we investigate whether and to what extent the\nperformance changes when moving from convex to nonconvex penalty functions.\nThird, we compare different methods to select the optimal tuning parameter\nwhich controls the intensity of the penalization. Empirical analyses on\nreal\\textendash world data show that these alternative methods outperform the\nsimple LASSO. This evidence becomes stronger when focusing on the extreme risk,\nwhich is strictly linked to the quantile regression method.\n"
    },
    {
        "paper_id": 1908.04837,
        "authors": "Ankush Agarwal, Matthew Lorig",
        "title": "The implied Sharpe ratio",
        "comments": "22 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an incomplete market, including liquidly-traded European options in an\ninvestment portfolio could potentially improve the expected terminal utility\nfor a risk-averse investor. However, unlike the Sharpe ratio, which provides a\nconcise measure of the relative investment attractiveness of different\nunderlying risky assets, there is no such measure available to help investors\nchoose among the different European options. We introduce a new concept -- the\nimplied Sharpe ratio -- which allows investors to make such a comparison in an\nincomplete financial market. Specifically, when comparing various European\noptions, it is the option with the highest implied Sharpe ratio that, if\nincluded in an investor's portfolio, will improve his expected utility the\nmost. Through the method of Taylor series expansion of the state-dependent\ncoefficients in a nonlinear partial differential equation, we also establish\nthe behaviour of the implied Sharpe ratio with respect to an investor's\nrisk-aversion parameter. In a series of numerical studies, we compare the\ninvestment attractiveness of different European options by studying their\nimplied Sharpe ratio.\n"
    },
    {
        "paper_id": 1908.04852,
        "authors": "Zahra Saki, Lori Rothenberg, Marguerite Moor, Ivan Kandilov and A.\n  Blanton Godfrey",
        "title": "Forecasting U.S. Textile Comparative Advantage Using Autoregressive\n  Integrated Moving Average Models and Time Series Outlier Analysis",
        "comments": "11 pages, 1Figure and 9 tables",
        "journal-ref": "2018 Joint Statistical Meeting, 1996-2006",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To establish an updated understanding of the U.S. textile and apparel (TAP)\nindustrys competitive position within the global textile environment, trade\ndata from UN-COMTRADE (1996-2016) was used to calculate the Normalized Revealed\nComparative Advantage (NRCA) index for 169 TAP categories at the four-digit\nHarmonized Schedule (HS) code level. Univariate time series using\nAutoregressive Integrated Moving Average (ARIMA) models forecast short-term\nfuture performance of Revealed categories with export advantage. Accompanying\noutlier analysis examined permanent level shifts that might convey important\ninformation about policy changes, influential drivers and random events.\n"
    },
    {
        "paper_id": 1908.049,
        "authors": "Chinonso Nwankwo, Weizhong Dai, Ruihua Liu",
        "title": "Compact Finite Difference Scheme with Hermite Interpolation for Pricing\n  American Put Options Based on Regime Switching Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a system of coupled free boundary problems for pricing American\nput options with regime-switching. To solve this system, we first employ the\nlogarithmic transformation to map the free boundary for each regime to\nmulti-fixed intervals and then eliminate the first-order derivative in the\ntransformed model by taking derivatives to obtain a system of partial\ndifferential equations which we call the asset-delta-gamma-speed equations. As\nsuch, the fourth-order compact finite difference scheme can be used for solving\nthis system. The influence of other asset, delta, gamma, and speed options in\nthe present regime is estimated based on Hermite interpolations. Finally, the\nnumerical method is tested with several examples. Our results show that the\nscheme provides an accurate solution that is fast in computation as compared\nwith other existing numerical methods.\n"
    },
    {
        "paper_id": 1908.04959,
        "authors": "Jong Jun Park and Kyungsub Lee",
        "title": "Computational method for probability distribution on recursive\n  relationships in financial applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In quantitative finance, it is often necessary to analyze the distribution of\nthe sum of specific functions of observed values at discrete points of an\nunderlying process. Examples include the probability density function, the\nhedging error, the Asian option, and statistical hypothesis testing. We propose\na method to calculate such a distribution, utilizing a recursive method, and\nexamine it using various examples. The results of the numerical experiment show\nthat our proposed method has high accuracy.\n"
    },
    {
        "paper_id": 1908.04962,
        "authors": "Shashank Oberoi and Mohammed Bilal Girach and Siddhartha P.\n  Chakrabarty",
        "title": "Can robust optimization offer improved portfolio performance?: An\n  empirical study of Indian market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The emergence of robust optimization has been driven primarily by the\nnecessity to address the demerits of the Markowitz model. There has been a\nnoteworthy debate regarding consideration of robust approaches as superior or\nat par with the Markowitz model, in terms of portfolio performance. In order to\naddress this skepticism, we perform empirical analysis of three robust\noptimization models, namely the ones based on box, ellipsoidal and separable\nuncertainty sets. We conclude that robust approaches can be considered as a\nviable alternative to the Markowitz model, not only in simulated data but also\nin a real market setup, involving the Indian indices of S&P BSE 30 and S&P BSE\n100. Finally, we offer qualitative and quantitative justification regarding the\npractical usefulness of robust optimization approaches from the point of view\nof number of stocks, sample size and types of data.\n"
    },
    {
        "paper_id": 1908.05002,
        "authors": "Mohammed Bilal Girach and Shashank Oberoi and Siddhartha P.\n  Chakrabarty",
        "title": "Is being `Robust' beneficial?: A perspective from the Indian market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of data uncertainty has motivated the incorporation of robust\noptimization in various arenas, beyond the Markowitz portfolio optimization.\nThis work presents the extension of the robust optimization framework for the\nminimization of downside risk measures, such as Value-at-Risk (VaR) and\nConditional Value-at-Risk (CVaR). We perform an empirical study of VaR and CVaR\nframeworks, with respect to their robust counterparts, namely, Worst-Case VaR\nand Worst-Case CVaR, using the market data as well as the simulated data. After\ndiscussing the practical usefulness of the robust optimization approaches from\nvarious standpoints, we infer various takeaways. The robust models in the case\nof VaR and CVaR minimization exhibit superior performance with respect to their\nbase versions in the cases involving higher number of stocks and simulated\nsetup respectively.\n"
    },
    {
        "paper_id": 1908.05089,
        "authors": "Kyungsub Lee and Byoung Ki Seo",
        "title": "Modeling microstructure price dynamics with symmetric Hawkes and\n  diffusion model using ultra-high-frequency stock data",
        "comments": null,
        "journal-ref": "Journal of Economic Dynamics and Control, Volume 79, 2017, Pages\n  154-183",
        "doi": "10.1016/j.jedc.2017.04.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examine the theoretical and empirical perspectives of the\nsymmetric Hawkes model of the price tick structure. Combined with the maximum\nlikelihood estimation, the model provides a proper method of volatility\nestimation specialized in ultra-high-frequency analysis. Empirical studies\nbased on the model using the ultra-high-frequency data of stocks in the S\\&P\n500 are performed. The performance of the volatility measure, intraday\nestimation, and the dynamics of the parameters are discussed. A new approach of\ndiffusion analogy to the symmetric Hawkes model is proposed with the\ndistributional properties very close to the Hawkes model. As a diffusion\nprocess, the model provides more analytical simplicity when computing the\nvariance formula, incorporating skewness and examining the probabilistic\nproperty. An estimation of the diffusion model is performed using the simulated\nmaximum likelihood method and shows similar patterns to the Hawkes model.\n"
    },
    {
        "paper_id": 1908.05105,
        "authors": "Kyungsub Lee and Byoung Ki Seo",
        "title": "Performance of tail hedged portfolio with third moment variation swap",
        "comments": null,
        "journal-ref": "Computational Economics, 2017, 50, pp 447--471",
        "doi": "10.1007/s10614-016-9593-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The third moment variation of a financial asset return process is defined by\nthe quadratic covariation between the return and square return processes. The\nskew and fat tail risk of an underlying asset can be hedged using a third\nmoment variation swap under which a predetermined fixed leg and the floating\nleg of the realized third moment variation are exchanged. The probability\ndensity function of the hedged portfolio with the third moment variation swap\nwas examined using a partial differential equation approach. An alternating\ndirection implicit method was used for numerical analysis of the partial\ndifferential equation. Under the stochastic volatility and jump diffusion\nstochastic volatility models, the distributions of the hedged portfolio return\nare symmetric and have more Gaussian-like thin-tails.\n"
    },
    {
        "paper_id": 1908.0513,
        "authors": "Yali Dou, Haiyan Liu, Georgios Aivaliotis",
        "title": "Dynamic Dependence Modeling in financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the dependence modeling of financial assets in a dynamic\nway and its critical role in measuring risk. Two new methods, called\nAccelerated Moving Window method and Bottom-up method are proposed to detect\nthe change of copula. The performance of these two methods together with Binary\nSegmentation \\cite{vostrikova1981detection} and Moving Window method\n\\cite{guegan2009forecasting} is compared based on simulated data. The\nbest-performing method is applied to Standard \\& Poor 500 and Nasdaq indices.\nValue-at-Risk and Expected Shortfall are computed from the dynamic and the\nstatic model respectively to illustrate the effectiveness of the best method as\nwell as the importance of dynamic dependence modeling through backtesting.\n"
    },
    {
        "paper_id": 1908.052,
        "authors": "Valery Baskakov, Nikolay Sheparnev and Evgeny Yanenko",
        "title": "Nonparametric modeling cash flows of insurance company",
        "comments": "24 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes an original methodology for constructing quantitative\nstatistical models based on multidimensional distribution functions constructed\non the basis of the insurance companies' data on inshurance policies (including\npolicies with deductible) and claims incurred. Real data of some Russian\ninsurance companies on non-life insurance contracts illustrate some\nopportunities of the proposed approach. The point and interval estimates of net\npremium, claims frequency, claims reserves including IBNR and OCR, are thus\nobtained. The resulting estimate of claims reserves falls in the range of\nreasonable estimates calculated on the basis of traditional reserving methods\n(the chain-ladder method, the frequency-severity method and the\nBornhuetter-Ferguson method).\n  The proposed methodology is based on additive estimates of a company's\nfinancial indicators, in the sense that they are calculated as a sum of\nestimates built separately for each element of the sample (claim). This allows\nusing the proposed methodology to model insurance companies' financial flows\nand, in particular, to solve the problems of reserve redistribution between\nparticular segments of insurance portfolio and/or time intervals; to adjust\nrisk as part of financial reporting under IAS 17 Insurance Contracts; and to\ndeal with many other tasks.\n  The accuracy of insurance companies' financial parameters estimate based on\nthe proposed methods was tested by statistical modeling. IBNR was used as the\ntest parameter. The modeling results showed a satisfactory accuracy of the\nproposed reserve estimates.\n"
    },
    {
        "paper_id": 1908.05405,
        "authors": "Kyungsub Lee",
        "title": "Risk-neutral option pricing under GARCH intensity model",
        "comments": null,
        "journal-ref": "International Journal of Pure and Applied Mathematics, 114, pp.\n  619-638, 2017",
        "doi": "10.12732/ijpam.v114i3.17",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The risk-neutral option pricing method under GARCH intensity model is\nexamined. The GARCH intensity model incorporates the characteristics of\nfinancial return series such as volatility clustering, leverage effect and\nconditional asymmetry. The GARCH intensity option pricing model has flexibility\nin changing the volatility according to the probability measure change.\n"
    },
    {
        "paper_id": 1908.05419,
        "authors": "Yuan Hu, Svetlozar T. Rachev and Frank J. Fabozzi",
        "title": "Modelling Crypto Asset Price Dynamics, Optimal Crypto Portfolio, and\n  Crypto Option Valuation",
        "comments": "25 pages, 6 figures",
        "journal-ref": "The Journal of Alternative Investments Apr 2021, jai.2021.1.133;\n  DOI: 10.3905/jai.2021.1.133",
        "doi": "10.3905/jai.2021.1.133",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite being described as a medium of exchange, cryptocurrencies do not have\nthe typical attributes of a medium of exchange. Consequently, cryptocurrencies\nare more appropriately described as crypto assets. A common investment\nattribute shared by the more than 2,500 crypto assets is that they are highly\nvolatile. An investor interested in reducing price volatility of a portfolio of\ncrypto assets can do so by constructing an optimal portfolio through standard\noptimization techniques that minimize tail risk. Because crypto assets are not\nbacked by any real assets, forming a hedge to reduce the risk contribution of a\nsingle crypto asset can only be done with another set of similar assets (i.e.,\na set of other crypto assets). A major finding of this paper is that crypto\nportfolios constructed via optimizations that minimize variance and Conditional\nValue at Risk outperform a major stock market index (the S$\\&$P 500). As of\nthis writing, options in which the underlying is a crypto asset index are not\ntraded, one of the reasons being that the academic literature has not\nformulated an acceptable fair pricing model. We offer a fair valuation model\nfor crypto asset options based on a dynamic pricing model for the underlying\ncrypto assets. The model was carefully backtested and therefore offers a\nreliable model for the underlying crypto assets in the natural world. We then\nobtain the valuation of crypto options by passing the natural world to the\nequivalent martingale measure via the Esscher transform. Because of the absence\nof traded crypto options we could not compare the prices obtained from our\nvaluation model to market prices. Yet, we can claim that if such options on\ncrypto assets are introduced, they should follow closely our theoretical prices\nafter adjusting for market frictions and design feature nuances.\n"
    },
    {
        "paper_id": 1908.05443,
        "authors": "Kristian Koerselman",
        "title": "Why Finnish polytechnics reject top applicants",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/09645292.2020.1787953",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I use a panel of higher education clearinghouse data to study the centralized\nassignment of applicants to Finnish polytechnics. I show that on a yearly\nbasis, large numbers of top applicants unnecessarily remain unassigned to any\nprogram. There are programs which rejected applicants would find acceptable,\nbut the assignment mechanism both discourages applicants from applying, and\nstops programs from admitting those who do. A mechanism which would admit each\nyear's most eligible applicants has the potential to substantially reduce\nre-applications, thereby shortening the long queues into Finnish higher\neducation.\n"
    },
    {
        "paper_id": 1908.05518,
        "authors": "Haohui 'Caron' Chen, Xun Li, Morgan Frank, Xiaozhen Qin, Weipan Xu,\n  Manuel Cebrian and Iyad Rahwan",
        "title": "Automation Impacts on China's Polarized Job Market",
        "comments": "28 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When facing threats from automation, a worker residing in a large Chinese\ncity might not be as lucky as a worker in a large U.S. city, depending on the\ntype of large city in which one resides. Empirical studies found that large\nU.S. cities exhibit resilience to automation impacts because of the increased\noccupational and skill specialization. However, in this study, we observe\npolarized responses in large Chinese cities to automation impacts. The\npolarization might be attributed to the elaborate master planning of the\ncentral government, through which cities are assigned with different industrial\ngoals to achieve globally optimal economic success and, thus, a fast-growing\neconomy. By dividing Chinese cities into two groups based on their\nadministrative levels and premium resources allocated by the central\ngovernment, we find that Chinese cities follow two distinct industrial\ndevelopment trajectories, one trajectory owning government support leads to a\ndiversified industrial structure and, thus, a diversified job market, and the\nother leads to specialty cities and, thus, a specialized job market. By\nrevisiting the automation impacts on a polarized job market, we observe a\nSimpson's paradox through which a larger city of a diversified job market\nresults in greater resilience, whereas larger cities of specialized job markets\nare more susceptible. These findings inform policy makers to deploy appropriate\npolicies to mitigate the polarized automation impacts.\n"
    },
    {
        "paper_id": 1908.0553,
        "authors": "Weipan Xu, Haohui'Caron' Chen, Enrique Frias-Martinez, Manuel Cebrian,\n  Xun Li",
        "title": "The inverted U-shaped effect of urban hotspots spatial compactness on\n  urban economic growth",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The compact city, as a sustainable concept, is intended to augment the\nefficiency of urban function. However, previous studies have concentrated more\non morphology than on structure. The present study focuses on urban structural\nelements, i.e., urban hotspots consisting of high-density and high-intensity\nsocioeconomic zones, and explores the economic performance associated with\ntheir spatial structure. We use nighttime luminosity (NTL) data and the Loubar\nmethod to identify and extract the hotspot and ultimately draw two conclusions.\nFirst, with population increasing, the hotspot number scales sublinearly with\nan exponent of approximately 0.50~0.55, regardless of the location in China,\nthe EU or the US, while the intersect values are totally different, which is\nmainly due to different economic developmental level. Secondly, we demonstrate\nthat the compactness of hotspots imposes an inverted U-shaped influence on\neconomic growth, which implies that an optimal compactness coefficient does\nexist. These findings are helpful for urban planning.\n"
    },
    {
        "paper_id": 1908.05534,
        "authors": "Frank Bosserhoff and Mitja Stadje",
        "title": "Mean-variance hedging of unit linked life insurance contracts in a\n  jump-diffusion model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a time-consistent mean-variance portfolio selection problem of an\ninsurer and allow for the incorporation of basis (mortality) risk. The optimal\nsolution is identified with a Nash subgame perfect equilibrium. We characterize\nan optimal strategy as solution of a system of partial integro-differential\nequations (PIDEs), a so called extended Hamilton-Jacobi-Bellman (HJB) system.\nWe prove that the equilibrium is necessarily a solution of the extended HJB\nsystem. Under certain conditions we obtain an explicit solution to the extended\nHJB system and provide the optimal trading strategies in closed-form. A\nsimulation shows that the previously found strategies yield payoffs whose\nexpectations and variances are robust regarding the distribution of jump sizes\nof the stock. The same phenomenon is observed when the variance is correctly\nestimated, but erroneously ascribed to the diffusion components solely.\nFurther, we show that differences in the insurance horizon and the time to\nmaturity of a longevity asset do not add to the variance of the terminal\nwealth.\n"
    },
    {
        "paper_id": 1908.0585,
        "authors": "Sander Willems",
        "title": "Linear Stochastic Dividend Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a new model for pricing stock and dividend\nderivatives. We jointly specify dynamics for the stock price and the dividend\nrate such that the stock price is positive and the dividend rate non-negative.\nIn its simplest form, the model features a dividend rate that is mean-reverting\naround a constant fraction of the stock price. The advantage of directly\nspecifying dynamics for the dividend rate, as opposed to the more common\napproach of modeling the dividend yield, is that it is easier to keep the\ndistribution of cumulative dividends tractable. The model is non-affine but\ndoes belong to the more general class of polynomial processes, which allows us\nto compute all conditional moments of the stock price and the cumulative\ndividends explicitly. In particular, we have closed-form expressions for the\nprices of stock and dividend futures. Prices of stock and dividend options are\naccurately approximated using a moment matching technique based on the\nprinciple of maximal entropy.\n"
    },
    {
        "paper_id": 1908.06207,
        "authors": "Erhan Bayraktar and Xin Zhang",
        "title": "On non-uniqueness in mean field games",
        "comments": "To appear in the Proceedings of the AMS. Keywords: Mean field game,\n  Entropy solution, master equation, Nash equilibrium, Non-uniqueness",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze an $N+1$-player game and the corresponding mean field game with\nstate space $\\{0,1\\}$. The transition rate of $j$-th player is the sum of his\ncontrol $\\alpha^j$ plus a minimum jumping rate $\\eta$. Instead of working under\nmonotonicity conditions, here we consider an anti-monotone running cost. We\nshow that the mean field game equation may have multiple solutions if $\\eta <\n\\frac{1}{2}$. We also prove that that although multiple solutions exist, only\nthe one coming from the entropy solution is charged (when $\\eta=0$), and\ntherefore resolve a conjecture of ArXiv: 1903.05788.\n"
    },
    {
        "paper_id": 1908.06355,
        "authors": "Mohammad Abedi, Daniel Bartolomeo",
        "title": "Entropic Dynamics of Stocks and European Options",
        "comments": null,
        "journal-ref": "Entropy 2019, 21(8), 765",
        "doi": "10.3390/e21080765",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an entropic framework to model the dynamics of stocks and European\nOptions. Entropic inference is an inductive inference framework equipped with\nproper tools to handle situations where incomplete information is available.\nThe objective of the paper is to lay down an alternative framework for modeling\ndynamics. An important information about the dynamics of a stock's price is\nscale invariance. By imposing the scale invariant symmetry, we arrive at\nchoosing the logarithm of the stock's price as the proper variable to model.\nThe dynamics of stock log price is derived using two pieces of information, the\ncontinuity of motion and the directionality constraint. The resulting model is\nthe same as the Geometric Brownian Motion, GBM, of the stock price which is\nmanifestly scale invariant. Furthermore, we come up with the dynamics of\nprobability density function, which is a Fokker--Planck equation. Next, we\nextend the model to value the European Options on a stock. Derivative\nsecurities ought to be prices such that there is no arbitrage. To ensure the\nno-arbitrage pricing, we derive the risk-neutral measure by incorporating the\nrisk-neutral information. Consequently, the Black--Scholes model and the\nBlack--Scholes-Merton differential equation are derived.\n"
    },
    {
        "paper_id": 1908.06358,
        "authors": "Mohammad Abedi and Daniel Bartolomeo",
        "title": "Entropic Dynamics of Exchange Rates and Options",
        "comments": null,
        "journal-ref": "Entropy 2019, 21(8), 765",
        "doi": "10.3390/e21060586",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An Entropic Dynamics of exchange rates is laid down to model the dynamics of\nforeign exchange rates, FX, and European Options on FX. The main objective is\nto represent an alternative framework to model dynamics. Entropic inference is\nan inductive inference framework equipped with proper tools to handle\nsituations where incomplete information is available. Entropic Dynamics is an\napplication of entropic inference, which is equipped with the entropic notion\nof time to model dynamics. The scale invariance is a symmetry of the dynamics\nof exchange rates, which is manifested in our formalism. To make the formalism\nmanifestly invariant under this symmetry, we arrive at choosing the logarithm\nof the exchange rate as the proper variable to model. By taking into account\nthe relevant information about the exchange rates, we derive the Geometric\nBrownian Motion, GBM, of the exchange rate, which is manifestly invariant under\nthe scale transformation. Securities should be valued such that there is no\narbitrage opportunity. To this end, we derive a risk-neutral measure to value\nEuropean Options on FX. The resulting model is the celebrated Garman-Kohlhagen\nmodel.\n"
    },
    {
        "paper_id": 1908.06731,
        "authors": "Maciej Ber\\k{e}sewicz and Greta Bia{\\l}kowska and Krzysztof\n  Marcinkowski and Magdalena Ma\\'slak and Piotr Opiela and Robert Pater and\n  Katarzyna Zadroga",
        "title": "Enhancing the Demand for Labour survey by including skills from online\n  job advertisements using model-assisted calibration",
        "comments": null,
        "journal-ref": "2021",
        "doi": "10.18148/srm/2021.v15i2.7670",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the article we describe an enhancement to the Demand for Labour (DL)\nsurvey conducted by Statistics Poland, which involves the inclusion of skills\nobtained from online job advertisements. The main goal is to provide estimates\nof the demand for skills (competences), which is missing in the DL survey. To\nachieve this, we apply a data integration approach combining traditional\ncalibration with the LASSO-assisted approach to correct representation error in\nthe online data. Faced with the lack of access to unit-level data from the DL\nsurvey, we use estimated population totals and propose a~bootstrap approach\nthat accounts for the uncertainty of totals reported by Statistics Poland. We\nshow that the calibration estimator assisted with LASSO outperforms traditional\ncalibration in terms of standard errors and reduces representation bias in\nskills observed in online job ads. Our empirical results show that online data\nsignificantly overestimate interpersonal, managerial and self-organization\nskills while underestimating technical and physical skills. This is mainly due\nto the under-representation of occupations categorised as Craft and Related\nTrades Workers and Plant and Machine Operators and Assemblers.\n"
    },
    {
        "paper_id": 1908.0689,
        "authors": "Song-Kyoo Kim",
        "title": "Advanced Mathematical Business Strategy Formulation Design",
        "comments": "The paper has been published on Mathematics",
        "journal-ref": "Mathematics 8:10 (2020), 1642",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the explicit design of strategy formulations to make\nthe best strategic choices from a conventional matrix form of representing\nstrategic choices. The explicit strategy formulation is an analytical model\nwhich is targeted to provide a mathematical strategy framework to find the best\nmoment for strategy shifting to prepare rapid market changes. This theoretical\nmodel could be adapted into practically any strategic decision making situation\nwhen a strategic formulation is described as a matrix form with quantitative\nmeasured decision parameters. Analytically tractable results are obtained by\nusing the fluctuation theory and these results are capable to predict the best\nmoments of changing strategies in a matrix form. This research helps strategy\ndecision makers who want to find the optimal moments of shifting present\nstrategies.\n"
    },
    {
        "paper_id": 1908.06927,
        "authors": "Irina Georgescu",
        "title": "Expected utility operators and coinsurance problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The expected utility operators introduced in a previous paper, offer a\nframework for a general risk aversion theory, in which risk is modelled by a\nfuzzy number $A$. In this paper we formulate a coinsurance problem in the\npossibilistic setting defined by an expected utility operator $T$. Some\nproperties of the optimal saving $T$-coinsurance rate are proved and an\napproximate calculation formula of this is established with respect to the\nArrow-Pratt index of the utility function of the policyholder, as well as the\nexpected value and the variance of a fuzzy number $A$. Various formulas of the\noptimal $T$-coinsurance rate are deduced for a few expected utility operators\nin case of a triangular fuzzy number and of some HARA and CRRA-type utility\nfunctions.\n"
    },
    {
        "paper_id": 1908.06971,
        "authors": "Nazmiye Ceren Abay, Cuneyt Gurcan Akcora, Yulia R. Gel, Umar D.\n  Islambekov, Murat Kantarcioglu, Yahui Tian, Bhavani Thuraisingham",
        "title": "ChainNet: Learning on Blockchain Graphs with Topological Features",
        "comments": "To Appear in the 2019 IEEE International Conference on Data Mining\n  (ICDM)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With emergence of blockchain technologies and the associated\ncryptocurrencies, such as Bitcoin, understanding network dynamics behind\nBlockchain graphs has become a rapidly evolving research direction. Unlike\nother financial networks, such as stock and currency trading, blockchain based\ncryptocurrencies have the entire transaction graph accessible to the public\n(i.e., all transactions can be downloaded and analyzed). A natural question is\nthen to ask whether the dynamics of the transaction graph impacts the price of\nthe underlying cryptocurrency. We show that standard graph features such as\ndegree distribution of the transaction graph may not be sufficient to capture\nnetwork dynamics and its potential impact on fluctuations of Bitcoin price. In\ncontrast, the new graph associated topological features computed using the\ntools of persistent homology, are found to exhibit a high utility for\npredicting Bitcoin price dynamics. %explain higher order interactions among the\nnodes in Blockchain graphs and can be used to build much more accurate price\nprediction models. Using the proposed persistent homology-based techniques, we\noffer a new elegant, easily extendable and computationally light approach for\ngraph representation learning on Blockchain.\n"
    },
    {
        "paper_id": 1908.07098,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Small-noise limit of the quasi-Gaussian log-normal HJM model",
        "comments": "12 pages, 2 figures",
        "journal-ref": "Operations Research Letters 2017, Volume 45, 6-11",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quasi-Gaussian HJM models are a popular approach for modeling the dynamics of\nthe yield curve. This is due to their low dimensional Markovian representation,\nwhich greatly simplifies their numerical implementation. We present a\nqualitative study of the solutions of the quasi-Gaussian log-normal HJM model.\nUsing a small-noise deterministic limit we show that the short rate may explode\nto infinity in finite time. This implies the explosion of the Eurodollar\nfutures prices in this model. We derive explicit explosion criteria under mild\nassumptions on the shape of the yield curve.\n"
    },
    {
        "paper_id": 1908.07102,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Explosion in the quasi-Gaussian HJM model",
        "comments": "21 pages, 3 figures",
        "journal-ref": "Finance and Stochastics 2018, Volume 22, Issue 3, 643-666",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the explosion of the solutions of the SDE in the quasi-Gaussian HJM\nmodel with a CEV-type volatility. The quasi-Gaussian HJM models are a popular\napproach for modeling the dynamics of the yield curve. This is due to their low\ndimensional Markovian representation which simplifies their numerical\nimplementation and simulation. We show rigorously that the short rate in these\nmodels explodes in finite time with positive probability, under certain\nassumptions for the model parameters, and that the explosion occurs in finite\ntime with probability one under some stronger assumptions. We discuss the\nimplications of these results for the pricing of the zero coupon bonds and\nEurodollar futures under this model.\n"
    },
    {
        "paper_id": 1908.07136,
        "authors": "Yixiao Li, Gloria Lin, Thomas Lau, Ruochen Zeng",
        "title": "A Review of Changepoint Detection Models",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of the change-point detection is to discover the abrupt\nproperty changes lying behind the time-series data. In this paper, we firstly\nsummarize the definition and in-depth implication of the changepoint detection.\nThe next stage is to elaborate traditional and some alternative model-based\nchangepoint detection algorithms. Finally, we try to go a bit further in the\ntheory and look into future research directions.\n"
    },
    {
        "paper_id": 1908.07244,
        "authors": "Shan Lu, Jichang Zhao and Huiwen Wang",
        "title": "The emergence of critical stocks in market crash",
        "comments": "The datasets analyzed during the current study are available in the\n  figshare.com repository, https://doi.org/10.6084/m9.figshare.8216582.v2",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In complex systems like financial market, risk tolerance of individuals is\ncrucial for system resilience.The single-security price limit, designed as risk\ntolerance to protect investors by avoiding sharp price fluctuation, is blamed\nfor feeding market panic in times of crash.The relationship between the\ncritical market confidence which stabilizes the whole system and the price\nlimit is therefore an important aspect of system resilience. Using a simplified\ndynamic model on networks of investors and stocks, an unexpected linear\nassociation between price limit and critical market confidence is theoretically\nderived and empirically verified in this paper. Our results highlight the\nimportance of relatively `small' but critical stocks that drive the system to\ncollapse by passing the failure from periphery to core. These small stocks,\nlargely originating from homogeneous investment strategies across the market,\nhas unintentionally suppressed system resilience with the exclusive increment\nof individual risk tolerance. Imposing random investment requirements to\nmitigate herding behavior can thus improve the market resilience.\n"
    },
    {
        "paper_id": 1908.07393,
        "authors": "Irvin Steve Cardenas, Jong-Hoon Kim",
        "title": "Robonomics: The Study of Robot-Human Peer-to-Peer Financial Transactions\n  and Agreements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of a blockchain has given way to the development of\ncryptocurrencies, enabled smart contracts, and unlocked a plethora of other\ndisruptive technologies. But, beyond its use case in cryptocurrencies, and in\nnetwork coordination and automation, blockchain technology may have serious\nsociotechnical implications in the future co-existence of robots and humans.\nMotivated by the recent explosion of interest around blockchains, and our\nextensive work on open-source blockchain technology and its integration into\nrobotics - this paper provides insights in ways in which blockchains and other\ndecentralized technologies can impact our interactions with robot agents and\nthe social integration of robots into human society.\n"
    },
    {
        "paper_id": 1908.07417,
        "authors": "Peter Carr and Sander Willems",
        "title": "A lognormal type stochastic volatility model with quadratic drift",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel one-factor stochastic volatility model where the\ninstantaneous volatility of the asset log-return is a diffusion with a\nquadratic drift and a linear dispersion function. The instantaneous volatility\nmean reverts around a constant level, with a speed of mean reversion that is\naffine in the instantaneous volatility level. The steady-state distribution of\nthe instantaneous volatility belongs to the class of Generalized Inverse\nGaussian distributions. We show that the quadratic term in the drift is crucial\nto avoid moment explosions and to preserve the martingale property of the stock\nprice process. Using a conveniently chosen change of measure, we relate the\nmodel to the class of polynomial diffusions. This remarkable relation allows us\nto develop a highly accurate option price approximation technique based on\northogonal polynomial expansions.\n"
    },
    {
        "paper_id": 1908.07479,
        "authors": "Alessio Arleo, Christos Tsigkanos, Chao Jia, Roger A. Leite, Ilir\n  Murturi, Manfred Klaffenboeck, Schahram Dustdar, Michael Wimmer, Silvia\n  Miksch, and Johannes Sorger",
        "title": "Sabrina: Modeling and Visualization of Economy Data with Incremental\n  Domain Knowledge",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/VISUAL.2019.8933598",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investment planning requires knowledge of the financial landscape on a large\nscale, both in terms of geo-spatial and industry sector distribution. There is\nplenty of data available, but it is scattered across heterogeneous sources\n(newspapers, open data, etc.), which makes it difficult for financial analysts\nto understand the big picture. In this paper, we present Sabrina, a financial\ndata analysis and visualization approach that incorporates a pipeline for the\ngeneration of firm-to-firm financial transaction networks. The pipeline is\ncapable of fusing the ground truth on individual firms in a region with\n(incremental) domain knowledge on general macroscopic aspects of the economy.\nSabrina unites these heterogeneous data sources within a uniform visual\ninterface that enables the visual analysis process. In a user study with three\ndomain experts, we illustrate the usefulness of Sabrina, which eases their\nanalysis process.\n"
    },
    {
        "paper_id": 1908.07626,
        "authors": "Maxim Bichuch and Jean-Pierre Fouque",
        "title": "Optimal Investment with Correlated Stochastic Volatility Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of portfolio allocation in the context of stocks evolving in\nrandom environments, that is with volatility and returns depending on random\nfactors, has attracted a lot of attention. The problem of maximizing a power\nutility at a terminal time with only one random factor can be linearized thanks\nto a classical distortion transformation. In the present paper, we address the\nsituation with several factors using a perturbation technique around the case\nwhere these factors are perfectly correlated reducing the problem to the case\nwith a single factor. Our proposed approximation requires to solve numerically\ntwo linear equations in lower dimension instead of a fully non-linear HJB\nequation. A rigorous accuracy result is derived by constructing sub- and super-\nsolutions so that their difference is at the desired order of accuracy. We\nillustrate our result with a particular model for which we have explicit\nformulas for the approximation. In order to keep the notations as explicit as\npossible, we treat the case with one stock and two factors and we describe an\nextension to the case with two stocks and two factors.\n"
    },
    {
        "paper_id": 1908.07659,
        "authors": "Spiridon Penev, Pavel Shevchenko and Wei Wu",
        "title": "Myopic robust index tracking with Bregman divergence",
        "comments": "To be published in Quantitative Finance",
        "journal-ref": null,
        "doi": "10.1080/14697688.2021.1950918",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Index tracking is a popular form of asset management. Typically, a quadratic\nfunction is used to define the tracking error of a portfolio and the look back\napproach is applied to solve the index tracking problem. We argue that a\nforward looking approach is more suitable, whereby the tracking error is\nexpressed as expectation of a function of the difference between the returns of\nthe index and of the portfolio. We also assume that there is an uncertainty in\nthe distribution of the assets, hence a robust version of the optimization\nproblem needs to be adopted. We use Bregman divergence in describing the\ndeviation between the nominal and actual distribution of the components of the\nindex. In this scenario, we derive the optimal robust index tracking strategy\nin a semi-analytical form as a solution of a system of nonlinear equations.\nSeveral numerical results are presented that allow us to compare the\nperformance of this robust strategy with the optimal non-robust strategy. We\nshow that, especially during market downturns, the robust strategy can be very\nadvantageous.\n"
    },
    {
        "paper_id": 1908.07813,
        "authors": "Takashi Shinzato",
        "title": "Relationship between optimal portfolios which can maximize and minimize\n  the expected return",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, the evaluation of the minimal investment risk of the\nquenched disordered system of a portfolio optimization problem and the\ninvestment concentration of the optimal portfolio has been actively\ninvestigated using the analysis methods of statistical mechanical informatics.\nHowever, the work to date has not sufficiently compared the optimal portfolios\nof different portfolio optimization problems. Therefore, in this paper, we use\nthe Lagrange undetermined multiplier method and replica analysis to examine the\nrelationship between the optimal portfolios of the expected return maximization\nproblem and the expected return minimization problem with constraints of budget\nand investment risk. In particular, we derive the mean square error and the\ncorrelation coefficient of the optimal portfolios of these maximization and\nminimization problems as functions of a variable (the degree of risk tolerance)\nthat can characterize the feasible subspace defined by the two constraints.\n"
    },
    {
        "paper_id": 1908.0787,
        "authors": "Felipe Del Canto M (Pontifical University of Chile, Institute of\n  Economics)",
        "title": "A complex net of intertwined complements: Measuring interdimensional\n  dependence among the poor",
        "comments": "19 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The choice of appropriate measures of deprivation, identification and\naggregation of poverty has been a challenge for many years. The works of Sen,\nAtkinson and others have been the cornerstone for most of the literature on\npoverty measuring. Recent contributions have focused in what we now know as\nmultidimensional poverty measuring. Current aggregation and identification\nmeasures for multidimensional poverty make the implicit assumption that\ndimensions are independent of each other, thus ignoring the natural dependence\nbetween them. In this article a variant of the usual method of deprivation\nmeasuring is presented. It allows the existence of the forementioned\nconnections by drawing from geometric and networking notions. This new\nmethodology relies on previous identification and aggregation methods, but with\nsmall modifications to prevent arbitrary manipulations. It is also proved that\nthis measure still complies with the axiomatic framework of its predecessor.\nMoreover, the general form of latter can be considered a particular case of\nthis new measure, although this identification is not unique.\n"
    },
    {
        "paper_id": 1908.07978,
        "authors": "G\\'abor Petneh\\'azi",
        "title": "Quantile Convolutional Neural Networks for Value at Risk Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a new method for forecasting Value at Risk.\nConvolutional neural networks can do time series forecasting, since they can\nlearn local patterns in time. A simple modification enables them to forecast\nnot the mean, but arbitrary quantiles of the distribution, and thus allows them\nto be applied to VaR-forecasting. The proposed model can learn from the price\nhistory of different assets, and it seems to produce fairly accurate forecasts.\n"
    },
    {
        "paper_id": 1908.07998,
        "authors": "Stephan Leitner and Friederike Wall",
        "title": "Decision-facilitating information in hidden-action setups: An\n  agent-based approach",
        "comments": "34 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The hidden-action model captures a fundamental problem of principal-agent\ntheory and provides an optimal sharing rule when only the outcome but not the\neffort can be observed. However, the hidden-action model builds on various\nexplicit and also implicit assumptions about the information of the contracting\nparties. This paper relaxes key assumptions regarding the availability of\ninformation included in the hidden-action model in order to study whether and,\nif so, how fast the optimal sharing rule is achieved and how this is affected\nby the various types of information employed in the principal-agent relation.\nOur analysis particularly focuses on information about the environment and\nabout feasible actions for the agent. We follow an approach to transfer\nclosed-form mathematical models into agent-based computational models and show\nthat the extent of information about feasible options to carry out a task only\nhas an impact on performance if decision makers are well informed about the\nenvironment, and that the decision whether to perform exploration or\nexploitation when searching for new feasible options only affects performance\nin specific situations. Having good information about the environment, on the\ncontrary, appears to be crucial in almost all situations.\n"
    },
    {
        "paper_id": 1908.07999,
        "authors": "Raehyun Kim, Chan Ho So, Minbyul Jeong, Sanghoon Lee, Jinkyu Kim,\n  Jaewoo Kang",
        "title": "HATS: A Hierarchical Graph Attention Network for Stock Movement\n  Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many researchers both in academia and industry have long been interested in\nthe stock market. Numerous approaches were developed to accurately predict\nfuture trends in stock prices. Recently, there has been a growing interest in\nutilizing graph-structured data in computer science research communities.\nMethods that use relational data for stock market prediction have been recently\nproposed, but they are still in their infancy. First, the quality of collected\ninformation from different types of relations can vary considerably. No\nexisting work has focused on the effect of using different types of relations\non stock market prediction or finding an effective way to selectively aggregate\ninformation on different relation types. Furthermore, existing works have\nfocused on only individual stock prediction which is similar to the node\nclassification task. To address this, we propose a hierarchical attention\nnetwork for stock prediction (HATS) which uses relational data for stock market\nprediction. Our HATS method selectively aggregates information on different\nrelation types and adds the information to the representations of each company.\nSpecifically, node representations are initialized with features extracted from\na feature extraction module. HATS is used as a relational modeling module with\ninitialized node representations. Then, node representations with the added\ninformation are fed into a task-specific layer. Our method is used for\npredicting not only individual stock prices but also market index movements,\nwhich is similar to the graph classification task. The experimental results\nshow that performance can change depending on the relational data used. HATS\nwhich can automatically select information outperformed all the existing\nmethods.\n"
    },
    {
        "paper_id": 1908.08036,
        "authors": "Yun-Cheng Tsai, Chun-Chieh Wang",
        "title": "Deep Reinforcement Learning for Foreign Exchange Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reinforcement learning can interact with the environment and is suitable for\napplications in decision control systems. Therefore, we used the reinforcement\nlearning method to establish a foreign exchange transaction, avoiding the\nlong-standing problem of unstable trends in deep learning predictions. In the\nsystem design, we optimized the Sure-Fire statistical arbitrage policy, set\nthree different actions, encoded the continuous price over a period of time\ninto a heat-map view of the Gramian Angular Field (GAF) and compared the Deep Q\nLearning (DQN) and Proximal Policy Optimization (PPO) algorithms. To test\nfeasibility, we analyzed three currency pairs, namely EUR/USD, GBP/USD, and\nAUD/USD. We trained the data in units of four hours from 1 August 2018 to 30\nNovember 2018 and tested model performance using data between 1 December 2018\nand 31 December 2018. The test results of the various models indicated that\nfavorable investment performance was achieved as long as the model was able to\nhandle complex and random processes and the state was able to describe the\nenvironment, validating the feasibility of reinforcement learning in the\ndevelopment of trading strategies.\n"
    },
    {
        "paper_id": 1908.0804,
        "authors": "Iordanis Kerenidis, Anupam Prakash, D\\'aniel Szil\\'agyi",
        "title": "Quantum Algorithms for Portfolio Optimization",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop the first quantum algorithm for the constrained portfolio\noptimization problem. The algorithm has running time $\\widetilde{O} \\left(\nn\\sqrt{r} \\frac{\\zeta \\kappa}{\\delta^2} \\log \\left(1/\\epsilon\\right) \\right)$,\nwhere $r$ is the number of positivity and budget constraints, $n$ is the number\nof assets in the portfolio, $\\epsilon$ the desired precision, and $\\delta,\n\\kappa, \\zeta$ are problem-dependent parameters related to the\nwell-conditioning of the intermediate solutions. If only a moderately accurate\nsolution is required, our quantum algorithm can achieve a polynomial speedup\nover the best classical algorithms with complexity $\\widetilde{O} \\left(\n\\sqrt{r}n^\\omega\\log(1/\\epsilon) \\right)$, where $\\omega$ is the matrix\nmultiplication exponent that has a theoretical value of around $2.373$, but is\ncloser to $3$ in practice. We also provide some experiments to bound the\nproblem-dependent factors arising in the running time of the quantum algorithm,\nand these experiments suggest that for most instances the quantum algorithm can\npotentially achieve an $O(n)$ speedup over its classical counterpart.\n"
    },
    {
        "paper_id": 1908.08127,
        "authors": "Mina Lee, Joseph Y. J. Chow, Gyugeun Yoon, Brian Yueshuai He",
        "title": "Forecasting e-scooter substitution of direct and access trips by mode\n  and distance",
        "comments": "30 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.trd.2021.102892",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An e-scooter trip model is estimated from four U.S. cities: Portland, Austin,\nChicago and New York City. A log-log regression model is estimated for\ne-scooter trips based on user age, population, land area, and the number of\nscooters. The model predicts 75K daily e-scooter trips in Manhattan for a\ndeployment of 2000 scooters, which translates to 77 million USD in annual\nrevenue. We propose a novel nonlinear, multifactor model to break down the\nnumber of daily trips by the alternative modes of transportation that they\nwould likely substitute based on statistical similarity. The model parameters\nreveal a relationship with direct trips of bike, walk, carpool, automobile and\ntaxi as well as access/egress trips with public transit in Manhattan. Our model\nestimates that e-scooters could replace 32% of carpool; 13% of bike; and 7.2%\nof taxi trips. The distance structure of revenue from access/egress trips is\nfound to differ from that of other substituted trips.\n"
    },
    {
        "paper_id": 1908.08168,
        "authors": "David Byrd and Tucker Hybinette Balch",
        "title": "Intra-day Equity Price Prediction using Deep Learning as a Measure of\n  Market Efficiency",
        "comments": "In journal submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In finance, the weak form of the Efficient Market Hypothesis asserts that\nhistoric stock price and volume data cannot inform predictions of future\nprices. In this paper we show that, to the contrary, future intra-day stock\nprices could be predicted effectively until 2009. We demonstrate this using two\ndifferent profitable machine learning-based trading strategies. However, the\neffectiveness of both approaches diminish over time, and neither of them are\nprofitable after 2009. We present our implementation and results in detail for\nthe period 2003-2017 and propose a novel idea: the use of such flexible machine\nlearning methods as an objective measure of relative market efficiency. We\nconclude with a candidate explanation, comparing our returns over time with\nhigh-frequency trading volume, and suggest concrete steps for further\ninvestigation.\n"
    },
    {
        "paper_id": 1908.08219,
        "authors": "Bartosz Bartkowski, Nils Droste, Mareike Lie{\\ss}, William\n  Sidemo-Holm, Ulrich Weller, Mark V. Brady",
        "title": "Implementing result-based agri-environmental payments by means of\n  modelling",
        "comments": null,
        "journal-ref": "Payments by modelled results: a novel design for\n  agri-environmental schemes. Land Use Policy 102: 105230 (2017)",
        "doi": "10.1016/j.landusepol.2020.105230",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  From a theoretical point of view, result-based agri-environmental payments\nare clearly preferable to action-based payments. However, they suffer from two\nmajor practical disadvantages: costs of measuring the results and payment\nuncertainty for the participating farmers. In this paper, we propose an\nalternative design to overcome these two disadvantages by means of modelling\n(instead of measuring) the results. We describe the concept of model-informed\nresult-based agri-environmental payments (MIRBAP), including a hypothetical\nexample of payments for the protection and enhancement of soil functions. We\noffer a comprehensive discussion of the relative advantages and disadvantages\nof MIRBAP, showing that it not only unites most of the advantages of\nresult-based and action-based schemes, but also adds two new advantages: the\npotential to address trade-offs among multiple policy objectives and management\nfor long-term environmental effects. We argue that MIRBAP would be a valuable\naddition to the agri-environmental policy toolbox and a reflection of recent\nadvancements in agri-environmental modelling.\n"
    },
    {
        "paper_id": 1908.08264,
        "authors": "Christian Bender and Nikolaus Schweizer",
        "title": "`Regression Anytime' with Brute-Force SVD Truncation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new least-squares Monte Carlo algorithm for the approximation of\nconditional expectations in the presence of stochastic derivative weights. The\nalgorithm can serve as a building block for solving dynamic programming\nequations, which arise, e.g., in non-linear option pricing problems or in\nprobabilistic discretization schemes for fully non-linear parabolic partial\ndifferential equations. Our algorithm can be generically applied when the\nunderlying dynamics stem from an Euler approximation to a stochastic\ndifferential equation. A built-in variance reduction ensures that the\nconvergence in the number of samples to the true regression function takes\nplace at an arbitrarily fast polynomial rate, if the problem under\nconsideration is smooth enough.\n"
    },
    {
        "paper_id": 1908.08442,
        "authors": "N. Meade and J.E. Beasley and C.J. Adcock",
        "title": "Quantitative portfolio selection: using density forecasting to find\n  consistent portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the knowledge that the ex-post performance of Markowitz efficient\nportfolios is inferior to that implied ex-ante, we make two contributions to\nthe portfolio selection literature. Firstly, we propose a methodology to\nidentify the region of risk-expected return space where ex-post performance\nmatches ex-ante estimates. Secondly, we extend ex-post efficient set\nmathematics to overcome the biases in the estimation of the ex-ante efficient\nfrontier. A density forecasting approach is used to measure the accuracy of\nex-ante estimates using the Berkowitz statistic, we develop this statistic to\nincrease its sensitivity to changes in the data generating process. The area of\nrisk-expected return space where the density forecasts are accurate, where\nex-post performance matches ex-ante estimates, is termed the consistency\nregion. Under the 'laboratory' conditions of a simulated multivariate normal\ndata set, we compute the consistency region and the estimated ex-post frontier.\nOver different sample sizes used for estimation, the behaviour of the\nconsistency region is shown to be both intuitively reasonable and to enclose\nthe estimated ex-post frontier. Using actual data from the constituents of the\nUS Dow Jones 30 index, we show that the size of the consistency region is time\ndependent and, in volatile conditions, may disappear. Using our development of\nthe Berkowitz statistic, we demonstrate the superior performance of an\ninvestment strategy based on consistent rather than efficient portfolios.\n"
    },
    {
        "paper_id": 1908.08684,
        "authors": "C.A. Valle and J.E. Beasley",
        "title": "A nonlinear optimisation model for constructing minimal drawdown\n  portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the problem of minimising drawdown in a portfolio\nof financial assets. Here drawdown represents the relative opportunity cost of\nthe single best missed trading opportunity over a specified time period. We\nformulate the problem (minimising average drawdown, maximum drawdown, or a\nweighted combination of the two) as a nonlinear program and show how it can be\npartially linearised by replacing one of the nonlinear constraints by\nequivalent linear constraints.\n  Computational results are presented (generated using the nonlinear solver\nSCIP) for three test instances drawn from the EURO STOXX 50, the FTSE 100 and\nthe S&P 500 with daily price data over the period 2010-2016. We present results\nfor long-only drawdown portfolios as well as results for portfolios with both\nlong and short positions. These indicate that (on average) our minimal drawdown\nportfolios dominate the market indices in terms of return, Sharpe ratio,\nmaximum drawdown and average drawdown over the (approximately 1800 trading day)\nout-of-sample period.\n"
    },
    {
        "paper_id": 1908.08702,
        "authors": "Oliver Braganza",
        "title": "A simple model suggesting economically rational sample-size choice\n  drives irreproducibility",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0229615",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several systematic studies have suggested that a large fraction of published\nresearch is not reproducible. One probable reason for low reproducibility is\ninsufficient sample size, resulting in low power and low positive predictive\nvalue. It has been suggested that insufficient sample-size choice is driven by\na combination of scientific competition and 'positive publication bias'. Here\nwe formalize this intuition in a simple model, in which scientists choose\neconomically rational sample sizes, balancing the cost of experimentation with\nincome from publication. Specifically, assuming that a scientist's income\nderives only from 'positive' findings (positive publication bias) and that\nindividual samples cost a fixed amount, allows to leverage basic statistical\nformulas into an economic optimality prediction. We find that if effects have\ni) low base probability, ii) small effect size or iii) low grant income per\npublication, then the rational (economically optimal) sample size is small.\nFurthermore, for plausible distributions of these parameters we find a robust\nemergence of a bimodal distribution of obtained statistical power and low\noverall reproducibility rates, both matching empirical findings. Finally, we\nexplore conditional equivalence testing as a means to align economic incentives\nwith adequate sample sizes. Overall, the model describes a simple mechanism\nexplaining both the prevalence and the persistence of small sample sizes, and\nis well suited for empirical validation. It proposes economic rationality, or\neconomic pressures, as a principal driver of irreproducibility and suggests\nstrategies to change this.\n"
    },
    {
        "paper_id": 1908.08721,
        "authors": "Anthony Strittmatter",
        "title": "Heterogeneous Earnings Effects of the Job Corps by Gender Earnings: A\n  Translated Quantile Approach",
        "comments": null,
        "journal-ref": "Labour Economics, 2019, 61",
        "doi": "10.1016/j.labeco.2019.101760",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Several studies of the Job Corps tend to nd more positive earnings effects\nfor males than for females. This effect heterogeneity favouring males contrasts\nwith the results of the majority of other training programmes' evaluations.\nApplying the translated quantile approach of Bitler, Hoynes, and Domina (2014),\nI investigate a potential mechanism behind the surprising findings for the Job\nCorps. My results provide suggestive evidence that the effect of heterogeneity\nby gender operates through existing gender earnings inequality rather than Job\nCorps trainability differences.\n"
    },
    {
        "paper_id": 1908.08777,
        "authors": "Knut Aase, Bernt {\\O}ksendal",
        "title": "Strategic Insider Trading Equilibrium with a Non-fiduciary Market Maker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The continuous-time version of Kyle's (1985) model is studied, in which\nmarket makers are not fiduciaries. They have some market power which they\nutilize to set the price to their advantage, resulting in positive expected\nprofits. This has several implications for the equilibrium, the most important\nbeing that by setting a modest fee conditional of the order flow, the market\nmaker is able to obtain a profit of the order of magnitude, and even better\nthan, a perfectly informed insider. Our model also indicates why speculative\nprices are more volatile than predicted by fundamentals.\n"
    },
    {
        "paper_id": 1908.08786,
        "authors": "Ryosuke Ishii and Kuninori Nakagawa",
        "title": "Government Expenditure on Research Plans and their Diversity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we consider research and development investment by the\ngovernment. Our study is motivated by the bias in the budget allocation owing\nto the competitive funding system. In our model, each researcher presents\nresearch plans and expenses, and the government selects a research plan in two\nperiods---before and after the government knows its favorite plan---and spends\nfunds on the adopted program in each period. We demonstrate that, in a subgame\nperfect equilibrium, the government adopts equally as many active plans as\npossible. In an equilibrium, the selected plans are distributed proportionally.\nThus, the investment in research projects is symmetric and unbiased. Our\nresults imply that equally widespread expenditure across all research fields is\nbetter than the selection of and concentration in some specific fields.\n"
    },
    {
        "paper_id": 1908.088,
        "authors": "John Stachurski, Junnan Zhang",
        "title": "Dynamic Programming with State-Dependent Discounting",
        "comments": "44 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the core results of discrete time infinite horizon dynamic\nprogramming to the case of state-dependent discounting. We obtain a condition\non the discount factor process under which all of the standard optimality\nresults can be recovered. We also show that the condition cannot be\nsignificantly weakened. Our framework is general enough to handle complications\nsuch as recursive preferences and unbounded rewards. Economic and financial\napplications are discussed.\n"
    },
    {
        "paper_id": 1908.08806,
        "authors": "Christian Bayer, Blanka Horvath, Aitor Muguruza, Benjamin Stemper and\n  Mehdi Tomas",
        "title": "On deep calibration of (rough) stochastic volatility models",
        "comments": "arXiv admin note: text overlap with arXiv:1901.09647",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Techniques from deep learning play a more and more important role for the\nimportant task of calibration of financial models. The pioneering paper by\nHernandez [Risk, 2017] was a catalyst for resurfacing interest in research in\nthis area. In this paper we advocate an alternative (two-step) approach using\ndeep learning techniques solely to learn the pricing map -- from model\nparameters to prices or implied volatilities -- rather than directly the\ncalibrated model parameters as a function of observed market data. Having a\nfast and accurate neural-network-based approximating pricing map (first step),\nwe can then (second step) use traditional model calibration algorithms. In this\nwork we showcase a direct comparison of different potential approaches to the\nlearning stage and present algorithms that provide a suffcient accuracy for\npractical use. We provide a first neural network-based calibration method for\nrough volatility models for which calibration can be done on the y. We\ndemonstrate the method via a hands-on calibration engine on the rough Bergomi\nmodel, for which classical calibration techniques are diffcult to apply due to\nthe high cost of all known numerical pricing methods. Furthermore, we display\nand compare different types of sampling and training methods and elaborate on\ntheir advantages under different objectives. As a further application we use\nthe fast pricing method for a Bayesian analysis of the calibrated model.\n"
    },
    {
        "paper_id": 1908.08954,
        "authors": "Xi Kleisinger-Yu, Vlatka Komaric, Martin Larsson, Markus Regez",
        "title": "A multi-factor polynomial framework for long-term electricity forwards\n  with delivery period",
        "comments": "Forthcoming in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a multi-factor polynomial framework to model and hedge long-term\nelectricity contracts with delivery period. This framework has several\nadvantages: the computation of forwards, risk premium and correlation between\ndifferent forwards are fully explicit, and the model can be calibrated to\nobserved electricity forward curves easily and well. Electricity markets suffer\nfrom non-storability and poor medium- to long-term liquidity. Therefore, we\nsuggest a rolling hedge which only uses liquid forward contracts and is\nrisk-minimizing in the sense of F\\\"ollmer and Schweizer. We calibrate the model\nto over eight years of German power calendar year forward curves and\ninvestigate the quality of the risk-minimizing hedge over various time\nhorizons.\n"
    },
    {
        "paper_id": 1908.0958,
        "authors": "Fehmina Malik and Manjesh K.~Hanawal and Yezekael Hayel and\n  Jayakrishnan Nair",
        "title": "Revenue Sharing in the Internet: A Moral Hazard Approach and a\n  Net-neutrality Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Revenue sharing contracts between Content Providers (CPs) and Internet\nService Providers (ISPs) can act as leverage for enhancing the infrastructure\nof the Internet. ISPs can be incentivized to make investments in network\ninfrastructure that improve Quality of Service (QoS) for users if attractive\ncontracts are negotiated between them and CPs. The idea here is that part of\nthe net profit gained by CPs are given to ISPs to invest in the network. The\nMoral Hazard economic framework is used to model such an interaction, in which\na principal determines a contract, and an agent reacts by adapting her effort.\nIn our setting, several competitive CPs interact through one common ISP. Two\ncases are studied: (i) the ISP differentiates between the CPs and makes a\n(potentially) different investment to improve the QoS of each CP, and (ii) the\nISP does not differentiate between CPs and makes a common investment for both.\nThe last scenario can be viewed as \\emph{network neutral behavior} on the part\nof the ISP. We analyse the optimal contracts and show that the CP that can\nbetter monetize its demand always prefers the non-neutral regime.\nInterestingly, ISP revenue, as well as social utility, are also found to be\nhigher under the non-neutral regime.\n"
    },
    {
        "paper_id": 1908.09609,
        "authors": "Anthony Strittmatter, Michael Lechner",
        "title": "Sorting on the Used-Car Market After the Volkswagen Emission Scandal",
        "comments": null,
        "journal-ref": "Journal of Environmental Economics and Management, 2020, 101",
        "doi": "10.1016/j.jeem.2020.102305",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The disclosure of the VW emission manipulation scandal caused a\nquasi-experimental market shock to the observable environmental quality of VW\ndiesel vehicles. To investigate the market reaction to this shock, we collect\ndata from a used-car online advertisement platform. We find that the supply of\nused VW diesel vehicles increases after the VW emission scandal. The positive\nsupply side effects increase with the probability of manipulation. Furthermore,\nwe find negative impacts on the asking prices of used cars subject to a high\nprobability of manipulation. We rationalize these findings with a model for\nsorting by the environmental quality of used cars.\n"
    },
    {
        "paper_id": 1908.0964,
        "authors": "Kenji Nagami",
        "title": "Expansion method for pricing foreign exchange options under stochastic\n  volatility and interest rates",
        "comments": "20 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some expansion methods have been proposed for approximately pricing options\nwhich has no exact closed formula. Benhamou et al. (2010) presents the smart\nexpansion method that directly expands the expectation value of payoff function\nwith respect to the volatility of volatility, then uses it to price options in\nthe stochastic volatility model. In this paper, we apply their method to the\nstochastic volatility model with stochastic interest rates, and present the\nexpansion formula for pricing options up to the second order. Then the\nnumerical studies are performed to compare our approximation formula with the\nMonte-Carlo simulation. It is found that our formula shows the numerically\ncomparable results with the method proposed by Grzelak et al. (2012) which uses\nthe approximation of characteristic function.\n"
    },
    {
        "paper_id": 1908.09686,
        "authors": "Zionam E. L. Rolim, Rafa\\\"el R. de Oliveira, and H\\'elio M. de\n  Oliveira",
        "title": "Industrial Concentration of the Brazilian Automobile Market and\n  Positioning in the World Market",
        "comments": "10 pages, 5 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper surveys the evolution of industrial concentration of the Brazilian\nautomotive market as well as its positioning in the worldmarket. Data available\nby OICA (International Organization of Motor Vehicle Manufacturers) were used\nto better understand the characteristics of the Brazilian market on the world\nstage. A cluster analysis algorithm (by the k-means technique) ranks Brazil\nwith a concentration profile in a group of countries like US and South Korea,\nin contrast to countries such as Germany, Canada and Japan, or even France and\nItaly. It is rather usual to characterize the market structure through\nindustrial concentration indices: we revisit CR ratios (concentration ratios),\nHHI (Herfindahl-Hirschman index), B (Rosenbluth index), and CCI (Horvath\ncomprehensive concentration index). Data of Anfavea-Brazil (Associacao Nacional\ndos Fabricantes de Veiculos Automotores) were used to estimate these indices in\nthe period 2012-2018 for the national automobile industry. The values obtained\nindicate that by 1998 the automotive sector was behaving as an\noligopoly-differentiated. However, the values of more recent periods\n(particularly CR4) strongly indicate that the sector is currently moderately\nconcentrated and is changing for a quasi-devolved market.\n"
    },
    {
        "paper_id": 1908.09706,
        "authors": "Tomoya Mori",
        "title": "Spatial pattern and city size distribution",
        "comments": "28 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many large cities are found at locations with certain first nature\nadvantages. Yet, those exogenous locational features may not be the most potent\nforces governing the spatial pattern of cities. In particular, population size,\nspacing and industrial composition of cities exhibit simple, persistent and\nmonotonic relationships. Theories of economic agglomeration suggest that this\nregularity is a consequence of interactions between endogenous agglomeration\nand dispersion forces. This paper reviews the extant formal models that explain\nthe spatial pattern together with the size distribution of cities, and\ndiscusses the remaining research questions to be answered in this literature.\nTo obtain results about explicit spatial patterns of cities, a model needs to\ndepart from the most popular two-region and systems-of-cities frameworks in\nurban and regional economics in which there is no variation in interregional\ndistance. This is one of the major reasons that only few formal models have\nbeen proposed in this literature. To draw implications as much as possible from\nthe extant theories, this review involves extensive discussions on the behavior\nof the many-region extension of these models. The mechanisms that link the\nspatial pattern of cities and the diversity in city sizes are also discussed in\ndetail.\n"
    },
    {
        "paper_id": 1908.09857,
        "authors": "Marek Capi\\'nski and Tomasz Zastawniak",
        "title": "Construction of Martingale Measure in the Hazard Process Model of Credit\n  Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In credit risk literature, the existence of an equivalent martingale measure\nis stipulated as one of the main assumptions in the hazard process model. Here\nwe show by construction the existence of a measure that turns the discounted\nstock and defaultable bond prices into martingales by identifying a\nno-arbitrage condition, in as weak a sense as possible, which facilitates such\na construction.\n"
    },
    {
        "paper_id": 1908.09976,
        "authors": "Andreas Lichtenstern, Pavel V. Shevchenko and Rudi Zagst",
        "title": "Optimal life-cycle consumption and investment decisions under\n  age-dependent risk preferences",
        "comments": null,
        "journal-ref": "Mathematics and Financial Economics, 2020",
        "doi": "10.1007/s11579-020-00276-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we solve the problem of maximizing the expected utility of\nfuture consumption and terminal wealth to determine the optimal pension or\nlife-cycle fund strategy for a cohort of pension fund investors. The setup is\nstrongly related to a DC pension plan where additionally (individual)\nconsumption is taken into account. The consumption rate is subject to a\ntime-varying minimum level and terminal wealth is subject to a terminal floor.\nMoreover, the preference between consumption and terminal wealth as well as the\nintertemporal coefficient of risk aversion are time-varying and therefore\ndepend on the age of the considered pension cohort. The optimal consumption and\ninvestment policies are calculated in the case of a Black-Scholes financial\nmarket framework and hyperbolic absolute risk aversion (HARA) utility\nfunctions. We generalize Ye (2008) (2008 American Control Conference, 356-362)\nby adding an age-dependent coefficient of risk aversion and extend Steffensen\n(2011) (Journal of Economic Dynamics and Control, 35(5), 659-667), Hentschel\n(2016) (Doctoral dissertation, Ulm University) and Aase (2017) (Stochastics,\n89(1), 115-141) by considering consumption in combination with terminal wealth\nand allowing for consumption and terminal wealth floors via an application of\nHARA utility functions. A case study on fitting several models to realistic,\ntime-dependent life-cycle consumption and relative investment profiles shows\nthat only our extended model with time-varying preference parameters provides\nsufficient flexibility for an adequate fit. This is of particular interest to\nlife-cycle products for (private) pension investments or pension insurance in\ngeneral.\n"
    },
    {
        "paper_id": 1908.10014,
        "authors": "Vikenty Mikheev and Serge E. Miheev",
        "title": "Christmas Jump in LIBOR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A short-term pattern in LIBOR dynamics was discovered. Namely, 2-month LIBOR\nexperiences a jump after Xmas. The sign and size of the jump depend on the data\ntrend on 21 days before Xmas.\n"
    },
    {
        "paper_id": 1908.10065,
        "authors": "Matthias Jordan, Volker Lenz, Markus Millinger, Katja Oehmichen,\n  Daniela Thr\\\"an",
        "title": "Future competitive bioenergy technologies in the German heat sector:\n  Findings from an economic optimization approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.energy.2019.116194",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Meeting the defined greenhouse gas (GHG) reduction targets in Germany is only\npossible by switching to renewable technologies in the energy sector. A major\nshare of that reduction needs to be covered by the heat sector, which accounts\nfor ~35% of the energy based emissions in Germany. Biomass is the renewable key\nplayer in the heterogeneous heat sector today. Its properties such as weather\nindependency, simple storage and flexible utilization open up a wide field of\napplications for biomass. However, in a future heat sector fulfilling GHG\nreduction targets and energy sectors being increasingly connected: which\nbioenergy technology concepts are competitive options against other renewable\nheating systems? In this paper, the cost optimal allocation of the limited\nGerman biomass potential is investigated under longterm scenarios using a\nmathematical optimization approach. The model results show that bioenergy can\nbe a competitive option in the future. Especially the use of biomass from\nresidues can be highly competitive in hybrid combined heat and power (CHP)\npellet combustion plants in the private household sector. However, towards\n2050, wood based biomass use in high temperature industry applications is found\nto be the most cost efficient way to reduce heat based emissions by 95% in\n2050.\n"
    },
    {
        "paper_id": 1908.10119,
        "authors": "Philipp Kluschke, Fabian Neumann",
        "title": "Interaction of a Hydrogen Refueling Station Network for Heavy-Duty\n  Vehicles and the Power System in Germany for 2050",
        "comments": "41 pages, 15 figures, 6 tables",
        "journal-ref": null,
        "doi": "10.1016/j.trd.2020.102358",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A potential solution to reduce greenhouse gas (GHG) emissions in the\ntransport sector is to use alternatively fueled vehicles (AFV). Heavy-duty\nvehicles (HDV) emit a large share of GHG emissions in the transport sector and\nare therefore the subject of growing attention from global regulators. Fuel\ncell and green hydrogen technologies are a promising option to decarbonize\nHDVs, as their fast refueling and long vehicle ranges are in line with current\nlogistic operation concepts. Moreover, the application of green hydrogen in\ntransport could enable more effective integration of renewable energies (RE)\nacross different energy sectors. This paper explores the interplay between HDV\nHydrogen Refueling Stations (HRS) that produce hydrogen locally and the power\nsystem by combining an infrastructure location planning model and an energy\nsystem optimization model that takes grid expansion options into account. Two\nscenarios - one sizing refueling stations in symbiosis with the power system\nand one sizing them independently of it - are assessed regarding their impacts\non the total annual energy system costs, regional RE integration and the\nlevelized cost of hydrogen (LCOH). The impacts are calculated based on\nlocational marginal pricing for 2050. Depending on the integration scenario, we\nfind average LCOH of between 5.66 euro/kg and 6.20 euro/kg, for which nodal\nelectricity prices are the main determining factor as well as a strong\ndifference in LCOH between north and south Germany. From a system perspective,\ninvesting in HDV-HRS in symbiosis with the power system rather than\nindependently promises cost savings of around one billion-euros per annum. We\ntherefore conclude that the co-optimization of multiple energy sectors is\nimportant for investment planning and has the potential to exploit synergies.\n"
    },
    {
        "paper_id": 1908.10242,
        "authors": "Stephan Eckstein, Michael Kupper",
        "title": "Martingale transport with homogeneous stock movements",
        "comments": "20 pages, 2 figures",
        "journal-ref": "Quantitative Finance (2021), 21:2, 271-280",
        "doi": "10.1080/14697688.2020.1787493",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a variant of the martingale optimal transport problem in a\nmulti-period setting to derive robust price bounds of a financial derivative.\nOn top of marginal and martingale constraints, we introduce a time-homogeneity\nassumption, which restricts the variability of the forward-looking transitions\nof the martingale across time. We provide a dual formulation in terms of\nsuperhedging and discuss relaxations of the time-homogeneity assumption by\nadding market frictions. In financial terms, the introduced time-homogeneity\ncorresponds to a time-consistency condition for call prices, given the state of\nthe stock. The time homogeneity assumption leads to improved price bounds as\nmarket data from many time points can be incorporated effectively. The approach\nis illustrated with two numerical examples.\n"
    },
    {
        "paper_id": 1908.10557,
        "authors": "Tomoo Kikuchi, Kazuo Nishimura, John Stachurski, Junnan Zhang",
        "title": "Coase Meets Bellman: Dynamic Programming for Production Networks",
        "comments": "32 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that competitive equilibria in a range of models related to\nproduction networks can be recovered as solutions to dynamic programs. Although\nthese programs fail to be contractive, we prove that they are tractable. As an\nillustration, we treat Coase's theory of the firm, equilibria in production\nchains with transaction costs, and equilibria in production networks with\nmultiple partners. We then show how the same techniques extend to other\nequilibrium and decision problems, such as the distribution of management\nlayers within firms and the spatial distribution of cities.\n"
    },
    {
        "paper_id": 1908.1068,
        "authors": "Jean-Bernard Chatelain (PSE), Kirsten Ralf",
        "title": "Publish and Perish: Creative Destruction and Macroeconomic Theory",
        "comments": null,
        "journal-ref": "History of Economic Ideas, Istituti editoriali e poligrafici\n  internazionali, 2018, 46 (2), pp.65-101",
        "doi": "10.19272/201806102004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A number of macroeconomic theories, very popular in the 1980s, seem to have\ncompletely disappeared and been replaced by the dynamic stochastic general\nequilibrium (DSGE) approach. We will argue that this replacement is due to a\ntacit agreement on a number of assumptions, previously seen as mutually\nexclusive, and not due to a settlement by 'nature'. As opposed to econometrics\nand microeconomics and despite massive progress in the access to data and the\nuse of statistical software, macroeconomic theory appears not to be a\ncumulative science so far. Observational equivalence of different models and\nthe problem of identification of parameters of the models persist as will be\nhighlighted by examining two examples: one in growth theory and a second in\ntesting inflation persistence.\n"
    },
    {
        "paper_id": 1908.10771,
        "authors": "Haoqian Li and Thomas Lau",
        "title": "Reinforcement Learning: Prediction, Control and Value Function\n  Approximation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the increasing power of computers and the rapid development of\nself-learning methodologies such as machine learning and artificial\nintelligence, the problem of constructing an automatic Financial Trading\nSystems (FTFs) becomes an increasingly attractive research topic. An intuitive\nway of developing such a trading algorithm is to use Reinforcement Learning\n(RL) algorithms, which does not require model-building. In this paper, we dive\ninto the RL algorithms and illustrate the definitions of the reward function,\nactions and policy functions in details, as well as introducing algorithms that\ncould be applied to FTFs.\n"
    },
    {
        "paper_id": 1908.10916,
        "authors": "Haoyang Cao and Xin Guo",
        "title": "MFGs for partially reversible investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes a class of infinite-time-horizon stochastic games with\nsingular controls motivated from the partially reversible problem. It provides\nan explicit solution for the mean-field game (MFG) and presents sensitivity\nanalysis to compare the solution for the MFG with that for the single-agent\ncontrol problem. It shows that in the MFG, model parameters not only affect the\noptimal strategies as in the single-agent case, but also influence the\nequilibrium price. It then establishes that the solution to the MFG is an\n$\\epsilon$-Nash Equilibrium to the corresponding $N$-player game, with\n$\\epsilon=O\\left(\\frac{1}{\\sqrt N}\\right)$.\n"
    },
    {
        "paper_id": 1908.11099,
        "authors": "Kosiorowski Daniel, Jerzy P. Rydlewski",
        "title": "Centrality-oriented Causality -- A Study of EU Agricultural Subsidies\n  and Digital Developement in Poland",
        "comments": null,
        "journal-ref": "Operations Research and Decisions, 2020, vol. 30, no. 3, pp. 47-63",
        "doi": "10.37190/ord200303",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Results of a convincing causal statistical inference related to\nsocio-economic phenomena are treated as especially desired background for\nconducting various socio-economic programs or government interventions.\nUnfortunately, quite often real socio-economic issues do not fulfill\nrestrictive assumptions of procedures of causal analysis proposed in the\nliterature. This paper indicates certain empirical challenges and conceptual\nopportunities related to applications of procedures of data depth concept into\na process of causal inference as to socio-economic phenomena. We show, how to\napply a statistical functional depths in order to indicate factual and\ncounterfactual distributions commonly used within procedures of causal\ninference. Thus a modification of Rubin causality concept is proposed, i.e., a\ncentrality-oriented causality concept. The presented framework is especially\nuseful in a context of conducting causal inference basing on official\nstatistics, i.e., basing on already existing databases. Methodological\nconsiderations related to extremal depth, modified band depth, Fraiman-Muniz\ndepth, and multivariate Wilcoxon sum rank statistic are illustrated by means of\nexample related to a study of an impact of EU direct agricultural subsidies on\na digital development in Poland in a period of 2012-2019.\n"
    },
    {
        "paper_id": 1908.11204,
        "authors": "C.M. Rodr\\'iguez-Mart\\'inez, H.F. Coronel-Brizio, A.R.\n  Hern\\'andez-Montoya",
        "title": "A multi-scale symmetry analysis of uninterrupted trends returns of daily\n  financial indices",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications , Vol. 574\n  Elsevier BV p. 125982 (2021-07)",
        "doi": "10.1016/j.physa.2021.125982",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a symmetry analysis of the distribution of variations of different\nfinancial indices, by means of a statistical procedure developed by the authors\nbased on a symmetry statistic by Einmahl and Mckeague. We applied this\nstatistical methodology to financial uninterrupted daily trends returns and to\nother derived observable. In our opinion, to study distributional symmetry,\ntrends returns offer more advantages than the commonly used daily financial\nreturns; the two most important being: 1) Trends returns involve sampling over\ndifferent time scales and 2) By construction, this variable time series\ncontains practically the same number of non-negative and negative entry values.\nWe also show that these time multi-scale returns display distributional\nbi-modality. Daily financial indices analyzed in this work, are the Mexican\nIPC, the American DJIA, DAX from Germany and the Japanese Market index Nikkei,\ncovering a time period from 11-08-1991 to 06-30-2017. We show that, at the time\nscale resolution and significance considered in this paper, it is almost always\nfeasible to find an interval of possible symmetry points containing one most\nplausible symmetry point denoted by C. Finally, we study the temporal evolution\nof C showing that this point is seldom zero and responds with sensitivity to\nextreme market events.\n"
    },
    {
        "paper_id": 1908.11212,
        "authors": "Kerda Varaku",
        "title": "Stock Price Forecasting and Hypothesis Testing Using Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we use Recurrent Neural Networks and Multilayer Perceptrons to\npredict NYSE, NASDAQ and AMEX stock prices from historical data. We experiment\nwith different architectures and compare data normalization techniques. Then,\nwe leverage those findings to question the efficient-market hypothesis through\na formal statistical test.\n"
    },
    {
        "paper_id": 1908.11433,
        "authors": "Sheida Hasani, Razieh Masoomi, Jamshid Ardalankia, Mohammadbashir\n  Sedighi, Hamid Jafari",
        "title": "Growth Dynamics of Value and Cost Trade-off in Temporal Networks",
        "comments": "6 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The question is: What does happen to the real-world networks which cause them\nnot to grow permanently? The idea here is that real-world networks have to pay\nthe cost of growth. We investigate the growth and trade-off between value and\ncost in the networks with cost and preferential attachment together. Since the\npreferential attachment in the BA model does not consider any stop against the\ninfinite growth of networks, we introduce a modified version of preferential\nattachment of the BA model. This idea makes sense because the growth of real\nnetworks may be finite. In the present study, by combining preferential\nattachment in the science of temporal networks (interval graphs), and, the\nfirst-order differential equations of value and cost of making links, the\nfuture equilibrium of an evolving network is illustrated. During the process of\nachieving a winning position, the variables against growth such as the\ncompetition cost, besides the internally structural cost may emerge. In the\nend, by applying this modified model, we found the circumstances in which a\ntrade-off between value and cost emerges.\n"
    },
    {
        "paper_id": 1908.11492,
        "authors": "Bastian Breitmayer, Tim Hasso, Matthias Pelster",
        "title": "Culture and the disposition effect",
        "comments": "11",
        "journal-ref": "Economics Letters (2019)",
        "doi": "10.1016/j.econlet.2019.108653",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the relationship between national culture and the disposition effect\nby investigating international differences in the degree of investors'\ndisposition effect. We utilize brokerage data of 387,993 traders from 83\ncountries and find great variation in the degree of the disposition effect\nacross the world. We find that the cultural dimensions of long-term orientation\nand indulgence help to explain why certain nationalities are more prone to the\ndisposition effect. We also find support on an international level for the role\nof age and gender in explaining the disposition effect.\n"
    },
    {
        "paper_id": 1908.11498,
        "authors": "Stefania Albanesi and Domonkos F. Vamossy",
        "title": "Predicting Consumer Default: A Deep Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model to predict consumer default based on deep learning. We\nshow that the model consistently outperforms standard credit scoring models,\neven though it uses the same data. Our model is interpretable and is able to\nprovide a score to a larger class of borrowers relative to standard credit\nscoring models while accurately tracking variations in systemic risk. We argue\nthat these properties can provide valuable insights for the design of policies\ntargeted at reducing consumer default and alleviating its burden on borrowers\nand lenders, as well as macroprudential regulation.\n"
    },
    {
        "paper_id": 1909.00024,
        "authors": "M. Keith Chen, Kareem Haggag, Devin G. Pope, and Ryne Rohla",
        "title": "Racial Disparities in Voting Wait Times: Evidence from Smartphone Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Equal access to voting is a core feature of democratic government. Using data\nfrom millions of smartphone users, we quantify a racial disparity in voting\nwait times across a nationwide sample of polling places during the 2016 U.S.\npresidential election. Relative to entirely-white neighborhoods, residents of\nentirely-black neighborhoods waited 29% longer to vote and were 74% more likely\nto spend more than 30 minutes at their polling place. This disparity holds when\ncomparing predominantly white and black polling places within the same states\nand counties, and survives numerous robustness and placebo tests. We shed light\non the mechanism for these results and discuss how geospatial data can be an\neffective tool to both measure and monitor these disparities going forward.\n"
    },
    {
        "paper_id": 1909.00344,
        "authors": "EunJeong Hwang and Yong-Hyuk Kim",
        "title": "Interdependency between the Stock Market and Financial News",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock prices are driven by various factors. In particular, many individual\ninvestors who have relatively little financial knowledge rely heavily on the\ninformation from news stories when making investment decisions in the stock\nmarket. However, these stories may not reflect future stock prices because of\nthe subjectivity in the news; stock prices may instead affect the news\ncontents. This study aims to discover whether it is news or stock prices that\nhave a greater impact on the other. To achieve this, we analyze the\nrelationship between news sentiment and stock prices based on time series\nanalysis using five different classification models. Our experimental results\nshow that stock prices have a bigger impact on the news contents than news does\non stock prices.\n"
    },
    {
        "paper_id": 1909.00354,
        "authors": "Andreas H Hamel, Birgit Rudloff, Zhou Zhou",
        "title": "Robust no arbitrage and the solvability of vector-valued utility\n  maximization problems",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A market model with $d$ assets in discrete time is considered where trades\nare subject to proportional transaction costs given via bid-ask spreads, while\nthe existence of a num\\`eraire is not assumed. It is shown that robust no\narbitrage holds if, and only if, there exists a Pareto solution for some\nvector-valued utility maximization problem with component-wise utility\nfunctions. Moreover, it is demonstrated that a consistent price process can be\nconstructed from the Pareto maximizer.\n"
    },
    {
        "paper_id": 1909.00386,
        "authors": "Du Nguyen",
        "title": "Vector Autoregressive Moving Average Model with Scalar Moving Average",
        "comments": "4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show Vector Autoregressive Moving Average models with scalar Moving\nAverage components could be estimated by generalized least square (GLS) for\neach fixed moving average polynomial. The conditional variance of the GLS model\nis the concentrated covariant matrix of the moving average process. Under GLS\nthe likelihood function of these models has similar format to their VAR\ncounterparts. Maximum likelihood estimate can be done by optimizing with\ngradient over the moving average parameters. These models are inexpensive\ngeneralizations of Vector Autoregressive models. We discuss a relationship\nbetween this result and the Borodin-Okounkov formula in operator theory.\n"
    },
    {
        "paper_id": 1909.00508,
        "authors": "Nathan Dahlin and Rahul Jain",
        "title": "Two-Stage Electricity Markets with Renewable Energy Integration: Market\n  Mechanisms and Equilibrium Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a two-stage market mechanism for trading electricity including\nrenewable generation as an alternative to the widely used multi-settlement\nmarket structure. The two-stage market structure allows for recourse decisions\nby the market operator, which are not possible in today's markets. We allow for\ndifferent conventional generation cost curves in the forward and the real-time\nstages. We have considered costs of demand response programs and black outs,\nand adopt a DC power flow model to account for network constraints. Our first\nresult is to show existence (by construction) of a sequential competitive\nequilibrium (SCEq) in such a two-stage market. We argue social welfare\nproperties of such an SCEq, and then design a market mechanism that achieves\nsocial welfare maximization when the market participants are non-strategic. We\nalso show that under either a congestion-free or a monopoly-free condition, an\nefficient Nash equilibrium exists.\n"
    },
    {
        "paper_id": 1909.0057,
        "authors": "P. Liebrich",
        "title": "A Relation between Short-Term and Long-Term Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work a relation between a measure of short-term arbitrage in the\nmarket and the excess growth of portfolios as a notion of long-term arbitrage\nis established. The former originates from \"Geometric Arbitrage Theory\" and the\nlatter from \"Stochastic Portfolio Theory\". Both aim to describe non-equilibrium\neffects in financial markets. Thereby, a connection between two different\ntheoretical frameworks of arbitrage is drawn.\n"
    },
    {
        "paper_id": 1909.00698,
        "authors": "Denis Belomestny and Leonid Iosipoi",
        "title": "Fourier transform MCMC, heavy tailed distributions and geometric\n  ergodicity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markov Chain Monte Carlo methods become increasingly popular in applied\nmathematics as a tool for numerical integration with respect to complex and\nhigh-dimensional distributions. However, application of MCMC methods to heavy\ntailed distributions and distributions with analytically intractable densities\nturns out to be rather problematic. In this paper, we propose a novel approach\ntowards the use of MCMC algorithms for distributions with analytically known\nFourier transforms and, in particular, heavy tailed distributions. The main\nidea of the proposed approach is to use MCMC methods in Fourier domain to\nsample from a density proportional to the absolute value of the underlying\ncharacteristic function. A subsequent application of the Parseval's formula\nleads to an efficient algorithm for the computation of integrals with respect\nto the underlying density. We show that the resulting Markov chain in Fourier\ndomain may be geometrically ergodic even in the case of heavy tailed original\ndistributions. We illustrate our approach by several numerical examples\nincluding multivariate elliptically contoured stable distributions.\n"
    },
    {
        "paper_id": 1909.00748,
        "authors": "Ulrich Horst, Xiaonyu Xia, Chao Zhou",
        "title": "Portfolio liquidation under factor uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal liquidation problem under the ambiguity with respect to\nprice impact parameters. Our main results show that the value function and the\noptimal trading strategy can be characterized by the solution to a semi-linear\nPDE with superlinear gradient, monotone generator and singular terminal value.\nWe also establish an asymptotic analysis of the robust model for small amount\nof uncertainty and analyse the effect of robustness on optimal trading\nstrategies and liquidation costs. In particular, in our model ambiguity\naversion is observationally equivalent to increased risk aversion. This\nsuggests that ambiguity aversion increases liquidation rates.\n"
    },
    {
        "paper_id": 1909.00822,
        "authors": "Yasuyuki Kusuda",
        "title": "Buy-Online-and-Pick-up-in-Store in Omnichannel Retailing",
        "comments": "9 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the model of Gao and Su (2016) and consider an\nomnichannel strategy in which inventory can be replenished when a retailer\nsells only in physical stores. With \"buy-online-and-pick-up-in-store\" (BOPS)\nhaving been introduced, consumers can choose to buy directly online, buy from a\nretailer using BOPS, or go directly to a store to make purchases without using\nBOPS. The retailer is able to select the inventory level to maximize the\nprobability of inventory availability at the store. Furthermore, the retailer\ncan incur an additional cost to reduce the BOPS ordering lead time, which\nresults in a lowered hassle cost for consumers who use BOPS. In conclusion, we\nfound that there are two types of equilibrium: that in which all consumers go\ndirectly to the store without using BOPS and that in which all consumers use\nBOPS.\n"
    },
    {
        "paper_id": 1909.01112,
        "authors": "Erhan Bayraktar, Jingjie Zhang, Zhou Zhou",
        "title": "Equilibrium concepts for time-inconsistent stopping problems in\n  continuous time",
        "comments": "Final version. To appear in Mathematical Finance.\n  Keywords:Time-inconsistency, optimal stopping, strong equilibria, weak\n  equilibria, mild equilibria, non-exponential discounting, subgame perfect\n  Nash equilibrium",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A \\emph{new} notion of equilibrium, which we call \\emph{strong equilibrium},\nis introduced for time-inconsistent stopping problems in continuous time.\nCompared to the existing notions introduced in ArXiv: 1502.03998 and ArXiv:\n1709.05181, which in this paper are called \\emph{mild equilibrium} and\n\\emph{weak equilibrium} respectively, a strong equilibrium captures the idea of\nsubgame perfect Nash equilibrium more accurately. When the state process is a\ncontinuous-time Markov chain and the discount function is log sub-additive, we\nshow that an optimal mild equilibrium is always a strong equilibrium. Moreover,\nwe provide a new iteration method that can directly construct an optimal mild\nequilibrium and thus also prove its existence.\n"
    },
    {
        "paper_id": 1909.01121,
        "authors": "Junbeom Lee, Xiang Yu, and Chao Zhou",
        "title": "Lifetime Ruin under High-watermark Fees and Drift Uncertainty",
        "comments": "Final version, forthcoming in Applied Mathematics and Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to make a new contribution to the study of lifetime ruin\nproblem by considering investment in two hedge funds with high-watermark fees\nand drift uncertainty. Due to multi-dimensional performance fees that are\ncharged whenever each fund profit exceeds its historical maximum, the value\nfunction is expected to be multi-dimensional. New mathematical challenges arise\nas the standard dimension reduction cannot be applied, and the convexity of the\nvalue function and Isaacs condition may not hold in our ruin probability\nminimization problem with drift uncertainty. We propose to employ the\nstochastic Perron's method to characterize the value function as the unique\nviscosity solution to the associated Hamilton Jacobi Bellman (HJB) equation\nwithout resorting to the proof of dynamic programming principle. The required\ncomparison principle is also established in our setting to close the loop of\nstochastic Perron's method.\n"
    },
    {
        "paper_id": 1909.01249,
        "authors": "Minda Ma and Weiguang Cai",
        "title": "CO2 mitigation model for China's residential building sector",
        "comments": "Applied Energy Symposium 2019: Low carbon cities and urban energy\n  systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to investigate the factors that can mitigate carbon-dioxide\n(CO2) intensity and further assess CMRBS in China based on a household scale\nvia decomposition analysis. Here we show that: Three types of housing economic\nindicators and the final emission factor significantly contributed to the\ndecrease in CO2 intensity in the residential building sector. In addition, the\nCMRBS from 2001-2016 was 1816.99 MtCO2, and the average mitigation intensity\nduring this period was 266.12 kgCO2/household/year. Furthermore, the\nenergy-conservation and emission-mitigation strategy caused CMRBS to\neffectively increase and is the key to promoting a more significant emission\nmitigation in the future. Overall, this paper covers the CMRBS assessment gap\nin China, and the proposed assessment model can be regarded as a reference for\nother countries and cities for measuring the retrospective CO2 mitigation\neffect in residential buildings.\n"
    },
    {
        "paper_id": 1909.01268,
        "authors": "Samuel Asante Gyamerah",
        "title": "Are Bitcoins price predictable? Evidence from machine learning\n  techniques using technical indicators",
        "comments": "29 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The uncertainties in future Bitcoin price make it difficult to accurately\npredict the price of Bitcoin. Accurately predicting the price for Bitcoin is\ntherefore important for decision-making process of investors and market players\nin the cryptocurrency market. Using historical data from 01/01/2012 to\n16/08/2019, machine learning techniques (Generalized linear model via penalized\nmaximum likelihood, random forest, support vector regression with linear\nkernel, and stacking ensemble) were used to forecast the price of Bitcoin. The\nprediction models employed key and high dimensional technical indicators as the\npredictors. The performance of these techniques were evaluated using mean\nabsolute percentage error (MAPE), root mean square error (RMSE), mean absolute\nerror (MAE), and coefficient of determination (R-squared). The performance\nmetrics revealed that the stacking ensemble model with two base learner (random\nforest and generalized linear model via penalized maximum likelihood) and\nsupport vector regression with linear kernel as meta-learner was the optimal\nmodel for forecasting Bitcoin price. The MAPE, RMSE, MAE, and R-squared values\nfor the stacking ensemble model were 0.0191%, 15.5331 USD, 124.5508 USD, and\n0.9967 respectively. These values show a high degree of reliability in\npredicting the price of Bitcoin using the stacking ensemble model. Accurately\npredicting the future price of Bitcoin will yield significant returns for\ninvestors and market players in the cryptocurrency market.\n"
    },
    {
        "paper_id": 1909.01413,
        "authors": "Xavier Calmet and Nathaniel Wiesendanger Shaw",
        "title": "An analytical perturbative solution to the Merton Garman model using\n  symmetries",
        "comments": null,
        "journal-ref": "J Futures Markets, 40, 3-22 (2020)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce an analytical perturbative solution to the Merton\nGarman model. It is obtained by doing perturbation theory around the exact\nanalytical solution of a model which possesses a two-dimensional Galilean\nsymmetry. We compare our perturbative solution of the Merton Garman model to\nMonte Carlo simulations and find that our solutions performs surprisingly well\nfor a wide range of parameters. We also show how to use symmetries to build\noption pricing models. Our results demonstrate that the concept of symmetry is\nimportant in mathematical finance.\n"
    },
    {
        "paper_id": 1909.01664,
        "authors": "Patrice Loisel (MISTEA)",
        "title": "Stochastic perturbations and fisheries management",
        "comments": "Natural Resource Modeling, Wiley, In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As most natural resources, fisheries are affected by random disturbances. The\nevolution of such resources may be modelled by a succession of deterministic\nprocess and random perturbations on biomass and/or growth rate at random times.\nWe analyze the impact of the characteristics of the perturbations on the\nmanagement of natural resources. We highlight the importance of using a dynamic\nprogramming approach in order to completely characterize the optimal solution,\nwe also present the properties of the controlled model and give the behavior of\nthe optimal harvest for specific jump kernels.\n"
    },
    {
        "paper_id": 1909.01739,
        "authors": "Michail Anthropelos and Tim J. Boonen",
        "title": "Nash Equilibria in Optimal Reinsurance Bargaining",
        "comments": "22 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a strategic behavior in reinsurance bilateral transactions,\nwhere agents choose the risk preferences they will appear to have in the\ntransaction. Within a wide class of risk measures, we identify agents'\nstrategic choices to a range of risk aversion coefficients. It is shown that at\nthe strictly beneficial Nash equilibria, agents appear homogeneous with respect\nto their risk preferences. While the game does not cause any loss of total\nwelfare gain, its allocation between agents is heavily affected by the agents'\nstrategic behavior. This allocation is reflected in the reinsurance premium,\nwhile the insurance indemnity remains the same in all strictly beneficial Nash\nequilibria. Furthermore, the effect of agents' bargaining power vanishes\nthrough the game procedure and the agent who gets more welfare gain is the one\nwho has an advantage in choosing the common risk aversion at the equilibrium.\n"
    },
    {
        "paper_id": 1909.0183,
        "authors": "J\\\"orn Sass, Dorothee Westphal",
        "title": "Robust Utility Maximizing Strategies under Model Uncertainty and their\n  Convergence",
        "comments": "31 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate a utility maximization problem with drift\nuncertainty in a multivariate continuous-time Black-Scholes type financial\nmarket which may be incomplete. We impose a constraint on the admissible\nstrategies that prevents a pure bond investment and we include uncertainty by\nmeans of ellipsoidal uncertainty sets for the drift. Our main results consist\nfirstly in finding an explicit representation of the optimal strategy and the\nworst-case parameter, secondly in proving a minimax theorem that connects our\nrobust utility maximization problem with the corresponding dual problem.\nThirdly, we show that, as the degree of model uncertainty increases, the\noptimal strategy converges to a generalized uniform diversification strategy.\n"
    },
    {
        "paper_id": 1909.0197,
        "authors": "Cheikh Mbaye and Abass Sagna, and Fr\\'ed\\'eric Vrins",
        "title": "Conditional survival probabilities under partial information: a\n  recursive quantization approach with applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a structural model where the survival/default state is observed\ntogether with a noisy version of the firm value process. This assumption makes\nthe model more realistic than most of the existing alternatives, but triggers\nimportant challenges related to the computation of conditional default\nprobabilities. In order to deal with general diffusions as firm value process,\nwe derive a numerical procedure based on the recursive quantization method to\napproximate it. Then, we investigate the error approximation induced by our\nprocedure. Eventually, numerical tests are performed to evaluate the\nperformance of the method, and an application is proposed to the pricing of CDS\noptions.\n"
    },
    {
        "paper_id": 1909.02182,
        "authors": "Anne-Sophie Krah, Zoran Nikoli\\'c, Ralf Korn",
        "title": "Machine Learning in Least-Squares Monte Carlo Proxy Modeling of Life\n  Insurance Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under the Solvency II regime, life insurance companies are asked to derive\ntheir solvency capital requirements from the full loss distributions over the\ncoming year. Since the industry is currently far from being endowed with\nsufficient computational capacities to fully simulate these distributions, the\ninsurers have to rely on suitable approximation techniques such as the\nleast-squares Monte Carlo (LSMC) method. The key idea of LSMC is to run only a\nfew wisely selected simulations and to process their output further to obtain a\nrisk-dependent proxy function of the loss. In this paper, we present and\nanalyze various adaptive machine learning approaches that can take over the\nproxy modeling task. The studied approaches range from ordinary and generalized\nleast-squares regression variants over GLM and GAM methods to MARS and kernel\nregression routines. We justify the combinability of their regression\ningredients in a theoretical discourse. Further, we illustrate the approaches\nin slightly disguised real-world experiments and perform comprehensive\nout-of-sample tests.\n"
    },
    {
        "paper_id": 1909.0222,
        "authors": "Krishna Dasaratha, Kevin He",
        "title": "An Experiment on Network Density and Sequential Learning",
        "comments": "Incorporates the experimental results from a previous version of\n  arXiv:1703.02105",
        "journal-ref": "Games and Economic Behavior, Vol. 128, July 2021, 182-192",
        "doi": "10.1016/j.geb.2021.04.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct a sequential social-learning experiment where subjects each guess\na hidden state based on private signals and the guesses of a subset of their\npredecessors. A network determines the observable predecessors, and we compare\nsubjects' accuracy on sparse and dense networks. Accuracy gains from social\nlearning are twice as large on sparse networks compared to dense networks.\nModels of naive inference where agents ignore correlation between observations\npredict this comparative static in network density, while the finding is\ndifficult to reconcile with rational-learning models.\n"
    },
    {
        "paper_id": 1909.02474,
        "authors": "Cheikh Mbaye and Fr\\'ed\\'eric Vrins",
        "title": "An arbitrage-free conic martingale model with application to credit risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conic martingales refer to Brownian martingales evolving between bounds.\nAmong other potential applications, they have been suggested for the sake of\nmodeling conditional survival probabilities under partial information, as usual\nin reduced-form models. Yet, conic martingale default models have a special\nfeature; in contrast to the class of Cox models, they fail to satisfy the\nso-called \\emph{immersion property}. Hence, it is not clear whether this setup\nis arbitrage-free or not. In this paper, we study the relevance of conic\nmartingales-driven default models for practical applications in credit risk\nmodeling. We first introduce an arbitrage-free conic martingale, namely the\n$\\Phi$-martingale, by showing that it fits in the class of Dynamized Gaussian\ncopula model of Cr\\'epey et al., thereby providing an explicit construction\nscheme for the default time. In particular, the $\\Phi$-martingale features\ninteresting properties inherent on its construction easing the practical\nimplementation. Eventually, we apply this model to CVA pricing under wrong-way\nrisk and CDS options, and compare our results with the JCIR++ (a.k.a. SSRJD)\nand TC-JCIR recently introduced as an alternative.\n"
    },
    {
        "paper_id": 1909.02899,
        "authors": "Kei Katahira, Yu Chen",
        "title": "An extended Speculation Game for the recovery of Hurst exponent of\n  financial time series",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The speculation game is an agent-based toy model to investigate the dynamics\nof the financial market. Our model has achieved the reproduction of 10 of the\nwell-known stylized facts for financial time series. However, there is also a\ndivergence from the behavior of real market. The market price of the model\ntends to be anti-persistent to the initial price, resulting in the quite small\nvalue of Hurst exponent of price change. To overcome this problem, we extend\nthe speculation game by introducing a perturbative part to the price change\nwith the consideration of other effects besides pure speculative behaviors.\n"
    },
    {
        "paper_id": 1909.02972,
        "authors": "Benjamin James Duthie",
        "title": "Portfolio optimisation under rough Heston models",
        "comments": "40 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis investigates Merton's portfolio problem under two different rough\nHeston models, which have a non-Markovian structure. The motivation behind this\nchoice of problem is due to the recent discovery and success of rough\nvolatility processes. The optimisation problem is solved from two different\napproaches: firstly by considering an auxiliary random process, which solves\nthe optimisation problem with the martingale optimality principle, and\nsecondly, by a finite dimensional approximation of the volatility process which\ncasts the problem into its classical stochastic control framework. In addition,\nwe show how classical results from Merton's portfolio optimisation problem can\nbe used to help motivate the construction of the solution in both cases. The\noptimal strategy under both approaches is then derived in a semi-closed form,\nand comparisons between the results made. The approaches discussed in this\nthesis, combined with the historical works on the distortion transformation,\nprovide a strong foundation to build models capable of handling increasing\ncomplexity demanded by the ever growing financial market.\n"
    },
    {
        "paper_id": 1909.03185,
        "authors": "Kei Katahira, Yu Chen",
        "title": "Heterogeneous wealth distribution, round-trip trading and the emergence\n  of volatility clustering in Speculation Game",
        "comments": "31 pages, 31 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study is a detailed analysis of Speculation Game, a minimal agent-based\nmodel of financial markets, in which the round-trip trading and the dynamic\nwealth evolution with variable trading volumes are implemented. Instead of\nherding behavior, we find that the emergence of volatility clustering can be\ninduced by the heterogeneous wealth distribution among traders. In particular,\nthe spontaneous redistribution of market wealth through repetitions of\nround-trip trades can widen the wealth disparity and establish the Pareto\ndistribution of the capital size. In the meantime, large fluctuations in price\nreturn are brought on by the intermittent placements of the relatively big\norders from rich traders. Empirical data are used to support the scenario\nderived from the model.\n"
    },
    {
        "paper_id": 1909.03278,
        "authors": "Wonsup Shin, Seok-Jun Bu, and Sung-Bae Cho",
        "title": "Automatic Financial Trading Agent for Low-risk Portfolio Management\n  using Deep Reinforcement Learning",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The autonomous trading agent is one of the most actively studied areas of\nartificial intelligence to solve the capital market portfolio management\nproblem. The two primary goals of the portfolio management problem are\nmaximizing profit and restrainting risk. However, most approaches to this\nproblem solely take account of maximizing returns. Therefore, this paper\nproposes a deep reinforcement learning based trading agent that can manage the\nportfolio considering not only profit maximization but also risk restraint. We\nalso propose a new target policy to allow the trading agent to learn to prefer\nlow-risk actions. The new target policy can be reflected in the update by\nadjusting the greediness for the optimal action through the hyper parameter.\nThe proposed trading agent verifies the performance through the data of the\ncryptocurrency market. The Cryptocurrency market is the best test-ground for\ntesting our trading agents because of the huge amount of data accumulated every\nminute and the market volatility is extremely large. As a experimental result,\nduring the test period, our agents achieved a return of 1800% and provided the\nleast risky investment strategy among the existing methods. And, another\nexperiment shows that the agent can maintain robust generalized performance\neven if market volatility is large or training period is short.\n"
    },
    {
        "paper_id": 1909.03379,
        "authors": "Neave O'Clery, Stephen Kinsella",
        "title": "Modular structure in labour networks reveals skill basins",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is an emerging consensus in the literature that locally embedded\ncapabilities and industrial know-how are key determinants of growth and\ndiversification processes. In order to model these dynamics as a branching\nprocess, whereby industries grow as a function of the availability of related\nor relevant skills, industry networks are typically employed. These networks,\nsometimes referred to as industry spaces, describe the complex structure of the\ncapability or skill overlap between industry pairs, measured here via\ninter-industry labour flows. Existing models typically deploy a local or\n'nearest neighbour' approach to capture the size of the labour pool available\nto an industry in related sectors. This approach, however, ignores higher order\ninteractions in the network, and the presence of industry clusters or groups of\nindustries which exhibit high internal skill overlap. We argue that these\nclusters represent skill basins in which workers circulate and diffuse\nknowledge, and delineate the size of the skilled labour force available to an\nindustry. By applying a multi-scale community detection algorithm to this\nnetwork of flows, we identify industry clusters on a range of scales, from many\nsmall clusters to few large groupings. We construct a new variable, cluster\nemployment, which captures the workforce available to an industry within its\nown cluster. Using UK data we show that this variable is predictive of\nindustry-city employment growth and, exploiting the multi-scale nature of the\nindustrial clusters detected, propose a methodology to uncover the optimal\nscale at which labour pooling operates.\n"
    },
    {
        "paper_id": 1909.03429,
        "authors": "Marina Volkova, Jol Stoffers, Dmitry Kochetkov",
        "title": "Education Projects for Sustainable Development: Evidence from Ural\n  Federal University",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.28890.08641",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sustainable development is a worldwide recognized social and political goal,\ndiscussed in both academic and political discourse and with much research on\nthe topic related to sustainable development in higher education. Since mental\nmodels are formed more effectively at school age, we propose a new way of\nthinking that will help achieve this goal. This paper was written in the\ncontext of Russia, where the topic of sustainable development in education is\npoorly developed. The authors used the classical methodology of the case\nanalysis. The analysis and interpretation of the results were conducted in the\nframework of the institutional theory. Presented is the case of Ural Federal\nUniversity, which has been working for several years on the creation of a\ndevice for the purification of industrial sewer water in the framework of an\ninitiative student group. Schoolchildren recently joined the program, and such\nprojects have been called university-to-school projects. Successful solutions\nof inventive tasks contribute to the formation of mental models. This case has\nbeen analyzed in terms of institutionalism, and the authors argue for the\nprimacy of mental institutions over normative ones during sustainable society\nconstruction. This case study is the first to analyze a partnership between a\nFederal University and local schools regarding sustainable education and\nproposes a new way of thinking.\n"
    },
    {
        "paper_id": 1909.0343,
        "authors": "Bernardo D'Auria and Jos\\'e Antonio Salmer\\'on",
        "title": "Insider information and its relation with the arbitrage condition and\n  the utility maximization problem",
        "comments": null,
        "journal-ref": "Mathematical Biosciences and Engineering, 2020, 17(2): 998-1019",
        "doi": "10.3934/mbe.2020053",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the well-known framework of financial portfolio optimization, we\nanalyze the existing relationships between the condition of arbitrage and the\nutility maximization in presence of \\emph{insider information}. We assume that,\nsince the initial time, the information flow is altered by adding the knowledge\nof an additional random variable including future information. In this context\nwe study the utility maximization problem under the logarithmic and the\nConstant Relative Risk Aversion (CRRA) utilities, with and without the\nrestriction of no temporary-bankruptcy. In particular, we show that the value\nof the insider information may be bounded while the arbitrage condition holds\nand we prove that the insider information does not always imply arbitrage for\nthe insider by providing an explicit example.\n"
    },
    {
        "paper_id": 1909.03574,
        "authors": "Diego Zabaljauregui",
        "title": "A fixed-point policy-iteration-type algorithm for symmetric nonzero-sum\n  stochastic impulse control games",
        "comments": "33 pages, 9 figures, 1 table",
        "journal-ref": "Applied Mathematics & Optimization (2020)",
        "doi": "10.1007/s00245-020-09694-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nonzero-sum stochastic differential games with impulse controls offer a\nrealistic and far-reaching modelling framework for applications within finance,\nenergy markets, and other areas, but the difficulty in solving such problems\nhas hindered their proliferation. Semi-analytical approaches make strong\nassumptions pertaining to very particular cases. To the author's best\nknowledge, the only numerical method in the literature is the heuristic one we\nput forward to solve an underlying system of quasi-variational inequalities.\nFocusing on symmetric games, this paper presents a simpler, more precise and\nefficient fixed-point policy-iteration-type algorithm which removes the strong\ndependence on the initial guess and the relaxation scheme of the previous\nmethod. A rigorous convergence analysis is undertaken with natural assumptions\non the players strategies, which admit graph-theoretic interpretations in the\ncontext of weakly chained diagonally dominant matrices. A novel provably\nconvergent single-player impulse control solver is also provided. The main\nalgorithm is used to compute with high precision equilibrium payoffs and Nash\nequilibria of otherwise very challenging problems, and even some which go\nbeyond the scope of the currently available theory.\n"
    },
    {
        "paper_id": 1909.03792,
        "authors": "Arezoo Hatefi Ghahfarrokhi, Mehrnoush Shamsfard",
        "title": "Tehran Stock Exchange Prediction Using Sentiment Analysis of Online\n  Textual Opinions",
        "comments": "Intelligent Systems in Accounting, Finance and Management (2019)",
        "journal-ref": null,
        "doi": "10.1002/isaf.1465",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the impact of the social media data in\npredicting the Tehran Stock Exchange (TSE) variables for the first time. We\nconsider the closing price and daily return of three different stocks for this\ninvestigation. We collected our social media data from Sahamyab.com/stocktwits\nfor about three months. To extract information from online comments, we propose\na hybrid sentiment analysis approach that combines lexicon-based and\nlearning-based methods. Since lexicons that are available for the Persian\nlanguage are not practical for sentiment analysis in the stock market domain,\nwe built a particular sentiment lexicon for this domain. After designing and\ncalculating daily sentiment indices using the sentiment of the comments, we\nexamine their impact on the baseline models that only use historical market\ndata and propose new predictor models using multi regression analysis. In\naddition to the sentiments, we also examine the comments volume and the users'\nreliabilities. We conclude that the predictability of various stocks in TSE is\ndifferent depending on their attributes. Moreover, we indicate that for\npredicting the closing price only comments volume and for predicting the daily\nreturn both the volume and the sentiment of the comments could be useful. We\ndemonstrate that Users' Trust coefficients have different behaviors toward the\nthree stocks.\n"
    },
    {
        "paper_id": 1909.03808,
        "authors": "Mi Chuanmin, Xu Runjie, Lin Qingtong",
        "title": "Systemic Risk Clustering of China Internet Financial Based on t-SNE\n  Machine Learning Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the rapid development of Internet finance, a large number of studies\nhave shown that Internet financial platforms have different financial systemic\nrisk characteristics when they are subject to macroeconomic shocks or fragile\ninternal crisis. From the perspective of regional development of Internet\nfinance, this paper uses t-SNE machine learning algorithm to obtain data mining\nof China's Internet finance development index involving 31 provinces and 335\ncities and regions. The conclusion of the peak and thick tail characteristics,\nthen proposed three classification risks of Internet financial systemic risk,\nproviding more regionally targeted recommendations for the systematic risk of\nInternet finance.\n"
    },
    {
        "paper_id": 1909.0387,
        "authors": "Stephan Eckstein, Gaoyue Guo, Tongseok Lim, Jan Obloj",
        "title": "Robust pricing and hedging of options on multiple assets and its\n  numerics",
        "comments": "Forthcoming in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider robust pricing and hedging for options written on multiple assets\ngiven market option prices for the individual assets. The resulting problem is\ncalled the multi-marginal martingale optimal transport problem. We propose two\nnumerical methods to solve such problems: using discretisation and linear\nprogramming applied to the primal side and using penalisation and deep neural\nnetworks optimisation applied to the dual side. We prove convergence for our\nmethods and compare their numerical performance. We show how adding further\ninformation about call option prices at additional maturities can be\nincorporated and narrows down the no-arbitrage pricing bounds. Finally, we\nobtain structural results for the case of the payoff given by a weighted sum of\ncovariances between the assets.\n"
    },
    {
        "paper_id": 1909.04009,
        "authors": "Yongyang Cai, William Brock, Anastasios Xepapadeas, Kenneth Judd",
        "title": "Climate Policy under Spatial Heat Transport: Cooperative and\n  Noncooperative Regional Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build a novel stochastic dynamic regional integrated assessment model\n(IAM) of the climate and economic system including a number of important\nclimate science elements that are missing in most IAMs. These elements are\nspatial heat transport from the Equator to the Poles, sea level rise,\npermafrost thaw and tipping points. We study optimal policies under cooperation\nand noncooperation between two regions (the North and the Tropic-South) in the\nface of risks and recursive utility. We introduce a new general computational\nalgorithm to find feedback Nash equilibrium. Our results suggest that when the\nelements of climate science are ignored, important policy variables such as the\noptimal regional carbon tax and adaptation could be seriously biased. We also\nfind the regional carbon tax is significantly smaller in the feedback Nash\nequilibrium than in the social planner's problem in each region, and the North\nhas higher carbon taxes than the Tropic-South.\n"
    },
    {
        "paper_id": 1909.04107,
        "authors": "Levi Boxell and Zachary Steinert-Threlkeld",
        "title": "Taxing dissent: The impact of a social media tax in Uganda",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the impact of a new tool for suppressing the expression of\ndissent---a daily tax on social media use. Using a synthetic control framework,\nwe estimate that the tax reduced the number of georeferenced Twitter users in\nUganda by 13 percent. The estimated treatment effects are larger for poorer and\nless frequent users. Despite the overall decline in Twitter use, tweets\nreferencing collective action increased by 31 percent and observed protests\nincreased by 47 percent. These results suggest that taxing social media use may\nnot be an effective tool for reducing political dissent.\n"
    },
    {
        "paper_id": 1909.04327,
        "authors": "Seung-Hyun Moon, Yong-Hyuk Kim, Byung-Ro Moon",
        "title": "Empirical investigation of state-of-the-art mean reversion strategies\n  for equity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent studies have shown that online portfolio selection strategies that\nexploit the mean reversion property can achieve excess return from equity\nmarkets. This paper empirically investigates the performance of\nstate-of-the-art mean reversion strategies on real market data. The aims of the\nstudy are twofold. The first is to find out why the mean reversion strategies\nperform extremely well on well-known benchmark datasets, and the second is to\ntest whether or not the mean reversion strategies work well on recent market\ndata. The mean reversion strategies used in this study are the passive\naggressive mean reversion (PAMR) strategy, the on-line moving average reversion\n(OLMAR) strategy, and the transaction cost optimization (TCO) strategies. To\ntest the strategies, we use the historical prices of the stocks that constitute\nS\\&P 500 index over the period from 2000 to 2017 as well as well-known\nbenchmark datasets. Our findings are that the well-known benchmark datasets\nfavor mean reversion strategies, and mean reversion strategies may fail even in\nfavorable market conditions, especially when there exist explicit or implicit\ntransaction costs.\n"
    },
    {
        "paper_id": 1909.04354,
        "authors": "Claudia Ceci, Katia Colaneri, R\\\"diger Frey and Verena K\\\"ock",
        "title": "Value adjustments and dynamic hedging of reinsurance counterparty risk",
        "comments": "27 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reinsurance counterparty credit risk (RCCR) is the risk of a loss arising\nfrom the fact that a reinsurance company is unable to fulfill her contractual\nobligations towards the ceding insurer. RCCR is an important risk category for\ninsurance companies which, so far, has been addressed mostly via qualitative\napproaches. In this paper we therefore study value adjustments and dynamic\nhedging for RCCR. We propose a novel model that accounts for contagion effects\nbetween the default of the reinsurer and the price of the reinsurance contract.\nWe characterize the value adjustment in a reinsurance contract via a partial\nintegro-differential equation (PIDE) and derive the hedging strategies using a\nquadratic method. The paper closes with a simulation study which shows that\ndynamic hedging strategies have the potential to significantly reduce RCCR.\n"
    },
    {
        "paper_id": 1909.04497,
        "authors": "Qiong Wu, Christopher G. Brinton, Zheng Zhang, Andrea Pizzoferrato,\n  Zhenming Liu, Mihai Cucuringu",
        "title": "Equity2Vec: End-to-end Deep Learning Framework for Cross-sectional Asset\n  Pricing",
        "comments": "9 pages",
        "journal-ref": "International Conference on AI in Finance, 2021",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing assets has attracted significant attention from the financial\ntechnology community. We observe that the existing solutions overlook the\ncross-sectional effects and not fully leveraged the heterogeneous data sets,\nleading to sub-optimal performance.\n  To this end, we propose an end-to-end deep learning framework to price the\nassets. Our framework possesses two main properties: 1) We propose Equity2Vec,\na graph-based component that effectively captures both long-term and evolving\ncross-sectional interactions. 2) The framework simultaneously leverages all the\navailable heterogeneous alpha sources including technical indicators, financial\nnews signals, and cross-sectional signals. Experimental results on datasets\nfrom the real-world stock market show that our approach outperforms the\nexisting state-of-the-art approaches. Furthermore, market trading simulations\ndemonstrate that our framework monetizes the signals effectively.\n"
    },
    {
        "paper_id": 1909.04521,
        "authors": "Mahmood Khosrowjerdi and Lutz Bornmann",
        "title": "Is culture related to strong science? An empirical investigation",
        "comments": "15 pages, 2 figures, 4 tables",
        "journal-ref": "Journal of Informetrics 15 (2021)",
        "doi": "10.1016/j.joi.2021.101160",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  National culture is among those societal factors which could influence\nresearch and innovation activities. In this study, we investigated the\nassociations of two national culture models with citation impact of nations\n(measured by the proportion of papers belonging to the 10% and 1% most cited\npapers in the corresponding fields, PPtop 10% and PPtop 1%). Bivariate\nstatistical analyses showed that of six Hofstede's national culture dimensions\n(HNCD), uncertainty avoidance and power distance had a statistically\nsignificant negative associa-tion, while individualism and indulgence had a\nstatistically significant positive associationwith both citation impact\nindicators. The study also revealed that of two Inglehart-Welzel cultural\nvalues (IWCV), the value survival versus self-expression is statistically\nsignificantly related to citation impact indicators. We additionally calculated\nmultiple regression analyses controlling for the possible effects of\nconfounding factors including national self-citations, international\nco-authorships, invest-ments in research and development, international migrant\nstock, number of researchers ofeach nation, language, and productivity. The\nresults revealed that the statistically significant associations of HNCD with\ncitation impact indicators disappeared. But the statistically significant\nrelationship between survivals versus self-expression values and citation\nimpact indicators remained stable even after controlling for the confounding\nvariables. Thus, the freedom of expression and trust in society might\ncontribute to better scholarly communication systems, higher level of\ninternational collaborations, and further quality research.\n"
    },
    {
        "paper_id": 1909.04602,
        "authors": "Matteo Burzoni, Marco Maggis",
        "title": "Arbitrage-free modeling under Knightian Uncertainty",
        "comments": "Final version. To appear in Mathematics and Financial Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Fundamental Theorem of Asset Pricing for a general financial\nmarket under Knightian Uncertainty. We adopt a functional analytic approach\nwhich require neither specific assumptions on the class of priors $\\mathcal{P}$\nnor on the structure of the state space. Several aspects of modeling under\nKnightian Uncertainty are considered and analyzed. We show the need for a\nsuitable adaptation of the notion of No Free Lunch with Vanishing Risk and\ndiscuss its relation to the choice of an appropriate filtration. In an abstract\nsetup, we show that absence of arbitrage is equivalent to the existence of\n\\emph{approximate} martingale measures sharing the same polar set of\n$\\mathcal{P}$. We then specialize the results to a discrete-time framework in\norder to obtain true martingale measures.\n"
    },
    {
        "paper_id": 1909.04767,
        "authors": "Tommaso Lando, Lucio Bertoli-Barsotti",
        "title": "Distorted stochastic dominance: a generalized family of stochastic\n  orders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a generalized family of stochastic orders, semiparametrized by a\ndistortion function H, namely H-distorted stochastic dominance, which may\ndetermine a continuum of dominance relations from the first- to the\nsecond-order stochastic dominance (and beyond). Such a family is especially\nsuitable for representing a decision maker's preferences in terms of risk\naversion and may be used in those situations in which a strong order does not\nhave enough discriminative power, whilst a weaker one is poorly representative\nof some classes of decision makers. In particular, we focus on the class of\npower distortion functions, yielding power-distorted stochastic dominance,\nwhich seems to be particularly appealing owing to its computational simplicity\nand some interesting statistical interpretations. Finally, we characterize\ndistorted stochastic dominance in terms of distortion functions yielding\nisotonic classes of distorted expectations.\n"
    },
    {
        "paper_id": 1909.04834,
        "authors": "Nasimeh Heydaribeni and Achilleas Anastasopoulos",
        "title": "Linear Equilibria for Dynamic LQG Games with Asymmetric Information and\n  Dependent Types",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a non-zero-sum linear quadratic Gaussian (LQG) dynamic game with\nasymmetric information. Each player observes privately a noisy version of a\n(hidden) state of the world $V$, resulting in dependent private observations.\nWe study perfect Bayesian equilibria (PBE) for this game with equilibrium\nstrategies that are linear in players' private estimates of $V$. The main\ndifficulty arises from the fact that players need to construct estimates on\nother players' estimate on $V$, which in turn would imply that an infinite\nhierarchy of estimates on estimates needs to be constructed, rendering the\nproblem unsolvable. We show that this is not the case: each player's estimate\non other players' estimates on $V$ can be summarized into her own estimate on\n$V$ and some appropriately defined public information. Based on this finding we\ncharacterize the PBE through a backward/forward algorithm akin to dynamic\nprogramming for the standard LQG control problem. Unlike the standard LQG\nproblem, however, Kalman filter covariance matrices, as well as some other\nrequired quantities, are observation-dependent and thus cannot be evaluated\noff-line through a forward recursion.\n"
    },
    {
        "paper_id": 1909.04853,
        "authors": "Qi Wang, Jos\\'e E. Figueroa-L\\'opez, and Todd Kuffner",
        "title": "Bayesian Inference on Volatility in the Presence of Infinite Jump\n  Activity and Microstructure Noise",
        "comments": "9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility estimation based on high-frequency data is key to accurately\nmeasure and control the risk of financial assets. A L\\'{e}vy process with\ninfinite jump activity and microstructure noise is considered one of the\nsimplest, yet accurate enough, models for financial data at high-frequency.\nUtilizing this model, we propose a \"purposely misspecified\" posterior of the\nvolatility obtained by ignoring the jump-component of the process. The\nmisspecified posterior is further corrected by a simple estimate of the\nlocation shift and re-scaling of the log likelihood. Our main result\nestablishes a Bernstein-von Mises (BvM) theorem, which states that the proposed\nadjusted posterior is asymptotically Gaussian, centered at a consistent\nestimator, and with variance equal to the inverse of the Fisher information. In\nthe absence of microstructure noise, our approach can be extended to inferences\nof the integrated variance of a general It\\^o semimartingale. Simulations are\nprovided to demonstrate the accuracy of the resulting credible intervals, and\nthe frequentist properties of the approximate Bayesian inference based on the\nadjusted posterior.\n"
    },
    {
        "paper_id": 1909.04903,
        "authors": "Samuel Asante Gyamerah",
        "title": "Estimating the volatility of Bitcoin using GARCH models",
        "comments": "23 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, an application of three GARCH-type models (sGARCH, iGARCH, and\ntGARCH) with Student t-distribution, Generalized Error distribution (GED), and\nNormal Inverse Gaussian (NIG) distribution are examined. The new development\nallows for the modeling of volatility clustering effects, the leptokurtic and\nthe skewed distributions in the return series of Bitcoin. Comparative to the\ntwo distributions, the normal inverse Gaussian distribution captured adequately\nthe fat tails and skewness in all the GARCH type models. The tGARCH model was\nthe best model as it described the asymmetric occurrence of shocks in the\nBitcoin market. That is, the response of investors to the same amount of good\nand bad news are distinct. From the empirical results, it can be concluded that\ntGARCH-NIG was the best model to estimate the volatility in the return series\nof Bitcoin. Generally, it would be optimal to use the NIG distribution in GARCH\ntype models since time series of most cryptocurrency are leptokurtic.\n"
    },
    {
        "paper_id": 1909.04981,
        "authors": "Martin Huber, Mark Schelker, Anthony Strittmatter",
        "title": "Direct and Indirect Effects based on Changes-in-Changes",
        "comments": null,
        "journal-ref": "Journal of Business and Economic Statistics, 2020, forthcoming",
        "doi": "10.1080/07350015.2020.1831929",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We propose a novel approach for causal mediation analysis based on\nchanges-in-changes assumptions restricting unobserved heterogeneity over time.\nThis allows disentangling the causal effect of a binary treatment on a\ncontinuous outcome into an indirect effect operating through a binary\nintermediate variable (called mediator) and a direct effect running via other\ncausal mechanisms. We identify average and quantile direct and indirect effects\nfor various subgroups under the condition that the outcome is monotonic in the\nunobserved heterogeneity and that the distribution of the latter does not\nchange over time conditional on the treatment and the mediator. We also provide\na simulation study and an empirical application to the Jobs II programme.\n"
    },
    {
        "paper_id": 1909.04986,
        "authors": "Jaros{\\l}aw Klamut and Tomasz Gubiec",
        "title": "Continuous Time Random Walk with correlated waiting times. The crucial\n  role of inter-trade times in volatility clustering",
        "comments": "28 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many physical, social or economical phenomena we observe changes of a\nstudied quantity only in discrete, irregularly distributed points in time. The\nstochastic process used by physicists to describe this kind of variables is the\nContinuous Time Random Walk (CTRW). Despite the popularity of this type of\nstochastic processes and strong empirical motivation, models with a long-term\nmemory within the sequence of time intervals between observations are missing.\nHere, we fill this gap by introducing a new family of CTRWs. The memory is\nintroduced to the model by the assumption that many consecutive time intervals\ncan be the same. Surprisingly, in this process we can observe a slowly decaying\nnonlinear autocorrelation function without a fat-tailed distribution of time\nintervals. Our model applied to high-frequency stock market data can\nsuccessfully describe the slope of decay of nonlinear autocorrelation function\nof stock market returns. The model achieves this result with no dependence\nbetween consecutive price changes. It proves the crucial role of inter-event\ntimes in the volatility clustering phenomenon observed in all stock markets.\n"
    },
    {
        "paper_id": 1909.05151,
        "authors": "Samuel Showalter and Jeffrey Gropp",
        "title": "Validating Weak-form Market Efficiency in United States Stock Markets\n  with Trend Deterministic Price Data and Machine Learning",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Efficient Market Hypothesis has been a staple of economics research for\ndecades. In particular, weak-form market efficiency -- the notion that past\nprices cannot predict future performance -- is strongly supported by\neconometric evidence. In contrast, machine learning algorithms implemented to\npredict stock price have been touted, to varying degrees, as successful.\nMoreover, some data scientists boast the ability to garner above-market returns\nusing price data alone. This study endeavors to connect existing econometric\nresearch on weak-form efficient markets with data science innovations in\nalgorithmic trading. First, a traditional exploration of stationarity in stock\nindex prices over the past decade is conducted with Augmented Dickey-Fuller and\nVariance Ratio tests. Then, an algorithmic trading platform is implemented with\nthe use of five machine learning algorithms. Econometric findings identify\npotential stationarity, hinting technical evaluation may be possible, though\nalgorithmic trading results find little predictive power in any machine\nlearning model, even when using trend-specific metrics. Accounting for\ntransaction costs and risk, no system achieved above-market returns\nconsistently. Our findings reinforce the validity of weak-form market\nefficiency.\n"
    },
    {
        "paper_id": 1909.05289,
        "authors": "Baptiste Barreau, Laurent Carlier, Damien Challet",
        "title": "Deep Prediction of Investor Interest: a Supervised Clustering Approach",
        "comments": null,
        "journal-ref": "Algorithmic Finance, vol. 8, no. 3-4, pp. 77-89, 2020",
        "doi": "10.3233/AF-200296",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel deep learning architecture suitable for the prediction of\ninvestor interest for a given asset in a given time frame. This architecture\nperforms both investor clustering and modelling at the same time. We first\nverify its superior performance on a synthetic scenario inspired by real data\nand then apply it to two real-world databases, a publicly available dataset\nabout the position of investors in Spanish stock market and proprietary data\nfrom BNP Paribas Corporate and Institutional Banking.\n"
    },
    {
        "paper_id": 1909.05335,
        "authors": "Kerem Ugurlu",
        "title": "Robust Utility Maximization with Drift and Volatility Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give explicit solutions for utility maximization of terminal wealth\nproblem $u(X_T)$ in the presence of Knightian uncertainty in continuous time\n$[0,T]$ in a complete market. We assume there is uncertainty on both drift and\nvolatility of the underlying stocks, which induce nonequivalent measures on\ncanonical space of continuous paths $\\O$. We take that the uncertainty set\nresides in compact sets that are time dependent. In this framework, we solve\nthe robust optimization problem with logarithmic, power and exponential utility\nfunctions, explicitly.\n"
    },
    {
        "paper_id": 1909.05501,
        "authors": "G\\'abor Petneh\\'azi and J\\'ozsef G\\'all",
        "title": "Mortality rate forecasting: can recurrent neural networks beat the\n  Lee-Carter model?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article applies a long short-term memory recurrent neural network to\nmortality rate forecasting. The model can be trained jointly on the mortality\nrate history of different countries, ages, and sexes. The RNN-based method\nseems to outperform the popular Lee-Carter model.\n"
    },
    {
        "paper_id": 1909.05604,
        "authors": "Emanuele Pugliese, Lorenzo Napolitano, Matteo Chinazzi, Guido\n  Chiarotti",
        "title": "The Emergence of Innovation Complexity at Different Geographical and\n  Technological Scales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define a novel quantitative strategy inspired by the ecological notion of\nnestedness to single out the scale at which innovation complexity emerges from\nthe aggregation of specialized building blocks. Our analysis not only suggests\nthat the innovation space can be interpreted as a natural system in which\nadvantageous capabilities are selected by evolutionary pressure, but also that\nthe emerging structure of capabilities is not independent of the scale of\nobservation at which they are observed. Expanding on this insight allows us to\nunderstand whether the capabilities characterizing the innovation space at a\ngiven scale are compatible with a complex evolutionary dynamics or, rather, a\nset of essentially independent activities allowing to reduce the system at that\nscale to a set of disjoint non interacting sub-systems. This yields a measure\nof the innovation complexity of the system, i.e. of the degree of\ninterdependence between the sets of capabilities underlying the system's\nbuilding blocks.\n"
    },
    {
        "paper_id": 1909.05689,
        "authors": "Mario Coccia",
        "title": "A new concept of technology with systemic-purposeful perpsective:\n  theory, examples and empirical application",
        "comments": "30 pages, 2 Figures, 1 Table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although definitions of technology exist to explain the patterns of\ntechnological innovations, there is no general definition that explain the role\nof technology for humans and other animal species in environment. The goal of\nthis study is to suggest a new concept of technology with a systemic-purposeful\nperspective for technology analysis. Technology here is a complex system of\nartifact, made and_or used by living systems, that is composed of more than one\nentity or sub-system and a relationship that holds between each entity and at\nleast one other entity in the system, selected considering practical, technical\nand_or economic characteristics to satisfy needs, achieve goals and_or solve\nproblems of users for purposes of adaptation and_or survival in environment.\nTechnology T changes current modes of cognition and action to enable makers\nand_or users to take advantage of important opportunities or to cope with\nconsequential environmental threats. Technology, as a complex system, is formed\nby different elements given by incremental and radical innovations.\nTechnological change generates the progress from a system T1 to T2, T3, etc.\ndriven by changes of technological trajectories and technological paradigms.\nSeveral examples illustrate here these concepts and a simple model with a\npreliminary empirical analysis shows how to operationalize the suggested\ndefinition of technology. Overall, then, the role of adaptation (i.e.\nreproductive advantage) can be explained as a main driver of technology use for\nadopters to take advantage of important opportunities or to cope with\nenvironmental threats. This study begins the process of clarifying and\ngeneralizing, as far as possible, the concept of technology with a new\nperspective that it can lay a foundation for the development of more\nsophisticated concepts and theories to explain technological and economic\nchange in environment.\n"
    },
    {
        "paper_id": 1909.05761,
        "authors": "Luisa Andreis, Maria Flora, Fulvio Fontini, Tiziano Vargiolu",
        "title": "Pricing Reliability Options under different electricity prices' regimes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reliability Options are capacity remuneration mechanisms aimed at enhancing\nsecurity of supply in electricity systems. They can be framed as call options\non electricity sold by power producers to System Operators. This paper provides\na comprehensive mathematical treatment of Reliability Options. Their value is\nfirst derived by means of closed-form pricing formulae, which are obtained\nunder several assumptions about the dynamics of electricity prices and strike\nprices. Then, the value of the Reliability Option is simulated under a\nreal-market calibration, using data of the Italian power market. We finally\nperform sensitivity analyses to highlight the impact of the level and\nvolatility of both power and strike price, of the mean reversion speeds and of\nthe correlation coefficient on the Reliability Options' value.\n"
    },
    {
        "paper_id": 1909.06036,
        "authors": "Arash Fahim, Yu-Jui Huang, Saeed Khalili",
        "title": "Generalized Duality for Model-Free Superhedging given Marginals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a discrete-time financial market, a generalized duality is established for\nmodel-free superhedging, given marginal distributions of the underlying asset.\nContrary to prior studies, we do not require contingent claims to be upper\nsemicontinuous, allowing for upper semi-analytic ones. The generalized duality\nstipulates an extended version of risk-neutral pricing. To compute the\nmodel-free superhedging price, one needs to find the supremum of expected\nvalues of a contingent claim, evaluated not directly under martingale\n(risk-neutral) measures, but along sequences of measures that converge, in an\nappropriate sense, to martingale ones. To derive the main result, we first\nestablish a portfolio-constrained duality for upper semi-analytic contingent\nclaims, relying on Choquet's capacitability theorem. As we gradually fade out\nthe portfolio constraint, the generalized duality emerges through delicate\nprobabilistic estimations.\n"
    },
    {
        "paper_id": 1909.06108,
        "authors": "Nikita Kozodoi, Panagiotis Katsas, Stefan Lessmann, Luis\n  Moreira-Matias, Konstantinos Papakonstantinou",
        "title": "Shallow Self-Learning for Reject Inference in Credit Scoring",
        "comments": "Preprint of the paper accepted to ECML PKDD 2019",
        "journal-ref": "ECML PKDD 2019. Lecture Notes in Computer Science, vol 11908.\n  Springer, Cham",
        "doi": "10.1007/978-3-030-46133-1_31",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit scoring models support loan approval decisions in the financial\nservices industry. Lenders train these models on data from previously granted\ncredit applications, where the borrowers' repayment behavior has been observed.\nThis approach creates sample bias. The scoring model (i.e., classifier) is\ntrained on accepted cases only. Applying the resulting model to screen credit\napplications from the population of all borrowers degrades model performance.\nReject inference comprises techniques to overcome sampling bias through\nassigning labels to rejected cases. The paper makes two contributions. First,\nwe propose a self-learning framework for reject inference. The framework is\ngeared toward real-world credit scoring requirements through considering\ndistinct training regimes for iterative labeling and model training. Second, we\nintroduce a new measure to assess the effectiveness of reject inference\nstrategies. Our measure leverages domain knowledge to avoid artificial labeling\nof rejected cases during strategy evaluation. We demonstrate this approach to\noffer a robust and operational assessment of reject inference strategies.\nExperiments on a real-world credit scoring data set confirm the superiority of\nthe adjusted self-learning framework over regular self-learning and previous\nreject inference strategies. We also find strong evidence in favor of the\nproposed evaluation measure assessing reject inference strategies more\nreliably, raising the performance of the eventual credit scoring model.\n"
    },
    {
        "paper_id": 1909.0626,
        "authors": "Alet Roux and Zhikang Xu",
        "title": "Optimal investment and contingent claim valuation with exponential\n  disutility under proportional transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider indifference pricing of contingent claims consisting of payment\nflows in a discrete time model with proportional transaction costs and under\nexponential disutility. This setting covers utility maximisation as a special\ncase. A dual representation is obtained for the associated disutility\nminimisation problem, together with a dynamic procedure for solving it. This\nleads to efficient and convergent numerical procedures for indifference\npricing, optimal trading strategies and shadow prices that apply to a wide\nrange of payoffs, a large range of time steps and all magnitudes of transaction\ncosts.\n"
    },
    {
        "paper_id": 1909.06332,
        "authors": "Jack Colleran, Harris K. Kim, Benny C. Pickle, John Sun",
        "title": "Comparative Companies' Stock Valuation through Financial Metrics and its\n  Social Implications",
        "comments": "23 pages, One table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Out of the companies, Dolby is the company with the best overall financial\nand operation health. According to the table that accounted its financial\nstatements for the past three years, Dolby has stable profit margins that\ngenerates a revenue in the billions, the only company in ten figures. Corporate\ncompetition to gain more patents as old ones expire may mean new jobs created,\nincreased funding for schools, investment in technology or engineering\neducation, and further need for purchase of marketing and salespeople.\n"
    },
    {
        "paper_id": 1909.06509,
        "authors": "Yuqing Wang, Yan Ru Pei",
        "title": "The Optimal Deterrence of Crime: A Focus on the Time Preference of DWI\n  Offenders",
        "comments": "63 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general model for finding the optimal penal strategy based on\nthe behavioral traits of the offenders. We focus on how the discount rate\n(level of time discounting) affects criminal propensity on the individual\nlevel, and how the aggregation of these effects influences criminal activities\non the population level. The effects are aggregated based on the distribution\nof discount rate among the population. We study this distribution empirically\nthrough a survey with 207 participants, and we show that it follows\nzero-inflated exponential distribution. We quantify the effectiveness of the\npenal strategy as its net utility for the population, and show how this\nquantity can be maximized. When we apply the maximization procedure on the\noffense of impaired driving (DWI), we discover that the effectiveness of DWI\ndeterrence depends critically on the amount of fine and prison condition.\n"
    },
    {
        "paper_id": 1909.06599,
        "authors": "Rick Bohte and Luca Rossini",
        "title": "Comparing the forecasting of cryptocurrencies by Bayesian time-varying\n  volatility models",
        "comments": "Forthcoming in \"Journal of Risk and Financial Management\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the forecasting ability of cryptocurrency time series.\nThis study is about the four most capitalized cryptocurrencies: Bitcoin,\nEthereum, Litecoin and Ripple. Different Bayesian models are compared,\nincluding models with constant and time-varying volatility, such as stochastic\nvolatility and GARCH. Moreover, some crypto-predictors are included in the\nanalysis, such as S\\&P 500 and Nikkei 225. In this paper the results show that\nstochastic volatility is significantly outperforming the benchmark of VAR in\nboth point and density forecasting. Using a different type of distribution, for\nthe errors of the stochastic volatility the student-t distribution came out to\nbe outperforming the standard normal approach.\n"
    },
    {
        "paper_id": 1909.06648,
        "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, and Matja\\v{z} Omladi\\v{c}",
        "title": "Relation between non-exchangeability and measures of concordance of\n  copulas",
        "comments": "27 pages, 11 figures",
        "journal-ref": "Journal of mathematical analysis and applications, vol. 487 (2020)",
        "doi": "10.1016/j.jmaa.2020.123951",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An investigation is presented of how a comprehensive choice of five most\nimportant measures of concordance (namely Spearman's rho, Kendall's tau, Gini's\ngamma, Blomqvist's beta, and their weaker counterpart Spearman's footrule)\nrelate to non-exchangeability, i.e., asymmetry on copulas. Besides these\nresults, the method proposed also seems to be new and may serve as a raw model\nfor exploration of the relationship between a specific property of a copula and\nsome of its measures of dependence structure, or perhaps the relationship\nbetween various measures of dependence structure themselves.\n"
    },
    {
        "paper_id": 1909.06759,
        "authors": "Loretta Mastroeni, Maurizio Naldi, Pierluigi Vellucci",
        "title": "Personal Finance Decisions with Untruthful Advisors: an Agent-Based\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investors usually resort to financial advisors to improve their investment\nprocess until the point of complete delegation on investment decisions. Surely,\nfinancial advice is potentially a correcting factor in investment decisions\nbut, in the past, the media and regulators blamed biased advisors for\nmanipulating the expectations of naive investors. In order to give an analytic\nformulation of the problem, we present an Agent-Based Model formed by\nindividual investors and a financial advisor. We parametrize the games by\nconsidering a compromise for the financial advisor (between a sufficient reward\nby bank and to keep his/her reputation), and a compromise for the customers\n(between the desired return and the proposed return by advisor). Then we obtain\nthe Nash equilibria and the best response functions of the resulting game. We\nalso describe the parameter regions in which these points result acceptable\nequilibria and the greediness/naivety of the customers emerge naturally from\nthe model. Finally, we focus on the efficiency of the best Nash equilibrium.\n"
    },
    {
        "paper_id": 1909.06854,
        "authors": "Athena Picarelli, Tiziano Vargiolu",
        "title": "Optimal management of pumped hydroelectric production with state\n  constrained optimal control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel technique to solve the problem of managing optimally a\npumped hydroelectric storage system. This technique relies on representing the\nsystem as a stochastic optimal control problem with state constraints, these\nlatter corresponding to the finite volume of the reservoirs. Following the\nrecent level-set approach presented in O. Bokanowski, A. Picarelli, H. Zidani,\n\"State-constrained stochastic optimal control problems via reachability\napproach\", SIAM J. Control and Optim. 54 (5) (2016), we transform the original\nconstrained problem in an auxiliary unconstrained one in augmented state and\ncontrol spaces, obtained by introducing an exact penalization of the original\nstate constraints. The latter problem is fully treatable by classical dynamic\nprogramming arguments.\n"
    },
    {
        "paper_id": 1909.07139,
        "authors": "Michele Azzone and Roberto Baviera",
        "title": "Additive normal tempered stable processes for equity derivatives and\n  power law scaling",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2021.1983200",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a simple model for equity index derivatives. The model\ngeneralizes well known L\\`evy Normal Tempered Stable processes (e.g. NIG and\nVG) with time dependent parameters. It accurately fits Equity index implied\nvolatility surfaces in the whole time range of quoted instruments, including\nsmall time horizon (few days) and long time horizon options (years). We prove\nthat the model is an Additive process that is constructed using an Additive\nsubordinator. This allows us to use classical L\\`evy-type pricing techniques.\nWe discuss the calibration issues in detail and we show that, in terms of mean\nsquared error, calibration is on average two orders of magnitude better than\nboth L\\`evy processes and Self-similar alternatives. We show that even if the\nmodel loses the classical stationarity property of L\\`evy processes, it\npresents interesting scaling properties for the calibrated parameters.\n"
    },
    {
        "paper_id": 1909.07288,
        "authors": "Patrice Loisel (MISTEA), Guillerme Duvilli\\'e (MAORE), Denis Barbeau,\n  Brigitte Charnomordic (MISTEA)",
        "title": "EvaSylv: A user-friendly software to evaluate forestry scenarii\n  including natural risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forest management relies on the evaluation of silviculture practices. The\nincrease in natural risk due to climate change makes it necessary to consider\nevaluation criteria that take natural risk into account. Risk integration in\nexisting software requires advanced programming skills.We propose a\nuser-friendly software to simulate even-aged and monospecific forest at the\nstand level, in order to evaluate and optimize forest management. The software\ngives the possibility to run management scenarii with or without considering\nthe impact of natural risk. The control variables are the dates and rates of\nthinning and the cutting age.The risk model is based on a Poisson processus.\nThe Faustmann approach, including tree damage risk, is used to evaluate future\nbenefits, economic or ecosystem services. It relies on the calculation of\nexpected values, for which a dedicated mathematical development has been done.\nThe optimized criteria used to evaluate the various scenarii are the Faustmann\nvalue and the Averaged yield value.We illustrate the approach and the software\non two case studies: economic optimization of a beech stand and carbon\nsequestration optimization of a pine stand.Software interface makes it easy for\nusers to write their own (growth-tree damage-economic) models without advanced\nprogramming skills. The possibility to run management scenarii with/without\nconsidering the impact of natural risk may contribute improving silviculture\nguidelines and adapting them to climate change. We propose future lines of\nresearch and improvement.\n"
    },
    {
        "paper_id": 1909.07319,
        "authors": "Tat Lung (Ron) Chan",
        "title": "An SFP--FCC Method for Pricing and Hedging Early-exercise Options under\n  L\\'evy Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper extends the Singular Fourier--Pad\\'e (SFP) method proposed by Chan\n(2018) to pricing/hedging early-exercise options--Bermudan, American and\ndiscrete-monitored barrier options--under a L\\'evy process. The current SFP\nmethod is incorporated with the Filon--Clenshaw--Curtis (FCC) rules invented by\nDom\\'inguez et al. (2011), and we call the new method SFP--FCC. The main\npurpose of using the SFP--FCC method is to require a small number of terms to\nyield fast error convergence and to formulate option pricing and option Greek\ncurves rather than individual prices/Greek values. We also numerically show\nthat the SFP--FCC method can retain a global spectral convergence rate in\noption pricing and hedging when the risk-free probability density function is\npiecewise smooth. Moreover, the computational complexity of the method is\n$\\mathcal{O}((L-1)(N+1)(\\tilde{N} \\log \\tilde{N}) )$ with $N$ a (small) number\nof complex Fourier series terms, $\\tilde{N}$ a number of Chebyshev series terms\nand $L$, the number of early-exercise/monitoring dates. Finally, we show that\nour method is more favourable than existing techniques in numerical\nexperiments.\n"
    },
    {
        "paper_id": 1909.07481,
        "authors": "Shenhao Wang, Baichuan Mo, Jinhua Zhao",
        "title": "Deep Neural Networks for Choice Analysis: Architectural Design with\n  Alternative-Specific Utility Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Whereas deep neural network (DNN) is increasingly applied to choice analysis,\nit is challenging to reconcile domain-specific behavioral knowledge with\ngeneric-purpose DNN, to improve DNN's interpretability and predictive power,\nand to identify effective regularization methods for specific tasks. This study\ndesigns a particular DNN architecture with alternative-specific utility\nfunctions (ASU-DNN) by using prior behavioral knowledge. Unlike a fully\nconnected DNN (F-DNN), which computes the utility value of an alternative k by\nusing the attributes of all the alternatives, ASU-DNN computes it by using only\nk's own attributes. Theoretically, ASU-DNN can dramatically reduce the\nestimation error of F-DNN because of its lighter architecture and sparser\nconnectivity. Empirically, ASU-DNN has 2-3% higher prediction accuracy than\nF-DNN over the whole hyperparameter space in a private dataset that we\ncollected in Singapore and a public dataset in R mlogit package. The\nalternative-specific connectivity constraint, as a domain-knowledge-based\nregularization method, is more effective than the most popular generic-purpose\nexplicit and implicit regularization methods and architectural hyperparameters.\nASU-DNN is also more interpretable because it provides a more regular\nsubstitution pattern of travel mode choices than F-DNN does. The comparison\nbetween ASU-DNN and F-DNN can also aid in testing the behavioral knowledge. Our\nresults reveal that individuals are more likely to compute utility by using an\nalternative's own attributes, supporting the long-standing practice in choice\nmodeling. Overall, this study demonstrates that prior behavioral knowledge\ncould be used to guide the architecture design of DNN, to function as an\neffective domain-knowledge-based regularization method, and to improve both the\ninterpretability and predictive power of DNN in choice analysis.\n"
    },
    {
        "paper_id": 1909.07748,
        "authors": "J. Lussange, I. Lazarevich, S. Bourgeois-Gironde, S. Palminteri, B.\n  Gutkin",
        "title": "Stock market microstructure inference via multi-agent reinforcement\n  learning",
        "comments": "15 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantitative finance has had a long tradition of a bottom-up approach to\ncomplex systems inference via multi-agent systems (MAS). These statistical\ntools are based on modelling agents trading via a centralised order book, in\norder to emulate complex and diverse market phenomena. These past financial\nmodels have all relied on so-called zero-intelligence agents, so that the\ncrucial issues of agent information and learning, central to price formation\nand hence to all market activity, could not be properly assessed. In order to\naddress this, we designed a next-generation MAS stock market simulator, in\nwhich each agent learns to trade autonomously via model-free reinforcement\nlearning. We calibrate the model to real market data from the London Stock\nExchange over the years $2007$ to $2018$, and show that it can faithfully\nreproduce key market microstructure metrics, such as various price\nautocorrelation scalars over multiple time intervals. Agent learning thus\nenables model emulation of the microstructure with greater realism.\n"
    },
    {
        "paper_id": 1909.07837,
        "authors": "Katia Colaneri and Stefano Herzel and Marco Nicolosi",
        "title": "The value of knowing the market price of risk",
        "comments": "34 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an optimal allocation problem in a financial market with\none risk-free and one risky asset, when the market is driven by a stochastic\nmarket price of risk. We solve the problem in continuous time, for an investor\nwith a Constant Relative Risk Aversion (CRRA) utility, under two scenarios:\nwhen the market price of risk is observable (the {\\em full information case}),\nand when it is not (the {\\em partial information case}). The corresponding\nmarket models are complete in the partial information case and incomplete in\nthe other case, hence the two scenarios exhibit rather different features. We\nstudy how the access to more accurate information on the market price of risk\naffects the optimal strategies and we determine the maximal price that the\ninvestor would be willing to pay to get such information. In particular, we\nexamine two cases of additional information, when an exact observation of the\nmarket price of risk is available either at time $0$ only (the {\\em initial\ninformation case}), or during the whole investment period (the {\\em dynamic\ninformation case}).\n"
    },
    {
        "paper_id": 1909.07896,
        "authors": "Ren\\'e A\\\"id, Giorgia Callegaro, Luciano Campi",
        "title": "No-Arbitrage Commodity Option Pricing with Market Manipulation",
        "comments": "33 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We design three continuous--time models in finite horizon of a commodity\nprice, whose dynamics can be affected by the actions of a representative\nrisk--neutral producer and a representative risk--neutral trader. Depending on\nthe model, the producer can control the drift and/or the volatility of the\nprice whereas the trader can at most affect the volatility. The producer can\naffect the volatility in two ways: either by randomizing her production rate\nor, as the trader, using other means such as spreading false information.\nMoreover, the producer contracts at time zero a fixed position in a European\nconvex derivative with the trader. The trader can be price-taker, as in the\nfirst two models, or she can also affect the volatility of the commodity price,\nas in the third model. We solve all three models semi--explicitly and give\nclosed--form expressions of the derivative price over a small time horizon,\npreventing arbitrage opportunities to arise. We find that when the trader is\nprice-taker, the producer can always compensate the loss in expected production\nprofit generated by an increase of volatility by a gain in the derivative\nposition by driving the price at maturity to a suitable level. Finally, in case\nthe trader is active, the model takes the form of a nonzero-sum\nlinear-quadratic stochastic differential game and we find that when the\nproduction rate is already at its optimal stationary level, there is an amount\nof derivative position that makes both players better off when entering the\ngame.\n"
    },
    {
        "paper_id": 1909.08021,
        "authors": "Victor Amelkin, Rakesh Vohra",
        "title": "Strategic Formation and Reliability of Supply Chain Networks",
        "comments": "revision 1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Supply chains are the backbone of the global economy. Disruptions to them can\nbe costly. Centrally managed supply chains invest in ensuring their resilience.\nDecentralized supply chains, however, must rely upon the self-interest of their\nindividual components to maintain the resilience of the entire chain.\n  We examine the incentives that independent self-interested agents have in\nforming a resilient supply chain network in the face of production disruptions\nand competition. In our model, competing suppliers are subject to yield\nuncertainty (they deliver less than ordered) and congestion (lead time\nuncertainty or, \"soft\" supply caps). Competing retailers must decide which\nsuppliers to link to based on both price and reliability. In the presence of\nyield uncertainty only, the resulting supply chain networks are sparse.\nRetailers concentrate their links on a single supplier, counter to the idea\nthat they should mitigate yield uncertainty by diversifying their supply base.\nThis happens because retailers benefit from supply variance. It suggests that\ncompetition will amplify output uncertainty. When congestion is included as\nwell, the resulting networks are denser and resemble the bipartite expander\ngraphs that have been proposed in the supply chain literature, thereby,\nproviding the first example of endogenous formation of resilient supply chain\nnetworks, without resilience being explicitly encoded in payoffs. Finally, we\nshow that a supplier's investments in improved yield can make it worse off.\nThis happens because high production output saturates the market, which, in\nturn lowers prices and profits for participants.\n"
    },
    {
        "paper_id": 1909.08047,
        "authors": "Matta Uma Maheswara Reddy",
        "title": "Option pricing under normal dynamics with stochastic volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we derive the price of a European call option of an asset\nfollowing a normal process assuming stochastic volatility. The volatility is\nassumed to follow the Cox Ingersoll Ross (CIR) process. We then use the fast\nFourier transform (FFT) to evaluate the option price given we know the\ncharacteristic function of the return analytically. We compare the results of\nfast Fourier transform with the Monte Carlo simulation results of our process.\nFurther, we present a numerical example to understand the normal implied\nvolatility of the model.\n"
    },
    {
        "paper_id": 1909.08308,
        "authors": "Can Yilmaz Altinigne, Harun Ozkan, Veli Can Kupeli, Zehra Cataltepe",
        "title": "An Empirical Study on Arrival Rates of Limit Orders and Order\n  Cancellation Rates in Borsa Istanbul",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Order book dynamics play an important role in both execution time and price\nformation of orders in an exchange market. In this study, we aim to model the\nlimit order arrival rates in the vicinity of the best bid and the best ask\nprice levels. We use limit order book data for Garanti Bank, which is one of\nthe most traded stocks in Borsa Istanbul. In order to model the daily, weekly,\nand monthly arrival of limit order quantities, three different discrete\nprobability distributions are tested: Geometric, Beta-Binomial and Discrete\nWeibull. Additionally, two theoretical models, namely, Exponential and Power\nlaw are also tested. We aim to model the arrival rates in the first fifteen bid\nand ask price levels. We use L1 norms in order to calculate the goodness-of-fit\nstatistics. Furthermore, we examine the structure of weekly and monthly mean\ncancellation rates in the first ten bid and ask price levels.\n"
    },
    {
        "paper_id": 1909.08564,
        "authors": "Juan Gonzalez-Blanco",
        "title": "Informe-pais Brasil",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Structural socioeconomic analysis of Brazil. All basic information about this\nSouth American country is gathered in a comprehensive outlook that includes the\nchallenges Brazil faces, as well as their causes and posible economic\nsolutions.\n"
    },
    {
        "paper_id": 1909.08648,
        "authors": "Rahul Srinivas Sucharitha and Seokcheon Lee",
        "title": "New Policy Design for Food Accessibility to the People in Need",
        "comments": "6 pages, 2018 IISE Annual Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Food insecurity is a term used to measure hunger and food deprivation of a\nlarge population. As per the 2015 statistics provided by Feeding America - one\nof the largest domestic hunger-relief organizations in the United States, 42.2\nmillion Americans live in food insecure households, including 29.1 million\nadults and 13.1 million children. This constitutes about 13.1% of households\nthat are food insecure. Food Banks have been developed to improve food security\nfor the needy. We have developed a novel food distribution policy using\nsuitable welfare and poverty indices and functions. In this work, we propose an\nequitable and fair distribution of donated foods as per the demands and\nrequirements of the people, thus ensuring minimum wastage of food (perishable\nand non-perishable) with focus towards nutrition. We present results and\nanalysis based on the application of the proposed policy using the information\nof a local food bank as a case study. The results show that the new policy\nperforms better than the current methods in terms of population being covered\nand reduction of food wastage obtaining suitable levels of nutrition.\n"
    },
    {
        "paper_id": 1909.08662,
        "authors": "Dangxing Chen",
        "title": "Does the leverage effect affect the return distribution?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The leverage effect refers to the generally negative correlation between the\nreturn of an asset and the changes in its volatility. There is broad agreement\nin the literature that the effect should be present for theoretical reasons,\nand it has been consistently found in empirical work. However, a few papers\nhave pointed out a puzzle: the return distributions of many assets do not\nappear to be affected by the leverage effect. We analyze the determinants of\nthe return distribution and find that the impact of the leverage effect comes\nprimarily from an interaction between the leverage effect and the\nmean-reversion effect. When the leverage effect is large and the mean-reversion\neffect is small, then the interaction exerts a strong effect on the return\ndistribution. However, if the mean-reversion effect is large, even a large\nleverage effect has little effect on the return distribution. To better\nunderstand the impact of the interaction effect, we propose an indirect method\nto measure it. We apply our methodology to empirical data and find that the S&P\n500 data exhibits a weak interaction effect, and consequently its returns\ndistribution is little impacted by the leverage effect. Furthermore, the\ninteraction effect is closely related to the size factor: small firms tend to\nhave a strong interaction effect and large firms tend to have a weak\ninteraction effect.\n"
    },
    {
        "paper_id": 1909.08664,
        "authors": "Johannes Wachs, Mih\\'aly Fazekas, J\\'anos Kert\\'esz",
        "title": "Corruption Risk in Contracting Markets: A Network Science Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use methods from network science to analyze corruption risk in a large\nadministrative dataset of over 4 million public procurement contracts from\nEuropean Union member states covering the years 2008-2016. By mapping\nprocurement markets as bipartite networks of issuers and winners of contracts\nwe can visualize and describe the distribution of corruption risk. We study the\nstructure of these networks in each member state, identify their cores and find\nthat highly centralized markets tend to have higher corruption risk. In all EU\ncountries we analyze, corruption risk is significantly clustered. However,\nthese risks are sometimes more prevalent in the core and sometimes in the\nperiphery of the market, depending on the country. This suggests that the same\nlevel of corruption risk may have entirely different distributions. Our\nframework is both diagnostic and prescriptive: it roots out where corruption is\nlikely to be prevalent in different markets and suggests that different\nanti-corruption policies are needed in different countries.\n"
    },
    {
        "paper_id": 1909.08706,
        "authors": "S\\'ergio Bacelar and Luis Antunes",
        "title": "Generational political dynamics of retirement pensions systems: An agent\n  based model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The increasing difficulties in financing the welfare state and in particular\npublic retirement pensions have been one of the outcomes both of the decrease\nof fertility and birth rates combined with the increase of life expectancy. The\ndynamics of retirement pensions are usually studied in Economics using\noverlapping generation models. These models are based on simplifying\nassumptions like the use of a representative agent to ease the problem of\ntractability. Alternatively, we propose to use agent-based modelling (ABM),\nrelaxing the need for those assumptions and enabling the use of interacting and\nheterogeneous agents assigning special importance to the study of\ninter-generational relations. We treat pension dynamics both in economics and\npolitical perspectives. The model we build, following the ODD protocol, will\ntry to understand the dynamics of choice of public versus private retirement\npensions resulting from the conflicting preferences of different agents but\nalso from the cooperation between them. The aggregation of these individual\npreferences is done by voting. We combine a microsimulation approach following\nthe evolution of synthetic populations along time, with the ABM approach\nstudying the interactions between the different agent types. Our objective is\nto depict the conditions for the survival of the public pensions system\nemerging from the relation between egoistic and altruistic individual and\ncollective behaviours.\n"
    },
    {
        "paper_id": 1909.08798,
        "authors": "Bashar H. Malkawi",
        "title": "Legal Architecture and Design for Gulf Cooperation Council Economic\n  Integration",
        "comments": "45",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Cooperation Council for the Arab States of the Gulf (GCC) is generally\nregarded as a success story for economic integration in Arab countries. The\nidea of regional integration gained ground by signing the GCC Charter. It\nenvisioned a closer economic relationship between member states.Although\neconomic integration among GCC member states is an ambitious step in the right\ndirection, there are gaps and challenges ahead. The best way to address the\ngaps and challenges that exist in formulating integration processes in the GCC\nis to start with a clear set of rules and put the necessary mechanisms in\nplace. Integration attempts must also exhibit a high level of commitment in\norder to deflect dynamics of disintegration that have all too often frustrated\nmeaningful integration in Arab countries. If the GCC can address these issues,\nit could become an economic powerhouse within Arab countries and even Asia.\n"
    },
    {
        "paper_id": 1909.08964,
        "authors": "Loc Tran, Linh Tran",
        "title": "To Detect Irregular Trade Behaviors In Stock Market By Using Graph Based\n  Ranking Methods",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To detect the irregular trade behaviors in the stock market is the important\nproblem in machine learning field. These irregular trade behaviors are\nobviously illegal. To detect these irregular trade behaviors in the stock\nmarket, data scientists normally employ the supervised learning techniques. In\nthis paper, we employ the three graph Laplacian based semi-supervised ranking\nmethods to solve the irregular trade behavior detection problem. Experimental\nresults show that that the un-normalized and symmetric normalized graph\nLaplacian based semi-supervised ranking methods outperform the random walk\nLaplacian based semi-supervised ranking method.\n"
    },
    {
        "paper_id": 1909.09057,
        "authors": "Osama D. Sweidan and Bashar H. Malkawi",
        "title": "The Effect of Oil Price on United Arab Emirates Goods Trade Deficit with\n  the United States",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We seek to investigate the effect of oil price on UAE goods trade deficit\nwith the U.S. The current increase in the price of oil and the absence of\nsignificant studies in the UAE economy are the main motives behind the current\nstudy. Our paper focuses on a small portion of UAE trade, which is 11% of the\nUAE foreign trade, however, it is a significant part since the U.S. is a major\ntrade partner with the UAE. The current paper concludes that oil price has a\nsignificant positive influence on real imports. At the same time, oil price\ndoes not have a significant effect on real exports. As a result, any increase\nin the price of oil increases goods trade deficit of the UAE economy. The\npolicy implication of the current paper is that the revenue of oil sales is not\nused to encourage UAE real exports.\n"
    },
    {
        "paper_id": 1909.09061,
        "authors": "Bashar H. Malkawi and Mohammad I. El-Shafie",
        "title": "The Design and Operation of Rules of Origin in Greater Arab Free Trade\n  Area: Challenges of Implementation and Reform",
        "comments": "25 pages",
        "journal-ref": "Volume 53 Journal of World Trade 243-272 (2019) (The Netherlands)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rules of origin (ROO) are pivotal element of the Greater Arab Free Trade Area\n(GAFTA). ROO are basically established to ensure that only eligible products\nreceive preferential tariff treatment. Taking into consideration the profound\nimplications of ROO for enhancing trade flows and facilitating the success of\nregional integration, this article sheds light on the way that ROO in GAFTA are\ndesigned and implemented. Moreover, the article examines the extent to which\nROO still represents an obstacle to the full implementation of GAFTA. In\naddition, the article provides ways to overcome the most important shortcomings\nof ROO text in the agreement and ultimately offering possible solutions to\nthose issues.\n"
    },
    {
        "paper_id": 1909.09198,
        "authors": "Cailin O'Connor",
        "title": "Methods, Models, and the Evolution of Moral Psychology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Why are we good? Why are we bad? Questions regarding the evolution of\nmorality have spurred an astoundingly large interdisciplinary literature. Some\nsignificant subset of this body of work addresses questions regarding our moral\npsychology: how did humans evolve the psychological properties which underpin\nour systems of ethics and morality? Here I do three things. First, I discuss\nsome methodological issues, and defend particularly effective methods for\naddressing many research questions in this area. Second, I give an in-depth\nexample, describing how an explanation can be given for the evolution of\nguilt---one of the core moral emotions---using the methods advocated here.\nLast, I lay out which sorts of strategic scenarios generally are the ones that\nour moral psychology evolved to `solve', and thus which models are the most\nuseful in further exploring this evolution.\n"
    },
    {
        "paper_id": 1909.09239,
        "authors": "T. R. Hurd",
        "title": "Systemic Cascades On Inhomogeneous Random Financial Networks",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This systemic risk paper introduces inhomogeneous random financial networks\n(IRFNs). Such models are intended to describe parts, or the entirety, of a\nhighly heterogeneous network of banks and their interconnections, in the global\nfinancial system. Both the balance sheets and the stylized crisis behaviour of\nbanks are ingredients of the network model. A systemic crisis is pictured as\ntriggered by a shock to banks' balance sheets, which then leads to the\npropagation of damaging shocks and the potential for amplification of the\ncrisis, ending with the system in a cascade equilibrium. Under some conditions\nthe model has ``locally tree-like independence (LTI)'', where a general\npercolation theoretic argument leads to an analytic fixed point equation\ndescribing the cascade equilibrium when the number of banks $N$ in the system\nis taken to infinity. This paper focusses on mathematical properties of the\nframework in the context of Eisenberg-Noe solvency cascades generalized to\naccount for fractional bankruptcy charges. New results including a definition\nand proof of the ``LTI property'' of the Eisenberg-Noe solvency cascade\nmechanism lead to explicit $N=\\infty$ fixed point equations that arise under\nvery general model specifications. The essential formulas are shown to be\nimplementable via well-defined approximation schemes, but numerical exploration\nof some of the wide range of potential applications of the method is left for\nfuture work.\n"
    },
    {
        "paper_id": 1909.09257,
        "authors": "Bastien Baldacci, Paul Jusselin and Mathieu Rosenbaum",
        "title": "How to design a derivatives market?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of designing a derivatives exchange aiming at\naddressing clients needs in terms of listed options and providing suitable\nliquidity. We proceed into two steps. First we use a quantization method to\nselect the options that should be displayed by the exchange. Then, using a\nprincipal-agent approach, we design a make take fees contract between the\nexchange and the market maker. The role of this contract is to provide\nincentives to the market maker so that he offers small spreads for the whole\nrange of listed options, hence attracting transactions and meeting the\ncommercial requirements of the exchange.\n"
    },
    {
        "paper_id": 1909.09412,
        "authors": "Dmitry Arkhangelsky, Guido W. Imbens",
        "title": "Doubly Robust Identification for Causal Panel Data Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study identification and estimation of causal effects in settings with\npanel data. Traditionally researchers follow model-based identification\nstrategies relying on assumptions governing the relation between the potential\noutcomes and the observed and unobserved confounders. We focus on a different,\ncomplementary approach to identification where assumptions are made about the\nconnection between the treatment assignment and the unobserved confounders.\nSuch strategies are common in cross-section settings but rarely used with panel\ndata. We introduce different sets of assumptions that follow the two paths to\nidentification and develop a doubly robust approach. We propose estimation\nmethods that build on these identification strategies.\n"
    },
    {
        "paper_id": 1909.09495,
        "authors": "Dmitry Zotikov and Anton Antonov",
        "title": "CME Iceberg Order Detection and Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose a method for detection and prediction of native and synthetic\niceberg orders on Chicago Mercantile Exchange. Native (managed by the exchange)\nicebergs are detected using discrepancies between the resting volume of an\norder and the actual trade size as indicated by trade summary messages, as well\nas by tracking order modifications that follow trade events. Synthetic (managed\nby market participants) icebergs are detected by observing limit orders\narriving within a short time frame after a trade. The obtained icebergs are\nthen used to train a model based on the Kaplan--Meier estimator, accounting for\norders that were cancelled after a partial execution. The model is utilized to\npredict the total size of newly detected icebergs. Out of sample validation is\nperformed on the full order depth data, performance metrics and quantitative\nestimates of hidden volume are presented.\n"
    },
    {
        "paper_id": 1909.09511,
        "authors": "Zhuo Jin, Huafu Liao, Yue Yang, Xiang Yu",
        "title": "Optimal Dividend Strategy for an Insurance Group with Contagious Default\n  Risk",
        "comments": "Final version, forthcoming in Scandinavian Actuarial Journal.\n  Keywords: Insurance group, credit default contagion, optimal dividend,\n  default-state-modulated barriers, recursive system of HJBVIs",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal dividend for a multi-line insurance group, in\nwhich each subsidiary runs a product line and is exposed to some external\ncredit risk. The default contagion is considered such that one default event\nmay increase the default probabilities of all surviving subsidiaries. The total\ndividend problem for the insurance group is investigated and we find that the\noptimal dividend strategy is still of the barrier type. Furthermore, we show\nthat the optimal barrier of each subsidiary is modulated by the default state.\nThat is, how many and which subsidiaries have defaulted will determine the\ndividend threshold of each surviving subsidiary. These conclusions are based on\nthe analysis of the associated recursive system of Hamilton-Jacobi-Bellman\nvariational inequalities (HJBVIs). The existence of the classical solution is\nestablished and the verification theorem is proved. In the case of two\nsubsidiaries, the value function and optimal barriers are given in analytical\nforms, allowing us to conclude that the optimal barrier of one subsidiary\ndecreases if the other subsidiary defaults.\n"
    },
    {
        "paper_id": 1909.09563,
        "authors": "Jialin Liu, Chih-Min Lin and Fei Chao",
        "title": "Gradient Boost with Convolution Neural Network for Stock Forecast",
        "comments": "UKCL2019.11pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market economy closely connects aspects to all walks of life. The stock\nforecast is one of task among studies on the market economy. However,\ninformation on markets economy contains a lot of noise and uncertainties, which\nlead economy forecasting to become a challenging task. Ensemble learning and\ndeep learning are the most methods to solve the stock forecast task. In this\npaper, we present a model combining the advantages of two methods to forecast\nthe change of stock price. The proposed method combines CNN and GBoost. The\nexperimental results on six market indexes show that the proposed method has\nbetter performance against current popular methods.\n"
    },
    {
        "paper_id": 1909.09571,
        "authors": "Angelos Filos",
        "title": "Reinforcement Learning for Portfolio Management",
        "comments": "Imperial College London MEng Thesis 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this thesis, we develop a comprehensive account of the expressive power,\nmodelling efficiency, and performance advantages of so-called trading agents\n(i.e., Deep Soft Recurrent Q-Network (DSRQN) and Mixture of Score Machines\n(MSM)), based on both traditional system identification (model-based approach)\nas well as on context-independent agents (model-free approach). The analysis\nprovides conclusive support for the ability of model-free reinforcement\nlearning methods to act as universal trading agents, which are not only capable\nof reducing the computational and memory complexity (owing to their linear\nscaling with the size of the universe), but also serve as generalizing\nstrategies across assets and markets, regardless of the trading universe on\nwhich they have been trained. The relatively low volume of daily returns in\nfinancial market data is addressed via data augmentation (a generative\napproach) and a choice of pre-training strategies, both of which are validated\nagainst current state-of-the-art models. For rigour, a risk-sensitive framework\nwhich includes transaction costs is considered, and its performance advantages\nare demonstrated in a variety of scenarios, from synthetic time-series\n(sinusoidal, sawtooth and chirp waves), simulated market series (surrogate data\nbased), through to real market data (S\\&P 500 and EURO STOXX 50). The analysis\nand simulations confirm the superiority of universal model-free reinforcement\nlearning agents over current portfolio management model in asset allocation\nstrategies, with the achieved performance advantage of as much as 9.2\\% in\nannualized cumulative returns and 13.4\\% in annualized Sharpe Ratio.\n"
    },
    {
        "paper_id": 1909.09641,
        "authors": "Satoshi Nakano and Kazuhiko Nishimura",
        "title": "Productivity propagation with networks transformation",
        "comments": "arXiv admin note: text overlap with arXiv:1902.01052",
        "journal-ref": "Journal of Macroeconomics 2021",
        "doi": "10.1016/j.jmacro.2020.103216",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model sectoral production by cascading binary compounding processes. The\nsequence of processes is discovered in a self-similar hierarchical structure\nstylized in the economy-wide networks of production. Nested substitution\nelasticities and Hicks-neutral productivity growth are measured such that the\ngeneral equilibrium feedbacks between all sectoral unit cost functions\nreplicate the transformation of networks observed as a set of two temporally\ndistant input-output coefficient matrices. We examine this system of unit cost\nfunctions to determine how idiosyncratic sectoral productivity shocks propagate\ninto aggregate macroeconomic fluctuations in light of potential network\ntransformation. Additionally, we study how sectoral productivity increments\npropagate into the dynamic general equilibrium, thereby allowing network\ntransformation and ultimately producing social benefits.\n"
    },
    {
        "paper_id": 1909.09824,
        "authors": "Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin",
        "title": "Desperate times call for desperate measures: government spending\n  multipliers in hard times",
        "comments": "15 pages, 5 figures, 2 table",
        "journal-ref": "Economic Inquiry, 58(4) October 2020, pp. 1949-1957",
        "doi": "10.1111/ecin.12919",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate state-dependent effects of fiscal multipliers and allow for\nendogenous sample splitting to determine whether the US economy is in a slack\nstate. When the endogenized slack state is estimated as the period of the\nunemployment rate higher than about 12 percent, the estimated cumulative\nmultipliers are significantly larger during slack periods than non-slack\nperiods and are above unity. We also examine the possibility of time-varying\nregimes of slackness and find that our empirical results are robust under a\nmore flexible framework. Our estimation results point out the importance of the\nheterogenous effects of fiscal policy and shed light on the prospect of fiscal\npolicy in response to economic shocks from the current COVID-19 pandemic.\n"
    },
    {
        "paper_id": 1909.10017,
        "authors": "Anita M. Bunea, Pietro Manfredi, Pompeo Della Posta, Mariangela\n  Guidolin",
        "title": "What do adoption patterns of solar panels observed so far tell about\n  governments' incentive? insight from diffusion models",
        "comments": "23 pages, 6 figures, 1 table, under review by Technological\n  Forecasting and Social Change",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper uses diffusion models to understand the main determinants of\ndiffusion of solar photovoltaic panels (SPP) worldwide, focusing on the role of\npublic incentives. We applied the generalized Bass model (GBM) to adoption data\nof 26 countries between 1992-2016. The SPP market appears as a frail and\ncomplicate one, lacking public media support. Even the major shocks in adoption\ncurves, following state incentive implemented after 2006, failed to go beyond\nshort-term effects and therefore were unable to provide sustained momentum to\nthe market. This suggests that further barriers to adoption should be removed.\n"
    },
    {
        "paper_id": 1909.10035,
        "authors": "Peter Carr, Liuren Wu, Zhibai Zhang",
        "title": "Using Machine Learning to Predict Realized Variance",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we formulate a regression problem to predict realized\nvolatility by using option price data and enhance VIX-styled volatility\nindices' predictability and liquidity. We test algorithms including regularized\nregression and machine learning methods such as Feedforward Neural Networks\n(FNN) on S&P 500 Index and its option data. By conducting a time series\nvalidation we find that both Ridge regression and FNN can improve volatility\nindexing with higher prediction performance and fewer options required. The\nbest approach found is to predict the difference between the realized\nvolatility and the VIX-styled index's prediction rather than to predict the\nrealized volatility directly, representing a successful combination of human\nlearning and machine learning. We also discuss suitability of different\nregression algorithms for volatility indexing and applications of our findings.\n"
    },
    {
        "paper_id": 1909.10187,
        "authors": "Jaegi Jeon, Geonwoo Kim, Jeonggyu Huh",
        "title": "Consistent and Efficient Pricing of SPX and VIX Options under Multiscale\n  Stochastic Volatility",
        "comments": "16 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study provides a consistent and efficient pricing method for both\nStandard & Poor's 500 Index (SPX) options and the Chicago Board Options\nExchange's Volatility Index (VIX) options under a multiscale stochastic\nvolatility model. To capture the multiscale volatility of the financial market,\nour model adds a fast scale factor to the well-known Heston volatility and we\nderive approximate analytic pricing formulas for the options under the model.\nThe analytic tractability can greatly improve the efficiency of calibration\ncompared to fitting procedures with the finite difference method or Monte Carlo\nsimulation. Our experiment using options data from 2016 to 2018 shows that the\nmodel reduces the errors on the training sets of the SPX and VIX options by\n9.9% and 13.2%, respectively, and decreases the errors on the test sets of the\nSPX and VIX options by 13.0\\% and 16.5\\%, respectively, compared to the\nsingle-scale model of Heston. The error reduction is possible because the\nadditional factor reflects short-term impacts on the market, which is difficult\nto achieve with only one factor. It highlights the necessity of modeling\nmultiscale volatility.\n"
    },
    {
        "paper_id": 1909.10233,
        "authors": "Sarah Perrin, Thierry Roncalli",
        "title": "Machine Learning Optimization Algorithms & Portfolio Allocation",
        "comments": "66 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization emerged with the seminal paper of Markowitz (1952).\nThe original mean-variance framework is appealing because it is very efficient\nfrom a computational point of view. However, it also has one well-established\nfailing since it can lead to portfolios that are not optimal from a financial\npoint of view. Nevertheless, very few models have succeeded in providing a real\nalternative solution to the Markowitz model. The main reason lies in the fact\nthat most academic portfolio optimization models are intractable in real life\nalthough they present solid theoretical properties. By intractable we mean that\nthey can be implemented for an investment universe with a small number of\nassets using a lot of computational resources and skills, but they are unable\nto manage a universe with dozens or hundreds of assets. However, the emergence\nand the rapid development of robo-advisors means that we need to rethink\nportfolio optimization and go beyond the traditional mean-variance optimization\napproach. Another industry has faced similar issues concerning large-scale\noptimization problems. Machine learning has long been associated with linear\nand logistic regression models. Again, the reason was the inability of\noptimization algorithms to solve high-dimensional industrial problems.\nNevertheless, the end of the 1990s marked an important turning point with the\ndevelopment and the rediscovery of several methods that have since produced\nimpressive results. The goal of this paper is to show how portfolio allocation\ncan benefit from the development of these large-scale optimization algorithms.\nNot all of these algorithms are useful in our case, but four of them are\nessential when solving complex portfolio optimization problems. These four\nalgorithms are the coordinate descent, the alternating direction method of\nmultipliers, the proximal gradient method and the Dykstra's algorithm.\n"
    },
    {
        "paper_id": 1909.10272,
        "authors": "Mehdi El Amrani, Antoine Jacquier, Claude Martini",
        "title": "Dynamics of symmetric SSVI smiles and implied volatility bubbles",
        "comments": "14 pages, 12 figures -- added a numerical example",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a dynamic version of the SSVI parameterisation for the total\nimplied variance, ensuring that European vanilla option prices are martingales,\nhence preventing the occurrence of arbitrage, both static and dynamic.\nInsisting on the constraint that the total implied variance needs to be null at\nthe maturity of the option, we show that no model--in our setting--allows for\nsuch behaviour. This naturally gives rise to the concept of implied volatility\nbubbles, whereby trading in an arbitrage-free way is only possible during part\nof the life of the contract, but not all the way until expiry.\n"
    },
    {
        "paper_id": 1909.10464,
        "authors": "Claudio Bellani and Damiano Brigo",
        "title": "Mechanics of good trade execution in the framework of linear temporary\n  market impact",
        "comments": "31 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define the concept of good trade execution and we construct explicit\nadapted good trade execution strategies in the framework of linear temporary\nmarket impact. Good trade execution strategies are dynamic, in the sense that\nthey react to the actual realisation of the traded asset price path over the\ntrading period; this is paramount in volatile regimes, where price trajectories\ncan considerably deviate from their expected value. Remarkably however, the\nimplementation of our strategies does not require the full specification of an\nSDE evolution for the traded asset price, making them robust across different\nmodels. Moreover, rather than minimising the expected trading cost, good trade\nexecution strategies minimise trading costs in a pathwise sense, a point of\nview not yet considered in the literature. The mathematical apparatus for such\na pathwise minimisation hinges on certain random Young differential equations\nthat correspond to the Euler-Lagrange equations of the classical Calculus of\nVariations. These Young differential equations characterise our good trade\nexecution strategies in terms of an initial value problem that allows for easy\nimplementations.\n"
    },
    {
        "paper_id": 1909.10578,
        "authors": "Giovanni Mariani, Yada Zhu, Jianbo Li, Florian Scheidegger, Roxana\n  Istrate, Costas Bekas, A. Cristiano I. Malossi",
        "title": "PAGAN: Portfolio Analysis with Generative Adversarial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since decades, the data science community tries to propose prediction models\nof financial time series. Yet, driven by the rapid development of information\ntechnology and machine intelligence, the velocity of today's information leads\nto high market efficiency. Sound financial theories demonstrate that in an\nefficient marketplace all information available today, including expectations\non future events, are represented in today prices whereas future price trend is\ndriven by the uncertainty. This jeopardizes the efforts put in designing\nprediction models. To deal with the unpredictability of financial systems,\ntoday's portfolio management is largely based on the Markowitz framework which\nputs more emphasis in the analysis of the market uncertainty and less in the\nprice prediction. The limitation of the Markowitz framework stands in taking\nvery strong ideal assumptions about future returns probability distribution.\n  To address this situation we propose PAGAN, a pioneering methodology based on\ndeep generative models. The goal is modeling the market uncertainty that\nultimately is the main factor driving future trends. The generative model\nlearns the joint probability distribution of price trends for a set of\nfinancial assets to match the probability distribution of the real market. Once\nthe model is trained, a portfolio is optimized by deciding the best\ndiversification to minimize the risk and maximize the expected returns observed\nover the execution of several simulations. Applying the model for analyzing\npossible futures, is as simple as executing a Monte Carlo simulation, a\ntechnique very familiar to finance experts. The experimental results on\ndifferent portfolios representing different geopolitical areas and industrial\nsegments constructed using real-world public data sets demonstrate promising\nresults.\n"
    },
    {
        "paper_id": 1909.1066,
        "authors": "Daiki Matsunaga, Toyotaro Suzumura, Toshihiro Takahashi",
        "title": "Exploring Graph Neural Networks for Stock Market Predictions with\n  Rolling Window Analysis",
        "comments": "NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy (Robust AI in FS),\n  Vancouver, Canada",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, there has been a surge of interest in the use of machine learning\nto help aid in the accurate predictions of financial markets. Despite the\nexciting advances in this cross-section of finance and AI, many of the current\napproaches are limited to using technical analysis to capture historical trends\nof each stock price and thus limited to certain experimental setups to obtain\ngood prediction results. On the other hand, professional investors additionally\nuse their rich knowledge of inter-market and inter-company relations to map the\nconnectivity of companies and events, and use this map to make better market\npredictions. For instance, they would predict the movement of a certain\ncompany's stock price based not only on its former stock price trends but also\non the performance of its suppliers or customers, the overall industry,\nmacroeconomic factors and trade policies. This paper investigates the\neffectiveness of work at the intersection of market predictions and graph\nneural networks, which hold the potential to mimic the ways in which investors\nmake decisions by incorporating company knowledge graphs directly into the\npredictive model. The main goal of this work is to test the validity of this\napproach across different markets and longer time horizons for backtesting\nusing rolling window analysis. In this work, we concentrate on the prediction\nof individual stock prices in the Japanese Nikkei 225 market over a period of\nroughly 20 years. For the knowledge graph, we use the Nikkei Value Search data,\nwhich is a rich dataset showing mainly supplier relations among Japanese and\nforeign companies. Our preliminary results show a 29.5% increase and a 2.2-fold\nincrease in the return ratio and Sharpe ratio, respectively, when compared to\nthe market benchmark, as well as a 6.32% increase and 1.3-fold increase,\nrespectively, compared to the baseline LSTM model.\n"
    },
    {
        "paper_id": 1909.10679,
        "authors": "C.Y.Tan, Y.B.Koh, K.H.Ng and K.H.Ng",
        "title": "Structural Change Analysis of Active Cryptocurrency Market",
        "comments": "18 pages, 6 figures and 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Structural Change Analysis of Active Cryptocurrency Market\n"
    },
    {
        "paper_id": 1909.10735,
        "authors": "Niushan Gao, Cosimo Munari, Foivos Xanthos",
        "title": "Stability properties of Haezendonck-Goovaerts premium principles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a variety of stability properties of Haezendonck-Goovaerts\npremium principles on their natural domain, namely Orlicz spaces. We show that\nsuch principles always satisfy the Fatou property. This allows to establish a\ntractable dual representation without imposing any condition on the reference\nOrlicz function. In addition, we show that Haezendonck-Goovaerts principles\nsatisfy the stronger Lebesgue property if and only if the reference Orlicz\nfunction fulfills the so-called $\\Delta_2$ condition. We also discuss\n(semi)continuity properties with respect to $\\Phi$-weak convergence of\nprobability measures. In particular, we show that Haezendonck-Goovaerts\nprinciples, restricted to the corresponding Young class, are always lower\nsemicontinuous with respect to the $\\Phi$-weak convergence.\n"
    },
    {
        "paper_id": 1909.10749,
        "authors": "S\\\"oren Christensen and Kristoffer Lindensj\\\"o",
        "title": "Moment constrained optimal dividends: precommitment \\& consistent\n  planning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A moment constraint that limits the number of dividends in the optimal\ndividend problem is suggested. This leads to a new type of time-inconsistent\nstochastic impulse control problem. First, the optimal solution in the\nprecommitment sense is derived. Second, the problem is formulated as an\nintrapersonal sequential dynamic game in line with Strotz' consistent planning.\nIn particular, the notions of pure dividend strategies and a (strong) subgame\nperfect Nash equilibrium are adapted. An equilibrium is derived using a smooth\nfit condition. The equilibrium is shown to be strong. The uncontrolled state\nprocess is a fairly general diffusion.\n"
    },
    {
        "paper_id": 1909.10762,
        "authors": "Akshay Bansal, Diganta Mukherjee",
        "title": "Optimizing Execution Cost Using Stochastic Control",
        "comments": null,
        "journal-ref": "New Perspectives and Challenges in Econophysics and Sociophysics;\n  pp 49-59, New Economic Windows Series 2019. Springer, Cham",
        "doi": "10.1007/978-3-030-11364-3_4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We devise an optimal allocation strategy for the execution of a predefined\nnumber of stocks in a given time frame using the technique of discrete-time\nStochastic Control Theory for a defined market model. This market structure\nallows an instant execution of the market orders and has been analyzed based on\nthe assumption of discretized geometric movement of the stock prices. We\nconsider two different cost functions where the first function involves just\nthe fiscal cost while the cost function of the second kind incorporates the\nrisks of non-strategic constrained investments along with fiscal costs.\nPrecisely, the strategic development of constrained execution of K stocks\nwithin a stipulated time frame of T units is established mathematically using a\nwell-defined stochastic behaviour of stock prices and the same is compared with\nsome of the commonly-used execution strategies using the historical stock price\ndata.\n"
    },
    {
        "paper_id": 1909.10801,
        "authors": "Michael Poli, Jinkyoo Park, Ilija Ilievski",
        "title": "WATTNet: Learning to Trade FX via Hierarchical Spatio-Temporal\n  Representation of Highly Multivariate Time Series",
        "comments": "Submitted to the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI 20)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finance is a particularly challenging application area for deep learning\nmodels due to low noise-to-signal ratio, non-stationarity, and partial\nobservability. Non-deliverable-forwards (NDF), a derivatives contract used in\nforeign exchange (FX) trading, presents additional difficulty in the form of\nlong-term planning required for an effective selection of start and end date of\nthe contract. In this work, we focus on tackling the problem of NDF tenor\nselection by leveraging high-dimensional sequential data consisting of spot\nrates, technical indicators and expert tenor patterns. To this end, we\nconstruct a dataset from the Depository Trust & Clearing Corporation (DTCC) NDF\ndata that includes a comprehensive list of NDF volumes and daily spot rates for\n64 FX pairs. We introduce WaveATTentionNet (WATTNet), a novel temporal\nconvolution (TCN) model for spatio-temporal modeling of highly multivariate\ntime series, and validate it across NDF markets with varying degrees of\ndissimilarity between the training and test periods in terms of volatility and\ngeneral market regimes. The proposed method achieves a significant positive\nreturn on investment (ROI) in all NDF markets under analysis, outperforming\nrecurrent and classical baselines by a wide margin. Finally, we propose two\northogonal interpretability approaches to verify noise stability and detect the\ndriving factors of the learned tenor selection strategy.\n"
    },
    {
        "paper_id": 1909.10807,
        "authors": "Carlo Campajola, Fabrizio Lillo and Daniele Tantari",
        "title": "Unveiling the relation between herding and liquidity with trader\n  lead-lag networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2020.1763442",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method to infer lead-lag networks of traders from the\nobservation of their trade record as well as to reconstruct their state of\nsupply and demand when they do not trade. The method relies on the Kinetic\nIsing model to describe how information propagates among traders, assigning a\npositive or negative \"opinion\" to all agents about whether the traded asset\nprice will go up or down. This opinion is reflected by their trading behavior,\nbut whenever the trader is not active in a given time window, a missing value\nwill arise. Using a recently developed inference algorithm, we are able to\nreconstruct a lead-lag network and to estimate the unobserved opinions, giving\na clearer picture about the state of supply and demand in the market at all\ntimes.\n  We apply our method to a dataset of clients of a major dealer in the Foreign\nExchange market at the 5 minutes time scale. We identify leading players in the\nmarket and define a herding measure based on the observed and inferred\nopinions. We show the causal link between herding and liquidity in the\ninter-dealer market used by dealers to rebalance their inventories.\n"
    },
    {
        "paper_id": 1909.10957,
        "authors": "Constandina Koki, Stefanos Leonardos and Georgios Piliouras",
        "title": "A Peek into the Unobservable: Hidden States and Bayesian Inference for\n  the Bitcoin and Ether Price Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conventional financial models fail to explain the economic and monetary\nproperties of cryptocurrencies due to the latter's dual nature: their usage as\nfinancial assets on the one side and their tight connection to the underlying\nblockchain structure on the other. In an effort to examine both components via\na unified approach, we apply a recently developed Non-Homogeneous Hidden Markov\n(NHHM) model with an extended set of financial and blockchain specific\ncovariates on the Bitcoin (BTC) and Ether (ETH) price data. Based on the\nobservable series, the NHHM model offers a novel perspective on the underlying\nmicrostructure of the cryptocurrency market and provides insight on\nunobservable parameters such as the behavior of investors, traders and miners.\nThe algorithm identifies two alternating periods (hidden states) of inherently\ndifferent activity -- fundamental versus uninformed or noise traders -- in the\nBitcoin ecosystem and unveils differences in both the short/long run dynamics\nand in the financial characteristics of the two states, such as significant\nexplanatory variables, extreme events and varying series autocorrelation. In a\nsomewhat unexpected result, the Bitcoin and Ether markets are found to be\ninfluenced by markedly distinct indicators despite their perceived correlation.\nThe current approach backs earlier findings that cryptocurrencies are unlike\nany conventional financial asset and makes a first step towards understanding\ncryptocurrency markets via a more comprehensive lens.\n"
    },
    {
        "paper_id": 1909.11009,
        "authors": "Fearghal Kearney, Han Lin Shang, Lisa Sheenan",
        "title": "Implied volatility surface predictability: the case of commodity markets",
        "comments": "35 pages, 6 figures, 9 tables, to appear in Journal of Banking and\n  Finance",
        "journal-ref": "Journal of Banking & Finance, 2019, 108, 105657",
        "doi": "10.1016/j.jbankfin.2019.105657",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent literature seek to forecast implied volatility derived from equity,\nindex, foreign exchange, and interest rate options using latent factor and\nparametric frameworks. Motivated by increased public attention borne out of the\nfinancialization of futures markets in the early 2000s, we investigate if these\nextant models can uncover predictable patterns in the implied volatility\nsurfaces of the most actively traded commodity options between 2006 and 2016.\nAdopting a rolling out-of-sample forecasting framework that addresses the\ncommon multiple comparisons problem, we establish that, for energy and precious\nmetals options, explicitly modeling the term structure of implied volatility\nusing the Nelson-Siegel factors produces the most accurate forecasts.\n"
    },
    {
        "paper_id": 1909.11532,
        "authors": "Yangang Chen, Justin W. L. Wan",
        "title": "Deep Neural Network Framework Based on Backward Stochastic Differential\n  Equations for Pricing and Hedging American Options in High Dimensions",
        "comments": "35 pages, 11 figures, 15 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deep neural network framework for computing prices and deltas of\nAmerican options in high dimensions. The architecture of the framework is a\nsequence of neural networks, where each network learns the difference of the\nprice functions between adjacent timesteps. We introduce the least squares\nresidual of the associated backward stochastic differential equation as the\nloss function. Our proposed framework yields prices and deltas on the entire\nspacetime, not only at a given point. The computational cost of the proposed\napproach is quadratic in dimension, which addresses the curse of dimensionality\nissue that state-of-the-art approaches suffer. Our numerical simulations\ndemonstrate these contributions, and show that the proposed neural network\nframework outperforms state-of-the-art approaches in high dimensions.\n"
    },
    {
        "paper_id": 1909.11633,
        "authors": "Mohsen Zamani, Mahdi Abolghasemi, Seyed Mohammad Seyed Hosseini, Mir\n  Saman Pishvaee",
        "title": "Considering pricing and uncertainty in designing a reverse logistics\n  network",
        "comments": null,
        "journal-ref": "International Journal of Industrial and Systems Engineering, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Companies try to maximize their profits by recovering returned products of\nhighly uncertain quality and quantity. In this paper, a reverse logistics\nnetwork for an Original Equipment Manufacturer (OEM) is presented. Returned\nproducts are selected for remanufacturing or scrapping, based on their quality\nand proportional prices are offered to customers. A Mixed Integer Non-linear\nProgramming (MINLP) model is proposed to determine the location of collection\ncenters, the optimum price of returned products and the sorting policy. The\nrisk in the objective function is measured using the Conditional Value at Risk\n(CVaR) metric. CVaR measures the risk of an investment in a conservative way by\nconsidering the maximum lost. The results are analyzed for various values of\nthe risk parameters ({\\alpha}, and {\\lambda}). These parameters indicate that\nconsidering risk affects prices, the classification of returned products, the\nlocation of collection centers and, consequently, the objective function. The\nmodel performs more conservatively when the weight of the CVaR part ({\\lambda})\nand the value of the confidence level {\\alpha} are increased. The results show\nthat better profits are obtained when we take CVaR into account.\n"
    },
    {
        "paper_id": 1909.11635,
        "authors": "Pierre Gosselin (IF), A\\\"ileen Lotz, Marc Wambst (IRMA)",
        "title": "A Statistical Field Approach to Capital Accumulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a model of capital accumulation for a large number of\nheterogenous producer-consumers in an exchange space in which interactions\ndepend on agents' positions. Each agent is described by his production,\nconsumption, stock of capital, as well as the position he occupies in this\nabstract space. Each agent produces one differentiated good whose price is\nfixed by market clearing conditions. Production functions are Cobb-Douglas, and\ncapital stocks follow the standard capital accumulation dynamic equation.\nAgents consume all goods but have a preference for goods produced by their\nclosest neighbors. Agents in the exchange space are subject both to attractive\nand repulsive forces. Exchanges drive agents closer, but beyond a certain level\nof proximity, agents will tend to crowd out more distant agents. The present\nmodel uses a formalism based on statistical field theory developed earlier by\nthe authors. This approach allows the analytical treatment of economic models\nwith an arbitrary number of agents, while preserving the system's interactions\nand complexity at the individual level. Our results show that the dynamics of\ncapital accumulation and agents' position in the exchange space are correlated.\nInteractions in the exchange space induce several phases of the system. A first\nphase appears when attractive forces are limited. In this phase, an initial\ncentral position in the exchange space favors capital accumulation in average\nand leads to a higher level of capital, while agents far from the center will\nexperience a slower accumulation process. A high level of initial capital\ndrives agents towards a central position, i.e. improve the terms of their\nexchanges: they experience a higher demand and higher prices for their product.\nAs usual, high capital productivity favors capital accumulation, while higher\nrates of capital depreciation reduce capital stock. In a second phase,\nattractive forces are predominant. The previous results remain, but an\nadditional threshold effect appears. Even though no restriction was imposed\ninitially on the system, two types of agents emerge, depending on their initial\nstock of capital. One type of agents will remain above the capital threshold\nand occupy and benefit from a central position. The other type will remain\nbelow the threshold, will not be able to break it and will remain at the\nperiphery of the exchange space. In this phase, capital distribution is less\nhomogenous than in the first phase.\n"
    },
    {
        "paper_id": 1909.1165,
        "authors": "David Byrd",
        "title": "Explaining Agent-Based Financial Market Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is intended to explain, in simple terms, some of the mechanisms\nand agents common to multiagent financial market simulations. We first discuss\nthe necessity to include an exogenous price time series (\"the fundamental\nvalue\") for each asset and three methods for generating that series. We then\nillustrate one process by which a Bayesian agent may receive limited\nobservations of the fundamental series and estimate its current and future\nvalues. Finally, we present two such agents widely examined in the literature,\nthe Zero Intelligence agent and the Heuristic Belief Learning agent, which\nimplement different approaches to order placement.\n"
    },
    {
        "paper_id": 1909.11794,
        "authors": "Takaaki Koike and Marius Hofert",
        "title": "Markov Chain Monte Carlo Methods for Estimating Systemic Risk\n  Allocations",
        "comments": "36 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel framework of estimating systemic risk measures and risk\nallocations based on Markov chain Monte Carlo (MCMC) methods. We consider a\nclass of allocations whose jth component can be written as some risk measure of\nthe jth conditional marginal loss distribution given the so-called crisis\nevent. By considering a crisis event as an intersection of linear constraints,\nthis class of allocations covers, for example, conditional Value-at-Risk\n(CoVaR), conditional expected shortfall (CoES), VaR contributions, and range\nVaR (RVaR) contributions as special cases. For this class of allocations,\nanalytical calculations are rarely available, and numerical computations based\non Monte Carlo (MC) methods often provide inefficient estimates due to the\nrare-event character of the crisis events. We propose an MCMC estimator\nconstructed from a sample path of a Markov chain whose stationary distribution\nis the conditional distribution given the crisis event. Efficient constructions\nof Markov chains, such as Hamiltonian Monte Carlo and Gibbs sampler, are\nsuggested and studied depending on the crisis event and the underlying loss\ndistribution. The efficiency of the MCMC estimators is demonstrated in a series\nof numerical experiments.\n"
    },
    {
        "paper_id": 1909.11836,
        "authors": "Anqi Li, Davin Raiha and Kenneth W. Shotts",
        "title": "Propaganda, Alternative Media, and Accountability in Fragile Democracies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model of electoral accountability with mainstream and\nalternative media. In addition to regular high- and low-competence types, the\nincumbent may be an aspiring autocrat who controls the mainstream media and\nwill subvert democracy if retained in office. A truthful alternative media can\nhelp voters identify and remove these subversive types while re-electing\ncompetent leaders. A malicious alternative media, in contrast, spreads false\naccusations about the incumbent and demotivates policy effort. If the\nalternative media is very likely be malicious and hence is unreliable, voters\nignore it and use only the mainstream media to hold regular incumbents\naccountable, leaving aspiring autocrats to win re-election via propaganda that\nportrays them as effective policymakers. When the alternative media's\nreliability is intermediate, voters heed its warnings about subversive\nincumbents, but the prospect of being falsely accused demotivates effort by\nregular incumbents and electoral accountability breaks down.\n"
    },
    {
        "paper_id": 1909.12063,
        "authors": "Qi Deng",
        "title": "Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The AIBC is an Artificial Intelligence and blockchain technology based\nlarge-scale decentralized ecosystem that allows system-wide low-cost sharing of\ncomputing and storage resources. The AIBC consists of four layers: a\nfundamental layer, a resource layer, an application layer, and an ecosystem\nlayer. The AIBC implements a two-consensus scheme to enforce upper-layer\neconomic policies and achieve fundamental layer performance and robustness: the\nDPoEV incentive consensus on the application and resource layers, and the DABFT\ndistributed consensus on the fundamental layer. The DABFT uses deep learning\ntechniques to predict and select the most suitable BFT algorithm in order to\nachieve the best balance of performance, robustness, and security. The DPoEV\nuses the knowledge map algorithm to accurately assess the economic value of\ndigital assets.\n"
    },
    {
        "paper_id": 1909.12243,
        "authors": "Yi Huang and Ishanu Chattopadhyay",
        "title": "Data Smashing 2.0: Sequence Likelihood (SL) Divergence For Fast Time\n  Series Comparison",
        "comments": "typos corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recognizing subtle historical patterns is central to modeling and forecasting\nproblems in time series analysis. Here we introduce and develop a new approach\nto quantify deviations in the underlying hidden generators of observed data\nstreams, resulting in a new efficiently computable universal metric for time\nseries. The proposed metric is in the sense that we can compare and contrast\ndata streams regardless of where and how they are generated and without any\nfeature engineering step. The approach proposed in this paper is conceptually\ndistinct from our previous work on data smashing, and vastly improves\ndiscrimination performance and computing speed. The core idea here is the\ngeneralization of the notion of KL divergence often used to compare probability\ndistributions to a notion of divergence in time series. We call this the\nsequence likelihood (SL) divergence, which may be used to measure deviations\nwithin a well-defined class of discrete-valued stochastic processes. We devise\nefficient estimators of SL divergence from finite sample paths and subsequently\nformulate a universal metric useful for computing distance between time series\nproduced by hidden stochastic generators.\n"
    },
    {
        "paper_id": 1909.12542,
        "authors": "Abhik Ghosh, Preety Shreya, Banasri Basu",
        "title": "Maximum Entropy Framework for a Universal Rank Order distribution with\n  Socio-economic Applications",
        "comments": "Pre-print, Under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive the maximum entropy characteristics of a particular\nrank order distribution, namely the discrete generalized beta distribution,\nwhich has recently been observed to be extremely useful in modelling many\nseveral rank-size distributions from different context in Arts and Sciences, as\na two-parameter generalization of Zipf's law. Although it has been seen to\nprovide excellent fits for several real world empirical datasets, the\nunderlying theory responsible for the success of this particular rank order\ndistribution is not explored properly. Here we, for the first time, provide its\ngenerating process which describes it as a natural maximum entropy distribution\nunder an appropriate bivariate utility constraint. Further, considering the\nsimilarity of the proposed utility function with the usual logarithmic utility\nfunction from economic literature, we have also explored its acceptability in\nuniversal modeling of different types of socio-economic factors within a\ncountry as well as across the countries. The values of distributional\nparameters estimated through a rigorous statistical estimation method, along\nwith the $entropy$ values, are used to characterize the distributions of all\nthese socio-economic factors over the years.\n"
    },
    {
        "paper_id": 1909.12578,
        "authors": "Nacira Agram, Bernt {\\O}ksendal",
        "title": "A financial market with singular drift and no arbitrage",
        "comments": "arXiv admin note: text overlap with arXiv:1504.02581",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a financial market where the risky asset is modelled by a geometric\nIt\\^o-L\\'{e}vy process, with a singular drift term. This can for example model\na situation where the asset price is partially controlled by a company which\nintervenes when the price is reaching a certain lower barrier. See e.g. Jarrow\n& Protter for an explanation and discussion of this model in the Brownian\nmotion case. As already pointed out by Karatzas & Shreve (in the continuous\nsetting), this allows for arbitrages in the market. However, the situation in\nthe case of jumps is not clear. Moreover, it is not clear what happens if there\nis a delay in the system.\n  In this paper we consider a jump diffusion market model with a singular drift\nterm modelled as the local time of a given process, and with a delay \\theta>0\nin the information flow available for the trader. We allow the stock price\ndynamics to depend on both a continuous process (Brownian motion) and a jump\nprocess (Poisson random measure). We believe that jumps and delays are\nessential in order to get more realistic financial market models.\n  Using white noise calculus we compute explicitly the optimal consumption rate\nand portfolio in this case and we show that the maximal value is finite as long\nas the delay \\theta> 0. This implies that there is no arbitrage in the market\nin that case. However, when \\theta goes to 0, the value goes to infinity. This\nis in agreement with the above result that is an arbitrage when there is no\ndelay.\n  Our model is also relevant for high frequency trading issues. This is because\nhigh frequency trading often leads to intensive trading taking place on close\nto infinitesimal lengths of time, which in the limit corresponds to trading on\ntime sets of measure 0. This may in turn lead to a singular drift in the\npricing dynamics. See e.g. Lachapelle et al and the references therein.\n"
    },
    {
        "paper_id": 1909.1273,
        "authors": "John Armstrong, Cristin Buescu",
        "title": "Collectivised Post-Retirement Investment",
        "comments": "Split original paper into two",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify the benefit of collectivised investment funds, in which the\nassets of members who die are shared among the survivors. For our model, with\nrealistic parameter choices, an annuity or individual fund requires\napproximately 20\\% more initial capital to provide as good an outcome as a\ncollectivised investment fund. We demonstrate the importance of the new concept\nof pension adequacy in defining investor preferences and determining optimal\nfund management. We show how to manage heterogeneous funds of investors with\ndiverse needs. Our framework can be applied to existing pension products, such\nas Collective Defined Contribution schemes.\n"
    },
    {
        "paper_id": 1909.12829,
        "authors": "Gang Li, Joy M. Field, Hongxun Jiang, Tian He, Youming Pang",
        "title": "Decision Models for Workforce and Technology Planning in Services",
        "comments": "31 pages, 7 tables",
        "journal-ref": "serv. sci. 7 (2015) 1-19",
        "doi": "10.1287/serv.2015.0094",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Today's service companies operate in a technology-oriented and\nknowledge-intensive environment while recruiting and training individuals from\nan increasingly diverse population. One of the resulting challenges is ensuring\nstrategic alignment between their two key resources - technology and workforce\n- through the resource planning and allocation processes. The traditional\nhierarchical decision approach to resource planning and allocation considers\nonly technology planning as a strategic-level decision, with workforce\nrecruiting and training planning as a subsequent tactical-level decision.\nHowever, two other decision approaches - joint and integrated - elevate\nworkforce planning to the same strategic level as technology planning. Thus we\ninvestigate the impact of strategically aligning technology and workforce\ndecisions through the comparison of joint and integrated models to each other\nand to a baseline hierarchical model in terms of the total cost. Numerical\nexperiments are conducted to characterize key features of solutions provided by\nthese approaches under conditions typically found in this type of service\ncompany. Our results show that the integrated model is the lowest cost across\nall conditions. This is because the integrated approach maintains a small but\nskilled workforce that can operate new and more advanced technology with higher\ncapacity. However, the cost performance of the joint model is very close to the\nintegrated model under many conditions and is easier to implement\ncomputationally and managerially, making it a good choice in many environments.\nManagerial insights derived from this study can serve as a valuable guide for\nchoosing the proper decision approach for technology-oriented and\nknowledge-intensive service companies.\n"
    },
    {
        "paper_id": 1909.12904,
        "authors": "Samudra Dasgupta and Arnab Banerjee",
        "title": "Quantum Annealing Algorithm for Expected Shortfall based Dynamic Asset\n  Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2008 mortgage crisis is an example of an extreme event. Extreme value\ntheory tries to estimate such tail risks. Modern finance practitioners prefer\nExpected Shortfall based risk metrics (which capture tail risk) over\ntraditional approaches like volatility or even Value-at-Risk. This paper\nprovides a quantum annealing algorithm in QUBO form for a dynamic asset\nallocation problem using expected shortfall constraint. It was motivated by the\nneed to refine the current quantum algorithms for Markowitz type problems which\nare academically interesting but not useful for practitioners. The algorithm is\ndynamic and the risk target emerges naturally from the market volatility.\nMoreover, it avoids complicated statistics like generalized pareto\ndistribution. It translates the problem into qubit form suitable for\nimplementation by a quantum annealer like D-Wave. Such QUBO algorithms are\nexpected to be solved faster using quantum annealing systems than any classical\nalgorithm using classical computer (but yet to be demonstrated at scale).\n"
    },
    {
        "paper_id": 1909.12926,
        "authors": "Bradley Miles and Dave Cliff",
        "title": "A Cloud-Native Globally Distributed Financial Exchange Simulator for\n  Studying Real-World Trading-Latency Issues at Planetary Scale",
        "comments": "10 pages, 5 figures. To be presented at the European Modelling and\n  Simulation Symposium (EMSS2019) Lisbon, Portugal, 18th-20th September 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a new public-domain open-source simulator of an electronic\nfinancial exchange, and of the traders that interact with the exchange, which\nis a truly distributed and cloud-native system that been designed to run on\nwidely available commercial cloud-computing services, and in which various\ncomponents can be placed in specified geographic regions around the world,\nthereby enabling the study of planetary-scale latencies in contemporary\nautomated trading systems. Our simulator allows an exchange server to be\nlaunched in the cloud, specifying a particular geographic zone for the cloud\nhosting service; automated-trading clients which attach to the exchange can\nthen also be launched in the cloud, in the same geographic zone and/or in\ndifferent zones anywhere else on the planet, and those clients are then subject\nto the real-world latencies introduced by planetary-scale cloud communication\ninterconnections. In this paper we describe the design and implementation of\nour simulator, called DBSE, which is based on a previous public-domain\nsimulator, extended in ways that are partly inspired by the architecture of the\nreal-world Jane Street Exchange. DBSE relies fundamentally on UDP and TCP\nnetwork communications protocols and implements a subset of the FIX de facto\nstandard protocol for financial information exchange. We show results from an\nexample in which the exchange server is remotely launched on a cloud facility\nlocated in London (UK), with trader clients running in Ohio (USA) and Sydney\n(Australia). We close with discussion of how our simulator could be further\nused to study planetary-scale latency arbitrage in financial markets.\n"
    },
    {
        "paper_id": 1909.12931,
        "authors": "D\\'ora Gr\\'eta Petr\\'oczy, L\\'aszl\\'o Csat\\'o",
        "title": "Revenue allocation in Formula One: a pairwise comparison approach",
        "comments": "19 pages, 3 figures, 6 tables",
        "journal-ref": "International Journal of General Systems, 50(3): 243-261, 2021",
        "doi": "10.1080/03081079.2020.1870224",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A model is proposed to allocate Formula One World Championship prize money\namong the constructors. The methodology is based on pairwise comparison\nmatrices, allows for the use of any weighting method, and makes possible to\ntune the level of inequality. We introduce an axiom called scale invariance,\nwhich requires the ranking of the teams to be independent of the parameter\ncontrolling inequality. The eigenvector method is revealed to violate this\ncondition in our dataset, while the row geometric mean method always satisfies\nit. The revenue allocation is not influenced by the arbitrary valuation given\nto the race prizes in the official points scoring system of Formula One and\ntakes the intensity of pairwise preferences into account, contrary to the\nstandard Condorcet method. Our approach can be used to share revenues among\ngroups when group members are ranked several times.\n"
    },
    {
        "paper_id": 1909.12946,
        "authors": "Toyotaro Suzumura, Yi Zhou, Natahalie Baracaldo, Guangnan Ye, Keith\n  Houck, Ryo Kawahara, Ali Anwar, Lucia Larise Stavarache, Yuji Watanabe, Pablo\n  Loyola, Daniel Klyashtorny, Heiko Ludwig, Kumar Bhaskaran",
        "title": "Towards Federated Graph Learning for Collaborative Financial Crimes\n  Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial crime is a large and growing problem, in some way touching almost\nevery financial institution. Financial institutions are the front line in the\nwar against financial crime and accordingly, must devote substantial human and\ntechnology resources to this effort. Current processes to detect financial\nmisconduct have limitations in their ability to effectively differentiate\nbetween malicious behavior and ordinary financial activity. These limitations\ntend to result in gross over-reporting of suspicious activity that necessitate\ntime-intensive and costly manual review. Advances in technology used in this\ndomain, including machine learning based approaches, can improve upon the\neffectiveness of financial institutions' existing processes, however, a key\nchallenge that most financial institutions continue to face is that they\naddress financial crimes in isolation without any insight from other firms.\nWhere financial institutions address financial crimes through the lens of their\nown firm, perpetrators may devise sophisticated strategies that may span across\ninstitutions and geographies. Financial institutions continue to work\nrelentlessly to advance their capabilities, forming partnerships across\ninstitutions to share insights, patterns and capabilities. These public-private\npartnerships are subject to stringent regulatory and data privacy requirements,\nthereby making it difficult to rely on traditional technology solutions. In\nthis paper, we propose a methodology to share key information across\ninstitutions by using a federated graph learning platform that enables us to\nbuild more accurate machine learning models by leveraging federated learning\nand also graph learning approaches. We demonstrated that our federated model\noutperforms local model by 20% with the UK FCA TechSprint data set. This new\nplatform opens up a door to efficiently detecting global money laundering\nactivity.\n"
    },
    {
        "paper_id": 1909.13019,
        "authors": "Abootaleb Shirvani, Stoyan V. Stoyanov, Frank J. Fabozzi, and\n  Svetlozar T. Rachev",
        "title": "Equity Premium Puzzle or Faulty Economic Modelling?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we revisit the equity premium puzzle reported in 1985 by Mehra\nand Prescott. We show that the large equity premium that they report can be\nexplained by choosing a more appropriate distribution for the return data. We\ndemonstrate that the high-risk aversion value observed by Mehra and Prescott\nmay be attributable to the problem of fitting a proper distribution to the\nhistorical returns and partly caused by poorly fitting the tail of the return\ndistribution. We describe a new distribution that better fits the return\ndistribution and when used to describe historical returns can explain the large\nequity risk premium and thereby explains the puzzle.\n"
    },
    {
        "paper_id": 1909.13102,
        "authors": "Shuzhen Yang",
        "title": "A varying terminal time mean-variance model",
        "comments": "19pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To improve the efficient frontier of the classical mean-variance model in\ncontinuous time, we propose a varying terminal time mean-variance model with a\nconstraint on the mean value of the portfolio asset, which moves with the\nvarying terminal time. Using the embedding technique from stochastic optimal\ncontrol in continuous time and varying the terminal time, we determine an\noptimal strategy and related deterministic terminal time for the model. Our\nresults suggest that doing so for an investment plan requires minimizing the\nvariance with a varying terminal time.\n"
    },
    {
        "paper_id": 1909.13179,
        "authors": "Tony Blakely, Nhung Nghiem, Murat Genc, Anja Mizdrak, Linda Cobiac,\n  Cliona Ni Mhurchu, Boyd Swinburn, Peter Scarborough, Christine Cleghorn",
        "title": "Modelling the health impact of food taxes and subsidies with price\n  elasticities: the case for additional scaling of food consumption using the\n  total food expenditure elasticity",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0230506",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Background Food taxes and subsidies are one intervention to address poor\ndiets. Price elasticity (PE) matrices are commonly used to model the change in\nfood purchasing. Usually a PE matrix is generated in one setting then applied\nto another setting with differing starting consumption and prices of foods.\nThis violates econometric assumptions resulting in likely misestimation of\ntotal food consumption. We illustrate rescaling all consumption after applying\na PE matrix using a total food expenditure elasticity (TFEe, the expenditure\nelasticity for all food combined given the policy induced change in the total\nprice of food). We use case studies of NZ$2 per 100g saturated fat (SAFA) tax,\nNZ$0.4 per 100g sugar tax, and a 20% fruit and vegetable (F&V) subsidy. Methods\nWe estimated changes in food purchasing using a NZ PE matrix applied\nconventionally, then with TFEe adjustment. Impacts were quantified for total\nfood expenditure and health adjusted life years (HALYs) for the total NZ\npopulation alive in 2011 over the rest of their lifetime using a multistate\nlifetable model. Results Two NZ studies gave TFEes of 0.68 and 0.83, with\ninternational estimates ranging from 0.46 to 0.90. Without TFEe adjustment,\ntotal food expenditure decreased with the tax policies and increased with the\nF&V subsidy, implausible directions of shift given economic theory. After TFEe\nadjustment, HALY gains reduced by a third to a half for the two taxes and\nreversed from an apparent health loss to a health gain for the F&V subsidy.\nWith TFEe adjustment, HALY gains (in 1000s) were 1,805 (95% uncertainty\ninterval 1,337 to 2,340) for the SAFA tax, 1,671 (1,220 to 2,269) for the sugar\ntax, and 953 (453 to 1,308) for the F&V subsidy. Conclusions If PE matrices are\napplied in settings beyond where they were derived, additional scaling is\nlikely required. We suggest that the TFEe is a useful scalar.\n"
    },
    {
        "paper_id": 1909.13366,
        "authors": "Aitor Muguruza",
        "title": "Not so Particular about Calibration: Smile Problem Resolved",
        "comments": "15 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel Monte Carlo based LSV calibration algorithm that applies\nto all stochastic volatility models, including the non-Markovian rough\nvolatility family. Our framework overcomes the limitations of the particle\nmethod proposed by Guyon and Henry-Labord\\`ere (2012) and theoretically\nguarantees a variance reduction without additional computational complexity.\nSpecifically, we obtain a closed-form and exact calibration method that allows\nus to remove the dependency on both the kernel function and bandwidth\nparameter. This makes the algorithm more robust and less prone to errors or\ninstabilities in a production environment. We test the efficiency of our\nalgorithm on various hybrid (rough) local stochastic volatility models.\n"
    },
    {
        "paper_id": 1909.1361,
        "authors": "Anastasis Kratsios",
        "title": "Partial Uncertainty and Applications to Risk-Averse Valuation",
        "comments": "37 Pages, 1 Figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces an intermediary between conditional expectation and\nconditional sublinear expectation, called R-conditioning. The R-conditioning of\na random-vector in $L^2$ is defined as the best $L^2$-estimate, given a\n$\\sigma$-subalgebra and a degree of model uncertainty. When the random vector\nrepresents the payoff of derivative security in a complete financial market,\nits R-conditioning with respect to the risk-neutral measure is interpreted as\nits risk-averse value. The optimization problem defining the optimization\nR-conditioning is shown to be well-posed. We show that the R-conditioning\noperators can be used to approximate a large class of sublinear expectations to\narbitrary precision. We then introduce a novel numerical algorithm for\ncomputing the R-conditioning. This algorithm is shown to be strongly\nconvergent.\n  Implementations are used to compare the risk-averse value of a Vanilla option\nto its traditional risk-neutral value, within the Black-Scholes-Merton\nframework. Concrete connections to robust finance, sensitivity analysis, and\nhigh-dimensional estimation are all treated in this paper.\n"
    },
    {
        "paper_id": 1910.00258,
        "authors": "Jan Posp\\'i\\v{s}il and Vladim\\'ir \\v{S}v\\'igler",
        "title": "Isogeometric analysis in option pricing",
        "comments": null,
        "journal-ref": "Int. J. Comput. Math 96(11), 2177--2200, 2019",
        "doi": "10.1080/00207160.2018.1494826",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Isogeometric analysis is a recently developed computational approach that\nintegrates finite element analysis directly into design described by\nnon-uniform rational B-splines (NURBS). In this paper we show that price\nsurfaces that occur in option pricing can be easily described by NURBS\nsurfaces. For a class of stochastic volatility models, we develop a methodology\nfor solving corresponding pricing partial integro-differential equations\nnumerically by isogeometric analysis tools and show that a very small number of\nspace discretization steps can be used to obtain sufficiently accurate results.\nPresented solution by finite element method is especially useful for\npractitioners dealing with derivatives where closed-form solution is not\navailable.\n"
    },
    {
        "paper_id": 1910.00321,
        "authors": "Vasilios Mavroudis, Hayden Melton",
        "title": "Libra: Fair Order-Matching for Electronic Financial Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While historically, economists have been primarily occupied with analyzing\nthe behaviour of the markets, electronic trading gave rise to a new class of\nunprecedented problems associated with market fairness, transparency and\nmanipulation. These problems stem from technical shortcomings that are not\naccounted for in the simple conceptual models used for theoretical market\nanalysis. They, thus, call for more pragmatic market design methodologies that\nconsider the various infrastructure complexities and their potential impact on\nthe market procedures. First, we formally define temporal fairness and then\nexplain why it is very difficult for order-matching policies to ensure it in\ncontinuous markets. Subsequently, we introduce a list of system requirements\nand evaluate existing \"fair\" market designs in various practical and\nadversarial scenarios. We conclude that they fail to retain their properties in\nthe presence of infrastructure inefficiencies and sophisticated technical\nmanipulation attacks. Based on these findings, we then introduce Libra, a\n\"fair\" policy that is resilient to gaming and tolerant of technical\ncomplications. Our security analysis shows that it is significantly more robust\nthan existing designs, while Libra's deployment (in a live foreign currency\nexchange) validated both its considerably low impact on the operation of the\nmarket and its ability to reduce speed-based predatory trading.\n"
    },
    {
        "paper_id": 1910.0064,
        "authors": "Mikhail Tselishchev",
        "title": "On the Concavity of Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that Expected Shortfall (also called Average Value-at-Risk)\nis a convex risk measure, i. e. Expected Shortfall of a convex linear\ncombination of arbitrary risk positions is not greater than a convex linear\ncombination with the same weights of Expected Shortfalls of the same risk\npositions. In this short paper we prove that Expected Shortfall is a concave\nrisk measure with respect to probability distributions, i. e. Expected\nShortfall of a finite mixture of arbitrary risk positions is not lower than the\nlinear combination of Expected Shortfalls of the same risk positions (with the\nsame weights as in the mixture).\n"
    },
    {
        "paper_id": 1910.00778,
        "authors": "Jaroslav Borovicka and John Stachurski",
        "title": "Stability of Equilibrium Asset Pricing Models: A Necessary and\n  Sufficient Condition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain an exact necessary and sufficient condition for the existence and\nuniqueness of equilibrium asset prices in infinite horizon, discrete-time,\narbitrage free environments. Through several applications we show how the\ncondition sharpens and improves on previous results. We connect the condition,\nand hence the problem of existence and uniqueness of asset prices, with the\nrecent literature on stochastic discount factor decompositions. Finally, we\ndiscuss computation of the test value associated with our condition, providing\na Monte Carlo method that is naturally parallelizable.\n"
    },
    {
        "paper_id": 1910.01034,
        "authors": "Karina Arias-Calluari, Morteza. N. Najafi, Michael S. Harr\\'e and\n  Fernando Alonso-Marroquin",
        "title": "Stationarity of the detrended price return in stock markets",
        "comments": "Submitted to \"Physica A\". This manuscript contents 12 pages and 11\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a governing equation for stock market indexes that\naccounts for non-stationary effects. This is a linear Fokker-Planck equation\n(FPE) that describes the time evolution of the probability distribution\nfunction (PDF) of the price return. By applying Ito's lemma, this FPE is\nassociated with a stochastic differential equation (SDE) that models the time\nevolution of the price return in a fashion different from the classical\nBlack-Scholes equation. Both FPE and SDE equations account for a deterministic\npart or trend, and a stationary, stochastic part as a q-Gaussian noise. The\nmodel is validated using the S\\&P500 index's data. After removing the trend\nfrom the index, we show that the detrended part is stationary by evaluating the\nHurst exponent of the multifractal time series, its power spectrum, and its\nautocorrelation.\n"
    },
    {
        "paper_id": 1910.01044,
        "authors": "Marco Piccirilli, Maren Diane Schmeck, Tiziano Vargiolu",
        "title": "Capturing the power options smile by an additive two-factor model for\n  overlapping futures prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce an additive two-factor model for electricity\nfutures prices based on Normal Inverse Gaussian L\\'evy processes, that fulfills\na no-overlapping-arbitrage (NOA) condition. We compute European option prices\nby Fourier transform methods, introduce a specific calibration procedure that\ntakes into account no-arbitrage constraints and fit the model to power option\nsettlement prices of the European Energy Exchange (EEX). We show that our model\nis able to reproduce the different levels and shapes of the implied volatility\n(IV) profiles displayed by options with a variety of delivery periods.\n"
    },
    {
        "paper_id": 1910.0133,
        "authors": "Xiao Fan Liu, Zeng-Xian Lin, Xiao-Pu Han",
        "title": "Homogeneity and heterogeneity of cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thousands of cryptocurrencies have been issued and publicly exchanged since\nBitcoin was invented in 2008. The total cryptocurrency market value exceeds 300\nbillion US dollars as of 2019. This paper analyzes the prices, volumes,\nblockchain transactions, coin difficulties and public opinion popularities of\n3607 actively exchanged cryptocurrencies. We aim to reveal and explain the\nhomogeneity, i.e., the strong correlation of market performance, and the\nheterogeneity, i.e., the imbalance of popularities and sophistications, of the\ncryptocurrencies.\n"
    },
    {
        "paper_id": 1910.01407,
        "authors": "Danilo Vassallo, Giacomo Bormetti and Fabrizio Lillo",
        "title": "A tale of two sentiment scales: Disentangling short-run and long-run\n  components in multivariate sentiment dynamics",
        "comments": "37 pages, 8 figures. The authors thank Thomson Reuters for kindly\n  providing Thomson Reuters MarketPsych Indices time series. We benefited from\n  discussion with Giuseppe Buccheri, Fulvio Corsi, Luca Trapin, as well as with\n  conference participants to the Quantitative Finance Workshop 2019 at ETH in\n  Zurich and the AMASES XLIII Conference in Perugia",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach to sentiment data filtering for a portfolio of\nassets. In our framework, a dynamic factor model drives the evolution of the\nobserved sentiment and allows to identify two distinct components: a long-term\ncomponent, modeled as a random walk, and a short-term component driven by a\nstationary VAR(1) process. Our model encompasses alternative approaches\navailable in literature and can be readily estimated by means of Kalman\nfiltering and expectation maximization. This feature makes it convenient when\nthe cross-sectional dimension of the portfolio increases. By applying the model\nto a portfolio of Dow Jones stocks, we find that the long term component\nco-integrates with the market principal factor, while the short term one\ncaptures transient swings of the market associated with the idiosyncratic\ncomponents and captures the correlation structure of returns. Using quantile\nregressions, we assess the significance of the contemporaneous and lagged\nexplanatory power of sentiment on returns finding strong statistical evidence\nwhen extreme returns, especially negative ones, are considered. Finally, the\nlagged relation is exploited in a portfolio allocation exercise.\n"
    },
    {
        "paper_id": 1910.01438,
        "authors": "S\\\"uhan Altay, Katia Colaneri, Zehra Eksi",
        "title": "Optimal Convergence Trading with Unobservable Pricing Errors",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a dynamic portfolio optimization problem related to convergence\ntrading, which is an investment strategy that exploits temporary mispricing by\nsimultaneously buying relatively underpriced assets and selling short\nrelatively overpriced ones with the expectation that their prices converge in\nthe future. We build on the model of Liu and Timmermann (2013) and extend it by\nincorporating unobservable Markov-modulated pricing errors into the price\ndynamics of two co-integrated assets. We characterize the optimal portfolio\nstrategies in full and partial information settings both under the assumption\nof unrestricted and beta-neutral strategies. By using the innovations approach,\nwe provide the filtering equation that is essential for solving the\noptimization problem under partial information. Finally, in order to illustrate\nthe model capabilities, we provide an example with a two-state Markov chain.\n"
    },
    {
        "paper_id": 1910.0149,
        "authors": "Yang Qu, Ming-Xi Wang",
        "title": "The option pricing model based on time values: an application of the\n  universal approximation theory on unbounded domains",
        "comments": "To appear in IJCNN 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a time value related decision function to treat a classical option\npricing problem raised by Hutchinson-Lo-Poggio. In numerical experiments, the\nnew decision function significantly improves the original model of\nHutchinson-Lo-Poggio with faster convergence and better generalization\nperformance. By proving a novel universal approximation theorem, we show that\nour decision function rather than Hutchinson-Lo-Poggio's can be approximated on\nthe entire domain of definition by neural networks. Thus the experimental\nresults are partially explained by the representation properties of networks.\n"
    },
    {
        "paper_id": 1910.01491,
        "authors": "Kei Nakagawa, Masaya Abe, Junpei Komiyama",
        "title": "A Robust Transferable Deep Learning Framework for Cross-sectional\n  Investment Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/DSAA49011.2020.00051",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock return predictability is an important research theme as it reflects our\neconomic and social organization, and significant efforts are made to explain\nthe dynamism therein. Statistics of strong explanative power, called \"factor\"\nhave been proposed to summarize the essence of predictive stock returns.\nAlthough machine learning methods are increasingly popular in stock return\nprediction, an inference of the stock returns is highly elusive, and still most\ninvestors, if partly, rely on their intuition to build a better decision\nmaking. The challenge here is to make an investment strategy that is consistent\nover a reasonably long period, with the minimum human decision on the entire\nprocess. To this end, we propose a new stock return prediction framework that\nwe call Ranked Information Coefficient Neural Network (RIC-NN). RIC-NN is a\ndeep learning approach and includes the following three novel ideas: (1)\nnonlinear multi-factor approach, (2) stopping criteria with ranked information\ncoefficient (rank IC), and (3) deep transfer learning among multiple regions.\nExperimental comparison with the stocks in the Morgan Stanley Capital\nInternational (MSCI) indices shows that RIC-NN outperforms not only\noff-the-shelf machine learning methods but also the average return of major\nequity investment funds in the last fourteen years.\n"
    },
    {
        "paper_id": 1910.01778,
        "authors": "Ibrahim Ekren and Sergey Nadtochiy",
        "title": "Utility-based pricing and hedging of contingent claims in Almgren-Chriss\n  model with temporary price impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we construct the utility-based optimal hedging strategy for a\nEuropean-type option in the Almgren-Chriss model with temporary price impact.\nThe main mathematical challenge of this work stems from the degeneracy of the\nsecond order terms and the quadratic growth of the first order terms in the\nassociated HJB equation, which makes it difficult to establish sufficient\nregularity of the value function needed to construct the optimal strategy in a\nfeedback form. By combining the analytic and probabilistic tools for describing\nthe value function and the optimal strategy, we establish the feedback\nrepresentation of the latter. We use this representation to derive an explicit\nasymptotic expansion of the utility indifference price of the option, which\nallows us to quantify the price impact in options' market via the price impact\ncoefficient in the underlying market.\n"
    },
    {
        "paper_id": 1910.01781,
        "authors": "Derek Singh, Shuzhong Zhang",
        "title": "Distributionally Robust XVA via Wasserstein Distance: Wrong Way\n  Counterparty Credit and Funding Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates calculations of robust XVA, in particular, credit\nvaluation adjustment (CVA) and funding valuation adjustment (FVA) for\nover-the-counter derivatives under distributional uncertainty using Wasserstein\ndistance as the ambiguity measure. Wrong way counterparty credit risk and\nfunding risk can be characterized (and indeed quantified) via the robust XVA\nformulations. The simpler dual formulations are derived using recent infinite\ndimensional Lagrangian duality results. Next, some computational experiments\nare conducted to measure the additional XVA charges due to distributional\nuncertainty under a variety of portfolio and market configurations. Finally\nsome suggestions for future work are discussed.\n"
    },
    {
        "paper_id": 1910.01928,
        "authors": "Josep Perell\\'o, Miquel Montero, Jaume Masoliver, J. Doyne Farmer,\n  John Geanakoplos",
        "title": "Statistical analysis and stochastic interest rate modelling for valuing\n  the future with implications in climate change mitigation",
        "comments": "29 pages, 5 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:1311.4068",
        "journal-ref": "Stat. Mech. (2020) 043210",
        "doi": "10.1088/1742-5468/ab7a1e",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High future discounting rates favor inaction on present expending while lower\nrates advise for a more immediate political action. A possible approach to this\nkey issue in global economy is to take historical time series for nominal\ninterest rates and inflation, and to construct then real interest rates and\nfinally obtaining the resulting discount rate according to a specific\nstochastic model. Extended periods of negative real interest rates, in which\ninflation dominates over nominal rates, are commonly observed, occurring in\nmany epochs and in all countries. This feature leads us to choose a well-known\nmodel in statistical physics, the Ornstein-Uhlenbeck model, as a basic\ndynamical tool in which real interest rates randomly fluctuate and can become\nnegative, even if they tend to revert to a positive mean value. By covering 14\ncountries over hundreds of years we suggest different scenarios and include an\nerror analysis in order to consider the impact of statistical uncertainty in\nour results. We find that only 4 of the countries have positive long-run\ndiscount rates while the other ten countries have negative rates. Even if one\nrejects the countries where hyperinflation has occurred, our results support\nthe need to consider low discounting rates. The results provided by these\nfourteen countries significantly increase the priority of confronting global\nactions such as climate change mitigation. We finally extend the analysis by\nfirst allowing for fluctuations of the mean level in the Ornstein-Uhlenbeck\nmodel and secondly by considering modified versions of the Feller and lognormal\nmodels. In both cases, results remain basically unchanged thus demonstrating\nthe robustness of the results presented.\n"
    },
    {
        "paper_id": 1910.02137,
        "authors": "Alexander T. I. Adamou, Yonatan Berman, Diomides P. Mavroyiannis and\n  Ole B. Peters",
        "title": "Microfoundations of Discounting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important question in economics is how people choose between different\npayments in the future. The classical normative model predicts that a decision\nmaker discounts a later payment relative to an earlier one by an exponential\nfunction of the time between them. Descriptive models use non-exponential\nfunctions to fit observed behavioral phenomena, such as preference reversal.\nHere we propose a model of discounting, consistent with standard axioms of\nchoice, in which decision makers maximize the growth rate of their wealth. Four\nspecifications of the model produce four forms of discounting -- no\ndiscounting, exponential, hyperbolic, and a hybrid of exponential and\nhyperbolic -- two of which predict preference reversal. Our model requires no\nassumption of behavioral bias or payment risk.\n"
    },
    {
        "paper_id": 1910.02144,
        "authors": "Ravi Kashyap",
        "title": "Concepts, Components and Collections of Trading Strategies and Market\n  Color",
        "comments": "The concepts in this present paper have circulated previously under\n  the title, \"Trading Strategies and Market Color: The Benefits of Friendship\n  with Quantitative Analysts and Financial Engineers\". This present article is\n  the first part of the two parts into which the original manuscript has been\n  split into. arXiv admin note: text overlap with arXiv:1603.06047",
        "journal-ref": null,
        "doi": "10.3905/jwm.2019.1.082",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper acts as a collection of various trading strategies and useful\npieces of market information that might help to implement such strategies. This\nlist is meant to be comprehensive (though by no means exhaustive) and hence we\nonly provide pointers and give further sources to explore each strategy\nfurther. To set the stage for this exploration, we consider the factors that\ndetermine good and bad trades, the notions of market efficiency, the real\nprospect amidst the seemingly high expectations of homogeneous expectations\nfrom human beings and the catch-22 connotations that arise while comprehending\nthe true meaning of rational investing. We can broadly classify trading ideas\nand client market color material into Delta-One and Derivative strategies since\nthis acts as a natural categorization that depends on the expertise of the\nvarious trading desks that will implement these strategies. For each strategy,\nwe will have a core idea and we will present different flavors of this central\ntheme to demonstrate that we can easily cater to the varying risk appetites,\nregional preferences, asset management styles, investment philosophies,\nliability constraints, investment horizons, notional trading size, trading\nfrequency and other preferences of different market participants.\n"
    },
    {
        "paper_id": 1910.02194,
        "authors": "Geoffrey Ramseyer, Ashish Goel, David Mazieres",
        "title": "Liquidity in Credit Networks with Constrained Agents",
        "comments": "To be published in TheWebConf 2020",
        "journal-ref": null,
        "doi": "10.1145/3366423.3380276",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to scale transaction rates for deployment across the global web,\nmany cryptocurrencies have deployed so-called \"Layer-2\" networks of private\npayment channels. An idealized payment network behaves like a Credit Network, a\nmodel for transactions across a network of bilateral trust relationships.\nCredit Networks capture many aspects of traditional currencies as well as new\nvirtual currencies and payment mechanisms. In the traditional credit network\nmodel, if an agent defaults, every other node that trusted it is vulnerable to\nloss. In a cryptocurrency context, trust is manufactured by capital deposits,\nand thus there arises a natural tradeoff between network liquidity (i.e. the\nfraction of transactions that succeed) and the cost of capital deposits.\n  In this paper, we introduce constraints that bound the total amount of loss\nthat the rest of the network can suffer if an agent (or a set of agents) were\nto default - equivalently, how the network changes if agents can support\nlimited solvency guarantees.\n  We show that these constraints preserve the analytical structure of a credit\nnetwork. Furthermore, we show that aggregate borrowing constraints greatly\nsimplify the network structure and in the payment network context achieve the\noptimal tradeoff between liquidity and amount of escrowed capital.\n"
    },
    {
        "paper_id": 1910.0231,
        "authors": "Marco Avellaneda",
        "title": "Hierarchical PCA and Applications to Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is widely known that the common risk-factors derived from PCA beyond the\nfirst eigenportfolio are generally difficult to interpret and thus to use in\npractical portfolio management. We explore a alternative approach (HPCA) which\nmakes strong use of the partition of the market into sectors. We show that this\napproach leads to no loss of information with respect to PCA in the case of\nequities (constituents of the S&P 500) and also that the associated common\nfactors admit simple interpretations. The model can also be used in markets in\nwhich the sectors have asynchronous price information, such as single-name\ncredit default swaps, generalizing the works of Cont and Kan (2011) and Ivanov\n(2016).\n"
    },
    {
        "paper_id": 1910.02466,
        "authors": "Xiao Chen, Jin Hyuk Choi, Kasper Larsen, Duane J. Seppi",
        "title": "Resolving asset pricing puzzles using price-impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve in closed-form an equilibrium model in which a finite number of\nexponential investors continuously consume and trade with price-impact.\nCompared to the analogous Pareto-efficient equilibrium model, price-impact has\nan amplification effect on risk-sharing distortions that helps resolve the\ninterest rate puzzle and the stock-price volatility puzzle and, to a lesser\nextent, affects the equity premium puzzle.\n"
    },
    {
        "paper_id": 1910.02546,
        "authors": "Du Nguyen",
        "title": "A theorem of Kalman and minimal state-space realization of Vector\n  Autoregressive Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a concept of $autoregressive$ (AR)state-space realization that\ncould be applied to all transfer functions $\\boldsymbol{T}(L)$ with\n$\\boldsymbol{T}(0)$ invertible. We show that a theorem of Kalman implies each\nVector Autoregressive model (with exogenous variables) has a minimal\n$AR$-state-space realization of form $\\boldsymbol{y}_t =\n\\sum_{i=1}^p\\boldsymbol{H}\\boldsymbol{F}^{i-1}\\boldsymbol{G}\\boldsymbol{x}_{t-i}+\\boldsymbol{\\epsilon}_t$\nwhere $\\boldsymbol{F}$ is a nilpotent Jordan matrix and $\\boldsymbol{H},\n\\boldsymbol{G}$ satisfy certain rank conditions. The case $VARX(1)$ corresponds\nto reduced-rank regression. Similar to that case, for a fixed Jordan form\n$\\boldsymbol{F}$, $\\boldsymbol{H}$ could be estimated by least square as a\nfunction of $\\boldsymbol{G}$. The likelihood function is a determinant ratio\ngeneralizing the Rayleigh quotient. It is unchanged if $\\boldsymbol{G}$ is\nreplaced by $\\boldsymbol{S}\\boldsymbol{G}$ for an invertible matrix\n$\\boldsymbol{S}$ commuting with $\\boldsymbol{F}$. Using this invariant\nproperty, the search space for maximum likelihood estimate could be constrained\nto equivalent classes of matrices satisfying a number of orthogonal relations,\nextending the results in reduced-rank analysis. Our results could be considered\na multi-lag canonical-correlation-analysis. The method considered here provides\na solution in the general case to the polynomial product regression model of\nVelu et. al. We provide estimation examples. We also explore how the estimates\nvary with different Jordan matrix configurations and discuss methods to select\na configuration. Our approach could provide an important dimensional reduction\ntechnique with potential applications in time series analysis and linear system\nidentification. In the appendix, we link the reduced configuration space of\n$\\boldsymbol{G}$ with a geometric object called a vector bundle.\n"
    },
    {
        "paper_id": 1910.0257,
        "authors": "Jessica LaVoice and Domonkos F. Vamossy",
        "title": "Racial Disparities in Debt Collection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows that black and Hispanic borrowers are 39% more likely to\nexperience a debt collection judgment than white borrowers, even after\ncontrolling for credit scores and other relevant credit attributes. The racial\ngap in judgments is more pronounced in areas with a high density of payday\nlenders, a high share of income-less households, and low levels of tertiary\neducation. State-level measures of racial discrimination cannot explain the\njudgment gap, nor can neighborhood-level differences in the previous share of\ncontested judgments or cases with attorney representation. A\nback-of-the-envelope calculation suggests that closing the racial wealth gap\ncould significantly reduce the racial disparity in debt collection judgments.\n"
    },
    {
        "paper_id": 1910.03056,
        "authors": "Francesco Cordoni, Luca Di Persio and Yilun Jiang",
        "title": "A bank salvage model by impulse stochastic controls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper is devoted to the study of a bank salvage model with finite\ntime horizon and subjected to stochastic impulse controls. In our model, the\nbank's default time is a completely inaccessible random quantity generating its\nown filtration, then reflecting the unpredictability of the event itself. In\nthis framework the main goal is to minimize the total cost of the central\ncontroller who can inject capital to save the bank from default. We address the\nlatter task showing that the corresponding quasi-variational inequality (QVI)\nadmits a unique viscosity solution, Lipschitz continuous in space and Holder\ncontinuous in time. Furthermore, under mild assumptions on the dynamics the\nsmooth-fit $W^{(1,2),p}_{loc}$ property is achieved for any $1<p<+\\infty$.\n"
    },
    {
        "paper_id": 1910.03068,
        "authors": "Mariana Khapko, Marius Zoican",
        "title": "Do speed bumps curb low-latency trading? Evidence from a laboratory\n  market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exchanges implement intentional trade delays to limit the harmful impact of\nlow-latency trading. Do such \"speed bumps\" curb investment in fast trading\ntechnology? Data is scarce since trading technologies are proprietary. We build\nan experimental trading platform where participants face speed bumps and can\ninvest in fast trading technology. We find that asymmetric speed bumps, on\naverage, reduce investment in speed by only 20%. Increasing the magnitude of\nthe speed bump by one standard deviation further reduces low-latency investment\nby 8.33%. Finally, introducing a symmetric speed bump leads to the same\ninvestment level as no speed bump at all.\n"
    },
    {
        "paper_id": 1910.03141,
        "authors": "Ahmet Ak, Oner Gumus",
        "title": "The Possible Effects of Personal Income Tax and Value Added Tax on\n  Consumer Behaviors",
        "comments": "10 pages, 23 references",
        "journal-ref": "International Journal of Tax Economics and Management, 1(1), 49-58\n  (2018)",
        "doi": "10.35935/tax/11.5849",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In economics literature, it is accepted that all people are rational and they\ntry to maximize their utilities as possible as they can. In addition, economic\ntheories are formed with the assumptions not suitable to real life. For\ninstance, indifference curves are drawn with the assumptions that there are two\ngoods, people are rational, more is preferred to less and so on. Hence, the\nconsumer behaviors are guessed according to this analysis. Nevertheless, these\nare invalid in real life. And this inconsistencey are examined by behavioral\neconomics and neuroeconomics. Behavioral economics claims that people can\nbehave what they are not expected since people can be irrational, their\nwillpower is limited and altruistic behaviors can be seen and they can give\nmore value to what they own. As a result of these, consumer behaviors become\nmore different than that of economic theory. In addition to behavioral\neconomics, neuroeconomics also examines consumer behaviors more differently\nthan mainstream economic theory. It emphasizes the people using prefrontial\ncortex of the brain are more rational than the people using hippocampus of the\nbrain. Therefore, people can make illogical choices compared to economic\ntheory. In these cases, levying taxes such as personal income tax or value\nadded tax can be ineffective or effective. In other words, the effect becomes\nambigious. Hence,the hypothesis that if government desires to levy personal\nincome tax or value added tax, it makes a detailed research in terms of\nproductivity of taxes forms the fundamental of this study.\n"
    },
    {
        "paper_id": 1910.03204,
        "authors": "Masayuki Sawada",
        "title": "Noncompliance in randomized control trials without exclusion\n  restrictions",
        "comments": "The first version of this paper has been circulated on the author's\n  website, https://masayukisawada.com, as the Job Market paper and the\n  dissertation chapter",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes a method to identify treatment effects without exclusion\nrestrictions in randomized experiments with noncompliance. Exploiting a\nbaseline survey commonly available in randomized experiments, I decompose the\nintention-to-treat effects conditional on the endogenous treatment status. I\nthen identify these parameters to understand the effects of the assignment and\ntreatment. The key assumption is that a baseline variable maintains rank orders\nsimilar to the control outcome. I also reveal that the change-in-changes\nstrategy may work without repeated outcomes. Finally, I propose a new estimator\nthat flexibly incorporates covariates and demonstrate its properties using two\nexperimental studies.\n"
    },
    {
        "paper_id": 1910.03245,
        "authors": "Ozan Akdogan",
        "title": "Vol-of-vol expansion for (rough) stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an asymptotic small noise expansion, a so called vol-of-vol\nexpansion, for potentially infinite dimensional and rough stochastic volatility\nmodels. Thereby we extend the scope of existing results for finite dimensional\nmodels and validate claims for infinite dimensional models. Furthermore we\nprovide new, explicit (in the sense of non-recursive) representations of the\nso-called push-down Malliavin weights that utilizes a precise understanding of\nthe terms of this expansion.\n"
    },
    {
        "paper_id": 1910.03383,
        "authors": "Davide La Torre, Tufail Malik, Simone Marsiglio",
        "title": "Optimal Control of Prevention and Treatment in a Basic\n  Macroeconomic-Epidemiological Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the optimal control of disease prevention and treatment in a basic\nSIS model. We develop a simple macroeconomic setup in which the social planner\ndetermines how to optimally intervene, through income taxation, in order to\nminimize the social cost, inclusive of infection and economic costs, of the\nspread of an epidemic disease. The disease lowers economic production and thus\nincome by reducing the size of the labor force employed in productive\nactivities, tightening thus the economy's overall resources constraint. We\nconsider a framework in which the planner uses the collected tax revenue to\nintervene in either prevention (aimed at reducing the rate of infection) or\ntreatment (aimed at increasing the speed of recovery). Both optimal prevention\nand treatment policies allow the economy to achieve a disease-free equilibrium\nin the long run but their associated costs are substantially different along\nthe transitional dynamic path. By quantifying the social costs associated with\nprevention and treatment we determine which policy is most cost-effective under\ndifferent circumstances, showing that prevention (treatment) is desirable\nwhenever the infectivity rate is low (high).\n"
    },
    {
        "paper_id": 1910.03421,
        "authors": "Saeed Assani, Jianlin Jiang, Ahmad Assani, Feng Yang",
        "title": "Estimating and decomposing most productive scale size in parallel DEA\n  networks with shared inputs: A case of China's Five-Year Plans",
        "comments": "26 pages, Data envelopment analysis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Attaining the optimal scale size of production systems is an issue frequently\nfound in the priority questions on management agendas of various types of\norganizations. Determining the most productive scale size (MPSS) allows the\ndecision makers not only to know the best scale size that their systems can\nachieve but also to tell the decision makers how to move the inefficient\nsystems onto the MPSS region. This paper investigates the MPSS concept for\nproduction systems consisting of multiple subsystems connected in parallel.\nFirst, we propose a relational model where the MPSS of the whole system and the\ninternal subsystems are measured in a single DEA implementation. Then, it is\nproved that the MPSS of the system can be decomposed as the weighted sum of the\nMPSS of the individual subsystems. The main result is that the system is\noverall MPSS if and only if it is MPSS in each subsystem. MPSS decomposition\nallows the decision makers to target the non-MPSS subsystems so that the\nnecessary improvements can be readily suggested. An application of China's\nFive-Year Plans (FYPs) with shared inputs is used to show the applicability of\nthe proposed model for estimating and decomposing MPSS in parallel network DEA.\nIndustry and Agriculture sectors are selected as two parallel subsystems in the\nFYPs. Interesting findings have been noticed. Using the same amount of\nresources, the Industry sector had a better economic scale than the Agriculture\nsector. Furthermore, the last two FYPs, 11th and 12th, were the perfect two\nFYPs among the others.\n"
    },
    {
        "paper_id": 1910.03669,
        "authors": "Michael D. Perlman",
        "title": "On the feasibility of parsimonious variable selection for Hotelling's\n  $T^2$-test",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hotelling's $T^2$-test for the mean of a multivariate normal distribution is\none of the triumphs of classical multivariate analysis. It is uniformly most\npowerful among invariant tests, and admissible, proper Bayes, and locally and\nasymptotically minimax among all tests. Nonetheless, investigators often prefer\nnon-invariant tests, especially those obtained by selecting only a small subset\nof variables from which the $T^2$-statistic is to be calculated, because such\nreduced statistics are more easily interpretable for their specific\napplication. Thus it is relevant to ask the extent to which power is lost when\nvariable selection is limited to very small subsets of variables, e.g. of size\none (yielding univariate Student-$t^2$ tests) or size two (yielding bivariate\n$T^2$-tests). This study presents some evidence, admittedly fragmentary and\nincomplete, suggesting that in some cases no power may be lost over a wide\nrange of alternatives.\n"
    },
    {
        "paper_id": 1910.03712,
        "authors": "Hector Galindo-Silva",
        "title": "Political Openness and Armed Conflict: Evidence from Local Councils in\n  Colombia",
        "comments": null,
        "journal-ref": "Eur. J. Polit. Econ. 67 (2021) 101984",
        "doi": "10.1016/j.ejpoleco.2020.101984",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, I empirically investigate how the openness of political\ninstitutions to diverse representation can impact conflict-related violence. By\nexploiting plausibly exogenous variations in the number of councillors in\nColombian municipalities, I develop two sets of results. First, regression\ndiscontinuity estimates show that larger municipal councils have a considerably\ngreater number of political parties with at least one elected representative. I\ninterpret this result as evidence that larger municipal councils are more open\nto diverse political participation. The estimates also reveal that\nnon-traditional parties are the main beneficiaries of this greater political\nopenness. Second, regression discontinuity estimates show that political\nopenness substantially decreases conflict-related violence, namely the killing\nof civilian non-combatants. By exploiting plausibly exogenous variations in\nlocal election results, I show that the lower level of political violence stems\nfrom greater participation by parties with close links to armed groups. Using\ndata about the types of violence employed by these groups, and representation\nat higher levels of government, I argue that armed violence has decreased not\nbecause of power-sharing arrangements involving armed groups linked to the\nparties with more political representation, but rather because armed groups\nwith less political power and visibility are deterred from initiating certain\ntypes of violence.\n"
    },
    {
        "paper_id": 1910.03793,
        "authors": "Dr Ma Nang Laik, Chester Mark Hong Wei",
        "title": "Creating a unique mobile financial services framework for Myanmar: A\n  Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Myanmar is languishing at the bottom of key international indexes. United\nNations considers the country as a structurally weak and vulnerable economy.\nYet, from 2011 when Myanmar ended decades of military rule and isolationism and\ntransited towards democracy, its breakneck development has led to many\nconsidering the country to be one of the final frontiers for growth in the Asia\nregion. One such industry that has benefitted from the opening of the country\nis telecommunications. The mobile penetration rate at 4.8% in 2011 has\nincreased significantly to 90% in 2016. Despite renewed optimism and\ndevelopment in the economy, one statistic remains disappointing. According to a\nreport by Asian Development Bank (ADB), only 23% of the adult population have\naccess to a bank account. This highlights a need to reach out and increase\naccess to financial resources to a population that is severely unbanked and\nunderbanked. This creates an interesting proposition of allowing both the\ntelecommunications and financial sector to form the mobile financial services\n(MFS) sector and meet the need of improving access to financial resources for\nthe population. This report explores the government role in supporting, growing\nand sustaining the MFS sector and conducts a comparative research into\nSingapore, Malaysia and Thailand to understand the steps taken by these\ngovernments to develop their own Financial Technology (FinTech), specifically\nMFS, industry. Finally, the report will present preliminary recommendations\nthat the Myanmar government could consider implementing to drive growth in its\nMFS sector.\n"
    },
    {
        "paper_id": 1910.038,
        "authors": "Lan Ju, Zhiyong Tu, Changyong Xue",
        "title": "Art Pricing with Computer Graphic Techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper makes the first attempt to introduce the tools from computer\ngraphics into the art pricing research. We argue that the creation of a\npainting calls for a combination of conceptual effort and painting effort from\nthe artist. However, as the important price determinants, both efforts are long\nmissing in the traditional hedonic model because they are hard to measure. This\npaper draws on the digital pictures of auctioned paintings from various\nrenowned artists, and applies the image recognition techniques to measure the\nvariances of lines and colors of these paintings. We then use them as the\nproxies for the artist's painting effort, and include them in the hedonic\nregression to test their significance. Our results show that the variances of\nlines and colors of a painting can significantly positively explain the sales\nprice in a general context. Our suggested measurements can better capture the\ncontent heterogeneity of paintings hence improving on the traditional art\npricing methodology. Our approach also provides a quantitative perspective for\nboth valuation and authentication of paintings.\n"
    },
    {
        "paper_id": 1910.03805,
        "authors": "Saeed Assani, Jianlin Jiang, Ahmad Assani, Feng Yang",
        "title": "Most productive scale size of China's regional R&D value chain: A mixed\n  structure network",
        "comments": "35 pages, Data envelopment analysis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper offers new mathematical models to measure the most productive\nscale size (MPSS) of production systems with mixed structure networks (mixed of\nseries and parallel). In the first property, we deal with a general multi-stage\nnetwork which can be transformed, using dummy processes, into a series of\nparallel networks. In the second property, we consider a direct network\ncombined with series and parallel structure. In this paper, we propose new\nmodels to measure the overall MPSS of the production systems and their internal\nprocesses. MPSS decomposition is discussed and examined. As a real-life\napplication, this study measures the efficiency and MPSS of research and\ndevelopment (R&D) activities of Chinese provinces within an R&D value chain\nnetwork. In the R&D value chain, profitability and marketability stages are\nconnected in series, where the profitability stage is composed of operation and\nR&D efforts connected in parallel. The MPSS network model provides not only the\nMPSS measurement but also values that indicate the appropriate degree of\nintermediate measures for the two stages. Improvement strategy is given for\neach region based on the gap between the current and the appropriate level of\nintermediate measures. Our findings show that the marketability efficiency\nvalues of Chinese R&D regions were low, and no regions are operated under the\nMPSS. As a result, most Chinese regions performed inefficiently regarding both\nprofitability and marketability. This finding provides initial evidence that\nthe generally lower profitability and marketability efficiency of Chinese\nregions is a severe problem that may be due to wasted resources on production\nand R&D.\n"
    },
    {
        "paper_id": 1910.03951,
        "authors": "Claus Baumgart, Johannes Krebs, Robert Lempertseder, Oliver Pfaffel",
        "title": "Quantifying Life Insurance Risk using Least-Squares Monte Carlo",
        "comments": null,
        "journal-ref": "Der Aktuar 02.2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a stochastic framework to quantify the biometric risk\nof an insurance portfolio in solvency regimes such as Solvency II or the Swiss\nSolvency Test (SST). The main difficulty in this context constitutes in the\nproper representation of long term risks in the profit-loss distribution over a\none year horizon. This will be resolved by using least-squares Monte Carlo\nmethods to quantify the impact of new experience on the annual re-valuation of\nthe portfolio. Therefore our stochastic model can be seen as an example for an\ninternal model, as allowed under Solvency II or the SST. Since our model does\nnot rely upon nested simulations it is computationally fast and easy to\nimplement.\n"
    },
    {
        "paper_id": 1910.03993,
        "authors": "Derek Singh and Shuzhong Zhang",
        "title": "Distributionally Robust XVA via Wasserstein Distance Part 2: Wrong Way\n  Funding Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates calculations of robust funding valuation adjustment\n(FVA) for over the counter (OTC) derivatives under distributional uncertainty\nusing Wasserstein distance as the ambiguity measure. Wrong way funding risk can\nbe characterized via the robust FVA formulation. The simpler dual formulation\nof the robust FVA optimization is derived. Next, some computational experiments\nare conducted to measure the additional FVA charge due to distributional\nuncertainty under a variety of portfolio and market configurations. Finally\nsome suggestions for future work, such as robust capital valuation adjustment\n(KVA) and margin valuation adjustment (MVA), are discussed.\n"
    },
    {
        "paper_id": 1910.04047,
        "authors": "Randall Martyr (1), John Moriarty (1) and Magnus Perninge (2) ((1)\n  Queen Mary University of London, (2) Linnaeus University)",
        "title": "Discrete-time risk-aware optimal switching with non-adapted costs",
        "comments": "29 pages including references. Keywords: infinite horizon, optimal\n  switching, risk measures, reflected backward stochastic difference equations,\n  hydropower planning. Fixed some typos; tidied up the presentation; added new\n  numerical example on hydropower planning. To appear in Journal of/Advances in\n  Applied Probability",
        "journal-ref": null,
        "doi": "10.1017/apr.2021.44",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve non-Markovian optimal switching problems in discrete time on an\ninfinite horizon, when the decision maker is risk aware and the filtration is\ngeneral, and establish existence and uniqueness of solutions for the associated\nreflected backward stochastic difference equations. An example application to\nhydropower planning is provided.\n"
    },
    {
        "paper_id": 1910.04075,
        "authors": "Lisha Lin, Yaqiong Li, Rui Gao, Jianhong Wu",
        "title": "The Numerical Simulation of Quanto Option Prices Using Bayesian\n  Statistical Methods",
        "comments": "31 pages, 5 figures and 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, the pricing of Quanto options is studied, where the underlying\nforeign asset and the exchange rate are correlated with each other. Firstly, we\nadopt Bayesian methods to estimate unknown parameters entering the pricing\nformula of Quanto options, including the volatility of stock, the volatility of\nexchange rate and the correlation. Secondly, we compute and predict prices of\ndifferent four types of Quanto options based on Bayesian posterior prediction\ntechniques and Monte Carlo methods. Finally, we provide numerical simulations\nto demonstrate the advantage of Bayesian method used in this paper comparing\nwith some other existing methods. This paper is a new application of the\nBayesian methods in the pricing of multi-asset options.\n"
    },
    {
        "paper_id": 1910.04083,
        "authors": "Mattathias Lerner",
        "title": "The Impacts of the Alaska Permanent Fund Dividend on High School Status\n  Completion Rates",
        "comments": "Written as M.A. thesis for QMSS program at Columbia University",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Direct cash transfer programs have shown success as poverty interventions in\nboth the developing and developed world, yet little research exists examining\nthe society-wide outcomes of an unconditional cash transfer program disbursed\nwithout means-testing. This paper attempts to determine the impact of direct\ncash transfers on educational outcomes in a developed society by investigating\nthe impacts of the Alaska Permanent Fund Dividend, which was launched in 1982\nand continues to be disbursed on an annual basis to every Alaskan. A synthetic\ncontrol model is deployed to examine the path of educational attainment among\nAlaskans between 1977 and 1991 in order to determine if high school status\ncompletion rates after the launch of the dividend diverge from the synthetic in\na manner suggestive of a treatment effect.\n"
    },
    {
        "paper_id": 1910.04155,
        "authors": "Boyan Durankev",
        "title": "Taxation and Social Justice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The link between taxation and justice is a classic debate issue, while also\nbeing very relevant at a time of changing environmental factors and conditions\nof the social and economic system. Technologically speaking, there are three\ntypes of taxes: progressive, proportional and regressive. Although justice,\nlike freedom, is an element and manifestation of the imagined reality in\ncitizens minds, the state must comply with it. In particular, the tax system\nhas to adapt to the mass imagined reality in order for it to appear fairer and\nmore acceptable.\n"
    },
    {
        "paper_id": 1910.04487,
        "authors": "Samuel Shye and Ido Haber",
        "title": "Risk as Challenge: A Dual System Stochastic Model for Binary Choice\n  Behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Challenge Theory (CT), a new approach to decision under risk departs\nsignificantly from expected utility, and is based on firmly psychological,\nrather than economic, assumptions. The paper demonstrates that a purely\ncognitive-psychological paradigm for decision under risk can yield excellent\npredictions, comparable to those attained by more complex economic or\npsychological models that remain attached to conventional economic constructs\nand assumptions. The study presents a new model for predicting the popularity\nof choices made in binary risk problems. A CT-based regression model is tested\non data gathered from 126 respondents who indicated their preferences with\nrespect to 44 choice problems. Results support CT's central hypothesis,\nstrongly associating between the Challenge Index (CI) attributable to every\nbinary risk problem, and the observed popularity of the bold prospect in that\nproblem (with r=-0.92 and r=-0.93 for gains and for losses, respectively). The\nnovelty of the CT perspective as a new paradigm is illuminated by its simple,\nsingle-index (CI) representation of psychological effects proposed by Prospect\nTheory for describing choice behavior (certainty effect, reflection effect,\noverweighting small probabilities and loss aversion).\n"
    },
    {
        "paper_id": 1910.04879,
        "authors": "Vinci Chow",
        "title": "Predicting Auction Price of Vehicle License Plate with Deep Residual\n  Learning",
        "comments": null,
        "journal-ref": "Trends and Applications in Knowledge Discovery and Data Mining.\n  PAKDD 2019. Lecture Notes in Computer Science, vol 11607. Springer, Cham",
        "doi": "10.1007/978-3-030-26142-9_16",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to superstition, license plates with desirable combinations of characters\nare highly sought after in China, fetching prices that can reach into the\nmillions in government-held auctions. Despite the high stakes involved, there\nhas been essentially no attempt to provide price estimates for license plates.\nWe present an end-to-end neural network model that simultaneously predict the\nauction price, gives the distribution of prices and produces latent feature\nvectors. While both types of neural network architectures we consider\noutperform simpler machine learning methods, convolutional networks outperform\nrecurrent networks for comparable training time or model complexity. The\nresulting model powers our online price estimator and search engine.\n"
    },
    {
        "paper_id": 1910.04943,
        "authors": "Bahman Angoshtari, Tim Leung",
        "title": "Optimal Trading of a Basket of Futures Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of dynamically trading multiple futures contracts with\ndifferent underlying assets. To capture the joint dynamics of stochastic bases\nfor all traded futures, we propose a new model involving a multi-dimensional\nscaled Brownian bridge that is stopped before price convergence. This leads to\nthe analysis of the corresponding Hamilton-Jacobi-Bellman (HJB) equations,\nwhose solutions are derived in semi-explicit form. The resulting optimal\ntrading strategy is a long-short policy that accounts for whether the futures\nare in contango or backwardation. Our model also allows us to quantify and\ncompare the values of trading in the futures markets when the underlying assets\nare traded or not. Numerical examples are provided to illustrate the optimal\nstrategies and the effects of model parameters.\n"
    },
    {
        "paper_id": 1910.0496,
        "authors": "Guiyuan Ma, Song-Ping Zhu, Ivan Guo",
        "title": "Valuation of contingent claims with short selling bans under an\n  equal-risk pricing framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the valuation of European contingent claims with short\nselling bans under the equal risk pricing (ERP) framework proposed in Guo and\nZhu (2017) where analytical pricing formulae were derived in the case of\nmonotonic payoffs under risk-neutral measures. We establish a unified framework\nfor this new pricing approach so that its range of application can be\nsignificantly expanded. The results of Guo and Zhu (2017) are extended to the\ncase of non-monotonic payoffs (such as a butterfly spread option) under\nrisk-neutral measures. We also provide numerical schemes for computing\nequal-risk prices under other measures such as the original physical measure.\nFurthermore, we demonstrate how short selling bans can affect the valuation of\ncontingent claims by comparing equal-risk prices with Black-Scholes prices.\n"
    },
    {
        "paper_id": 1910.05056,
        "authors": "Emilio Said (FiQuant, MICS)",
        "title": "How Option Hedging Shapes Market Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a perturbation theory of the market impact based on an extension\nof the framework proposed by [Loeper, 2018] -- originally based on [Liu and\nYong, 2005] -- in which we consider only local linear market impact. We study\nthe execution process of hedging derivatives and show how these hedging\nmetaorders can explain some stylized facts observed in the empirical market\nimpact literature. As we are interested in the execution process of hedging we\nwill establish that the arbitrage opportunities that exist in the discrete time\nsetting vanish when the trading frequency goes to infinity letting us to derive\na pricing equation. Furthermore our approach retrieves several results already\nestablished in the option pricing literature such that the spot dynamics\nmodified by the market impact. We also study the relaxation of our hedging\nmetaorders based on the fair pricing hypothesis and establish a relation\nbetween the immediate impact and the permanent impact which is in agreement\nwith recent empirical studies on the subject.\n"
    },
    {
        "paper_id": 1910.05078,
        "authors": "Deli Chen, Yanyan Zou, Keiko Harimoto, Ruihan Bao, Xuancheng Ren, Xu\n  Sun",
        "title": "Incorporating Fine-grained Events in Stock Movement Prediction",
        "comments": "Accepted by 2th ECONLP workshop in EMNLP2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Considering event structure information has proven helpful in text-based\nstock movement prediction. However, existing works mainly adopt the\ncoarse-grained events, which loses the specific semantic information of diverse\nevent types. In this work, we propose to incorporate the fine-grained events in\nstock movement prediction. Firstly, we propose a professional finance event\ndictionary built by domain experts and use it to extract fine-grained events\nautomatically from finance news. Then we design a neural model to combine\nfinance news with fine-grained event structure and stock trade data to predict\nthe stock movement. Besides, in order to improve the generalizability of the\nproposed method, we design an advanced model that uses the extracted\nfine-grained events as the distant supervised label to train a multi-task\nframework of event extraction and stock prediction. The experimental results\nshow that our method outperforms all the baselines and has good\ngeneralizability.\n"
    },
    {
        "paper_id": 1910.05137,
        "authors": "J. Lussange, S. Bourgeois-Gironde, S. Palminteri, B. Gutkin",
        "title": "Stock price formation: useful insights from a multi-agent reinforcement\n  learning model",
        "comments": "arXiv admin note: text overlap with arXiv:1909.07748",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the past, financial stock markets have been studied with previous\ngenerations of multi-agent systems (MAS) that relied on zero-intelligence\nagents, and often the necessity to implement so-called noise traders to\nsub-optimally emulate price formation processes. However recent advances in the\nfields of neuroscience and machine learning have overall brought the\npossibility for new tools to the bottom-up statistical inference of complex\nsystems. Most importantly, such tools allows for studying new fields, such as\nagent learning, which in finance is central to information and stock price\nestimation. We present here the results of a new generation MAS stock market\nsimulator, where each agent autonomously learns to do price forecasting and\nstock trading via model-free reinforcement learning, and where the collective\nbehaviour of all agents decisions to trade feed a centralised double-auction\nlimit order book, emulating price and volume microstructures. We study here\nwhat such agents learn in detail, and how heterogenous are the policies they\ndevelop over time. We also show how the agents learning rates, and their\npropensity to be chartist or fundamentalist impacts the overall market\nstability and agent individual performance. We conclude with a study on the\nimpact of agent information via random trading.\n"
    },
    {
        "paper_id": 1910.05209,
        "authors": "Jos\\'e Cl\\'audio do Nascimento",
        "title": "Rational hyperbolic discounting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How much should you receive in a week to be indifferent to \\$ 100 in six\nmonths? Note that the indifference requires a rule to ensure the similarity\nbetween early and late payments. Assuming that rational individuals have low\naccuracy, then the following rule is valid: if the amounts to be paid are much\nless than the personal wealth, then the $q$-exponential discounting guarantees\nindifference in several periods. Thus, the discounting can be interpolated\nbetween hyperbolic and exponential functions due to the low accuracy to\ndistinguish time averages when the payments have low impact on personal wealth.\nTherefore, there are physical conditions that allow the hyperbolic discounting\nregardless psycho-behavioral assumption.\n"
    },
    {
        "paper_id": 1910.05219,
        "authors": "Jangho Yang and Torsten Heinrich and Julian Winkler and Fran\\c{c}ois\n  Lafond and Pantelis Koutroumpis and J. Doyne Farmer",
        "title": "Measuring productivity dispersion: a parametric approach using the\n  L\\'{e}vy alpha-stable distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that value added per worker is extremely heterogeneous among\nfirms, but relatively little has been done to characterize this heterogeneity\nmore precisely. Here we show that the distribution of value-added per worker\nexhibits heavy tails, a very large support, and consistently features a\nproportion of negative values, which prevents log transformation. We propose to\nmodel the distribution of value added per worker using the four parameter\nL\\'evy stable distribution, a natural candidate deriving from the Generalised\nCentral Limit Theorem, and we show that it is a better fit than key\nalternatives. Fitting a distribution allows us to capture dispersion through\nthe tail exponent and scale parameters separately. We show that these\nparametric measures of dispersion are at least as useful as interquantile\nratios, through case studies on the evolution of dispersion in recent years and\nthe correlation between dispersion and intangible capital intensity.\n"
    },
    {
        "paper_id": 1910.05536,
        "authors": "Xuanwu Yue, Jiaxin Bai, Qinhan Liu, Yiyang Tang, Abishek Puri, Ke Li,\n  Huamin Qu",
        "title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/TVCG.2019.2934660",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantitative Investment, built on the solid foundation of robust financial\ntheories, is at the center stage in investment industry today. The essence of\nquantitative investment is the multi-factor model, which explains the\nrelationship between the risk and return of equities. However, the multi-factor\nmodel generates enormous quantities of factor data, through which even\nexperienced portfolio managers find it difficult to navigate. This has led to\nportfolio analysis and factor research being limited by a lack of intuitive\nvisual analytics tools. Previous portfolio visualization systems have mainly\nfocused on the relationship between the portfolio return and stock holdings,\nwhich is insufficient for making actionable insights or understanding market\ntrends. In this paper, we present sPortfolio, which, to the best of our\nknowledge, is the first visualization that attempts to explore the factor\ninvestment area. In particular, sPortfolio provides a holistic overview of the\nfactor data and aims to facilitate the analysis at three different levels: a\nRisk-Factor level, for a general market situation analysis; a\nMultiple-Portfolio level, for understanding the portfolio strategies; and a\nSingle-Portfolio level, for investigating detailed operations. The system's\neffectiveness and usability are demonstrated through three case studies. The\nsystem has passed its pilot study and is soon to be deployed in industry.\n"
    },
    {
        "paper_id": 1910.05555,
        "authors": "Ann Sebastian and Tim Gebbie",
        "title": "Systematic Asset Allocation using Flexible Views for South African\n  Markets",
        "comments": "22 pages and 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We implement a systematic asset allocation model using the Historical\nSimulation with Flexible Probabilities (HS-FP) framework developed by Meucci.\nThe HS-FP framework is a flexible non-parametric estimation approach that\nconsiders future asset class behavior to be conditional on time and market\nenvironments, and derives a forward looking distribution that is consistent\nwith this view while remaining close as possible to the prior distribution. The\nframework derives the forward looking distribution by applying unequal time and\nstate conditioned probabilities to historical observations of asset class\nreturns. This is achieved using relative entropy to find estimates with the\nleast distortion to the prior distribution. Here, we use the HS-FP framework on\nSouth African financial market data for asset allocation purposes; by\nestimating expected returns, correlations and volatilities that are better\nrepresented through the measured market cycle. We demonstrated a range of state\nvariables that can be useful towards understanding market environments.\nConcretely, we compare the out-of-sample performance for a specific\nconfiguration of the HS-FP model relative to classic Mean Variance\nOptimization(MVO) and Equally Weighted (EW) benchmark models. The framework\ndisplays low probability of backtest overfitting and the out-of-sample net\nreturns and Sharpe ratio point estimates of the HS-FP model outperforms the\nbenchmark models. However, the results are inconsistent when training windows\nare varied, the Sharpe ratio is seen to be inflated, and the method does not\ndemonstrate statistically significant out-performance on a gross and net basis.\n"
    },
    {
        "paper_id": 1910.05561,
        "authors": "Bruno Scalzo Dees, Ljubisa Stankovic, Anthony G. Constantinides,\n  Danilo P. Mandic",
        "title": "Portfolio Cuts: A Graph-Theoretic Framework to Diversification",
        "comments": "5 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investment returns naturally reside on irregular domains, however, standard\nmultivariate portfolio optimization methods are agnostic to data structure. To\nthis end, we investigate ways for domain knowledge to be conveniently\nincorporated into the analysis, by means of graphs. Next, to relax the\nassumption of the completeness of graph topology and to equip the graph model\nwith practically relevant physical intuition, we introduce the portfolio cut\nparadigm. Such a graph-theoretic portfolio partitioning technique is shown to\nallow the investor to devise robust and tractable asset allocation schemes, by\nvirtue of a rigorous graph framework for considering smaller, computationally\nfeasible, and economically meaningful clusters of assets, based on graph cuts.\nIn turn, this makes it possible to fully utilize the asset returns covariance\nmatrix for constructing the portfolio, even without the requirement for its\ninversion. The advantages of the proposed framework over traditional methods\nare demonstrated through numerical simulations based on real-world price data.\n"
    },
    {
        "paper_id": 1910.05596,
        "authors": "Carolina Mattsson",
        "title": "Networks of monetary flow at native resolution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  People and companies move money with every financial transaction they make.\nWe aim to understand how such activity gives rise to large-scale patterns of\nmonetary flow. In this work, we trace the movement of e-money through the\naccounts of a mobile money system using the provider's own transaction records.\nThe resulting transaction sequences---balance-respecting trajectories---are\ndata objects that represent observed monetary flows. Common sequential motifs\ncorrespond to known use-cases of mobile money: digital payments, digital\ntransfers, and money storage. We find that each activity creates a distinct\nnetwork structure within the system, and we uncover coordinated gaming of the\nmobile money provider's commission schedule. Moreover, we find that e-money\npasses through the system in anywhere from minutes to months. This pronounced\nheterogeneity, even within the same use-case, can inform the modeling of\nturnover in money supply. Our methodology relates economic activity at the\ntransaction level to large-scale patterns of monetary flow, broadening the\nscope of empirical study about the network and temporal structure of the\neconomy.\n"
    },
    {
        "paper_id": 1910.05658,
        "authors": "Mohammad Rasoolinejad",
        "title": "Universal Basic Income: The Last Bullet in the Darkness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Universal Basic Income (UBI) has recently been gaining traction. Arguments\nexist on both sides in favor of and against it. Like any other financial tool,\nUBI can be useful if used with discretion. This paper seeks to clarify how UBI\naffects the economy, including how it can be beneficial. The key point is to\nregulate the rate of UBI based on the inflation rate. This should be done by an\nindependent institution from the executive branch of the government. If\nimplemented correctly, UBI can add a powerful tool to the Federal Reserve\ntoolkit. UBI can be used to reintroduce inflation to the countries which suffer\nlong-lasting deflationary environment. UBI has the potential to decrease the\nwealth disparity, decrease the national debt, increase productivity, and\nincrease comparative advantage of the economy. UBI also can substitute the\ncurrent welfare systems because of its transparency and efficiency. This\narticle focuses more on the United States, but similar ideas can be implemented\nin other developed nations.\n"
    }
]