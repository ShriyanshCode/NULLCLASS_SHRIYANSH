[
    {
        "paper_id": 1306.2719,
        "authors": "M. H. A. Davis, M. R. Pistorius",
        "title": "Explicit solution of an inverse first-passage time problem for L\\'{e}vy\n  processes and counterparty credit risk",
        "comments": "Published at http://dx.doi.org/10.1214/14-AAP1051 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 5, 2383-2415",
        "doi": "10.1214/14-AAP1051",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a given Markov process $X$ and survival function $\\overline{H}$ on\n$\\mathbb{R}^+$, the inverse first-passage time problem (IFPT) is to find a\nbarrier function $b:\\mathbb{R}^+\\to[-\\infty,+\\infty]$ such that the survival\nfunction of the first-passage time $\\tau_b=\\inf \\{t\\ge0:X(t)<b(t)\\}$ is given\nby $\\overline{H}$. In this paper, we consider a version of the IFPT problem\nwhere the barrier is fixed at zero and the problem is to find an initial\ndistribution $\\mu$ and a time-change $I$ such that for the time-changed process\n$X\\circ I$ the IFPT problem is solved by a constant barrier at the level zero.\nFor any L\\'{e}vy process $X$ satisfying an exponential moment condition, we\nderive the solution of this problem in terms of $\\lambda$-invariant\ndistributions of the process $X$ killed at the epoch of first entrance into the\nnegative half-axis. We provide an explicit characterization of such\ndistributions, which is a result of independent interest. For a given\nmulti-variate survival function $\\overline{H}$ of generalized frailty type, we\nconstruct subsequently an explicit solution to the corresponding IFPT with the\nbarrier level fixed at zero. We apply these results to the valuation of\nfinancial contracts that are subject to counterparty credit risk.\n"
    },
    {
        "paper_id": 1306.2728,
        "authors": "David Johnstone, Dennis Lindley",
        "title": "Mean-Variance and Expected Utility: The Borch Paradox",
        "comments": "Published in at http://dx.doi.org/10.1214/12-STS408 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)",
        "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 223-237",
        "doi": "10.1214/12-STS408",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The model of rational decision-making in most of economics and statistics is\nexpected utility theory (EU) axiomatised by von Neumann and Morgenstern, Savage\nand others. This is less the case, however, in financial economics and\nmathematical finance, where investment decisions are commonly based on the\nmethods of mean-variance (MV) introduced in the 1950s by Markowitz. Under the\nMV framework, each available investment opportunity (\"asset\") or portfolio is\nrepresented in just two dimensions by the ex ante mean and standard deviation\n$(\\mu,\\sigma)$ of the financial return anticipated from that investment.\nUtility adherents consider that in general MV methods are logically incoherent.\nMost famously, Norwegian insurance theorist Borch presented a proof suggesting\nthat two-dimensional MV indifference curves cannot represent the preferences of\na rational investor (he claimed that MV indifference curves \"do not exist\").\nThis is known as Borch's paradox and gave rise to an important but generally\nlittle-known philosophical literature relating MV to EU. We examine the main\nearly contributions to this literature, focussing on Borch's logic and the\narguments by which it has been set aside.\n"
    },
    {
        "paper_id": 1306.2751,
        "authors": "Paolo Guasoni, Johannes Muhle-Karbe, Hao Xing",
        "title": "Robust Portfolios and Weak Incentives in Long-Run Investments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When the planning horizon is long, and the safe asset grows indefinitely,\nisoelastic portfolios are nearly optimal for investors who are close to\nisoelastic for high wealth, and not too risk averse for low wealth. We prove\nthis result in a general arbitrage-free, frictionless, semimartingale model. As\na consequence, optimal portfolios are robust to the perturbations in\npreferences induced by common option compensation schemes, and such incentives\nare weaker when their horizon is longer. Robust option incentives are possible,\nbut require several, arbitrarily large exercise prices, and are not always\nconvex.\n"
    },
    {
        "paper_id": 1306.2793,
        "authors": "Christian Bayer, Peter Friz, Peter Laurence",
        "title": "On the probability density function of baskets",
        "comments": "Appeared in: Large Deviations and Asymptotic Methods in Finance,\n  Springer proceedings in Mathematics & Statistics, Editors: Friz, P.K.,\n  Gatheral, J., Gulisashvili, A., Jacquier, A., Teichmann, J., 2015, with minor\n  typos removed",
        "journal-ref": null,
        "doi": "10.1007/978-3-319-11605-1_16",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The state price density of a basket, even under uncorrelated Black-Scholes\ndynamics, does not allow for a closed from density. (This may be rephrased as\nstatement on the sum of lognormals and is especially annoying for such are used\nmost frequently in Financial and Actuarial Mathematics.) In this note we\ndiscuss short time and small volatility expansions, respectively. The method\nworks for general multi-factor models with correlations and leads to the\nanalysis of a system of ordinary (Hamiltonian) differential equations.\nSurprisingly perhaps, even in two asset Black-Scholes situation (with its flat\ngeometry), the expansion can degenerate at a critical (basket) strike level; a\nphenomena which seems to have gone unnoticed in the literature to date.\nExplicit computations relate this to a phase transition from a unique to more\nthan one \"most-likely\" paths (along which the diffusion, if suitably\nconditioned, concentrates in the afore-mentioned regimes). This also provides a\n(quantifiable) understanding of how precisely a presently out-of-money basket\noption may still end up in-the-money.\n"
    },
    {
        "paper_id": 1306.2802,
        "authors": "Albert Altarovici, Johannes Muhle-Karbe, H. Mete Soner",
        "title": "Asymptotics for Fixed Transaction Costs",
        "comments": "39 pages, 3 figures. Added: proof of Weak Dynamic Programming",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An investor with constant relative risk aversion trades a safe and several\nrisky assets with constant investment opportunities. For a small fixed\ntransaction cost, levied on each trade regardless of its size, we explicitly\ndetermine the leading-order corrections to the frictionless value function and\noptimal policy.\n"
    },
    {
        "paper_id": 1306.282,
        "authors": "Emmanuel Frenod (LMBA, INRIA Nancy - Grand Est / IECN / LSIIT / IRMA),\n  Jean-Philippe Gouigoux (LMBA), Landry Tour\\'e (LMBA)",
        "title": "Modeling and Solving Alternative Financial Solutions Seeking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we build a method to optimize Multi-Year Prospective Budgets.\nFirst we present a systemic model of Local Community Finances. Then, from two\nacceptable Multi-Year Prospective Budgets the method implements a Genetic\nAlgorithm to generate a collection of admissible Multi-Year Prospective Budgets\namong which Decision-Makers can choose. The method is tested on simplified\ncases and on in operational situation and gives satisfactory results.\n"
    },
    {
        "paper_id": 1306.2831,
        "authors": "Hao Meng (ECUST), Wen-Jie Xie (ECUST), Zhi-Qiang Jiang (ECUST), Boris\n  Podobnik (BU and ZSEM), Wei-Xing Zhou (ECUST), H. Eugene Stanley (BU)",
        "title": "Systemic risk and spatiotemporal dynamics of the US housing market",
        "comments": "8 RevTex pages including 4 eps figures",
        "journal-ref": "Scientific Reports 4, 3655 (2014)",
        "doi": "10.1038/srep03655",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Housing markets play a crucial role in economies and the collapse of a\nreal-estate bubble usually destabilizes the financial system and causes\neconomic recessions. We investigate the systemic risk and spatiotemporal\ndynamics of the US housing market (1975-2011) at the state level based on the\nRandom Matrix Theory (RMT). We identify rich economic information in the\nlargest eigenvalues deviating from RMT predictions and unveil that the\ncomponent signs of the eigenvectors contain either geographical information or\nthe extent of differences in house price growth rates or both. Our results show\nthat the US housing market experienced six different regimes, which is\nconsistent with the evolution of state clusters identified by the box\nclustering algorithm and the consensus clustering algorithm on the partial\ncorrelation matrices. Our analysis uncovers that dramatic increases in the\nsystemic risk are usually accompanied with regime shifts, which provides a\nmeans of early detection of housing bubbles.\n"
    },
    {
        "paper_id": 1306.2832,
        "authors": "Olivier Gu\\'eant, Guillaume Royer",
        "title": "VWAP execution and guaranteed VWAP",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal liquidation using VWAP strategies has been considered in the\nliterature, though never in the presence of permanent market impact and only\nrarely with execution costs. Moreover, only VWAP strategies have been studied\nand the pricing of guaranteed VWAP contracts has never been addressed. In this\narticle, we develop a model to price guaranteed VWAP contracts in a general\nframework for market impact and we highlight the differences between an agency\nVWAP and a guaranteed VWAP contract. Numerical methods and applications are\nalso provided.\n"
    },
    {
        "paper_id": 1306.2834,
        "authors": "Mauro Bernardi, Ghislaine Gayraud, Lea Petrella",
        "title": "Bayesian inference for CoVaR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent financial disasters emphasised the need to investigate the consequence\nassociated with the tail co-movements among institutions; episodes of contagion\nare frequently observed and increase the probability of large losses affecting\nmarket participants' risk capital. Commonly used risk management tools fail to\naccount for potential spillover effects among institutions because they provide\nindividual risk assessment. We contribute to analyse the interdependence\neffects of extreme events providing an estimation tool for evaluating the\nConditional Value-at-Risk (CoVaR) defined as the Value-at-Risk of an\ninstitution conditioned on another institution being under distress. In\nparticular, our approach relies on Bayesian quantile regression framework. We\npropose a Markov chain Monte Carlo algorithm exploiting the Asymmetric Laplace\ndistribution and its representation as a location-scale mixture of Normals.\nMoreover, since risk measures are usually evaluated on time series data and\nreturns typically change over time, we extend the CoVaR model to account for\nthe dynamics of the tail behaviour. Application on U.S. companies belonging to\ndifferent sectors of the Standard and Poor's Composite Index (S&P500) is\nconsidered to evaluate the marginal contribution to the overall systemic risk\nof each individual institution\n"
    },
    {
        "paper_id": 1306.311,
        "authors": "R\\'emy Chicheportiche and Jean-Philippe Bouchaud",
        "title": "Some applications of first-passage ideas to finance",
        "comments": "30 pages. To appear in the special volume \"First-Passage Phenomena\n  and Their Applications\", Eds. R. Metzler, G. Oshanin, S. Redner. World\n  Scientific (2013)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many problems in finance are related to first passage times. Among all of\nthem, we chose three on which we contributed personally. Our first example\nrelates Kolmogorov-Smirnov like goodness-of-fit tests, modified in such a way\nthat tail events and core events contribute equally to the test (in the\nstandard Kolmogorov-Smirnov, the tails contribute very little to the measure of\ngoodness-of-fit). We show that this problem can be mapped onto that of a random\nwalk inside moving walls. The second example is the optimal time to sell an\nasset (modelled as a random walk with drift) such that the sell time is as\nclose as possible to the time at which the asset reaches its maximum value. The\nlast example concerns optimal trading in the presence of transaction costs. In\nthis case, the optimal strategy is to wait until the predictor reaches (plus or\nminus) a threshold value before buying or selling. The value of this threshold\nis found by mapping the problem onto that of a random walk between two walls.\n"
    },
    {
        "paper_id": 1306.3359,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "Making Mean-Variance Hedging Implementable in a Partially Observable\n  Market",
        "comments": "comments added. supplementary contents to the version accepted by QF",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The mean-variance hedging (MVH) problem is studied in a partially observable\nmarket where the drift processes can only be inferred through the observation\nof asset or index processes. Although most of the literatures treat the MVH\nproblem by the duality method, here we study a system consisting of three BSDEs\nderived by Mania and Tevzadze (2003) and Mania et.al.(2008) and try to provide\nmore explicit expressions directly implementable by practitioners. Under the\nBayesian and Kalman-Bucy frameworks, we find that a relevant BSDE yields a\nsemi-closed solution via a simple set of ODEs which allow a quick numerical\nevaluation. This renders remaining problems equivalent to solving European\ncontingent claims under a new forward measure, and it is straightforward to\nobtain a forward looking non-sequential Monte Carlo simulation scheme. We also\ngive a special example where the hedging position is available in a semi-closed\nform. For more generic setups, we provide explicit expressions of approximate\nhedging portfolio by an asymptotic expansion. These analytic expressions not\nonly allow the hedgers to update the hedging positions in real time but also\nmake a direct analysis of the terminal distribution of the hedged portfolio\nfeasible by standard Monte Carlo simulation.\n"
    },
    {
        "paper_id": 1306.3395,
        "authors": "Joachim Kaldasch",
        "title": "Evolutionary Model of a Anonymous Consumer Durable Market",
        "comments": "preprint",
        "journal-ref": "Physica A 390(2011)2692 2715",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An analytic model is presented that considers the evolution of a market of\ndurable goods. The model suggests that after introduction goods spread always\naccording to a Bass diffusion. However, this phase will be followed by a\ndiffusion process for durable consumer goods governed by a\nvariation-selection-reproduction mechanism and the growth dynamics can be\ndescribed by a replicator equation. Describing the aggregate sales as the sum\nof first, multiple and replacement purchase the product life cycle can be\nderived. Replacement purchase causes periodic variations of the sales\ndetermined by the finite lifetime of the good (Juglar cycles). The model\nsuggests that both, Bass- and Gompertz diffusion may contribute to the product\nlife cycle of a consumer durable. The theory contains the standard equilibrium\nview of a market as a special case. It depends on the time scale, whether an\nequilibrium or evolutionary description is more appropriate. The evolutionary\nframework is used to derive also the size, growth rate and price distribution\nof manufacturing business units. It predicts that the size distribution of the\nbusiness units (products) is lognormal, while the growth rates exhibit a\nLaplace distribution. Large price deviations from the mean price are also\ngoverned by a Laplace distribution (fat tails). These results are in agreement\nwith empirical findings. The explicit comparison of the time evolution of\nconsumer durables with empirical investigations confirms the close relationship\nbetween price decline and Gompertz diffusion, while the product life cycle can\nbe described qualitatively for a long time period.\n"
    },
    {
        "paper_id": 1306.3422,
        "authors": "Sebastian M. Krause, Tiago P. Peixoto, Stefan Bornholdt",
        "title": "Spontaneous centralization of control in a network of company ownerships",
        "comments": "5 Pages, 7 figures",
        "journal-ref": "PLoS ONE 8(12): e80303 (2013)",
        "doi": "10.1371/journal.pone.0080303",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We introduce a model for the adaptive evolution of a network of company\nownerships. In a recent work it has been shown that the empirical global\nnetwork of corporate control is marked by a central, tightly connected \"core\"\nmade of a small number of large companies which control a significant part of\nthe global economy. Here we show how a simple, adaptive \"rich get richer\"\ndynamics can account for this characteristic, which incorporates the increased\nbuying power of more influential companies, and in turn results in even higher\ncontrol. We conclude that this kind of centralized structure can emerge without\nit being an explicit goal of these companies, or as a result of a\nwell-organized strategy.\n"
    },
    {
        "paper_id": 1306.3437,
        "authors": "Sanjay Mehrotra, David Papp",
        "title": "A cutting surface algorithm for semi-infinite convex programming with an\n  application to moment robust optimization",
        "comments": "Accepted in SIAM Journal on Optimization. 28 pages, 2 figures.\n  Keywords: semi-infinite programming, robust optimization, distributionally\n  robust optimization, stochastic programming, moment matching, column\n  generation, cutting surface methods, cutting plane methods, moment problem",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present and analyze a central cutting surface algorithm for general\nsemi-infinite convex optimization problems, and use it to develop a novel\nalgorithm for distributionally robust optimization problems in which the\nuncertainty set consists of probability distributions with given bounds on\ntheir moments. Moments of arbitrary order, as well as non-polynomial moments\ncan be included in the formulation. We show that this gives rise to a hierarchy\nof optimization problems with decreasing levels of risk-aversion, with classic\nrobust optimization at one end of the spectrum, and stochastic programming at\nthe other. Although our primary motivation is to solve distributionally robust\noptimization problems with moment uncertainty, the cutting surface method for\ngeneral semi-infinite convex programs is also of independent interest. The\nproposed method is applicable to problems with non-differentiable semi-infinite\nconstraints indexed by an infinite-dimensional index set. Examples comparing\nthe cutting surface algorithm to the central cutting plane algorithm of\nKortanek and No demonstrate the potential of our algorithm even in the solution\nof traditional semi-infinite convex programming problems whose constraints are\ndifferentiable and are indexed by an index set of low dimension. After the rate\nof convergence analysis of the cutting surface algorithm, we extend the\nauthors' moment matching scenario generation algorithm to a probabilistic\nalgorithm that finds optimal probability distributions subject to moment\nconstraints. The combination of this distribution optimization method and the\ncentral cutting surface algorithm yields a solution to a family of\ndistributionally robust optimization problems that are considerably more\ngeneral than the ones proposed to date.\n"
    },
    {
        "paper_id": 1306.3479,
        "authors": "Helena Jasiulewicz, Wojciech Kordecki",
        "title": "Ruin probability of a discrete-time risk process with proportional\n  reinsurance and investment for exponential and Pareto distributions",
        "comments": null,
        "journal-ref": "Operations Research and Decisions 25(3)(2015)",
        "doi": "10.5277/ord150302",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper a quantitative analysis of the ruin probability in finite time\nof discrete risk process with proportional reinsurance and investment of\nfinance surplus is focused on. It is assumed that the total loss on a unit\ninterval has a light-tailed distribution -- exponential distribution and a\nheavy-tailed distribution -- Pareto distribution. The ruin probability for\nfinite-horizon 5 and 10 was determined from recurrence equations. Moreover for\nexponential distribution the upper bound of ruin probability by Lundberg\nadjustment coefficient is given. For Pareto distribution the adjustment\ncoefficient does not exist, hence an asymptotic approximation of the ruin\nprobability if an initial capital tends to infinity is given. Obtained\nnumerical results are given as tables and they are illustrated as graphs.\n"
    },
    {
        "paper_id": 1306.3531,
        "authors": "Argyn Kuketayev",
        "title": "The convergence of regional house prices in the USA in the context of\n  the stress testing of financial institutions",
        "comments": "38 pages, 7 tables, 30 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  I studied the convergence of regional house prices to national prices in USA\nby analyzing time-series of house price indices of 9 Census Divisions. I found\nthe evidence of the convergence in some parts of the country using asymmetric\nunit root tests. The fact that the evidence of the convergence is not present\nin large parts of the country raises an issue of execution and interpretation\nof results of Federal Reserve Bank's annual stress testing of the US banking\nsystem.\n"
    },
    {
        "paper_id": 1306.3554,
        "authors": "Timothy J. Garrett",
        "title": "Thermodynamics of long-run economic innovation and growth",
        "comments": "31 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1002/2013EF000171",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article derives prognostic expressions for the evolution of globally\naggregated economic wealth, productivity, inflation, technological change,\ninnovation and growth. The approach is to treat civilization as an open,\nnon-equilibrium thermodynamic system that dissipates energy and diffuses matter\nin order to sustain existing circulations and to further its material growth.\nAppealing to a prior result that established a fixed relationship between a\nvery general representation of global economic wealth and rates of global\nprimary energy consumption, physically derived expressions for economic\nquantities follow. The analysis suggests that wealth can be expressed in terms\nof the length density of civilization's networks and the availability of energy\nresources. Rates of return on wealth are accelerated by energy reserve\ndiscovery, improvements to human and infrastructure longevity, and a more\ncommon culture, or a lowering of the amount of energy required to diffuse raw\nmaterials into civilization's bulk. According to a logistic equation, rates of\nreturn are slowed by past growth, and if rates of return approach zero, such\n\"slowing down\" makes civilization fragile with respect to externally imposed\nnetwork decay. If past technological change has been especially rapid, then\ncivilization is particularly vulnerable to newly unfavorable conditions that\nmight force a switch into a mode of accelerating collapse.\n"
    },
    {
        "paper_id": 1306.3704,
        "authors": "Fabio Caccioli, J. Doyne Farmer, Nick Foti, and Daniel Rockmore",
        "title": "How interbank lending amplifies overlapping portfolio contagion: A case\n  study of the Austrian banking network",
        "comments": "24 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In spite of the growing theoretical literature on cascades of failures in\ninterbank lending networks, empirical results seem to suggest that networks of\ndirect exposures are not the major channel of financial contagion. In this\npaper we show that networks of interbank exposures can however significantly\namplify contagion due to overlapping portfolios. To illustrate this point, we\nconsider the case of the Austrian interbank network and perform stress tests on\nit according to different protocols. We consider in particular contagion due to\n(i) counterparty loss; (ii) roll-over risk; and (iii) overlapping portfolios.\nWe find that the average number of bankruptcies caused by counterparty loss and\nroll-over risk is fairly small if these contagion mechanisms are considered in\nisolation. Once portfolio overlaps are also accounted for, however, we observe\nthat the network of direct interbank exposures significantly contributes to\nsystemic risk.\n"
    },
    {
        "paper_id": 1306.3856,
        "authors": "Samuel R\\\"onnqvist and Peter Sarlin",
        "title": "From Text to Bank Interrelation Maps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  In the wake of the ongoing global financial crisis, interdependencies among\nbanks have come into focus in trying to assess systemic risk. To date, such\nanalysis has largely been based on numerical data. By contrast, this study\nattempts to gain further insight into bank interconnections by tapping into\nfinancial discussion. Co-mentions of bank names are turned into a network,\nwhich can be visualized and analyzed quantitatively, in order to illustrate\ncharacteristics of individual banks and the network as a whole. The approach\nallows for the study of temporal dynamics of the network, to highlight changing\npatterns of discussion that reflect real-world events, the current financial\ncrisis in particular. For instance, it depicts how connections from distressed\nbanks to other banks and supervisory authorities have emerged and faded over\ntime, as well as how global shifts in network structure coincide with severe\ncrisis episodes. The usage of textual data holds an additional advantage in the\npossibility of gaining a more qualitative understanding of an observed\ninterrelation, through its context. We illustrate our approach using a case\nstudy on Finnish banks and financial institutions. The data set comprises 3.9M\nposts from online, financial and business-related discussion, during the years\n2004 to 2012. Future research includes analyzing European news articles with a\nbroader perspective, and a focus on improving semantic description of\nrelations.\n"
    },
    {
        "paper_id": 1306.3923,
        "authors": "Albert Ferreiro-Castilla and Kees van Schaik",
        "title": "Applying the Wiener-Hopf Monte Carlo simulation technique for Levy\n  processes to path functionals such as first passage times, undershoots and\n  overshoots",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we apply the recently established Wiener-Hopf Monte Carlo (WHMC)\nsimulation technique for Levy processes from Kuznetsov et al. [17] to path\nfunctionals, in particular first passage times, overshoots, undershoots and the\nlast maximum before the passage time. Such functionals have many applications,\nfor instance in finance (the pricing of exotic options in a Levy model) and\ninsurance (ruin time, debt at ruin and related quantities for a Levy insurance\nrisk process). The technique works for any Levy process whose running infimum\nand supremum evaluated at an independent exponential time allows sampling from.\nThis includes classic examples such as stable processes, subclasses of\nspectrally one sided Levy processes and large new families such as meromorphic\nLevy processes. Finally we present some examples. A particular aspect that is\nillustrated is that the WHMC simulation technique performs much better at\napproximating first passage times than a `plain' Monte Carlo simulation\ntechnique based on sampling increments of the Levy process.\n"
    },
    {
        "paper_id": 1306.407,
        "authors": "Wei Chen",
        "title": "Fractional G-White Noise Theory, Wavelet Decomposition for Fractional\n  G-Brownian Motion, and Bid-Ask Pricing Application to Finance Under\n  Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  G-framework is presented by Peng [41] for measure risk under uncertainty. In\nthis paper, we define fractional G-Brownian motion (fGBm). Fractional\nG-Brownian motion is a centered G-Gaussian process with zero mean and\nstationary increments in the sense of sub-linearity with Hurst index $H\\in\n(0,1)$. This process has stationary increments, self-similarity, and long rang\ndependence properties in the sense of sub-linearity. These properties make the\nfractional G-Brownian motion a suitable driven process in mathematical finance.\nWe construct wavelet decomposition of the fGBm by wavelet with compactly\nsupport. We develop fractional G-white noise theory, define G-It\\^o-Wick\nstochastic integral, establish the fractional G-It\\^o formula and the\nfractional G-Clark-Ocone formula, and derive the G-Girsanov's Theorem. For\napplication the G-white noise theory, we consider the financial market modelled\nby G-Wick-It\\^o type of SDE driven by fGBm. The financial asset price modelled\nby fGBm has volatility uncertainty, using G-Girsanov's Theorem and\nG-Clark-Ocone Theorem, we derive that sublinear expectation of the discounted\nEuropean contingent claim is the bid-ask price of the claim.\n"
    },
    {
        "paper_id": 1306.4619,
        "authors": "Jean-Fran\\c{c}ois Renaud",
        "title": "On the time spent in the red by a refracted L\\'evy risk process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce an insurance ruin model with adaptive premium\nrate, thereafter refered to as restructuring/refraction, in which classical\nruin and bankruptcy are distinguished. In this model, the premium rate is\nincreased as soon as the wealth process falls into the red zone and is brought\nback to its regular level when the process recovers. The analysis is mainly\nfocused on the time a refracted L\\'evy risk process spends in the red zone\n(analogous to the duration of the negative surplus). Building on results from\nKyprianou and Loeffen (2010) and Loeffen et al. (2012), we identify the\ndistribution of various functionals related to occupation times of refracted\nspectrally negative L\\'evy processes. For example, these results are used to\ncompute the probability of bankruptcy and the probability of Parisian ruin in\nthis model with restructuring.\n"
    },
    {
        "paper_id": 1306.4733,
        "authors": "Tomasz R. Bielecki and Marek Rutkowski",
        "title": "Valuation and hedging of OTC contracts with funding costs,\n  collateralization and counterparty credit risk: Part 1",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The research presented in this work is motivated by some recent papers\nregarding hedging and valuation of financial securities subject to funding\ncosts, collateralization and counterparty credit risk. Our goal is to provide a\nsound theoretical underpinning for some results presented in these papers by\ndeveloping a unified martingale framework for the non-linear approach to\nhedging and pricing of OTC financial contracts. The impact that various funding\nbases and margin covenants exert on the values and hedging strategies for OTC\ncontracts is examined.\n"
    },
    {
        "paper_id": 1306.4769,
        "authors": "Giuseppe Buccheri, Stefano Marmi and Rosario N. Mantegna",
        "title": "Evolution of correlation structure of industrial indices of US equity\n  markets",
        "comments": "8 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.88.012806",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the dynamics of correlations present between pairs of industry\nindices of US stocks traded in US markets by studying correlation based\nnetworks and spectral properties of the correlation matrix. The study is\nperformed by using 49 industry index time series computed by K. French and E.\nFama during the time period from July 1969 to December 2011 that is spanning\nmore than 40 years. We show that the correlation between industry indices\npresents both a fast and a slow dynamics. The slow dynamics has a time scale\nlonger than five years showing that a different degree of diversification of\nthe investment is possible in different periods of time. On top to this slow\ndynamics, we also detect a fast dynamics associated with exogenous or\nendogenous events. The fast time scale we use is a monthly time scale and the\nevaluation time period is a 3 month time period. By investigating the\ncorrelation dynamics monthly, we are able to detect two examples of fast\nvariations in the first and second eigenvalue of the correlation matrix. The\nfirst occurs during the dot-com bubble (from March 1999 to April 2001) and the\nsecond occurs during the period of highest impact of the subprime crisis (from\nAugust 2008 to August 2009).\n"
    },
    {
        "paper_id": 1306.4958,
        "authors": "M. Hossein Partovi",
        "title": "Hedging and Leveraging: Principal Portfolios of the Capital Asset\n  Pricing Model",
        "comments": "8 pages, submitted for publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The principal portfolios of the standard Capital Asset Pricing Model (CAPM)\nare analyzed and found to have remarkable hedging and leveraging properties.\nPrincipal portfolios implement a recasting of any correlated asset set of N\nrisky securities into an equivalent but uncorrelated set when short sales are\nallowed. While a determination of principal portfolios in general requires a\ndetailed knowledge of the covariance matrix for the asset set, the rather\nsimple structure of CAPM permits an accurate solution for any reasonably large\nasset set that reveals interesting universal properties. Thus for an asset set\nof size N, we find a market-aligned portfolio, corresponding to the market\nportfolio of CAPM, as well as N-1 market-orthogonal portfolios which are market\nneutral and strongly leveraged. These results provide new insight into the\nreturn-volatility structure of CAPM, and demonstrate the effect of unbridled\nleveraging on volatility.\n"
    },
    {
        "paper_id": 1306.4975,
        "authors": "Raoul Golan and Austin Gerig",
        "title": "A Stochastic Feedback Model for Volatility",
        "comments": "5 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time series exhibit a number of interesting properties that are\ndifficult to explain with simple models. These properties include fat-tails in\nthe distribution of price fluctuations (or returns) that are slowly removed at\nlonger timescales, strong autocorrelations in absolute returns but zero\nautocorrelation in returns themselves, and multifractal scaling. Although the\nunderlying cause of these features is unknown, there is growing evidence they\noriginate in the behavior of volatility, i.e., in the behavior of the magnitude\nof price fluctuations. In this paper, we posit a feedback mechanism for\nvolatility that closely reproduces the non-trivial properties of empirical\nprices. The model is parsimonious, contains only two parameters that are easily\nestimated, fits empirical data better than standard models, and can be grounded\nin a straightforward framework where volatility fluctuations are driven by the\nestimation error of an exogenous Poisson rate.\n"
    },
    {
        "paper_id": 1306.4994,
        "authors": "Helena Jasiulewicz, Wojciech Kordecki",
        "title": "Additive versus multiplicative parameters - applications in economics\n  and finance",
        "comments": null,
        "journal-ref": "Annals of Operations Research (2016) 238:299-313",
        "doi": "10.1007/s10479-015-2035-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we pay our attention to geometric parameters and their\napplications in economics and finance. We discuss the multiplicative models in\nwhich a geometric mean and a geometric standard deviation are more natural than\narithmetic ones. We give two examples from Warsaw Stock Exchange in 1995--2009\nand from a bid of 52-week treasury bills in 1992--2009 in Poland as an\nillustrative example. For distributions having applications in finance and\ninsurance we give their multiplicative parameters as well as their estimations.\nWe consider, among others, heavy-tailed distributions such as lognormal and\nPareto distribution, applied to modelling of large losses.\n"
    },
    {
        "paper_id": 1306.5082,
        "authors": "Martin Larsson",
        "title": "Non-Equivalent Beliefs and Subjective Equilibrium Bubbles",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a dynamic equilibrium model where agents exhibit a strong\nform of belief heterogeneity: they disagree about zero probability events. It\nis shown that, somewhat surprisingly, equilibrium exists in this setting, and\nthat the disagreement about nullsets naturally leads to equilibrium asset\npricing bubbles. The bubbles are subjective in the sense that they are\nperceived by some but not necessarily all agents. In contrast to existing\nmodels, bubbles arise with no restrictions on trade beyond a standard solvency\nconstraint.\n"
    },
    {
        "paper_id": 1306.5145,
        "authors": "Dorje C. Brody and Lane P. Hughston",
        "title": "Social Discounting and the Long Rate of Interest",
        "comments": "30 pages, version to appear in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The well-known theorem of Dybvig, Ingersoll and Ross shows that the long\nzero-coupon rate can never fall. This result, which, although undoubtedly\ncorrect, has been regarded by many as surprising, stems from the implicit\nassumption that the long-term discount function has an exponential tail. We\nrevisit the problem in the setting of modern interest rate theory, and show\nthat if the long \"simple\" interest rate (or Libor rate) is finite, then this\nrate (unlike the zero-coupon rate) acts viably as a state variable, the value\nof which can fluctuate randomly in line with other economic indicators. New\ninterest rate models are constructed, under this hypothesis and certain\ngeneralizations thereof, that illustrate explicitly the good asymptotic\nbehaviour of the resulting discount bond systems. The conditions necessary for\nthe existence of such \"hyperbolic\" and \"generalized hyperbolic\" long rates are\nthose of so-called social discounting, which allow for long-term cash flows to\nbe treated as broadly \"just as important\" as those of the short or medium term.\nAs a consequence, we are able to provide a consistent arbitrage-free valuation\nframework for the cost-benefit analysis and risk management of long-term social\nprojects, such as those associated with sustainable energy, resource\nconservation, and climate change.\n"
    },
    {
        "paper_id": 1306.5198,
        "authors": "Tomasz R. Bielecki, Igor Cialenco, Samuel Drapeau, Martin Karliczek",
        "title": "Dynamic Assessment Indices",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a unified framework, which allows, in particular, to\nstudy the structure of dynamic monetary risk measures and dynamic acceptability\nindices. The main mathematical tool, which we use here, and which allows us to\nsignificantly generalize existing results is the theory of $L^0$-modules. In\nthe first part of the paper we develop the general theory and provide a robust\nrepresentation of conditional assessment indices, and in the second part we\napply this theory to dynamic acceptability indices acting on stochastic\nprocesses.\n"
    },
    {
        "paper_id": 1306.5302,
        "authors": "Diane Wilcox, Tim Gebbie",
        "title": "Factorising equity returns in an emerging market through exogenous\n  shocks and capital flows",
        "comments": "27 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A technique from stochastic portfolio theory [Fernholz, 1998] is applied to\nanalyse equity returns of Small, Mid and Large cap portfolios in an emerging\nmarket through periods of growth and regional crises, up to the onset of the\nglobal financial crisis. In particular, we factorize portfolios in the South\nAfrican market in terms of distribution of capital, change of stock ranks in\nportfolios, and the effect due to dividends for the period Nov 1994 to May\n2007. We discuss the results in the context of broader economic thinking to\nconsider capital flows as risk factors, turning around more established\napproaches which use macroeconomic and socio-economic conditions to explain\nForeign Direct Investment (into the economy) and Net Portfolio Investment (into\nequity and bond markets).\n"
    },
    {
        "paper_id": 1306.5447,
        "authors": "Matthew Lorig, Stefano Pagliarani, Andrea Pascucci",
        "title": "Explicit implied volatilities for multifactor local-stochastic\n  volatility models",
        "comments": "33 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an asset whose risk-neutral dynamics are described by a general\nclass of local-stochastic volatility models and derive a family of asymptotic\nexpansions for European-style option prices and implied volatilities. Our\nimplied volatility expansions are explicit; they do not require any special\nfunctions nor do they require numerical integration. To illustrate the accuracy\nand versatility of our method, we implement it under five different model\ndynamics: CEV local volatility, quadratic local volatility, Heston stochastic\nvolatility, $3/2$ stochastic volatility, and SABR local-stochastic volatility.\n"
    },
    {
        "paper_id": 1306.551,
        "authors": "Beno\\^it Collins, David McDonald and Nadia Saad",
        "title": "Compound Wishart Matrices and Noisy Covariance Matrices: Risk\n  Underestimation",
        "comments": "24 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we obtain a property of the expectation of the inverse of\ncompound Wishart matrices which results from their orthogonal invariance. Using\nthis property as well as results from random matrix theory (RMT), we derive the\nasymptotic effect of the noise induced by estimating the covariance matrix on\ncomputing the risk of the optimal portfolio. This in turn enables us to get an\nasymptotically unbiased estimator of the risk of the optimal portfolio not only\nfor the case of independent observations but also in the case of correlated\nobservations. This improvement provides a new approach to estimate the risk of\na portfolio based on covariance matrices estimated from exponentially weighted\nmoving averages of stock returns.\n"
    },
    {
        "paper_id": 1306.5705,
        "authors": "Babacar Seck, Robert J. Elliott, Jean-Pierre Gueyie",
        "title": "Computational Dynamic Market Risk Measures in Discrete Time Setting",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Different approaches to defining dynamic market risk measures are available\nin the literature. Most are focused or derived from probability theory,\neconomic behavior or dynamic programming. Here, we propose an approach to\ndefine and implement dynamic market risk measures based on recursion and state\neconomy representation. The proposed approach is to be implementable and to\ninherit properties from static market risk measures.\n"
    },
    {
        "paper_id": 1306.6267,
        "authors": "Stefan Tappe, and Thorsten Schmidt",
        "title": "Dynamic Term Structure Modelling with Default and Mortality Risk: New\n  Results on Existence and Monotonicity",
        "comments": "27 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers general term structure models like the ones appearing in\nportfolio credit risk modelling or life insurance. We give a general model\nstarting from families of forward rates driven by infinitely many Brownian\nmotions and an integer-valued random measure, generalizing existing approaches\nin the literature. Then we derive drift conditions which are equivalent to no\nasymptotic free lunch on the considered market. Existence results are also\ngiven. In practice, models possessing a certain monotonicity are favorable and\nwe study general conditions which guarantee this. The setup is illustrated with\nsome examples.\n"
    },
    {
        "paper_id": 1306.6402,
        "authors": "Jia-Wen Gu, Bo Jiang, Wai-Ki Ching and Harry Zheng",
        "title": "On Modeling Economic Default Time: A Reduced-Form Model Approach",
        "comments": "arXiv admin note: text overlap with arXiv:1012.0843 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the aftermath of the global financial crisis, much attention has been paid\nto investigating the appropriateness of the current practice of default risk\nmodeling in banking, finance and insurance industries. A recent empirical study\nby Guo et al.(2008) shows that the time difference between the economic and\nrecorded default dates has a significant impact on recovery rate estimates. Guo\net al.(2011) develop a theoretical structural firm asset value model for a firm\ndefault process that embeds the distinction of these two default times. To be\nmore consistent with the practice, in this paper, we assume the market\nparticipants cannot observe the firm asset value directly and developed a\nreduced-form model to characterize the economic and recorded default times. We\nderive the probability distribution of these two default times. The numerical\nstudy on the difference between these two shows that our proposed model can\nboth capture the features and fit the empirical data.\n"
    },
    {
        "paper_id": 1306.6583,
        "authors": "Glenn Ierley",
        "title": "A note on Keen's model: The limits of Schumpeter's \"Creative\n  Destruction\"",
        "comments": "25 pages, 12 figures, JEL classification: B50, C62, C63, E12, E47",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a general solution for a recent model by Keen for\nendogenous money creation. The solution provides an analytic framework that\nexplains all significant dynamical features of Keen's model and their\nparametric dependence, including an exact result for both the period and\nsubsidence rate of the Great Moderation. It emerges that Keen's model has just\ntwo possible long term solutions: stable growth or terminal collapse. While\ncollapse can come about immediately from economies that are nonviable by virtue\nof unsuitable parameters or initial conditions, in general the collapse is\npreceded by an interval of exponential growth. In first approximation, the\nduration of that exponential growth is half a period of a sinusoidal\noscillation. The period is determined by reciprocal of the imaginary part of\none root of a certain quintic polynomial. The real part of the same root\ndetermines the rate of growth of the economy. The coefficients of that\npolynomial depend in a complicated way upon the numerous parameters in the\nproblem and so, therefore, the pattern of roots. For a favorable choice of\nparameters, the salient root is purely real. This is the circumstance that\nadmits the second possible long term solution, that of indefinite stable\ngrowth, i.e. an infinite period.\n"
    },
    {
        "paper_id": 1306.6588,
        "authors": "Pierre Nyquist",
        "title": "Moderate deviations for importance sampling estimators of risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Importance sampling has become an important tool for the computation of\ntail-based risk measures. Since such quantities are often determined mainly by\nrare events standard Monte Carlo can be inefficient and importance sampling\nprovides a way to speed up computations. This paper considers moderate\ndeviations for the weighted empirical process, the process analogue of the\nweighted empirical measure, arising in importance sampling. The moderate\ndeviation principle is established as an extension of existing results. Using a\ndelta method for large deviations established by Gao and Zhao (Ann. Statist.,\n2011) together with classical large deviation techniques, the moderate\ndeviation principle for the weighted empirical process is extended to\nfunctionals of the weighted empirical process which correspond to risk\nmeasures. The main results are moderate deviation principles for importance\nsampling estimators of the quantile function of a distribution and Expected\nShortfall.\n"
    },
    {
        "paper_id": 1306.6715,
        "authors": "David Chisholm, Graham Andersen",
        "title": "The Meaning of Probability of Default for Asset-backed Loans",
        "comments": "28 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The authors examine the concept of probability of default for asset-backed\nloans. In contrast to unsecured loans it is shown that probability of default\ncan be defined as either a measure of the likelihood of the borrower failing to\nmake required payments, or as the likelihood of an insufficiency of collateral\nvalue on foreclosure. Assuming expected loss is identical under either\ndefinition, this implies a corresponding pair of definitions for loss given\ndefault. Industry treatment of probability of default for asset-backed loans\nappears to inconsistently blend the two types of definition.\n  The authors develop a mathematical treatment of asset-backed loans which\nconsistently applies each type of definition in a framework to produce the same\nexpected loss and allows translation between the two frameworks.\n"
    },
    {
        "paper_id": 1307.0114,
        "authors": "Lisa R. Goldberg, Ola Mahmoud",
        "title": "Risk Without Return",
        "comments": null,
        "journal-ref": "Journal of Investment Strategies (2)2: 111-120, 2013",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk-only investment strategies have been growing in popularity as\ntraditional in- vestment strategies have fallen short of return targets over\nthe last decade. However, risk-based investors should be aware of four things.\nFirst, theoretical considerations and empirical studies show that apparently\ndictinct risk-based investment strategies are manifestations of a single\neffect. Second, turnover and associated transaction costs can be a substantial\ndrag on return. Third, capital diversification benefits may be reduced. Fourth,\nthere is an apparent connection between performance and risk diversification.\nTo analyze risk diversification benefits in a consistent way, we introduce the\nRisk Diversification Index (RDI) which measures risk concentrations and\ncomplements the Herfindahl-Herschman Index (HHI) for capital concentrations.\n"
    },
    {
        "paper_id": 1307.019,
        "authors": "R. Pincak",
        "title": "D-Brane solutions under market panic",
        "comments": "11 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:hep-th/0412306, arXiv:physics/0205053, arXiv:hep-th/0212134 by other\n  authors",
        "journal-ref": "International Journal of Geometric Methods in Modern Physics 15\n  (2018) 1850099",
        "doi": "10.1142/S0219887818500998",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relativistic quantum mechanic approach is used to develop a stock market\ndynamics. The relativistic is conceptional here as the meaning of big external\nvolatility or volatility shock on a financial market. We used a differential\ngeometry approach with the parallel transport of the prices to obtain a direct\nshift of the stock price movement. The prices are represented here as electrons\nwith different spin orientation. Up and down orientations of the spin particle\nare likened here as an increase or a decrease of stock prices. The paralel\ntransport of stock prices is enriched about Riemann curvature which describes\nsome arbitrage opportunities in the market. To solve the stock-price dynamics,\nwe used the Dirac equation for bispinors on the spherical brane-world. We found\nthat when a spherical brane is abbreviated to the disk on the equator, we\nconverge to the ideal behaviour of financial market where Black Scholes as well\nas semi-classical equations are sufficient. Full spherical brane-world\nscenarios can descibe a non-equilibrium market behaviour were all arbitrage\nopportunities as well as transaction costs are take into account.\n"
    },
    {
        "paper_id": 1307.0444,
        "authors": "Marcus Hildmann, Andreas Ulbig and G\\\"oran Andersson",
        "title": "Revisiting the Merit-Order Effect of Renewable Energy Sources",
        "comments": "Working Paper (9 pages, 11 figures, 5 tables) - Some revisions since\n  last version (10 February 2014). (Under 2nd review for IEEE Transactions on\n  Power Systems)",
        "journal-ref": null,
        "doi": "10.1109/PESGM.2015.7286477",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An on-going debate in the energy economics and power market community has\nraised the question if energy-only power markets are increasingly failing due\nto growing feed-in shares from subsidized renewable energy sources (RES). The\nshort answer to this is: No, they are not failing. Energy-based power markets\nare, however, facing several market distortions, namely from the gap between\nthe electricity volume traded at day-ahead markets versus the overall\nelectricity consumption as well as the (wrong) regulatory assumption that\nvariable RES generation, i.e., wind and photovoltaic (PV), truly have zero\nmarginal operation costs. In this paper we show that both effects over-amplify\nthe well-known merit-order effect of RES power feed-in beyond a level that is\nexplainable by underlying physical realities, i.e., thermal power plants being\nwilling to accept negative electricity prices to be able to stay online due to\nconsiderations of wear & tear and start-stop constraints. We analyze the\nimpacts of wind and PV power feed-in on the day-ahead market for a region that\nis already today experiencing significant feed-in tariff (FIT)-subsidized RES\npower feed-in, the EPEX German-Austrian market zone ($\\approx\\,$20% FIT share).\nOur analysis shows that, if the necessary regulatory adaptations are taken,\ni.e., increasing the day-ahead market's share of overall load demand and using\nthe true marginal costs of RES units in the merit-order, energy-based power\nmarkets can remain functional despite high RES power feed-in.\n"
    },
    {
        "paper_id": 1307.045,
        "authors": "M. Andrecut",
        "title": "Portfolio Optimization in R",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of finding the efficient frontier associated with the\nrisk-return portfolio optimization model. We derive the analytical expression\nof the efficient frontier for a portfolio of N risky assets, and for the case\nwhen a risk-free asset is added to the model. Also, we provide an R\nimplementation, and we discuss in detail a numerical example of a portfolio of\nseveral risky common stocks.\n"
    },
    {
        "paper_id": 1307.0684,
        "authors": "Pauline Barrieu, Giacomo Scandolo",
        "title": "Assessing Financial Model Risk",
        "comments": "23 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Model risk has a huge impact on any risk measurement procedure and its\nquantification is therefore a crucial step. In this paper, we introduce three\nquantitative measures of model risk when choosing a particular reference model\nwithin a given class: the absolute measure of model risk, the relative measure\nof model risk and the local measure of model risk. Each of the measures has a\nspecific purpose and so allows for flexibility. We illustrate the various\nnotions by studying some relevant examples, so as to emphasize the\npracticability and tractability of our approach.\n"
    },
    {
        "paper_id": 1307.0785,
        "authors": "Tahir Choulli and Junfeng Ma",
        "title": "Explicit Description of HARA Forward Utilities and Their Optimal\n  Portfolios",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with forward performances of HARA type. Precisely, for a\nmarket model in which stock price processes are modeled by a locally bounded\n$d$-dimensional semimartingale, we elaborate a complete and explicit\ncharacterization for this type of forward utilities. Furthermore, the optimal\nportfolios for each of these forward utilities are explicitly described. Our\napproach is based on the minimal Hellinger martingale densities that are\nobtained from the important statistical concept of Hellinger process. These\nmartingale densities were introduced recently, and appeared herein tailor-made\nfor these forward utilities. After outlining our parametrization method for the\nHARA forward, we provide illustrations on discrete-time market models. Finally,\nwe conclude our paper by pointing out a number of related open questions.\n"
    },
    {
        "paper_id": 1307.0817,
        "authors": "Leonardo Bargigli, Andrea Lionetto, Stefano Viaggiu",
        "title": "A Statistical Test of Walrasian Equilibrium by Means of Complex Networks\n  Theory",
        "comments": "Title modified. Version published on J. Stat. Phys",
        "journal-ref": "J. Stat. Phys. (2016)",
        "doi": "10.1007/s10955-016-1599-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We represent an exchange economy in terms of statistical ensembles for\ncomplex networks by introducing the concept of market configuration. This is\ndefined as a sequence of nonnegative discrete random variables $\\{w_{ij}\\}$\ndescribing the flow of a given commodity from agent $i$ to agent $j$. This\nsequence can be arranged in a nonnegative matrix $W$ which we can regard as the\nrepresentation of a weighted and directed network or digraph $G$. Our main\nresult consists in showing that general equilibrium theory imposes highly\nrestrictive conditions upon market configurations, which are in most cases not\nfulfilled by real markets. An explicit example with reference to the e-MID\ninterbank credit market is provided.\n"
    },
    {
        "paper_id": 1307.0872,
        "authors": "Anis Matoussi, Hanen Mezghani and Mohamed Mnif",
        "title": "Maximization of recursive utilities under convex portfolio constraints",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust maximization problem from terminal wealth and consumption\nunder a convex constraints on the portfolio. We state the existence and the\nuniqueness of the consumption-investment strategy by studying the associated\nquadratic backward stochastic differential equation (BSDE in short). We\ncharacterize the optimal control by using the duality method and deriving a\ndynamic maximum principle.\n"
    },
    {
        "paper_id": 1307.132,
        "authors": "M. Basei, A. Cesaroni, T. Vargiolu",
        "title": "Optimal exercise of swing contracts in energy markets: an integral\n  constrained stochastic optimal control problem",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize the value of swing contracts in continuous time as the unique\nviscosity solution of a Hamilton-Jacobi-Bellman equation with suitable boundary\nconditions. The case of contracts with penalties is straightforward, and in\nthat case only a terminal condition is needed. Conversely, the case of\ncontracts with strict constraints gives rise to a stochastic control problem\nwith a nonstandard state constraint. We approach this problem by a penalty\nmethod: we consider a general constrained problem and approximate the value\nfunction with a sequence of value functions of appropriate unconstrained\nproblems with a penalization term in the objective functional. Coming back to\nthe case of swing contracts with strict constraints, we finally characterize\nthe value function as the unique viscosity solution with polynomial growth of\nthe Hamilton-Jacobi-Bellman equation subject to appropriate boundary\nconditions.\n"
    },
    {
        "paper_id": 1307.1501,
        "authors": "Rafal Kulik and Philippe Soulier",
        "title": "Heavy tailed time series with extremal independence",
        "comments": "Revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider strictly stationary heavy tailed time series whose\nfinite-dimensional exponent measures are concentrated on axes, and hence their\nextremal properties cannot be tackled using classical multivariate regular\nvariation that is suitable for time series with extremal dependence. We recover\nrelevant information about limiting behavior of time series with extremal\nindependence by introducing a sequence of scaling functions and conditional\nscaling exponent. Both quantities provide more information about joint extremes\nthan a widely used tail dependence coefficient. We calculate the scaling\nfunctions and the scaling exponent for variety of models, including Markov\nchains, exponential autoregressive model, stochastic volatility with heavy\ntailed innovations or volatility.\n"
    },
    {
        "paper_id": 1307.1685,
        "authors": "Pierre Degond (IMT), Jian-Guo Liu, Christian Ringhofer",
        "title": "Evolution of the distribution of wealth in an economic environment\n  driven by local Nash equilibria",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10955-013-0888-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present and analyze a model for the evolution of the wealth distribution\nwithin a heterogeneous economic environment. The model considers a system of\nrational agents interacting in a game theoretical framework, through fairly\ngeneral assumptions on the cost function. This evolution drives the dynamic of\nthe agents in both wealth and economic configuration variables. We consider a\nregime of scale separation where the large scale dynamics is given by a\nhydrodynamic closure with a Nash equilibrium serving as the local thermodynamic\nequilibrium. The result is a system of gas dynamics-type equations for the\ndensity and average wealth of the agents on large scales. We recover the\ninverse gamma distribution as an equilibrium in the particular case of\nquadratic cost functions which has been previously considered in the\nliterature.\n"
    },
    {
        "paper_id": 1307.2014,
        "authors": "Dariusz Grech and Grzegorz Pamu{\\l}a",
        "title": "On the multifractal effects generated by monofractal signals",
        "comments": "36 pages, 41 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2013.07.045",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study quantitatively the level of false multifractal signal one may\nencounter while analyzing multifractal phenomena in time series within\nmultifractal detrended fluctuation analysis (MF-DFA). The investigated effect\nappears as a result of finite length of used data series and is additionally\namplified by the long-term memory the data eventually may contain. We provide\nthe detailed quantitative description of such apparent multifractal background\nsignal as a threshold in spread of generalized Hurst exponent values $\\Delta h$\nor a threshold in the width of multifractal spectrum $\\Delta \\alpha$ below\nwhich multifractal properties of the system are only apparent, i.e. do not\nexist, despite $\\Delta\\alpha\\neq0$ or $\\Delta h\\neq 0$. We find this effect\nquite important for shorter or persistent series and we argue it is linear with\nrespect to autocorrelation exponent $\\gamma$. Its strength decays according to\npower law with respect to the length of time series. The influence of basic\nlinear and nonlinear transformations applied to initial data in finite time\nseries with various level of long memory is also investigated. This provides\nadditional set of semi-analytical results. The obtained formulas are\nsignificant in any interdisciplinary application of multifractality, including\nphysics, financial data analysis or physiology, because they allow to separate\nthe 'true' multifractal phenomena from the apparent (artificial) multifractal\neffects. They should be a helpful tool of the first choice to decide whether we\ndo in particular case with the signal with real multiscaling properties or not.\n"
    },
    {
        "paper_id": 1307.2048,
        "authors": "Gregor Wergen",
        "title": "Modeling record-breaking stock prices",
        "comments": "20 pages, 28 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2013.11.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the statistics of record-breaking events in daily stock prices of\n366 stocks from the Standard and Poors 500 stock index. Both the record events\nin the daily stock prices themselves and the records in the daily returns are\ndiscussed. In both cases we try to describe the record statistics of the stock\ndata with simple theoretical models. The daily returns are compared to i.i.d.\nRV's and the stock prices are modeled using a biased random walk, for which the\nrecord statistics are known. These models agree partly with the behavior of the\nstock data, but we also identify several interesting deviations. Most\nimportantly, the number of records in the stocks appears to be systematically\ndecreased in comparison with the random walk model. Considering the\nautoregressive AR(1) process, we can predict the record statistics of the daily\nstock prices more accurately. We also compare the stock data with simulations\nof the record statistics of the more complicated GARCH(1,1) model, which, in\ncombination with the AR(1) model, gives the best agreement with the\nobservational data. To better understand our findings, we discuss the survival\nand first-passage times of stock prices on certain intervals and analyze the\ncorrelations between the individual record events. After recapitulating some\nrecent results for the record statistics of ensembles of N stocks, we also\npresent some new observations for the weekly distributions of record events.\n"
    },
    {
        "paper_id": 1307.2169,
        "authors": "Ricardo Lopez-Ruiz, Elyas Shivanian and Jose-Luis Lopez",
        "title": "Random Market Models with an H-Theorem",
        "comments": "11 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:1104.2187",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this communication, some economic models given by functional mappings are\naddressed. These are models for random markets where agents trade by pairs and\nexchange their money in a random and conservative way. They display the\nexponential wealth distribution as asymptotic equilibrium, independently of the\neffectiveness of the transactions and of the limitation of the total wealth.\nThe entropy increases with time in these models and the existence of an\nH-theorem is computationally checked. Also, it is shown that any small\nperturbation of the models equations make them to lose the exponential\ndistribution as an equilibrium solution.\n"
    },
    {
        "paper_id": 1307.2176,
        "authors": "Chantal C. Cantarelli, Bent Flybjerg, Eric J. E. Molin, and Bert van\n  Wee",
        "title": "Cost overruns in Large-Scale Transportation Infrastructure Projects:\n  Explanations and Their Theoretical Embeddedness",
        "comments": null,
        "journal-ref": "European Journal of Transport and Infrastructure Research, vol.\n  10, no. 1, March 2010, 5-18",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing large-scale transportation infrastructure projects is difficult due\nto frequent misinformation about the costs which results in large cost overruns\nthat often threaten the overall project viability. This paper investigates the\nexplanations for cost overruns that are given in the literature. Overall, four\ncategories of explanations can be distinguished: technical, economic,\npsychological, and political. Political explanations have been seen to be the\nmost dominant explanations for cost overruns. Agency theory is considered the\nmost interesting for political explanations and an eclectic theory is also\nconsidered possible. Nonpolitical explanations are diverse in character,\ntherefore a range of different theories (including rational choice theory and\nprospect theory), depending on the kind of explanation is considered more\nappropriate than one all-embracing theory.\n"
    },
    {
        "paper_id": 1307.2177,
        "authors": "Chantal C. Cantarelli, Bent Flybjerg, Bert van Wee, and Eric J. E.\n  Molin",
        "title": "Lock-in and Its Influence on the Project Performance of Large-Scale\n  Transportation Infrastructure Projects. Investigating the Way in Which\n  Lock-in Can Emerge and Affect Cost Overruns",
        "comments": null,
        "journal-ref": "Environment and Planning B: Planning and Design, vol. 37, no 5,\n  May 2010, 792-807",
        "doi": "10.1068/b36017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lock-in, the escalating commitment of decision-makers to an ineffective\ncourse of action, has the potential to explain the large cost overruns in large\nscale transportation infrastructure projects. Lock-in can occur both at the\ndecision-making level (before the decision to build) and at the project level\n(after the decision to build) and can influence the extent of overruns in two\nways. The first involves the methodology of calculating cost overruns according\nto the formal decision to build. Due to lock-in, however, the real decision to\nbuild is made much earlier in the decision-making process and the costs\nestimated at that stage are often much lower than those that are estimated at a\nlater stage in the decision-making process, thus increasing cost overruns. The\nsecond way that lock-in can affect cost overruns is through practice. Although\ndecisions about the project (design and implementation) need to be made,\nlock-in can lead to inefficient decisions that involve higher costs. Sunk costs\n(in terms of both time and money), the need for justification, escalating\ncommitment, and inflexibility and the closure of alternatives are indicators of\nlock-in. In this paper, two case studies, of the Betuweroute and the HSL-South\nprojects in the Netherlands, demonstrate the presence of lock-in and its\ninfluence on the extent of cost overruns at both the decision-making and\nproject levels. This suggests that recognition of lock-in as an explanation for\ncost overruns significantly contributes to the understanding of the inadequate\nplanning process of projects and allows development of more appropriate means.\n"
    },
    {
        "paper_id": 1307.2178,
        "authors": "Chantal C. Cantarelli, Eric J. E. Molin, Bert van Wee, and Bent\n  Flyvbjerg",
        "title": "Characteristics of Cost Overruns for Dutch Transport Infrastructure\n  Projects and the Importance of the Decision to Build and Project Phases",
        "comments": null,
        "journal-ref": "Transport Policy, vol. 22, July 2012, 49-56",
        "doi": "10.1016/j.tranpol.2012.04.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a methodology similar to that used the in the worldwide research, the\ncost performance of Dutch large-scale transport infrastructure projects is\ndetermined. In the Netherlands, cost overruns are as common as cost underruns\nbut because cost overruns are larger than cost underruns projects on average\nhave a cost overrun of 16.5%. The focus on one country further enabled to\nconsider cost overruns during different project development phases. It turned\nout that in the Netherlands the majority of the cost overrun occurs in the\npre-construction phase (the period between the formal decision to build and the\nstart of construction). The frequency as well as the magnitude of\npre-construction cost overrun is significantly higher than in the construction\nphase. The used methodology of calculating cost overruns does however not take\nlock-in into account. This phenomenon shows that the real decision to build was\ntaken much earlier in the decision-making process. Since estimated costs are\nusually lower during these earlier stages, the cost overruns based on this real\ndecision to build are likely to be much higher. Cost overruns presented in\nstudies are therefore often underestimated and the problem of cost overruns is\nmuch larger than we think.\n"
    },
    {
        "paper_id": 1307.2179,
        "authors": "Chantal C. Cantarelli, Bert van Wee, Eric J. E. Molin, and Bent\n  Flyvbjerg",
        "title": "Different Cost Performance: Different Determinants? The Case of Cost\n  Overruns in Dutch Transportation Infrastructure Projects",
        "comments": null,
        "journal-ref": "Transport Policy, vol. 22, July 2012, 88-95",
        "doi": "10.1016/j.tranpol.2012.04.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines three independent explanatory variables and their\nrelation with cost overrun in order to decide whether this is different for\nDutch infrastructure projects compared to worldwide findings. The three\nindependent variables are project type (road, rail, and fixed link projects),\nproject size (measured in terms of estimated costs) and the length of the\nproject implementation phase. For Dutch projects, average cost overrun is 10.6%\nfor rail, 18.6% for roads and 21.7% for fixed links. For project size, small\nDutch projects have the largest average percentage cost overruns but in terms\nof total overrun, large projects have a larger share. The length of the\nimplementation phase and especially the length of the pre-construction phase\nare important determinants of cost overruns in the Netherlands. With each\nadditional year of pre-construction, percentage cost overrun increases by five\npercentage points. In contrast, the length of the construction phase has hardly\nany influence on cost overruns. This is an important contribution to current\nknowledge about cost overruns, because the period in which projects are most\nprone to cost overruns is narrowed down considerably, at least in the\nNetherlands. This means that period can be focused on to determine the causes\nand cures of overruns.\n"
    },
    {
        "paper_id": 1307.218,
        "authors": "Chantal C. Cantarelli, Caspar G. Chorus, and Scott W. Cunningham",
        "title": "Explaining Cost Overruns of Large-Scale Transportation Infrastructure\n  Projects using a Signalling Game",
        "comments": null,
        "journal-ref": "Transportmetrica A: Transport Science, vol. 9, no. 3, 2013,\n  239-258",
        "doi": "10.1080/18128602.2011.565817",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Strategic behaviour is one of the main explanations for cost overruns. It can\ntheoretically be supported by agency theory, in which strategic behaviour is\nthe result of asymmetric information between the principal and agent. This\npaper gives a formal account of this relation by a signalling game. This is a\ngame with incomplete information which considers the way in which parties\nanticipate upon other parties' behaviour in choosing a course of action. The\ngame shows how cost overruns are the result of an inappropriate signal. This\nmakes it impossible for the principal to distinguish between the types of\nagents, and hence, allows for strategic behaviour. It is illustrated how cost\noverruns can be avoided by means of two policy measures, e.g. an accountability\nstructure and benchmarking.\n"
    },
    {
        "paper_id": 1307.2181,
        "authors": "Chantal C. Cantarelli, Bent Flyvbjerg, and S{\\o}ren L. Buhl",
        "title": "Geographical Variation in Project Cost Performance: The Netherlands\n  versus Worldwide",
        "comments": null,
        "journal-ref": "Journal of Transport Geography, vol. 24, September 2012, 324-331",
        "doi": "10.1016/j.jtrangeo.2012.03.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cost overruns in transport infrastructure projects know no geographical\nlimits, overruns are a global phenomenon. Nevertheless, the size of cost\noverruns varies with location. In the Netherlands, cost overruns appear to be\nsmaller compared to the rest of the world. This paper tests whether Dutch\nprojects perform significantly better in terms of cost overruns than other\ngeographical areas. It is concluded that for road and tunnel projects, the\nNetherlands performs similarly to the rest of the world. For rail projects,\nDutch projects perform considerably better, with projects having significantly\nlower percentage cost overruns in real terms (11%) compared to projects in\nother North West European countries (27%) and in other geographical areas\n(44%). Bridge projects also have considerably smaller cost overruns: 7% in the\nNetherlands compared with 45% in other NW European countries and 27% in other\ngeographical areas. In explaining cost overruns, geography should therefore\nclearly be taken into consideration.\n"
    },
    {
        "paper_id": 1307.2218,
        "authors": "Laetitia Badouraly Kassim (LJK), J\\'er\\^ome Lelong (LJK), Imane\n  Loumrhari (LJK)",
        "title": "Importance sampling for jump processes and applications to finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adaptive importance sampling techniques are widely known for the Gaussian\nsetting of Brownian driven diffusions. In this work, we want to extend them to\njump processes. Our approach relies on a change of the jump intensity combined\nwith the standard exponential tilting for the Brownian motion. The free\nparameters of our framework are optimized using sample average approximation\ntechniques. We illustrate the efficiency of our method on the valuation of\nfinancial derivatives in several jump models.\n"
    },
    {
        "paper_id": 1307.2278,
        "authors": "William L. Gottesman, Andrew James Reagan, Peter Sheridan Dodds",
        "title": "Collective Philanthropy: Describing and Modeling the Ecology of Giving",
        "comments": "16 pages, 14 figures, 4 tables, to appear in PLoS ONE",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0098876",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reflective of income and wealth distributions, philanthropic gifting appears\nto follow an approximate power-law size distribution as measured by the size of\ngifts received by individual institutions. We explore the ecology of gifting by\nanalysing data sets of individual gifts for a diverse group of institutions\ndedicated to education, medicine, art, public support, and religion. We find\nthat the detailed forms of gift-size distributions differ across but are\nrelatively constant within charity categories. We construct a model for how a\ndonor's income affects their giving preferences in different charity\ncategories, offering a mechanistic explanation for variations in institutional\ngift-size distributions. We discuss how knowledge of gift-sized distributions\nmay be used to assess an institution's gift-giving profile, to help set\nfundraising goals, and to design an institution-specific giving pyramid.\n"
    },
    {
        "paper_id": 1307.2436,
        "authors": "Philip Protter",
        "title": "Strict Local Martingales with Jumps",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A strict local martingale is a local martingale which is not a martingale.\nThere are few explicit examples of \"naturally occurring\" strict local\nmartingales with jumps available in the literature. The purpose of this paper\nis to provide such examples, and to illustrate how they might arise via\nfiltration shrinkage, a phenomenon we would contend is common in applications\nsuch as filtering, control, and especially in mathematical finance. We give a\nmethod for constructing such examples and analyze one particular method in\ndetail.\n"
    },
    {
        "paper_id": 1307.2465,
        "authors": "Stefano Olgiati, Alessandro Danovi",
        "title": "Contraction or steady state? An analysis of credit risk management in\n  Italy in the period 2008-2012",
        "comments": "Presented at the New York School of Business-Copenhagen Business\n  School International Risk Management Conference 2013: Enduring Financial\n  Stability: Contemporary Challenges for Financial Risk Management and\n  Governance-Credit Risk and Tools for Financial Stability, Copenhagen (DK)\n  June 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk management in Italy is characterized, in the period June 2008 to\nJune 2012, by frequent (frequency=0.5 cycles per year) and intense (peak\namplitude: mean=39.2 billion Euros, s.e.=2.83 billion Euros) quarterly\ncontractions and expansions around the mean (915.4 billion Euros, s.e.=3.59\nbillion Euros) of the nominal total credit used by non-financial corporations.\nSuch frequent and intense fluctuations are frequently ascribed to exogenous\nBasel II procyclical effects on credit flow into the economy and, consequently,\nBasel III output based point in time Credit to GDP countercyclical buffering\nadvocated. We have tested the opposite null hypotheses that such variation is\nsignificantly correlated to actual default rates, and that such correlation is\nexplained by fluctuations of credit supply around a steady state. We have found\nthat, in the period June 2008 to June 2012 (n=17), linear regression of credit\ngrowth rates on default rates reveals a negative correlation of r=minus 0.6903\nwith R squared=0.4765, and that credit supply fluctuates steadily around the\ndefault rate with an Internal Steady State Parameter SSP=0.00245 with chi\nsquared=37.47 (v=16, P<.005). We conclude that fluctuations of the total credit\nused by non-financial corporations are exhaustively explained by variation of\nthe independent variable default rate, and that credit variation fluctuates\naround a steady state. We conclude that credit risk management in Italy has\nbeen effective in parameterizing credit supply variation to default rates\nwithin the Basel II operating framework. Basel III prospective countercyclical\npoint in time output buffers based on filtered Credit to GDP ratios and dynamic\nprovisioning proposals should take into account this underlying steady state\nstatistical pattern.\n"
    },
    {
        "paper_id": 1307.2493,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "On model-independent pricing/hedging using shortfall risk and quantiles",
        "comments": "Preliminary version. Keywords: model-independent hedging/pricing,\n  marginal constraints, shortfall risk, quantile hedging, optimal transport",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the pricing and hedging of exotic options in a model-independent\nset-up using \\emph{shortfall risk and quantiles}. We assume that the marginal\ndistributions at certain times are given. This is tantamount to calibrating the\nmodel to call options with discrete set of maturities but a continuum of\nstrikes. In the case of pricing with shortfall risk, we prove that the minimum\ninitial amount is equal to the super-hedging price plus the inverse of the\nutility at the given shortfall level. In the second result, we show that the\nquantile hedging problem is equivalent to super-hedging problems for knockout\noptions. These results generalize the duality results of [5,6] to the model\nindependent setting of [1].\n"
    },
    {
        "paper_id": 1307.2562,
        "authors": "Cody B. Hyndman and Menachem Wenger",
        "title": "Valuation Perspectives and Decompositions for Variable Annuities with\n  GMWB riders",
        "comments": "18 pages, proof of Lemma A.1 expanded for clarity",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2014.02.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The guaranteed minimum withdrawal benefit (GMWB) rider, as an add on to a\nvariable annuity (VA), guarantees the return of premiums in the form of peri-\nodic withdrawals while allowing policyholders to participate fully in any\nmarket gains. GMWB riders represent an embedded option on the account value\nwith a fee structure that is different from typical financial derivatives. We\nconsider fair pricing of the GMWB rider from a financial economic perspective.\nParticular focus is placed on the distinct perspectives of the insurer and\npolicyholder and the unifying relationship. We extend a decomposition of the VA\ncontract into components that reflect term-certain payments and embedded\nderivatives to the case where the policyholder has the option to surrender, or\nlapse, the contract early.\n"
    },
    {
        "paper_id": 1307.2824,
        "authors": "Moshe A. Milevsky, Thomas S. Salisbury",
        "title": "Optimal Retirement Tontines for the 21st Century: With Reference to\n  Mortality Derivatives in 1693",
        "comments": null,
        "journal-ref": "Proceedings of the Living to 100 Symposium, Society of Actuaries,\n  Orlando FL (2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical tontines promised enormous rewards to the last survivors at the\nexpense of those who died early. While this design appealed to the gambling\ninstinct, it is a suboptimal way to manage longevity risk during retirement.\nThis is why fair life annuities making constant payments -- where the insurance\ncompany is exposed to the longevity risk -- induces greater lifetime utility.\nHowever, tontines do not have to be designed using a winner-take-all approach\nand insurance companies do not actually sell fair life annuities, partially due\nto aggregate longevity risk.\n  In this paper we derive the tontine structure that maximizes lifetime\nutility, but doesn't expose the sponsor to any longevity risk. We examine its\nsensitivity to the size of the tontine pool; individual longevity risk\naversion; and subjective health status. The optimal tontine varies with the\nindividual's longevity risk aversion $\\gamma$ and the number of participants\n$n$, which is problematic for product design. That said, we introduce a\nstructure called a natural tontine whose payout declines in exact proportion to\nthe (expected) survival probabilities, which is near-optimal for all $\\gamma$\nand $n$. We compare the utility of optimal tontines to the utility of loaded\nlife annuities under reasonable demographic and economic conditions and find\nthat the life annuity's advantage over tontines, is minimal.\n  We also review and analyze the first-ever mortality-derivative issued by the\nBritish government, known as King Williams's tontine of 1693. We shed light on\nthe preferences and beliefs of those who invested in the tontines vs. the\nannuities and argue that tontines should be re-introduced and allowed to\nco-exist with life annuities. Individuals would likely select a portfolio of\ntontines and annuities that suit their personal preferences for consumption and\nlongevity risk, as they did over 320 years ago.\n"
    },
    {
        "paper_id": 1307.2849,
        "authors": "Giorgio Ferrari, Frank Riedel, Jan-Henrik Steg",
        "title": "Continuous-Time Public Good Contribution under Uncertainty: A Stochastic\n  Control Approach",
        "comments": "38 pages. This version is a revised version of the paper\n  \"Continuous-Time Public Good Contribution under Uncertainty\". Added new\n  results, improved exposition of the results and changed some proofs",
        "journal-ref": "Applied Mathematics and Optimization, 75 (2017), pp. 429-470",
        "doi": "10.1007/s00245-016-9337-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study continuous-time stochastic control problems with both\nmonotone and classical controls motivated by the so-called public good\ncontribution problem. That is the problem of n economic agents aiming to\nmaximize their expected utility allocating initial wealth over a given time\nperiod between private consumption and irreversible contributions to increase\nthe level of some public good. We investigate the corresponding social planner\nproblem and the case of strategic interaction between the agents, i.e. the\npublic good contribution game. We show existence and uniqueness of the social\nplanner's optimal policy, we characterize it by necessary and sufficient\nstochastic Kuhn-Tucker conditions and we provide its expression in terms of the\nunique optional solution of a stochastic backward equation. Similar stochastic\nfirst order conditions prove to be very useful for studying any Nash equilibria\nof the public good contribution game. In the symmetric case they allow us to\nprove (qualitative) uniqueness of the Nash equilibrium, which we again\nconstruct as the unique optional solution of a stochastic backward equation. We\nfinally also provide a detailed analysis of the so-called free rider effect.\n"
    },
    {
        "paper_id": 1307.306,
        "authors": "Ladislav Kristoufek and Miloslav Vosvrda",
        "title": "Measuring capital market efficiency: Long-term memory, fractal dimension\n  and approximate entropy",
        "comments": "12 pages, 1 figure, 4 tables",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2014-50113-6",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We utilize long-term memory, fractal dimension and approximate entropy as\ninput variables for the Efficiency Index [Kristoufek & Vosvrda (2013), Physica\nA 392]. This way, we are able to comment on stock market efficiency after\ncontrolling for different types of inefficiencies. Applying the methodology on\n38 stock market indices across the world, we find that the most efficient\nmarkets are situated in the Eurozone (the Netherlands, France and Germany) and\nthe least efficient ones in the Latin America (Venezuela and Chile).\n"
    },
    {
        "paper_id": 1307.3597,
        "authors": "Marcel Nutz",
        "title": "Utility Maximization under Model Uncertainty in Discrete Time",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a general formulation of the utility maximization problem under\nnondominated model uncertainty in discrete time and show that an optimal\nportfolio exists for any utility function that is bounded from above. In the\nunbounded case, integrability conditions are needed as nonexistence may arise\neven if the value function is finite.\n"
    },
    {
        "paper_id": 1307.3672,
        "authors": "Sona Kilianova and Daniel Sevcovic",
        "title": "Transformation Method for Solving Hamilton-Jacobi-Bellman Equation for\n  Constrained Dynamic Stochastic Optimal Allocation Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose and analyze a method based on the Riccati\ntransformation for solving the evolutionary Hamilton-Jacobi-Bellman equation\narising from the stochastic dynamic optimal allocation problem. We show how the\nfully nonlinear Hamilton-Jacobi-Bellman equation can be transformed into a\nquasi-linear parabolic equation whose diffusion function is obtained as the\nvalue function of certain parametric convex optimization problem. Although the\ndiffusion function need not be sufficiently smooth, we are able to prove\nexistence, uniqueness and derive useful bounds of classical H\\\"older smooth\nsolutions. We furthermore construct a fully implicit iterative numerical scheme\nbased on finite volume approximation of the governing equation. A numerical\nsolution is compared to a semi-explicit traveling wave solution by means of the\nconvergence ratio of the method. We compute optimal strategies for a portfolio\ninvestment problem motivated by the German DAX 30 Index as an example of\napplication of the method.\n"
    },
    {
        "paper_id": 1307.4591,
        "authors": "Giuseppe Benedetti and Luciano Campi",
        "title": "Utility indifference valuation for non-smooth payoffs with an\n  application to power derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of exponential utility indifference valuation under\nthe simplified framework where traded and nontraded assets are uncorrelated but\nwhere the claim to be priced possibly depends on both. Traded asset prices\nfollow a multivariate Black and Scholes model, while nontraded asset prices\nevolve as generalized Ornstein-Uhlenbeck processes. We provide a BSDE\ncharacterization of the utility indifference price (UIP) for a large class of\nnon-smooth, possibly unbounded, payoffs depending simultaneously on both\nclasses of assets. Focusing then on European claims and using the Gaussian\nstructure of the model allows us to employ some BSDE techniques (in particular,\na Malliavin-type representation theorem due to Ma (2002)) to prove the\nregularity of Z and to characterize the UIP for possibly discontinuous European\npayoffs as a viscosity solution of a suitable PDE with continuous space\nderivatives. The optimal hedging strategy is also identified essentially as the\ndelta hedging strategy corresponding to the UIP. Since there are no closed-form\nformulas in general, we also obtain asymptotic expansions for prices and\nhedging strategies when the risk aversion parameter is small. Finally, our\nresults are applied to pricing and hedging power derivatives in various\nstructural models for energy markets.\n"
    },
    {
        "paper_id": 1307.4643,
        "authors": "Damien Challet, Ahmed Bel Hadj Ayed",
        "title": "Predicting financial markets with Google Trends and not so random\n  keywords",
        "comments": "8 pages, 4 figures. First names and last names swapped",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We check the claims that data from Google Trends contain enough data to\npredict future financial index returns. We first discuss the many subtle (and\nless subtle) biases that may affect the backtest of a trading strategy,\nparticularly when based on such data. Expectedly, the choice of keywords is\ncrucial: by using an industry-grade backtesting system, we verify that random\nfinance-related keywords do not to contain more exploitable predictive\ninformation than random keywords related to illnesses, classic cars and arcade\ngames. We however show that other keywords applied on suitable assets yield\nrobustly profitable strategies, thereby confirming the intuition of Preis et\nal. (2013)\n"
    },
    {
        "paper_id": 1307.4727,
        "authors": "Ladislav Kristoufek",
        "title": "Testing power-law cross-correlations: Rescaled covariance test",
        "comments": "15 pages, 4 figures",
        "journal-ref": "European Physical Journal B 86:418, 2013",
        "doi": "10.1140/epjb/e2013-40705-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new test for detection of power-law cross-correlations among a\npair of time series - the rescaled covariance test. The test is based on a\npower-law divergence of the covariance of the partial sums of the long-range\ncross-correlated processes. Utilizing a heteroskedasticity and auto-correlation\nrobust estimator of the long-term covariance, we develop a test with desirable\nstatistical properties which is well able to distinguish between short- and\nlong-range cross-correlations. Such test should be used as a starting point in\nthe analysis of long-range cross-correlations prior to an estimation of\nbivariate long-term memory parameters. As an application, we show that the\nrelationship between volatility and traded volume, and volatility and returns\nin the financial markets can be labeled as the one with power-law\ncross-correlations.\n"
    },
    {
        "paper_id": 1307.4813,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "On utility maximization with derivatives under model uncertainty",
        "comments": "Robust utility maximization, model uncertainty, semi-static hedging",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the robust utility maximization using a static holding in\nderivatives and a dynamic holding in the stock. There is no fixed model for the\nprice of the stock but we consider a set of probability measures (models) which\nare not necessarily dominated by a fixed probability measure. By assuming that\nthe set of physical probability measures is convex and weakly compact, we\nobtain the duality result and the existence of an optimizer.\n"
    },
    {
        "paper_id": 1307.4821,
        "authors": "Takashi Ichinomiya",
        "title": "Power-law exponent of the Bouchaud-M\\'ezard model on regular random\n  network",
        "comments": "To be pubished in Phys. Rev. E",
        "journal-ref": "Phys. Rev. E 88, 012819 (2013)",
        "doi": "10.1103/PhysRevE.88.012819",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Bouchaud-M\\'ezard model on a regular random network. By assuming\nadiabaticity and independency, and utilizing the generalized central limit\ntheorem and the Tauberian theorem, we derive an equation that determines the\nexponent of the probability distribution function of the wealth as\n$x\\rightarrow \\infty$. The analysis shows that the exponent can be smaller than\n2, while a mean-field analysis always gives the exponent as being larger than\n2. The results of our analysis are shown to be good agreement with those of the\nnumerical simulations.\n"
    },
    {
        "paper_id": 1307.5122,
        "authors": "Maciej Trzetrzelewski",
        "title": "Relativistic Black-Scholes model",
        "comments": "18 pages, published",
        "journal-ref": "EPL (Europhysics Letters), Volume 117, Number 3, 2017",
        "doi": "10.1209/0295-5075/117/38004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Black-Scholes equation, after a certain coordinate transformation, is\nequivalent to the heat equation. On the other hand the relativistic extension\nof the latter, the telegraphers equation, can be derived from the Euclidean\nversion of the Dirac equation. Therefore the relativistic extension of the\nBlack-Scholes model follows from relativistic quantum mechanics quite\nnaturally. We investigate this particular model for the case of European\nvanilla options. Due to the notion of locality incorporated in this way one\nfinds that the volatility frown-like effect appears when comparing to the\noriginal Black-Scholes model.\n"
    },
    {
        "paper_id": 1307.5163,
        "authors": "Gordan Zitkovic",
        "title": "Dynamic Programming for controlled Markov families: abstractly and over\n  Martingale Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an abstract control-theoretic framework in which the validity of\nthe dynamic programming principle can be established in continuous time by a\nverification of a small number of structural properties. As an application we\ntreat several cases of interest, most notably the lower-hedging and\nutility-maximization problems of financial mathematics both of which are\nnaturally posed over ``sets of martingale measures''.\n"
    },
    {
        "paper_id": 1307.5268,
        "authors": "Yavni Bar-Yam, Marco Lagi, Yaneer Bar-Yam",
        "title": "South African Riots: Repercussion of the Global Food Crisis and US\n  Drought",
        "comments": "9 pages, 4 figures. New England Complex Systems Institute Report\n  2013-01-02",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High and volatile global food prices have led to food riots and played a\ncritical role in triggering the Arab Spring revolutions in recent years. The\nsevere drought in the US in the summer of 2012 led to a new increase in food\nprices. Through the fall, they remained at a threshold above which the riots\nand revolutions had predominantly occurred. Global prices at this level create\nconditions where an exacerbating local circumstance can trigger unrest. Global\ncorn (maize) prices reached new highs, and countries that depend mostly on\nmaize are more likely to experience high local food prices and associated\npressures toward social unrest. Here we analyze the conditions in South Africa,\nwhich is a heavily maize-dependent country. Coinciding with increased consumer\nfood indices this summer, massive labor strikes in mining and agriculture have\nled to the greatest single incident of social violence since the fall of\napartheid in 1994. Worker demands for dramatic pay increases reflect that their\nwages have not kept up with drastic increases in the prices of necessities,\nespecially food. Without attention to the global food price situation, more\nincidents of food-based social instability are likely to arise. Other countries\nthat have manifested food-related protests and riots in 2012 include Haiti and\nArgentina. Moreover, these cases of unrest are just the most visible symptom of\nwidespread suffering of poor populations worldwide due to elevated food prices.\nPolicy decisions that would directly impact food prices are decreasing the\nconversion of maize to ethanol in the US, and reimposing regulations on\ncommodity futures markets to prevent excessive speculation, which we have shown\ncauses bubbles and crashes in these markets. Absent such policy actions,\ngovernments and companies should track and mitigate the impact of high and\nvolatile food prices on citizens and employees.\n"
    },
    {
        "paper_id": 1307.5319,
        "authors": "Stanislao Gualdi, Marco Tarzia, Francesco Zamponi, Jean-Philippe\n  Bouchaud",
        "title": "Tipping points in macroeconomic Agent-Based models",
        "comments": "42 pages, 8 figures. Important revisions with respect to v2. Final\n  version, to appear in a special issue of the Journal of Economic Dynamics and\n  Control dedicated to the CRISIS project ( http://www.crisis-economics.eu )",
        "journal-ref": "Journal of Economic Dynamics & Control 50, 29-61 (2015)",
        "doi": "10.1016/j.jedc.2014.08.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this work is to explore the possible types of phenomena that\nsimple macroeconomic Agent-Based models (ABM) can reproduce. We propose a\nmethodology, inspired by statistical physics, that characterizes a model\nthrough its 'phase diagram' in the space of parameters. Our first motivation is\nto understand the large macro-economic fluctuations observed in the 'Mark I'\nABM. Our major finding is the generic existence of a phase transition between a\n'good economy' where unemployment is low, and a 'bad economy' where\nunemployment is high. We introduce a simpler framework that allows us to show\nthat this transition is robust against many modifications of the model, and is\ngenerically induced by an asymmetry between the rate of hiring and the rate of\nfiring of the firms. The unemployment level remains small until a tipping\npoint, beyond which the economy suddenly collapses. If the parameters are such\nthat the system is close to this transition, any small fluctuation is amplified\nas the system jumps between the two equilibria. We have explored several\nnatural extensions of the model. One is to introduce a bankruptcy threshold,\nlimiting the leverage of firms. This leads to a rich phase diagram with, in\nparticular, a region where acute endogenous crises occur, during which the\nunemployment rate shoots up before the economy can recover. We also introduce\nsimple wage policies. This leads to inflation (in the 'good' phase) or\ndeflation (in the 'bad' phase), but leaves the overall phase diagram of the\nmodel essentially unchanged. We have also started exploring the effect of\nsimple monetary policies that attempt to contain rising unemployment and defang\ncrises. We end the paper with general comments on the usefulness of ABMs to\nmodel macroeconomic phenomena, in particular in view of the time needed to\nreach a steady state that raises the issue of ergodicity in these models.\n"
    },
    {
        "paper_id": 1307.5336,
        "authors": "Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius",
        "title": "Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts",
        "comments": "To be published in Journal of the American Society for Information\n  Science and Technology",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of robo-readers to analyze news texts is an emerging technology trend\nin computational finance. In recent research, a substantial effort has been\ninvested to develop sophisticated financial polarity-lexicons that can be used\nto investigate how financial sentiments relate to future company performance.\nHowever, based on experience from other fields, where sentiment analysis is\ncommonly applied, it is well-known that the overall semantic orientation of a\nsentence may differ from the prior polarity of individual words. The objective\nof this article is to investigate how semantic orientations can be better\ndetected in financial and economic news by accommodating the overall\nphrase-structure information and domain-specific use of language. Our three\nmain contributions are: (1) establishment of a human-annotated finance\nphrase-bank, which can be used as benchmark for training and evaluating\nalternative models; (2) presentation of a technique to enhance financial\nlexicons with attributes that help to identify expected direction of events\nthat affect overall sentiment; (3) development of a linearized phrase-structure\nmodel for detecting contextual semantic orientations in financial and economic\nnews texts. The relevance of the newly added lexicon features and the benefit\nof using the proposed learning-algorithm are demonstrated in a comparative\nstudy against previously used general sentiment models as well as the popular\nword frequency models used in recent financial studies. The proposed framework\nis parsimonious and avoids the explosion in feature-space caused by the use of\nconventional n-gram features.\n"
    },
    {
        "paper_id": 1307.544,
        "authors": "Mehdi Lallouache, Fr\\'ed\\'eric Abergel",
        "title": "Tick Size Reduction and Price Clustering in a FX Order Book",
        "comments": "17 pages, Minor revisions",
        "journal-ref": "Physica A, Volume 416, 15 December 2014, Pages 488-498",
        "doi": "10.1016/j.physa.2014.09.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the statistical properties of the EBS order book for the\nEUR/USD and USD/JPY currency pairs and the impact of a ten-fold tick size\nreduction on its dynamics. A large fraction of limit orders are still placed\nright at or halfway between the old allowed prices. This generates price\nbarriers where the best quotes lie for much of the time, which causes the\nemergence of distinct peaks in the average shape of the book at round\ndistances. Furthermore, we argue that this clustering is mainly due to manual\ntraders who remained set to the old price resolution. Automatic traders easily\ntake price priority by submitting limit orders one tick ahead of clusters, as\nshown by the prominence of buy (sell) limit orders posted with rightmost digit\none (nine).\n"
    },
    {
        "paper_id": 1307.554,
        "authors": "Dorje C. Brody, Lane P. Hughston, Xun Yang",
        "title": "On the Pricing of Storable Commodities",
        "comments": "Version to appear as Chapter 17 in Financial Informatics: An\n  Information-Based Approach to Asset Pricing. D. C. Brody, L. P. Hughston & A.\n  Macrina (editors). Singapore: World Scientific Publishing Company (2022)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces an information-based model for the pricing of storable\ncommodities such as crude oil and natural gas. The model uses the concept of\nmarket information about future supply and demand as a basis for valuation.\nPhysical ownership of a commodity is taken to provide a stream of convenience\ndividends equivalent to a continuous cash flow. The market filtration is\nassumed to be generated jointly by (i) current and past levels of the dividend\nrate, and (ii) partial information concerning the future of the dividend flow.\nThe price of a commodity is the expectation under a suitable pricing measure of\nthe totality of the discounted risk-adjusted future convenience dividend,\nconditional on the information provided by the market filtration. In the\nsituation where the dividend rate is modelled by an Ornstein-Uhlenbeck process,\nthe prices of options on commodities can be derived in closed form. The\napproach that we present can be applied to other assets that yield potentially\nnegative effective cash flows, such as real estate, factories, refineries,\nmines, and power generating plants.\n"
    },
    {
        "paper_id": 1307.5602,
        "authors": "Yaroslav Ivanenko and Illya Pasichnichenko",
        "title": "Uncertainty and absence of arbitrage opportunity",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is shown that absence of arbitrage opportunity in financial markets is a\nparticular case of existence of uncertainty in decision system. Absence of\narbitrage opportunity is considered in the sense of the Arrow-Debreu model of\nfinancial market with a riskless asset, while uncertainty (or ambiguity) is\ndefined on the basis of the principle of internal coherence of M. Allais.\n"
    },
    {
        "paper_id": 1307.5617,
        "authors": "Tobias Harks and Philipp von Falkenhausen",
        "title": "Robust Quantitative Comparative Statics for a Multimarket Paradox",
        "comments": "23 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a quantitative approach to comparative statics that allows to\nbound the maximum effect of an exogenous parameter change on a system's\nequilibrium. The motivation for this approach is a well known paradox in\nmultimarket Cournot competition, where a positive price shock on a monopoly\nmarket may actually reduce the monopolist's profit. We use our approach to\nquantify for the first time the worst case profit reduction for multimarket\noligopolies exposed to arbitrary positive price shocks. For markets with affine\nprice functions and firms with convex cost technologies, we show that the\nrelative profit loss of any firm is at most 25% no matter how many firms\ncompete in the oligopoly. We further investigate the impact of positive price\nshocks on total profit of all firms as well as on social welfare. We find tight\nbounds also for these measures showing that total profit and social welfare\ndecreases by at most 25% and 16.6%, respectively. Finally, we show that in our\nmodel, mixed, correlated and coarse correlated equilibria are essentially\nunique, thus, all our bounds apply to these game solutions as well.\n"
    },
    {
        "paper_id": 1307.5881,
        "authors": "Freddy Delbaen",
        "title": "A Remark on the Structure of Expectiles",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expectiles were defined using a minimisation principle. They form a special\nclass of coherent risk measures. We will describe the scenario set and we will\nshow that there is a most severe commonotonic risk measure that is smaller than\nthe given expectile.\n"
    },
    {
        "paper_id": 1307.5975,
        "authors": "Ovidiu Racorean",
        "title": "Correct usage of transmission coefficient for timing the market",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traders and investors involved in an option contract having the underlying\nstock in range bound are likely to lose their initial investment. Timing in\nbuying an option contract is of capital importance. In a recent article [1] the\nhypothesis of range bound market is used in conjunction to Black-Scholes\nequation to find the transmission coefficient relation that help market\nprofessionals to correctly timing their investment and risk taking decisions.\nThe present paper explores the theoretical basis of transmission coefficient\nand its empirical evidence on the market.\n"
    },
    {
        "paper_id": 1307.5981,
        "authors": "Krenar Avdulaj, Jozef Barunik",
        "title": "Are benefits from oil - stocks diversification gone? New evidence from a\n  dynamic copula and high frequency data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Oil is perceived as a good diversification tool for stock markets. To fully\nunderstand this potential, we propose a new empirical methodology that combines\ngeneralized autoregressive score copula functions with high frequency data and\nallows us to capture and forecast the conditional time-varying joint\ndistribution of the oil -- stocks pair accurately. Our realized GARCH with\ntime-varying copula yields statistically better forecasts of the dependence and\nquantiles of the distribution relative to competing models. Employing a\nrecently proposed conditional diversification benefits measure that considers\nhigher-order moments and nonlinear dependence from tail events, we document\ndecreasing benefits from diversification over the past ten years. The\ndiversification benefits implied by our empirical model are, moreover, strongly\nvaried over time. These findings have important implications for asset\nallocation, as the benefits of including oil in stock portfolios may not be as\nlarge as perceived.\n"
    },
    {
        "paper_id": 1307.602,
        "authors": "Jean-Fran\\c{c}ois Chassagneux, Romuald Elie, Idris Kharroubi",
        "title": "When terminal facelift enforces Delta constraints",
        "comments": "37 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the super-replication of non path-dependent European\nclaims under additional convex constraints on the number of shares held in the\nportfolio. The corresponding super-replication price of a given claim has been\nwidely studied in the literature and its terminal value, which dominates the\nclaim of interest, is the so-called facelift transform of the claim. We\ninvestigate under which conditions the super-replication price and strategy of\na large class of claims coincide with the exact replication price and strategy\nof the facelift transform of this claim. In one dimension, we observe that this\nproperty is satisfied for any local volatility model. In any dimension, we\nexhibit an analytical necessary and sufficient condition for this property,\nwhich combines the dynamics of the stock together with the characteristics of\nthe closed convex set of constraints. To obtain this condition, we introduce\nthe notion of first order viability property for linear parabolic PDEs. We\ninvestigate in details several practical cases of interest: multidimensional\nBlack Scholes model, non-tradable assets or short selling restrictions.\n"
    },
    {
        "paper_id": 1307.6036,
        "authors": "Claudia Ceci, Katia Colaneri, Alessandra Cretarola",
        "title": "A Benchmark Approach to Risk-Minimization under Partial Information",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2014.01.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a risk-minimizing hedging problem for a semimartingale\nincomplete financial market where d+1 assets are traded continuously and whose\nprice is expressed in units of the num\\'{e}raire portfolio. According to the\nso-called benchmark approach, we investigate the (benchmarked) risk-minimizing\nstrategy in the case where there are restrictions on the available information.\nMore precisely, we characterize the optimal strategy as the integrand appearing\nin the Galtchouk-Kunita-Watanabe decomposition of the benchmarked claim under\npartial information and provide its description in terms of the integrands in\nthe classical Galtchouk-Kunita-Watanabe decomposition under full information\nvia dual predictable projections. Finally, we apply the results in the case of\na Markovian jump-diffusion driven market model where the assets prices dynamics\ndepend on a stochastic factor which is not observable by investors.\n"
    },
    {
        "paper_id": 1307.6046,
        "authors": "Ladislav Kristoufek",
        "title": "Mixed-correlated ARFIMA processes for power-law cross-correlations",
        "comments": "12 pages, 7 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 392(24), pp.\n  6484-6493, 2013",
        "doi": "10.1016/j.physa.2013.08.041",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a general framework of the Mixed-correlated ARFIMA (MC-ARFIMA)\nprocesses which allows for various specifications of univariate and bivariate\nlong-term memory. Apart from a standard case when $H_{xy}={1}{2}(H_x+H_y)$,\nMC-ARFIMA also allows for processes with $H_{xy}<{1}{2}(H_x+H_y)$ but also for\nlong-range correlated processes which are either short-range cross-correlated\nor simply correlated. The major contribution of MC-ARFIMA lays in the fact that\nthe processes have well-defined asymptotic properties for $H_x$, $H_y$ and\n$H_{xy}$, which are derived in the paper, so that the processes can be used in\nsimulation studies comparing various estimators of the bivariate Hurst exponent\n$H_{xy}$. Moreover, the framework allows for modeling of processes which are\nfound to have $H_{xy}<{1}{2}(H_x+H_y)$.\n"
    },
    {
        "paper_id": 1307.6322,
        "authors": "Fulvio Baldovin, Massimiliano Caporin, Michele Caraglio, Attilio\n  Stella and Marco Zamparo",
        "title": "Option pricing with non-Gaussian scaling and infinite-state switching\n  volatility",
        "comments": "Revised version. 31 pages, 4 figures",
        "journal-ref": "Journal of Econometrics 187 (2015) 486-497",
        "doi": "10.1016/j.jeconom.2015.02.033",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility clustering, long-range dependence, and non-Gaussian scaling are\nstylized facts of financial assets dynamics. They are ignored in the Black &\nScholes framework, but have a relevant impact on the pricing of options written\non financial assets. Using a recent model for market dynamics which adequately\ncaptures the above stylized facts, we derive closed form equations for option\npricing, obtaining the Black & Scholes as a special case. By applying our\npricing equations to a major equity index option dataset, we show that\ninclusion of stylized features in financial modeling moves derivative prices\nabout 30% closer to the market values without the need of calibrating models\nparameters on available derivative prices.\n"
    },
    {
        "paper_id": 1307.6332,
        "authors": "Ole E. Barndorff-Nielsen, Fred Espen Benth, Almut E. D. Veraart",
        "title": "Modelling energy spot prices by volatility modulated L\\'{e}vy-driven\n  Volterra processes",
        "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ476 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2013, Vol. 19, No. 3, 803-845",
        "doi": "10.3150/12-BEJ476",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces the class of volatility modulated L\\'{e}vy-driven\nVolterra (VMLV) processes and their important subclass of L\\'{e}vy\nsemistationary (LSS) processes as a new framework for modelling energy spot\nprices. The main modelling idea consists of four principles: First,\ndeseasonalised spot prices can be modelled directly in stationarity. Second,\nstochastic volatility is regarded as a key factor for modelling energy spot\nprices. Third, the model allows for the possibility of jumps and extreme spikes\nand, lastly, it features great flexibility in terms of modelling the\nautocorrelation structure and the Samuelson effect. We provide a detailed\nanalysis of the probabilistic properties of VMLV processes and show how they\ncan capture many stylised facts of energy markets. Further, we derive forward\nprices based on our new spot price models and discuss option pricing. An\nempirical example based on electricity spot prices from the European Energy\nExchange confirms the practical relevance of our new modelling framework.\n"
    },
    {
        "paper_id": 1307.6486,
        "authors": "Cyril Durand and Marek Rutkowski",
        "title": "CVA for Bilateral Counterparty Risk under Alternative Settlement\n  Conventions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We depart from the usual methods for pricing contracts with the counterparty\ncredit risk found in most of the existing literature. In effect, typically,\nthese models do not account for either systemic effects or at-first-default\ncontagion and postulate that the contract value at default equals either the\nrisk-free value or the pre-default value. We propose instead a fairly general\nframework, which allows us to perform effective Credit Value Adjustment (CVA)\ncomputations for a contract with bilateral counterparty risk in the presence of\nsystemic and wrong or right way risks. Our general methodology focuses on the\nrole of alternative settlement clauses, but it is also aimed to cover various\nfeatures of margin agreements. A comparative analysis of numerical results\nreported in the final section supports our initial conjecture that alternative\nspecifications of settlement values have a non-negligible impact on the CVA\ncomputation for contracts with bilateral counterparty risk. This emphasizes the\npractical importance of more sophisticated models that are capable of fully\nreflecting the actual features of financial contracts, as well as the influence\nof the market environment.\n"
    },
    {
        "paper_id": 1307.6695,
        "authors": "Nassim Nicholas Taleb",
        "title": "Where Do Thin Tails Come From?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The literature of heavy tails (typically) starts with a random walk and finds\nmechanisms that lead to fat tails under aggregation. We follow the inverse\nroute and show how starting with fat tails we get to thin-tails when deriving\nthe probability distribution of the response to a random variable. We introduce\na general dose-response curve and argue that the left and right-boundedness or\nsaturation of the response in natural things leads to thin-tails, even when the\n\"underlying\" random variable at the source of the exposure is fat-tailed.\n"
    },
    {
        "paper_id": 1307.6727,
        "authors": "Ovidiu Racorean",
        "title": "Quantum Tunneling of Stock Price in Range Bound Market Conditions",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Applications of Quantum Tunneling effect have long gone beyond the\ntraditional physical meaning. Initially created by Gamow to explain\n{\\alpha}-decay of nuclear particles, along the time, quantum tunneling found\nfertile domain of research in chemistry and recently in biology, where the new\ndiscipline of Quantum Biology emerges. The present paper extends the\napplicability of quantum tunneling to financial markets. In a recent paper [1]\na time-independent equation for pricing the options having the underlying stock\nin a range bound markets is found. The equation is identical with a\ntime-independent Schrodinger equation but incorporates elements of finance. The\nfinancial time-independent equation for option pricing is solved to explain a\nparticular explosive violent movement of stock price in range bound markets.\nThe aforementioned particular stock price movement is assimilated with a\nquantum tunneling effect. The probability of stock price to quantum tunneling\nout of the bounded region, known as transmission coefficient, is deduced.\nQuantum aspects of tunneling effect in financial markets are discussed. Recent\nevidences of price quantum tunneling in stock market are also shown.\n"
    },
    {
        "paper_id": 1307.6974,
        "authors": "Ashadun Nobi, Seong Eun Maeng, Gyeong Gyun Ha, Jae Woo Lee",
        "title": "Network Topologies of Financial Market During the Global Financial\n  Crisis",
        "comments": "19 pages, 8 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We consider the effects of the global financial crisis through a local Korean\nfinancial market around the 2008 crisis. We analyze 185 individual stock prices\nbelonging to the KOSPI (Korea Composite Stock Price Index), cosidering three\ntime periods: the time before, during, and after the crisis. The complex\nnetworks generate from the fully connected correlation network by using the\ncross-correlation coefficients among the stock price time series of the\ncompanies. We generate the threshold networks (TN), the minimal spanning tees\n(MST), and the hierarchical network (HN) from the fully connected\ncross-correlation networks. By assigning a threshold value of the\ncross-correlation coefficient, we obtain the threshold networks. We observe the\npower law of the degree distribution in the limited range of the threshold. The\ndegree distribution of the largest cluster in the threshold networks during the\ncrisis is fatter than other periods. The clustering coefficient of the\nthreshold networks follows the power law in the scaling range. We also generate\nthe minimal spanning trees from the fully connected correlation networks. The\nMST during the crisis period shrinks in comparison to the periods before and\nafter the crisis. The cophenetic correlation coefficient increases during the\ncrisis, indicating that the hierarchical structure increases during this\nperiod. When the crisis hit the market, the companies behave synchronously and\ntheir correlations become stronger than the normal period.\n"
    },
    {
        "paper_id": 1307.707,
        "authors": "Runhuan Feng and Hans W. Volkmer",
        "title": "An identity of hitting times and its application to the valuation of\n  guaranteed minimum withdrawal benefit",
        "comments": "25 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we explore an identity in distribution of hitting times of a\nfinite variation process (Yor's process) and a diffusion process (geometric\nBrownian motion with affine drift), which arise from various applications in\nfinancial mathematics. As a result, we provide analytical solutions to the fair\ncharge of variable annuity guaranteed minimum withdrawal benefit (GMWB) from a\npolicyholder's point of view, which was only previously obtained in the\nliterature by numerical methods. We also use complex inversion methods to\nderive analytical solutions to the fair charge of the GMWB from an insurer's\npoint of view, which is used in the market practice, however, based on Monte\nCarlo simulations. Despite of their seemingly different formulations, we can\nprove under certain assumptions the two pricing approaches are equivalent.\n"
    },
    {
        "paper_id": 1307.7178,
        "authors": "Maya Briani, Lucia Caramellino, Antonino Zanette",
        "title": "A hybrid approach for the implementation of the Heston model",
        "comments": null,
        "journal-ref": "IMA Journal of Management Mathematics 28 4 (2017) 467-500",
        "doi": "10.1093/imaman/dpv032",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a hybrid tree-finite difference method in order to approximate the\nHeston model. We prove the convergence by embedding the procedure in a\nbivariate Markov chain and we study the convergence of European and American\noption prices. We finally provide numerical experiments that give accurate\noption prices in the Heston model, showing the reliability and the efficiency\nof the algorithm.\n"
    },
    {
        "paper_id": 1307.7244,
        "authors": "Lajos Gergely Gyurk\\'o, Terry Lyons, Mark Kontkowski, Jonathan Field",
        "title": "Extracting information from the signature of a financial data stream",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market events such as order placement and order cancellation are examples of\nthe complex and substantial flow of data that surrounds a modern financial\nengineer. New mathematical techniques, developed to describe the interactions\nof complex oscillatory systems (known as the theory of rough paths) provides\nnew tools for analysing and describing these data streams and extracting the\nvital information. In this paper we illustrate how a very small number of\ncoefficients obtained from the signature of financial data can be sufficient to\nclassify this data for subtle underlying features and make useful predictions.\n  This paper presents financial examples in which we learn from data and then\nproceed to classify fresh streams. The classification is based on features of\nstreams that are specified through the coordinates of the signature of the\npath. At a mathematical level the signature is a faithful transform of a\nmultidimensional time series. (Ben Hambly and Terry Lyons \\cite{uniqueSig}),\nHao Ni and Terry Lyons \\cite{NiLyons} introduced the possibility of its use to\nunderstand financial data and pointed to the potential this approach has for\nmachine learning and prediction.\n  We evaluate and refine these theoretical suggestions against practical\nexamples of interest and present a few motivating experiments which demonstrate\ninformation the signature can easily capture in a non-parametric way avoiding\ntraditional statistical modelling of the data. In the first experiment we\nidentify atypical market behaviour across standard 30-minute time buckets\nsampled from the WTI crude oil future market (NYMEX). The second and third\nexperiments aim to characterise the market \"impact\" of and distinguish between\nparent orders generated by two different trade execution algorithms on the FTSE\n100 Index futures market listed on NYSE Liffe.\n"
    },
    {
        "paper_id": 1307.802,
        "authors": "Helena Aro",
        "title": "Systematic and non-systematic mortality risk in pension portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the effects of non-systematic and systematic mortality risks on the\nrequired initial capital in a pension plan, in the presence of financial risks.\nWe discover that for a pension plan with few members the impact of pooling on\nthe required capital per person is strong, but non-systematic risk diminishes\nrapidly as the number of members increases. Systematic mortality risk, on the\nother hand, is a significant source of risk is a pension portfolio.\n"
    },
    {
        "paper_id": 1307.8261,
        "authors": "Helena Aro and Teemu Pennanen",
        "title": "Liability-driven investment in longevity risk management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies optimal investment from the point of view of an investor\nwith longevity-linked liabilities. The relevant optimization problems rarely\nare analytically tractable, but we are able to show numerically that liability\ndriven investment can significantly outperform common strategies that do not\ntake the liabilities into account. In problems without liabilities the\nadvantage disappears, which suggests that the superiority of the proposed\nstrategies is indeed based on connections between liabilities and asset\nreturns.\n"
    },
    {
        "paper_id": 1307.8308,
        "authors": "Y. Shi, A. N. Gorban, T. Y. Yang",
        "title": "Is it possible to predict long-term success with k-NN? Case Study of\n  four market indices (FTSE100, DAX, HANGSENG, NASDAQ)",
        "comments": "21 pages, 14 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-6596/490/1/012082",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  This case study tests the possibility of prediction for \"success\" (or\n\"winner\") components of four stock & shares market indices in a time period of\nthree years from 02-Jul-2009 to 29-Jun-2012.We compare their performance ain\ntwo time frames: initial frame three months at the beginning\n(02/06/2009-30/09/2009) and the final three month frame\n(02/04/2012-29/06/2012). To label the components, average price ratio between\ntwo time frames in descending order is computed. The average price ratio is\ndefined as the ratio between the mean prices of the beginning and final time\nperiod. The \"winner\" components are referred to the top one third of total\ncomponents in the same order as average price ratio it means the mean price of\nfinal time period is relatively higher than the beginning time period. The\n\"loser\" components are referred to the last one third of total components in\nthe same order as they have higher mean prices of beginning time period. We\nanalyse, is there any information about the winner-looser separation in the\ninitial fragments of the daily closing prices log-returns time series. The\nLeave-One-Out Cross-Validation with k-NN algorithm is applied on the daily\nlog-return of components using a distance and proximity in the experiment. By\nlooking at the error analysis, it shows that for HANGSENG and DAX index, there\nare clear signs of possibility to evaluate the probability of long-term\nsuccess. The correlation distance matrix histograms and 2-D/3-D elastic maps\ngenerated from ViDaExpert show that the winner components are closer to each\nother and winner/loser components are separable on elastic maps for HANGSENG\nand DAX index while for the negative possibility indices, there is no sign of\nseparation.\n"
    },
    {
        "paper_id": 1308.021,
        "authors": "Jozef Barunik and Evzen Kocenda and Lukas Vacha",
        "title": "Gold, Oil, and Stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employ a wavelet approach and conduct a time-frequency analysis of dynamic\ncorrelations between pairs of key traded assets (gold, oil, and stocks)\ncovering the period from 1987 to 2012. The analysis is performed on both\nintra-day and daily data. We show that heterogeneity in correlations across a\nnumber of investment horizons between pairs of assets is a dominant feature\nduring times of economic downturn and financial turbulence for all three pairs\nof the assets under research. Heterogeneity prevails in correlations between\ngold and stocks. After the 2008 crisis, correlations among all three assets\nincrease and become homogenous: the timing differs for the three pairs but\ncoincides with the structural breaks that are identified in specific\ncorrelation dynamics. A strong implication emerges: during the period under\nresearch, and from a different-investment-horizons perspective, all three\nassets could be used in a well-diversified portfolio only during relatively\nshort periods.\n"
    },
    {
        "paper_id": 1308.0526,
        "authors": "Riccardo Chiarucci, Franco Ruzzenenti, Maria I. Loffredo",
        "title": "Detecting spatial homogeneity in the world trade web with Detrended\n  Fluctuation Analysis",
        "comments": "15 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.01.019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a spatially embedded network, that is a network where nodes can be\nuniquely determined in a system of coordinates, links' weights might be\naffected by metric distances coupling every pair of nodes (dyads). In order to\nassess to what extent metric distances affect relationships (link's weights) in\na spatially embedded network, we propose a methodology based on DFA (Detrended\nFluctuation Analysis). DFA is a well developed methodology to evaluate\nautocorrelations and estimate long-range behaviour in time series. We argue it\ncan be further extended to spatially ordered series in order to assess\nautocorrelations in values. A scaling exponent of 0.5 (uncorrelated data) would\nthereby signal a perfect homogeneous space embedding the network. We apply the\nproposed methodology to the World Trade Web (WTW) during the years 1949-2000\nand we find, in some contrast with predictions of gravity models, a declining\ninfluence of distances on trading relationships.\n"
    },
    {
        "paper_id": 1308.0652,
        "authors": "Teruyoshi Kobayashi, Kohei Hasui",
        "title": "Efficient immunization strategies to prevent financial contagion",
        "comments": "16 pages, 6 figures",
        "journal-ref": "Scientific Reports 4, 3834, 2014",
        "doi": "10.1038/srep03834",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many immunization strategies have been proposed to prevent infectious viruses\nfrom spreading through a network. In this study, we propose efficient\nimmunization strategies to prevent a default contagion that might occur in a\nfinancial network. An essential difference from the previous studies on\nimmunization strategy is that we take into account the possibility of serious\nside effects. Uniform immunization refers to a situation in which banks are\n\"vaccinated\" with a common low-risk asset. The riskiness of immunized banks\nwill decrease significantly, but the level of systemic risk may increase due to\nthe de-diversification effect. To overcome this side effect, we propose another\nimmunization strategy, counteractive immunization, which prevents pairs of\nbanks from failing simultaneously. We find that counteractive immunization can\nefficiently reduce systemic risk without altering the riskiness of individual\nbanks.\n"
    },
    {
        "paper_id": 1308.0665,
        "authors": "Hyukjae Park",
        "title": "Efficient valuation method for the SABR model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we show how the scaling symmetry of the SABR model can be\nutilized to efficiently price European options. For special kinds of payoffs,\nthe complexity of the problem is reduced by one dimension. For more generic\npayoffs, instead of solving the 1+2 dimensional SABR PDE, it is sufficient to\nsolve $N_V$ uncoupled 1+1 dimensional PDE's, where $N_V$ is the number of\npoints used to discretize one dimension. Furthermore, the symmetry argument\nenables us to obtain prices of multiple options, whose payoffs are related to\neach other by convolutions, by valuing one of them. The results of the method\nare compared with the Monte Carlo simulation.\n"
    },
    {
        "paper_id": 1308.0669,
        "authors": "X.F. Jiang, T.T. Chen, and B. Zheng",
        "title": "Time-reversal asymmetry in financial systems",
        "comments": "17 pages, 8 figures, Accepted by Physica A. arXiv admin note:\n  substantial text overlap with arXiv:1002.3747",
        "journal-ref": "Physica A, Volume 392, Issue 21, 1 November 2013, Pages 5369-5375",
        "doi": "10.1016/j.physa.2013.07.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the large-fluctuation dynamics in financial markets, based on\nthe minute-to-minute and daily data of the Chinese Indices and German DAX. The\ndynamic relaxation both before and after the large fluctuations is\ncharacterized by a power law, and the exponents $p_\\pm$ usually vary with the\nstrength of the large fluctuations. The large-fluctuation dynamics is\ntime-reversal symmetric at the time scale in minutes, while asymmetric at the\ndaily time scale. Careful analysis reveals that the time-reversal asymmetry is\nmainly induced by external forces. It is also the external forces which drive\nthe financial system to a non-stationary state. Different characteristics of\nthe Chinese and German stock markets are uncovered.\n"
    },
    {
        "paper_id": 1308.0773,
        "authors": "Teruyoshi Kobayashi",
        "title": "Network versus portfolio structure in financial systems",
        "comments": null,
        "journal-ref": "European Physical Journal B, 86 10 (2013) 434",
        "doi": "10.1140/epjb/e2013-40072-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The question of how to stabilize financial systems has attracted considerable\nattention since the global financial crisis of 2007-2009. Recently, Beale et\nal. (\"Individual versus systemic risk and the regulator's dilemma\", Proc Natl\nAcad Sci USA 108: 12647-12652, 2011) demonstrated that higher portfolio\ndiversity among banks would reduce systemic risk by decreasing the risk of\nsimultaneous defaults at the expense of a higher likelihood of individual\ndefaults. In practice, however, a bank default has an externality in that it\nundermines other banks' balance sheets. This paper explores how each of these\ndifferent sources of risk, simultaneity risk and externality, contributes to\nsystemic risk. The results show that the allocation of external assets that\nminimizes systemic risk varies with the topology of the financial network as\nlong as asset returns have negative correlations. In the model, a well-known\ncentrality measure, PageRank, reflects an appropriately defined \"infectiveness\"\nof a bank. An important result is that the most infective bank need not always\nbe the safest bank. Under certain circumstances, the most infective node should\nact as a firewall to prevent large-scale collective defaults. The introduction\nof a counteractive portfolio structure will significantly reduce systemic risk.\n"
    },
    {
        "paper_id": 1308.0889,
        "authors": "Silvia Angilella and Sebastiano Mazz\\`u",
        "title": "The Financing of Innovative SMEs: a multicriteria credit rating model",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Small Medium-sized Enterprises (SMEs) face many obstacles when they try to\naccess credit market. These obstacles are increased if the SMEs are innovative.\nIn this case, financial data are insufficient or even not reliable. Thus, when\nbuilding a judgemental rating model, mainly based on qualitative criteria (soft\ninformation), it is very important to finance SMEs' activities. Until now,\nthere isn't a multicriteria credit risk model based on soft information for\ninnovative SMEs. In this paper, we try to fill this gap by presenting a\nmulticriteria credit risk model, specifically, ELECTRE-TRI. To obtain robust\nSMEs' assignments to the risk classes, a SMAA-TRI analysis is also implemented.\nIn fact, SMAA-TRI incorporates ELECTRE-TRI by considering different sets of\npreference parameters with Monte Carlo simulations. Finally, we carry out some\nreal case studies, with the aim of illustrating the multicriteria credit risk\nmodel proposed.\n"
    },
    {
        "paper_id": 1308.0925,
        "authors": "Ming-Xia Li (ECUST), Zhi-Qiang Jiang (ECUST), Wen-Jie Xie (ECUST),\n  Xiong Xiong (TJU), Wei Zhang (TJU), Wei-Xing Zhou (ECUST)",
        "title": "Unveiling correlations between financial variables and topological\n  metrics of trading networks: Evidence from a stock and its warrant",
        "comments": "3 tables and 5 pages",
        "journal-ref": "Physica A 419, 575-584 (2015)",
        "doi": "10.1016/j.physa.2014.10.039",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traders adopt different trading strategies to maximize their returns in\nfinancial markets. These trading strategies not only results in specific\ntopological structures in trading networks, which connect the traders with the\npairwise buy-sell relationships, but also have potential impacts on market\ndynamics. Here, we present a detailed analysis on how the market behaviors are\ncorrelated with the structures of traders in trading networks based on audit\ntrail data for the Baosteel stock and its warrant at the transaction level from\n22 August 2005 to 23 August 2006. In our investigation, we divide each trade\nday into 48 time windows with a length of five minutes, construct a trading\nnetwork within each window, and obtain a time series of over 1,100 trading\nnetworks. We find that there are strongly simultaneous correlations between the\ntopological metrics (including network centralization, assortative index, and\naverage path length) of trading networks that characterize the patterns of\norder execution and the financial variables (including return, volatility,\nintertrade duration, and trading volume) for the stock and its warrant. Our\nanalysis may shed new lights on how the microscopic interactions between\nelements within complex system affect the system's performance.\n"
    },
    {
        "paper_id": 1308.0931,
        "authors": "Taras Bodnar, Arjun K. Gupta and Nestor Parolya",
        "title": "Optimal Linear Shrinkage Estimator for Large Dimensional Precision\n  Matrix",
        "comments": "26 pages, 5 figures. This version includes the case c>1 with the\n  generalized inverse of the sample covariance matrix. The abstract was updated\n  accordingly",
        "journal-ref": "Journal of Multivariate Analysis, Volume 146, 2016, 223-236",
        "doi": "10.1016/j.jmva.2015.09.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we construct an optimal shrinkage estimator for the precision\nmatrix in high dimensions. We consider the general asymptotics when the number\nof variables $p\\rightarrow\\infty$ and the sample size $n\\rightarrow\\infty$ so\nthat $p/n\\rightarrow c\\in (0, +\\infty)$. The precision matrix is estimated\ndirectly, without inverting the corresponding estimator for the covariance\nmatrix. The recent results from the random matrix theory allow us to find the\nasymptotic deterministic equivalents of the optimal shrinkage intensities and\nestimate them consistently. The resulting distribution-free estimator has\nalmost surely the minimum Frobenius loss. Additionally, we prove that the\nFrobenius norms of the inverse and of the pseudo-inverse sample covariance\nmatrices tend almost surely to deterministic quantities and estimate them\nconsistently. At the end, a simulation is provided where the suggested\nestimator is compared with the estimators for the precision matrix proposed in\nthe literature. The optimal shrinkage estimator shows significant improvement\nand robustness even for non-normally distributed data.\n"
    },
    {
        "paper_id": 1308.0958,
        "authors": "Nassim N. Taleb and Constantine Sandis",
        "title": "The Skin In The Game Heuristic for Protection Against Tail Events",
        "comments": null,
        "journal-ref": "Review of Behavioral Economics, Jan 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Standard economic theory makes an allowance for the agency problem, but not\nthe compounding of moral hazard in the presence of informational opacity,\nparticularly in what concerns high-impact events in fat tailed domains (under\nslow convergence for the law of large numbers). Nor did it look at exposure as\na filter that removes nefarious risk takers from the system so they stop\nharming others. \\textcolor{red}{ (In the language of probability, skin in the\ngame creates an absorbing state for the agent, not just the principal)}. But\nthe ancients did; so did many aspects of moral philosophy. We propose a global\nand morally mandatory heuristic that anyone involved in an action which can\npossibly generate harm for others, even probabilistically, should be required\nto be exposed to some damage, regardless of context. While perhaps not\nsufficient, the heuristic is certainly necessary hence mandatory. It is\nsupposed to counter voluntary and involuntary risk hiding$-$ and risk transfer\n$-$ in the tails. We link the rule to various philosophical approaches to\nethics and moral luck.\n"
    },
    {
        "paper_id": 1308.1154,
        "authors": "Fei Ren and Wei-Xing Zhou",
        "title": "Dynamic evolution of cross-correlations in the Chinese stock market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0097711",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the dynamic evolution of cross-correlations in the Chinese stock\nmarket mainly based on the random matrix theory (RMT). The correlation matrices\nconstructed from the return series of 367 A-share stocks traded on the Shanghai\nStock Exchange from January 4, 1999 to December 30, 2011 are calculated over a\nmoving window with a size of 400 days. The evolutions of the statistical\nproperties of the correlation coefficients, eigenvalues, and eigenvectors of\nthe correlation matrices are carefully analyzed. We find that the stock\ncorrelations are significantly increased in the periods of two market crashes\nin 2001 and 2008, during which only five eigenvalues significantly deviate from\nthe random correlation matrix, and the systemic risk is higher in these\nvolatile periods than calm periods. By investigating the significant\ncontributors of the deviating eigenvectors in different moving windows, we\nobserve a dynamic evolution behavior in business sectors such as IT,\nelectronics, and real estate, which lead the rise (drop) before (after) the\ncrashes.\n"
    },
    {
        "paper_id": 1308.1221,
        "authors": "Jozef Barunik and Evzen Kocenda and Lukas Vacha",
        "title": "Asymmetric connectedness of stocks: How does bad and good volatility\n  spill over the U.S. stock market?",
        "comments": "arXiv admin note: text overlap with arXiv:1405.2445",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Asymmetries in volatility spillovers are highly relevant to risk valuation\nand portfolio diversification strategies in financial markets. Yet, the large\nliterature studying information transmission mechanisms ignores the fact that\nbad and good volatility may spill over at different magnitudes. This paper\nfills this gap with two contributions. One, we suggest how to quantify\nasymmetries in volatility spillovers due to bad and good volatility. Two, using\nhigh frequency data covering most liquid U.S. stocks in seven sectors, we\nprovide ample evidence of the asymmetric connectedness of stocks. We\nuniversally reject the hypothesis of symmetric connectedness at the\ndisaggregate level but in contrast, we document the symmetric transmission of\ninformation in an aggregated portfolio. We show that bad and good volatility is\ntransmitted at different magnitudes in different sectors, and the asymmetries\nsizably change over time. While negative spillovers are often of substantial\nmagnitudes, they do not strictly dominate positive spillovers. We find that the\noverall intra-market connectedness of U.S. stocks increased substantially with\nthe increased uncertainty of stock market participants during the financial\ncrisis.\n"
    },
    {
        "paper_id": 1308.1321,
        "authors": "Zaiwen Wen, Xianhua Peng, Xin Liu, Xiaoling Sun and Xiaodi Bai",
        "title": "Asset Allocation under the Basel Accord Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial institutions are currently required to meet more stringent capital\nrequirements than they were before the recent financial crisis; in particular,\nthe capital requirement for a large bank's trading book under the Basel 2.5\nAccord more than doubles that under the Basel II Accord. The significant\nincrease in capital requirements renders it necessary for banks to take into\naccount the constraint of capital requirement when they make asset allocation\ndecisions. In this paper, we propose a new asset allocation model that\nincorporates the regulatory capital requirements under both the Basel 2.5\nAccord, which is currently in effect, and the Basel III Accord, which was\nrecently proposed and is currently under discussion. We propose an unified\nalgorithm based on the alternating direction augmented Lagrangian method to\nsolve the model; we also establish the first-order optimality of the limit\npoints of the sequence generated by the algorithm under some mild conditions.\nThe algorithm is simple and easy to implement; each step of the algorithm\nconsists of solving convex quadratic programming or one-dimensional\nsubproblems. Numerical experiments on simulated and real market data show that\nthe algorithm compares favorably with other existing methods, especially in\ncases in which the model is non-convex.\n"
    },
    {
        "paper_id": 1308.1492,
        "authors": "Walter Schachermayer",
        "title": "Admissible Trading Strategies under Transaction Costs",
        "comments": "Paper has been expanded by inserting section 2 The num\\'eraire-free\n  setting",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A well known result in stochastic analysis reads as follows: for an\n$\\mathbb{R}$-valued super-martingale $X = (X_t)_{0\\leq t \\leq T}$ such that the\nterminal value $X_T$ is non-negative, we have that the entire process $X$ is\nnon-negative. An analogous result holds true in the no arbitrage theory of\nmathematical finance: under the assumption of no arbitrage, a portfolio process\n$x+(H\\cdot S)$ verifying $x+(H\\cdot S)_T\\geq 0$ also satisfies $x+(H\\cdot\nS)_t\\geq 0,$ for all $0 \\leq t \\leq T$.\n  In the present paper we derive an analogous result in the presence of\ntransaction costs. A counter-example reveals that the consideration of\ntransaction costs makes things more delicate than in the frictionless setting.\n"
    },
    {
        "paper_id": 1308.1616,
        "authors": "Varsha S. Kulkarni",
        "title": "Complexity, Chaos, and the Duffing-Oscillator Model: An Analysis of\n  Inventory Fluctuations in Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Apparently random financial fluctuations often exhibit varying levels of\ncomplexity, chaos. Given limited data, predictability of such time series\nbecomes hard to infer. While efficient methods of Lyapunov exponent computation\nare devised, knowledge about the process driving the dynamics greatly\nfacilitates the complexity analysis. This paper shows that quarterly inventory\nchanges of wheat in the global market, during 1974-2012, follow a nonlinear\ndeterministic process. Lyapunov exponents of these fluctuations are computed\nusing sliding time windows each of length 131 quarters. Weakly chaotic behavior\nalternates with non-chaotic behavior over the entire period of analysis. More\nimportantly, in this paper, a cubic dependence of price changes on inventory\nchanges leads to establishment of deterministic Duffing-Oscillator-Model(DOM)\nas a suitable candidate for examining inventory fluctuations of wheat. DOM\nrepresents the interaction of commodity production cycle with an external\nintervention in the market. Parameters obtained for shifting time zones by\nfitting the Fourier estimated time signals to DOM are able to generate\nresponses that reproduce the true chaotic nature exhibited by the empirical\nsignal at that time. Endowing the parameters with suitable meanings, one may\ninfer that temporary changes in speculation reflect the pattern of inventory\nvolatility that drives the transitions between chaotic and non-chaotic\nbehavior.\n"
    },
    {
        "paper_id": 1308.1704,
        "authors": "Dorival Le\\~ao, Alberto Ohashi and Vinicius Siqueira",
        "title": "A general Multidimensional Monte Carlo Approach for Dynamic Hedging\n  under stochastic volatility",
        "comments": "Some typos are corrected in Section 6",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we introduce a Monte Carlo method for the dynamic hedging of\ngeneral European-type contingent claims in a multidimensional Brownian\narbitrage-free market. Based on bounded variation martingale approximations for\nGaltchouk-Kunita-Watanabe decompositions, we propose a feasible and\nconstructive methodology which allows us to compute pure hedging strategies\nw.r.t arbitrary square-integrable claims in incomplete markets. In particular,\nthe methodology can be applied to quadratic hedging-type strategies for fully\npath-dependent options with stochastic volatility and discontinuous payoffs. We\nillustrate the method with numerical examples based on generalized\nFollmer-Schweizer decompositions, locally-risk minimizing and mean-variance\nhedging strategies for vanilla and path-dependent options written on local\nvolatility and stochastic volatility models.\n"
    },
    {
        "paper_id": 1308.1749,
        "authors": "Il Gu Yi, Gabjin Oh, and Beom Jun Kim",
        "title": "Fractality of profit landscapes and validation of time series models for\n  stock prices",
        "comments": "10pages, 6figures",
        "journal-ref": "Eur. Phys. J. B (2013) 86, 349",
        "doi": "10.1140/epjb/e2013-31116-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply a simple trading strategy for various time series of real and\nartificial stock prices to understand the origin of fractality observed in the\nresulting profit landscapes. The strategy contains only two parameters $p$ and\n$q$, and the sell (buy) decision is made when the log return is larger\n(smaller) than $p$ ($-q$). We discretize the unit square $(p, q) \\in [0, 1]\n\\times [0, 1]$ into the $N \\times N$ square grid and the profit $\\Pi (p, q)$ is\ncalculated at the center of each cell. We confirm the previous finding that\nlocal maxima in profit landscapes are scattered in a fractal-like fashion: The\nnumber M of local maxima follows the power-law form $M \\sim N^{a}$, but the\nscaling exponent $a$ is found to differ for different time series. From\ncomparisons of real and artificial stock prices, we find that the fat-tailed\nreturn distribution is closely related to the exponent $a \\approx 1.6$ observed\nfor real stock markets. We suggest that the fractality of profit landscape\ncharacterized by $a \\approx 1.6$ can be a useful measure to validate time\nseries model for stock prices.\n"
    },
    {
        "paper_id": 1308.2172,
        "authors": "Rene Carmona, Jean-Pierre Fouque, Li-Hsien Sun",
        "title": "Mean Field Games and Systemic Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple model of inter-bank borrowing and lending where the\nevolution of the log-monetary reserves of $N$ banks is described by a system of\ndiffusion processes coupled through their drifts in such a way that stability\nof the system depends on the rate of inter-bank borrowing and lending. Systemic\nrisk is characterized by a large number of banks reaching a default threshold\nby a given time horizon.\n  Our model incorporates a game feature where each bank controls its rate of\nborrowing/lending to a central bank. The optimization reflects the desire of\neach bank to borrow from the central bank when its monetary reserve falls below\na critical level or lend if it rises above this critical level which is chosen\nhere as the average monetary reserve. Borrowing from or lending to the central\nbank is also subject to a quadratic cost at a rate which can be fixed by the\nregulator. We solve explicitly for Nash equilibria with finitely many players,\nand we show that in this model the central bank acts as a clearing house,\nadding liquidity to the system without affecting its systemic risk. We also\nstudy the corresponding Mean Field Game in the limit of large number of banks\nin the presence of a common noise.\n"
    },
    {
        "paper_id": 1308.2191,
        "authors": "Asim Ghosh",
        "title": "Econophysics Research in India in the last two Decades",
        "comments": "To be published in \"IIM Kozhikode Society & Management Review\", Sage\n  publication (USA), Vol. 2 Issue 2 (2013) [Special Issue on Econophysics,\n  Guest Eds: K. Gangopadhyay and K. Guhathakurta]",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss here researches on econophysics done from India in the last two\ndecades. The term `econophysics' was formally coined in India (Kolkata) in\n1995. Since then many research papers, books, reviews, etc. have been written\nby scientists. Many institutions are now involved in this research field and\nmany conferences are being organized there. In this article we give an account\n(of papers, books, reviews, papers in proceedings volumes etc.) of this\nresearch from India.\n"
    },
    {
        "paper_id": 1308.225,
        "authors": "Erhan Bayraktar, Sergey Nadtochiy",
        "title": "Weak reflection principle for L\\'evy processes",
        "comments": "Published at http://dx.doi.org/10.1214/14-AAP1073 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 6, 3251-3294",
        "doi": "10.1214/14-AAP1073",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a new mathematical technique which allows us to\nexpress the joint distribution of a Markov process and its running maximum (or\nminimum) through the marginal distribution of the process itself. This\ntechnique is an extension of the classical reflection principle for Brownian\nmotion, and it is obtained by weakening the assumptions of symmetry required\nfor the classical reflection principle to work. We call this method a weak\nreflection principle and show that it provides solutions to many problems for\nwhich the classical reflection principle is typically used. In addition, unlike\nthe classical reflection principle, the new method works for a much larger\nclass of stochastic processes which, in particular, do not possess any strong\nsymmetries. Here, we review the existing results which establish the weak\nreflection principle for a large class of time-homogeneous diffusions on a real\nline and then proceed to extend this method to the L\\'{e}vy processes with\none-sided jumps (subject to some admissibility conditions). Finally, we\ndemonstrate the applications of the weak reflection principle in financial\nmathematics, computational methods and inverse problems.\n"
    },
    {
        "paper_id": 1308.2254,
        "authors": "Sergey Nadtochiy and Michael Tehranchi",
        "title": "Optimal investment for all time horizons and Martin boundary of\n  space-time diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with the axiomatic foundation and explicit\nconstruction of a general class of optimality criteria that can be used for\ninvestment problems with multiple time horizons, or when the time horizon is\nnot known in advance. Both the investment criterion and the optimal strategy\nare characterized by the Hamilton-Jacobi-Bellman equation on a semi-infinite\ntime interval. In the case when this equation can be linearized, the problem\nreduces to a time-reversed parabolic equation, which cannot be analyzed via the\nstandard methods of partial differential equations. Under the additional\nuniform ellipticity condition, we make use of the available description of all\nminimal solutions to such equations, along with some basic facts from potential\ntheory and convex analysis, to obtain an explicit integral representation of\nall positive solutions. These results allow us to construct a large family of\nthe aforementioned optimality criteria, including some closed form examples in\nrelevant financial models.\n"
    },
    {
        "paper_id": 1308.2324,
        "authors": "Jing Li and Mingxin Xu",
        "title": "Optimal Dynamic Portfolio with Mean-CVaR Criterion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR) are popular risk\nmeasures from academic, industrial and regulatory perspectives. The problem of\nminimizing CVaR is theoretically known to be of Neyman-Pearson type binary\nsolution. We add a constraint on expected return to investigate the Mean-CVaR\nportfolio selection problem in a dynamic setting: the investor is faced with a\nMarkowitz type of risk reward problem at final horizon where variance as a\nmeasure of risk is replaced by CVaR. Based on the complete market assumption,\nwe give an analytical solution in general. The novelty of our solution is that\nit is no longer Neyman-Pearson type where the final optimal portfolio takes\nonly two values. Instead, in the case where the portfolio value is required to\nbe bounded from above, the optimal solution takes three values; while in the\ncase where there is no upper bound, the optimal investment portfolio does not\nexist, though a three-level portfolio still provides a sub-optimal solution.\n"
    },
    {
        "paper_id": 1308.2326,
        "authors": "Peter Carr and Sergey Nadtochiy",
        "title": "Local Variance Gamma and Explicit Calibration to Option Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In some options markets (e.g. commodities), options are listed with only a\nsingle maturity for each underlying. In others, (e.g. equities, currencies),\noptions are listed with multiple maturities. In this paper, we provide an\nalgorithm for calibrating a pure jump Markov martingale model to match the\nmarket prices of European options of multiple strikes and maturities. This\nalgorithm only requires solutions of several one-dimensional root-search\nproblems, as well as application of elementary functions. We show how to\nconstruct a time-homogeneous process which meets a single smile, and a\npiecewise time-homogeneous process which can meet multiple smiles.\n"
    },
    {
        "paper_id": 1308.2572,
        "authors": "A. K. Bahl, O. Baltzer, A. Rau-Chaplin, B. Varghese and A. Whiteway",
        "title": "Achieving Speedup in Aggregate Risk Analysis using Multiple GPUs",
        "comments": "Workshop Proceedings of International Conference on Parallel\n  Processing, Lyon, France, 2013, 8 pages. arXiv admin note: text overlap with\n  arXiv:1308.2066",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic simulation techniques employed for the analysis of portfolios of\ninsurance/reinsurance risk, often referred to as `Aggregate Risk Analysis', can\nbenefit from exploiting state-of-the-art high-performance computing platforms.\nIn this paper, parallel methods to speed-up aggregate risk analysis for\nsupporting real-time pricing are explored. An algorithm for analysing aggregate\nrisk is proposed and implemented for multi-core CPUs and for many-core GPUs.\nExperimental studies indicate that GPUs offer a feasible alternative solution\nover traditional high-performance computing systems. A simulation of 1,000,000\ntrials with 1,000 catastrophic events per trial on a typical exposure set and\ncontract structure is performed in less than 5 seconds on a multiple GPU\nplatform. The key result is that the multiple GPU implementation can be used in\nreal-time pricing scenarios as it is approximately 77x times faster than the\nsequential counterpart implemented on a CPU.\n"
    },
    {
        "paper_id": 1308.2608,
        "authors": "Taras Bodnar, Arjun K. Gupta and Nestor Parolya",
        "title": "On the Strong Convergence of the Optimal Linear Shrinkage Estimator for\n  Large Dimensional Covariance Matrix",
        "comments": "21 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1308.0931, revised version (Journal of Multivariate Analysis)",
        "journal-ref": "Journal of Multivariate Analysis, Volume 132, 2014, pp. 215-228",
        "doi": "10.1016/j.jmva.2014.08.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we construct an optimal linear shrinkage estimator for the\ncovariance matrix in high dimensions. The recent results from the random matrix\ntheory allow us to find the asymptotic deterministic equivalents of the optimal\nshrinkage intensities and estimate them consistently. The developed\ndistribution-free estimators obey almost surely the smallest Frobenius loss\nover all linear shrinkage estimators for the covariance matrix. The case we\nconsider includes the number of variables $p\\rightarrow\\infty$ and the sample\nsize $n\\rightarrow\\infty$ so that $p/n\\rightarrow c\\in (0, +\\infty)$.\nAdditionally, we prove that the Frobenius norm of the sample covariance matrix\ntends almost surely to a deterministic quantity which can be consistently\nestimated.\n"
    },
    {
        "paper_id": 1308.2688,
        "authors": "Alet Roux and Tomasz Zastawniak",
        "title": "American options with gradual exercise under proportional transaction\n  costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  American options in a multi-asset market model with proportional transaction\ncosts are studied in the case when the holder of an option is able to exercise\nit gradually at a so-called mixed (randomised) stopping time. The introduction\nof gradual exercise leads to tighter bounds on the option price when compared\nto the case studied in the existing literature, where the standard assumption\nis that the option can only be exercised instantly at an ordinary stopping\ntime. Algorithmic constructions for the bid and ask prices and the associated\nsuperhedging strategies and optimal mixed stoping times for an American option\nwith gradual exercise are developed and implemented, and dual representations\nare established.\n"
    },
    {
        "paper_id": 1308.2732,
        "authors": "Igor Borovikov and Michael Sadovsky",
        "title": "A relative information approach to financial time series analysis using\n  binary $N$-grams dictionaries",
        "comments": "13 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we present a novel approach to statistical analysis of financial time\nseries. The approach is based on $n$-grams frequency dictionaries derived from\nthe quantized market data. Such dictionaries are studied by evaluating their\ninformation capacity using relative entropy. A specific quantization of\n(originally continuous) financial data is considered: so called binary\nquantization. Possible applications of the proposed technique include market\nevent study with the $n$-grams of higher information value. The finite length\nof the input data presents certain computational and theoretical challenges\ndiscussed in the paper. also, some other versions of a quantization are\ndiscussed.\n"
    },
    {
        "paper_id": 1308.2836,
        "authors": "Susanne M. Schennach",
        "title": "Regressions with Berkson errors in covariates - A nonparametric approach",
        "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1122 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 3, 1642-1668",
        "doi": "10.1214/13-AOS1122",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper establishes that so-called instrumental variables enable the\nidentification and the estimation of a fully nonparametric regression model\nwith Berkson-type measurement error in the regressors. An estimator is proposed\nand proven to be consistent. Its practical performance and feasibility are\ninvestigated via Monte Carlo simulations as well as through an epidemiological\napplication investigating the effect of particulate air pollution on\nrespiratory health. These examples illustrate that Berkson errors can clearly\nnot be neglected in nonlinear regression models and that the proposed method\nrepresents an effective remedy.\n"
    },
    {
        "paper_id": 1308.2957,
        "authors": "Alain B\\'elanger, Gaston Giroux, Miguel Moisan-Poisson",
        "title": "Over-the-counter market models with several assets",
        "comments": "39 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study two classes of over-the-counter markets specified by systems of\nODE's, in the spirit of Duffie-Garleanu-Pedersen, Econometrica, 2005. We first\ncompute the steady states for many of these ODE's. Then we obtain the prices at\nwhich investors trade with each other at these steady states. Finally, we study\nthe stability of the solutions of these ODE's.\n"
    },
    {
        "paper_id": 1308.3331,
        "authors": "Walter Farkas, Pablo Koch-Medina, Cosimo Munari",
        "title": "Measuring risk with multiple eligible assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The risk of financial positions is measured by the minimum amount of capital\nto raise and invest in eligible portfolios of traded assets in order to meet a\nprescribed acceptability constraint. We investigate nondegeneracy, finiteness\nand continuity properties of these risk measures with respect to multiple\neligible assets. Our finiteness and continuity results highlight the interplay\nbetween the acceptance set and the class of eligible portfolios. We present a\nsimple, alternative approach to the dual representation of convex risk measures\nby directly applying to the acceptance set the external characterization of\nclosed, convex sets. We prove that risk measures are nondegenerate if and only\nif the pricing functional admits a positive extension which is a supporting\nfunctional for the underlying acceptance set, and provide a characterization of\nwhen such extensions exist. Finally, we discuss applications to set-valued risk\nmeasures, superhedging with shortfall risk, and optimal risk sharing.\n"
    },
    {
        "paper_id": 1308.3378,
        "authors": "Fred Espen Benth and Salvador Ortiz-Latorre",
        "title": "A pricing measure to explain the risk premium in power markets",
        "comments": "37 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In electricity markets, it is sensible to use a two-factor model with mean\nreversion for spot prices. One of the factors is an Ornstein-Uhlenbeck (OU)\nprocess driven by a Brownian motion and accounts for the small variations. The\nother factor is an OU process driven by a pure jump L\\'evy process and models\nthe characteristic spikes observed in such markets. When it comes to pricing, a\npopular choice of pricing measure is given by the Esscher transform that\npreserves the probabilistic structure of the driving L\\'evy processes, while\nchanging the levels of mean reversion. Using this choice one can generate\nstochastic risk premiums (in geometric spot models) but with\n(deterministically) changing sign. In this paper we introduce a pricing change\nof measure, which is an extension of the Esscher transform. With this new\nchange of measure we also can slow down the speed of mean reversion and\ngenerate stochastic risk premiums with stochastic non constant sign, even in\narithmetic spot models. In particular, we can generate risk profiles with\npositive values in the short end of the forward curve and negative values in\nthe long end. Finally, our pricing measure allows us to have a stationary spot\ndynamics while still having randomly fluctuating forward prices for contracts\nfar from maturity.\n"
    },
    {
        "paper_id": 1308.3668,
        "authors": "A. Morozovskiy, A.A. Snarskii, I.V. Bezsudnov, V.A. Sevryukov, J.\n  Malinsky",
        "title": "Kinetic properties in inhomogeneous self-aware media",
        "comments": "Chapter 23 of the book \"Transport processes in macroscopically\n  disordered media (From mean field theory to percolation)\" accepted for\n  publication by Springer [32 pages, 1 figure, 2 tables]",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The new framework for finance is proposed. This framework based on three\nknown approaches in econophysics. Assumptions of the framework are the\nfollowing: 1. For the majority of situations market follows non-arbitrage\ncondition. 2. For the small number of situations market influenced by the\nactions of big firms. 3. If actions of big players lead to the arbitrage\nopportunity, small players could self-organize to take advantage of this\nopportunity. The framework is an attempt to combine approaches of Bouchaud,\nGabaix, Sornette, Stanley and coauthors. Suggested framework is applied for the\nanalysis of market impact models, behavior of big players, self-organization of\nmarket firm and volatility description.\n"
    },
    {
        "paper_id": 1308.3892,
        "authors": "D\\'aniel Kondor, M\\'arton P\\'osfai, Istv\\'an Csabai and G\\'abor Vattay",
        "title": "Do the rich get richer? An empirical analysis of the BitCoin transaction\n  network",
        "comments": "Project website: http://www.vo.elte.hu/bitcoin/; updated after\n  publication",
        "journal-ref": "PLoS ONE 9(2): e86197, 2014",
        "doi": "10.1371/journal.pone.0086197",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  The possibility to analyze everyday monetary transactions is limited by the\nscarcity of available data, as this kind of information is usually considered\nhighly sensitive. Present econophysics models are usually employed on presumed\nrandom networks of interacting agents, and only macroscopic properties (e.g.\nthe resulting wealth distribution) are compared to real-world data. In this\npaper, we analyze BitCoin, which is a novel digital currency system, where the\ncomplete list of transactions is publicly available. Using this dataset, we\nreconstruct the network of transactions, and extract the time and amount of\neach payment. We analyze the structure of the transaction network by measuring\nnetwork characteristics over time, such as the degree distribution, degree\ncorrelations and clustering. We find that linear preferential attachment drives\nthe growth of the network. We also study the dynamics taking place on the\ntransaction network, i.e. the flow of money. We measure temporal patterns and\nthe wealth accumulation. Investigating the microscopic statistics of money\nmovement, we find that sublinear preferential attachment governs the evolution\nof the wealth distribution. We report a scaling relation between the degree and\nwealth associated to individual nodes.\n"
    },
    {
        "paper_id": 1308.3961,
        "authors": "Desislava Chetalova, Thilo A. Schmitt, Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Portfolio return distributions: Sample statistics with non-stationary\n  correlations",
        "comments": "Revised version, 12 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider random vectors drawn from a multivariate normal distribution and\ncompute the sample statistics in the presence of non-stationary correlations.\nFor this purpose, we construct an ensemble of random correlation matrices and\naverage the normal distribution over this ensemble. The resulting distribution\ncontains a modified Bessel function of the second kind whose behavior differs\nsignificantly from the multivariate normal distribution, in the central part as\nwell as in the tails. This result is then applied to asset returns. We compare\nwith empirical return distributions using daily data from the Nasdaq Composite\nIndex in the period from 1992 to 2012. The comparison reveals good agreement,\nthe average portfolio return distribution describes the data well especially in\nthe central part of the distribution. This in turn confirms our ansatz to model\nthe non-stationarity by an ensemble average.\n"
    },
    {
        "paper_id": 1308.3966,
        "authors": "Changki Kim and Yangho Choi and Woojoo Lee and Jae Youn Ahn",
        "title": "Analyzing Herd Behavior in Global Stock Markets: An Intercontinental\n  Comparison",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Herd behavior is an important economic phenomenon, especially in the context\nof the recent financial crises. In this paper, herd behavior in global stock\nmarkets is investigated with a focus on intercontinental comparison. Since most\nexisting herd behavior indices do not provide a comparative method, we propose\na new herd behavior index and demonstrate its desirable properties through\nsimple theoretical models. As for empirical analysis, we use global stock\nmarket data from Morgan Stanley Capital International to study herd behavior\nespecially during periods of financial crises in detail.\n"
    },
    {
        "paper_id": 1308.4276,
        "authors": "Filip Zikes and Jozef Barunik",
        "title": "Semiparametric Conditional Quantile Models for Financial Returns and\n  Realized Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates how the conditional quantiles of future returns and\nvolatility of financial assets vary with various measures of ex-post variation\nin asset prices as well as option-implied volatility. We work in the flexible\nquantile regression framework and rely on recently developed model-free\nmeasures of integrated variance, upside and downside semivariance, and jump\nvariation. Our results for the S&P 500 and WTI Crude Oil futures contracts show\nthat simple linear quantile regressions for returns and heterogenous quantile\nautoregressions for realized volatility perform very well in capturing the\ndynamics of the respective conditional distributions, both in absolute terms as\nwell as relative to a couple of well-established benchmark models. The models\ncan therefore serve as useful risk management tools for investors trading the\nfutures contracts themselves or various derivative contracts written on\nrealized volatility.\n"
    },
    {
        "paper_id": 1308.4363,
        "authors": "Alexander M. G. Cox and Jiajie Wang",
        "title": "Optimal robust bounds for variance options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Robust, or model-independent properties of the variance swap are well-known,\nand date back to Dupire and Neuberger, who showed that, given the price of\nco-terminal call options, the price of a variance swap was exactly specified\nunder the assumption that the price process is continuous. In Cox and Wang we\nshowed that a lower bound on the price of a variance call could be established\nusing a solution to the Skorokhod embedding problem due to Root. In this paper,\nwe provide a construction, and a proof of optimality of the upper bound, using\nresults of Rost and Chacon, and show how this proof can be used to determine a\nsuper-hedging strategy which is model-independent. In addition, we outline how\nthe hedging strategy may be computed numerically. Using these methods, we also\nshow that the Heston-Nandi model is 'asymptotically extreme' in the sense that,\nfor large maturities, the Heston-Nandi model gives prices for variance call\noptions which are approximately the lowest values consistent with the same call\nprice data.\n"
    },
    {
        "paper_id": 1308.5019,
        "authors": "Matthew Lorig, Stefano Pagliarani, Andrea Pascucci",
        "title": "A Taylor series approach to pricing and implied vol for LSV models",
        "comments": "10 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using classical Taylor series techniques, we develop a unified approach to\npricing and implied volatility for European-style options in a general\nlocal-stochastic volatility setting. Our price approximations require only a\nnormal CDF and our implied volatility approximations are fully explicit (ie,\nthey require no special functions, no infinite series and no numerical\nintegration). As such, approximate prices can be computed as efficiently as\nBlack-Scholes prices, and approximate implied volatilities can be computed\nnearly instantaneously.\n"
    },
    {
        "paper_id": 1308.5064,
        "authors": "Vivien Brunel",
        "title": "Analytical models of operational risk and new results on the correlation\n  problem",
        "comments": "9 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a portfolio approach for operational risk quantification based on\na class of analytical models from which we derive new results on the\ncorrelation problem. In particular, we show that uniform correlation is a\nrobust assumption for measuring capital charges in these models.\n"
    },
    {
        "paper_id": 1308.5152,
        "authors": "Ilya Tkachev and Alessandro Abate",
        "title": "Computation of ruin probabilities for general discrete-time Markov\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the ruin problem over a risk process described by a discrete-time\nMarkov model. In contrast to previous studies that focused on the asymptotic\nbehaviour of ruin probabilities for large values of the initial capital, we\nprovide a new technique to compute the quantity of interest for any initial\nvalue, and with any given precision. Rather than focusing on a particular model\nfor risk processes, we give a general characterization of the ruin probability\nby providing corresponding recursions and fixpoint equations. Since such\nequations for the ruin probability are ill-posed in the sense that they do not\nallow for unique solutions, we approximate the ruin probability by a\ntwo-barrier ruin probability, for which fixpoint equations are well-posed. We\nalso show how good the introduced approximation is by providing an explicit\nbound on the error and by characterizing the cases when the error converges to\nzero. The presented technique and results are supported by two computational\nexamples over models known in the literature, one of which is extremely\nheavy-tailed.\n"
    },
    {
        "paper_id": 1308.5376,
        "authors": "Soumik Pal and Ting-Kam Leonard Wong",
        "title": "Energy, entropy, and arbitrage",
        "comments": "21 pages, 7 figures. Substantially revised",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a pathwise approach to analyze the relative performance of an\nequity portfolio with respect to a benchmark market portfolio. In this\nenergy-entropy framework, the relative performance is decomposed into three\ncomponents: a volatility term, a relative entropy term measuring the distance\nbetween the portfolio weights and the market capital distribution, and another\nentropy term that can be controlled by the investor by adopting a suitable\nrebalancing strategy. This framework leads to a class of portfolio strategies\nthat allows one to outperform, in the long run, a market that is diverse and\nsufficiently volatile in the sense of stochastic portfolio theory. The\nframework is illustrated with several empirical examples.\n"
    },
    {
        "paper_id": 1308.5658,
        "authors": "D. S. Grebenkov and J. Serror",
        "title": "Following a Trend with an Exponential Moving Average: Analytical Results\n  for a Gaussian Model",
        "comments": null,
        "journal-ref": "Physica A 394, 288-303 (2014)",
        "doi": "10.1016/j.physa.2013.10.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate how price variations of a stock are transformed into profits\nand losses (P&Ls) of a trend following strategy. In the frame of a Gaussian\nmodel, we derive the probability distribution of P&Ls and analyze its moments\n(mean, variance, skewness and kurtosis) and asymptotic behavior (quantiles). We\nshow that the asymmetry of the distribution (with often small losses and less\nfrequent but significant profits) is reminiscent to trend following strategies\nand less dependent on peculiarities of price variations. At short times, trend\nfollowing strategies admit larger losses than one may anticipate from standard\nGaussian estimates, while smaller losses are ensured at longer times. Simple\nexplicit formulas characterizing the distribution of P&Ls illustrate the basic\nmechanisms of momentum trading, while general matrix representations can be\napplied to arbitrary Gaussian models. We also compute explicitly annualized\nrisk adjusted P&L and strategy turnover to account for transaction costs. We\ndeduce the trend following optimal timescale and its dependence on both\nauto-correlation level and transaction costs. Theoretical results are\nillustrated on the Dow Jones index.\n"
    },
    {
        "paper_id": 1308.5836,
        "authors": "Roland Langrock, Th\\'eo Michelot, Alexander Sohn, Thomas Kneib",
        "title": "Semiparametric stochastic volatility modelling using penalized splines",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic volatility (SV) models mimic many of the stylized facts attributed\nto time series of asset returns, while maintaining conceptual simplicity. The\ncommonly made assumption of conditionally normally distributed or\nStudent-t-distributed returns, given the volatility, has however been\nquestioned. In this manuscript, we introduce a novel maximum penalized\nlikelihood approach for estimating the conditional distribution in an SV model\nin a nonparametric way, thus avoiding any potentially critical assumptions on\nthe shape. The considered framework exploits the strengths both of the powerful\nhidden Markov model machinery and of penalized B-splines, and constitutes a\npowerful and flexible alternative to recently developed Bayesian approaches to\nsemiparametric SV modelling. We demonstrate the feasibility of the approach in\na simulation study before outlining its potential in applications to three\nseries of returns on stocks and one series of stock index returns.\n"
    },
    {
        "paper_id": 1308.612,
        "authors": "Krenar Avdulaj and Jozef Barunik",
        "title": "Can we still benefit from international diversification? The case of the\n  Czech and German stock markets",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1307.5981",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the findings of the recent literature is that the 2008 financial\ncrisis caused reduction in international diversification benefits. To fully\nunderstand the possible potential from diversification, we build an empirical\nmodel which combines generalised autoregressive score copula functions with\nhigh frequency data, and allows us to capture and forecast the conditional\ntime-varying joint distribution of stock returns. Using this novel methodology\nand fresh data covering five years after the crisis, we compute the conditional\ndiversification benefits to answer the question, whether it is still\ninteresting for an international investor to diversify. As diversification\ntools, we consider the Czech PX and the German DAX broad stock indices, and we\nfind that the diversification benefits strongly vary over the 2008--2013 crisis\nyears.\n"
    },
    {
        "paper_id": 1308.6148,
        "authors": "Pawe{\\l} O\\'swi\\c{e}cimka, Stanis{\\l}aw Dro\\.zd\\.z, Marcin Forczek,\n  Stanis{\\l}aw Jadach, Jaros{\\l}aw Kwapie\\'n",
        "title": "Detrended Cross-Correlation Analysis Consistently Extended to\n  Multifractality",
        "comments": "13 pages, 16 figures, Phys. Rev. E in print",
        "journal-ref": "Phys. Rev. E 89, 023305 (2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel algorithm - Multifractal Cross-Correlation Analysis\n(MFCCA) - that constitutes a consistent extension of the Detrended\nCross-Correlation Analysis (DCCA) and is able to properly identify and quantify\nsubtle characteristics of multifractal cross-correlations between two time\nseries. Our motivation for introducing this algorithm is that the already\nexisting methods like MF-DXA have at best serious limitations for most of the\nsignals describing complex natural processes and often indicate multifractal\ncross-correlations when there are none. The principal component of the present\nextension is proper incorporation of the sign of fluctuations to their\ngeneralized moments. Furthermore, we present a broad analysis of the model\nfractal stochastic processes as well as of the real-world signals and show that\nMFCCA is a robust and selective tool at the same time, and therefore allows for\na reliable quantification of the cross-correlative structure of analyzed\nprocesses. In particular, it allows one to identify the boundaries of the\nmultifractal scaling and to analyze a relation between the generalized Hurst\nexponent and the multifractal scaling parameter $\\lambda_q$. This relation\nprovides information about character of potential multifractality in\ncross-correlations and thus enables a deeper insight into dynamics of the\nanalyzed processes than allowed by any other related method available so far.\nBy using examples of time series from stock market, we show that financial\nfluctuations typically cross-correlate multifractally only for relatively large\nfluctuations, whereas small fluctuations remain mutually independent even at\nmaximum of such cross-correlations. Finally, we indicate possible utility of\nMFCCA to study effects of the time-lagged cross-correlations.\n"
    },
    {
        "paper_id": 1308.6256,
        "authors": "Wei Chen",
        "title": "G-consistent price system and bid-ask pricing for European contingent\n  claims under Knightian uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The target of this paper is to consider model the risky asset price on the\nfinancial market under the Knightian uncertainty, and pricing the ask and bid\nprices of the uncertain risk. We use the nonlinear analysis tool, i.e., G-frame\nwork [26], to construct the model of the risky asset price and bid-ask pricing\nfor the European contingent claims under Knightian uncertain financial market.\nFirstly, we consider the basic risky asset price model on the uncertain\nfinancial market, which we construct here is the model with drift uncertain and\nvolatility uncertain. We describe such model by using generalized G-Brownian\nmotion and call it as G-asset price system. We present the uncertain risk\npremium which is uncertain and distributed with maximum distribution. We derive\nthe closed form of bid-ask price of the European contingent claim against the\nunderlying risky asset with G-asset price system as the discounted conditional\nG-expecation of the claim, and the bid and ask prices are the viscosity\nsolutions to the nonlinear HJB equations.Furthermore, we consider the main part\nof this paper, i.e., consider the risky asset on the Knightian uncertain\nfinancial market with the price fluctuation shows as continuous trajectories.\nWe propose the G-conditional full support condition by using uncertain\ncapacity, and the risky asset price path satisfying the G-conditional full\nsupport condition could be approximated by its G-consistent asset price\nsystems. We derive that the bid and ask prices of the European contingent claim\nagainst such risky asset under uncertain can be expressed by discounted of some\nconditional G-expectation of the claim. We give examples, such as G-Markovian\nprocesses and the geometric fractional G-Brownian motion [9], satisfying the\nG-conditional full support condition.\n"
    },
    {
        "paper_id": 1308.6387,
        "authors": "Kyong-Hui Kim and Myong-Guk Sin",
        "title": "Efficient hedging in general Black-Scholes model",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An investor faced with a contingent claim may eliminate risk by perfect\nhedging, but as it is often quite expensive, he seeks partial hedging (quantile\nhedging or efficient hedging) that requires less capital and reduces the risk.\nEfficient hedging for European call option was considered in the standard\nBlack-Scholes model with constant drift and volatility coefficients. In this\npaper we considered the efficient hedging for European call option in general\nBlack-Scholes model $dX_t=X_t(m(t)dt+\\sigma (t)dw(t))$ with time-varying drift\nand volatility coefficients and in fractional Black-Scholes model\n$dX_t=X_t(\\sigma B_H(t)+mdt)$ with constant coefficients.\n"
    },
    {
        "paper_id": 1308.6465,
        "authors": "Carole Bernard, Franck Moraux, Ludger Rueschendorf, Steven Vanduffel",
        "title": "Optimal Payoffs under State-dependent Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most decision theories, including expected utility theory, rank dependent\nutility theory and cumulative prospect theory, assume that investors are only\ninterested in the distribution of returns and not in the states of the economy\nin which income is received. Optimal payoffs have their lowest outcomes when\nthe economy is in a downturn, and this feature is often at odds with the needs\nof many investors. We introduce a framework for portfolio selection within\nwhich state-dependent preferences can be accommodated. Specifically, we assume\nthat investors care about the distribution of final wealth and its interaction\nwith some benchmark. In this context, we are able to characterize optimal\npayoffs in explicit form. Furthermore, we extend the classical expected utility\noptimization problem of Merton to the state-dependent situation. Some\napplications in security design are discussed in detail and we also solve some\nstochastic extensions of the target probability optimization problem.\n"
    },
    {
        "paper_id": 1308.6756,
        "authors": "Vladimir Filimonov, Didier Sornette",
        "title": "Apparent criticality and calibration issues in the Hawkes self-excited\n  point process model: application to high-frequency financial data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a careful analysis of possible issues on the application of the\nself-excited Hawkes process to high-frequency financial data. We carefully\nanalyze a set of effects leading to significant biases in the estimation of the\n\"criticality index\" n that quantifies the degree of endogeneity of how much\npast events trigger future events. We report a number of model biases that are\nintrinsic to the estimation of brnaching ratio (n) when using power law memory\nkernels. We demonstrate that the calibration of the Hawkes process on mixtures\nof pure Poisson process with changes of regime leads to completely spurious\napparent critical values for the branching ratio (n~1) while the true value is\nactually n=0. More generally, regime shifts on the parameters of the Hawkes\nmodel and/or on the generating process itself are shown to systematically lead\nto a significant upward bias in the estimation of the branching ratio. We also\ndemonstrate the importance of the preparation of the high-frequency financial\ndata and give special care to the decrease of quality of the timestamps of tick\ndata due to latency and grouping of messages to packets by the stock exchange.\nAltogether, our careful exploration of the caveats of the calibration of the\nHawkes process stresses the need for considering all the above issues before\nany conclusion can be sustained. In this respect, because the above effects are\nplaguing their analyses, the claim by Hardiman, Bercot and Bouchaud (2013) that\nfinancial market have been continuously functioning at or close to criticality\n(n~1) cannot be supported. In contrast, our previous results on E-mini S&P 500\nFutures Contracts and on major commodity future contracts are upheld.\n"
    },
    {
        "paper_id": 1308.6759,
        "authors": "Yipeng Yang, Allanus Tsoi",
        "title": "Prospect Agents and the Feedback Effect on Price Fluctuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A microeconomic approach is proposed to derive the fluctuations of risky\nasset price, where the market participants are modeled as prospect trading\nagents. As asset price is generated by the temporary equilibrium between demand\nand supply, the agents' trading behaviors can affect the price process in turn,\nwhich is called the feedback effect. The prospect agents make actions based on\ntheir reactions to gains and losses, and as a consequence of the feedback\neffect, a relationship between the agents' trading behavior and the price\nfluctuations is constructed, which explains the implied volatility skew and\nsmile observed in actual market.\n"
    },
    {
        "paper_id": 1309.0046,
        "authors": "Xiaoshan Chen, Yu-Jui Huang, Qingshuo Song, Chao Zhu",
        "title": "The Stochastic Solution to a Cauchy Problem for Degenerate Parabolic\n  Equations",
        "comments": "Keywords: local martingales, local stochastic solutions, degenerate\n  Cauchy problems, Feynman-Kac formula, necessary and sufficient condition for\n  uniqueness, comparison principle",
        "journal-ref": "Journal of Mathematical Analysis and Applications, Vol. 451, Issue\n  1 (2017), pp 448-472",
        "doi": "10.1016/j.jmaa.2017.02.021",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the stochastic solution to a Cauchy problem for a degenerate\nparabolic equation arising from option pricing. When the diffusion coefficient\nof the underlying price process is locally H\\\"older continuous with exponent\n$\\delta\\in (0, 1]$, the stochastic solution, which represents the price of a\nEuropean option, is shown to be a classical solution to the Cauchy problem.\nThis improves the standard requirement $\\delta\\ge 1/2$. Uniqueness results,\nincluding a Feynman-Kac formula and a comparison theorem, are established\nwithout assuming the usual linear growth condition on the diffusion\ncoefficient. When the stochastic solution is not smooth, it is characterized as\nthe limit of an approximating smooth stochastic solutions. In deriving the main\nresults, we discover a new, probabilistic proof of Kotani's criterion for\nmartingality of a one-dimensional diffusion in natural scale.\n"
    },
    {
        "paper_id": 1309.011,
        "authors": "Tinne Haentjens and Karel in 't Hout",
        "title": "ADI schemes for pricing American options under the Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/1350486X.2015.1009129",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper a simple, effective adaptation of Alternating Direction\nImplicit (ADI) time discretization schemes is proposed for the numerical\npricing of American-style options under the Heston model via a partial\ndifferential complementarity problem. The stability and convergence of the new\nmethods are extensively investigated in actual, challenging applications. In\naddition a relevant theoretical result is proved.\n"
    },
    {
        "paper_id": 1309.0218,
        "authors": "Ladislav Kristoufek and Jiri Skuhrovec",
        "title": "Exponential and power laws in public procurement markets",
        "comments": "6 pages, 3 figures",
        "journal-ref": "EPL 99, art. 28005, 2012",
        "doi": "10.1209/0295-5075/99/28005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the first time ever, we analyze a unique public procurement database,\nwhich includes information about a number of bidders for a contract, a final\nprice, an identification of a winner and an identification of a contracting\nauthority for each of more than 40,000 public procurements in the Czech\nRepublic between 2006 and 2011, focusing on the distributional properties of\nthe variables of interest. We uncover several scaling laws -- the exponential\nlaw for the number of bidders, and the power laws for the total revenues and\ntotal spendings of the participating companies, which even follows the Zipf's\nlaw for the 100 most spending institutions. We propose an analogy between\nextensive and non-extensive systems in physics and the public procurement\nmarket situations. Through an entropy maximization, such the analogy yields\nsome interesting results and policy implications with respect to the\nMaxwell-Boltzmann and Pareto distributions in the analyzed quantities.\n"
    },
    {
        "paper_id": 1309.026,
        "authors": "Daniel Levin, Terry Lyons and Hao Ni",
        "title": "Learning from the past, predicting the statistics for the future,\n  learning an evolving system",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We bring the theory of rough paths to the study of non-parametric statistics\non streamed data. We discuss the problem of regression where the input variable\nis a stream of information, and the dependent response is also (potentially) a\nstream.\n  A certain graded feature set of a stream, known in the rough path literature\nas the signature, has a universality that allows formally, linear regression to\nbe used to characterise the functional relationship between independent\nexplanatory variables and the conditional distribution of the dependent\nresponse.\n  This approach, via linear regression on the signature of the stream, is\nalmost totally general, and yet it still allows explicit computation. The\ngrading allows truncation of the feature set and so leads to an efficient local\ndescription for streams (rough paths). In the statistical context this method\noffers potentially significant, even transformational dimension reduction.\n  By way of illustration, our approach is applied to stationary time series\nincluding the familiar AR model and ARCH model. In the numerical examples we\nexamined, our predictions achieve similar accuracy to the Gaussian Process (GP)\napproach with much lower computational cost especially when the sample size is\nlarge.\n"
    },
    {
        "paper_id": 1309.0348,
        "authors": "Michael Dittmar (Institute of Particle Physics, ETH Zurich,\n  Switzerland)",
        "title": "Development Towards Sustainability: How to judge past and proposed\n  policies?",
        "comments": "13 pages, 2 figures, Paper presented at the 2013 World Resource Forum\n  in Davos, Switzerland. Keywords: Natural Capital, IPAT equation,\n  unsustainable living, development towards sustainability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The scientific data about the state of our planet, presented at the 2012\n(Rio+20) summit, documented that today's human family lives even less\nsustainably than it did in 1992. The data indicate furthermore that the\nenvironmental impacts from our current economic activities are so large, that\nwe are approaching situations where potentially controllable regional problems\ncan easily lead to uncontrollable global disasters.\n  Assuming that (1) the majority of the human family, once adequately informed,\nwants to achieve a \"sustainable way of life\" and (2) that the \"development\ntowards sustainability\" roadmap will be based on scientific principles, one\nmust begin with unambiguous and quantifiable definitions of these goals. As\nwill be demonstrated, the well known scientific method to define abstract and\ncomplex issues by their negation, satisfies these requirements. Following this\nnew approach, it also becomes possible to decide if proposed and actual\npolicies changes will make our way of life less unsustainable, and thus move us\npotentially into the direction of sustainability. Furthermore, if potentially\ndangerous tipping points are to be avoided, the transition roadmap must include\nsome minimal speed requirements. Combining the negation method and the time\nevolution of that remaining natural capital in different domains, the\ntransition speed for a \"development towards sustainability\" can be quantified\nat local, regional and global scales.\n  The presented ideas allow us to measure the rate of natural capital depletion\nand the rate of restoration that will be required if humanity is to avoid\nreaching a sustainable future by a collapse transition.\n"
    },
    {
        "paper_id": 1309.0362,
        "authors": "Mikl\\'os R\\'asonyi and Andrea Meireles Rodrigues",
        "title": "Continuous-Time Portfolio Optimisation for a Behavioural Investor with\n  Bounded Utility on Gains",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines an optimal investment problem in a continuous-time\n(essentially) complete financial market with a finite horizon. We deal with an\ninvestor who behaves consistently with principles of Cumulative Prospect\nTheory, and whose utility function on gains is bounded above. The\nwell-posedness of the optimisation problem is trivial, and a necessary\ncondition for the existence of an optimal trading strategy is derived. This\ncondition requires that the investor's probability distortion function on\nlosses does not tend to 0 near 0 faster than a given rate, which is determined\nby the utility function. Under additional assumptions, we show that this\ncondition is indeed the borderline for attainability, in the sense that for\nslower convergence of the distortion function there does exist an optimal\nportfolio.\n"
    },
    {
        "paper_id": 1309.0461,
        "authors": "Paulwin Graewe, Ulrich Horst, Jinniao Qiu",
        "title": "A Non-Markovian Liquidation Problem and Backward SPDEs with Singular\n  Terminal Conditions",
        "comments": null,
        "journal-ref": "SIAM J. Control Optim. 53 (2015) 690-711",
        "doi": "10.1137/130944084",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish existence, uniqueness and regularity of solution results for a\nclass of backward stochastic partial differential equations with singular\nterminal condition. The equation describes the value function of non-Markovian\nstochastic optimal control problem in which the terminal state of the\ncontrolled process is pre-specified. The analysis of such control problems is\nmotivated by models of optimal portfolio liquidation.\n"
    },
    {
        "paper_id": 1309.0474,
        "authors": "Paulwin Graewe, Ulrich Horst, Eric S\\'er\\'e",
        "title": "Smooth solutions to portfolio liquidation problems under price-sensitive\n  market impact",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.spa.2017.06.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the stochastic control problem of a financial trader that needs\nto unwind a large asset portfolio within a short period of time. The trader can\nsimultaneously submit active orders to a primary market and passive orders to a\ndark pool. Our framework is flexible enough to allow for price-dependent impact\nfunctions describing the trading costs in the primary market and\nprice-dependent adverse selection costs associated with dark pool trading. We\nprove that the value function can be characterized in terms of the unique\nsmooth solution to a PDE with singular terminal value, establish its explicit\nasymptotic behavior at the terminal time, and give the optimal trading strategy\nin feedback form.\n"
    },
    {
        "paper_id": 1309.0491,
        "authors": "Jozef Barunik, Lukas Vacha",
        "title": "Contagion among Central and Eastern European stock markets during the\n  financial crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper contributes to the literature on international stock market\ncomovements and contagion. The novelty of our approach lies in application of\nwavelet tools to high-frequency financial market data, which allows us to\nunderstand the relationship between stock markets in a time-frequency domain.\nWhile major part of economic time series analysis is done in time or frequency\ndomain separately, wavelet analysis combines these two fundamental approaches.\nWavelet techniques uncover interesting dynamics of correlations between the\nCentral and Eastern European (CEE) stock markets and the German DAX at various\ninvestment horizons. The results indicate that connection of the CEE markets to\nthe leading market of the region is significantly lower at higher frequencies\nin comparison to the lower frequencies. Contrary to previous literature, we\ndocument significantly lower contagion between the CEE markets and the German\nDAX after the large 2008 stock market crash.\n"
    },
    {
        "paper_id": 1309.0557,
        "authors": "Chulmin Kang, Wanmo Kang",
        "title": "Exact Simulation of Wishart Multidimensional Stochastic Volatility Model",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we propose an exact simulation method of the Wishart\nmultidimensional stochastic volatility (WMSV) model, which was recently\nintroduced by Da Fonseca et al. \\cite{DGT08}. Our method is based onanalysis of\nthe conditional characteristic function of the log-price given volatility\nlevel. In particular, we found an explicit expression for the conditional\ncharacteristic function for the Heston model. We perform numerical experiments\nto demonstrate the performance and accuracy of our method. As a result of\nnumerical experiments, it is shown that our new method is much faster and\nreliable than Euler discretization method.\n"
    },
    {
        "paper_id": 1309.0582,
        "authors": "Ladislav Kristoufek and Petra Lunackova",
        "title": "Long-term memory in electricity prices: Czech market evidence",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We analyze long-term memory properties of hourly prices of electricity in the\nCzech Republic between 2009 and 2012. As the dynamics of the electricity prices\nis dominated by cycles -- mainly intraday and daily -- we opt for the detrended\nfluctuation analysis, which is well suited for such specific series. We find\nthat the electricity prices are non-stationary but strongly mean-reverting\nwhich distinguishes them from other financial assets which are usually\ncharacterized as unit root series. Such description is attributed to specific\nfeatures of electricity prices, mainly to non-storability. Additionally, we\nargue that the rapid mean-reversion is due to the principles of electricity\nspot prices. These properties are shown to be stable across all studied years.\n"
    },
    {
        "paper_id": 1309.0602,
        "authors": "Aki-Hiro Sato and Hideki Takayasu",
        "title": "Segmentation procedure based on Fisher's exact test and its application\n  to foreign exchange rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes the segmentation procedure of univariate time series\nbased on Fisher's exact test. We show that an adequate change point can be\ndetected as the minimum value of p-value. It is shown that the proposed\nprocedure can detect change points for an artificial time series. We apply the\nproposed method to find segments of the foreign exchange rates recursively. It\nis also applied to randomly shuffled time series. It concludes that the\nrandomly shuffled data can be used as a level to determine the null hypothesis.\n"
    },
    {
        "paper_id": 1309.1138,
        "authors": "Hai-Chuan Xu, Wei Zhang, Yi-Fang Liu",
        "title": "Short-term Market Reaction after Trading Halts in Chinese Stock Market",
        "comments": "11 pages, 8 figures, Physica A (2014)",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.01.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the dynamics of absolute return, trading volume and\nbid-ask spread after the trading halts using high-frequency data from the\nShanghai Stock Exchange. We deal with all three types of trading halts, namely\nintraday halts, one-day halts and inter-day halts, of 203 stocks in Shanghai\nStock Exchange from August 2009 to August 2011. We find that absolute return,\ntrading volume, and in case of bid-ask spread around intraday halts share the\nsame pattern with a sharp peak and a power law relaxation after that. While for\ndifferent types of trading halts, the peaks' height and the relaxation\nexponents are different. From the perspective of halt reasons or halt duration,\nthe relaxation exponents of absolute return after inter-day halts are larger\nthan that after intraday halts and one-day halts, which implies that inter-day\nhalts are most effective. From the perspective of price trends, the relaxation\nexponents of excess absolute return and excess volume for positive events are\nlarger than that for negative events in case of intraday halts and one-day\nhalts, implying that positive events are more effective than negative events\nfor intraday halts and one-day halts. In contrast, negative events are more\neffective than positive events for inter-day halts.\n"
    },
    {
        "paper_id": 1309.142,
        "authors": "Erhan Bayraktar and Yuchong Zhang",
        "title": "Fundamental Theorem of Asset Pricing under Transaction costs and Model\n  uncertainty",
        "comments": "Final version. To appear in Mathematics of Operations Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the Fundamental Theorem of Asset Pricing for a discrete time\nfinancial market where trading is subject to proportional transaction cost and\nthe asset price dynamic is modeled by a family of probability measures,\npossibly non-dominated. Using a backward-forward scheme, we show that when the\nmarket consists of a money market account and a single stock, no-arbitrage in a\nquasi-sure sense is equivalent to the existence of a suitable family of\nconsistent price systems. We also show that when the market consists of\nmultiple dynamically traded assets and satisfies \\emph{efficient friction},\nstrict no-arbitrage in a quasi-sure sense is equivalent to the existence of a\nsuitable family of strictly consistent price systems.\n"
    },
    {
        "paper_id": 1309.1492,
        "authors": "Ladislav Kristoufek and Miloslav Vosvrda",
        "title": "Commodity futures and market efficiency",
        "comments": "20 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We analyze the market efficiency of 25 commodity futures across various\ngroups -- metals, energies, softs, grains and other agricultural commodities.\nTo do so, we utilize recently proposed Efficiency Index to find that the most\nefficient of all the analyzed commodities is heating oil, closely followed by\nWTI crude oil, cotton, wheat and coffee. On the other end of the ranking, we\ndetect live cattle and feeder cattle. The efficiency is also found to be\ncharacteristic for specific groups of commodities -- energy commodities being\nthe most efficient and the other agricultural commodities (formed mainly of\nlivestock) the least efficient groups. We also discuss contributions of the\nlong-term memory, fractal dimension and approximate entropy to the total\ninefficiency. Last but not least, we come across the nonstandard relationship\nbetween the fractal dimension and Hurst exponent. For the analyzed dataset, the\nrelationship between these two is positive meaning that local persistence\n(trending) is connected to global anti-persistence. We attribute this to\nspecifics of commodity futures which might be predictable in a short term and\nlocally but in a long term, they return to their fundamental price.\n"
    },
    {
        "paper_id": 1309.1647,
        "authors": "Hyong-Chol O, Song-Yon Kim, Dong-Hyok Kim and Chol-Hyok Pak",
        "title": "Comprehensive Unified Models of Structural and Reduced Form Models for\n  Defaultable Fixed Income Bonds (Part 1: One factor-model, Part 2:Two\n  factors-model)",
        "comments": "36 pages, 1 figure, Ver. 1 descrives two factor model, ver.2 added\n  one factor model and ver.3 revised the title",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2330041, 10.2139/ssrn.2337846",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing formulae for defaultable corporate bonds with discrete coupons under\nconsideration of the government taxes in the united model of structural and\nreduced form models are provided. The aim of this paper is to generalize the\ncomprehensive structural model for defaultable fixed income bonds (considered\nin [1]) into a comprehensive unified model of structural and reduced form\nmodels. Here we consider the one factor model and the two factor model. In the\none factor model the bond holders receive the deterministic coupon at\npredetermined coupon dates and the face value (debt) and the coupon at the\nmaturity as well as the effect of government taxes which are paid on the\nproceeds of an investment in bonds is considered under constant short rate. In\nthe two factor model the bond holders receive the stochastic coupon (discounted\nvalue of that at the maturity) at predetermined coupon dates and the face value\n(debt) and the coupon at the maturity as well as the effect of government taxes\nwhich are paid on the proceeds of an investment in bonds is considered under\nstochastic short rate. The expected default event occurs when the equity value\nis not enough to pay coupon or debt at the coupon dates or maturity and\nunexpected default event can occur at the first jump time of a Poisson process\nwith the given default intensity provided by a step function of time variable.\nWe consider the model and pricing formula for equity value and using it\ncalculate expected default barrier. Then we provide pricing model and formula\nfor defaultable corporate bonds with discrete coupons and consider its duration\nand the effect of the government taxes.\n"
    },
    {
        "paper_id": 1309.1757,
        "authors": "Ivan Kitov, Oleg Kitov",
        "title": "Inflation, unemployment, and labor force. Phillips curves and long-term\n  projections for Japan",
        "comments": "32 pages, 11 figures. arXiv admin note: substantial text overlap with\n  arXiv:1002.0277",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The evolution of the rate of price inflation and unemployment in Japan has\nbeen modeled within the Phillips curve framework. As an extension to the\nPhillips curve, we represent both variables as linear functions of the change\nrate of labor force. All models were first estimated in 2005 for the period\nbetween 1980 and 2003. Here we update these original models with data through\n2012. The revisited models accurately describe disinflation during the 1980s\nand 1990s as well as the whole deflationary period started in the late 1990s.\nThe Phillips curve for Japan confirms the original concept that growing\nunemployment results in decreasing inflation. A linear and lagged generalized\nPhillips curve expressed as a link between inflation, unemployment, and labor\nforce has been also re-estimated and validated by new data. Labor force\nprojections allow a long-term inflation and unemployment forecast: the GDP\ndeflator will be negative (between -0.5% and -2% per year) during the next 40\nyears. The rate of unemployment will increase from 4.3% in 2012 to 5.5% in\n2050.\n"
    },
    {
        "paper_id": 1309.1844,
        "authors": "Adrien Nguyen Huu (FiME Lab, IMPA)",
        "title": "Investment under uncertainty, competition and regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a randomization procedure undertaken in real option games\nwhich can serve as a basic model of regulation in a duopoly model of preemptive\ninvestment. We recall the rigorous framework of [M. Grasselli, V. Lecl\\`ere and\nM. Ludkovsky, Priority Option: the value of being a leader, International\nJournal of Theoretical and Applied Finance, 16, 2013], and extend it to a\nrandom regulator. This model generalizes and unifies the different competitive\nframeworks proposed in the literature, and creates a new one similar to a\nStackelberg leadership. We fully characterize strategic interactions in the\nseveral situations following from the parametrization of the regulator.\nFinally, we study the effect of the coordination game and uncertainty of\noutcome when agents are risk-averse, providing new intuitions for the standard\ncase.\n"
    },
    {
        "paper_id": 1309.1871,
        "authors": "Takero Ibuki, Shunsuke Higano, Sei Suzuki, Jun-ichi Inoue and Anirban\n  Chakraborti",
        "title": "Statistical inference of co-movements of stocks during a financial\n  crisis",
        "comments": "16 pages, 6 figures. To appear in the Proceedings of the\n  International Meeting on \"Inference, Computation, and Spin Glasses\" held\n  during July 28-30, 2013, at Sapporo, Japan",
        "journal-ref": null,
        "doi": "10.1088/1742-6596/473/1/012008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to figure out and to forecast the emergence phenomena of social\nsystems, we propose several probabilistic models for the analysis of financial\nmarkets, especially around a crisis. We first attempt to visualize the\ncollective behaviour of markets during a financial crisis through\ncross-correlations between typical Japanese daily stocks by making use of\nmulti- dimensional scaling. We find that all the two-dimensional points\n(stocks) shrink into a single small region when a economic crisis takes place.\nBy using the properties of cross-correlations in financial markets especially\nduring a crisis, we next propose a theoretical framework to predict several\ntime-series simultaneously. Our model system is basically described by a\nvariant of the multi-layered Ising model with random fields as non-stationary\ntime series. Hyper-parameters appearing in the probabilistic model are\nestimated by means of minimizing the 'cumulative error' in the past market\nhistory. The justification and validity of our approaches are numerically\nexamined for several empirical data sets.\n"
    },
    {
        "paper_id": 1309.1953,
        "authors": "Marcel Ausloos",
        "title": "Econophysics: Comments on a few Applications, Successes, Methods, &\n  Models",
        "comments": "prepared for IIMK Society \\& Management Review Vol 2(2), July 2013,\n  Special issue on \"Econophysics: Perspectives and Prospects\"",
        "journal-ref": "IMK Society \\& Management Review 2(2), 101-115 (2013), Special\n  issue on \"Econophysics: Perspectives and Prospects\"",
        "doi": "10.1177/2277975213507832",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For this special issue, the article aims at discussing a few econophysics\nproblems studied so far rather successfully. The following \"applications\" in\nmicro-econo-physics are considered : (i) financial crashes; it is emphasized\nthat one can distinguish between endogenous and exogenous causes; (ii)\nportofolio control, selection and inherent risk measure; (iii) foreign currency\nexchanges, also distinguishing endogenous and exogenous money control; (iv)\nprice and asset evolution values. It is shown that some macro-econo-physics\nproblem have been also tackled, like geographic/political constraints, the\nglobalization of the economy and country clustering. Moreover, it is daring to\nsuggest prospect for studies and researches, whence presenting some selection\nof a few interesting perspectives.\n"
    },
    {
        "paper_id": 1309.1988,
        "authors": "Johannes Ruf and Wolfgang Runggaldier",
        "title": "A Systematic Approach to Constructing Market Models With Arbitrage",
        "comments": "Very minor changes",
        "journal-ref": "forthcoming in \"Arbitrage, Credit and Informational Risks\",\n  Proceedings of the Sino-French Research Program in Financial Mathematics\n  Conference, Beijing June 2013",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This short note provides a systematic construction of market models without\nunbounded profits but with arbitrage opportunities.\n"
    },
    {
        "paper_id": 1309.213,
        "authors": "Davide Fiaschi, Imre Kondor, Matteo Marsili, Valerio Volpati",
        "title": "The Interrupted Power Law and The Size of Shadow Banking",
        "comments": "12 pages, 5 figures, 2 tables. To appear in Plos ONE 2014",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0094237",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using public data (Forbes Global 2000) we show that the asset sizes for the\nlargest global firms follow a Pareto distribution in an intermediate range,\nthat is ``interrupted'' by a sharp cut-off in its upper tail, where it is\ntotally dominated by financial firms. This flattening of the distribution\ncontrasts with a large body of empirical literature which finds a Pareto\ndistribution for firm sizes both across countries and over time. Pareto\ndistributions are generally traced back to a mechanism of proportional random\ngrowth, based on a regime of constant returns to scale. This makes our findings\nof an ``interrupted'' Pareto distribution all the more puzzling, because we\nprovide evidence that financial firms in our sample should operate in such a\nregime. We claim that the missing mass from the upper tail of the asset size\ndistribution is a consequence of shadow banking activity and that it provides\nan (upper) estimate of the size of the shadow banking system. This estimate --\nwhich we propose as a shadow banking index -- compares well with estimates of\nthe Financial Stability Board until 2009, but it shows a sharper rise in shadow\nbanking activity after 2010. Finally, we propose a proportional random growth\nmodel that reproduces the observed distribution, thereby providing a\nquantitative estimate of the intensity of shadow banking activity.\n"
    },
    {
        "paper_id": 1309.2211,
        "authors": "Evelina Shamarova and Rui S\\'a Pereira",
        "title": "Hedging in a market with jumps - an FBSDE approach",
        "comments": "Major changes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for hedging in a market with jumps for a large investor.\nThe dynamics of the stock prices and the value process is governed by\nforward-backward SDEs driven by Teugels martingales. Unlike known FBSDE market\nmodels, ours accounts for jumps in stock prices. Moreover, it allows to find an\noptimal hedging strategy.\n"
    },
    {
        "paper_id": 1309.2274,
        "authors": "Henrique N. S\\'a Earp and Ademar R. Romeiro",
        "title": "The Entropy Law and the impossibility of perpetual economic growth",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Every production-recycling iteration accumulates an inevitable proportion of\nits matter-energy in the environment, lest the production process itself would\nbe a system in perpetual motion, violating the second law of Thermodynamics.\nSuch high-entropy matter depletes finite stocks of ecosystem services provided\nby the ecosphere, hence are incompatible with the long-term growth in the\nmaterial scale of the economic process. Moreover, the complex natural systems\ngoverning such stocks respond to depletion by possibly sudden environmental\ntransitions, thus hindering markets' very ability to adapt to the new\nequilibrium conditions. Consequently, uncertainty of critical resilience\nthresholds constrains material economic growth.\n"
    },
    {
        "paper_id": 1309.2383,
        "authors": "Alexander Novikov, Nino Kordzakhia",
        "title": "On lower and upper bounds for Asian-type options: a unified approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of dealing with financial risk management problems it is\ndesirable to have accurate bounds for option prices in situations when pricing\nformulae do not exist in the closed form. A unified approach for obtaining\nupper and lower bounds for Asian-type options, including options on VWAP, is\nproposed in this paper. The bounds obtained are applicable to the continuous\nand discrete-time frameworks for the case of time-dependent interest rates.\nNumerical examples are provided to illustrate the accuracy of the bounds.\n"
    },
    {
        "paper_id": 1309.2411,
        "authors": "Raffaello Morales, T. Di Matteo and Tomaso Aste",
        "title": "Dependency Structure and Scaling Properties of Financial Time Series Are\n  Related",
        "comments": "12 pages, 8 figures",
        "journal-ref": "Scientific Reports 4, 4589, 2014",
        "doi": "10.1038/srep04589",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report evidence of a deep interplay between cross-correlations\nhierarchical properties and multifractality of New York Stock Exchange daily\nstock returns. The degree of multifractality displayed by different stocks is\nfound to be positively correlated to their depth in the hierarchy of\ncross-correlations. We propose a dynamical model that reproduces this\nobservation along with an array of other empirical properties. The structure of\nthis model is such that the hierarchical structure of heterogeneous risks plays\na crucial role in the time evolution of the correlation matrix, providing an\ninterpretation to the mechanism behind the interplay between cross-correlation\nand multifractality in financial markets, where the degree of multifractality\nof stocks is associated to their hierarchical positioning in the\ncross-correlation structure. Empirical observations reported in this paper\npresent a new perspective towards the merging of univariate multi scaling and\nmultivariate cross-correlation properties of financial time series.\n"
    },
    {
        "paper_id": 1309.2416,
        "authors": "Taisei Kaizoji",
        "title": "Modeling of Stock Returns and Trading Volume",
        "comments": "17 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we investigate the statistical properties of the returns and\nthe trading volume. We show a typical example of power-law distributions of the\nreturn and of the trading volume. Next, we propose an interacting agent model\nof stock markets inspired from statistical mechanics [24] to explore the\nempirical findings. We show that as the interaction among the interacting\ntraders strengthens both the returns and the trading volume present power-law\nbehavior.\n"
    },
    {
        "paper_id": 1309.2728,
        "authors": "Erhan Bayraktar, Yuchong Zhang, Zhou Zhou",
        "title": "A note on the Fundamental Theorem of Asset Pricing under model\n  uncertainty",
        "comments": "Final version. To appear in Risks",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the results of ArXiv:1305.6008 on the Fundamental Theorem of\nAsset Pricing and the super-hedging theorem can be extended to the case in\nwhich the options available for static hedging (\\emph{hedging options}) are\nquoted with bid-ask spreads. In this set-up, we need to work with the notion of\n\\emph{robust no-arbitrage} which turns out to be equivalent to no-arbitrage\nunder the additional assumption that hedging options with non-zero spread are\n\\emph{non-redundant}. A key result is the closedness of the set of attainable\nclaims, which requires a new proof in our setting.\n"
    },
    {
        "paper_id": 1309.297,
        "authors": "F. Sorrentino, D. Tolic, R. Fierro, J.R. Gordon, and A. Mammoli",
        "title": "Stability analysis of a model for the market dynamics of a smart grid",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the dynamics of a smart grid system characterized by widespread\ndistributed generation and storage devices. We assume that agents are free to\ntrade electric energy over the network and we focus on the emerging market\ndynamics. We consider three different models for the market dynamics for which\nwe present a stability analysis. We see that stability depends on the specific\nform of the market dynamics and it may depend on the structure of the\nunderlying network topology. We run numerical simulations that confirm our\ntheoretical predictions. As an example, we test our model for the market\ndynamics over a real network topology, namely, the Tramway 11 Feeder from New\nMexico's power network.\n"
    },
    {
        "paper_id": 1309.2982,
        "authors": "Erhan Bayraktar, Yu-Jui Huang, Zhou Zhou",
        "title": "On hedging American options under model uncertainty",
        "comments": "Final version. To appear in SIAM Journal on Financial Mathematics\n  (SIFIN)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider as given a discrete time financial market with a risky asset and\noptions written on that asset and determine both the sub- and super-hedging\nprices of an American option in the model independent framework of\nArXiv:1305.6008. We obtain the duality of results for the sub- and\nsuper-hedging prices. For the sub-hedging prices we discuss whether the sup and\ninf in the dual representation can be exchanged (a counter example shows that\nthis is not true in general). For the super-hedging prices we discuss several\nalternative definitions and argue why our choice is more reasonable. Then\nassuming that the path space is compact, we construct a discretization of the\npath space and demonstrate the convergence of the hedging prices at the optimal\nrate. The latter result would be useful for numerical computation of the\nhedging prices. Our results generalize those of ArXiv:1304.3574 to the case\nwhen static positions in (finitely many) European options can be used in the\nhedging portfolio.\n"
    },
    {
        "paper_id": 1309.3035,
        "authors": "D.J. Manuge",
        "title": "Multi-Asset Option Pricing with Exponential L\\'evy Processes and the\n  Mellin Transform",
        "comments": "Material presented at AMMCS-2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exponential L\\'evy processes have been used for modelling financial\nderivatives because of their ability to exhibit many empirical features of\nmarkets. Using their multidimensional analogue, a general analytic pricing\nformula is obtained, allowing for the direct valuation of multi-asset options\non $n \\in \\z^+$ risky assets. By providing alternate expressions for\nmulti-asset option payoffs, the general pricing formula can reduce to many\npopular cases, including American basket options which are considered herein.\nThis work extends previous results of basket options to dimensions $n \\geq 3$\nand more generally, to payoff functions that satisfy Lipschitz continuity.\n"
    },
    {
        "paper_id": 1309.3057,
        "authors": "Archil Gulisashvili, Peter Tankov",
        "title": "Tail behavior of sums and differences of log-normal random variables",
        "comments": "Published at http://dx.doi.org/10.3150/14-BEJ665 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2016, Vol. 22, No. 1, 444-493",
        "doi": "10.3150/14-BEJ665",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present sharp tail asymptotics for the density and the distribution\nfunction of linear combinations of correlated log-normal random variables, that\nis, exponentials of components of a correlated Gaussian vector. The asymptotic\nbehavior turns out to depend on the correlation between the components, and the\nexplicit solution is found by solving a tractable quadratic optimization\nproblem. These results can be used either to approximate the probability of\ntail events directly, or to construct variance reduction procedures to estimate\nthese probabilities by Monte Carlo methods. In particular, we propose an\nefficient importance sampling estimator for the left tail of the distribution\nfunction of the sum of log-normal variables. As a corollary of the tail\nasymptotics, we compute the asymptotics of the conditional law of a Gaussian\nrandom vector given a linear combination of exponentials of its components. In\nrisk management applications, this finding can be used for the systematic\nconstruction of stress tests, which the financial institutions are required to\nconduct by the regulators. We also characterize the asymptotic behavior of the\nValue at Risk for log-normal portfolios in the case where the confidence level\ntends to one.\n"
    },
    {
        "paper_id": 1309.3102,
        "authors": "R\\'emy Chicheportiche and Jean-Philippe Bouchaud",
        "title": "A nested factor model for non-linear dependences in stock returns",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2014.994668",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of our work is to propose a natural framework to account for all the\nempirically known properties of the multivariate distribution of stock returns.\nWe define and study a \"nested factor model\", where the linear factors part is\nstandard, but where the log-volatility of the linear factors and of the\nresiduals are themselves endowed with a factor structure and residuals. We\npropose a calibration procedure to estimate these log-vol factors and the\nresiduals. We find that whereas the number of relevant linear factors is\nrelatively large (10 or more), only two or three log-vol factors emerge in our\nanalysis of the data. In fact, a minimal model where only one log-vol factor is\nconsidered is already very satisfactory, as it accurately reproduces the\nproperties of bivariate copulas, in particular the dependence of the\nmedial-point on the linear correlation coefficient, as reported in\nChicheportiche and Bouchaud (2012). We have tested the ability of the model to\npredict Out-of-Sample the risk of non-linear portfolios, and found that it\nperforms significantly better than other schemes.\n"
    },
    {
        "paper_id": 1309.3399,
        "authors": "Karol Wawrzyniak and Wojciech Wi\\'slicki",
        "title": "Grand canonical minority game as a sign predictor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the extended model of Minority game (MG), incorporating\nvariable number of agents and therefore called Grand Canonical, is used for\nprediction. We proved that the best MG-based predictor is constituted by a\ntremendously degenerated system, when only one agent is involved. The\nprediction is the most efficient if the agent is equipped with all strategies\nfrom the Full Strategy Space. Each of these filters is evaluated and, in each\nstep, the best one is chosen. Despite the casual simplicity of the method its\nusefulness is invaluable in many cases including real problems. The significant\npower of the method lies in its ability to fast adaptation if \\lambda-GCMG\nmodification is used. The success rate of prediction is sensitive to the\nproperly set memory length. We considered the feasibility of prediction for the\nMinority and Majority games. These two games are driven by different dynamics\nwhen self-generated time series are considered. Both dynamics tend to be the\nsame when a feedback effect is removed and an exogenous signal is applied.\n"
    },
    {
        "paper_id": 1309.3479,
        "authors": "Jan Kallsen, Shen Li",
        "title": "Portfolio Optimization under Small Transaction Costs: a Convex Duality\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor with constant absolute risk aversion who trades a\nrisky asset with general Ito dynamics, in the presence of small proportional\ntransaction costs. Kallsen and Muhle-Karbe (2012) formally derived the\nleading-order optimal trading policy and the associated welfare impact of\ntransaction costs. In the present paper, we carry out a convex duality approach\nfacilitated by the concept of shadow price processes in order to verify the\nmain results of Kallsen and Muhle-Karbe under well-defined regularity\nconditions.\n"
    },
    {
        "paper_id": 1309.3639,
        "authors": "Alessio Emanuele Biondo, Alessandro Pluchino, Andrea Rapisarda, Dirk\n  Helbing",
        "title": "Reducing Financial Avalanches By Random Investments",
        "comments": "8 pages, 8 figures - Revised version accepted for publication in\n  Phys. Rev. E",
        "journal-ref": "Phys. Rev. E 88, 062814 (2013)",
        "doi": "10.1103/PhysRevE.88.062814",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Building on similarities between earthquakes and extreme financial events, we\nuse a self-organized criticality-generating model to study herding and\navalanche dynamics in financial markets. We consider a community of interacting\ninvestors, distributed on a small-world network, who bet on the bullish\n(increasing) or bearish (decreasing) behavior of the market which has been\nspecified according to the S&P500 historical time series. Remarkably, we find\nthat the size of herding-related avalanches in the community can be strongly\nreduced by the presence of a relatively small percentage of traders, randomly\ndistributed inside the network, who adopt a random investment strategy. Our\nfindings suggest a promising strategy to limit the size of financial bubbles\nand crashes. We also obtain that the resulting wealth distribution of all\ntraders corresponds to the well-known Pareto power law, while the one of random\ntraders is exponential. In other words, for technical traders, the risk of\nlosses is much greater than the probability of gains compared to those of\nrandom traders.\n"
    },
    {
        "paper_id": 1309.3721,
        "authors": "Jin Hyuk Choi",
        "title": "Asymptotic analysis for Merton's problem with transaction costs in power\n  utility case",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the optimal investment and consumption problem with proportional\ntransaction costs. We prove that both the value function and the slopes of the\nlines demarcating the no-trading region are analytic functions of cube root of\nthe transaction cost parameter. Also, we can explicitly calculate the\ncoefficients of the fractional power series expansions of the value function\nand the no-trading region.\n"
    },
    {
        "paper_id": 1309.3771,
        "authors": "Dmitry Schmerling",
        "title": "New models of income distribution, graduation as the explanation of Gini\n  coefficient",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper covers the new model of wage distribution in typical group of\npeople. The model provides the opportunity to reparameterize applicable income\ndistribution model: Pareto, logarithmically normal, logarithmically logistic,\nDagum etc. The model ensures the graduation of Gini index values by polynomial\ndegree of wage distribution as well as different types of income distribution.\nThe given approach clarifies the nature of income inequality.\n"
    },
    {
        "paper_id": 1309.3832,
        "authors": "Robert B. Gramacy and Mike Ludkovski",
        "title": "Sequential Design for Optimal Stopping Problems",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": "10.1137/140980089",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new approach to solve optimal stopping problems via simulation.\nWorking within the backward dynamic programming/Snell envelope framework, we\naugment the methodology of Longstaff-Schwartz that focuses on approximating the\nstopping strategy. Namely, we introduce adaptive generation of the stochastic\ngrids anchoring the simulated sample paths of the underlying state process.\nThis allows for active learning of the classifiers partitioning the state space\ninto the continuation and stopping regions. To this end, we examine sequential\ndesign schemes that adaptively place new design points close to the stopping\nboundaries. We then discuss dynamic regression algorithms that can implement\nsuch recursive estimation and local refinement of the classifiers. The new\nalgorithm is illustrated with a variety of numerical experiments, showing that\nan order of magnitude savings in terms of design size can be achieved. We also\ncompare with existing benchmarks in the context of pricing multi-dimensional\nBermudan options.\n"
    },
    {
        "paper_id": 1309.3844,
        "authors": "Mikhail Kopytin, Evgeniy Kazantsev",
        "title": "Futures market efficiency diagnostics via temporal two-point\n  correlations. Russian market case study",
        "comments": "13 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a two-point correlation technique, we study emergence of market\nefficiency in the emergent Russian futures market by focusing on lagged\ncorrelations. The correlation strength of leader-follower effects in the lagged\ninter-market correlations on the hourly time frame is seen to be significant\ninitially (2009-2011) but gradually goes down, as the erstwhile leader\ninstruments -- crude oil, the USD/RUB exchange rate, and the Russian stock\nmarket index -- seem to lose the leader status. An inefficiency index, based on\ntwo-point correlations, is proposed and its history is established.\n"
    },
    {
        "paper_id": 1309.405,
        "authors": "Pavel Krej\\v{c}\\'i, Harbir Lamba, Sergey Melnik and Dmitrii Rachinskii",
        "title": "Analytical solution for a class of network dynamics with mechanical and\n  financial applications",
        "comments": "12 pages, 9 figures, 1 table",
        "journal-ref": "Phys. Rev. E 90, 032822 (2014)",
        "doi": "10.1103/PhysRevE.90.032822",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that for a certain class of dynamics at the nodes the response of a\nnetwork of any topology to arbitrary inputs is defined in a simple way by its\nresponse to a monotone input. The nodes may have either a discrete or\ncontinuous set of states and there is no limit on the complexity of the\nnetwork. The results provide both an efficient numerical method and the\npotential for accurate analytic approximation of the dynamics on such networks.\nAs illustrative applications, we introduce a quasistatic mechanical model with\nobjects interacting via frictional forces, and a financial market model with\navalanches and critical behavior that are generated by momentum trading\nstrategies.\n"
    },
    {
        "paper_id": 1309.4156,
        "authors": "Gautier M. Krings, Jean-Fran\\c{c}ois Carpantier and Jean-Charles\n  Delvenne",
        "title": "Trade integration and trade imbalances in the European Union: a network\n  perspective",
        "comments": "12 pages + 11 pictures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0083448",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the ever more integrated and ever more unbalanced trade\nrelationships between European countries. To better capture the complexity of\neconomic networks, we propose two global measures that assess the trade\nintegration and the trade imbalances of the European countries. These measures\nare the network (or indirect) counterparts to traditional (or direct) measures\nsuch as the trade-to-GDP (Gross Domestic Product) and trade deficit-to-GDP\nratios. Our indirect tools account for the European inter-country trade\nstructure and follow (i) a decomposition of the global trade flow into\nelementary flows that highlight the long-range dependencies between exporting\nand importing economies and (ii) the commute-time distance for trade\nintegration,which measures the impact of a perturbation in the economy of a\ncountry on another country, possibly through intermediate partners by domino\neffect. Our application addresses the impact of the launch of the Euro. We find\nthat the indirect imbalance measures better identify the countries ultimately\nbearing deficits and surpluses, by neutralizing the impact of trade transit\ncountries, such as the Netherlands. Among others, we find that ultimate\nsurpluses of Germany are quite concentrated in only three partners. We also\nshow that for some countries, the direct and indirect measures of trade\nintegration diverge, thereby revealing that these countries (e.g. Greece and\nPortugal) trade to a smaller extent with countries considered as central in the\nEuropean Union network.\n"
    },
    {
        "paper_id": 1309.4916,
        "authors": "Bruno Bouchard (CEREMADE, CREST), Ludovic Moreau, Mete H. Soner",
        "title": "Hedging under an expected loss constraint with small transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of option hedging in a market with proportional\ntransaction costs. Since super-replication is very costly in such markets, we\nreplace perfect hedging with an expected loss constraint. Asymptotic analysis\nfor small transactions is used to obtain a tractable model. A general expansion\ntheory is developed using the dynamic programming approach. Explicit formulae\nare also obtained in the special cases of an exponential or power loss\nfunction. As a corollary, we retrieve the asymptotics for the exponential\nutility indifference price.\n"
    },
    {
        "paper_id": 1309.503,
        "authors": "Mitsuaki Murota, Jun-ichi Inoue",
        "title": "Characterizing financial crisis by means of the three states random\n  field Ising model",
        "comments": "16 pages, 14 figures, using svmult.cls",
        "journal-ref": "Econophysics of Agent-based Models, New Economic Windows,\n  Springer-Verlag (Italy, Milan), pp. 83-98 (2013)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a formula of time-series prediction by means of three states\nrandom field Ising model (RFIM). At the economic crisis due to disasters or\ninternational disputes, the stock price suddenly drops. The macroscopic\nphenomena should be explained from the corresponding microscopic view point\nbecause there are existing a huge number of active traders behind the crushes.\nHence, here we attempt to model the artificial financial market in which each\ntrader $i$ can choose his/her decision among `buying', `selling' or `staying\n(taking a wait-and-see attitude)', each of which corresponds to a realization\nof the three state Ising spin, namely, $S_{i}=+1$, -1 and $S_{i}=0$,\nrespectively. The decision making of traders is given by the Gibbs-Boltzmann\ndistribution with the energy function. The energy function contains three\ndistinct terms, namely, the ferromagnetic two-body interaction term (endogenous\ninformation), random field term as external information (exogenous news), and\nchemical potential term which controls the number of traders who are watching\nthe market calmly at the instance. We specify the details of the model system\nfrom the past financial market data to determine the conjugate hyper-parameters\nand draw each parameter flow as a function of time-step. Especially we will\nexamine to what extent one can characterize the crisis by means of a brand-new\norder parameter --- `turnover' --- which is defined as the number of active\ntraders who post their decisions $S_{i}=1,-1$, instead of $S_{i}=0$.\n"
    },
    {
        "paper_id": 1309.5046,
        "authors": "Jackie Jianhong Shen",
        "title": "A Pre-Trade Algorithmic Trading Model under Given Volume Measures and\n  Generic Price Dynamics (GVM-GPD)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We make several improvements to the mean-variance framework for optimal\npre-trade algorithmic execution, by working with volume measures and generic\nprice dynamics. Volume measures are the continuum analogies for discrete volume\nprofiles commonly implemented in the execution industry. Execution then becomes\nan absolutely continuous measure over such a measure space, and its\nRadon-Nikodym derivative is commonly known as the Participation of Volume (PoV)\nfunction. The four impact cost components are all consistently built upon the\nPoV function. Some novel efforts are made for these linear impact models by\nhaving market signals more properly expressed. For the opportunistic cost, we\nare able to go beyond the conventional Brownian-type motions. By working\ndirectly with the auto-covariances of the price dynamics, we remove the\nMarkovian restriction associated with Brownians and thus allow potential memory\neffects in the price dynamics. In combination, the final execution model\nbecomes a constrained quadratic programming problem in infinite-dimensional\nHilbert spaces. Important linear constraints such as participation capping are\nall permissible. Uniqueness and existence of optimal solutions are established\nvia the theory of positive compact operators in Hilbert spaces. Several typical\nnumerical examples explain both the behavior and versatility of the model.\n"
    },
    {
        "paper_id": 1309.5053,
        "authors": "He Chen, Jun-ichi Inoue",
        "title": "Learning curve for collective behavior of zero-intelligence agents in\n  successive job-hunting processes with a diversity of Jaynes-Shannon's MaxEnt\n  principle",
        "comments": "18 pages, 9 figures",
        "journal-ref": "Evolutionary and Institutional Economics Review, Vol. 10, No. 1,\n  pp. 55-80 (2013)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by the unsupervised learning or self-organization in the machine\nlearning context, here we attempt to draw `learning curve' for the collective\nbehavior of job-seeking `zero-intelligence' labors in successive job-hunting\nprocesses. Our labor market is supposed to be opened especially for university\ngraduates in Japan, where the students have several successive chances\nn=0,1,2,... to obtain their positions within an academic (business) year. In\nthis sense, the `cumulative unemployment rate' in our model system is regarded\nas an error-measurement in the collective intelligence of students, and the\njob-hunting stage n-dependence of the error constructs a learning curve. In our\nsimple toy-model of probabilistic labor market, the diversity of students'\nbehavior is built-in by means of the Jaynes-Shannon's MaxEnt (Maximum Entropy)\nprinciple. Then, we discuss the speed of convergence for the error-measurement,\nwhere we consider a scenario in which the students do not use any information\nabout the result of job-hunting processes in the previous stage. Our approach\nenables us to examine the existence of the condition on which macroscopic\nquantity, say, `stage-wise unemployment rate' becomes `scale-invariant' in the\nsense that it does not depend on the job-hunting stage n. From the macroscopic\nview point, the problem could be regarded as a human resource allocation.\n"
    },
    {
        "paper_id": 1309.5073,
        "authors": "R\\'emy Chicheportiche",
        "title": "Non-linear dependences in finance",
        "comments": "PhD Thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula.\n"
    },
    {
        "paper_id": 1309.5094,
        "authors": "Ying Jiao, Olivier Klopfenstein and Peter Tankov",
        "title": "Hedging under multiple risk constraints",
        "comments": "29 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the asset-liability management of a nuclear power plant\noperator, we consider the problem of finding the least expensive portfolio,\nwhich outperforms a given set of stochastic benchmarks. For a specified loss\nfunction, the expected shortfall with respect to each of the benchmarks\nweighted by this loss function must remain bounded by a given threshold. We\nconsider different alternative formulations of this problem in a complete\nmarket setting, establish the relationship between these formulations, present\na general resolution methodology via dynamic programming in a non-Markovian\ncontext and give explicit solutions in special cases.\n"
    },
    {
        "paper_id": 1309.5156,
        "authors": "He Chen, Jun-ichi Inoue",
        "title": "Statistical Mechanics of Labor Markets",
        "comments": "15 pages, 7 figures using svmult.cls",
        "journal-ref": "Econophysics of systemic risk and network dynamics, New Economic\n  Windows 2013, Springer-Verlag (Italy, Milan), pp. 157-171 (2012)",
        "doi": "10.1007/978-88-470-2553-0_11",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a probabilistic model of labor markets for university graduates,\nin particular, in Japan. To make a model of the market efficiently, we take\ninto account several hypotheses. Namely, each company fixes the (business year\nindependent) number of opening positions for newcomers. The ability of\ngathering newcomers depends on the result of job matching process in past\nbusiness years. This fact means that the ability of the company is weaken if\nthe company did not make their quota or the company gathered applicants too\nmuch over the quota. All university graduates who are looking for their jobs\ncan access the public information about the ranking of companies. Assuming the\nabove essential key points, we construct the local energy function of each\ncompany and describe the probability that an arbitrary company gets students at\neach business year by a Boltzmann-Gibbs distribution. We evaluate the relevant\nphysical quantities such as the employment rate. We find that the system\nundergoes a sort of `phase transition' from the `good employment phase' to\n`poor employment phase' when one controls the degree of importance for the\nranking.\n"
    },
    {
        "paper_id": 1309.5158,
        "authors": "He Chen, Jun-ichi Inoue",
        "title": "Dynamics of probabilistic labor markets: statistical physics perspective",
        "comments": "12 pages, 5 figures using svmult.cls",
        "journal-ref": "Lecture Notes in Economics and Mathematical Systems, Vol. 662, pp.\n  53-64, \"Managing Market Complexity\", Springer (2012)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a toy probabilistic model to analyze job-matching processes in\nrecent Japanese labor markets for university graduates by means of statistical\nphysics. We show that the aggregation probability of each company is rewritten\nby means of non-linear map under several conditions. Mathematical treatment of\nthe map enables us to discuss the condition on which the rankings of arbitrary\ntwo companies are reversed during the dynamics. The so-called `mismatch'\nbetween students and companies is discussed from both empirical and theoretical\nviewpoints.\n"
    },
    {
        "paper_id": 1309.5235,
        "authors": "Christoph K\\\"uhn, Johannes Muhle-Karbe",
        "title": "Optimal Liquidity Provision",
        "comments": "22 pages, to appear in \"Stochastic Processes and Their Applications\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A small investor provides liquidity at the best bid and ask prices of a limit\norder market. For small spreads and frequent orders of other market\nparticipants, we explicitly determine the investor's optimal policy and\nwelfare. In doing so, we allow for general dynamics of the mid price, the\nspread, and the order flow, as well as for arbitrary preferences of the\nliquidity provider under consideration.\n"
    },
    {
        "paper_id": 1309.5245,
        "authors": "Thilo A. Schmitt, Desislava Chetalova, Rudi Sch\\\"afer, Thomas Guhr",
        "title": "Credit Risk and the Instability of the Financial System: an Ensemble\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1209/0295-5075/105/38004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The instability of the financial system as experienced in recent years and in\nprevious periods is often linked to credit defaults, i.e., to the failure of\nobligors to make promised payments. Given the large number of credit contracts,\nthis problem is amenable to be treated with approaches developed in statistical\nphysics. We introduce the idea of ensemble averaging and thereby uncover\ngeneric features of credit risk. We then show that the often advertised concept\nof diversification, i.e., reducing the risk by distributing it, is deeply\nflawed when it comes to credit risk. The risk of extreme losses remain due to\nthe ever present correlations, implying a substantial and persistent intrinsic\ndanger to the financial system.\n"
    },
    {
        "paper_id": 1309.5274,
        "authors": "Eric Beutner, Janina Schweizer, Antoon Pelsser",
        "title": "Fast Convergence of Regress-Later Estimates in Least Squares Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many problems in financial engineering involve the estimation of unknown\nconditional expectations across a time interval. Often Least Squares Monte\nCarlo techniques are used for the estimation. One method that can be combined\nwith Least Squares Monte Carlo is the \"Regress-Later\" method. Unlike\nconventional methods where the value function is regressed on a set of basis\nfunctions valued at the beginning of the interval, the \"Regress-Later\" method\nregresses the value function on a set of basis functions valued at the end of\nthe interval. The conditional expectation across the interval is then computed\nexactly for each basis function. We provide sufficient conditions under which\nwe derive the convergence rate of Regress-Later estimators. Importantly, our\nresults hold on non-compact sets. We show that the Regress-Later method is\ncapable of converging significantly faster than conventional methods and\nprovide an explicit example. Achieving faster convergence speed provides a\nstrong motivation for using Regress-Later methods in estimating conditional\nexpectations across time.\n"
    },
    {
        "paper_id": 1309.5466,
        "authors": "Dariusz Grech, Grzegorz Pamu{\\l}a",
        "title": "New measure of multifractality and its application in finances",
        "comments": "17 pages, 4 tables, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an alternative method for analysis of multifractal properties of\ntime series. The new approach takes into account the behaviour of the whole\nmultifractal profile of the generalized Hurst exponent $h(q)$ for all moment\norders $q$, not limited only to the edge values of $h(q)$ describing in MFDFA\nscaling properties of smallest and largest fluctuations in signal. The meaning\nof this new measure is clarified and its properties are investigated for\nsynthetic multifractal data and real signals taken from stock market. We show\nthat the proposed new measure is free of problems one can meet in real\nnonstationary signals, while searching their multifractal signatures.\n"
    },
    {
        "paper_id": 1309.5565,
        "authors": "Mohamad Houda (LMRS)",
        "title": "Call option on the maximum of the interest rate in the one factor affine\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine an explicit formula for the Laplace transform of the price of an\noption on a maximal interest rate when the instantaneous rate satisfies\nCox-Ingersoll-Ross's model. This generalizes considerably one result of\nLeblanc-Scaillet.\n"
    },
    {
        "paper_id": 1309.5703,
        "authors": "Magomet Yandiev, Alexander Pakhalov",
        "title": "The Relationship Between Stock Market Parameters and Interbank Lending\n  Market: an Empirical Evidence",
        "comments": "15 pages, 11 appendixes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article presents calculations that prove practical importance of the\nearlier derived theoretical relationship between the interest rate on the\ninterbank credit market, volume of investment and the quantity of securities\ntradable on the stock exchange.\n"
    },
    {
        "paper_id": 1309.5806,
        "authors": "Pierre Blanc, R\\'emy Chicheportiche, Jean-Philippe Bouchaud",
        "title": "The fine structure of volatility feedback II: overnight and intra-day\n  effects",
        "comments": null,
        "journal-ref": "Physica A 402 (2014) 58-75",
        "doi": "10.1016/j.physa.2014.01.047",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We decompose, within an ARCH framework, the daily volatility of stocks into\novernight and intra-day contributions. We find, as perhaps expected, that the\novernight and intra-day returns behave completely differently. For example,\nwhile past intra-day returns affect equally the future intra-day and overnight\nvolatilities, past overnight returns have a weak effect on future intra-day\nvolatilities (except for the very next one) but impact substantially future\novernight volatilities. The exogenous component of overnight volatilities is\nfound to be close to zero, which means that the lion's share of overnight\nvolatility comes from feedback effects. The residual kurtosis of returns is\nsmall for intra-day returns but infinite for overnight returns. We provide a\nplausible interpretation for these findings, and show that our\nIntra-Day/Overnight model significantly outperforms the standard ARCH framework\nbased on daily returns for Out-of-Sample predictions.\n"
    },
    {
        "paper_id": 1309.5859,
        "authors": "Giorgio Fagiolo, Marina Mastrorillo",
        "title": "Migration and Trade: A Complex-Network Approach",
        "comments": "9 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the relationships between migration and trade using a\ncomplex-network approach. We show that: (i) both weighted and binary versions\nof the networks of international migration and trade are strongly correlated;\n(ii) such correlations can be mostly explained by country economic/demographic\nsize and geographical distance; (iii) pairs of countries that are more central\nin the international-migration network trade more.\n"
    },
    {
        "paper_id": 1309.6105,
        "authors": "Aleksejus Kononovicius, Vygintas Gontis",
        "title": "Control of the socio-economic systems using herding interactions",
        "comments": "8 pages, 3 figures",
        "journal-ref": "Physica A 405, pp. 80-84, 2014",
        "doi": "10.1016/j.physa.2014.03.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Collective behavior of the complex socio-economic systems is heavily\ninfluenced by the herding, group, behavior of individuals. The importance of\nthe herding behavior may enable the control of the collective behavior of the\nindividuals. In this contribution we consider a simple agent-based herding\nmodel modified to include agents with controlled state. We show that in certain\ncase even the smallest fixed number of the controlled agents might be enough to\ncontrol the behavior of a very large system.\n"
    },
    {
        "paper_id": 1309.6141,
        "authors": "D\\\"orte Kreher",
        "title": "Change of measure up to a random time: Details",
        "comments": "revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends results of Mortimer and Williams (1991) about changes of\nprobability measure up to a random time under the assumptions that all\nmartingales are continuous and that the random time avoids stopping times. We\nconsider locally absolutely continuous measure changes up to a random time,\nchanges of probability measure up to and after an honest time, and changes of\nprobability measure up to a pseudo-stopping time. Moreover, we apply our\nresults to construct a change of probability measure that is equivalent to the\nenlargement formula and to build for a certain class of pseudo-stopping times a\nclass of measure changes that preserve the pseudo-stopping time property.\nFurthermore, we study for a price process modeled by a continuous\nsemimartingale the stability of the No Free Lunch with Vanishing Risk (NFLVR)\nproperty up to a random time, that avoids stopping times, in the progressively\nenlarged filtration and provide sufficient conditions for this stability in\nterms of the Az\\'ema supermartingale.\n"
    },
    {
        "paper_id": 1309.6164,
        "authors": "Kerry W. Fendick",
        "title": "Pricing and Hedging Derivative Securities with Unknown Local\n  Volatilities",
        "comments": "48 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A common assumption in financial engineering is that the market price for any\nderivative coincides with an objectively defined risk-neutral price - a\nplausible assumption only if traders collectively possess objective knowledge\nabout the price dynamics of the underlying security over short time scales.\nHere we assume that traders have an objective knowledge about the underlying\nsecurity's price trajectories only for large time scales. We show that\navoidance of arbitrage that is still feasible uniquely determines the prices of\noptions with large expiration times, and we derive limit theorems useful for\nestimation of model parameters and present-value analysis of derivative\nportfolios.\n"
    },
    {
        "paper_id": 1309.6287,
        "authors": "S\\'ebastien Gadat, Laurent Miclo, Fabien Panloup",
        "title": "A stochastic model for speculative bubbles",
        "comments": "53 Pages, 8 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to provide a simple modelling of speculative bubbles and\nderive some quantitative properties of its dynamical evolution. Starting from a\ndescription of individual speculative behaviours, we build and study a second\norder Markov process, which after simple transformations can be viewed as a\nturning two-dimensional Gaussian process. Then, our main problem is to ob- tain\nsome bounds for the persistence rate relative to the return time to a given\nprice. In our main results, we prove with both spectral and probabilistic\nmethods that this rate is almost proportional to the turning frequency {\\omega}\nof the model and provide some explicit bounds. In the continuity of this\nresult, we build some estimators of {\\omega} and of the pseudo-period of the\nprices. At last, we end the paper by a proof of the quasi-stationary\ndistribution of the process, as well as the existence of its persistence rate.\n"
    },
    {
        "paper_id": 1309.6505,
        "authors": "Hyong-Chol O and Ji-Sok Kim",
        "title": "General Properties of Solutions to Inhomogeneous Black-Scholes Equations\n  with Discontinuous Maturity Payoffs and Application",
        "comments": "26 pages, 4 figures. In Version 2 the section 5 is revised",
        "journal-ref": "Journal of Differential Equation, Vol.260, Issue 4, 15, Feb, 2016,\n  3151-3172",
        "doi": "10.1016/j.jde.2015.08.036",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide representations of solutions to terminal value problems of\ninhomogeneous Black-Scholes equations and studied such general properties as\nmin-max estimates, gradient estimates, monotonicity and convexity of the\nsolutions with respect to the stock price variable, which are important for\nfinancial security pricing. In particular, we focus on finding representation\nof the gradient (with respect to the stock price variable) of solutions to the\nterminal value problems with discontinuous terminal payoffs or inhomogeneous\nterms. Such terminal value problems are often encountered in pricing problems\nof compound-like options such as Bermudan options or defaultable bonds with\ndiscrete default barrier, default intensity and endogenous default recovery.\nOur results are applied in pricing defaultable discrete coupon bonds.\n"
    },
    {
        "paper_id": 1309.6725,
        "authors": "Igor Skachkov",
        "title": "Optimal Execution Trajectories. Linear Market Impact with Exponential\n  Decay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal execution of portfolio transactions is the essential part of\nalgorithmic trading. In this paper we present in simple analytical form the\noptimal trajectory for risk-averse trader with the assumption of exponential\nmarket recovery and short-time investment horizon.\n"
    },
    {
        "paper_id": 1309.6929,
        "authors": "Ventura Charlin (VC Consultants, Santiago, Chile) and Arturo Cifuentes\n  (Financial Regulation Center, Faculty of Economics and Business, University\n  of Chile, Santiago, Chile)",
        "title": "A new financial metric for the art market",
        "comments": "36 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new financial metric for the art market. The metric\nis based on the price per unit of area and is applicable to two-dimensional art\nobjects such as paintings.\n"
    },
    {
        "paper_id": 1309.7119,
        "authors": "Yanshan Wang",
        "title": "Stock price direction prediction by directly using prices data: an\n  empirical study on the KOSPI and HSI",
        "comments": "in International Journal of Business Intelligence and Data Mining,\n  2014",
        "journal-ref": null,
        "doi": "10.1504/IJBIDM.2014.065091",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The prediction of a stock market direction may serve as an early\nrecommendation system for short-term investors and as an early financial\ndistress warning system for long-term shareholders. Many stock prediction\nstudies focus on using macroeconomic indicators, such as CPI and GDP, to train\nthe prediction model. However, daily data of the macroeconomic indicators are\nalmost impossible to obtain. Thus, those methods are difficult to be employed\nin practice. In this paper, we propose a method that directly uses prices data\nto predict market index direction and stock price direction. An extensive\nempirical study of the proposed method is presented on the Korean Composite\nStock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual\nconstituents included in the indices. The experimental results show notably\nhigh hit ratios in predicting the movements of the individual constituents in\nthe KOSPI and HIS.\n"
    },
    {
        "paper_id": 1309.7222,
        "authors": "Julien Vedani (SAF), Fabien Ramaharobandro",
        "title": "Continuous compliance: a proxy-based monitoring framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the Own Risk and Solvency Assessment framework, the Solvency II\ndirective introduces the need for insurance undertakings to have efficient\ntools enabling the companies to assess the continuous compliance with\nregulatory solvency requirements. Because of the great operational complexity\nresulting from each complete evaluation of the Solvency Ratio, this monitoring\nis often complicated to implement in practice. This issue is particularly\nimportant for life insurance companies due to the high complexity to project\nlife insurance liabilities. It appears relevant in such a context to use\nparametric tools, such as Curve Fitting and Least Squares Monte Carlo in order\nto estimate, on a regular basis, the impact on the economic own funds and on\nthe regulatory capital of the company of any change over time of its underlying\nrisk factors. In this article, we first outline the principles of the\ncontinuous compliance requirement then we propose and implement a possible\nmonitoring tool enabling to approximate the eligible elements and the\nregulatory capital over time. In a final section we compare the use of the\nCurve Fitting and the Least Squares Monte Carlo methodologies in a standard\nempirical finite sample framework, and stress adapted advices for future\nproxies users.\n"
    },
    {
        "paper_id": 1309.7368,
        "authors": "Christoph K\\\"uhn, Bj\\\"orn Ulbricht",
        "title": "Modeling capital gains taxes for trading strategies of infinite\n  variation",
        "comments": "30 pages 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we show that the payment flow of a linear tax on trading\ngains from a security with a semimartingale price process can be constructed\nfor all c\\`agl\\`ad and adapted trading strategies. It is characterized as the\nunique continuous extension of the tax payments for elementary strategies\nw.r.t. the convergence \"uniformly in probability\". In this framework we prove\nthat under quite mild assumptions dividend payoffs have almost surely a\nnegative effect on investor's after-tax wealth if the riskless interest rate is\nalways positive.\n"
    },
    {
        "paper_id": 1309.7374,
        "authors": "Zbigniew Michna and Peter Nielsen",
        "title": "The impact of lead time forecasting on the bullwhip effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we quantify the bullwhip effect (the variance amplification\nin replenishment orders) when demands and lead times are predicted in a simple\ntwo-stage supply chain with one supplier and one retailer. In recent research\nthe impact of stochastic order lead time on the bullwhip effect is\ninvestigated, but the effect of needing to predict / estimate the lead time is\nnot considered in the supply chain models. Under uncertainty conditions it is\nnecessary to estimate the lead time for a member of the supply chain to place\nan order. We find a new cause of the bullwhip effect in the form of lead time\nforecasting and we give an exact form of the bullwhip effect measure (the ratio\nof variances) when demands and lead times are predicted by moving averages. In\nthe bullwhip effect measure we discover two terms amplifying the effect which\nare the result of lead time estimation\n"
    },
    {
        "paper_id": 1309.7507,
        "authors": "Qing Zhang",
        "title": "When to sell a Markov chain asset?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with an optimal stock selling rule under a Markov\nchain model. The objective is to find an optimal stopping time to sell the\nstock so as to maximize an expected return. Solutions to the associated\nvariational inequalities are obtained. Closed-form solutions are given in terms\nof a set of threshold levels. Verification theorems are provided to justify\ntheir optimality. Finally, numerical examples are reported to illustrate the\nresults.\n"
    },
    {
        "paper_id": 1309.7759,
        "authors": "Hans F\\\"ollmer, Alexander Schied",
        "title": "Probabilistic aspects of finance",
        "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP05 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1306-1326",
        "doi": "10.3150/12-BEJSP05",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the past decades, advanced probabilistic methods have had significant\nimpact on the field of finance, both in academia and in the financial industry.\nConversely, financial questions have stimulated new research directions in\nprobability. In this survey paper, we review some of these developments and\npoint to some areas that might deserve further investigation. We start by\nreviewing the basics of arbitrage pricing theory, with special emphasis on\nincomplete markets and on the different roles played by the \"real-world\"\nprobability measure and its equivalent martingale measures. We then focus on\nthe issue of model ambiguity, also called Knightian uncertainty. We present two\ncase studies in which it is possible to deal with Knightian uncertainty in\nmathematical terms. The first case study concerns the hedging of derivatives,\nsuch as variance swaps, in a strictly pathwise sense. The second one deals with\ncapital requirements and preferences specified by convex and coherent risk\nmeasures. In the final two sections we discuss mathematical issues arising from\nthe dramatic increase of algorithmic trading in modern financial markets.\n"
    },
    {
        "paper_id": 1309.7833,
        "authors": "Ale\\v{s} \\v{C}ern\\'y, Stephan Denkl, Jan Kallsen",
        "title": "Hedging in L\\'evy Models and the Time Step Equivalent of Jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider option hedging in a model where the underlying follows an\nexponential L\\'evy process. We derive approximations to the variance-optimal\nand to some suboptimal strategies as well as to their mean squared hedging\nerrors. The results are obtained by considering the L\\'evy model as a\nperturbation of the Black-Scholes model. The approximations depend on the first\nfour moments of logarithmic stock returns in the L\\'evy model and option price\nsensitivities (greeks) in the limiting Black-Scholes model. We illustrate\nnumerically that our formulas work well for a variety of L\\'evy models\nsuggested in the literature. From a theoretical point of view, it turns out\nthat jumps have a similar effect on hedging errors as discrete-time hedging in\nthe Black-Scholes model.\n"
    },
    {
        "paper_id": 1310.0032,
        "authors": "Irene Klein, Thorsten Schmidt, Josef Teichmann",
        "title": "When roll-overs do not qualify as num\\'eraire: bond markets beyond short\n  rate paradigms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate default-free bond markets where the standard relationship\nbetween a possibly existing bank account process and the term structure of bond\nprices is broken, i.e. the bank account process is not a valid num\\'eraire. We\nargue that this feature is not the exception but rather the rule in bond\nmarkets when starting with, e.g., terminal bonds as num\\'eraires.\n  Our setting are general c\\`adl\\`ag processes as bond prices, where we employ\ndirectly methods from large financial markets. Moreover, we do not restrict\nprice process to be semimartingales, which allows for example to consider\nmarkets driven by fractional Brownian motion. In the core of the article we\nrelate the appropriate no arbitrage assumptions (NAFL), i.e. no asymptotic free\nlunch, to the existence of an equivalent local martingale measure with respect\nto the terminal bond as num\\'eraire, and no arbitrage opportunities of the\nfirst kind (NAA1) to the existence of a supermartingale deflator, respectively.\nIn all settings we obtain existence of a generalized bank account as a limit of\nconvex combinations of roll-over bonds.\n  Additionally we provide an alternative definition of the concept of a\nnum\\'eraire, leading to a possibly interesting connection to bubbles. If we can\nconstruct a bank account process through roll-overs, we can relate the\nimpossibility of taking the bank account as num\\'eraire to liquidity effects.\nHere we enter endogenously the arena of multiple yield curves.\n  The theory is illustrated by several examples.\n"
    },
    {
        "paper_id": 1310.0057,
        "authors": "Dominic K. Albino, Anzi Hu, Yaneer Bar-Yam",
        "title": "Corporations and Regulators: The Game of Influence in Regulatory Capture",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a market system, regulations are designed to prevent or rectify market\nfailures that inhibit fair exchange, such as monopoly or transactions with\nhidden costs. Because regulations reduce profits to those possessing unfair\nadvantage, these advantaged corporations (whether individuals, companies, or\nother collective organizations) are motivated to influence regulators.\nRegulatory bodies created to protect the market are instead co-opted to advance\nthe interests of the corporations they are charged to regulate. This\nwide-spread influence, known as \"regulatory capture,\" has been recognized for\nover 100 years, and according to expectations of rational behavior, will exist\nwherever it is in the mutual self-interest of corporations and regulators. Here\nwe model the interaction between corporations and regulators using a new game\ntheory framework explicitly accounting for players' mutual influence, and\ndemonstrate the incentive for collusion. Communication between corporations and\nregulators enables them to collude and split the resulting profits. We identify\nwhen collusion is profitable for both parties. The intuitive results show that\ncapture occurs when the benefits to the corporation outweigh the costs to the\nregulator. Under these conditions, the corporation can compensate the regulator\nfor costs incurred and, further, provide a profit to both parties. In the real\nworld, benefits often far outweigh costs, providing large incentives to collude\nand making capture likely. Regulatory capture is inhibited by decreasing the\ninfluence between parties through strict separation, independent market\nknowledge and research by regulators, regulatory and market transparency,\nregulatory accountability for market failures, widely distributed regulatory\ncontrol, and anti-corruption enforcement.\n"
    },
    {
        "paper_id": 1310.0092,
        "authors": "Carole Bernard, Zhenyu Cui, Don McLeish",
        "title": "On the martingale property in stochastic volatility models based on\n  time-homogeneous diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lions and Musiela (2007) give sufficient conditions to verify when a\nstochastic exponential of a continuous local martingale is a martingale or a\nuniformly integrable martingale. Blei and Engelbert (2009) and Mijatovi\\'c and\nUrusov (2012c) give necessary and sufficient conditions in the case of perfect\ncorrelation (\\rho=1). For financial applications, such as checking the\nmartingale property of the stock price process in correlated stochastic\nvolatility models, we extend their work to the arbitrary correlation case\n(-1<=\\rho<=1). We give a complete classification of the convergence properties\nof integral functionals of time-homogeneous diffusions and generalize results\nin Mijatovi\\'c and Urusov (2012b) (2012c) with alternate proofs avoiding the\nuse of separating times (concept introduced by Cherny and Urusov (2004) and\nextensively used in the proofs of Mijatovi\\'c and Urusov (2012c)).\n"
    },
    {
        "paper_id": 1310.0099,
        "authors": "Carole Bernard, Zhenyu Cui, Don McLeish",
        "title": "Convergence of the discrete variance swap in time-homogeneous diffusion\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  In stochastic volatility models based on time-homogeneous diffusions, we\nprovide a simple necessary and sufficient condition for the discretely sampled\nfair strike of a variance swap to converge to the continuously sampled fair\nstrike. It extends Theorem 3.8 of Jarrow, Kchia, Larsson and Protter (2013) and\ngives an affirmative answer to a problem posed in this paper in the case of 3/2\nstochastic volatility model. We also give precise conditions (not based on\nasymptotics) when the discrete fair strike of the variance swap is higher than\nthe continuous one and discuss the convex order conjecture proposed by\nKeller-Ressel and Griessler (2012) in this context.\n"
    },
    {
        "paper_id": 1310.0762,
        "authors": "Jan A. Lipski and Ryszard Kutner",
        "title": "Agent-Based Stock Market Model with Endogenous Agents' Impact",
        "comments": "Submitted to the Journal of Economic Interaction and Coordination",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The three-state agent-based 2D model of financial markets as proposed by\nGiulia Iori has been extended by introducing increasing trust in the correctly\npredicting agents, a more realistic consultation procedure as well as a formal\nvalidation mechanism. This paper shows that such a model correctly reproduces\nthe three fundamental stylised facts: fat-tail log returns, power-law\nvolatility autocorrelation decay in time and volatility clustering.\n"
    },
    {
        "paper_id": 1310.102,
        "authors": "Stefano De Marco, Caroline Hillairet, Antoine Jacquier",
        "title": "Shapes of implied volatility with positive mass at zero",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the shapes of the implied volatility when the underlying\ndistribution has an atom at zero and analyse the impact of a mass at zero on\nat-the-money implied volatility and the overall level of the smile. We further\nshow that the behaviour at small strikes is uniquely determined by the mass of\nthe atom up to high asymptotic order, under mild assumptions on the remaining\ndistribution on the positive real line. We investigate the structural\ndifference with the no-mass-at-zero case, showing how one\ncan--theoretically--distinguish between mass at the origin and a\nheavy-left-tailed distribution. We numerically test our model-free results in\nstochastic models with absorption at the boundary, such as the CEV process, and\nin jump-to-default models. Note that while Lee's moment formula tells that\nimplied variance is at most asymptotically linear in log-strike, other\ncelebrated results for exact smile asymptotics such as Benaim and Friz (09) or\nGulisashvili (10) do not apply in this setting--essentially due to the\nbreakdown of Put-Call duality.\n"
    },
    {
        "paper_id": 1310.1102,
        "authors": "Louis Paulot",
        "title": "Arbitrage-Free Pricing Before and Beyond Probabilities",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \"Fundamental theorem of asset pricing\" roughly states that absence of\narbitrage opportunity in a market is equivalent to the existence of a\nrisk-neutral probability. We give a simple counterexample to this\noversimplified statement. Prices are given by linear forms which do not always\ncorrespond to probabilities. We give examples of such cases. We also show that\narbitrage freedom is equivalent to the continuity of the pricing linear form in\nthe relevant topology. Finally we analyze the possible loss of martingality of\nasset prices with lognormal stochastic volatility. For positive correlation\nmartingality is lost when the financial process is modelled through standard\nprobability theory. We show how to recover martingality using the appropriate\nmathematical tools.\n"
    },
    {
        "paper_id": 1310.1103,
        "authors": "Jose Blanchet and Xinyun Chen",
        "title": "Continuous-time Modeling of Bid-Ask Spread and Price Dynamics in Limit\n  Order Books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a continuous time model for the joint evolution of the mid price\nand the bid-ask spread from a multiscale analysis of the whole limit order book\n(LOB) dynamics. We model the LOB as a multiclass queueing system and perform\nour asymptotic analysis using stylized features observed empirically. We argue\nthat in the asymptotic regime supported by empirical observations the mid price\nand bid-ask-spread can be described using only certain parameters of the book\n(not the whole book itself). Our limit process is characterized by reflecting\nbehavior and state-dependent jumps. Our analysis allows to explain certain\ncharacteristics observed in practice such as: the connection between power-law\ndecaying tails in the volumes of the order book and the returns, as well as\nstatistical properties of the long-run spread distribution.\n"
    },
    {
        "paper_id": 1310.1142,
        "authors": "Anna Aksamit, Tahir Choulli, Jun Deng, and Monique Jeanblanc",
        "title": "Non-Arbitrage up to Random Horizon for Semimartingale Models",
        "comments": "40 pages. This version develops in details the ideas and the results\n  of the previous version and fixes a glitch in the quasi-left-continuous case",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the question of how an arbitrage-free semimartingale\nmodel is affected when stopped at a random horizon. We focus on\nNo-Unbounded-Profit-with-Bounded-Risk (called NUPBR hereafter) concept, which\nis also known in the literature as the first kind of non-arbitrage. For this\nnon-arbitrage notion, we obtain two principal results. The first result lies in\ndescribing the pairs of market model and random time for which the resulting\nstopped model fulfills NUPBR condition. The second main result characterises\nthe random time models that preserve the NUPBR property after stopping for any\nmarket model. These results are elaborated in a very general market model, and\nwe also pay attention to some particular and practical models. The analysis\nthat drives these results is based on new stochastic developments in\nsemimartingale theory with progressive enlargement. Furthermore, we construct\nexplicit martingale densities (deflators) for some classes of local martingales\nwhen stopped at random time.\n"
    },
    {
        "paper_id": 1310.1342,
        "authors": "Olivera Kostoska and Pece Mitrevski",
        "title": "Estimating the FDI Impact on Economic Growth and Export Performances of\n  the European Economies in Transition",
        "comments": null,
        "journal-ref": "The Young Economists Journal, Year VI, No. 11, ISSN: 1583-9982,\n  pp. 115-126, 2008",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the last two decades, Foreign Direct Investment (FDI) has been\nobserved as one of the prime instruments in the process of restructuring the\nEuropean economies in transition. Many scholars argue that FDI is expected to\nbe a source of valuable technology transfer thus might certainly have positive\neffects on host country development efforts. Nonetheless, there are no\nclear-cut findings about the FDI genuine performances in supporting the\neconomic growth, productivity and export improvements within the European\ntransition countries. Using a large and comprehensive data set, we will\ntherefore analyze the linkage between FDI and above mentioned variables, so as\nto recommend national policy appropriate measures aimed at averting negative\nand strengthening the positive FDI spillovers.\n"
    },
    {
        "paper_id": 1310.1444,
        "authors": "Ladislav Kristoufek",
        "title": "Can Google Trends search queries contribute to risk diversification?",
        "comments": "11 pages, 3 figures",
        "journal-ref": "Scientific Reports 3:2713, 2013",
        "doi": "10.1038/srep02713",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Portfolio diversification and active risk management are essential parts of\nfinancial analysis which became even more crucial (and questioned) during and\nafter the years of the Global Financial Crisis. We propose a novel approach to\nportfolio diversification using the information of searched items on Google\nTrends. The diversification is based on an idea that popularity of a stock\nmeasured by search queries is correlated with the stock riskiness. We penalize\nthe popular stocks by assigning them lower portfolio weights and we bring\nforward the less popular, or peripheral, stocks to decrease the total riskiness\nof the portfolio. Our results indicate that such strategy dominates both the\nbenchmark index and the uniformly weighted portfolio both in-sample and\nout-of-sample.\n"
    },
    {
        "paper_id": 1310.1446,
        "authors": "Ladislav Kristoufek",
        "title": "Fractal Markets Hypothesis and the Global Financial Crisis: Wavelet\n  Power Evidence",
        "comments": "12 pages, 6 figures",
        "journal-ref": "Scientific Reports 3:2857, 2013",
        "doi": "10.1038/srep02857",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We analyze whether the prediction of the fractal markets hypothesis about a\ndominance of specific investment horizons during turbulent times holds. To do\nso, we utilize the continuous wavelet transform analysis and obtained wavelet\npower spectra which give the crucial information about the variance\ndistribution across scales and its evolution in time. We show that the most\nturbulent times of the Global Financial Crisis can be very well characterized\nby the dominance of short investment horizons which is in hand with the\nassertions of the fractal markets hypothesis.\n"
    },
    {
        "paper_id": 1310.1601,
        "authors": "Ajay Singh and Dinghai Xu",
        "title": "Random Matrix Application to Correlations Among Volatility of Assets",
        "comments": "17 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we apply tools from the random matrix theory (RMT) to\nestimates of correlations across volatility of various assets in the S&P 500.\nThe volatility inputs are estimated by modeling price fluctuations as\nGARCH(1,1) process. The corresponding correlation matrix is constructed. It is\nfound that the distribution of a significant number of eigenvalues of the\nvolatility correlation matrix matches with the analytical result from the RMT.\nFurthermore, the empirical estimates of short and long-range correlations among\neigenvalues, which are within the RMT bounds, match with the analytical results\nfor Gaussian Orthogonal ensemble (GOE) of the RMT. To understand the\ninformation content of the largest eigenvectors, we estimate the contribution\nof GICS industry groups in each eigenvector. In comparison with eigenvectors of\ncorrelation matrix for price fluctuations, only few of the largest eigenvectors\nof volatility correlation matrix are dominated by a single industry group. We\nalso study correlations among `volatility return' and get similar results.\n"
    },
    {
        "paper_id": 1310.1634,
        "authors": "Fariba Karimi and Matthias Raddant",
        "title": "Cascades in real interbank markets",
        "comments": null,
        "journal-ref": "Computational Economics, January 2016, Volume 47, Issue 1, pp\n  49-66",
        "doi": "10.1007/s10614-014-9478-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze cascades of defaults in an interbank loan market. The novel\nfeature of this study is that the network structure and the size distribution\nof banks are derived from empirical data. We find that the ability of a\ndefaulted institution to start a cascade depends on an interplay of shock size\nand connectivity. Further results indicate that the ability to limit default\nrisk by spreading the lending to many counterparts decreased with the financial\ncrisis. To evaluate the influence of the network structure on market stability,\nwe compare the simulated cascades from the empirical network with results from\ndifferent randomized network models. The results show that the empirical\nnetwork has non-random features, which cannot be captured by rewired networks.\nThe analysis also reveals that simulations assuming homogeneity for size of\nbanks and loan contracts dramatically overestimates the fragility of the\ninterbank market.\n"
    },
    {
        "paper_id": 1310.1756,
        "authors": "Pietro Fodra, Huy\\^en Pham",
        "title": "High frequency trading and asymptotics for small risk aversion in a\n  Markov renewal model",
        "comments": "30 pages, new asymptotic results, typos corrected, new\n  bibliographical references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a an optimal high frequency trading problem within a market\nmicrostructure model designed to be a good compromise between accuracy and\ntractability. The stock price is driven by a Markov Renewal Process (MRP),\nwhile market orders arrive in the limit order book via a point process\ncorrelated with the stock price itself. In this framework, we can reproduce the\nadverse selection risk, appearing in two different forms: the usual one due to\nbig market orders impacting the stock price and penalizing the agent, and the\nweak one due to small market orders and reducing the probability of a\nprofitable execution. We solve the market making problem by stochastic control\ntechniques in this semi-Markov model. In the no risk-aversion case, we provide\nexplicit formula for the optimal controls and characterize the value function\nas a simple linear PDE. In the general case, we derive the optimal controls and\nthe value function in terms of the previous result, and illustrate how the risk\naversion influences the trader strategy and her expected gain. Finally, by\nusing a perturbation method, approximate optimal controls for small risk\naversions are explicitly computed in terms of two simple PDE's, reducing\ndrastically the computational cost and enlightening the financial\ninterpretation of the results.\n"
    },
    {
        "paper_id": 1310.1786,
        "authors": "Ivan Kitov, Oleg Kitov",
        "title": "Inflation, unemployment, and labour force. Phillips curves and long-term\n  projections for Austria",
        "comments": "22 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the rate of inflation and unemployment in Austria since the early\n1960s within the Phillips/Fisher framework. The change in labour force is the\ndriving force representing economic activity in the Phillips curve. For\nAustria, this macroeconomic variable was first tested as a predictor of\ninflation and unemployment in 2005 with the involved time series ended in 2003.\nHere we extend all series by nine new readings available since 2003 and\nre-estimate the previously estimated relationships between inflation,\nunemployment, and labour force. As before, a structural break is allowed in\nthese relationships, which is related to numerous changes in definitions in the\n1980s. The break year is estimated together with other model parameters by the\nBoundary Element Method with the LSQ fitting between observed and predicted\nintegral curves. The precision of inflation prediction, as described by the\nroot-mean-square (forecasting) error is by 20% to 70% better than that\nestimated by AR(1) model. The estimates of model forecasting error are\navailable for those time series where the change in labour force leads by one\n(the GDP deflator) or two (CPI) years. For the whole period between 1965 and\n2012 as well as for the intervals before and after the structural break (1986\nfor all inflation models) separately, our model is superior to the na\\\"ive\nforecasting, which in turn, is not worse than any other forecasting model. The\nlevel of statistical reliability and the predictive power of the link between\ninflation and labour force imply that the National Bank of Austria does not\ncontrol inflation and unemployment beyond revisions to definitions. The labour\nforce projection provided by Statistic Austria allows foreseeing inflation at a\nforty-year horizon: the rate of CPI inflation will hover around 1.3% and the\nGDP deflator will likely sink below zero between 2018 and 2034.\n"
    },
    {
        "paper_id": 1310.1882,
        "authors": "Erindi Allaj",
        "title": "Implicit transaction costs and the fundamental theorems of asset pricing",
        "comments": "International Journal of Theoretical and Applied Finance, 20(04) 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies arbitrage pricing theory in financial markets with\nimplicit transaction costs. We extend the existing theory to include the more\nrealistic possibility that the price at which the investors trade is dependent\non the traded volume. The investors in the market always buy at the ask and\nsell at the bid price. Implicit transaction costs are composed of two terms,\none is able to capture the bid-ask spread, and the second the price impact.\nMoreover, a new definition of a self-financing portfolio is obtained. The\nself-financing condition suggests that continuous trading is possible, but is\nrestricted to predictable trading strategies having c\\'adl\\'ag\n(right-continuous with left limits) and c\\'agl\\'ad (left-continuous with right\nlimits) paths of bounded quadratic variation and of finitely many jumps. That\nis, c\\'adl\\'ag and c\\'agl\\'ad predictable trading strategies of infinite\nvariation, with finitely many jumps and of finite quadratic variation are\nallowed in our setting. Restricting ourselves to c\\'agl\\'ad predictable trading\nstrategies, we show that the existence of an equivalent probability measure is\nequivalent to the absence of arbitrage opportunities, so that the first\nfundamental theorem of asset pricing (FFTAP) holds. It is also shown that the\nuse of continuous and bounded variation trading strategies can improve the\nefficiency of hedging in a market with implicit transaction costs. To better\nunderstand how to apply the theory proposed we provide an example of an\nimplicit transaction cost economy that is linear and non-linear in the order\nsize.\n"
    },
    {
        "paper_id": 1310.2033,
        "authors": "Thibault Jaisson, Mathieu Rosenbaum",
        "title": "Limit theorems for nearly unstable Hawkes processes",
        "comments": "Published in at http://dx.doi.org/10.1214/14-AAP1005 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 2, 600-631",
        "doi": "10.1214/14-AAP1005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Because of their tractability and their natural interpretations in term of\nmarket quantities, Hawkes processes are nowadays widely used in high-frequency\nfinance. However, in practice, the statistical estimation results seem to show\nthat very often, only nearly unstable Hawkes processes are able to fit the data\nproperly. By nearly unstable, we mean that the $L^1$ norm of their kernel is\nclose to unity. We study in this work such processes for which the stability\ncondition is almost violated. Our main result states that after suitable\nrescaling, they asymptotically behave like integrated Cox-Ingersoll-Ross\nmodels. Thus, modeling financial order flows as nearly unstable Hawkes\nprocesses may be a good way to reproduce both their high and low frequency\nstylized facts. We then extend this result to the Hawkes-based price model\nintroduced by Bacry et al. [Quant. Finance 13 (2013) 65-77]. We show that under\na similar criticality condition, this process converges to a Heston model.\nAgain, we recover well-known stylized facts of prices, both at the\nmicrostructure level and at the macroscopic scale.\n"
    },
    {
        "paper_id": 1310.222,
        "authors": "M.E.Kahil",
        "title": "Geometrization of Econophysics : An Alternative Approach for Measuring\n  Elements of Risk Management of an Economic System",
        "comments": "12 pages, LaTex file",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relationship between micro-structure and macro-structure of complex\nsystems using information geometry has been dealt by several authors. From this\nperspective, we are going to apply it as a geometrical structure connecting\nboth microeconomics and macroeconomics . The results lead us to introduce new\nmodified quantities into both micro-macro economics that enable us to describe\nthe link between them. The importance of such a scheme is to find out -with\nsome accuracy- a new method can be introduced for examining the stability of an\neconomic system. This type of requirement is expressed by examining the\nstability of the equations of path deviations for some economic systems as\ndescribed in a statistical manifold. Such a geometization scheme of economic\nsystems is an important step toward identifying risk management factors and so\ncontributes to the growing literature of econophysics.\n"
    },
    {
        "paper_id": 1310.2446,
        "authors": "Thomas Bury",
        "title": "A statistical physics perspective on criticality in financial markets",
        "comments": "23 pages, 19 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2013/11/P11004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock markets are complex systems exhibiting collective phenomena and\nparticular features such as synchronization, fluctuations distributed as\npower-laws, non-random structures and similarity to neural networks. Such\nspecific properties suggest that markets operate at a very special point.\nFinancial markets are believed to be critical by analogy to physical systems\nbut few statistically founded evidence have been given. Through a data-based\nmethodology and comparison to simulations inspired by statistical physics of\ncomplex systems, we show that the Dow Jones and indices sets are not rigorously\ncritical. However, financial systems are closer to the criticality in the crash\nneighborhood.\n"
    },
    {
        "paper_id": 1310.2567,
        "authors": "Salvador Pueyo",
        "title": "Is it a power law distribution? The case of economic contractions",
        "comments": "21 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the first steps to understand and forecast economic downturns is\nidentifying their frequency distribution, but it remains uncertain. This\nproblem is common in phenomena displaying power-law-like distributions. Power\nlaws play a central role in complex systems theory; therefore, the current\nlimitations in the identification of this distribution in empirical data are a\nmajor obstacle to pursue the insights that the complexity approach offers in\nmany fields. This paper addresses this issue by introducing a reliable\nmethodology with a solid theoretical foundation, the Taylor Series-Based Power\nLaw Range Identification Method. When applied to time series from 39 countries,\nthis method reveals a well-defined power law in the relative per capita GDP\ncontractions that span from 5.53% to 50%, comprising 263 events. However, this\nobservation does not suffice to attribute recessions to some specific\nmechanism, such as self-organized criticality. The paper highlights a set of\npoints requiring more study so as to discriminate among models compatible with\nthe power law, as needed to develop sound tools for the management of\nrecessions.\n"
    },
    {
        "paper_id": 1310.2798,
        "authors": "Timothy C. Johnson",
        "title": "Reciprocity as the foundation of Financial Economics",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1210.5390",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper argues that the fundamental principle of contemporary financial\neconomics is balanced reciprocity, not the principle of utility maximisation\nthat is important in economics more generally. The argument is developed by\nanalysing the mathematical Fundamental Theory of Asset Pricing with reference\nto the emergence of mathematical probability in the seventeenth century in the\ncontext of the ethical assessment of commercial contracts. This analysis is\nundertaken within a framework of Pragmatic philosophy and Virtue Ethics. The\npurpose of the paper is to mitigate future financial crises by reorienting\nfinancial economics to emphasise the objectives of market stability and social\ncohesion rather than individual utility maximisation.\n"
    },
    {
        "paper_id": 1310.2964,
        "authors": "Si Chen",
        "title": "Optimistic versus Pessimistic--Optimal Judgemental Bias with Reference\n  Point",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a model of reference-dependent assessment of subjective\nbeliefs in which loss-averse people optimally choose the expectation as the\nreference point to balance the current felicity from the optimistic\nanticipation and the future disappointment from the realisation. The choice of\nover-optimism or over-pessimism depends on the real chance of success and\noptimistic decision makers prefer receiving early information. In the portfolio\nchoice problem, pessimistic investors tend to trade conservatively, however,\nthey might trade aggressively if they are sophisticated enough to recognise the\nbiases since low expectation can reduce their fear of loss.\n"
    },
    {
        "paper_id": 1310.2973,
        "authors": "Jin Hyuk Choi and Kasper Larsen",
        "title": "Taylor approximation of incomplete Radner equilibrium models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the setting of exponential investors and uncertainty governed by Brownian\nmotions we first prove the existence of an incomplete equilibrium for a general\nclass of models. We then introduce a tractable class of exponential-quadratic\nmodels and prove that the corresponding incomplete equilibrium is characterized\nby a coupled set of Riccati equations. Finally, we prove that these\nexponential-quadratic models can be used to approximate the incomplete models\nwe studied in the first part.\n"
    },
    {
        "paper_id": 1310.3052,
        "authors": "Hansjoerg Albrecher and Jevgenijs Ivanovs",
        "title": "Power identities for L\\'evy risk models under taxation and capital\n  injections",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a spectrally negative L\\'evy process which is\nrefracted at its running maximum and at the same time reflected from below at a\ncertain level. Such a process can for instance be used to model an insurance\nsurplus process subject to tax payments according to a loss-carry-forward\nscheme together with the flow of minimal capital injections required to keep\nthe surplus process non-negative. We characterize the first passage time over\nan arbitrary level and the cumulative amount of injected capital up to this\ntime by their joint Laplace transform, and show that it satisfies a simple\npower relation to the case without refraction. It turns out that this identity\ncan also be extended to a certain type of refraction from below. The net\npresent value of tax collected before the cumulative injected capital exceeds a\ncertain amount is determined, and a numerical illustration is provided.\n"
    },
    {
        "paper_id": 1310.3061,
        "authors": "Stefan Gerhold, I. Cetin G\\\"ul\\\"um, Arpad Pinter",
        "title": "Small-maturity asymptotics for the at-the-money implied volatility slope\n  in L\\'evy models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the at-the-money strike derivative of implied volatility as the\nmaturity tends to zero. Our main results quantify the behavior of the slope for\ninfinite activity exponential L\\'evy models including a Brownian component. As\nauxiliary results, we obtain asymptotic expansions of short maturity\nat-the-money digital call options, using Mellin transform asymptotics. Finally,\nwe discuss when the at-the-money slope is consistent with the steepness of the\nsmile wings, as given by Lee's moment formula.\n"
    },
    {
        "paper_id": 1310.3077,
        "authors": "Peter Bank and Antje Fruth",
        "title": "Optimal Order Scheduling for Deterministic Liquidity Patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a broker who has to place a large order which consumes a sizable\npart of average daily trading volume. The broker's aim is thus to minimize\nexecution costs he incurs from the adverse impact of his trades on market\nprices. By contrast to the previous literature, see, e.g., Obizhaeva and Wang\n(2005), Predoiu, Shaikhet, and Shreve (2011), we allow the liquidity parameters\nof market depth and resilience to vary deterministically over the course of the\ntrading period. The resulting singular optimal control problem is shown to be\ntractable by methods from convex analysis and, under minimal assumptions, we\nconstruct an explicit solution to the scheduling problem in terms of some\nconcave envelope of the resilience adjusted market depth.\n"
    },
    {
        "paper_id": 1310.3083,
        "authors": "Evangelos F. Magirou",
        "title": "A note on the policy implications of the fiscal multiplier",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an elementary analysis of the dynamical aspects of the GDP /\ngovernment surplus multiplier with relevance to the assessment of a country's\ndebt repayment policy. We show the (at first) counter intuitive result that in\norder to reduce the Debt/GDP ratio, countries with high Debt to GDP should go\ninto further debt, as long as the Debt to GDP ratio is roughly greater than the\ninverse of the multiplier. Thus small values of the multiplier make further\ndebt undesirable, and conversely.\n"
    },
    {
        "paper_id": 1310.3113,
        "authors": "Peter Bank and Selim G\\\"okay",
        "title": "Superreplication when trading at market indifference prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study superreplication of European contingent claims in discrete time in a\nlarge trader model with market indifference prices recently proposed by Bank\nand Kramkov. We introduce a suitable notion of efficient friction in this\nframework, adopting a terminology introduced by Kabanov, Rasonyi, and Stricker\nin the context of models with proportional transaction costs. In our framework,\nefficient friction ensures that large positions of the investor may lead to\nlarge losses, a fact from which we derive the existence of superreplicating\nstrategies. We illustrate that without this condition there may be no\nsuperreplicating strategy with minimal costs. In our main result, we establish\nefficient friction under a tail condition on the conditional distributions of\nthe traded securities and under an asymptotic criterion on risk aversions of\nthe market makers. Another result asserts that strict monotonicity of the\nconditional essential infima and suprema of the security prices is sufficient\nfor efficient friction. We give examples that satisfy the assumptions in our\nconditions, which include non-degenerate finite sample space models as well as\nLevy processes and an affine stochastic volatility model of\nBarndorff-Nielsen-Shepard type.\n"
    },
    {
        "paper_id": 1310.3347,
        "authors": "Takashi Kato, Jun Sekine, Kenichi Yoshikawa",
        "title": "Order Estimates for the Exact Lugannani-Rice Expansion",
        "comments": "32 pages, 9 figures",
        "journal-ref": "Japan Journal of Industrial and Applied Mathematics, Vol.33(1),\n  pp.25-61, 2016",
        "doi": "10.1007/s13160-015-0199-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Lugannani-Rice formula is a saddlepoint approximation method for\nestimating the tail probability distribution function, which was originally\nstudied for the sum of independent identically distributed random variables.\nBecause of its tractability, the formula is now widely used in practical\nfinancial engineering as an approximation formula for the distribution of a\n(single) random variable. In this paper, the Lugannani-Rice approximation\nformula is derived for a general, parametrized sequence of random variables and\nthe order estimates of the approximation are given.\n"
    },
    {
        "paper_id": 1310.3386,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Regulatory-Optimal Funding",
        "comments": "20 pages; 8 figures; 2 tables, Risk, April 2014",
        "journal-ref": "Risk, 2014, 27(4), 64-69",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Funding is a cost to trading desks that they see as an input. Current\nFVA-related literature reflects this by also taking funding costs as an input,\nusually constant, and always risk-neutral. However, this funding curve is the\noutput from a Treasury point of view. Treasury must consider\nRegulatory-required liquidity buffers, and both risk-neutral (Q) and physical\nmeasures (P). We describe the Treasury funding problem and optimize against\nboth measures, using the Regulatory requirement as a constraint. We develop\ntheoretically optimal strategies for Q and P, then demonstrate a combined\napproach in four markets (USD, JPY, EUR, GBP). Since we deal with physical\nmeasures we develop appropriate statistical tests, and demonstrate highly\nsignificant (p<0.00001), out-of-sample, improvements on hedged funding with a\ncombined approach achieving 44% to 71% of a perfect information criterion. Thus\nregulatory liquidity requirements change both the funding problem and funding\ncosts.\n"
    },
    {
        "paper_id": 1310.3396,
        "authors": "Thomas Schmelzer and Raphael Hauser",
        "title": "Seven Sins in Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although modern portfolio theory has been in existence for over 60 years,\nfund managers often struggle to get its models to produce reliable portfolio\nallocations without strongly constraining the decision vector by tight bands of\nstrategic allocation targets. The two main root causes to this problem are\ninadequate parameter estimation and numerical artifacts. When both obstacles\nare overcome, portfolio models yield excellent allocations. In this paper,\nwhich is primarily aimed at practitioners, we discuss the most common mistakes\nin setting up portfolio models and in solving them algorithmically.\n"
    },
    {
        "paper_id": 1310.3397,
        "authors": "Thomas Schmelzer, Raphael Hauser, Erling Andersen and Joachim Dahl",
        "title": "Regression techniques for Portfolio Optimisation using MOSEK",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regression is widely used by practioners across many disciplines. We\nreformulate the underlying optimisation problem as a second-order conic program\nproviding the flexibility often needed in applications. Using examples from\nportfolio management and quantitative trading we solve regression problems with\nand without constraints. Several Python code fragments are given. The code and\ndata are available online at http://www.github.com/tschm/MosekRegression.\n"
    },
    {
        "paper_id": 1310.3572,
        "authors": "Ankush Agarwal",
        "title": "Asymptotic expansion for characteristic function in Heston stochastic\n  volatility model with fast mean-reverting correction",
        "comments": "6 pages. arXiv admin note: text overlap with arXiv:1007.4366 by other\n  authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we derive the characteristic function expansion for logarithm\nof the underlying asset price in corrected Heston model as proposed by Fouque\nand Lorig.\n"
    },
    {
        "paper_id": 1310.3694,
        "authors": "Christian Bender, Nikolaus Schweizer, Jia Zhuo",
        "title": "A primal-dual algorithm for BSDEs",
        "comments": null,
        "journal-ref": "Mathematical Finance, Vol. 27, 866-901, 2017",
        "doi": "10.1111/mafi.12100",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize the primal-dual methodology, which is popular in the pricing of\nearly-exercise options, to a backward dynamic programming equation associated\nwith time discretization schemes of (reflected) backward stochastic\ndifferential equations (BSDEs). Taking as an input some approximate solution of\nthe backward dynamic program, which was pre-computed, e.g., by least-squares\nMonte Carlo, our methodology allows to construct a confidence interval for the\nunknown true solution of the time discretized (reflected) BSDE at time 0. We\nnumerically demonstrate the practical applicability of our method in two\nfive-dimensional nonlinear pricing problems where tight price bounds were\npreviously unavailable.\n"
    },
    {
        "paper_id": 1310.3716,
        "authors": "Paolo Sgrignoli, Rodolfo Metulini, Stefano Schiavo, Massimo Riccaboni",
        "title": "The Relation Between Global Migration and Trade Networks",
        "comments": "16 pages, 3 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 417,\n  1 January 2015, Pages 245-260",
        "doi": "10.1016/j.physa.2014.09.037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a methodology to analyze and compare multiple global\nnetworks. We focus our analysis on the relation between human migration and\ntrade. First, we identify the subset of products for which the presence of a\ncommunity of migrants significantly increases trade intensity. To assure\ncomparability across networks, we apply a hypergeometric filter to identify\nlinks for which migration and trade intensity are both significantly higher\nthan expected. Next we develop an econometric methodology, inspired by spatial\neconometrics, to measure the effect of migration on international trade while\ncontrolling for network interdependencies. Overall, we find that migration\nsignificantly boosts trade across sectors and we are able to identify product\ncategories for which this effect is particularly strong.\n"
    },
    {
        "paper_id": 1310.386,
        "authors": "Hongzhong Zhang, Tim Leung, Olympia Hadjiliadis",
        "title": "Stochastic Modeling and Fair Valuation of Drawdown Insurance",
        "comments": "25 pages, 6 figures. Insurance: Mathematics and Economics,\n  Forthcoming 2013",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2013.10.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the stochastic modeling of market drawdown events and the\nfair valuation of insurance contracts based on drawdowns. We model the asset\ndrawdown process as the current relative distance from the historical maximum\nof the asset value. We first consider a vanilla insurance contract whereby the\nprotection buyer pays a constant premium over time to insure against a drawdown\nof a pre-specified level. This leads to the analysis of the conditional Laplace\ntransform of the drawdown time, which will serve as the building block for\ndrawdown insurance with early cancellation or drawup contingency. For the\ncancellable drawdown insurance, we derive the investor's optimal cancellation\ntiming in terms of a two-sided first passage time of the underlying drawdown\nprocess. Our model can also be applied to insure against a drawdown by a\ndefaultable stock. We provide analytic formulas for the fair premium and\nillustrate the impact of default risk.\n"
    },
    {
        "paper_id": 1310.3984,
        "authors": "Ladislav Kristoufek",
        "title": "Measuring correlations between non-stationary series with DCCA\n  coefficient",
        "comments": "10 pages",
        "journal-ref": "Physica A: Statistical Mechanics and Its Applications 402, pp.\n  291-298, 2014",
        "doi": "10.1016/j.physa.2014.01.058",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  In this short report, we investigate the ability of the DCCA coefficient to\nmeasure correlation level between non-stationary series. Based on a wide Monte\nCarlo simulation study, we show that the DCCA coefficient can estimate the\ncorrelation coefficient accurately regardless the strength of non-stationarity\n(measured by the fractional differencing parameter $d$). For a comparison, we\nalso report the results for the standard Pearson's correlation coefficient. The\nDCCA coefficient dominates the Pearson's coefficient for non-stationary series.\n"
    },
    {
        "paper_id": 1310.4067,
        "authors": "D.L. Wilcox and T.J.Gebbie",
        "title": "On pricing kernels, information and risk",
        "comments": "20 pages, 3 figures, 1 table",
        "journal-ref": "Investment Analysts Journal, 2015, Vol 44, No 1, 1-19",
        "doi": "10.1080/10293523.2014.994437",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the finding that cross-sectional characteristic based models have\nyielded portfolios with higher excess monthly returns but lower risk than their\narbitrage pricing theory counterparts in an analysis of equity returns of\nstocks listed on the JSE. Under the assumption of general no-arbitrage\nconditions, we argue that evidence in favour of characteristic based pricing\nimplies that information is more likely assimilated by means of nonlinear\npricing kernels for the markets considered.\n"
    },
    {
        "paper_id": 1310.4142,
        "authors": "Liviu-Adrian Cotfas and Nicolae Cotfas",
        "title": "Quantum harmonic oscillator in option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Scholes model anticipates rather well the observed prices for\noptions in the case of a strike price that is not too far from the current\nprice of the underlying asset. Some useful extensions can be obtained by an\nadequate modification of the coefficients in the Black-Scholes equation. We\ninvestigate from a mathematical point of view an extension directly related to\nthe quantum harmonic oscillator. In the considered case, the solution is the\nsum of a series involving the Hermite-Gauss functions. A finite-dimensional\nversion is obtained by using a finite oscillator and the Harper functions. This\nsimplified model keeps the essential characteristics of the continuous one and\nuses finite sums instead of series and integrals.\n"
    },
    {
        "paper_id": 1310.4403,
        "authors": "J.F. Mercure and H. Pollitt and U. Chewpreecha and P. Salas and A.\n  Foley and P. B. Holden and N. R. Edwards",
        "title": "Complexity, economic science and possible economic benefits of climate\n  change mitigation policy",
        "comments": "13 pages and 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conventional economic analysis of stringent climate change mitigation policy\ngenerally concludes various levels of economic slowdown as a result of\nsubstantial spending on low carbon technology. Equilibrium economics however\ncould not explain or predict the current economic crisis, which is of financial\nnature. Meanwhile the economic impacts of climate policy find their source\nthrough investments for the diffusion of environmental innovations, in parts a\nfinancial problem. Here, we expose how results of economic analysis of climate\nchange mitigation policy depend entirely on assumptions and theory concerning\nthe finance of the diffusion of innovations, and that in many cases, results\nare simply re-iterations of model assumptions. We show that, while equilibrium\neconomics always predict economic slowdown, methods using non-equilibrium\napproaches suggest the opposite could occur. We show that the solution to\nunderstanding the economic impacts of reducing greenhouse gas emissions lies\nwith research on the dynamics of the financial sector interacting with\ninnovation and technology developments, economic history providing powerful\ninsights through important analogies with previous historical waves of\ninnovation.\n"
    },
    {
        "paper_id": 1310.4471,
        "authors": "Aur\\'elien Alfonsi, Alexander Schied, Florian Kl\\\"ock",
        "title": "Multivariate transient price impact and matrix-valued positive definite\n  functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model for linear transient price impact for multiple assets\nthat takes cross-asset impact into account. Our main goal is to single out\nproperties that need to be imposed on the decay kernel so that the model admits\nwell-behaved optimal trade execution strategies. We first show that the\nexistence of such strategies is guaranteed by assuming that the decay kernel\ncorresponds to a matrix-valued positive definite function. An example\nillustrates, however, that positive definiteness alone does not guarantee that\noptimal strategies are well-behaved. Building on previous results from the\none-dimensional case, we investigate a class of nonincreasing, nonnegative and\nconvex decay kernels with values in the symmetric $K\\times K$ matrices. We show\nthat these decay kernels are always positive definite and characterize when\nthey are even strictly positive definite, a result that may be of independent\ninterest. Optimal strategies for kernels from this class are well-behaved when\none requires that the decay kernel is also commuting. We show how such decay\nkernels can be constructed by means of matrix functions and provide a number of\nexamples. In particular we completely solve the case of matrix exponential\ndecay.\n"
    },
    {
        "paper_id": 1310.4538,
        "authors": "Martin Gremm",
        "title": "The Origin of Fat Tails",
        "comments": "17 Pages, 11 Figures, typos corrected",
        "journal-ref": "International Journal of Theoretical and Applied Finance, Vol. 18,\n  No. 8, 2015",
        "doi": "10.1142/S0219024915500545",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a random walk model of asset returns where the parameters depend\non market stress. Stress is measured by, e.g., the value of an implied\nvolatility index. We show that model parameters including standard deviations\nand correlations can be estimated robustly and that all distributions are\napproximately normal. Fat tails in observed distributions occur because time\nseries sample different stress levels and therefore different normal\ndistributions. This provides a quantitative description of the observed\ndistribution including the fat tails. We discuss simple applications in risk\nmanagement and portfolio construction.\n"
    },
    {
        "paper_id": 1310.4539,
        "authors": "Gianbiagio Curato, Fabrizio Lillo",
        "title": "Modeling the coupled return-spread high frequency dynamics of large tick\n  assets",
        "comments": "28 pages, 13 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2015/01/P01028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large tick assets, i.e. assets where one tick movement is a significant\nfraction of the price and bid-ask spread is almost always equal to one tick,\ndisplay a dynamics in which price changes and spread are strongly coupled. We\nintroduce a Markov-switching modeling approach for price change, where the\nlatent Markov process is the transition between spreads. We then use a finite\nMarkov mixture of logit regressions on past squared returns to describe the\ndependence of the probability of price changes. The model can thus be seen as a\nDouble Chain Markov Model. We show that the model describes the shape of return\ndistribution at different time aggregations, volatility clustering, and the\nanomalous decrease of kurtosis of returns. We calibrate our models on Nasdaq\nstocks and we show that this model reproduces remarkably well the statistical\nproperties of real data.\n"
    },
    {
        "paper_id": 1310.4783,
        "authors": "Matyas Barczy, Gyula Pap",
        "title": "Asymptotic properties of maximum likelihood estimators for Heston models\n  based on continuous time observations",
        "comments": "44 pages. Title has been changed",
        "journal-ref": "Statistics 50 (2), 2016, 389-417",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study asymptotic properties of maximum likelihood estimators for Heston\nmodels based on continuous time observations of the log-price process. We\ndistinguish three cases: subcritical (also called ergodic), critical and\nsupercritical. In the subcritical case, asymptotic normality is proved for all\nthe parameters, while in the critical and supercritical cases, non-standard\nasymptotic behavior is described.\n"
    },
    {
        "paper_id": 1310.4994,
        "authors": "Cheng Li and Hao Xing",
        "title": "Asymptotic Glosten Milgrom equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the Glosten Milgrom model whose risky asset value admits\nan arbitrary discrete distribution. Contrast to existing results on insider's\nmodels, the insider's optimal strategy in this model, if exists, is not of\nfeedback type. Therefore a weak formulation of equilibrium is proposed. In this\nweak formulation, the inconspicuous trade theorem still holds, but the\noptimality for the insider's strategy is not enforced. However, the insider can\nemploy some feedback strategy whose associated expected profit is close to the\noptimal value, when the order size is small. Moreover this discrepancy\nconverges to zero when the order size diminishes. The existence of such a weak\nequilibrium is established, in which the insider's strategy converges to the\nKyle optimal strategy when the order size goes to zero.\n"
    },
    {
        "paper_id": 1310.5114,
        "authors": "Thomas Gueudr\\'e and Alexander Dobrinevski and Jean-Philippe Bouchaud",
        "title": "Explore or exploit? A generic model and an exactly solvable case",
        "comments": "5 pages 2 figures",
        "journal-ref": "Phys. Rev. Lett. 112, 050602 (2014)",
        "doi": "10.1103/PhysRevLett.112.050602",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finding a good compromise between the exploitation of known resources and the\nexploration of unknown, but potentially more profitable choices, is a general\nproblem, which arises in many different scientific disciplines. We propose a\nstylized model for these exploration-exploitation situations, including\npopulation or economic growth, portfolio optimisation, evolutionary dynamics,\nor the problem of optimal pinning of vortices or dislocations in disordered\nmaterials. We find the exact growth rate of this model for tree-like geometries\nand prove the existence of an optimal migration rate in this case. Numerical\nsimulations in the one-dimensional case confirm the generic existence of an\noptimum.\n"
    },
    {
        "paper_id": 1310.5306,
        "authors": "Panagiotis Papaioannnou, Lucia Russo, George Papaioannou, Constantinos\n  Siettos",
        "title": "Can social microblogging be used to forecast intraday exchange rates?",
        "comments": "This is a prior version of the paper published at NETNOMICS. The\n  final publication is available at\n  http://www.springer.com/economics/economic+theory/journal/11066",
        "journal-ref": "Netnomics, 14, 47-68 (2013)",
        "doi": "10.1007/s11066-013-9079-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Efficient Market Hypothesis (EMH) is widely accepted to hold true under\ncertain assumptions. One of its implications is that the prediction of stock\nprices at least in the short run cannot outperform the random walk model. Yet,\nrecently many studies stressing the psychological and social dimension of\nfinancial behavior have challenged the validity of the EMH. Towards this aim,\nover the last few years, internet-based communication platforms and search\nengines have been used to extract early indicators of social and economic\ntrends. Here, we used Twitter's social networking platform to model and\nforecast the EUR/USD exchange rate in a high-frequency intradaily trading\nscale. Using time series and trading simulations analysis, we provide some\nevidence that the information provided in social microblogging platforms such\nas Twitter can in certain cases enhance the forecasting efficiency regarding\nthe very short (intradaily) forex.\n"
    },
    {
        "paper_id": 1310.5388,
        "authors": "Leonidas Sandoval Junior",
        "title": "Structure and causality relations in a global network of financial\n  companies",
        "comments": null,
        "journal-ref": "Entropy 16 (2014) 4443-4482",
        "doi": "10.3390/e16084443",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work uses the stocks of the 197 largest companies in the world, in terms\nof market capitalization, in the financial area in the study of causal\nrelationships between them using Transfer Entropy, which is calculated using\nthe stocks of those companies and their counterparts lagged by one day. With\nthis, we can assess which companies influence others according to sub-areas of\nthe financial sector, which are banks, diversified financial services, savings\nand loans, insurance, private equity funds, real estate investment companies,\nand real estate trust funds. We also analyzed the causality relations between\nthose stocks and the network formed by them based on this measure, verifying\nthat they cluster mainly according to countries of origin, and then by industry\nand sub-industry. Then we collected data on the stocks of companies in the\nfinancial sector of some countries that are suffering the most with the current\ncredit crisis: Greece, Cyprus, Ireland, Spain, Portugal, and Italy, and assess,\nalso using transfer entropy, which companies from the largest 197 are most\naffected by the stocks of these countries in crisis. The intention is to map a\nnetwork of influences that may be used in the study of possible contagions\noriginating in those countries in financial crisis.\n"
    },
    {
        "paper_id": 1310.554,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Frequency Effects on Predictability of Stock Returns",
        "comments": "8 pages, 16 figures, submitted for possible publication to\n  Computational Intelligence for Financial Engineering and Economics 2014\n  conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose that predictability is a prerequisite for profitability on\nfinancial markets. We look at ways to measure predictability of price changes\nusing information theoretic approach and employ them on all historical data\navailable for NYSE 100 stocks. This allows us to determine whether frequency of\nsampling price changes affects the predictability of those. We also relations\nbetween price changes predictability and the deviation of the price formation\nprocesses from iid as well as the stock's sector. We also briefly comment on\nthe complicated relationship between predictability of price changes and the\nprofitability of algorithmic trading.\n"
    },
    {
        "paper_id": 1310.6025,
        "authors": "Iosif Pinelis",
        "title": "An optimal three-way stable and monotonic spectrum of bounds on\n  quantiles: a spectrum of coherent measures of financial risk and economic\n  inequality",
        "comments": null,
        "journal-ref": "Risks, 2(3):349--392 (September 2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A certain spectrum, indexed by a\\in[0,\\infty], of upper bounds P_a(X;x) on\nthe tail probability P(X\\geq x), with P_0(X;x)=P(X\\geq x) and P_\\infty(X;x)\nbeing the best possible exponential upper bound on P(X\\geq x), is shown to be\nstable and monotonic in a, x, and X, where x is a real number and X is a random\nvariable. The bounds P_a(X;x) are optimal values in certain minimization\nproblems. The corresponding spectrum, also indexed by a\\in[0,\\infty], of upper\nbounds Q_a(X;p) on the (1-p)-quantile of X is stable and monotonic in a, p, and\nX, with Q_0(X;p) equal the largest (1-p)-quantile of X. In certain sense, the\nquantile bounds Q_a(X;p) are usually close enough to the true quantiles\nQ_0(X;p). Moreover, Q_a(X;p) is subadditive in X if a\\geq 1, as well as\npositive-homogeneous and translation-invariant, and thus is a so-called\ncoherent measure of risk. A number of other useful properties of the bounds\nP_a(X;x) and Q_a(X;p) are established. In particular, quite similarly to the\nbounds P_a(X;x) on the tail probabilities, the quantile bounds Q_a(X;p) are the\noptimal values in certain minimization problems. This allows for a\ncomparatively easy incorporation of the bounds P_a(X;x) and Q_a(X;p) into more\nspecialized optimization problems. It is shown that the minimization problems\nfor which P_a(X;x) and Q_a(X;p) are the optimal values are in a certain sense\ndual to each other; in the case a=\\infty this corresponds to the bilinear\nLegendre--Fenchel duality. In finance, the (1-p)-quantile Q_0(X;p) is known as\nthe value-at-risk (VaR), whereas the value of Q_1(X;p) is known as the\nconditional value-at-risk (CVaR) and also as the expected shortfall (ES),\naverage value-at-risk (AVaR), and expected tail loss (ETL). It is shown that\nthe quantile bounds Q_a(X;p) can be used as measures of economic inequality.\nThe spectrum parameter, a, may be considered an index of sensitivity to\nrisk/inequality.\n"
    },
    {
        "paper_id": 1310.632,
        "authors": "Mathieu Moslonka-Lefebvre, Herv\\'e Monod, Christopher A. Gilligan,\n  Elisabeta Vergu and Jo\\~ao A. N. Filipe",
        "title": "Epidemics in markets with trade friction and imperfect transactions",
        "comments": "51 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market trade-routes can support infectious-disease transmission, impacting\nbiological populations and even disrupting causal trade. Epidemiological models\nincreasingly account for reductions in infectious contact, such as\nrisk-aversion behaviour in response to pathogen outbreaks. However, market\ndynamics clearly differ from simple risk-aversion, as are driven by different\nmotivation and conditioned by trade constraints, known in economics as\nfriction, that arise because exchanges are costly. Here we develop a novel\neconomic-market model where transient and long-term market dynamics are\ndetermined by trade friction and agent adaptation, and can influence disease\ntransmission. We specify the participants, frequency, volume, and price in\ntrade transactions, and investigate, using analytical insights and simulation,\nhow trade friction affects joint market and epidemiological dynamics. The\nfriction values explored encompass estimates from French cattle and pig\nmarkets. We show that, when trade is the dominant route of transmission, market\nfriction can be a significantly stronger determinant of epidemics than\nrisk-aversion behaviour. In particular, there is a critical friction level\nabove which epidemics do not occur. For a given level of friction, open\nunregulated markets can boost epidemics compared with closed or tightly\nregulated markets. Our results are robust to model specificities and can hold\nin the presence of non-trade disease-transmission routes. In particular, we try\nto explain why outbreaks in French livestock markets appear more frequently in\ncattle than swine despite swine trade-flow being larger. To minimize contagion\nin markets, safety policies could generate incentives for larger-volume,\nless-frequent transactions, increasing trade friction without necessarily\naffecting overall trade flow.\n"
    },
    {
        "paper_id": 1310.6486,
        "authors": "Antoaneta Sergueiva",
        "title": "Systemic Risk Identification, Modelling, Analysis, and Monitoring: An\n  Integrated Approach",
        "comments": "The author is grateful to J Doyne Farmer and Yaneer Bar-Yam for their\n  constructive comments and time, and to Kevin James for the opportunity to\n  attend and present at the Bank of England seminars on systemic risk and\n  financial stability. Would like to thank Jeffrey Johnson for kindly providing\n  a copy of his forthcoming book in advance, and to Marzena Rostek for sending\n  a recent unpublished",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research capacity is critical in understanding systemic risk and informing\nnew regulation. Banking regulation has not kept pace with all the complexities\nof financial innovation. The academic literature on systemic risk is rapidly\nexpanding. The majority of papers analyse a single source or a consolidated\nsource of risk and its effect. A fraction of publications quantify systemic\nrisk measures or formulate penalties for systemically important financial\ninstitutions that are of practical regulatory relevance. The challenges facing\nsystemic risk evaluation and regulation still persist, as the definition of\nsystemic risk is somewhat unsettled and that affects attempts to provide\nsolutions. Our understanding of systemic risk is evolving and the awareness of\ndata relevance is rising gradually; this challenge is reflected in the focus of\nmajor international research initiatives. There is a consensus that the direct\nand indirect costs of a systemic crisis are enormous as opposed to preventing\nit, and that without regulation the externalities will not be prevented; but\nthere is no consensus yet on the extent and detail of regulation, and research\nexpectations are to facilitate the regulatory process. This report outlines an\nintegrated approach for systemic risk evaluation based on multiple types of\ninterbank exposures through innovative modelling approaches as tensorial\nmultilayer networks, suggests how to relate underlying economic data and how to\nextend the network to cover financial market information. We reason about data\nrequirements and time scale effects, and outline a multi-model hypernetwork of\nsystemic risk knowledge as a scenario analysis and policy support tool. The\nargument is that logical steps forward would incorporate the range of risk\nsources and their interrelated effects as contributions towards an overall\nsystemic risk indicator, would perform an integral analysis of ...\n"
    },
    {
        "paper_id": 1310.6526,
        "authors": "Lancelot F. James, Dohyun Kim and Zhiyuan Zhang",
        "title": "Exact simulation pricing with Gamma processes and their extensions",
        "comments": "Forthcoming The Journal of Computational Finance",
        "journal-ref": "Journal of Computational Finance, 17(2013), 3-39",
        "doi": "10.21314/JCF.2013.259",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exact path simulation of the underlying state variable is of great practical\nimportance in simulating prices of financial derivatives or their sensitivities\nwhen there are no analytical solutions for their pricing formulas. However, in\ngeneral, the complex dependence structure inherent in most nontrivial\nstochastic volatility (SV) models makes exact simulation difficult. In this\npaper, we present a nontrivial SV model that parallels the notable Heston SV\nmodel in the sense of admitting exact path simulation as studied by Broadie and\nKaya. The instantaneous volatility process of the proposed model is driven by a\nGamma process. Extensions to the model including superposition of independent\ninstantaneous volatility processes are studied. Numerical results show that the\nproposed model outperforms the Heston model and two other L\\'evy driven SV\nmodels in terms of model fit to the real option data. The ability to exactly\nsimulate some of the path-dependent derivative prices is emphasized. Moreover,\nthis is the first instance where an infinite-activity volatility process can be\napplied exactly in such pricing contexts.\n"
    },
    {
        "paper_id": 1310.6819,
        "authors": "Yiran Sheng",
        "title": "Valuing FtD Contract under Copula Approach via Monte-Carlo Stimulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article aims to discuss some basics in field of credit modeling,\nspecifically the pricing issue of FtD contract. We demonstrate how the popular\ncopula approach is used in pricing FtD contract, and give a stimulation example\nof such practice based on SAS 9.1.\n"
    },
    {
        "paper_id": 1310.6822,
        "authors": "Yiran Sheng, Ruokun Huang",
        "title": "Optimal Choice under Short Sell Limit with Sharpe Ratio as Criterion\n  among Multiple Assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is the term paper of the course Investments. We mainly focus on\nmodeling long-term investment decisions of a typical utility-maximizing\nindividual, with features of Chinese stock market in perspective. We adopt an\nOR based methodology with market information as input parameters to carry out\nthe solution. Two main features of this article are: first, we take the no\nshort-sell constraint in Chinese stock market into consideration and use an\napproach otherwise identical to Markowitz to work out the optimal portfolio\nchoice; this method has critical and practical implication to Chinese\ninvestors. Second, we incorporate the benefits of multiple assets into one\nsingle well-defined utility function and use a MIQP procedure to derive the\noptimal allocation of funds upon each of them along the time-line.\n"
    },
    {
        "paper_id": 1310.6873,
        "authors": "Thomas R. Hurd, Davide Cellai, Sergey Melnik, Quentin Shao",
        "title": "Double Cascade Model of Financial Crises",
        "comments": "28 pages, 7 figures",
        "journal-ref": "International Journal of Theoretical and Applied Finance 19,\n  1650041 (2016)",
        "doi": "10.1142/S0219024916500412",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The scope of financial systemic risk research encompasses a wide range of\ninterbank channels and effects, including asset correlation shocks, default\ncontagion, illiquidity contagion, and asset fire sales. This paper introduces a\nfinancial network model that combines the default and liquidity stress\nmechanisms into a \"double cascade mapping\". The progress and eventual result of\nthe crisis is obtained by iterating this mapping to its fixed point. Unlike\nsimpler models, this model can therefore quantify how illiquidity or default of\none bank influences the overall level of liquidity stress and default in the\nsystem. Large-network asymptotic cascade mapping formulas are derived that can\nbe used for efficient network computations of the double cascade. Numerical\nexperiments then demonstrate that these asymptotic formulas agree qualitatively\nwith Monte Carlo results for large finite networks, and quantitatively except\nwhen the initial system is placed in an exceptional \"knife-edge\" configuration.\nThe experiments clearly support the main conclusion that when banks respond to\nliquidity stress by hoarding liquidity, then in the absence of asset fire\nsales, the level of defaults in a financial network is negatively related to\nthe strength of bank liquidity hoarding and the eventual level of stress in the\nnetwork.\n"
    },
    {
        "paper_id": 1310.7018,
        "authors": "Rafal Rak, Stanislaw Drozdz, Jaroslaw Kwapien, Pawel Oswiecimka",
        "title": "Stock returns versus trading volume: is the correspondence more general?",
        "comments": null,
        "journal-ref": "Acta Phys. Pol. B, 44 (2013) 2035-2050",
        "doi": "10.5506/APhysPolB.44.2035",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a quantitative analysis of the relationship between the\nstock market returns and corresponding trading volumes using high- frequency\ndata from the Polish stock market. First, for stocks that were traded for\nsuffciently long period of time, we study the return and volume distributions\nand identify their consistency with the power-law functions. We find that, for\nmajority of stocks, the scaling exponents of both distri- butions are\nsystematically related by about a factor of 2 with the ones for the returns\nbeing larger. Second, we study the empirical price impact of trades of a given\nvolume and find that this impact can be well described by a square-root\ndependence: r(V) V^(1/2). We conclude that the prop- erties of data from the\nPolish market resemble those reported in literature concerning certain mature\nmarkets.\n"
    },
    {
        "paper_id": 1310.7128,
        "authors": "Lorenzo Giada and Claudio Nordio",
        "title": "Restructuring the \"one-way CSA\" counterparty risk in a CDO",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how to restructure the counterparty risk faced by the originator of a\nsecuritization or covered bond arising from an interest rate hedging swap\nassisted by a \"one-way\" collateral agreement. This risk emerges when the swap\nis negotiated between the special purpose vehicle and a third party that covers\nitself through a back-to-back swap with the originator. We show that the\ncounterparty risk of the originator may be removed by adding a chain of\nback-to-back credit derivatives between the three parties (originator,\ncounterparty and vehicle).\n"
    },
    {
        "paper_id": 1310.728,
        "authors": "Peter Bank and Dmitry Kramkov",
        "title": "The stochastic field of aggregate utilities and its saddle conjugate",
        "comments": "62 pages. arXiv admin note: substantial text overlap with\n  arXiv:1110.3224, arXiv:1110.3229",
        "journal-ref": "Tr. Mat. Inst. Steklova, 2014, Volume 287, Pages 21-60",
        "doi": "10.1134/S0371968514040037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe the sample paths of the stochastic field $F = F_t(v,x,q)$ of\naggregate utilities parameterized by Pareto weights $v$ and total cash amounts\n$x$ and stocks' quantities $q$ in an economy. We also describe the sample paths\nof the stochastic field $G = G_t(u,y,q)$, which is conjugate to $F$ with\nrespect to the saddle arguments $(v,x)$, and obtain various conjugacy relations\nbetween these stochastic fields. The results of this paper play a key role in\nour study of a continuous-time price impact model.\n"
    },
    {
        "paper_id": 1310.7857,
        "authors": "Christian Bender, Mikko S. Pakkanen, Hasanjan Sayit",
        "title": "Sticky continuous processes have consistent price systems",
        "comments": "10 pages, v3: incorporates minor corrections and the proof of the\n  main result has been clarified, to appear in Journal of Applied Probability",
        "journal-ref": "Journal of Applied Probability 2015, Vol. 52, No. 2, 586-594",
        "doi": "10.1239/jap/1437658617",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under proportional transaction costs, a price process is said to have a\nconsistent price system, if there is a semimartingale with an equivalent\nmartingale measure that evolves within the bid-ask spread. We show that a\ncontinuous, multi-asset price process has a consistent price system, under\narbitrarily small proportional transaction costs, if it satisfies a natural\nmulti-dimensional generalization of the stickiness condition introduced by\nGuasoni [Math. Finance 16(3), 569-582 (2006)].\n"
    },
    {
        "paper_id": 1310.8169,
        "authors": "Thomas Bury",
        "title": "Predicting trend reversals using market instantaneous state",
        "comments": "18 pages, 15 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.02.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Collective behaviours taking place in financial markets reveal strongly\ncorrelated states especially during a crisis period. A natural hypothesis is\nthat trend reversals are also driven by mutual influences between the different\nstock exchanges. Using a maximum entropy approach, we find coordinated\nbehaviour during trend reversals dominated by the pairwise component. In\nparticular, these events are predicted with high significant accuracy by the\nensemble's instantaneous state.\n"
    },
    {
        "paper_id": 1310.8296,
        "authors": "Hyong-chol O, Yong-hwa Ro and Ning Wan",
        "title": "The Use of Numeraires in Multi-dimensional Black-Scholes Partial\n  Differential Equations",
        "comments": "22 pages, This is English translation of the paper\n  \"KISU-MATH-2008-Cn-R-005\" with same title written in Chinese\n  (http://ssrn.com/abstract=731544) and version 2 revised references",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2348594",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The change of numeraire gives very important computational simplification in\noption pricing. This technique reduces the number of sources of risks that need\nto be accounted for and so it is useful in pricing complicated derivatives that\nhave several sources of risks. In this article, we considered the underlying\nmathematical theory of numeraire technique in the viewpoint of PED theory and\nillustrated it with five concrete pricing problems. In the viewpoint of PED\ntheory, the numeraire technique is a method of reducing the dimension of status\nspaces where PDE is defined.\n"
    },
    {
        "paper_id": 1310.8431,
        "authors": "Viktor Zharkov",
        "title": "Multiagent's model of stock market with p-adic description of prices",
        "comments": "26 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1210.1022; and text overlap with arXiv:1203.4979 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new multiagent model of the stock market is formulated that contains four\nstates in which the agents may be located. Next, the model is reformulated in\nthe language of the functional integral containing fluctuations of prices and\nquantities of cash flows. It is shown that in the functional integral of that\ntype description of the prices is given not by the real numbers as is made in\nmany papers but the p-adic numbers. It is shown in the following simple\nexamples extracted from the proposed theory that the p-adic description of\nprices gives good description of fractal behavior of the trends. The formula is\ngiven for the p-adic mapping of prices. Using this formula we obtain the main\np-adic patterns which are the same as the patterns of Elliott wave theory, this\nfact allows us to give a rigorous mathematical proof of this theory. Our model\nis the only model that gives a strict point like description of the fractal\nbehavior of prices. The developed approach opens the possibility to give the\nformulation of p-adic technical analysis of the stock market.\n"
    },
    {
        "paper_id": 1310.8604,
        "authors": "Matias Leppisaari",
        "title": "Modeling catastrophic deaths using EVT with a microsimulation approach\n  to reinsurance pricing",
        "comments": "32 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, a marked Poisson process (MPP) model for life catastrophe risk was\nproposed in [6]. We provide a justification and further support for the model\nby considering more general Poisson point processes in the context of extreme\nvalue theory (EVT), and basing the choice of model on statistical tests and\nmodel comparisons. A case study examining accidental deaths in the Finnish\npopulation is provided.\n  We further extend the applicability of the catastrophe risk model by\nconsidering small and big accidents separately; the resulting combined MPP\nmodel can flexibly capture the whole range of accidental death counts. Using\nthe proposed model, we present a simulation framework for pricing (life)\ncatastrophe reinsurance, based on modeling the underlying policies at\nindividual contract level. The accidents are first simulated at population\nlevel, and their effect on a specific insurance company is then determined by\nexplicitly simulating the resulting insured deaths. The proposed\nmicrosimulation approach can potentially lead to more accurate results than the\ntraditional methods, and to a better view of risk, as it can make use of all\nthe information available to the re/insurer and can explicitly accommodate even\ncomplex re/insurance terms and product features. As an example we price several\nexcess reinsurance contracts. The proposed simulation model is also suitable\nfor solvency assessment.\n"
    },
    {
        "paper_id": 1311.0118,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Regulatory-Compliant Derivatives Pricing is Not Risk-Neutral",
        "comments": "12 pages; 3 figures, Risk, September 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regulations impose idiosyncratic capital and funding costs for holding\nderivatives. Capital requirements are costly because derivatives desks are\nrisky businesses; funding is costly in part because regulations increase the\nminimum funding tenor. Idiosyncratic costs mean no single measure makes\nderivatives martingales for all market participants. Hence Regulatory-compliant\npricing is not risk-neutral. This has implications for exit prices and\nmark-to-market.\n"
    },
    {
        "paper_id": 1311.0236,
        "authors": "Arianna Agosto, Enrico Moretto",
        "title": "Variance matters (in stochastic dividend discount models)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic dividend discount models (Hurley and Johnson, 1994 and 1998, Yao,\n1997) present expressions for the expected value of stock prices when future\ndividends evolve according to some random scheme. In this paper we try to offer\na more precise view on this issue proposing a closed-form formula for the\nvariance of stock prices.\n"
    },
    {
        "paper_id": 1311.027,
        "authors": "Marie Kratz",
        "title": "There is a VaR beyond usual approximations",
        "comments": "33 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Basel II and Solvency 2 both use the Value-at-Risk (VaR) as the risk measure\nto compute the Capital Requirements. In practice, to calibrate the VaR, a\nnormal approximation is often chosen for the unknown distribution of the yearly\nlog returns of financial assets. This is usually justified by the use of the\nCentral Limit Theorem (CLT), when assuming aggregation of independent and\nidentically distributed (iid) observations in the portfolio model. Such a\nchoice of modeling, in particular using light tail distributions, has proven\nduring the crisis of 2008/2009 to be an inadequate approximation when dealing\nwith the presence of extreme returns; as a consequence, it leads to a gross\nunderestimation of the risks. The main objective of our study is to obtain the\nmost accurate evaluations of the aggregated risks distribution and risk\nmeasures when working on financial or insurance data under the presence of\nheavy tail and to provide practical solutions for accurately estimating high\nquantiles of aggregated risks. We explore a new method, called Normex, to\nhandle this problem numerically as well as theoretically, based on properties\nof upper order statistics. Normex provides accurate results, only weakly\ndependent upon the sample size and the tail index. We compare it with existing\nmethods.\n"
    },
    {
        "paper_id": 1311.0354,
        "authors": "Assa Hirbod, Morales Manuel and Omidi Firouzi Hassan",
        "title": "On the Capital Allocation Problem for a New Coherent Risk Measure in\n  Collective Risk Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new coherent cumulative risk measure on\n$\\mathcal{R}_L^p$, the space of c\\`adl\\`ag processes having Laplace transform.\nThis new coherent risk measure turns out to be tractable enough within a class\nof models where the aggregate claims is driven by a spectrally positive L\\'evy\nprocess. Moreover, we study the problem of capital allocation in an insurance\ncontext and we show that the capital allocation problem for this risk measure\nhas a unique solution determined by the Euler allocation method. Some examples\nare provided.\n"
    },
    {
        "paper_id": 1311.0414,
        "authors": "Ted Theodosopoulos",
        "title": "On Agents and Equilibria",
        "comments": "6 pages, essay inspired by discussions at the 18th Annual Workshop on\n  Economic Science with Heterogeneous Interacting Agents in Reykjavik",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This essay discusses the advantages of a probabilistic agent-based approach\nto questions in theoretical economics, from the nature of economic agents, to\nthe nature of the equilibria supported by their interactions. One idea we\npropose is that \"agents\" are meta-individual, hierarchically structured\nobjects, that include as irreducible components groupings of different\ndimensions. We also explore the effects of non-ergodicity, by constructing a\nsimple stochastic model for the contingent nature of economic interactions.\n"
    },
    {
        "paper_id": 1311.0498,
        "authors": "Konstantinos Spiliopoulos, Richard B. Sowers",
        "title": "Default Clustering in Large Pools: Large Deviations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study large deviations and rare default clustering events in a dynamic\nlarge heterogeneous portfolio of interconnected components. Defaults come as\nPoisson events and the default intensities of the different components in the\nsystem interact through the empirical default rate and via systematic effects\nthat are common to all components. We establish the large deviations principle\nfor the empirical default rate for such an interacting particle system. The\nrate function is derived in an explicit form that is amenable to numerical\ncomputations and derivation of the most likely path to failure for the system\nitself. Numerical studies illustrate the theoretical findings. An understanding\nof the role of the preferred paths to large default rates and the most likely\nways in which contagion and systematic risk combine to lead to large default\nrates would give useful insights into how to optimally safeguard against such\nevents.\n"
    },
    {
        "paper_id": 1311.053,
        "authors": "K. Triantafyllopoulos",
        "title": "Multivariate stochastic volatility modelling using Wishart\n  autoregressive processes",
        "comments": "29 pages, 3 figures, 2 tables",
        "journal-ref": "Journal of Time Series Analysis, 2012, 33, 48-60",
        "doi": "10.1111/j.1467-9892.2011.00738.x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new multivariate stochastic volatility estimation procedure for financial\ntime series is proposed. A Wishart autoregressive process is considered for the\nvolatility precision covariance matrix, for the estimation of which a two step\nprocedure is adopted. The first step is the conditional inference on the\nautoregressive parameters and the second step is the unconditional inference,\nbased on a Newton-Raphson iterative algorithm. The proposed methodology, which\nis mostly Bayesian, is suitable for medium dimensional data and it bridges the\ngap between closed-form estimation and simulation-based estimation algorithms.\nAn example, consisting of foreign exchange rates data, illustrates the proposed\nmethodology.\n"
    },
    {
        "paper_id": 1311.0657,
        "authors": "Ladislav Kristoufek",
        "title": "Detrending moving-average cross-correlation coefficient: Measuring\n  cross-correlations between non-stationary series",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.03.015",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  In the paper, we introduce a new measure of correlation between possibly\nnon-stationary series. As the measure is based on the detrending moving-average\ncross-correlation analysis (DMCA), we label it as the DMCA coefficient\n$\\rho_{DMCA}(\\lambda)$ with a moving average window length $\\lambda$. We\nanalytically show that the coefficient ranges between -1 and 1 as a standard\ncorrelation does. In the simulation study, we show that the values of\n$\\rho_{DMCA}(\\lambda)$ very well correspond to the true correlation between the\nanalyzed series regardless the (non-)stationarity level. Dependence of the\nnewly proposed measure on other parameters -- correlation level, moving average\nwindow length and time series length -- is discussed as well.\n"
    },
    {
        "paper_id": 1311.0675,
        "authors": "Nikolai Dokuchaev",
        "title": "On strong binomial approximation for stochastic processes and\n  applications for financial modelling",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers binomial approximation of continuous time stochastic\nprocesses. It is shown that, under some mild integrability conditions, a\nprocess can be approximated in mean square sense and in other strong metrics by\nbinomial processes, i.e., by processes with fixed size binary increments at\nsampling points. Moreover, this approximation can be causal, i.e., at every\ntime it requires only past historical values of the underlying process. In\naddition, possibility of approximation of solutions of stochastic differential\nequations by solutions of ordinary equations with binary noise is established.\nSome consequences for the financial modelling and options pricing models are\ndiscussed.\n"
    },
    {
        "paper_id": 1311.0688,
        "authors": "Francesca Biagini, Alessandro Gnoatto, Maximilian H\\\"artel",
        "title": "Affine HJM Framework on $S_{d}^{+}$ and Long-Term Yield",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop the HJM framework for forward rates driven by affine processes on\nthe state space of symmetric positive matrices. In this setting we find a\nrepresentation for the long-term yield and investigate the yield's asymptotic\nbehaviour.\n"
    },
    {
        "paper_id": 1311.1097,
        "authors": "Ivan Kitov, Oleg Kitov",
        "title": "Does Banque de France control inflation and unemployment?",
        "comments": "25 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We re-estimate statistical properties and predictive power of a set of\nPhillips curves, which are expressed as linear and lagged relationships between\nthe rates of inflation, unemployment, and change in labour force. For France,\nseveral relationships were estimated eight years ago. The change rate of labour\nforce was used as a driving force of inflation and unemployment within the\nPhillips curve framework. The set of nested models starts with a simplistic\nversion without autoregressive terms and one lagged term of explanatory\nvariable. The lag is determined empirically together with all coefficients. The\nmodel is estimated using the Boundary Element Method (BEM) with the least\nsquares method applied to the integral solutions of the differential equations.\nAll models include one structural break might be associated with revisions to\ndefinitions and measurement procedures in the 1980s and 1990s as well as with\nthe change in monetary policy in 1994-1995. For the GDP deflator, our original\nmodel provided a root mean squared forecast error (RMSFE) of 1.0% per year at a\nfour-year horizon for the period between 1971 and 2004. The rate of CPI\ninflation is predicted with RMSFE=1.5% per year. For the naive (no change)\nforecast, RMSFE at the same time horizon is 2.95% and 3.3% per year,\nrespectively. Our model outperforms the naive one by a factor of 2 to 3. The\nrelationships for inflation were successfully tested for cointegration. We have\nformally estimated several vector error correction (VEC) models for two\nmeasures of inflation. At a four year horizon, the estimated VECMs provide\nsignificant statistical improvements on the results obtained by the BEM:\nRMSFE=0.8% per year for the GDP deflator and ~1.2% per year for CPI. For a two\nyear horizon, the VECMs improve RMSFEs by a factor of 2, with the smallest\nRMSFE=0.5% per year for the GDP deflator.\n"
    },
    {
        "paper_id": 1311.1122,
        "authors": "Rodrigue Oeuvray and Pascal Junod",
        "title": "On time scaling of semivariance in a jump-diffusion process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to examine the time scaling of the semivariance when\nreturns are modeled by various types of jump-diffusion processes, including\nstochastic volatility models with jumps in returns and in volatility. In\nparticular, we derive an exact formula for the semivariance when the volatility\nis kept constant, explaining how it should be scaled when considering a lower\nfrequency. We also provide and justify the use of a generalization of the\nBall-Torous approximation of a jump-diffusion process, this new model appearing\nto deliver a more accurate estimation of the downside risk. We use Markov Chain\nMonte Carlo (MCMC) methods to fit our stochastic volatility model. For the\ntests, we apply our methodology to a highly skewed set of returns based on the\nBarclays US High Yield Index, where we compare different time scalings for the\nsemivariance. Our work shows that the square root of the time horizon seems to\nbe a poor approximation in the context of semivariance and that our methodology\nbased on jump-diffusion processes gives much better results.\n"
    },
    {
        "paper_id": 1311.1154,
        "authors": "Kim Song Yon, Kim Mun Chol",
        "title": "Modeling of Volatility with Non-linear Time Series Model",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, non-linear time series models are used to describe volatility\nin financial time series data. To describe volatility, two of the non-linear\ntime series are combined into form TAR (Threshold Auto-Regressive Model) with\nAARCH (Asymmetric Auto-Regressive Conditional Heteroskedasticity) error term\nand its parameter estimation is studied.\n"
    },
    {
        "paper_id": 1311.1535,
        "authors": "Mireille Bossy and Nadia Maizi and Odile Pourtallier",
        "title": "Nash equilibrium for coupling of CO2 allowances and electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we present an existence result of a Nash equilibrium between\nelectricity producers selling their production on an electricity market and\nbuying CO2 emission allowances on an auction carbon market. The producers'\nstrategies integrate the coupling of the two markets via the cost functions of\nthe electricity production. We set out a clear Nash equilibrium that can be\nused to compute equilibrium prices on both markets as well as the related\nelectricity produced and CO2 emissions covered.\n"
    },
    {
        "paper_id": 1311.1545,
        "authors": "Stefano De Marco, Peter Friz",
        "title": "Varadhan's formula, conditioned diffusions, and local volatilities",
        "comments": "34 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by marginals-mimicking results for It\\^o processes via SDEs and by\ntheir applications to volatility modeling in finance, we discuss the weak\nconvergence of the law of a hypoelliptic diffusions conditioned to belong to a\ntarget affine subspace at final time, namely $\\mathcal{L}(Z_t|Y_t = y)$ if\n$X_{\\cdot}=(Y_\\cdot,Z_{\\cdot})$. To do so, we revisit Varadhan-type estimates\nin a small-noise regime (as opposed to small-time), studying the density of the\nlower-dimensional component $Y$. The application to stochastic volatility\nmodels include the small-time and, for certain models, the large-strike\nasymptotics of the Gyongy-Dupire's local volatility function. The final product\nare asymptotic formulae that can (i) motivate parameterizations of the local\nvolatility surface and (ii) be used to extrapolate local volatilities in a\ngiven model.\n"
    },
    {
        "paper_id": 1311.1562,
        "authors": "Wei He and Yeneng Sun",
        "title": "Stationary Markov Perfect Equilibria in Discounted Stochastic Games",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existence of stationary Markov perfect equilibria in stochastic games is\nshown under a general condition called \"(decomposable) coarser transition\nkernels\". This result covers various earlier existence results on correlated\nequilibria, noisy stochastic games, stochastic games with finite actions and\nstate-independent transitions, and stochastic games with mixtures of constant\ntransition kernels as special cases. A remarkably simple proof is provided via\nestablishing a new connection between stochastic games and conditional\nexpectations of correspondences. New applications of stochastic games are\npresented as illustrative examples, including stochastic games with endogenous\nshocks and a stochastic dynamic oligopoly model.\n"
    },
    {
        "paper_id": 1311.1715,
        "authors": "Ren Liu, Johannes Muhle-Karbe",
        "title": "Portfolio Choice with Stochastic Investment Opportunities: a User's\n  Guide",
        "comments": "31 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey reviews portfolio choice in settings where investment\nopportunities are stochastic due to, e.g., stochastic volatility or return\npredictability. It is explained how to heuristically compute candidate optimal\nportfolios using tools from stochastic control, and how to rigorously verify\ntheir optimality by means of convex duality. Special emphasis is placed on\nlong-horizon asymptotics, that lead to particularly tractable results.\n"
    },
    {
        "paper_id": 1311.1924,
        "authors": "Mel MacMahon, Diego Garlaschelli",
        "title": "Community detection for correlation matrices",
        "comments": "Final version, accepted for publication on PRX",
        "journal-ref": "Physical Review X 5, 021006 (2015)",
        "doi": "10.1103/PhysRevX.5.021006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A challenging problem in the study of complex systems is that of resolving,\nwithout prior information, the emergent, mesoscopic organization determined by\ngroups of units whose dynamical activity is more strongly correlated internally\nthan with the rest of the system. The existing techniques to filter\ncorrelations are not explicitly oriented towards identifying such modules and\ncan suffer from an unavoidable information loss. A promising alternative is\nthat of employing community detection techniques developed in network theory.\nUnfortunately, this approach has focused predominantly on replacing network\ndata with correlation matrices, a procedure that tends to be intrinsically\nbiased due to its inconsistency with the null hypotheses underlying the\nexisting algorithms. Here we introduce, via a consistent redefinition of null\nmodels based on random matrix theory, the appropriate correlation-based\ncounterparts of the most popular community detection techniques. Our methods\ncan filter out both unit-specific noise and system-wide dependencies, and the\nresulting communities are internally correlated and mutually anti-correlated.\nWe also implement multiresolution and multifrequency approaches revealing\nhierarchically nested sub-communities with `hard' cores and `soft' peripheries.\nWe apply our techniques to several financial time series and identify\nmesoscopic groups of stocks which are irreducible to a standard, sectorial\ntaxonomy, detect `soft stocks' that alternate between communities, and discuss\nimplications for portfolio optimization and risk management.\n"
    },
    {
        "paper_id": 1311.2273,
        "authors": "V. A. Kalyagin, A. P. Koldanov, P. A. Koldanov, P. M. Pardalos, V. A.\n  Zamaraev",
        "title": "Measures of uncertainty in market network analysis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.06.054",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Statistical uncertainty of different filtration techniques for market network\nanalysis is studied. Two measures of statistical uncertainty are discussed. One\nis based on conditional risk for multiple decision statistical procedures and\nanother one is based on average fraction of errors. It is shown that for some\nimportant cases the second measure is a particular case of the first one.\nStatistical uncertainty for some popular market network structures is analyzed.\nResults of numerical evaluation of statistical uncertainty for minimum spanning\ntree, market graph, maximum cliques and maximum independent sets are given. The\nmost stable structures are derived.\n"
    },
    {
        "paper_id": 1311.2278,
        "authors": "Juan Luis Lopez and Jesus Guillermo Contreras",
        "title": "Performance of multifractal detrended fluctuation analysis on short time\n  series",
        "comments": "9 pages, 8 figures",
        "journal-ref": "Phys. Rev. E 87, 022918 (2013)",
        "doi": "10.1103/PhysRevE.87.022918",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The performance of the multifractal detrended analysis on short time series\nis evaluated for synthetic samples of several mono- and multifractal models.\nThe reconstruction of the generalized Hurst exponents is used to determine the\nrange of applicability of the method and the precision of its results as a\nfunction of the decreasing length of the series. As an application the series\nof the daily exchange rate between the U.S. dollar and the euro is studied.\n"
    },
    {
        "paper_id": 1311.2511,
        "authors": "M. Andrecut",
        "title": "Spin Glasses and Nonlinear Constraints in Portfolio Optimization",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physleta.2013.12.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the portfolio optimization problem with the obligatory deposits\nconstraint. Recently it has been shown that as a consequence of this nonlinear\nconstraint, the solution consists of an exponentially large number of optimal\nportfolios, completely different from each other, and extremely sensitive to\nany changes in the input parameters of the problem, making the concept of\nrational decision making questionable. Here we reformulate the problem using a\nquadratic obligatory deposits constraint, and we show that from the physics\npoint of view, finding an optimal portfolio amounts to calculating the\nmean-field magnetizations of a random Ising model with the constraint of a\nconstant magnetization norm. We show that the model reduces to an eigenproblem,\nwith 2N solutions, where N is the number of assets defining the portfolio.\nAlso, in order to illustrate our results, we present a detailed numerical\nexample of a portfolio of several risky common stocks traded on the Nasdaq\nMarket.\n"
    },
    {
        "paper_id": 1311.255,
        "authors": "Mads Nielsen",
        "title": "The Kelly growth optimal strategy with a stop-loss rule",
        "comments": "13 pages, 4 figures. Submitted to Quantitative Finance 29 May 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  From the Hamilton-Jacobi-Bellman equation for the value function we derive a\nnon-linear partial differential equation for the optimal portfolio strategy\n(the dynamic control). The equation is general in the sense that it does not\ndepend on the terminal utility and provides additional analytical insight for\nsome optimal investment problems with known solutions. Furthermore, when\nboundary conditions for the optimal strategy can be established independently,\nit is considerably simpler than the HJB to solve numerically. Using this method\nwe calculate the Kelly growth optimal strategy subject to a periodically reset\nstop-loss rule.\n"
    },
    {
        "paper_id": 1311.3019,
        "authors": "Masahiko Egami and Tadao Oryu",
        "title": "An Excursion-Theoretic Approach to Regulator's Bank Reorganization\n  Problem",
        "comments": "16 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The importance of the global financial system cannot be exaggerated. When a\nlarge financial institution becomes problematic and is bailed out, that bank is\noften claimed as \"too big to fail\". On the other hand, to prevent bank's\nfailure, regulatory authorities adopt the Prompt Corrective Action (PCA)\nagainst a bank that violates certain criteria, often measured by its leverage\nratio. In this article, we provide a framework where one can analyze the cost\nand effect of PCA's. We model a large bank with deteriorating asset and\nregulatory actions attempting to prevent a failure. The model uses the\nexcursion theory of Levy processes and finds an optimal leverage ratio that\ntriggers a PCA. A nice feature includes it incorporates the fact that social\ncost associated with PCA's are be greatly affected by the size of banks subject\nto PCA's, so that one can see the cost of rescuing a bank \"too big to fail\".\n"
    },
    {
        "paper_id": 1311.3529,
        "authors": "Sigrid Kallblad, Jan Obloj and Thaleia Zariphopoulou",
        "title": "Time--consistent investment under model uncertainty: the robust forward\n  criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We combine forward investment performance processes and ambiguity averse\nportfolio selection. We introduce the notion of robust forward criteria which\naddresses the issues of ambiguity in model specification and in preferences and\ninvestment horizon specification. It describes the evolution of time-consistent\nambiguity averse preferences.\n  We first focus on establishing dual characterizations of the robust forward\ncriteria. This offers various advantages as the dual problem amounts to a\nsearch for an infimum whereas the primal problem features a saddle-point. Our\napproach is based on ideas developed in Schied (2007) and Zitkovic (2009). We\nthen study in detail non-volatile criteria. In particular, we solve explicitly\nthe example of an investor who starts with a logarithmic utility and applies a\nquadratic penalty function. The investor builds a dynamical estimate of the\nmarket price of risk $\\hat \\lambda$ and updates her stochastic utility in\naccordance with the so-perceived elapsed market opportunities. We show that\nthis leads to a time-consistent optimal investment policy given by a fractional\nKelly strategy associated with $\\hat \\lambda$. The leverage is proportional to\nthe investor's confidence in her estimate $\\hat \\lambda$.\n"
    },
    {
        "paper_id": 1311.3764,
        "authors": "Abhijnan Rej",
        "title": "Modeling systemic risks in financial markets",
        "comments": "9 pages, discussion paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We survey systemic risks to financial markets and present a high-level\ndescription of an algorithm that measures systemic risk in terms of coupled\nnetworks.\n"
    },
    {
        "paper_id": 1311.3871,
        "authors": "Hongli Zeng, R\\'emi Lemoy and Mikko Alava",
        "title": "Financial interaction networks inferred from traded volumes",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2014/07/P07008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to use the advanced inference techniques available for Ising models,\nwe transform complex data (real vectors) into binary strings, by local\naveraging and thresholding. This transformation introduces parameters, which\nmust be varied to characterize the behaviour of the system. The approach is\nillustrated on financial data, using three inference methods -- equilibrium,\nsynchronous and asynchronous inference -- to construct functional connections\nbetween stocks. We show that the traded volume information is enough to obtain\nwell known results about financial markets, which use however the presumably\nricher price information: collective behaviour (\"market mode\") and strong\ninteractions within industry sectors. Synchronous and asynchronous Ising\ninference methods give results which are coherent with equilibrium ones, and\nmore detailed since the obtained interaction networks are directed.\n"
    },
    {
        "paper_id": 1311.3881,
        "authors": "Samy Jazaerli and Yuri F. Saporito",
        "title": "Functional Ito Calculus, Path-dependence and the Computation of Greeks",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": "10.1016/j.spa.2017.03.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dupire's functional It\\^o calculus provides an alternative approach to the\nclassical Malliavin calculus for the computation of sensitivities, also called\nGreeks, of path-dependent derivatives prices. In this paper, we introduce a\nmeasure of path-dependence of functionals within the functional It\\^o calculus\nframework. Namely, we consider the Lie bracket of the space and time functional\nderivatives, which we use to classify functionals accordingly to their degree\nof path-dependence. We then revisit the problem of efficient numerical\ncomputation of Greeks for path-dependent derivatives using integration by parts\ntechniques. Special attention is paid to path-dependent functionals with zero\nLie bracket, called locally weakly path-dependent functionals in our\nclassification. Hence, we derive the weighted-expectation formulas for their\nGreeks. In the more general case of fully path-dependent functionals, we show\nthat, equipped with the functional It\\^o calculus, we are able to analyze the\neffect of the Lie bracket on the computation of Greeks. Moreover, we are also\nable to consider the more general dynamics of path-dependent volatility. These\nwere not achieved using Malliavin calculus.\n"
    },
    {
        "paper_id": 1311.4057,
        "authors": "Th\\'eophile Griveau-Billion, Jean-Charles Richard and Thierry Roncalli",
        "title": "A Fast Algorithm for Computing High-dimensional Risk Parity Portfolios",
        "comments": "9 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a cyclical coordinate descent (CCD) algorithm for\nsolving high dimensional risk parity problems. We show that this algorithm\nconverges and is very fast even with large covariance matrices (n > 500).\nComparison with existing algorithms also shows that it is one of the most\nefficient algorithms.\n"
    },
    {
        "paper_id": 1311.4068,
        "authors": "Jaume Masoliver, Miquel Montero, Josep Perell\\'o, John Geanakoplos, J.\n  Doyne Farmer",
        "title": "Uncertain growth and the value of the future",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For environmental problems such as global warming future costs must be\nbalanced against present costs. This is traditionally done using an exponential\nfunction with a constant discount rate, which reduces the present value of\nfuture costs. The result is highly sensitive to the choice of discount rate and\nhas generated a major controversy as to the urgency for immediate action. We\nstudy analytically several standard interest rate models from finance and\ncompare their properties to empirical data. From historical time series for\nnominal interest rates and inflation covering 14 countries over hundreds of\nyears, we find that extended periods of negative real interest rates are\ncommon, occurring in many epochs in all countries. This leads us to choose the\nOrnstein-Uhlenbeck model, in which real short run interest rates fluctuate\nstochastically and can become negative, even if they revert to a positive mean\nvalue. We solve the model in closed form and prove that the long-run discount\nrate is always less than the mean; indeed it can be zero or even negative,\ndespite the fact that the mean short term interest rate is positive. We fit the\nparameters of the model to the data, and find that nine of the countries have\npositive long run discount rates while five have negative long-run discount\nrates. Even if one rejects the countries where hyperinflation has occurred, our\nresults support the low discounting rate used in the Stern report over higher\nrates advocated by others.\n"
    },
    {
        "paper_id": 1311.4074,
        "authors": "Wenqing Bao, ChunLi Chen and Jin E. Zhang",
        "title": "Option Pricing with Lie Symmetry Analysis and Similarity Reduction\n  Method",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With some transformations, we convert the problem of option pricing under\nstate-dependent volatility into an initial value problem of the Fokker-Planck\nequation with a certain potential. By using the Lie symmetry analysis and\nsimilarity reduction method, we are able to reduce the dimensions of the\npartial differential equation and find some of its particular solutions of the\nequation. A few case studies demonstrate that our new method can be used to\nproduce analytical option pricing formulas for certain volatility functions.\n"
    },
    {
        "paper_id": 1311.4078,
        "authors": "Vincent Vargas, Tung-Lam Dao, Jean-Philippe Bouchaud",
        "title": "Skew and implied leverage effect: smile dynamics revisited",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the ``Smile Dynamics'' problem, which consists in relating the\nimplied leverage (i.e. the correlation of the at-the-money volatility with the\nreturns of the underlying) and the skew of the option smile. The ratio between\nthese two quantities, called ``Skew-Stickiness Ratio'' (SSR) by Bergomi (Smile\nDynamics IV, RISK, 94-100, December 2009), saturates to the value 2 for linear\nmodels in the limit of small maturities, and converges to 1 for long\nmaturities. We show that for more general, non-linear models (such as the\nasymmetric GARCH model), Bergomi's result must be modified, and can be larger\nthan 2 for small maturities. The discrepancy comes from the fact that the\nvolatility skew is, in general, different from the skewness of the underlying.\nWe compare our theory with empirical results, using data both from option\nmarkets and from the underlying price series, for the S&P500 and the DAX. We\nfind, among other things, that although both the implied leverage and the skew\nappear to be too strong on option markets, their ratio is well explained by the\ntheory. We observe that the SSR indeed becomes larger than 2 for small\nmaturities.\n"
    },
    {
        "paper_id": 1311.416,
        "authors": "Benjamin Myers and Austin Gerig",
        "title": "Simulating the Synchronizing Behavior of High-Frequency Trading in\n  Multiple Markets",
        "comments": "7 pages, 4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nearly one-half of all trades in financial markets are executed by\nhigh-speed, autonomous computer programs -- a type of trading often called\nhigh-frequency trading (HFT). Although evidence suggests that HFT increases the\nefficiency of markets, it is unclear how or why it produces this outcome. Here\nwe create a simple model to study the impact of HFT on investors who trade\nsimilar securities in different markets. We show that HFT can improve liquidity\nby allowing more transactions to take place without adversely affecting pricing\nor volatility. In the model, HFT synchronizes the prices of the securities,\nwhich allows buyers and sellers to find one another across markets and\nincreases the likelihood of competitive orders being filled.\n"
    },
    {
        "paper_id": 1311.423,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Structural Changes on Warsaw's Stock Exchange: the end of Financial\n  Crisis",
        "comments": "19 pages, 15 figures, submitted to Physica A",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyse the structure of Warsaw's stock market using complex\nsystems methodology together with network science and information theory. We\nfind minimal spanning trees for log returns on Warsaw's stock exchange for\nyearly times series between 2000 and 2013. For each stock in those trees we\ncalculate its Markov centrality measure to estimate its importance in the\nnetwork. We also estimate entropy rate for each of those time series using\nLempel-Ziv algorithm based estimator to study the predictability of those price\nchanges. The division of the studied stocks into 26 sectors allows us to study\nthe changing structure of the Warsaw's stock market and conclude that the\nfinancial crisis sensu stricto has ended on Warsaw's stock market in 2012-13.\nWe also comment on the history and the outlook of the Warsaw's market based on\nthe log returns, their average, variability, entropy and the centrality of a\nstock in the dependency network.\n"
    },
    {
        "paper_id": 1311.4249,
        "authors": "Jean-Pierre Fouque, Yuri F. Saporito, Jorge P. Zubelli",
        "title": "Multiscale Stochastic Volatility Model for Derivatives on Futures",
        "comments": "30 pages and 3 figures",
        "journal-ref": null,
        "doi": "10.1142/S0219024914500435",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a new method to compute the first-order\napproximation of the price of derivatives on futures in the context of\nmultiscale stochastic volatility of Fouque \\textit{et al.} (2011, CUP). It\nprovides an alternative method to the singular perturbation technique presented\nin Hikspoors and Jaimungal (2008). The main features of our method are twofold:\nfirstly, it does not rely on any additional hypothesis on the regularity of the\npayoff function, and secondly, it allows an effective and straightforward\ncalibration procedure of the model to implied volatilities. These features were\nnot achieved in previous works. Moreover, the central argument of our method\ncould be applied to interest rate derivatives and compound derivatives. The\nonly pre-requisite of our approach is the first-order approximation of the\nunderlying derivative. Furthermore, the model proposed here is well-suited for\ncommodities since it incorporates mean reversion of the spot price and\nmultiscale stochastic volatility. Indeed, the model was validated by\ncalibrating it to options on crude-oil futures, and it displays a very good fit\nof the implied volatility.\n"
    },
    {
        "paper_id": 1311.4266,
        "authors": "Younes Boujelb\\`ene, Sihem Khemakhem",
        "title": "Pr\\'evision du risque de cr\\'edit : Une \\'etude comparative entre\n  l'Analyse Discriminante et l'Approche Neuronale",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Banks are interested in evaluating the risk of the financial distress before\ngiving out a loan. Many researchers proposed the use of models based on the\nNeural Networks in order to help the banker better make a decision. The\nobjective of this paper is to explore a new practical way based on the Neural\nNetworks that would help improve the capacity of the banker to predict the risk\nclass of the companies asking for a loan. This work is motivated by the\ninsufficiency of traditional prevision models. The sample consists of 86\nTunisian firms and 15 financial ratios are calculated, over the period from\n2005 to 2007. The results are compared with those of discriminant analysis.\nThey show that the neural networks technique is the best in term of\npredictability.\n"
    },
    {
        "paper_id": 1311.4274,
        "authors": "Yi-Fang Liu, Wei Zhang, Chao Xu, J{\\o}rgen Vitting Andersen, Hai-Chuan\n  Xu",
        "title": "Impact of information cost and switching of trading strategies in an\n  artificial stock market",
        "comments": "15 pages, 9 figures, Physica A, 2014",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.04.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the switching of trading strategies and its effect on the\nmarket volatility in a continuous double auction market. We describe the\nbehavior when some uninformed agents, who we call switchers, decide whether or\nnot to pay for information before they trade. By paying for the information\nthey behave as informed traders. First we verify that our model is able to\nreproduce some of the stylized facts in real financial markets. Next we\nconsider the relationship between switching and the market volatility under\ndifferent structures of investors. We find that there exists a positive\nrelationship between the market volatility and the percentage of switchers. We\ntherefore conclude that the switchers are a destabilizing factor in the market.\nHowever, for a given fixed percentage of switchers, the proportion of switchers\nthat decide to buy information at a given moment of time is negatively related\nto the current market volatility. In other words, if more agents pay for\ninformation to know the fundamental value at some time, the market volatility\nwill be lower. This is because the market price is closer to the fundamental\nvalue due to information diffusion between switchers.\n"
    },
    {
        "paper_id": 1311.4342,
        "authors": "Olivier Gu\\'eant, Jiang Pu",
        "title": "Option pricing and hedging with execution costs and market impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article considers the pricing and hedging of a call option when\nliquidity matters, that is, either for a large nominal or for an illiquid\nunderlying asset. In practice, as opposed to the classical assumptions of a\nprice-taking agent in a frictionless market, traders cannot be perfectly hedged\nbecause of execution costs and market impact. They indeed face a trade-off\nbetween hedging errors and costs that can be solved by using stochastic optimal\ncontrol. Our modelling framework, which is inspired by the recent literature on\noptimal execution, makes it possible to account for both execution costs and\nthe lasting market impact of trades. Prices are obtained through the\nindifference pricing approach. Numerical examples are provided, along with\ncomparisons to standard methods.\n"
    },
    {
        "paper_id": 1311.4503,
        "authors": "Idris Kharroubi (CREST, CEREMADE), Nicolas Langren\\'e (LPMA), Huy\\^en\n  Pham (CREST, LPMA)",
        "title": "A numerical algorithm for fully nonlinear HJB equations: an approach by\n  control randomization",
        "comments": null,
        "journal-ref": "Monte Carlo Methods and Applications 20(2) 145-165 (2014)",
        "doi": "10.1515/mcma-2013-0024",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a probabilistic numerical algorithm to solve Backward Stochastic\nDifferential Equations (BSDEs) with nonnegative jumps, a class of BSDEs\nintroduced in [9] for representing fully nonlinear HJB equations. In\nparticular, this allows us to numerically solve stochastic control problems\nwith controlled volatility, possibly degenerate. Our backward scheme, based on\nleast-squares regressions, takes advantage of high-dimensional properties of\nMonte-Carlo methods, and also provides a parametric estimate in feedback form\nfor the optimal control. A partial analysis of the error of the scheme is\nprovided, as well as numerical tests on the problem of superreplication of\noption with uncertain volatilities and/or correlations, including a detailed\ncomparison with the numerical results from the alternative scheme proposed in\n[7].\n"
    },
    {
        "paper_id": 1311.4698,
        "authors": "Christoph Aistleitner and Markus Hofer and Robert Tichy",
        "title": "A central limit theorem for Latin hypercube sampling with dependence and\n  application to exotic basket option pricing",
        "comments": null,
        "journal-ref": "International Journal of Theoretical & Applied Finance, 15 (2012),\n  no. 7, (20 pages)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of estimating $\\mathbb{E} [f(U^1, \\ldots, U^d)]$,\nwhere $(U^1, \\ldots, U^d)$ denotes a random vector with uniformly distributed\nmarginals. In general, Latin hypercube sampling (LHS) is a powerful tool for\nsolving this kind of high-dimensional numerical integration problem. In the\ncase of dependent components of the random vector $(U^1, \\ldots, U^d)$ one can\nachieve more accurate results by using Latin hypercube sampling with dependence\n(LHSD). We state a central limit theorem for the $d$-dimensional LHSD\nestimator, by this means generalising a result of Packham and Schmidt.\nFurthermore we give conditions on the function $f$ and the distribution of\n$(U^1, \\ldots, U^d)$ under which a reduction of variance can be achieved.\nFinally we compare the effectiveness of Monte Carlo and LHSD estimators\nnumerically in exotic basket option pricing problems.\n"
    },
    {
        "paper_id": 1311.4771,
        "authors": "G. Kavitha, A. Udhayakumar, D. Nagarajan",
        "title": "Stock Market Trend Analysis Using Hidden Markov Models",
        "comments": "International Journal of Computer Science and Information Security\n  october 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price movements of stock market are not totally random. In fact, what drives\nthe financial market and what pattern financial time series follows have long\nbeen the interest that attracts economists, mathematicians and most recently\ncomputer scientists [17]. This paper gives an idea about the trend analysis of\nstock market behaviour using Hidden Markov Model (HMM). The trend once followed\nover a particular period will sure repeat in future. The one day difference in\nclose value of stocks for a certain period is found and its corresponding\nsteady state probability distribution values are determined. The pattern of the\nstock market behaviour is then decided based on these probability values for a\nparticular time. The goal is to figure out the hidden state sequence given the\nobservation sequence so that the trend can be analyzed using the steady state\nprobability distribution( ) values. Six optimal hidden state sequences are\ngenerated and compared. The one day difference in close value when considered\nis found to give the best optimum state sequence.\n"
    },
    {
        "paper_id": 1311.4798,
        "authors": "Leonardo Bargigli, Giovanni di Iasio, Luigi Infante, Fabrizio Lillo,\n  Federico Pierobon",
        "title": "The multiplex structure of interbank networks",
        "comments": "41 pages, 8 figures, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The interbank market has a natural multiplex network representation. We\nemploy a unique database of supervisory reports of Italian banks to the Banca\nd'Italia that includes all bilateral exposures broken down by maturity and by\nthe secured and unsecured nature of the contract. We find that layers have\ndifferent topological properties and persistence over time. The presence of a\nlink in a layer is not a good predictor of the presence of the same link in\nother layers. Maximum entropy models reveal different unexpected substructures,\nsuch as network motifs, in different layers. Using the total interbank network\nor focusing on a specific layer as representative of the other layers provides\na poor representation of interlinkages in the interbank market and could lead\nto biased estimation of systemic risk.\n"
    },
    {
        "paper_id": 1311.4969,
        "authors": "Kyungsub Lee",
        "title": "Recursive formula for arithmetic Asian option prices",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/fut.21591",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a recursive formula for arithmetic Asian option prices with finite\nobservation times in semimartingale models. The method is based on the\nrelationship between the risk-neutral expectation of the quadratic variation of\nthe return process and European option prices. The computation of arithmetic\nAsian option prices is straightforward whenever European option prices are\navailable. Applications with numerical results under the Black-Scholes\nframework and the exponential L\\'evy model are proposed.\n"
    },
    {
        "paper_id": 1311.4973,
        "authors": "Geon Ho Choe and Kyungsub Lee",
        "title": "High moment variations and their application",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/fut.21635",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method of measuring the third and fourth moments of return\ndistribution based on quadratic variation method when the return process is\nassumed to have zero drift. The realized third and fourth moments variations\ncomputed from high frequency return series are good approximations to\ncorresponding actual moments of the return distribution. An investor holding an\nasset with skewed or fat-tailed distribution is able to hedge the tail risk by\ncontracting the third or fourth moment swap under which the float leg of\nrealized variation and the predetermined fixed leg are exchanged. Thus\nconstructed portfolio follows more Gaussian-like distribution and hence the\ninvestor effectively hedge the tail risk.\n"
    },
    {
        "paper_id": 1311.4977,
        "authors": "Geon Ho Choe and Kyungsub Lee",
        "title": "Conditional correlation in asset return and GARCH intensity model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10182-013-0219-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an asset return series there is a conditional asymmetric dependence\nbetween current return and past volatility depending on the current return's\nsign. To take into account the conditional asymmetry, we introduce new models\nfor asset return dynamics in which frequencies of the up and down movements of\nasset price have conditionally independent Poisson distributions with\nstochastic intensities. The intensities are assumed to be stochastic recurrence\nequations of the GARCH type in order to capture the volatility clustering and\nthe leverage effect. We provide an important linkage between our model and\nexisting GARCH, explain how to apply maximum likelihood estimation to determine\nthe parameters in the intensity model and show empirical results with the S&P\n500 index return series.\n"
    },
    {
        "paper_id": 1311.5036,
        "authors": "Kyungsub Lee",
        "title": "Probabilistic and statistical properties of moment variations and their\n  use in inference and estimation based on high frequency return data",
        "comments": null,
        "journal-ref": "tudies in Nonlinear Dynamics & Econometrics, 2016, 20.1: 19-36",
        "doi": "10.1515/snde-2014-0037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the probabilistic properties of the variation based third and\nfourth moments of financial returns as estimators of the actual moments of the\nreturn distributions. The moment variations are defined under non-parametric\nassumptions with quadratic variation method but for the computational\ntractability, we use a square root stochastic volatility model for the\nderivations of moment conditions for estimations. Using the S\\&P 500 index high\nfrequency data, the realized versions of the moment variations is used for the\nestimation of a stochastic volatility model. We propose a simple estimation\nmethod of a stochastic volatility model using the sample averages of the\nvariations and ARMA estimation. In addition, we compare the results with a\ngeneralized method of moments estimation based on the successive relation\nbetween realized moments and their lagged values.\n"
    },
    {
        "paper_id": 1311.5101,
        "authors": "R\\'emy Chicheportiche, Anirban Chakraborti",
        "title": "Copulas and time series with long-ranged dependences",
        "comments": "11 pages, 8 figures",
        "journal-ref": "Phys. Rev. E 89, 042117 (2014)",
        "doi": "10.1103/PhysRevE.89.042117",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review ideas on temporal dependences and recurrences in discrete time\nseries from several areas of natural and social sciences. We revisit existing\nstudies and redefine the relevant observables in the language of copulas (joint\nlaws of the ranks). We propose that copulas provide an appropriate mathematical\nframework to study non-linear time dependences and related concepts - like\naftershocks, Omori law, recurrences, waiting times. We also critically argue\nusing this global approach that previous phenomenological attempts involving\nonly a long-ranged autocorrelation function lacked complexity in that they were\nessentially mono-scale.\n"
    },
    {
        "paper_id": 1311.512,
        "authors": "Catherine Donnelly",
        "title": "Actuarial fairness and solidarity in pooled annuity funds",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Various types of structures that enable a group of individuals to pool their\nmortality risk have been proposed in the literature. Collectively, the\nstructures are called pooled annuity funds. Since the pooled annuity funds\npropose different methods of pooling mortality risk, we investigate the\nconnections between them and find that they are genuinely different for a\nfinite heterogeneous membership profile.\n  We discuss the importance of actuarial fairness, defined as the expected\nbenefits equalling the contributions for each member, in the context of pooling\nmortality risk and comment on whether actuarial unfairness can be seen as\nsolidarity between members. We show that, with a finite number of members in\nthe fund, the group self-annuitization scheme is not actuarially fair: some\nmembers subsidize the other members. The implication is that the members who\nare subsidizing the others may obtain a higher expected benefit by joining a\nfund with a more favourable membership profile. However, we find that the\nsubsidies are financially significant only for very small or highly\nheterogeneous membership profiles.\n"
    },
    {
        "paper_id": 1311.5211,
        "authors": "Andrei Kapaev",
        "title": "Remark on repo and options",
        "comments": "10 pages, 2 figures",
        "journal-ref": "Advances in Economics and Business, Vol. 1(2) (2013) pp. 213 - 221",
        "doi": "10.13189/aeb.2013.010216",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The general and special repo rates are related with the prices of the\nEuropean call- and American put-options. The evaluation takes into account\nspecific business models of the parties in the repo agreement and the law\nrestrictions. Using the repo-option relation, an alternative to the\nBlack-Scholes method of option pricing is presented. The empirical data on the\ngeneral and special repo rates are explained.\n"
    },
    {
        "paper_id": 1311.5511,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory: A puzzling collection of myths based on\n  hyperbolic illusions",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Unified Growth Theory is a puzzling collection of myths based on\nillusions created by hyperbolic distributions. Some of these myths are\ndiscussed. The examination of data shows that the three stages of growth\n(Malthusian Regime, Post-Malthusian Regime and Modern Growth Regime) did not\nexist and that Industrial Revolution had no influence on the economic growth\nand on the growth of human population. All elaborate explanations revolving\naround phantom features created by hyperbolic illusions might be fascinating\nbut they are scientifically unacceptable and, consequently, they do not explain\nthe economic growth. The data clearly indicate that the economic growth was not\nas complicated as described by the Unified Growth Theory but elegantly simple.\n"
    },
    {
        "paper_id": 1311.5661,
        "authors": "Ioane Muni Toke",
        "title": "The order book as a queueing system: average depth and influence of the\n  size of limit orders",
        "comments": null,
        "journal-ref": "Quantitative Finance, 15(5), 795-808. (2015)",
        "doi": "10.1080/14697688.2014.963654",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the analytical properties of a one-side order book model in which\nthe flows of limit and market orders are Poisson processes and the distribution\nof lifetimes of cancelled orders is exponential. Although simplistic, the model\nprovides an analytical tractability that should not be overlooked. Using basic\nresults for birth-and-death processes, we build an analytical formula for the\nshape (depth) of a continuous order book model which is both founded by market\nmechanisms and very close to empirically tested formulas. We relate this shape\nto the probability of execution of a limit order, highlighting a law of\nconservation of the flows of orders in an order book. We then extend our model\nby allowing random sizes of limit orders, hereby allowing to study the\nrelationship between the size of the incoming limit orders and the shape of the\norder book. Our theoretical model shows that, for a given total volume of\nincoming limit orders, the less limit orders are submitted (i.e. the larger the\naverage size of these limit orders), the deeper is the order book around the\nspread. This theoretical relationship is finally empirically tested on several\nstocks traded on the Paris stock exchange.\n"
    },
    {
        "paper_id": 1311.5753,
        "authors": "M. Wilinski, B. Szewczak, T. Gubiec, R. Kutner and Z. R. Struzik",
        "title": "Nucleation, condensation and lambda-transition on a real-life stock\n  market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We fill a void in merging empirical and phenomenological characterisation of\nthe dynamical phase transitions in complex systems by identifying three of them\non real-life financial markets. We extract and interpret the empirical,\nnumerical, and semi-analytical evidences for the existence of these phase\ntransitions, by considering the Frankfurt Stock Exchange (FSE), as a typical\nexample of a financial market of a medium size. Using the canonical object for\nthe graph theory, i.e. the Minimal Spanning Tree (MST) network, we observe: (i)\nThe initial phase transition from the equilibrium to non-equilibrium MST\nnetwork in its nucleation phase, occurring at some critical time. Coalescence\nof edges on the FSE's transient leader is observed within the nucleation and is\napproximately characterized by the Lifsthiz-Slyozov growth exponent; (ii) The\nnucleation accelerates and transforms to the condensation process, in the\nsecond phase transition, forming a logarithmically diverging lambda-peak of\nshort-range order parameters at the subsequent critical time - an analogon of\nsuch a transition in superfluidity; (iii) In the third phase transition, the\npeak logarithmically decreases over three quarters of the year, resulting in a\nfew loosely connected sub-graphs. This peak is reminiscent of a non-equilibrium\nsuperstar-like superhub or a `dragon king' effect, abruptly accelerating the\nevolution of the leader company. All these phase transitions are caused by the\nfew richest vertices, which drift towards the leader and provide the most of\nthe edges increasing the leader's degree. Thus, we capture an amazing\nphenomenon, likely of a more universal character, where a peripheral vertex\nbecomes the one which is over dominating the complex network during an\nexceptionally long period of time.\n"
    },
    {
        "paper_id": 1311.6027,
        "authors": "Archil Gulisashvili",
        "title": "Left-wing asymptotics of the implied volatility in the presence of atoms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the asymptotic behavior of the implied volatility in stochastic\nasset price models with atoms. In such models, the asset price distribution has\na singular component at zero. Examples of models with atoms include the\nconstant elasticity of variance model, jump-to-default models, and stochastic\nmodels described by processes stopped at the first hitting time of zero. For\nmodels with atoms, the behavior of the implied volatility at large strikes is\nsimilar to that in models without atoms. On the other hand, the behavior of the\nimplied volatility at small strikes is influenced significantly by the atom at\nzero. S. De Marco, C. Hillairet, and A. Jacquier found an asymptotic formula\nfor the implied volatility at small strikes with two terms and also provided an\nincomplete description of the third term. In the present paper, we obtain a new\nasymptotic formula for the left wing of the implied volatility, which is\nqualitatively different from the De Marco-Hillairet-Jacquier formula. The new\nformula contains three explicit terms and an error estimate. We show how to\nderive the De Marco-Hillairet-Jacquier formula from our formula, and compare\nthe performance of the two formulas in the case of the CEV model. The resulting\ngraphs show that the new formula provides a notably better approximation to the\nsmile in the CEV model than the De Marco-Hillairet-Jacquier formula.\n"
    },
    {
        "paper_id": 1311.608,
        "authors": "Zuo Quan Xu",
        "title": "A New Characterization of Comonotonicity and its Application in\n  Behavioral Finance",
        "comments": "This paper has been withdrawn by the author. The main results are\n  existing in the literature, so we withdraw it",
        "journal-ref": "Journal of Mathematical Analysis and Applications, Vol.418 (2014),\n  612-625",
        "doi": "10.1016/j.jmaa.2014.03.053",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that an $\\mathbb{R}$-valued random vector $(X_1, X_2,\n\\cdots, X_n)$ is comonotonic if and only if $(X_1, X_2, \\cdots, X_n)$ and\n$(Q_1(U), Q_2(U),\\cdots, Q_n(U))$ coincide \\emph{in distribution}, for\n\\emph{any} random variable $U$ uniformly distributed on the unit interval\n$(0,1)$, where $Q_k(\\cdot)$ are the quantile functions of $X_k$, $k=1,2,\\cdots,\nn$. It is natural to ask whether $(X_1, X_2, \\cdots, X_n)$ and $(Q_1(U),\nQ_2(U),\\cdots, Q_n(U))$ can coincide \\emph{almost surely} for \\emph{some}\nspecial $U$. In this paper, we give a positive answer to this question by\nconstruction. We then apply this result to a general behavioral investment\nmodel with a law-invariant preference measure and develop a universal framework\nto link the problem to its quantile formulation. We show that any optimal\ninvestment output should be anti-comonotonic with the market pricing kernel.\nUnlike previous studies, our approach avoids making the assumption that the\npricing kernel is atomless, and consequently, we overcome one of the major\ndifficulties encountered when one considers behavioral economic equilibrium\nmodels in which the pricing kernel is a yet-to-be-determined unknown random\nvariable. The method is applicable to many other models such as risk sharing\nmodel.\n"
    },
    {
        "paper_id": 1311.6179,
        "authors": "Lingjiong Zhu",
        "title": "Optimal Strategies for a Long-Term Static Investor",
        "comments": "14 pages",
        "journal-ref": "Stochastic Models 2014, Volume 30, Issue 3, 300-318",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The optimal strategies for a long-term static investor are studied. Given a\nportfolio of a stock and a bond, we derive the optimal allocation of the\ncapitols to maximize the expected long-term growth rate of a utility function\nof the wealth. When the bond has constant interest rate, three models for the\nunderlying stock price processes are studied: Heston model, 3/2 model and jump\ndiffusion model. We also study the optimal strategies for a portfolio in which\nthe stock price process follows a Black-Scholes model and the bond process has\na Vasicek interest rate that is correlated to the stock price.\n"
    },
    {
        "paper_id": 1311.6187,
        "authors": "Nicolas Perkowski, David J. Pr\\\"omel",
        "title": "Pathwise stochastic integrals for model free finance",
        "comments": "Published at http://dx.doi.org/10.3150/15-BEJ735 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2016, Vol. 22, No. 4, 2486-2520",
        "doi": "10.3150/15-BEJ735",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present two different approaches to stochastic integration in frictionless\nmodel free financial mathematics. The first one is in the spirit of It\\^o's\nintegral and based on a certain topology which is induced by the outer measure\ncorresponding to the minimal superhedging price. The second one is based on the\ncontrolled rough path integral. We prove that every \"typical price path\" has a\nnaturally associated It\\^o rough path, and justify the application of the\ncontrolled rough path integral in finance by showing that it is the limit of\nnon-anticipating Riemann sums, a new result in itself. Compared to the first\napproach, rough paths have the disadvantage of severely restricting the space\nof integrands, but the advantage of being a Banach space theory. Both\napproaches are based entirely on financial arguments and do not require any\nprobabilistic structure.\n"
    },
    {
        "paper_id": 1311.6257,
        "authors": "Samuel N. Cohen and Robert J. Elliott",
        "title": "Filters and smoothers for self-exciting Markov modulated counting\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a self-exciting counting process, the parameters of which depend\non a hidden finite-state Markov chain. We derive the optimal filter and\nsmoother for the hidden chain based on observation of the jump process. This\nfilter is in closed form and is finite dimensional. We demonstrate the\nperformance of this filter both with simulated data, and by analysing the\n`flash crash' of 6th May 2010 in this framework.\n"
    },
    {
        "paper_id": 1311.6262,
        "authors": "Iacopo Mastromatteo, Bence Toth, Jean-Philippe Bouchaud",
        "title": "Agent-based models for latent liquidity and concave price impact",
        "comments": "36 pages, 12 figures, published version",
        "journal-ref": "Phys. Rev. E 89, 042805 (2014)",
        "doi": "10.1103/PhysRevE.89.042805",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the \"epsilon-intelligence\" model of Toth et al.(2011), that was\nproposed as a minimal framework to understand the square-root dependence of the\nimpact of meta-orders on volume in financial markets. The basic idea is that\nmost of the daily liquidity is \"latent\" and furthermore vanishes linearly\naround the current price, as a consequence of the diffusion of the price\nitself. However, the numerical implementation of Toth et al. was criticised as\nbeing unrealistic, in particular because all the \"intelligence\" was conferred\nto market orders, while limit orders were passive and random. In this work, we\nstudy various alternative specifications of the model, for example allowing\nlimit orders to react to the order flow, or changing the execution protocols.\nBy and large, our study lends strong support to the idea that the square-root\nimpact law is a very generic and robust property that requires very few\ningredients to be valid. We also show that the transition from super-diffusion\nto sub-diffusion reported in Toth et al. is in fact a cross-over, but that the\noriginal model can be slightly altered in order to give rise to a genuine phase\ntransition, which is of interest on its own. We finally propose a general\ntheoretical framework to understand how a non-linear impact may appear even in\nthe limit where the bias in the order flow is vanishingly small.\n"
    },
    {
        "paper_id": 1311.7027,
        "authors": "Claudio Fontana",
        "title": "A note on arbitrage, approximate arbitrage and the fundamental theorem\n  of asset pricing",
        "comments": "10 pages",
        "journal-ref": "Stochastics, 2014, vol. 86(6), 922-931",
        "doi": "10.1080/17442508.2014.895358",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a critical analysis of the proof of the fundamental theorem of\nasset pricing given in the paper \"Arbitrage and approximate arbitrage: the\nfundamental theorem of asset pricing\" by B. Wong and C.C. Heyde (Stochastics,\n2010) in the context of incomplete It\\^o-process models. We show that their\napproach can only work in the known case of a complete financial market model\nand give an explicit counterexample.\n"
    },
    {
        "paper_id": 1311.7419,
        "authors": "Sigrid K\\\"allblad",
        "title": "Risk- and ambiguity-averse portfolio optimization with quasiconcave\n  utility functionals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by recent axiomatic developments, we study the risk- and\nambiguity-averse investment problem where trading takes place over a fixed\nfinite horizon and terminal payoffs are evaluated according to a criterion\ndefined in terms of a quasiconcave utility functional. We extend to the present\nsetting certain existence and duality results established for the so-called\nvariational preferences by Schied (2007). The results are proven by building on\nexisting results for the classical utility maximization problem.\n"
    },
    {
        "paper_id": 1312.0128,
        "authors": "Damiano Brigo, Andrea Pallavicini",
        "title": "CCPs, Central Clearing, CSA, Credit Collateral and Funding Costs\n  Valuation FAQ: Re-hypothecation, CVA, Closeout, Netting, WWR, Gap-Risk,\n  Initial and Variation Margins, Multiple Discount Curves, FVA?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a dialogue on Funding Costs and Counterparty Credit Risk modeling,\ninclusive of collateral, wrong way risk, gap risk and possible Central Clearing\nimplementation through CCPs. This framework is important following the fact\nthat derivatives valuation and risk analysis has moved from exotic derivatives\nmanaged on simple single asset classes to simple derivatives embedding the new\nor previously neglected types of complex and interconnected nonlinear risks we\naddress here. This dialogue is the continuation of the \"Counterparty Risk,\nCollateral and Funding FAQ\" by Brigo (2011). In this dialogue we focus more on\nfunding costs for the hedging strategy of a portfolio of trades, on the\nnon-linearities emerging from assuming borrowing and lending rates to be\ndifferent, on the resulting aggregation-dependent valuation process and its\noperational challenges, on the implications of the onset of central clearing,\non the macro and micro effects on valuation and risk of the onset of CCPs, on\ninitial and variation margins impact on valuation, and on multiple discount\ncurves. Through questions and answers (Q&A) between a senior expert and a\njunior colleague, and by referring to the growing body of literature on the\nsubject, we present a unified view of valuation (and risk) that takes all such\naspects into account.\n"
    },
    {
        "paper_id": 1312.0161,
        "authors": "Angelo Tartaglia",
        "title": "Science and the Future: Introduction",
        "comments": "21 pages, 7 figures, Introductory talk to the conference Science and\n  the Future held at the Politecnico, Turin, Italy from 28 to 31 October 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The contradiction between physical and economical sciences concerning the\ngrowth of the production/consumption mechanism is analyzed. It is then shown\nthat if one wishes to keep the security level stable or to enhance it in a\ngrowing economy the cost of security grows faster than the gross wealth. The\nresult is a typical evolution in which the net wealth increases up to a\nmaximum, then abruptly collapses. Besides this, any system of relations based\non a growing volume of exchanges is bound to go progressively out of control.\nThe voluntary blindness of the ruling classes toward these facts is leading our\nsocieties to a disaster. This fate is not inescapable provided we learn to\ndismantle the myth of perpetual growth.\n"
    },
    {
        "paper_id": 1312.0283,
        "authors": "Zhenyu Cui",
        "title": "Stochastic areas of diffusions and applications in risk theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  In this paper we study the stochastic area swept by a regular\ntime-homogeneous diffusion till a stopping time. This unifies some recent\nliterature in this area. Through stochastic time change we establish a link\nbetween the stochastic area and the stopping time of another associated\ntime-homogeneous diffusion. Then we characterize the Laplace transform of the\nstochastic area in terms of the eigenfunctions of the associated diffusion. We\nalso explicitly obtain the integer moments of the stochastic area in terms of\nscale and speed densities of the associated diffusion. Specifically we study in\ndetail three stopping times: the first passage time to a constant level, the\nfirst drawdown time and the Azema-Yor stopping time. We also study the total\noccupation area of the diffusion below a constant level. We show applications\nof the results to a new structural model of default (Yildirim 2006), the Omega\nrisk model of bankruptcy in risk analysis (Gerber, Shiu and Yang 2012), and a\ndiffusion risk model with surplus-dependent tax (Albrecher and Hipp 2007, Li,\nTang and Zhou 2013).\n"
    },
    {
        "paper_id": 1312.0323,
        "authors": "Alejandro Jenkins",
        "title": "Towards a microeconomic theory of the finance-driven business cycle",
        "comments": "6 pages, no figures. Presented at 1st International Congress on\n  Actuarial Science and Quantitative Finance, Bogot\\'a, Colombia, 20 Jun. 2014.\n  v2: References updated and improved, some points clarified, matches published\n  version",
        "journal-ref": "Laissez-Faire 42, 12-20 (2015)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I sketch a program for a microeconomic theory of the main component of the\nbusiness cycle as a recurring disequilibrium, driven by incompleteness of the\nfinancial market and by information asymmetries between borrowers and lenders.\nThis proposal seeks to incorporate five distinct but connected processes that\nhave been discussed at varying lengths in the literature: the leverage cycle,\nfinancial panic, debt deflation, debt overhang, and deleveraging of households.\nIn the wake of the 2007-08 financial crisis, policy responses by central banks\nhave addressed only financial panic and debt deflation. Debt overhang and the\nslowness of household deleveraging account for the Keynesian \"excessive saving\"\nseen in recessions, which raises questions about the suitability of the\nstandard Keynesian remedies.\n"
    },
    {
        "paper_id": 1312.0424,
        "authors": "Rodrigo S. Targino, Gareth W. Peters, Georgy Sofronov, Pavel V.\n  Shevchenko",
        "title": "Optimal insurance purchase strategies via optimal multiple stopping\n  times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a class of insurance products where the policy holder\nhas the option to insure $k$ of its annual Operational Risk losses in a horizon\nof $T$ years. This involves a choice of $k$ out of $T$ years in which to apply\nthe insurance policy coverage by making claims against losses in the given\nyear. The insurance product structure presented can accommodate any kind of\nannual mitigation, but we present three basic generic insurance policy\nstructures that can be combined to create more complex types of coverage.\nFollowing the Loss Distributional Approach (LDA) with Poisson distributed\nannual loss frequencies and Inverse-Gaussian loss severities we are able to\ncharacterize in closed form analytical expressions for the multiple optimal\ndecision strategy that minimizes the expected Operational Risk loss over the\nnext $T$ years. For the cases where the combination of insurance policies and\nLDA model does not lead to closed form expressions for the multiple optimal\ndecision rules, we also develop a principled class of closed form\napproximations to the optimal decision rule. These approximations are developed\nbased on a class of orthogonal Askey polynomial series basis expansion\nrepresentations of the annual loss compound process distribution and functions\nof this annual loss.\n"
    },
    {
        "paper_id": 1312.0506,
        "authors": "Marc Busse, Michel Dacorogna, Marie Kratz",
        "title": "The impact of systemic risk on the diversification benefits of a risk\n  portfolio",
        "comments": "17 pages, 5 tableaux",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk diversification is the basis of insurance and investment. It is thus\ncrucial to study the effects that could limit it. One of them is the existence\nof systemic risk that affects all the policies at the same time. We introduce\nhere a probabilistic approach to examine the consequences of its presence on\nthe risk loading of the premium of a portfolio of insurance policies. This\napproach could be easily generalized for investment risk. We see that, even\nwith a small probability of occurrence, systemic risk can reduce dramatically\nthe diversification benefits. It is clearly revealed via a non-diversifiable\nterm that appears in the analytical expression of the variance of our models.\nWe propose two ways of introducing it and discuss their advantages and\nlimitations. By using both VaR and TVaR to compute the loading, we see that\nonly the latter captures the full effect of systemic risk when its probability\nto occur is low\n"
    },
    {
        "paper_id": 1312.0514,
        "authors": "Alexander Lipton, Umberto Pesavento, Michael G Sotiropoulos",
        "title": "Trade arrival dynamics and quote imbalance in a limit order book",
        "comments": "15 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the dynamics of the bid and ask queues of a limit order book and\ntheir relationship with the intensity of trade arrivals. In particular, we\nstudy the probability of price movements and trade arrivals as a function of\nthe quote imbalance at the top of the limit order book. We propose a stochastic\nmodel in an attempt to capture the joint dynamics of the top of the book queues\nand the trading process, and describe a semi-analytic approach to calculate the\nrelative probability of market events. We calibrate the model using historical\nmarket data and discuss the quality of fit and practical applications of the\nresults.\n"
    },
    {
        "paper_id": 1312.0557,
        "authors": "Steven E. Pav",
        "title": "Asymptotic distribution of the Markowitz portfolio",
        "comments": "65 pages; this revision fixes some oversized figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The asymptotic distribution of the Markowitz portfolio is derived, for the\ngeneral case (assuming fourth moments of returns exist), and for the case of\nmultivariate normal returns. The derivation allows for inference which is\nrobust to heteroskedasticity and autocorrelation of moments up to order four.\nAs a side effect, one can estimate the proportion of error in the Markowitz\nportfolio due to mis-estimation of the covariance matrix. A likelihood ratio\ntest is given which generalizes Dempster's Covariance Selection test to allow\ninference on linear combinations of the precision matrix and the Markowitz\nportfolio. Extensions of the main method to deal with hedged portfolios,\nconditional heteroskedasticity, conditional expectation, and constrained\nestimation are given. It is shown that the Hotelling-Lawley statistic\ngeneralizes the (squared) Sharpe ratio under the conditional expectation model.\nAsymptotic distributions of all four of the common `MGLH' statistics are found,\nassuming random covariates. Examples are given demonstrating the possible uses\nof these results.\n"
    },
    {
        "paper_id": 1312.0563,
        "authors": "Weibing Huang, Charles-Albert Lehalle and Mathieu Rosenbaum",
        "title": "Simulating and analyzing order book data: The queue-reactive model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through the analysis of a dataset of ultra high frequency order book updates,\nwe introduce a model which accommodates the empirical properties of the full\norder book together with the stylized facts of lower frequency financial data.\nTo do so, we split the time interval of interest into periods in which a well\nchosen reference price, typically the mid price, remains constant. Within these\nperiods, we view the limit order book as a Markov queuing system. Indeed, we\nassume that the intensities of the order flows only depend on the current state\nof the order book. We establish the limiting behavior of this model and\nestimate its parameters from market data. Then, in order to design a relevant\nmodel for the whole period of interest, we use a stochastic mechanism that\nallows for switches from one period of constant reference price to another.\nBeyond enabling to reproduce accurately the behavior of market data, we show\nthat our framework can be very useful for practitioners, notably as a market\nsimulator or as a tool for the transaction cost analysis of complex trading\nalgorithms.\n"
    },
    {
        "paper_id": 1312.069,
        "authors": "Li-Xin Zhong, Wen-Juan Xu, Ping Huang, Chen-Yang Zhong, Tian Qiu",
        "title": "Self-organization and phase transition in financial markets with\n  multiple choices",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.05.039",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market confidence is essential for successful investing. By incorporating\nmulti-market into the evolutionary minority game, we investigate the effects of\ninvestor beliefs on the evolution of collective behaviors and asset prices.\nWhen there exists another investment opportunity, market confidence, including\noverconfidence and under-confidence, is not always good or bad for investment.\nThe roles of market confidence is closely related to market impact. For low\nmarket impact, overconfidence in a particular asset makes an investor become\ninsensitive to losses and a delayed strategy adjustment leads to a decline in\nwealth, and thereafter, one's runaway from the market. For high market impact,\nunder-confidence in a particular asset makes an investor over-sensitive to\nlosses and one's too frequent strategy adjustment leads to a large fluctuation\nin asset prices, and thereafter, a decrease in the number of agents. At an\nintermediate market impact, the phase transition occurs. No matter what the\nmarket impact is, an equilibrium between different markets exists, which is\nreflected in the occurrence of similar price fluctuations in different markets.\nA theoretical analysis indicates that such an equilibrium results from the\ncoupled effects of strategy updating and shift in investment. The runaway of\nthe agents trading a specific asset will lead to a decline in the asset price\nvolatility and such a decline will be inhibited by the clustering of the\nstrategies. A uniform strategy distribution will lead to a large fluctuation in\nasset prices and such a fluctuation will be suppressed by the decrease in the\nnumber of agents in the market. A functional relationship between the price\nfluctuations and the numbers of agents is found.\n"
    },
    {
        "paper_id": 1312.1006,
        "authors": "Tomasz R. Bielecki, Igor Cialenco, Marcin Pitera",
        "title": "Dynamic Limit Growth Indices in Discrete Time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new class of mappings, called Dynamic Limit Growth Indices, that\nare designed to measure the long-run performance of a financial portfolio in\ndiscrete time setup. We study various important properties for this new class\nof measures, and in particular, we provide necessary and sufficient condition\nfor a Dynamic Limit Growth Index to be a dynamic assessment index. We also\nestablish their connection with classical dynamic acceptability indices, and we\nshow how to construct examples of Dynamic Limit Growth Indices using dynamic\nrisk measures and dynamic certainty equivalents. Finally, we propose a new\ndefinition of time consistency, suitable for these indices, and we study time\nconsistency for the most notable representative of this class -- the dynamic\nanalog of risk sensitive criterion.\n"
    },
    {
        "paper_id": 1312.1473,
        "authors": "Francesco Audrino and Lorenzo Camponovo",
        "title": "Oracle Properties and Finite Sample Inference of the Adaptive Lasso for\n  Time Series Regression Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive new theoretical results on the properties of the adaptive least\nabsolute shrinkage and selection operator (adaptive lasso) for time series\nregression models. In particular, we investigate the question of how to conduct\nfinite sample inference on the parameters given an adaptive lasso model for\nsome fixed value of the shrinkage parameter. Central in this study is the test\nof the hypothesis that a given adaptive lasso parameter equals zero, which\ntherefore tests for a false positive. To this end we construct a simple testing\nprocedure and show, theoretically and empirically through extensive Monte Carlo\nsimulations, that the adaptive lasso combines efficient parameter estimation,\nvariable selection, and valid finite sample inference in one step. Moreover, we\nanalytically derive a bias correction factor that is able to significantly\nimprove the empirical coverage of the test on the active variables. Finally, we\napply the introduced testing procedure to investigate the relation between the\nshort rate dynamics and the economy, thereby providing a statistical foundation\n(from a model choice perspective) to the classic Taylor rule monetary policy\nmodel.\n"
    },
    {
        "paper_id": 1312.1578,
        "authors": "Arthur M. Berd, Elena Ranguelova, Antonio Baldaque da Silva",
        "title": "Credit Portfolio Management in a Turning Rates Environment",
        "comments": "15 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a detailed account of correlations between credit sector/quality and\ntreasury curve factors, using the robust framework of the Barclays POINT Global\nRisk Model. Consistent with earlier studies, we find a strong negative\ncorrelation between sector spreads and rate shifts. However, we also observe\nthat the correlations between spreads and Treasury twists reversed recently,\nwhich is likely attributable to the Fed's ongoing quantitative easing. We also\nfind that short-term effective durations in the banking industry are now\nsignificantly lower than historical patterns would indicate. Our findings are\nrelevant for credit portfolio managers contemplating the impact of rising\ninterest rates and steepening Treasury curve on corporate bond portfolios.\n"
    },
    {
        "paper_id": 1312.1645,
        "authors": "Susanne Emmer, Marie Kratz, Dirk Tasche",
        "title": "What is the best risk measure in practice? A comparison of standard\n  measures",
        "comments": "27 pages, 1 table",
        "journal-ref": "Journal of Risk 18(2), 31-60, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expected Shortfall (ES) has been widely accepted as a risk measure that is\nconceptually superior to Value-at-Risk (VaR). At the same time, however, it has\nbeen criticised for issues relating to backtesting. In particular, ES has been\nfound not to be elicitable which means that backtesting for ES is less\nstraightforward than, e.g., backtesting for VaR. Expectiles have been suggested\nas potentially better alternatives to both ES and VaR. In this paper, we\nrevisit commonly accepted desirable properties of risk measures like coherence,\ncomonotonic additivity, robustness and elicitability. We check VaR, ES and\nExpectiles with regard to whether or not they enjoy these properties, with\nparticular emphasis on Expectiles. We also consider their impact on capital\nallocation, an important issue in risk management. We find that, despite the\ncaveats that apply to the estimation and backtesting of ES, it can be\nconsidered a good risk measure. As a consequence, there is no sufficient\nevidence to justify an all-inclusive replacement of ES by Expectiles in\napplications. For backtesting ES, we propose an empirical approach that\nconsists in replacing ES by a set of four quantiles, which should allow to make\nuse of backtesting methods for VaR.\n  Keywords: Backtesting; capital allocation; coherence; diversification;\nelicitability; expected shortfall; expectile; forecasts; probability integral\ntransform (PIT); risk measure; risk management; robustness; value-at-risk\n"
    },
    {
        "paper_id": 1312.2004,
        "authors": "Valerii Salov",
        "title": "Optimal Trading Strategies as Measures of Market Disequilibrium",
        "comments": "222 pages, 45 figures, 21 tables, 243 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For classification of the high frequency trading quantities, waiting times,\nprice increments within and between sessions are referred to as the a-, b-, and\nc-increments. Statistics of the a-b-c-increments are computed for the Time &\nSales records posted by the Chicago Mercantile Exchange Group for the futures\ntraded on Globex. The Weibull, Kumaraswamy, Riemann and Hurwitz Zeta,\nparabolic, Zipf-Mandelbrot distributions are tested for the a- and\nb-increments. A discrete version of the Fisher-Tippett distribution is\nsuggested for approximating the extreme b-increments. Kolmogorov and Uspenskii\nclassification of stochastic, typical, and chaotic random sequences is reviewed\nwith regard to the futures price limits. Non-parametric L1 and log-likelihood\ntests are applied to check dependencies between the a- and b-increments. The\nmaximum profit strategies and optimal trading elements are suggested as\nmeasures of frequency and magnitude of the market offers and disequilibrium.\nEmpirical cumulative distribution functions of optimal profits are reported. A\nfew classical papers are reviewed with more details in order to trace the\norigin and foundation of modern finance.\n"
    },
    {
        "paper_id": 1312.2048,
        "authors": "Brian P. Hanley",
        "title": "The False Premises and Promises of Bitcoin",
        "comments": "28 pages, 6 figures. JEL: E21, E22, E42, E51, G21, G29, G28 Section\n  2.6 has been broken out into a separate paper, and that unwieldy section is\n  replaced by a short bit referencing that new paper titled, \"A zero-sum\n  monetary system, interest rates, and implications.\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Designed to compete with fiat currencies, bitcoin proposes it is a\ncrypto-currency alternative. Bitcoin makes a number of false claims, including:\nsolving the double-spending problem is a good thing; bitcoin can be a reserve\ncurrency for banking; hoarding equals saving, and that we should believe\nbitcoin can expand by deflation to become a global transactional currency\nsupply. Bitcoin's developers combine technical implementation proficiency with\nignorance of currency and banking fundamentals. This has resulted in a failed\nattempt to change finance. A set of recommendations to change finance are\nprovided in the Afterword: Investment/venture banking for the masses; Venture\nbanking to bring back what investment banks once were; Open-outcry exchange for\nall CDS contracts; Attempting to develop CDS type contracts on investments in\nstartup and existing enterprises; and Improving the connection between startup\ntech/ideas, business organization and investment.\n"
    },
    {
        "paper_id": 1312.2179,
        "authors": "Marc Diener (JAD), Pheakdei Mauk (JAD)",
        "title": "On the implicit interest rate in the Yunus equation",
        "comments": "4 pages (101-104). Colloque \\`a la m\\'emoire d'Emmanuel Isambert,\n  Paris : France (2007)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In his book with Alan Jolis, Vers un monde sans pauvret\\'e (1997) Yunus gives\nthe example of a microcredit loan of 1000BDT reimbursed via 50 weekly\nsettlements of 22BDT and correctly claims that this corresponds to the annual\ninterest rate of 20%. But this is without taking into account that if the\nborrower has good reasons not to pay at one installment, she can postpone of\none week all remaining settlements, under the same conditions, so without extra\ncost. This of course leads to a lower implicit interest rate. Introducing a\nsimple geometric law model for the time between settlements, this turns the\nimplicit interest rate into a random variable, whose laws is still unknown but\nfor which we provide simulated empirical distribution density function. De ning\nby actuarial expected rate the real number r that satis es the expectation of\nthe random Yunus equation, we compute this number as a function of the\nprobability p of in-time installment. This allows in turn to compute the\nimplicit probability p which is to the value of p corresponding to the observed\n3% default rate, where in practice, \\default\" means \\more than four weeks\ndelay\". The mathematical tool used is the probability generating function, the\ncomputer tool is the Scilab algebraic equation solver.\n"
    },
    {
        "paper_id": 1312.2203,
        "authors": "Kai Nie and Man Yu",
        "title": "Research on fresh agriculture product based on overconfidence of the\n  retailer under options and spot markets dominated",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we analyze the application of options contract in special\ncommodity supply chain such as fresh agricultural products. This problem is\ndiscussed in the point of the retailer. When spot market and future market are\nboth available, we discuss how the retailer chooses the optimal production.\nFurthermore, overconfidence is introduced to the supply chain of the fresh\nagricultural products, which has not happened before. Then,based on the\noverconfidence of the retailer, we explore how overconfidence affects the\nsupply chain system under different circumstances. At last, we get the\nconclusion that different overconfidence level has different affection on\nretailer's optimal ordering quantity and profit.\n"
    },
    {
        "paper_id": 1312.2281,
        "authors": "John Armstrong, Martin Forde, Matthew Lorig, Hongzhong Zhang",
        "title": "Small-time asymptotics for a general local-stochastic volatility model\n  with a jump-to-default: curvature and the heat kernel expansion",
        "comments": "27 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1137/140971397",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compute a sharp small-time estimate for implied volatility under a general\nuncorrelated local-stochastic volatility model. For this we use the Bellaiche\n\\cite{Bel81} heat kernel expansion combined with Laplace's method to integrate\nover the volatility variable on a compact set, and (after a gauge\ntransformation) we use the Davies \\cite{Dav88} upper bound for the heat kernel\non a manifold with bounded Ricci curvature to deal with the tail integrals. If\nthe correlation $\\rho < 0$, our approach still works if the drift of the\nvolatility takes a specific functional form and there is no local volatility\ncomponent, and our results include the SABR model for $\\beta=1, \\rho \\le 0$.\n\\bl{For uncorrelated stochastic volatility models, our results also include a\nSABR-type model with $\\beta=1$ and an affine mean-reverting drift, and the\nexponential Ornstein-Uhlenbeck model.} We later augment the model with a single\njump-to-default with intensity $\\lm$, which produces qualitatively different\nbehaviour for the short-maturity smile; in particular, for $\\rho=0$,\nlog-moneyness $x > 0$, the implied volatility increases by $\\lm f(x) t +o(t) $\nfor some function $f(x)$ which blows up as $x \\searrow 0$. Finally, we compare\nour result with the general asymptotic expansion in Lorig, Pagliarani \\&\nPascucci \\cite{LPP15}, and we verify our results numerically for the SABR model\nusing Monte Carlo simulation and the exact closed-form solution given in\nAntonov \\& Spector \\cite{AS12} for the case $\\rho=0$.\n"
    },
    {
        "paper_id": 1312.2302,
        "authors": "Rene Carmona and Kevin Webster",
        "title": "The Self-Financing Equation in High Frequency Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High Frequency Trading (HFT) represents an ever growing proportion of all\nfinancial transactions as most markets have now switched to electronic order\nbook systems. The main goal of the paper is to propose continuous time\nequations which generalize the self-financing relationships of frictionless\nmarkets to electronic markets with limit order books. We use NASDAQ ITCH data\nto identify significant empirical features such as price impact and recovery,\nrough paths of inventories and vanishing bid-ask spreads. Starting from these\nfeatures, we identify microscopic identities holding on the trade clock, and\nthrough a diffusion limit argument, derive continuous time equations which\nprovide a macroscopic description of properties of the order book. These\nequations naturally differentiate between trading via limit and market orders.\nWe give several applications (including hedging European options with limit\norders, market maker optimal spread choice, and toxicity indexes) to illustrate\ntheir impact and how they can be used to the benefit of Low Frequency Traders\n(LFTs).\n"
    },
    {
        "paper_id": 1312.2362,
        "authors": "Maciej Jagielski and Ryszard Kutner",
        "title": "Modelling the income distribution in the European Union: An application\n  for the initial analysis of the recent worldwide financial crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By using methods of statistical physics, we focus on the quantitative\nanalysis of the economic income data descending from different databases. To\nexplain our approach, we introduce the necessary theoretical background, the\nextended Yakovenko et al. (EY) model. This model gives an analytical\ndescription of the annual household incomes of all society classes in the\nEuropean Union (i.e., the low-, medium-, and high-income ones) by a single\nunified formula based on unified formalism. We show that the EY model is very\nuseful for the analyses of various income datasets, in particular, in the case\nof a smooth matching of two different datasets. The completed database which we\nhave constructed using this matching emphasises the significance of the\nhigh-income society class in the analysis of all household incomes. For\ninstance, the Pareto exponent, which characterises this class, defines the Zipf\nlaw having an exponent much lower than the one characterising the medium-income\nsociety class. This result makes it possible to clearly distinguish between\nmedium- and high-income society classes. By using our approach, we found that\nthe high-income society class almost disappeared in 2009, which defines this\nyear as the most difficult for the EU. To our surprise, this is a contrast with\n2008, considered the first year of a worldwide financial crisis, when the\nstatus of the high-income society class was similar to that of 2010. This,\nperhaps, emphasises that the crisis in the EU was postponed by about one year\nin comparison with the United States.\n"
    },
    {
        "paper_id": 1312.2433,
        "authors": "Anna Aksamit, Tahir Choulli, Jun Deng, Monique Jeanblanc",
        "title": "Arbitrages in a Progressive Enlargement Setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper completes the analysis of Choulli et al. Non-Arbitrage up to\nRandom Horizons and after Honest Times for Semimartingale Models and contains\ntwo principal contributions. The first contribution consists in providing and\nanalysing many practical examples of market models that admit classical\narbitrages while they preserve the No Unbounded Profit with Bounded Risk (NUPBR\nhereafter) under random horizon and when an honest time is incorporated for\nparticular cases of models. For these markets, we calculate explicitly the\narbitrage opportunities. The second contribution lies in providing simple\nproofs for the stability of the No Unbounded Profit with Bounded Risk under\nrandom horizon and after honest time satisfying additional important condition\nfor particular cases of models.\n"
    },
    {
        "paper_id": 1312.2641,
        "authors": "Wiroy Shin",
        "title": "Simultaneous auctions for complementary goods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an environment of simultaneous, separate, first-price\nauctions for complementary goods. Agents observe private values of each good\nbefore making bids, and the complementarity between goods is explicitly\nincorporated in their utility. For simplicity, a model is presented with two\nfirst-price auctions and two bidders. We show that a monotone pure-strategy\nBayesian Nash Equilibrium exists in the environment.\n"
    },
    {
        "paper_id": 1312.2693,
        "authors": "Ioannis Praggidis, Periklis Gogas, Vasilios Plakandaras, Theophilos\n  Papadimitriou",
        "title": "Fiscal shocks and asymmetric effects: a comparative analysis",
        "comments": "21 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We empirically test the effects of unanticipated fiscal policy shocks on the\ngrowth rate and the cyclical component of real private output and reveal\ndifferent types of asymmetries in fiscal policy implementation. The data used\nare quarterly U.S. observati ons over the period 1967:1 to 2011:4. In doing so,\nwe use both a vector autoregressive and the novel support vector machines\nsystems in order to extract the fiscal policy shocks series. The latter has\nnever been used before in a similar macroeconomic setting. Within our research\nframework, in order to test the robustness of our results to alternative\naggregate money supply definitions we use two alternative moentary aggregates.\nThese are the commonly reported by central banks and policy makers simple sum\nmonetary aggregates at the MZM level of aggregation and the alternative CFS\nDivisia MZM aggregate. From each of these four systems we extracted four types\nof shocks: a negative and a positive government spending shock and a negative\nand a positive government revenue shock. These eight different types of\nunanticipated fiscal policy shocks are next used to empirically examine their\neffects on the growth rate and the cyclical component of real private GNP in\ntwo sets of regressions: one that assumes only contemporaneous effects of the\nshocks on output and one that is augmented with four lags of each fiscal shock.\n"
    },
    {
        "paper_id": 1312.2722,
        "authors": "Maciej Jagielski and Ryszard Kutner",
        "title": "Modelling of the European Union income distribution by extended\n  Yakovenko formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We found a unified formula for description of the household incomes of all\nsociety classes, for instance, for the European Union in years 2005-2010. The\nformula is more general than well known that of Yakovenko et al. because, it\nsatisfactorily describes not only the household incomes of low- and\nmedium-income society classes but also the household incomes of the high-income\nsociety class. As a striking result, we found that the high-income society\nclass almost disappeared in year 2009, in opposite to situation in remaining\nyears, where this class played a significant role.\n"
    },
    {
        "paper_id": 1312.2754,
        "authors": "Emilie Fabre, Guillaume Royer and Nizar Touzi",
        "title": "Liquidation of an indivisible asset with independent investment",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an extension of the explicit solution of a mixed optimal\nstopping-optimal stochastic control problem introduced by Henderson and Hobson.\nThe problem examines wether the optimal investment problem on a local\nmartingale financial market is affected by the optimal liquidation of an\nindependent indivisible asset. The indivisible asset process is defined by a\nhomogeneous scalar stochastic differential equation, and the investor's\npreferences are defined by a general expected utility function. The value\nfunction is obtained in explicit form, and we prove the existence of an optimal\nstopping-investment strategy characterized as the limit of an explicit\nmaximizing strategy. Our approach is based on the standard dynamic programming\napproach.\n"
    },
    {
        "paper_id": 1312.3211,
        "authors": "A. H. Davison and T. Sidogi",
        "title": "Barrier Option Pricing",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use Lie symmetry methods to price certain types of barrier options.\nUsually Lie symmetry methods cannot be used to solve the Black-Scholes equation\nfor options because the function defining the maturity condition for an option\nis not smooth. However, for barrier options, this restriction can be\naccommodated and a symmetry analysis utilised to find new solutions.\n"
    },
    {
        "paper_id": 1312.3247,
        "authors": "Vadim Nastasiuk",
        "title": "Emergent quantum mechanics of finances",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.02.037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is an attempt at understanding the quantum-like dynamics of\nfinancial markets in terms of non-differentiable price-time continuum having\nfractal properties. The main steps of this development are the statistical\nscaling, the non-differentiability hypothesis, and the equations of motion\nentailed by this hypothesis. From perspective of the proposed theory the\ndynamics of S&P500 index are analyzed.\n"
    },
    {
        "paper_id": 1312.3314,
        "authors": "Matthew Lorig, Stefano Pagliarani, Andrea Pascucci",
        "title": "Analytical expansions for parabolic equations",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the Cauchy problem associated with a general parabolic partial\ndifferential equation in $d$ dimensions. We find a family of closed-form\nasymptotic approximations for the unique classical solution of this equation as\nwell as rigorous short-time error estimates. Using a boot-strapping technique,\nwe also provide convergence results for arbitrarily large time intervals.\n"
    },
    {
        "paper_id": 1312.3349,
        "authors": "Igor Skachkov",
        "title": "Market Impact Paradoxes",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The market impact (MI) of Volume Weighted Average Price (VWAP) orders is a\nconvex function of a trading rate, but most empirical estimates of transaction\ncost are concave functions. How is this possible? We show that isochronic\n(constant trading time) MI is slightly convex, and isochoric (constant trading\nvolume) MI is concave. We suggest a model that fits all trading regimes and\nguarantees no-dynamic-arbitrage.\n"
    },
    {
        "paper_id": 1312.3789,
        "authors": "Patrick Henaff (IAE Paris), Ismail Laachir (UMA), Francesco Russo\n  (UMA)",
        "title": "Gas storage valuation and hedging. A quantification of the model risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0219493713500196",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the valuation and hedging of gas storage facilities,\nusing a spot-based valuation framework coupled with a financial hedging\nstrategy implemented with futures contracts. The first novelty consist in\nproposing a model that unifies the dynamics of the futures curve and the spot\nprice, which accounts for the main stylized facts of the US natural gas market,\nsuch as seasonality and presence of price spikes. The second aspect of the\npaper is related to the quantification of model uncertainty related to the spot\ndynamics.\n"
    },
    {
        "paper_id": 1312.3826,
        "authors": "Hao Liao, Rui Xiao, Duanbing Chen, Matus Medo, Yi-Cheng Zhang",
        "title": "Firm competition in a probabilistic framework of consumer choice",
        "comments": "9 pages, 7 figures",
        "journal-ref": "Physica A 400, 47, 2014",
        "doi": "10.1016/j.physa.2013.12.026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a probabilistic consumer choice framework based on information\nasymmetry between consumers and firms. This framework makes it possible to\nstudy market competition of several firms by both quality and price of their\nproducts. We find Nash market equilibria and other optimal strategies in\nvarious situations ranging from competition of two identical firms to firms of\ndifferent sizes and firms which improve their efficiency.\n"
    },
    {
        "paper_id": 1312.3894,
        "authors": "G. D'Amico, F. Petroni, F. Prattico",
        "title": "Semi-Markov Models in High Frequency Finance: A Review",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1109.4259,\n  arXiv:1205.2551, arXiv:1103.6143, arXiv:1305.0436, arXiv:1202.3535",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we describe three stochastic models based on a semi-Markov\nchains approach and its generalizations to study the high frequency price\ndynamics of traded stocks. The three models are: a simple semi-Markov chain\nmodel, an indexed semi-Markov chain model and a weighted indexed semi-Markov\nchain model. We show, through Monte Carlo simulations, that the models are able\nto reproduce important stylized facts of financial time series as the\npersistence of volatility. In particular, we analyzed high frequency data from\nthe Italian stock market from the first of January 2007 until end of December\n2010 and we apply to it the semi-Markov chain model and the indexed semi-Markov\nchain model. The last model, instead, is applied to data from Italian and\nGerman stock markets from January 1, 2007 until the end of December 2010.\n"
    },
    {
        "paper_id": 1312.3917,
        "authors": "Erhan Bayraktar and Xiang Yu",
        "title": "On the Market Viability under Proportional Transaction Costs",
        "comments": "Final version. To appear in Mathematical Finance. Keywords:\n  Proportional Transaction Costs, (Robust) No Unbounded Profit with Bounded\n  Risk, Strictly Consistent Local Martingale Systems, (Robust) No Local\n  Arbitrage with Bounded Portfolios, Utility Maximization, Market Viability,\n  Num\\'eraire Portfolios",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the market viability with proportional transaction costs.\nInstead of requiring the existence of strictly consistent price systems (SCPS)\nas in the literature, we show that strictly consistent local martingale systems\n(SCLMS) can successfully serve as the dual elements such that the market\nviability can be verified. We introduce two weaker notions of no arbitrage\nconditions on market models named no unbounded profit with bounded risk (NUPBR)\nand no local arbitrage with bounded portfolios (NLABP). In particular, we show\nthat the NUPBR and NLABP conditions in the robust sense for the smaller bid-ask\nspreads is the equivalent characterization of the existence of SCLMS for\ngeneral market models. We also discuss the implications for the utility\nmaximization problem.\n"
    },
    {
        "paper_id": 1312.4227,
        "authors": "Jarno Talponen",
        "title": "Matching distributions: Asset pricing with density shape correction",
        "comments": "JEL: G10, G12, G13",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a statistical-static hedging technique for pricing assets\nconsidered as single-step stochastic cash flows. The valuation is based on\nconstructing in a canonical way a European style derivative on a benchmark\nsecurity such that the physical payoff distribution coincides with the\n(corrected) physical asset price distribution. It turns out that this pricing\ntechnique is economically viable under some natural cases. The fundamental\nproperties of the pricing rule arising in this way are investigated here. This\ngives rise to a novel way of estimating state price density. Our approach has\nsome tangible benefits: its principle is transparent, and it is easy to\nimplement numerically while avoiding many issues typically involved in such an\nestimation. As an application, it is shown how this method can be used in\nperforming kurtosis corrections to the standard Black-Scholes-Merton model by a\nmixture of several types of distributions. In fact, the technique is\nnon-parametric in nature, and it can handle in principle any physical\ndistribution, e.g., a multimodal one. Some other interesting applications are\ndiscussed as well.\n"
    },
    {
        "paper_id": 1312.4296,
        "authors": "Claudio Fontana",
        "title": "No-arbitrage conditions and absolutely continuous changes of measure",
        "comments": "14 pages. Arbitrage, Credit and Informational Risks (C. Hillairet, M.\n  Jeanblanc and Y. Jiao, eds.), Peking University Series in Mathematics, Vol.\n  6, World Scientific, 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the stability of several no-arbitrage conditions with respect to\nabsolutely continuous, but not necessarily equivalent, changes of measure. We\nfirst consider models based on continuous semimartingales and show that\nno-arbitrage conditions weaker than NA and NFLVR are always stable. Then, in\nthe context of general semimartingale models, we show that an absolutely\ncontinuous change of measure does never introduce arbitrages of the first kind\nas long as the change of measure density process can reach zero only\ncontinuously.\n"
    },
    {
        "paper_id": 1312.4385,
        "authors": "Claudia Ceci, Katia Colaneri and Alessandra Cretarola",
        "title": "Local risk-minimization under restricted information to asset prices",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the local risk-minimization approach for a\nsemimartingale financial market where there are restrictions on the available\ninformation to agents who can observe at least the asset prices. We\ncharacterize the optimal strategy in terms of suitable decompositions of a\ngiven contingent claim, with respect to a filtration representing the\ninformation level, even in presence of jumps. Finally, we discuss some\npractical examples in a Markovian framework and show that the computation of\nthe optimal strategy leads to filtering problems under the real-world\nprobability measure and under the minimalmartingale measure.\n"
    },
    {
        "paper_id": 1312.4443,
        "authors": "Tommaso Paletta, Arturo Leccadito, Radu Tunaru",
        "title": "Pricing and Hedging Basket Options with Exact Moment Matching",
        "comments": "35 pages, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Theoretical models applied to option pricing should take into account the\nempirical characteristics of the underlying financial time series. In this\npaper, we show how to price basket options when assets follow a shifted\nlog-normal process with jumps capable of accommodating negative skewness. Our\ntechnique is based on the Hermite polynomial expansion that can match exactly\nthe first m moments of the model implied-probability distribution. This method\nis shown to provide superior results for basket options not only with respect\nto pricing but also for hedging.\n"
    },
    {
        "paper_id": 1312.4622,
        "authors": "Jack Sarkissian",
        "title": "Coupled mode theory of stock price formation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a theory of bid and ask price dynamics where the two prices form\ndue to interaction of buy and sell orders. In this model the two prices are\nrepresented by eigenvalues of a 2x2 price operator corresponding to \"bid\" and\n\"ask\" eigenstates. Matrix elements of price operator fluctuate in time which\nresults in phase jitter for eigenstates. We show that the theory reflects very\nimportant characteristics of bid and ask dynamics and order density in the\norder book. Calibration examples are provided for stocks at various time\nscales. Lastly, this model allows to quantify and measure risk associated with\nspread and its fluctuations.\n"
    },
    {
        "paper_id": 1312.4803,
        "authors": "Pawe{\\l} O\\'swi\\k{e}cimka, Stanis{\\l}aw Dro\\.zd\\.z, Robert\n  G\\k{e}barowski, Andrzej Z. G\\'orski, Jaros{\\l}aw Kwapie\\'n",
        "title": "Multiscaling edge effects in an agent-based money emergence model",
        "comments": "15 pages, 7 figures",
        "journal-ref": "Acta Phys. Pol. B 46, 1579-1592 (2015)",
        "doi": "10.5506/APhysPolB.46.1579",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An agent-based computational economical toy model for the emergence of money\nfrom the initial barter trading, inspired by Menger's postulate that money can\nspontaneously emerge in a commodity exchange economy, is extensively studied.\nThe model considered, while manageable, is significantly complex, however. It\nis already able to reveal phenomena that can be interpreted as emergence and\ncollapse of money as well as the related competition effects. In particular, it\nis shown that - as an extra emerging effect - the money lifetimes near the\ncritical threshold value develop multiscaling, which allow one to set parallels\nto critical phenomena and, thus, to the real financial markets.\n"
    },
    {
        "paper_id": 1312.4979,
        "authors": "Huy N. Chau and Peter Tankov",
        "title": "Market models with optimal arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct and study market models admitting optimal arbitrage. We say that\na model admits optimal arbitrage if it is possible, in a zero-interest rate\nsetting, starting with an initial wealth of 1 and using only positive\nportfolios, to superreplicate a constant c>1. The optimal arbitrage strategy is\nthe strategy for which this constant has the highest possible value. Our\ndefinition of optimal arbitrage is similar to the one in Fernholz and Karatzas\n(2010), where optimal relative arbitrage with respect to the market portfolio\nis studied. In this work we present a systematic method to construct market\nmodels where the optimal arbitrage strategy exists and is known explicitly. We\nthen develop several new examples of market models with arbitrage, which are\nbased on economic agents' views concerning the impossibility of certain events\nrather than ad hoc constructions. We also explore the concept of fragility of\narbitrage introduced in Guasoni and Rasonyi (2012), and provide new examples of\narbitrage models which are not fragile in this sense.\n"
    },
    {
        "paper_id": 1312.5073,
        "authors": "Anne Balter, Antoon Pelsser, Peter Schotman",
        "title": "Extrapolating the term structure of interest rates with parameter\n  uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing extremely long-dated liabilities market consistently deals with the\ndecline in liquidity of financial instruments on long maturities. The aim is to\nquantify the uncertainty of rates up to maturities of a century. We assume that\nthe interest rates follow the affine mean-reverting Vasicek model. We model\nparameter uncertainty by Bayesian distributions over the parameters. The\ncross-sectional and time series parameters are obtained via the restricted\nbivariate VAR(1) model. The empirical example shows extremely low confidence in\nlong term extrapolations due to the accumulated effect of the mean-reversion`s\nbehaviour close to the unit root.\n"
    },
    {
        "paper_id": 1312.5116,
        "authors": "David R. Banos, Giulia Di Nunno, Frank Proske",
        "title": "Sensitivity analysis in a market with memory",
        "comments": "Withdrawn by the authors due to an error in equation (2.6). A new\n  work is in preparation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A general market model with memory is considered in terms of stochastic\nfunctional differential equations. We aim at representation formulae for the\nsensitivity analysis of the dependence of option prices on the memory. This\nimplies a generalization of the concept of delta.\n"
    },
    {
        "paper_id": 1312.5271,
        "authors": "Michel Fliess (LIX, AL.I.E.N.), C\\'edric Join (AL.I.E.N., CRAN, INRIA\n  Lille - Nord Europe)",
        "title": "Systematic and multifactor risk models revisited",
        "comments": "First Paris Financial Management Conference, Paris : France (2013)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systematic and multifactor risk models are revisited via methods which were\nalready successfully developed in signal processing and in automatic control.\nThe results, which bypass the usual criticisms on those risk modeling, are\nillustrated by several successful computer experiments.\n"
    },
    {
        "paper_id": 1312.5496,
        "authors": "Carles Bret\\'o",
        "title": "On idiosyncratic stochasticity of financial leverage effects",
        "comments": "8 pages, 2 figures",
        "journal-ref": "Statistics & Probability Letters 91 (2014) 20-26",
        "doi": "10.1016/j.spl.2014.04.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model leverage as stochastic but independent of return shocks and of\nvolatility and perform likelihood-based inference via the recently developed\niterated filtering algorithm using S&P500 data, contributing new evidence to\nthe still slim empirical support for random leverage variation.\n"
    },
    {
        "paper_id": 1312.5617,
        "authors": "Olivier Gu\\'eant, Jiang Pu, Guillaume Royer",
        "title": "Accelerated Share Repurchase: pricing and execution strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we consider the optimal execution problem associated to\naccelerated share repurchase contracts. When firms want to repurchase their own\nshares, they often enter such a contract with a bank. The bank buys the shares\nfor the firm and is paid the average market price over the execution period,\nthe length of the period being decided upon by the bank during the buying\nprocess. Mathematically, the problem is new and related to both option pricing\n(Asian and Bermudan options) and optimal execution. We provide a model, along\nwith associated numerical methods, to determine the optimal stopping time and\nthe optimal buying strategy of the bank.\n"
    },
    {
        "paper_id": 1312.566,
        "authors": "Benjamin Jourdain (CERMICS, INRIA Paris-Rocquencourt), Julien Reygner\n  (CERMICS, LPMA)",
        "title": "Capital distribution and portfolio performance in the mean-field Atlas\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a mean-field version of rank-based models of equity markets such as\nthe Atlas model introduced by Fernholz in the framework of Stochastic Portfolio\nTheory. We obtain an asymptotic description of the market when the number of\ncompanies grows to infinity. Then, we discuss the long-term capital\ndistribution. We recover the Pareto-like shape of capital distribution curves\nusually derived from empirical studies, and provide a new description of the\nphase transition phenomenon observed by Chatterjee and Pal. Finally, we address\nthe performance of simple portfolio rules and highlight the influence of the\nvolatility structure on the growth of portfolios.\n"
    },
    {
        "paper_id": 1312.5693,
        "authors": "Alexander Lipton, Andrey Gal, and Andris Lasis",
        "title": "Pricing of vanilla and first generation exotic options in the local\n  stochastic volatility framework: survey and new results",
        "comments": "59 pages, 8 figures, submitted for publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic volatility (SV) and local stochastic volatility (LSV) processes\ncan be used to model the evolution of various financial variables such as FX\nrates, stock prices, and so on. Considerable efforts have been devoted to\npricing derivatives written on underliers governed by such processes. Many\nissues remain, though, including the efficacy of the standard alternating\ndirection implicit (ADI) numerical methods for solving SV and LSV pricing\nproblems. In general, the amount of required computations for these methods is\nvery substantial. In this paper we address some of these issues and propose a\nviable alternative to the standard ADI methods based on Galerkin-Ritz ideas. We\nalso discuss various approaches to solving the corresponding pricing problems\nin a semi-analytical fashion. We use the fact that in the zero correlation case\nsome of the pricing problems can be solved analytically, and develop a\nclosed-form series expansion in powers of correlation. We perform a thorough\nbenchmarking of various numerical solutions by using analytical and\nsemi-analytical solutions derived in the paper.\n"
    },
    {
        "paper_id": 1312.5807,
        "authors": "Ting Zhang, Hwai-Chung Ho, Martin Wendler, Wei Biao Wu",
        "title": "Block Sampling under Strong Dependence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper considers the block sampling method for long-range dependent\nprocesses. Our theory generalizes earlier ones by Hall, Jing and Lahiri (1998)\non functionals of Gaussian processes and Nordman and Lahiri (2005) on linear\nprocesses. In particular, we allow nonlinear transforms of linear processes.\nUnder suitable conditions on physical dependence measures, we prove the\nvalidity of the block sampling method. The problem of estimating the\nself-similar index is also studied.\n"
    },
    {
        "paper_id": 1312.5911,
        "authors": "Adam D. Bull",
        "title": "Estimating time-changes in noisy L\\'evy models",
        "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1250 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 5, 2026-2057",
        "doi": "10.1214/14-AOS1250",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In quantitative finance, we often model asset prices as a noisy Ito\nsemimartingale. As this model is not identifiable, approximating by a\ntime-changed Levy process can be useful for generative modelling. We give a new\nestimate of the normalised volatility or time change in this model, which\nobtains minimax convergence rates, and is unaffected by infinite-variation\njumps. In the semimartingale model, our estimate remains accurate for the\nnormalised volatility, obtaining convergence rates as good as any previously\nimplied in the literature.\n"
    },
    {
        "paper_id": 1312.5919,
        "authors": "Nico Achtsis and Dirk Nuyens",
        "title": "A Monte Carlo method for optimal portfolio executions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traders are often faced with large block orders in markets with limited\nliquidity and varying volatility. Executing the entire order at once usually\nincurs a large trading cost because of this limited liquidity. In order to\nminimize this cost traders split up large orders over time. Varying volatility\nhowever implies that they now take on price risk, as the underlying assets'\nprices can move against the traders over the execution period. This execution\nproblem therefore requires a careful balancing between trading slow to reduce\nliquidity cost and trading fast to reduce the volatility cost. R. Almgren\nsolved this problem for a market with one asset and stochastic liquidity and\nvolatility parameters, using a mean-variance framework. This leads to a\nnonlinear PDE that needs to be solved numerically. We propose a different\napproach using (quasi-)Monte Carlo which can handle any number of assets.\nFurthermore, our method can be run in real-time and allows the trader to change\nthe parameters of the underlying stochastic processes on-the-fly.\n"
    },
    {
        "paper_id": 1312.6032,
        "authors": "Giulia Di Nunno, Steffen Sjursen",
        "title": "Information and optimal investment in defaultable assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal investment in an asset subject to risk of default for\ninvestors that rely on different levels of information. The price dynamics can\ninclude noises both from a Wiener process and a Poisson random measure with\ninfinite activity. The default events are modelled via a counting process in\nline with large part of the literature in credit risk. In order to deal with\nboth cases of inside and partial information we consider the framework of the\nanticipating calculus of forward integration. This does not require a priori\nassumptions typical of the framework of enlargement of filtrations. We find\nnecessary and sufficient conditions for the existence of a locally maximizing\nportfolio of the expected utility at terminal time. We consider a large class\nof utility functions. In addition we show that the existence of the solution\nimplies the semi-martingale property of the noises driving the stock. Some\ndiscussion on unicity of the maxima is included.\n"
    },
    {
        "paper_id": 1312.635,
        "authors": "Caihua Chen, Xindan Li, Caleb Tolman, Suyang Wang, Yinyu Ye",
        "title": "Sparse Portfolio Selection via Quasi-Norm Regularization",
        "comments": "34 pages,7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  In this paper, we propose $\\ell_p$-norm regularized models to seek\nnear-optimal sparse portfolios. These sparse solutions reduce the complexity of\nportfolio implementation and management. Theoretical results are established to\nguarantee the sparsity of the second-order KKT points of the $\\ell_p$-norm\nregularized models. More interestingly, we present a theory that relates\nsparsity of the KKT points with Projected correlation and Projected Sharpe\nratio. We also design an interior point algorithm to obtain an approximate\nsecond-order KKT solution of the $\\ell_p$-norm models in polynomial time with a\nfixed error tolerance, and then test our $\\ell_p$-norm modes on S&P 500\n(2008-2012) data and international market data.\\ The computational results\nillustrate that the $\\ell_p$-norm regularized models can generate portfolios of\nany desired sparsity with portfolio variance and portfolio return comparable to\nthose of the unregularized Markowitz model with cardinality constraint. Our\nanalysis of a combined model lead us to conclude that sparsity is not directly\nrelated to overfitting at all. Instead, we find that sparsity moderates\noverfitting only indirectly. A combined $\\ell_1$-$\\ell_p$ model shows that the\nproper choose of leverage, which is the amount of additional buying-power\ngenerated by selling short can mitigate overfitting; A combined\n$\\ell_2$-$\\ell_p$ model is able to produce extremely high performing portfolios\nthat exceeded the 1/N strategy and all $\\ell_1$ and $\\ell_2$ regularized\nportfolios.\n"
    },
    {
        "paper_id": 1312.6443,
        "authors": "Scott Lawrence and Qin Liu and Victor M. Yakovenko",
        "title": "Global inequality in energy consumption from 1980 to 2010",
        "comments": "15 pages, 14 figures, 2 movies; v.2 correction to the inset in the\n  movie Lorenz-Energy.mov, no changes in the paper",
        "journal-ref": "Entropy 15, 5565-5579 (2013)",
        "doi": "10.3390/e15125565",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the global probability distribution of energy consumption per capita\naround the world using data from the U.S. Energy Information Administration\n(EIA) for 1980-2010. We find that the Lorenz curves have moved up during this\ntime period, and the Gini coefficient G has decreased from 0.66 in 1980 to 0.55\nin 2010, indicating a decrease in inequality. The global probability\ndistribution of energy consumption per capita in 2010 is close to the\nexponential distribution with G=0.5. We attribute this result to the\nglobalization of the world economy, which mixes the world and brings it closer\nto the state of maximal entropy. We argue that global energy production is a\nlimited resource that is partitioned among the world population. The most\nprobable partition is the one that maximizes entropy, thus resulting in the\nexponential distribution function. A consequence of the latter is the law of\n1/3: the top 1/3 of the world population consumes 2/3 of produced energy. We\nalso find similar results for the global probability distribution of CO2\nemissions per capita.\n"
    },
    {
        "paper_id": 1312.6456,
        "authors": "Mohammad Mousavi, Peter W. Glynn",
        "title": "Exact Simulation of Non-stationary Reflected Brownian Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops the first method for the exact simulation of reflected\nBrownian motion (RBM) with non-stationary drift and infinitesimal variance. The\nrunning time of generating exact samples of non-stationary RBM at any time $t$\nis uniformly bounded by $\\mathcal{O}(1/\\bar\\gamma^2)$ where $\\bar\\gamma$ is the\naverage drift of the process. The method can be used as a guide for planning\nsimulations of complex queueing systems with non-stationary arrival rates\nand/or service time.\n"
    },
    {
        "paper_id": 1312.6804,
        "authors": "Teruyoshi Kobayashi",
        "title": "A model of financial contagion with variable asset returns may be\n  replaced with a simple threshold model of cascades",
        "comments": "8 pages, including 2 figures",
        "journal-ref": "Economics Letters 124, 113-116, 2014",
        "doi": "10.1016/j.econlet.2014.05.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I show the equivalence between a model of financial contagion and the\nthreshold model of global cascades proposed by Watts (2002). The model\nfinancial network comprises banks that hold risky external assets as well as\ninterbank assets. It is shown that a simple threshold model can replicate the\nsize and the frequency of financial contagion without using information about\nindividual balance sheets. Keywords: financial network, cascades, financial\ncontagion, systemic risk.\n"
    },
    {
        "paper_id": 1312.6841,
        "authors": "Zhongliang Tuo",
        "title": "Hedging Against the Interest-rate Risk by Measuring the Yield-curve\n  Movement",
        "comments": "12 pages, 2 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By adopting the polynomial interpolation method, we propose an approach to\nhedge against the interest-rate risk of the default-free bonds by measuring the\nnonparallel movement of the yield-curve, such as the translation, the rotation\nand the twist. The empirical analysis shows that our hedging strategies are\ncomparable to traditional duration-convexity strategy, or even better when we\nhave more suitable hedging instruments on hand. The article shows that this\nstrategy is flexible and robust to cope with the interest-rate risk and can\nhelp fine-tune a position as time changes.\n"
    },
    {
        "paper_id": 1312.7057,
        "authors": "Ting Ting Chen and Tetsuya Takaishi",
        "title": "Empirical Study of the GARCH model with Rational Errors",
        "comments": "10 pages",
        "journal-ref": "Journal of Physics: Conference Series 454 (2013) 012040",
        "doi": "10.1088/1742-6596/454/1/012040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the GARCH model with a fat-tailed error distribution described by a\nrational function and apply it for the stock price data on the Tokyo Stock\nExchange. To determine the model parameters we perform the Bayesian inference\nto the model. The Bayesian inference is implemented by the Metropolis-Hastings\nalgorithm with an adaptive multi-dimensional Student's t-proposal density. In\norder to compare the model with the GARCH model with the standard normal errors\nwe calculate information criterions: AIC and DIC, and find that both criterions\nfavor the GARCH model with a rational error distribution. We also calculate the\naccuracy of the volatility by using the realized volatility and find that a\ngood accuracy is obtained for the GARCH model with a rational error\ndistribution. Thus we conclude that the GARCH model with a rational error\ndistribution is superior to the GARCH model with the normal errors and it can\nbe used as an alternative GARCH model to those with other fat-tailed\ndistributions.\n"
    },
    {
        "paper_id": 1312.7328,
        "authors": "Matthew Lorig, Stefano Pagliarani, Andrea Pascucci",
        "title": "A family of density expansions for L\\'evy-type processes",
        "comments": "30 Pages, 3 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1304.1849",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a defaultable asset whose risk-neutral pricing dynamics are\ndescribed by an exponential Levy-type martingale subject to default. This class\nof models allows for local volatility, local default intensity, and a locally\ndependent Levy measure. Generalizing and extending the novel adjoint expansion\ntechnique of Pagliarani, Pascucci, and Riga (2013), we derive a family of\nasymptotic expansions for the transition density of the underlying as well as\nfor European-style option prices and defaultable bond prices. For the density\nexpansion, we also provide error bounds for the truncated asymptotic series.\nOur method is numerically efficient; approximate transition densities and\nEuropean option prices are computed via Fourier transforms; approximate bond\nprices are computed as finite series. Additionally, as in Pagliarani et al.\n(2013), for models with Gaussian-type jumps, approximate option prices can be\ncomputed in closed form. Sample Mathematica code is provided.\n"
    },
    {
        "paper_id": 1312.7346,
        "authors": "Godfrey Charles-Cadogan and John A. Cole",
        "title": "Bankruptcy Risk Induced by Career Concerns of Regulators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model in which a regulator employs mechanism design to embed\nher human capital beta signal(s) in a firm's capital structure, in order to\nenhance the value of her post career change indexed executive stock option\ncontract with the firm. We prove that the agency cost of this revolving door\nbehavior increases the firm's financial leverage, bankruptcy risk, and affects\nestimation of firm value at risk (VaR).\n"
    },
    {
        "paper_id": 1312.736,
        "authors": "Alexander Schied and Tao Zhang",
        "title": "A state-constrained differential game arising in optimal portfolio\n  liquidation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider $n$ risk-averse agents who compete for liquidity in an\nAlmgren--Chriss market impact model. Mathematically, this situation can be\ndescribed by a Nash equilibrium for a certain linear-quadratic differential\ngame with state constraints. The state constraints enter the problem as\nterminal boundary conditions for finite and infinite time horizons. We prove\nexistence and uniqueness of Nash equilibria and give closed-form solutions in\nsome special cases. We also analyze qualitative properties of the equilibrium\nstrategies and provide corresponding financial interpretations.\n"
    },
    {
        "paper_id": 1312.746,
        "authors": "Yuri Biondi and Simone Righi",
        "title": "What does the financial market pricing do? A simulation analysis with a\n  view to systemic volatility, exuberance and vagary",
        "comments": "30 pages, 5 figures. 25th Annual EAEPE Conference 2013, Research Area\n  S (Evolutionary Economic Simulation), Paris, November 2013",
        "journal-ref": "Journal of Economic Interaction and Coordination, May 2015",
        "doi": "10.1007/s11403-015-0159-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Biondi et al. (2012) develop an analytical model to examine the emergent\ndynamic properties of share market price formation over time, capable to\ncapture important stylized facts. These latter properties prove to be sensitive\nto regulatory regimes for fundamental information provision, as well as to\nmarket confidence conditions among actual and potential investors. Regimes\nbased upon mark-to-market (fair value) measurement of traded security, while\ngenerating higher linear correlation between market prices and fundamental\nsignals, also involve higher market instability and volatility. These regimes\nalso incur more relevant episodes of market exuberance and vagary in some\nregions of the market confidence space, where lower market liquidity further\noccurs.\n"
    },
    {
        "paper_id": 1312.7545,
        "authors": "Peter Sarlin and Henrik J. Nyman",
        "title": "The process of macroprudential oversight in Europe",
        "comments": "Pre-print submitted for publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2007--2008 financial crisis has paved the way for the use of\nmacroprudential policies in supervising the financial system as a whole. This\npaper views macroprudential oversight in Europe as a process, a sequence of\nactivities with the ultimate aim of safeguarding financial stability. To\nconceptualize a process in this context, we introduce the notion of a public\ncollaborative process (PCP). PCPs involve multiple organizations with a common\nobjective, where a number of dispersed organizations cooperate under various\nunstructured forms and take a collaborative approach to reaching the final\ngoal. We argue that PCPs can and should essentially be managed using the tools\nand practices common for business processes. To this end, we conduct an\nassessment of process readiness for macroprudential oversight in Europe. Based\nupon interviews with key European policymakers and supervisors, we provide an\nanalysis model to assess the maturity of five process enablers for\nmacroprudential oversight. With the results of our analysis, we give clear\nrecommendations on the areas that need further attention when macroprudential\noversight is being developed, in addition to providing a general purpose\nframework for monitoring the impact of improvement efforts.\n"
    },
    {
        "paper_id": 1312.786,
        "authors": "Wolfgang Kuhle",
        "title": "A Global Game with Heterogenous Priors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper relaxes the common prior assumption in the public and private\ninformation game of Morris and Shin (2000, 2004). For the generalized game,\nwhere the agent's prior expectations are heterogenous, it derives a sharp\ncondition for the emergence of unique/multiple equilibria. This condition\nindicates that unique equilibria are played if player's public disagreement is\nsubstantial. If disagreement is small, equilibrium multiplicity depends on the\nrelative precisions of private signals and subjective priors. Extensions to\nenvironments with public signals of exogenous and endogenous quality show that\nprior heterogeneity, unlike heterogeneity in private information, provides a\nrobust anchor for unique equilibria. Finally, irrespective of whether priors\nare common or not, we show that public signals can ensure equilibrium\nuniqueness, rather than multiplicity, if they are sufficiently precise.\n"
    },
    {
        "paper_id": 1401.0124,
        "authors": "Hayafumi Watanabe",
        "title": "Mean field approximation for biased diffusion on Japanese inter-firm\n  trading network",
        "comments": null,
        "journal-ref": "PLoS ONE 9(3): e91704 (2014)",
        "doi": "10.1371/journal.pone.0091704",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By analysing the financial data of firms across Japan, a nonlinear power law\nwith an exponent of 1.3 was observed between the number of business partners\n(i.e. the degree of the inter-firm trading network) and sales. In a previous\nstudy using numerical simulations, we found that this scaling can be explained\nby both the money-transport model, where a firm (i.e. customer) distributes\nmoney to its out-edges (suppliers) in proportion to the in-degree of\ndestinations, and by the correlations among the Japanese inter-firm trading\nnetwork. However, in this previous study, we could not specifically identify\nwhat types of structure properties (or correlations) of the network determine\nthe 1.3 exponent. In the present study, we more clearly elucidate the\nrelationship between this nonlinear scaling and the network structure by\napplying mean-field approximation of the diffusion in a complex network to this\nmoney-transport model. Using theoretical analysis, we obtained the mean-field\nsolution of the model and found that, in the case of the Japanese firms, the\nscaling exponent of 1.3 can be determined from the power law of the average\ndegree of the nearest neighbours of the network with an exponent of -0.7.\n"
    },
    {
        "paper_id": 1401.0301,
        "authors": "Reza Farrahi Moghaddam, Fereydoun Farrahi Moghaddam, and Mohamed\n  Cheriet",
        "title": "IIGHGINT: A generalization to the modified GHG intensity universal\n  indicator toward a production/consumption insensitive border carbon tax",
        "comments": "17 pages, 3 figures. Pre-print of a chapter submitted to the book\n  \"Taxes and the Economy: Government Policies, Macroeconomic Factors and\n  Impacts on Consumption and the Environment\" (NOVA Science Publishers)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A global agreement on how to reduce and cap human footprint, especially their\nGHG emissions, is very unlikely in near future. At the same time, bilateral\nagreements would be inefficient because of their neural and balanced nature.\nTherefore, unilateral actions would have attracted attention as a practical\noption. However, any unilateral action would most likely fail if it is not fair\nand also if it is not consistent with the world trade organization's (WTO's)\nrules, considering highly heterogeneity of the global economy. The modified GHG\nintensity (MGHGINT) indicator, hereafter called Inequality-adjusted\nProduction-based GHGINT (IPGHGINT), was put forward to address this need in the\nform of a universal indicator applicable to every region regardless of its\neconomic and social status. Nonetheless, the original MGHGINT indicator ignores\nhidden consumption-related emissions, and therefore it could be unfair to some\nproduction-oriented regions in the current bipolar production/consumption\nworld. Here, we propose two generalizations, called Inequality-adjusted\nConsumption-based GHGINT (ICGHGINT) and Inequality-adjusted\nProduction/Consumption-Insensitive GHGINT (IIGHGINT), to the IPGHGINT in order\nto combine both production and consumption emissions in a unified and balanced\nmanner. The impact of this generalizations on the associated border carbon tax\nrates is evaluated in order to validate their practicality.\n"
    },
    {
        "paper_id": 1401.0462,
        "authors": "Chester Curme, Michele Tumminello, Rosario N. Mantegna, H. Eugene\n  Stanley, Dror Y. Kenett",
        "title": "Emergence of statistically validated financial intraday lead-lag\n  relationships",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  According to the leading models in modern finance, the presence of intraday\nlead-lag relationships between financial assets is negligible in efficient\nmarkets. With the advance of technology, however, markets have become more\nsophisticated. To determine whether this has resulted in an improved market\nefficiency, we investigate whether statistically significant lagged correlation\nrelationships exist in financial markets. We introduce a numerical method to\nstatistically validate links in correlation-based networks, and employ our\nmethod to study lagged correlation networks of equity returns in financial\nmarkets. Crucially, our statistical validation of lead-lag relationships\naccounts for multiple hypothesis testing over all stock pairs. In an analysis\nof intraday transaction data from the periods 2002--2003 and 2011--2012, we\nfind a striking growth in the networks as we increase the frequency with which\nwe sample returns. We compute how the number of validated links and the\nmagnitude of correlations change with increasing sampling frequency, and\ncompare the results between the two data sets. Finally, we compare topological\nproperties of the directed correlation-based networks from the two periods\nusing the in-degree and out-degree distributions and an analysis of three-node\nmotifs. Our analysis suggests a growth in both the efficiency and instability\nof financial markets over the past decade.\n"
    },
    {
        "paper_id": 1401.0562,
        "authors": "Maxim Bichuch and Ronnie Sircar",
        "title": "Optimal Investment with Transaction Costs and Stochastic Volatility",
        "comments": "27 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two major financial market complexities are transaction costs and uncertain\nvolatility, and we analyze their joint impact on the problem of portfolio\noptimization. When volatility is constant, the transaction costs optimal\ninvestment problem has a long history, especially in the use of asymptotic\napproximations when the cost is small. Under stochastic volatility, but with no\ntransaction costs, the Merton problem under general utility functions can also\nbe analyzed with asymptotic methods. Here, we look at the long-run growth rate\nproblem when both complexities are present, using separation of time scales\napproximations. This leads to perturbation analysis of an eigenvalue problem.\nWe find the first term in the asymptotic expansion in the time scale parameter,\nof the optimal long-term growth rate, and of the optimal strategy, for fixed\nsmall transaction costs.\n"
    },
    {
        "paper_id": 1401.0677,
        "authors": "Wei Chen",
        "title": "G-Doob-Meyer Decomposition and its Application in Bid-Ask Pricing for\n  American Contingent Claim Under Knightian Uncertainty",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The target of this paper is to establish the bid-ask pricing frame work for\nthe American contingent claims against risky assets with G-asset price systems\n(see \\cite{Chen2013b}) on the financial market under Knight uncertainty. First,\nwe prove G-Dooby-Meyer decomposition for G-supermartingale. Furthermore, we\nconsider bid-ask pricing American contingent claims under Knight uncertain, by\nusing G-Dooby-Meyer decomposition, we construct dynamic superhedge stragies for\nthe optimal stopping problem, and prove that the value functions of the optimal\nstopping problems are the bid and ask prices of the American contingent claims\nunder Knight uncertain. Finally, we consider a free boundary problem, prove the\nstrong solution existence of the free boundary problem, and derive that the\nvalue function of the optimal stopping problem is equivalent to the strong\nsolution to the free boundary problem.\n"
    },
    {
        "paper_id": 1401.0903,
        "authors": "Emmanuel Bacry and Jean-Francois Muzy",
        "title": "Second order statistics characterization of Hawkes processes and\n  non-parametric estimation",
        "comments": "25 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the jumps correlation matrix of a multivariate Hawkes process is\nrelated to the Hawkes kernel matrix through a system of Wiener-Hopf integral\nequations. A Wiener-Hopf argument allows one to prove that this system (in\nwhich the kernel matrix is the unknown) possesses a unique causal solution and\nconsequently that the second-order properties fully characterize a Hawkes\nprocess. The numerical inversion of this system of integral equations allows us\nto propose a fast and efficient method, which main principles were initially\nsketched in [Bacry and Muzy, 2013], to perform a non-parametric estimation of\nthe Hawkes kernel matrix. In this paper, we perform a systematic study of this\nnon-parametric estimation procedure in the general framework of marked Hawkes\nprocesses. We describe precisely this procedure step by step. We discuss the\nestimation error and explain how the values for the main parameters should be\nchosen. Various numerical examples are given in order to illustrate the broad\npossibilities of this estimation procedure ranging from 1-dimensional\n(power-law or non positive kernels) up to 3-dimensional (circular dependence)\nprocesses. A comparison to other non-parametric estimation procedures is made.\nApplications to high frequency trading events in financial markets and to\nearthquakes occurrence dynamics are finally considered.\n"
    },
    {
        "paper_id": 1401.1292,
        "authors": "Chih-Hao Lin, Chia-Seng Chang and Sai-Ping Li",
        "title": "An Empirical Method to Measure Stochasticity and Multifractality in\n  Nonlinear Time Series",
        "comments": "10 pages, 11 figures",
        "journal-ref": "Phys. Rev. E88(2013) 062912",
        "doi": "10.1103/PhysRevE.88.062912",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An empirical algorithm is used here to study the stochastic and multifractal\nnature of nonlinear time series. A parameter can be defined to quantitatively\nmeasure the deviation of the time series from a Wiener process so that the\nstochasticity of different time series can be compared. The local volatility of\nthe time series under study can be constructed using this algorithm and the\nmultifractal structure of the time series can be analyzed by using this local\nvolatility. As an example, we employ this method to analyze financial time\nseries from different stock markets. The result shows that while developed\nmarkets evolve very much like an Ito process, the emergent markets are far from\nefficient. Differences about the multifractal structures and leverage effects\nbetween developed and emergent markets are discussed. The algorithm used here\ncan be applied in a similar fashion to study time series of other complex\nsystems.\n"
    },
    {
        "paper_id": 1401.1457,
        "authors": "Anna Zaremba and Tomaso Aste",
        "title": "Measures of Causality in Complex Datasets with application to financial\n  data",
        "comments": "40 pages; 13 figures",
        "journal-ref": "Entropy 16 (2014) 2309-2349",
        "doi": "10.3390/e16042309",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates the causality structure of financial time series.\nWe concentrate on three main approaches to measuring causality: linear Granger\ncausality, kernel generalisations of Granger causality (based on ridge\nregression and the Hilbert--Schmidt norm of the cross-covariance operator) and\ntransfer entropy, examining each method and comparing their theoretical\nproperties, with special attention given to the ability to capture nonlinear\ncausality. We also present the theoretical benefits of applying non-symmetrical\nmeasures rather than symmetrical measures of dependence. We apply the measures\nto a range of simulated and real data. The simulated data sets were generated\nwith linear and several types of nonlinear dependence, using bivariate, as well\nas multivariate settings. An application to real-world financial data\nhighlights the practical difficulties, as well as the potential of the methods.\nWe use two real data sets: (1) U.S. inflation and one-month Libor; (2) S$\\&$P\ndata and exchange rates for the following currencies: AUDJPY, CADJPY, NZDJPY,\nAUDCHF, CADCHF, NZDCHF. Overall, we reach the conclusion that no single method\ncan be recognised as the best in all circumstances, and each of the methods has\nits domain of best applicability. We also highlight areas for improvement and\nfuture research.\n"
    },
    {
        "paper_id": 1401.161,
        "authors": "Luxi Chen",
        "title": "Computation of the \"Enrichment\" of a Value Functions of an Optimization\n  Problem on Cumulated Transaction-Costs through a Generalized Lax-Hopf Formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Lax-Hopf formula simplifies the value function of an intertemporal\noptimization (infinite dimensional) problem associated with a convex\ntransaction-cost function which depends only on the transactions (velocities)\nof a commodity evolution: it states that the value function is equal to the\nmarginal fonction of a finite dimensional problem with respect to durations and\naverage ransactions, much simpler to solve. The average velocity of the value\nfunction on a investment temporal window is regarded as an enrichment,\nproportional to the profit and inversely proportional to the investment\nduration. At optimum, the Lax-Hopf formula implies that the enrichment is equal\nto the cost of the average transaction on the investment temporal window. In\nthis study, we generalize the Lax-Hopf formula when the transaction-cost\nfunction depends also on time and commodity, for reducing the infinite\ndimensional problem to a finite dimensional problem. For that purpose, we\nintroduce the moderated ansaction-cost function which depends only on the\nduration and on a commodity. Here again, the generalized Lax-Hopf formula\nreduces the computation of the value function to the marginal fonction of an\noptimization problem on durations and commodities involving the moderated\ntransaction cost function. At optimum, the enrichment of the value function is\nstill equal to the moderated transition cost-function of average transaction.\n"
    },
    {
        "paper_id": 1401.1639,
        "authors": "Qian Lin and Frank Riedel",
        "title": "Optimal consumption and portfolio choice with ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider optimal consumption and portfolio choice in the presence of\nKnightian uncertainty in continuous-time. We embed the problem into the new\nframework of stochastic calculus for such settings, dealing in particular with\nthe issue of non-equivalent multiple priors. We solve the problem completely by\nidentifying the worst--case measure. Our setup also allows to consider interest\nrate uncertainty; we show that under some robust parameter constellations, the\ninvestor optimally puts all his wealth into the asset market, and does not save\nor borrow at all.\n"
    },
    {
        "paper_id": 1401.1757,
        "authors": "Mark Tucker and J. Mark Bull",
        "title": "An efficient algorithm for the calculation of reserves for non-unit\n  linked life policies",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The underlying stochastic nature of the requirements for the Solvency II\nregulations has introduced significant challenges if the required calculations\nare to be performed correctly, without resorting to excessive approximations,\nwithin practical timescales. It is generally acknowledged by practising\nactuaries within UK life offices that it is currently impossible to correctly\nfulfil the requirements imposed by Solvency II using existing computational\ntechniques based on commercially available valuation packages. Our work has\nalready shown that it is possible to perform profitability calculations at a\nfar higher rate than is achievable using commercial packages. One of the key\nfactors in achieving these gains is to calculate reserves using recurrence\nrelations that scale linearly with the number of time steps. Here, we present a\ngeneral vector recurrence relation which can be used for a wide range of\nnon-unit linked policies that are covered by Solvency II; such contracts\ninclude annuities, term assurances, and endowments. Our results suggest that by\nusing an optimised parallel implementation of this algorithm, on an affordable\nhardware platform, it is possible to perform the `brute force' approach to\ndemonstrating solvency in a realistic timescale (of the order of a few hours).\n"
    },
    {
        "paper_id": 1401.1851,
        "authors": "Robert A. Jarrow and Martin Larsson",
        "title": "Informational Efficiency under Short Sale Constraints",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A constrained informationally efficient market is defined to be one whose\nprice process arises as the outcome of some equilibrium where agents face\nrestrictions on trade. This paper investigates the case of short sale\nconstraints, a setting which despite its simplicity, generates new insights. In\nparticular, it is shown that short sale constrained informationally efficient\nmarkets always admit equivalent supermartingale measures and local martingale\ndeflators, but not necessarily local martingale measures. And if in addition\nsome local martingale deflator turns the price process into a true martingale,\nthen the market is constrained informationally efficient. Examples are given to\nillustrate the subtle phenomena that can arise in the presence of short sale\nconstraints, with particular attention to representative agent equilibria and\nthe different notions of no arbitrage.\n"
    },
    {
        "paper_id": 1401.1856,
        "authors": "Alexander Kushpel",
        "title": "Pricing of basket options I",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1309.4546",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing of high-dimensional options is a deep problem of the Theoretical\nFinancial Mathematics. In this article we present a new class of L\\'{e}vy\ndriven models of stock markets. In our opinion, any market model should be\nbased on a transparent and intuitively easily acceptable concept. In our case\nthis is a linear system of stochastic equations. Our market model is based on\nthe principle of inheritance, i.e. for the particular choice of parameters it\ncoincides with known models. Also, the model proposed is effectively\nnumerically realizable. For the class of models under cosideration, we give an\nexplicit representations of characteristic functions. This allows us us to\nconstruct a sequence of approximation formulas to price basket options. We show\nthat our approximation formulas have almost optimal rate of convergence in the\nsense of respective n-widths.\n"
    },
    {
        "paper_id": 1401.1888,
        "authors": "Li-Xin Wang",
        "title": "Dynamical Models of Stock Prices Based on Technical Trading Rules Part\n  I: The Models",
        "comments": null,
        "journal-ref": "IEEE Trans. on Fuzzy Systems, Vol. 23, No. 4, pp. 787-801, 2015",
        "doi": "10.1109/TFUZZ.2014.2327994",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we use fuzzy systems theory to convert the technical trading\nrules commonly used by stock practitioners into excess demand functions which\nare then used to drive the price dynamics. The technical trading rules are\nrecorded in natural languages where fuzzy words and vague expressions abound.\nIn Part I of this paper, we will show the details of how to transform the\ntechnical trading heuristics into nonlinear dynamic equations. First, we define\nfuzzy sets to represent the fuzzy terms in the technical trading rules; second,\nwe translate each technical trading heuristic into a group of fuzzy IF-THEN\nrules; third, we combine the fuzzy IF-THEN rules in a group into a fuzzy\nsystem; and finally, the linear combination of these fuzzy systems is used as\nthe excess demand function in the price dynamic equation. We transform a wide\nvariety of technical trading rules into fuzzy systems, including moving average\nrules, support and resistance rules, trend line rules, big buyer, big seller\nand manipulator rules, band and stop rules, and volume and relative strength\nrules. Simulation results show that the price dynamics driven by these\ntechnical trading rules are complex and chaotic, and some common phenomena in\nreal stock prices such as jumps, trending and self-fulfilling appear naturally.\n"
    },
    {
        "paper_id": 1401.1891,
        "authors": "Li-Xin Wang",
        "title": "Dynamical Models of Stock Prices Based on Technical Trading Rules Part\n  II: Analysis of the Models",
        "comments": null,
        "journal-ref": "IEEE Trans. on Fuzzy Systems, Vol. 23, No. 4, pp. 1127-1141, 2015",
        "doi": "10.1109/TFUZZ.2014.2346244",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Part II of this paper, we concentrate our analysis on the price dynamical\nmodel with the moving average rules developed in Part I of this paper. By\ndecomposing the excessive demand function, we reveal that it is the interplay\nbetween trend-following and contrarian actions that generates the price chaos,\nand give parameter ranges for the price series to change from divergence to\nchaos and to oscillation. We prove that the price dynamical model has an\ninfinite number of equilibrium points but all these equilibrium points are\nunstable. We demonstrate the short-term predictability of the return volatility\nand derive the detailed formula of the Lyapunov exponent as function of the\nmodel parameters. We show that although the price is chaotic, the volatility\nconverges to some constant very quickly at the rate of the Lyapunov exponent.\nWe extract the formula relating the converged volatility to the model\nparameters based on Monte-Carlo simulations. We explore the circumstances under\nwhich the returns show independency and illustrate in details how the\nindependency index changes with the model parameters. Finally, we plot the\nstrange attractor and return distribution of the chaotic price model to\nillustrate the complex structure and fat-tailed distribution of the returns.\n"
    },
    {
        "paper_id": 1401.1892,
        "authors": "Li-Xin Wang",
        "title": "Dynamical Models of Stock Prices Based on Technical Trading Rules Part\n  III: Application to Hong Kong Stocks",
        "comments": null,
        "journal-ref": "IEEE Trans. on Fuzzy Systems, Vol. 23, No. 5, pp. 1680-1697, 2015",
        "doi": "10.1109/TFUZZ.2014.2374193",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Part III of this study, we apply the price dynamical model with big buyers\nand big sellers developed in Part I of this paper to the daily closing prices\nof the top 20 banking and real estate stocks listed in the Hong Kong Stock\nExchange. The basic idea is to estimate the strength parameters of the big\nbuyers and the big sellers in the model and make buy/sell decisions based on\nthese parameter estimates. We propose two trading strategies: (i)\nFollow-the-Big-Buyer which buys when big buyer begins to appear and there is no\nsign of big sellers, holds the stock as long as the big buyer is still there,\nand sells the stock once the big buyer disappears; and (ii) Ride-the-Mood which\nbuys as soon as the big buyer strength begins to surpass the big seller\nstrength, and sells the stock once the opposite happens. Based on the testing\nover 245 two-year intervals uniformly distributed across the seven years from\n03-July-2007 to 02-July-2014 which includes a variety of scenarios, the net\nprofits would increase 67% or 120% on average if an investor switched from the\nbenchmark Buy-and-Hold strategy to the Follow-the-Big-Buyer or Ride-the-Mood\nstrategies during this period, respectively.\n"
    },
    {
        "paper_id": 1401.1916,
        "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu",
        "title": "Multiple-output support vector regression with a firefly algorithm for\n  interval-valued stock price index forecasting",
        "comments": "33 pages",
        "journal-ref": "Knowledge-based Systems. 55, 2013:87-100",
        "doi": "10.1016/j.knosys.2013.10.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Highly accurate interval forecasting of a stock price index is fundamental to\nsuccessfully making a profit when making investment decisions, by providing a\nrange of values rather than a point estimate. In this study, we investigate the\npossibility of forecasting an interval-valued stock price index series over\nshort and long horizons using multi-output support vector regression (MSVR).\nFurthermore, this study proposes a firefly algorithm (FA)-based approach, built\non the established MSVR, for determining the parameters of MSVR (abbreviated as\nFA-MSVR). Three globally traded broad market indices are used to compare the\nperformance of the proposed FA-MSVR method with selected counterparts. The\nquantitative and comprehensive assessments are performed on the basis of\nstatistical criteria, economic criteria, and computational cost. In terms of\nstatistical criteria, we compare the out-of-sample forecasting using\ngoodness-of-forecast measures and testing approaches. In terms of economic\ncriteria, we assess the relative forecast performance with a simple trading\nstrategy. The results obtained in this study indicate that the proposed FA-MSVR\nmethod is a promising alternative for forecasting interval-valued financial\ntime series.\n"
    },
    {
        "paper_id": 1401.1954,
        "authors": "Stefan Gerhold, Johannes F. Morgenbesser, Axel Zrunek",
        "title": "Refined wing asymptotics for the Merton and Kou jump diffusion models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Refining previously known estimates, we give large-strike asymptotics for the\nimplied volatility of Merton's and Kou's jump diffusion models. They are\ndeduced from call price approximations by transfer results of Gao and Lee. For\nthe Merton model, we also analyse the density of the underlying and show that\nit features an interesting \"almost power law\" tail.\n"
    },
    {
        "paper_id": 1401.2314,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "Optimal Hedging for Fund & Insurance Managers with Partially Observable\n  Investment Flows",
        "comments": "Revised version. Forthcoming in Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  All the financial practitioners are working in incomplete markets full of\nunhedgeable risk-factors. Making the situation worse, they are only equipped\nwith the imperfect information on the relevant processes. In addition to the\nmarket risk, fund and insurance managers have to be prepared for sudden and\npossibly contagious changes in the investment flows from their clients so that\nthey can avoid the over- as well as under-hedging. In this work, the prices of\nsecurities, the occurrences of insured events and (possibly a network of) the\ninvestment flows are used to infer their drifts and intensities by a stochastic\nfiltering technique. We utilize the inferred information to provide the optimal\nhedging strategy based on the mean-variance (or quadratic) risk criterion. A\nBSDE approach allows a systematic derivation of the optimal strategy, which is\nshown to be implementable by a set of simple ODEs and the standard Monte Carlo\nsimulation. The presented framework may also be useful for manufactures and\nenergy firms to install an efficient overlay of dynamic hedging by financial\nderivatives to minimize the costs.\n"
    },
    {
        "paper_id": 1401.2524,
        "authors": "Nassim Nicholas Taleb",
        "title": "Four Points Beginner Risk Managers Should Learn from Jeff Holman's\n  Mistakes in the Discussion of Antifragile",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical\nerrors students should learn to avoid: 1) Mistaking tails (4th moment) for\nvolatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the\nhedging wihout the underlying, 4) The necessity of a numeraire in finance.\n"
    },
    {
        "paper_id": 1401.2531,
        "authors": "Weiyin Fei",
        "title": "Optimal control of uncertain stochastic systems with Markovian switching\n  and its applications to portfolio decisions",
        "comments": "21 pages, 2 figures",
        "journal-ref": "Cybernetics and Systems: An International Journal, Volume 45,\n  Issue 1, 2014",
        "doi": "10.1080/01969722.2014.862445",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper first describes a class of uncertain stochastic control systems\nwith Markovian switching, and derives an It\\^o-Liu formula for Markov-modulated\nprocesses. And we characterize an optimal control law, which satisfies the\ngeneralized Hamilton-Jacobi-Bellman (HJB) equation with Markovian switching.\nThen, by using the generalized HJB equation, we deduce the optimal consumption\nand portfolio policies under uncertain stochastic financial markets with\nMarkovian switching. Finally, for constant relative risk-aversion (CRRA)\nfelicity functions, we explicitly obtain the optimal consumption and portfolio\npolicies. Moreover, we also make an economic analysis through numerical\nexamples.\n"
    },
    {
        "paper_id": 1401.2548,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Mutual Information Rate-Based Networks in Financial Markets",
        "comments": "12 pages, 12 figures, 2 tables, submitted to PRE",
        "journal-ref": "Phys. Rev. E 89, 2014, 052801",
        "doi": "10.1103/PhysRevE.89.052801",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the last years efforts in econophysics have been shifted to study how\nnetwork theory can facilitate understanding of complex financial markets. Main\npart of these efforts is the study of correlation-based hierarchical networks.\nThis is somewhat surprising as the underlying assumptions of research looking\nat financial markets is that they behave chaotically. In fact it's common for\neconophysicists to estimate maximal Lyapunov exponent for log returns of a\ngiven financial asset to confirm that prices behave chaotically. Chaotic\nbehaviour is only displayed by dynamical systems which are either non-linear or\ninfinite-dimensional. Therefore it seems that non-linearity is an important\npart of financial markets, which is proved by numerous studies confirming\nfinancial markets display significant non-linear behaviour, yet network theory\nis used to study them using almost exclusively correlations and partial\ncorrelations, which are inherently dealing with linear dependencies only. In\nthis paper we introduce a way to incorporate non-linear dynamics and\ndependencies into hierarchical networks to study financial markets using mutual\ninformation and its dynamical extension: the mutual information rate. We\nestimate it using multidimensional Lempel-Ziv complexity and then convert it\ninto an Euclidean metric in order to find appropriate topological structure of\nnetworks modelling financial markets. We show that this approach leads to\ndifferent results than correlation-based approach used in most studies, on the\nbasis of 15 biggest companies listed on Warsaw Stock Exchange in the period of\n2009-2012 and 91 companies listed on NYSE100 between 2003 and 2013, using\nminimal spanning trees and planar maximally filtered graphs.\n"
    },
    {
        "paper_id": 1401.286,
        "authors": "Frantisek Slanina",
        "title": "Complex temporal structure of activity in on-line electronic auctions",
        "comments": null,
        "journal-ref": "Advances in Complex Systems Vol. 15, (2012) 1250053",
        "doi": "10.1142/S0219525912500531",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze empirical data from the internet auction site Aukro.cz. The time\nseries of activity shows truncated fractal structure on scales from about 1\nminute to about 1 day. The distribution of waiting times as well as the\ndistribution of number of auctions within fixed interval is a power law, with\nexponents $1.5$ and $3$, respectively. Possible implications for the modeling\nof stock-market fluctuations are briefly discussed.\n"
    },
    {
        "paper_id": 1401.2867,
        "authors": "Guy Cirier (LSTA)",
        "title": "Bayesian analysis of redistribution policy with a fixed scale",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A government has to finance a risk for its population. It shares the charges\namong the population with a fixed scale based on economic criteria. Various\norganisms have to collect and to redistribute fairly the subsidies. Under these\nconditions, when the size of the organisms is varied, the distribution's laws\nof the criteria are exponential families and criteria are semi linear\nsufficient statistics.\n"
    },
    {
        "paper_id": 1401.29,
        "authors": "Elisa Appolloni and Andrea Ligori",
        "title": "Efficient tree methods for pricing digital barrier options",
        "comments": "21 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an efficient lattice procedure which permits to obtain European\nand American option prices under the Black and Scholes model for digital\noptions with barrier features. Numerical results show the accuracy of the\nproposed method.\n"
    },
    {
        "paper_id": 1401.2954,
        "authors": "E. M. S. Ribeiro and G. A. Prataviera",
        "title": "Information theoretic approach for accounting classification",
        "comments": "Final version. Published in Physica A",
        "journal-ref": "Physica A 416 (2014), 651-660",
        "doi": "10.1016/j.physa.2014.09.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider an information theoretic approach for the\naccounting classification process. We propose a matrix formalism and an\nalgorithm for calculations of information theoretic measures associated to\naccounting classification. The formalism may be useful for further\ngeneralizations and computer-based implementation. Information theoretic\nmeasures, mutual information and symmetric uncertainty, were evaluated for\ndaily transactions recorded in the chart of accounts of a small company during\ntwo years. Variation in the information measures due the aggregation of data in\nthe process of accounting classification is observed. In particular, the\nsymmetric uncertainty seems to be a useful parameter for comparing companies\nover time or in different sectors or different accounting choices and\nstandards.\n"
    },
    {
        "paper_id": 1401.2982,
        "authors": "James J. Angel",
        "title": "When Finance Meets Physics: The Impact of the Speed of Light on\n  Financial Markets and their Regulation",
        "comments": "Financial Review, 2014, forthcoming",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern physics has demonstrated that matter behaves very differently as it\napproaches the speed of light. This paper explores the implications of modern\nphysics to the operation and regulation of financial markets. Information\ncannot move faster than the speed of light. The geographic separation of market\ncenters means that relativistic considerations need to be taken into account in\nthe regulation of markets. Observers in different locations may simultaneously\nobserve different best prices. Regulators may not be able to determine which\ntransactions occurred first, leading to problems with best execution and\ntrade-through rules. Catastrophic software glitches can quantum tunnel through\nseemingly impregnable quality control procedures.\n"
    },
    {
        "paper_id": 1401.3103,
        "authors": "Peiteng Shi, Jiang Zhang, Bo Yang, Jingfei Luo",
        "title": "Hierarchicality of Trade Flow Networks Reveals Complexity of Products",
        "comments": "14 pages,7 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0098247",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With globalization, countries are more connected than before by trading\nflows, which currently amount to at least 36 trillion dollars. Interestingly,\napproximately 30-60 percent of global exports consist of intermediate products.\nTherefore, the trade flow network of a particular product with high added\nvalues can be regarded as a value chain. The problem is weather we can\ndiscriminate between these products based on their unique flow network\nstructure. This paper applies the flow analysis method developed in ecology to\n638 trading flow networks of different products. We claim that the allometric\nscaling exponent $\\eta$ can be used to characterize the degree of\nhierarchicality of a flow network, i.e., whether the trading products flow on\nlong hierarchical chains. Then, the flow networks of products with higher added\nvalues and complexity, such as machinery&transport equipment with larger\nexponents, are highlighted. These higher values indicate that their trade flow\nnetworks are more hierarchical. As a result, without extra data such as global\ninput-output table, we can identify the product categories with higher\ncomplexity and the relative importance of a country in the global value chain\nsolely by the trading network.\n"
    },
    {
        "paper_id": 1401.3121,
        "authors": "Pablo Koch-Medina, Cosimo Munari",
        "title": "Law-invariant risk measures: extension properties and qualitative\n  robustness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize when a convex risk measure associated to a law-invariant\nacceptance set in $L^\\infty$ can be extended to $L^p$, $1\\leq p<\\infty$,\npreserving finiteness and continuity. This problem is strongly connected to the\nstatistical robustness of the corresponding risk measures. Special attention is\npaid to concrete examples including risk measures based on expected utility,\nmax-correlation risk measures, and distortion risk measures.\n"
    },
    {
        "paper_id": 1401.3133,
        "authors": "Pablo Koch-Medina, Santiago Moreno-Bromberg, Cosimo Munari",
        "title": "Capital adequacy tests and limited liability of financial institutions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theory of acceptance sets and their associated risk measures plays a key\nrole in the design of capital adequacy tests. The objective of this paper is to\ninvestigate, in the context of bounded financial positions, the class of\nsurplus-invariant acceptance sets. These are characterized by the fact that\nacceptability does not depend on the positive part, or surplus, of a capital\nposition. We argue that surplus invariance is a reasonable requirement from a\nregulatory perspective, because it focuses on the interests of liability\nholders of a financial institution. We provide a dual characterization of\nsurplus-invariant, convex acceptance sets, and show that the combination of\nsurplus invariance and coherence leads to a narrow range of capital adequacy\ntests, essentially limited to scenario-based tests. Finally, we emphasize the\nadvantages of dealing with surplus-invariant acceptance sets as the primary\nobject rather than directly with risk measures, such as loss-based and\nexcess-invariant risk measures, which have been recently studied by Cont,\nDeguest, and He (2013) and by Staum (2013), respectively.\n"
    },
    {
        "paper_id": 1401.3145,
        "authors": "Stefano Nasini, Jordi Castro, Pau Fonseca i Casas",
        "title": "Bartering integer commodities with exogenous prices",
        "comments": "30 pages, 5 sections, 10 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The analysis of markets with indivisible goods and fixed exogenous prices has\nplayed an important role in economic models, especially in relation to wage\nrigidity and unemployment. This research report provides a mathematical and\ncomputational details associated to the mathematical programming based\napproaches proposed by Nasini et al. (accepted 2014) to study pure exchange\neconomies where discrete amounts of commodities are exchanged at fixed prices.\nBarter processes, consisting in sequences of elementary reallocations of couple\nof commodities among couples of agents, are formalized as local searches\nconverging to equilibrium allocations. A direct application of the analyzed\nprocesses in the context of computational economics is provided, along with a\nJava implementation of the approaches described in this research report.\n"
    },
    {
        "paper_id": 1401.3167,
        "authors": "Volker Kr\\\"atschmer, Alexander Schied, Henryk Z\\\"ahle",
        "title": "Quasi-Hadamard differentiability of general risk functionals and its\n  application",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply a suitable modification of the functional delta method to\nstatistical functionals that arise from law-invariant coherent risk measures.\nTo this end we establish differentiability of the statistical functional in a\nrelaxed Hadamard sense, namely with respect to a suitably chosen norm and in\nthe directions of a specifically chosen \"tangent space\". We show that this\nnotion of quasi-Hadamard differentiability yields both strong laws and limit\ntheorems for the asymptotic distribution of the plug-in estimators. Our results\ncan be regarded as a contribution to the statistics and numerics of risk\nmeasurement and as a case study for possible refinements of the functional\ndelta method through fine-tuning the underlying notion of differentiability\n"
    },
    {
        "paper_id": 1401.3261,
        "authors": "Dylan Possama\\\"i and Guillaume Royer",
        "title": "General indifference pricing with small transaction costs",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the utility indifference price of a European option in the context\nof small transaction costs. Considering the general setup allowing consumption\nand a general utility function at final time T, we obtain an asymptotic\nexpansion of the utility indifference price as a function of the asymptotic\nexpansions of the utility maximization problems with and without the European\ncontingent claim. We use the tools developed in [54] and [48] based on\nhomogenization and viscosity solutions to characterize these expansions.\nFinally we study more precisely the example of exponential utilities, in\nparticular recovering under weaker assumptions the results of [6].\n"
    },
    {
        "paper_id": 1401.3281,
        "authors": "Didier Sornette and Peter Cauwels",
        "title": "A Creepy World",
        "comments": "16 pages with 5 figures. Notenstein White Paper Series, November 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the mechanics of creep in material sciences as a metaphor, we present a\ngeneral framework to understand the evolution of financial, economic and social\nsystems and to construct scenarios for the future. In a nutshell, highly\nnon-linear out-of-equilibrium systems subjected to exogenous perturbations tend\nto exhibit a long phase of slow apparent stable evolution, which are nothing\nbut slow maturations towards instabilities, failures and changes of regimes.\nWith examples from history where a small event had a cataclysmic consequence,\nwe propose a novel view of the current state of the world via the logical\nscenarios that derive, avoiding the traps of an illusionary stability and\nsimple linear extrapolation. The endogenous scenarios are \"muddling along\",\n\"managing through\" and \"blood red abyss\". The exogenous scenarios are \"painful\nadjustment\" and \"golden east\".\n"
    },
    {
        "paper_id": 1401.3316,
        "authors": "Petr Jizba and Jan Korbel",
        "title": "Multifractal Diffusion Entropy Analysis: Optimal Bin Width of\n  Probability Histograms",
        "comments": "25 pages, 7 figures, LaTeX with the Elsevier article class,\n  substantially revised version",
        "journal-ref": "Physica A 413 (2014) 438-458",
        "doi": "10.1016/j.physa.2014.07.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the framework of Multifractal Diffusion Entropy Analysis we propose a\nmethod for choosing an optimal bin-width in histograms generated from\nunderlying probability distributions of interest. The method presented uses\ntechniques of R\\'{e}nyi's entropy and the mean squared error analysis to\ndiscuss the conditions under which the error in the multifractal spectrum\nestimation is minimal. We illustrate the utility of our approach by focusing on\na scaling behavior of financial time series. In particular, we analyze the\nS&P500 stock index as sampled at a daily rate in the time period 1950-2013. In\norder to demonstrate a strength of the method proposed we compare the\nmultifractal $\\delta$-spectrum for various bin-widths and show the robustness\nof the method, especially for large values of $q$. For such values, other\nmethods in use, e.g., those based on moment estimation, tend to fail for\nheavy-tailed data or data with long correlations. Connection between the\n$\\delta$-spectrum and R\\'{e}nyi's $q$ parameter is also discussed and\nelucidated on a simple example of multiscale time series.\n"
    },
    {
        "paper_id": 1401.3589,
        "authors": "Boualem Djehiche and Bj\\\"orn L\\\"ofdahl",
        "title": "Risk aggregation and stochastic claims reserving in disability insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a large, homogeneous portfolio of life or disability annuity\npolicies. The policies are assumed to be independent conditional on an external\nstochastic process representing the economic-demographic environment. Using a\nconditional law of large numbers, we establish the connection between claims\nreserving and risk aggregation for large portfolios. Further, we derive a\npartial differential equation for moments of present values. Moreover, we show\nhow statistical multi-factor intensity models can be approximated by one-factor\nmodels, which allows for solving the PDEs very efficiently. Finally, we give a\nnumerical example where moments of present values of disability annuities are\ncomputed using finite difference methods and Monte Carlo simulations.\n"
    },
    {
        "paper_id": 1401.3911,
        "authors": "Worapree Maneesoonthorn, Catherine S. Forbes and Gael M. Martin",
        "title": "Inference on Self-Exciting Jumps in Prices and Volatility using High\n  Frequency Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic jumps in the price and volatility of an asset are modelled using a\njoint Hawkes process in conjunction with a bivariate jump diffusion. A state\nspace representation is used to link observed returns, plus nonparametric\nmeasures of integrated volatility and price jumps, to the specified model\ncomponents; with Bayesian inference conducted using a Markov chain Monte Carlo\nalgorithm. An evaluation of marginal likelihoods for the proposed model\nrelative to a large number of alternative models, including some that have\nfeatured in the literature, is provided. An extensive empirical investigation\nis undertaken using data on the S&P500 market index over the 1996 to 2014\nperiod, with substantial support for dynamic jump intensities - including in\nterms of predictive accuracy - documented.\n"
    },
    {
        "paper_id": 1401.3921,
        "authors": "A. Galichon, P. Henry-Labord\\`ere, N. Touzi",
        "title": "A stochastic control approach to no-arbitrage bounds given marginals,\n  with an application to lookback options",
        "comments": "Published in at http://dx.doi.org/10.1214/13-AAP925 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2014, Vol. 24, No. 1, 312-336",
        "doi": "10.1214/13-AAP925",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of superhedging under volatility uncertainty for an\ninvestor allowed to dynamically trade the underlying asset, and statically\ntrade European call options for all possible strikes with some given maturity.\nThis problem is classically approached by means of the Skorohod Embedding\nProblem (SEP). Instead, we provide a dual formulation which converts the\nsuperhedging problem into a continuous martingale optimal transportation\nproblem. We then show that this formulation allows us to recover previously\nknown results about lookback options. In particular, our methodology induces a\nnew proof of the optimality of Az\\'{e}ma-Yor solution of the SEP for a certain\nclass of lookback options. Unlike the SEP technique, our approach applies to a\nlarge class of exotics and is suitable for numerical approximation techniques.\n"
    },
    {
        "paper_id": 1401.3994,
        "authors": "Damiano Brigo and Andrea Pallavicini",
        "title": "CCP Cleared or Bilateral CSA Trades with Initial/Variation Margins under\n  credit, funding and wrong-way risks: A Unified Valuation Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The introduction of CCPs in most derivative transactions will dramatically\nchange the landscape of derivatives pricing, hedging and risk management, and,\naccording to the TABB group, will lead to an overall liquidity impact about 2\nUSD trillions. In this article we develop for the first time a comprehensive\napproach for pricing under CCP clearing, including variation and initial\nmargins, gap credit risk and collateralization, showing concrete examples for\ninterest rate swaps. Mathematically, the inclusion of asymmetric borrowing and\nlending rates in the hedge of a claim lead to nonlinearities showing up in\nclaim dependent pricing measures, aggregation dependent prices, nonlinear PDEs\nand BSDEs. This still holds in presence of CCPs and CSA. We introduce a\nmodeling approach that allows us to enforce rigorous separation of the\ninterconnected nonlinear risks into different valuation adjustments where the\nkey pricing nonlinearities are confined to a funding costs component that is\nanalyzed through numerical schemes for BSDEs. We present a numerical case study\nfor Interest Rate Swaps that highlights the relative size of the different\nvaluation adjustments and the quantitative role of initial and variation\nmargins, of liquidity bases, of credit risk, of the margin period of risk and\nof wrong way risk correlations.\n"
    },
    {
        "paper_id": 1401.4331,
        "authors": "Miroslav Pi\\v{s}t\\v{e}k, Frantisek Slanina",
        "title": "Diversity of scales makes an advantage: The case of the Minority Game",
        "comments": null,
        "journal-ref": "Physica A 390 (2011) 2549-2561",
        "doi": "10.1016/j.physa.2011.03.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the Minority Game as a testing frame for the problem of the emergence\nof diversity in socio-economic systems. For the MG with heterogeneous impacts,\nwe show that the direct generalization of the usual agents' profit does not fit\nsome real-world situations. As a typical example we use the traffic formulation\nof the MG. Taking into account vehicles of various lengths it can easily happen\nthat one of the roads is crowded by a few long trucks and the other contains\nmore drivers but still is less covered by vehicles. Most drivers are in the\nshorter queue, so the majority win. To describe such situations, we generalized\nthe formula for agents' profit by explicitly introducing utility function\ndepending on an agent's impact. Then, the overall profit of the system may\nbecome positive depending on the actual choice of the utility function. We\ninvestigated several choices of the utility function and showed that this\nvariant of the MG may turn into a positive sum game.\n"
    },
    {
        "paper_id": 1401.4387,
        "authors": "Fausto Bonacina, Marco D'Errico, Enrico Moretto, Silvana Stefani, Anna\n  Torriero",
        "title": "A Multiple Network Approach to Corporate Governance",
        "comments": "version 2",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we consider Corporate Governance (CG) ties among companies from\na multiple network perspective. Such a structure naturally arises from the\nclose interrelation between the Shareholding Network (SH) and the Board of\nDirectors network (BD). In order to capture the simultaneous effects of both\nnetworks on CG, we propose to model the CG multiple network structure via\ntensor analysis. In particular, we consider the TOPHITS model, based on the\nPARAFAC tensor decomposition, to show that tensor techniques can be\nsuccessfully applied in this context. By providing some empirical results from\nthe Italian financial market in the univariate case, we then show that a\ntensor--based multiple network approach can reveal important information.\n"
    },
    {
        "paper_id": 1401.455,
        "authors": "Lorenzo Pareschi and Giuseppe Toscani",
        "title": "Wealth distribution and collective knowledge. A Boltzmann approach",
        "comments": "21 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:q-bio/0312018 by other authors",
        "journal-ref": null,
        "doi": "10.1098/rsta.2013.0396",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and discuss a nonlinear kinetic equation of Boltzmann type which\ndescribes the influence of knowledge in the evolution of wealth in a system of\nagents which interact through the binary trades introduced in Cordier,\nPareschi, Toscani, J. Stat. Phys. 2005. The trades, which include both saving\npropensity and the risks of the market, are here modified in the risk and\nsaving parameters, which now are assumed to depend on the personal degree of\nknowledge. The numerical simulations show that the presence of knowledge has\nthe potential to produce a class of wealthy agents and to account for a larger\nproportion of wealth inequality.\n"
    },
    {
        "paper_id": 1401.4664,
        "authors": "W.P. Weijland",
        "title": "Mathematical Foundations for the Economy of Giving",
        "comments": "15 pages; 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows how we can build a model for transactions when goods are\ngiven away in the expectation of a later settlement. In settings where people\nkeep track of their social accounts we are able to redefine concepts like\naccount balance, yield curve and the law of diminishing returns. The model\nprovides us with a result that expresses how people have a structural\npreference for one recipient over the other regardless the actual account\nbalance. Hence a building block in the social fabric of a community. Finally, a\nfundamental theorem is presented to show how suppliers and recipients use their\naccount balance in order to reach an equilibrium in the exchange of goods much\nlike the traditional balance between supply and demand.\n"
    },
    {
        "paper_id": 1401.4698,
        "authors": "Mathias Beiglb\\\"ock, Marcel Nutz",
        "title": "Martingale Inequalities and Deterministic Counterparts",
        "comments": "22 pages",
        "journal-ref": "Electronic Journal of Probability, Vol. 19, No. 95, pp. 1-15, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study martingale inequalities from an analytic point of view and show that\na general martingale inequality can be reduced to a pair of deterministic\ninequalities in a small number of variables. More precisely, the optimal bound\nin the martingale inequality is determined by a fixed point of a simple\nnonlinear operator involving a concave envelope. Our results yield an\nexplanation for certain inequalities that arise in mathematical finance in the\ncontext of robust hedging.\n"
    },
    {
        "paper_id": 1401.4704,
        "authors": "Martha G. Alatriste Contreras, Giorgio Fagiolo",
        "title": "Propagation of Economic Shocks in Input-Output Networks: A Cross-Country\n  Analysis",
        "comments": "9 pages, 12 figures, supplemental material section",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates how economic shocks propagate and amplify through the\ninput-output network connecting industrial sectors in developed economies. We\nstudy alternative models of diffusion on networks and we calibrate them using\ninput-output data on real-world inter-sectoral dependencies for several\nEuropean countries before the Great Depression. We show that the impact of\neconomic shocks strongly depends on the nature of the shock and country size.\nShocks that impact on final demand without changing production and the\ntechnological relationships between sectors have on average a large but very\nhomogeneous impact on the economy. Conversely, when shocks change also the\nmagnitudes of input-output across-sector interdependencies (and possibly sector\nproduction), the economy is subject to predominantly large but more\nheterogeneous avalanche sizes. In this case, we also find that: (i) the more a\nsector is globally central in the country network, the largest its impact; (ii)\nthe largest European countries, such as those constituting the core of the\nEuropean Union's economy, typically experience the largest avalanches,\nsignaling their intrinsic higher vulnerability to economic shocks.\n"
    },
    {
        "paper_id": 1401.4787,
        "authors": "Steven Kou and Xianhua Peng",
        "title": "On the Measurement of Economic Tail Risk",
        "comments": "51 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper attempts to provide a decision-theoretic foundation for the\nmeasurement of economic tail risk, which is not only closely related to utility\ntheory but also relevant to statistical model uncertainty. The main result is\nthat the only risk measures that satisfy a set of economic axioms for the\nChoquet expected utility and the statistical property of elicitability (i.e.\nthere exists an objective function such that minimizing the expected objective\nfunction yields the risk measure) are the mean functional and the median\nshortfall, which is the median of tail loss distribution. Elicitability is\nimportant for backtesting. We also extend the result to address model\nuncertainty by incorporating multiple scenarios. As an application, we argue\nthat median shortfall is a better alternative than expected shortfall for\nsetting capital requirements in Basel Accords.\n"
    },
    {
        "paper_id": 1401.4887,
        "authors": "Gani Aldashev and Serik Aldashev and Timoteo Carletti",
        "title": "On Convergence in the Spatial AK Growth Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent research in economic theory attempts to study optimal economic growth\nand spatial location of economic activity in a unified framework. So far, the\nkey result of this literature - asymptotic convergence, even in the absence of\ndecreasing returns to capital - relies on specific assumptions about the\nobjective of the social planner. We show that this result does not depend on\nsuch restrictive assumptions and obtains for a broader class of objective\nfunctions. We also generalize this finding, allowing for the time-varying\ntechnology parameter, and provide an explicit solution for the dynamics of\nspatial distribution of the capital stock.\n"
    },
    {
        "paper_id": 1401.5314,
        "authors": "Eduardo Viegas, Stuart P. Cockburn, Henrik Jeldtoft Jensen, Geoffrey\n  B. West",
        "title": "Why free markets die: An evolutionary perspective",
        "comments": "13 pages and 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Company mergers and acquisitions are often perceived to act as catalysts for\ncorporate growth in free markets systems: it is conventional wisdom that those\nactivities lead to better and more efficient markets. However, the broad\nadoption of this perception into corporate strategy is prone to result in a\nless diverse and more unstable environment, dominated by either very large or\nvery small niche entities. We show here that ancestry, i.e. the cumulative\nhistory of mergers, is the key characteristic that encapsulates the diverse\nrange of drivers behind mergers and acquisitions, across a range of industries\nand geographies. A long-term growth analysis reveals that entities which have\nbeen party to fewer mergers tend to grow faster than more highly acquisitive\nbusinesses.\n"
    },
    {
        "paper_id": 1401.5431,
        "authors": "Laura Morino and Wolfgang J. Ruggaldier",
        "title": "On multicurve models for the term structure",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of multi-curve modeling we consider a two-curve setup, with\none curve for discounting (OIS swap curve) and one for generating future cash\nflows (LIBOR for a give tenor). Within this context we present an approach for\nthe clean-valuation pricing of FRAs and CAPs (linear and nonlinear derivatives)\nwith one of the main goals being also that of exhibiting an \"adjustment factor\"\nwhen passing from the one-curve to the two-curve setting. The model itself\ncorresponds to short rate modeling where the short rate and a short rate spread\nare driven by affine factors; this allows for correlation between short rate\nand short rate spread as well as to exploit the convenient affine structure\nmethodology. We briefly comment also on the calibration of the model\nparameters, including the correlation factor.\n"
    },
    {
        "paper_id": 1401.5452,
        "authors": "G. Papaioannou, P. Papaioannou, N. Parliaris",
        "title": "Modeling the stylized facts of wholesale system marginal price (SMP) and\n  the impacts of regulatory reforms on the Greek Electricity Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents the results of an empirical research with the target of\nmodeling the stylized facts of the daily expost System Marginal Price (SMP) of\nthe Greek wholesale electricity market, using data from January 2004 to\nDecember of 2011. SMP is considered here as the footprint of an underline\nstochastic and nonlinear process that bears all the information reflecting not\nonly the effects of changes in endogenous or fundamental factors of the market\nbut also the impacts of a series of regulatory reforms that have continuously\nchanged the market's microstructure. To capture the dynamics of the conditional\nmean and volatility of SMP that generate the stylized facts(mean reversion,\nprice spikes, fat tails price distribution etc), a number of ARMAX GARCH models\nhave been estimated using as regressors an extensive set of fundamental factors\nin the Greek electricity market as well as dummy variables that mimic the\nhistory of Regulator's interventions. The findings show that changes in the\nmicrostructure of the market caused by the reforms have strongly affected the\ndynamic evolution of SMP and that the best found model captures adequately the\nstylized facts of the series that other electricity and financial markets\nshare. The dynamics of the conditional volatility generated by the model can be\nextremely useful in the efforts that are under way towards market restructuring\nso the Greek market to be more compatible with the requirements of the European\nTarget Model.\n"
    },
    {
        "paper_id": 1401.5666,
        "authors": "M. Duembgen, L. C. G. Rogers",
        "title": "Estimate nothing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the econometrics of financial time series, it is customary to take some\nparametric model for the data, and then estimate the parameters from historical\ndata. This approach suffers from several problems. Firstly, how is estimation\nerror to be quantified, and then taken into account when making statements\nabout the future behaviour of the observed time series? Secondly, decisions may\nbe taken today committing to future actions over some quite long horizon, as in\nthe trading of derivatives; if the model is re-estimated at some intermediate\ntime, our earlier decisions would need to be revised - but the derivative has\nalready been traded at the earlier price. Thirdly, the exact form of the\nparametric model to be used is generally taken as given at the outset; other\ncompetitor models might possibly work better in some circumstances, but the\nmethodology does not allow them to be factored into the inference. What we\npropose here is a very simple (Bayesian) alternative approach to inference and\naction in financial econometrics which deals decisively with all these issues.\nThe key feature is that nothing is being estimated.\n"
    },
    {
        "paper_id": 1401.6383,
        "authors": "Jarno Talponen and Lauri Viitasaari",
        "title": "Multidimensional Breeden-Litzenberger representation for state price\n  densities and static hedging",
        "comments": "The earlier version of arXiv:1305.5963 contains material now\n  appearing here",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we consider European options of type $h(X^1_T, X^2_T,\\ldots,\nX^n_T)$ depending on several underlying assets. We study how such options can\nbe valued in terms of simple vanilla options in non-specified market models. We\nconsider different approaches related to static hedging and derive several\npricing formulas for a wide class of payoff functions $h:\\R_+^n\\rightarrow \\R$.\nWe also give new relations between prices of different options both in one\ndimensional and multidimensional case.\n"
    },
    {
        "paper_id": 1401.6408,
        "authors": "M. Bernardi, L. Petrella",
        "title": "Interconnected risk contributions: an heavy-tail approach to analyse US\n  financial sectors",
        "comments": "arXiv admin note: text overlap with arXiv:1312.6407",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a multivariate model-based approach to measure the\ndynamic evolution of tail risk interdependence among US banks, financial\nservices and insurance sectors. To deeply investigate the risk contribution of\ninsurers we consider separately life and non-life companies. To achieve this\ngoal we apply the multivariate student-t Markov Switching model and the\nMultiple-CoVaR (CoES) risk measures introduced in Bernardi et. al. (2013b) to\naccount for both the known stylised characteristics of the data and the\ncontemporaneous joint distress events affecting financial sectors. Our\nempirical investigation finds that banks appear to be the major source of risk\nfor all the remaining sectors, followed by the financial services and the\ninsurance sectors, showing that insurance sector significantly contributes as\nwell to the overall risk. Moreover, we find that the role of each sector in\ncontributing to other sectors distress evolves over time accordingly to the\ncurrent predominant financial condition, implying different interconnection\nstrength.\n"
    },
    {
        "paper_id": 1401.6735,
        "authors": "Marcelo J. Villena and Axel A. Araneda",
        "title": "Option Pricing of Twin Assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How to price and hedge claims on nontraded assets are becoming increasingly\nimportant matters in option pricing theory today. The most common practice to\ndeal with these issues is to use another similar or \"closely related\" asset or\nindex which is traded, for hedging purposes. Implicitly, traders assume here\nthat the higher the correlation between the traded and nontraded assets, the\nbetter the hedge is expected to perform. This raises the question as to how\n\\textquoteleft{}closely related\\textquoteright{} the assets really are. In this\npaper, the concept of twin assets is introduced, focusing the discussion\nprecisely in what does it mean for two assets to be similar. Our findings point\nto the fact that, in order to have very similar assets, for example identical\ntwins, high correlation measures are not enough. Specifically, two basic\ncriteria of similarity are pointed out: i) the coefficient of variation of the\nassets and ii) the correlation between assets. From here, a method to measure\nthe level of similarity between assets is proposed, and secondly, an option\npricing model of twin assets is developed. The proposed model allows us to\nprice an option of one nontraded asset using its twin asset, but this time\nknowing explicitly what levels of errors we are facing. Finally, some numerical\nillustrations show how twin assets behave depending upon their levels of\nsimilarities, and how their potential differences will traduce in MAPE (mean\nabsolute percentage error) for the proposed option pricing model.\n"
    },
    {
        "paper_id": 1401.6955,
        "authors": "Radoslava Mirkov and Thomas Maul and Ronald Hochreiter and Holger\n  Thomae",
        "title": "Modeling Credit Spreads Using Nonlinear Regression",
        "comments": "Poster presentation at IWSM 2013",
        "journal-ref": "Proceedings of IWSM 2013, Volume 2: 697-700. 2013",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The term structure of credit spreads is studied with an aim to predict its\nfuture movements. A completely new approach to tackle this problem is\npresented, which utilizes nonlinear parametric models. The Brain-Cousens\nregression model with five parameters is chosen to describe the term structure\nof credit spreads. Further, we investigate the dependence of the parameter\nchanges over time and the determinants of credit spreads.\n"
    },
    {
        "paper_id": 1401.717,
        "authors": "John Goddard, Enrico Onali",
        "title": "Self-affinity in financial asset returns",
        "comments": null,
        "journal-ref": "International Review of Financial Analysis 24, 2012, 1 11",
        "doi": "10.1016/j.irfa.2012.06.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test for departures from normal and independent and identically\ndistributed (NIID) returns, when returns under the alternative hypothesis are\nself-affine. Self-affine returns are either fractionally integrated and\nlong-range dependent, or drawn randomly from an L-stable distribution with\ninfinite higher-order moments. The finite sample performance of estimators of\nthe two forms of self-affinity is explored in a simulation study which\ndemonstrates that, unlike rescaled range analysis and other conventional\nestimation methods, the variant of fluctuation analysis that considers finite\nsample moments only is able to identify either form of self-affinity. However,\nwhen returns are self-affine and long-range dependent under the alternative\nhypothesis, rescaled range analysis has greater power than fluctuation\nanalysis. The finite-sample properties of the estimators when returns exhibit\neither form of self-affinity can be exploited to determine the source of\nself-affinity in empirical returns data. The techniques are illustrated by\nmeans of an analysis of the fractal properties of the daily logarithmic returns\nfor the indices of 11 stock markets.\n"
    },
    {
        "paper_id": 1401.7198,
        "authors": "Beatrice Acciaio, Claudio Fontana, Constantinos Kardaras",
        "title": "Arbitrage of the first kind and filtration enlargements in\n  semimartingale financial models",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a general semimartingale financial model, we study the stability of the No\nArbitrage of the First Kind (NA1) (or, equivalently, No Unbounded Profit with\nBounded Risk) condition under initial and under progressive filtration\nenlargements. In both cases, we provide a simple and general condition which is\nsufficient to ensure this stability for any fixed semimartingale model.\nFurthermore, we give a characterisation of the NA1 stability for all\nsemimartingale models.\n"
    },
    {
        "paper_id": 1401.7344,
        "authors": "Brian P. Hanley",
        "title": "Release of the Kraken: A Novel Money Multiplier Equation's Debut in 21st\n  Century Banking",
        "comments": "22 pages, 3 figures, 5 significant equations (of 7). Published in\n  Economics E-Journal",
        "journal-ref": "Economics: The Open-Access, Open-Assessment E-Journal, Vol. 6,\n  2012-3",
        "doi": "10.5018/economics-ejournal.ja.2012-3",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Historically, the banking multiplier has been in a range of 4 to 100, with\n25% to 1% reserve ratios at most layers of the banking system encompassing the\nmajority of its range in recent centuries. Here it is shown that multipliers\nover 1 000 can occur from a new mechanism in banking. This new multiplier uses\na default insurance note to insure an outstanding loan in order to return the\nvalue of the insured amount into capital. The economic impact of this invention\nis calculably greater than the original invention of reserve banking. The\nconsequence of this lending invention is to render the existing money\nmultiplier equations of reserve banking obsolete where it occurs. The equations\ndescribing this new multiplier do not converge. Each set of parameters for\nreserve percentage, nesting depth, etc. creates a unique logarithmic curve\nrather than approaching a limit. Thus it is necessary to show the behavior of\nthis new equation by numerical methods. Understanding this new multiplier and\nassociated issues is necessary for economic analyses of the Global Financial\nCrisis.\n"
    },
    {
        "paper_id": 1401.745,
        "authors": "B. Podobnik, A. Majdandzic, C. Curme, Z. Qiao, W.-X. Zhou, H. E.\n  Stanley, and B. Li",
        "title": "Network Risk and Forecasting Power in Phase-Flipping Dynamical Networks",
        "comments": "5 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.89.042807",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to model volatile real-world network behavior, we analyze\nphase-flipping dynamical scale-free network in which nodes and links fail and\nrecover. We investigate how stochasticity in a parameter governing the recovery\nprocess affects phase-flipping dynamics, and find the probability that no more\nthan q% of nodes and links fail. We derive higher moments of the fractions of\nactive nodes and active links, $f_n(t)$ and $f_{\\ell}(t)$, and define two\nestimators to quantify the level of risk in a network. We find hysteresis in\nthe correlations of $f_n(t)$ due to failures at the node level, and derive\nconditional probabilities for phase-flipping in networks. We apply our model to\neconomic and traffic networks.\n"
    },
    {
        "paper_id": 1401.7496,
        "authors": "Sorin Solomon and Natasa Golo",
        "title": "Microeconomic Structure determines Macroeconomic Dynamics. Aoki defeats\n  the Representative Agent",
        "comments": "42 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Masanao Aoki developed a new methodology for a basic problem of economics:\ndeducing rigorously the macroeconomic dynamics as emerging from the\ninteractions of many individual agents. This includes deduction of the fractal\n/ intermittent fluctuations of macroeconomic quantities from the granularity of\nthe mezo-economic collective objects (large individual wealth, highly\nproductive geographical locations, emergent technologies, emergent economic\nsectors) in which the micro-economic agents self-organize.\n  In particular, we present some theoretical predictions, which also met\nextensive validation from empirical data in a wide range of systems: - The\nfractal Levy exponent of the stock market index fluctuations equals the Pareto\nexponent of the investors wealth distribution. The origin of the macroeconomic\ndynamics is therefore found in the granularity induced by the wealth / capital\nof the wealthiest investors. - Economic cycles consist of a Schumpeter\n'creative destruction' pattern whereby the maxima are cusp-shaped while the\nminima are smooth. In between the cusps, the cycle consists of the sum of 2\n'crossing exponentials': one decaying and the other increasing.\n  This unification within the same theoretical framework of short term market\nfluctuations and long term economic cycles offers the perspective of a genuine\nconceptual synthesis between micro- and macroeconomics. Joining another giant\nof contemporary science - Phil Anderson - Aoki emphasized the role of rare,\nlarge fluctuations in the emergence of macroeconomic phenomena out of\nmicroscopic interactions and in particular their non self-averaging, in the\nlanguage of statistical physics. In this light, we present a simple stochastic\nmulti-sector growth model.\n"
    },
    {
        "paper_id": 1401.7615,
        "authors": "Marcelo M. de Oliveira and Alexandre C. L. Almeida",
        "title": "Testing for rational speculative bubbles in the Brazilian residential\n  real-estate market",
        "comments": null,
        "journal-ref": "Contemporary Studies in Economic and Financial Analysis 96 (2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Speculative bubbles have been occurring periodically in local or global real\nestate markets and are considered a potential cause of economic crises. In this\ncontext, the detection of explosive behaviors in the financial market and the\nimplementation of early warning diagnosis tests are of critical importance. The\nrecent increase in Brazilian housing prices has risen concerns that the\nBrazilian economy may have a speculative housing bubble. In the present paper,\nwe employ a recently proposed recursive unit root test in order to identify\npossible speculative bubbles in data from the Brazilian residential real-estate\nmarket. The empirical results show evidence for speculative price bubbles both\nin Rio de Janeiro and Sao Paulo, the two main Brazilian cities.\n"
    },
    {
        "paper_id": 1401.7913,
        "authors": "Lorenz Schneider and Bertrand Tavin",
        "title": "From the Samuelson Volatility Effect to a Samuelson Correlation Effect:\n  Evidence from Crude Oil Calendar Spread Options",
        "comments": "32 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a multi-factor stochastic volatility model based on the\nCIR/Heston stochastic volatility process. In order to capture the Samuelson\neffect displayed by commodity futures contracts, we add expiry-dependent\nexponential damping factors to their volatility coefficients. The pricing of\nsingle underlying European options on futures contracts is straightforward and\ncan incorporate the volatility smile or skew observed in the market. We\ncalculate the joint characteristic function of two futures contracts in the\nmodel in analytic form and use the one-dimensional Fourier inversion method of\nCaldana and Fusai (JBF 2013) to price calendar spread options. The model leads\nto stochastic correlation between the returns of two futures contracts. We\nillustrate the distribution of this correlation in an example. We then propose\nanalytical expressions to obtain the copula and copula density directly from\nthe joint characteristic function of a pair of futures. These expressions are\nconvenient to analyze the term-structure of dependence between the two futures\nproduced by the model. In an empirical application we calibrate the proposed\nmodel to volatility surfaces of vanilla options on WTI. In this application we\nprovide evidence that the model is able to produce the desired stylized facts\nin terms of volatility and dependence. In a separate appendix, we give guidance\nfor the implementation of the proposed model and the Fourier inversion results\nby means of one and two-dimensional FFT methods.\n"
    },
    {
        "paper_id": 1401.8026,
        "authors": "Sebastian Poledna and Stefan Thurner",
        "title": "Elimination of systemic risk in financial networks by means of a\n  systemic risk transaction tax",
        "comments": "18 pages, 7 figures",
        "journal-ref": "Quantitative Finance 16 1469-7696 2016",
        "doi": "10.1080/14697688.2016.1156146",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are exposed to systemic risk (SR), the risk that a major\nfraction of the system ceases to function, and collapses. It has recently\nbecome possible to quantify SR in terms of underlying financial networks where\nnodes represent financial institutions, and links capture the size and maturity\nof assets (loans), liabilities, and other obligations, such as derivatives. We\ndemonstrate that it is possible to quantify the share of SR that individual\nliabilities within a financial network contribute to the overall SR. We use\nempirical data of nationwide interbank liabilities to show that the marginal\ncontribution to overall SR of liabilities for a given size varies by a factor\nof a thousand. We propose a tax on individual transactions that is proportional\nto their marginal contribution to overall SR. If a transaction does not\nincrease SR it is tax-free. With an agent-based model (CRISIS macro-financial\nmodel) we demonstrate that the proposed \"Systemic Risk Tax\" (SRT) leads to a\nself-organised restructuring of financial networks that are practically free of\nSR. The SRT can be seen as an insurance for the public against costs arising\nfrom cascading failure. ABM predictions are shown to be in remarkable agreement\nwith the empirical data and can be used to understand the relation of credit\nrisk and SR.\n"
    },
    {
        "paper_id": 1401.8065,
        "authors": "Yoshihiro Yura and Hideki Takayasu and Didier Sornette and Misako\n  Takayasu",
        "title": "Financial Brownian particle in the layered order book fluid and\n  Fluctuation-Dissipation relations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1103/PhysRevLett.112.098703",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel description of the dynamics of the order book of\nfinancial markets as that of an effective colloidal Brownian particle embedded\nin fluid particles. The analysis of a comprehensive market data enables us to\nidentify all motions of the fluid particles. Correlations between the motions\nof the Brownian particle and its surrounding fluid particles reflect specific\nlayering interactions; in the inner-layer, the correlation is strong and with\nshort memory while, in the outer-layer, it is weaker and with long memory. By\ninterpreting and estimating the contribution from the outer-layer as a drag\nresistance, we demonstrate the validity of the fluctuation-dissipation relation\n(FDR) in this non-material Brownian motion process.\n"
    },
    {
        "paper_id": 1401.8106,
        "authors": "Stanislav S. Borysov, Alexander V. Balatsky",
        "title": "Cross-correlation asymmetries and causal relationships between stock and\n  market risk",
        "comments": "17 pages, 8 figures, 1 table",
        "journal-ref": "PLoS ONE 9(8): e105874 (2014)",
        "doi": "10.1371/journal.pone.0105874",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study historical correlations and lead-lag relationships between\nindividual stock risk (volatility of daily stock returns) and market risk\n(volatility of daily returns of a market-representative portfolio) in the US\nstock market. We consider the cross-correlation functions averaged over all\nstocks, using 71 stock prices from the Standard \\& Poor's 500 index for\n1994--2013. We focus on the behavior of the cross-correlations at the times of\nfinancial crises with significant jumps of market volatility. The observed\nhistorical dynamics showed that the dependence between the risks was almost\nlinear during the US stock market downturn of 2002 and after the US housing\nbubble in 2007, remaining on that level until 2013. Moreover, the averaged\ncross-correlation function often had an asymmetric shape with respect to zero\nlag in the periods of high correlation. We develop the analysis by the\napplication of the linear response formalism to study underlying causal\nrelations. The calculated response functions suggest the presence of\ncharacteristic regimes near financial crashes, when the volatility of an\nindividual stock follows the market volatility and vice versa.\n"
    },
    {
        "paper_id": 1401.8142,
        "authors": "Miriam Kie{\\ss}ling, Sascha Kurz, and J\\\"org Rambau",
        "title": "The Integrated Size and Price Optimization Problem",
        "comments": "26 pages, 9 tables",
        "journal-ref": "M. Kie{\\ss}ling, S. Kurz, and J. Rambau: The integrated size and\n  price optimization problem, Numerical Algebra, Control and Optimization, Vol.\n  2, Nr. 4, Pages 669-693, 2012",
        "doi": "10.3934/naco.2012.2.669",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the Integrated Size and Price Optimization Problem (ISPO) for a\nfashion discounter with many branches. Based on a two-stage stochastic\nprogramming model with recourse, we develop an exact algorithm and a\nproduction-compliant heuristic that produces small optimality gaps. In a field\nstudy we show that a distribution of supply over branches and sizes based on\nISPO solutions is significantly better than a one-stage optimization of the\ndistribution ignoring the possibility of optimal pricing.\n"
    },
    {
        "paper_id": 1401.8271,
        "authors": "Adrien Nguyen Huu (FiME Lab, IMPA), Nadia Oudjane (FiME Lab)",
        "title": "Hedging Expected Losses on Derivatives in Electricity Futures Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the problem of pricing and hedging derivatives of Electricity\nFutures contract when the underlying asset is not available. We propose to use\na cross hedging strategy based on the Futures contract covering the larger\ndelivery period. A quick overview of market data shows a basis risk for this\nmarket incompleteness. For that purpose we formulate the pricing problem in a\nstochastic target form along the lines of Bouchard and al. (2008), with a\nmoment loss function. Following the same techniques as in the latter, we avoid\nto demonstrate the uniqueness of the value function by comparison arguments and\nexplore convex duality methods to provide a semi-explicit solution to the\nproblem. We then propose numerical results to support the new hedging strategy\nand compare our method to the Black-Scholes naive approach.\n"
    },
    {
        "paper_id": 1402.0139,
        "authors": "Arslan Tariq Rana and Mazen Kebewar",
        "title": "The Political Economy of FDI flows into Developing Countries: Does the\n  depth of International Trade Agreements Matter?",
        "comments": "University of Orleans (France)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is considerable debate whether the domestic political institutions\n(specifically, the country s level of democracy) of the host developing country\ntoward foreign investors are effective in establishing the credibility of\ncommitments are still underway, researchers have also analyzed the effect of\ninternational institutions such as (GATT-WTO) membership and Bilateral\nInvestment treaties (BIT) in their role of establishing the credibility of\ncommitment to attract foreign investments. We argue that there are qualitative\ndifferences among various types of trade agreements and full-fledged trade\nagreements (FTA-CU) provide credibility to foreign investors and democracy\nlevel in the host country conditions this effect whereas the partial scope\nagreements (PSA) are not sufficient in providing credibility of commitments and\nnot moderated by democracy. This paper analyses the impact of heterogeneous\nTAs, and their interaction with domestic institutions, on FDI inflows.\nStatistical analyses for 122 developing countries from 1970 to 2005 support\nthis argument. The method adopted relies on fixed effects estimator which is\nrobust to control endogeneity on a large panel dataset. The strict erogeneity\nof results by using a method suggested by Baier and Bergstrand (2007) and no\nfeedback effect found in sample. The results state that (1) More the FTA-CU\nconcluded, larger the amount of FDI inflows are attracted into the developing\ncountries and PSA are insignificant in determining the FDI inflow; (2) FTA CU\nare complementary to democratic regime whereas the conditional effect of PSA\nwith democracy on levels of FDI inflows is insignificant.\n"
    },
    {
        "paper_id": 1402.0176,
        "authors": "Sorin Solomon, Natasa Golo (Racah Institute of Physics, Hebrew\n  University)",
        "title": "Minsky Financial Instability, Interscale Feedback, Percolation and\n  Marshall-Walras Disequilibrium",
        "comments": null,
        "journal-ref": "Accounting, Economics and Law. Volume 3, Issue 3, Pages 167-260,\n  2013",
        "doi": "10.1515/ael-2013-0029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study analytically and numerically Minsky instability as a combination of\ntop-down, bottom-up and peer-to-peer positive feedback loops. The peer-to-peer\ninteractions are represented by the links of a network formed by the\nconnections between firms, contagion leading to avalanches and percolation\nphase transitions propagating across these links. The global parameter in the\ntop-bottom, bottom-up feedback loop is the interest rate. Before the Minsky\nmoment, in the Minsky Loans Accelerator stage, the relevant bottom parameter\nrepresenting the individual firms micro-states is the quantity of loans. After\nthe Minsky moment, in the Minsky Crisis Accelerator stage, the relevant bottom\nparameters are the number of ponzi units / quantity of failures, defaults. We\nrepresent the top-bottom, bottom-up interactions on a plot similar to the\nMarshal-Walras diagram for quantity-price market equilibrium (where the\ninterest rate is the analog of the price). The Minsky instability is then\nsimply emerging as a consequence of the fixed point (the intersection of the\nsupply and demand curves) being unstable (repulsive). In the presence of\nnetwork effects, one obtains more than one fixed point and a few dynamic\nregimes (phases). We describe them and their implications for understanding,\npredicting and steering economic instability.\n"
    },
    {
        "paper_id": 1402.0243,
        "authors": "Fabian Dickmann and Nikolaus Schweizer",
        "title": "Faster Comparison of Stopping Times by Nested Conditional Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that deliberately introducing a nested simulation stage can lead to\nsignificant variance reductions when comparing two stopping times by Monte\nCarlo. We derive the optimal number of nested simulations and prove that the\nalgorithm is remarkably robust to misspecifications of this number. The method\nis applied to several problems related to Bermudan/American options. In these\napplications, our method allows to substantially increase the efficiency of\nother variance reduction techniques, namely, Quasi-Control Variates and\nMultilevel Monte Carlo.\n"
    },
    {
        "paper_id": 1402.091,
        "authors": "Yavni Bar-Yam, Marcus A.M. de Aguiar and Yaneer Bar-Yam",
        "title": "The $500.00 AAPL close: Manipulation or hedging? A quantitative analysis",
        "comments": "18 pages, 3 figures. Also available at\n  http://necsi.edu/research/economics/options/AAPLclose.html",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Why do a market's prices move up or down? Claims about causes are made\nwithout actual information, and accepted or dismissed based upon poor or\nnon-existent evidence. Here we investigate the price movements that ended with\nApple stock closing at \\$500.00 on January 18, 2013. There is a ready\nexplanation for this price movement: market manipulation by those who sold\nstock options, who stood to directly benefit from this closing price. Indeed,\none web commentator predicted this otherwise unlikely event publicly. This\nexplanation was subsequently dismissed by press articles that claim that stock\nprices end near such round numbers based upon legitimate hedging activity. But\nhow can we know? We show that the accepted model that points to hedging as the\ndriving cause of prices is not quantitatively consistent with the price\nmovement on that day. The price moved upward too quickly over a period in which\nthe hedgers' position would require selling rather than buying. Under these\nconditions hedgers would have driven the price away from the strike price\nrather than toward it. We also show that a long published theory of the role of\nhedging is incomplete mathematically, and that the correct theory results in\nmuch weaker price movements. This evidence substantially weakens the case of\nthose who claim hedging as cause of anomalous market price movements. The\nexplanation that market manipulation is responsible for the final close cannot\nbe dismissed based upon unsubstantiated, even invalid, hedging claims. Such\nproffered explanations shield potential illegal activity from further inquiry\neven though the claims behind those explanations have not been demonstrated.\n"
    },
    {
        "paper_id": 1402.1046,
        "authors": "F. Y. Ouyang, B. Zheng, X.F. Jiang",
        "title": "Spatial and temporal structures of four financial markets in Greater\n  China",
        "comments": "7 figures. Accepted by Physica A",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications Volume 402,\n  2014, Pages 236",
        "doi": "10.1016/j.physa.2014.02.006",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We investigate the spatial and temporal structures of four financial markets\nin Greater China. In particular, we uncover different characteristics of the\nfour markets by analyzing the sector and subsector structures which are\ndetected through the random matrix theory. Meanwhile, we observe that the\nTaiwan and Hongkong stock markets show a negative return-volatility\ncorrelation, i.e., the so-called leverage effect. The Shanghai and Shenzhen\nstock markets are more complicated. Before the year 2000, the two markets\nexhibit a strong positive return-volatility correlation, which is called the\nanti-leverage effect. After 2000, however, it gradually changes to the leverage\neffect. We also find that the recurrence interval distributions of both the\ntrading volume volatilities and price volatilities follow a power law behavior,\nwhile the exponents vary among different markets.\n"
    },
    {
        "paper_id": 1402.1052,
        "authors": "Adrien Nguyen Huu (CEE-M), Oumar Mbodji, A Nguyen-Huu, Traian A. Pirvu",
        "title": "Optimal Sharing Rule for a Household with a Portfolio Management Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Merton problem of optimal consumption-investment for the case of\ntwo investors sharing a final wealth. The typical example would be a husband\nand wife sharing a portfolio looking to optimize the expected utility of\nconsumption and final wealth. Each agent has different utility function and\ndiscount factor. An explicit formulation for the optimal consumptions and\nportfolio can be obtained in the case of a complete market. The problem is\nshown to be equivalent to maximizing three different utilities separately with\nseparate initial wealths. We study a numerical example where the market price\nof risk is assumed to be mean reverting, and provide insights on the influence\nof risk aversion or discount rates on the initial optimal allocation.\n"
    },
    {
        "paper_id": 1402.1255,
        "authors": "Samuel E. Vazquez",
        "title": "Option Pricing, Historical Volatility and Tail Risks",
        "comments": "29 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the problem of pricing options with historical volatility\nestimators. We do this in the context of a generalized GARCH model with\nmultiple time scales and asymmetry. It is argued that the reason for the\nobserved volatility risk premium is tail risk aversion. We parametrize such\nrisk aversion in terms of three coefficients: convexity, skew and kurtosis risk\npremium. We propose that option prices under the real-world measure are not\nmartingales, but that their drift is governed by such tail risk premia. We then\nderive a fair-pricing equation for options and show that the solutions can be\nwritten in terms of a stochastic volatility model in continuous time and under\na martingale probability measure. This gives a precise connection between the\npricing and real-world probability measures, which cannot be obtained using\nGirsanov Theorem. We find that the convexity risk premium, not only shifts the\noverall implied volatility level, but also changes its term structure.\nMoreover, the skew risk premium makes the skewness of the volatility smile\nsteeper than a pure historical estimate. We derive analytical formulas for\ncertain implied moments using the Bergomi-Guyon expansion. This allows for very\nfast calibrations of the models. We show examples of a particular model which\ncan reproduce the observed SPX volatility surface using very few parameters.\n"
    },
    {
        "paper_id": 1402.1281,
        "authors": "Ovidiu Racorean",
        "title": "Crossing Stocks and the Positive Grassmannian I: The Geometry behind\n  Stock Market",
        "comments": "21 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It seems to be very unlikely that all relevant information in the stock\nmarket could be fully encoded in a geometrical shape. Still,the present paper\nwill reveal the geometry behind the stock market transactions. The prices of\nmarket index (DJIA) stock components are arranged in ascending order from the\nsmallest one in the left to the highest in the right. In such arrangement, as\nstock prices changes due to daily market quotations, it could be noticed that\nthe price of a certain stock get over /under the price of a neighbor stock.\nThese stocks are crossing. Arranged this way, the diagram of successive stock\ncrossings is nothing else than a permutation diagram. From this point on the\nfinancial and combinatorial concepts are netted together to build a bridge\nconnecting the stock market to a beautiful geometrical object that will be\ncalled stock market polytope. The stock market polytope is associated with the\nremarkable structure of positive Grassmannian . This procedure makes all the\nrelevant information about the stock market encoded in the geometrical shape of\nthe stock market polytope more readable.\n"
    },
    {
        "paper_id": 1402.1288,
        "authors": "Thibault Jaisson",
        "title": "Market impact as anticipation of the order flow imbalance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we assume that the permanent market impact of metaorders is\nlinear and that the price is a martingale. Those two hypotheses enable us to\nderive the evolution of the price from the dynamics of the flow of market\norders. For example, if the market order flow is assumed to follow a nearly\nunstable Hawkes process, we retrieve the apparent long memory of the flow\ntogether with a power law impact function which is consistent with the\ncelebrated square root law. We also link the long memory exponent of the sign\nof market orders with the impact function exponent. One of the originalities of\nour approach is that our results are derived without assuming that market\nparticipants are able to detect the beginning of metaorders.\n"
    },
    {
        "paper_id": 1402.1405,
        "authors": "Dror Y. Kenett, Xuqing Huang, Irena Vodenska, Shlomo Havlin, and H.\n  Eugene Stanley",
        "title": "Partial correlation analysis: Applications for financial markets",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The presence of significant cross-correlations between the synchronous time\nevolution of a pair of equity returns is a well-known empirical fact. The\nPearson correlation is commonly used to indicate the level of similarity in the\nprice changes for a given pair of stocks, but it does not measure whether other\nstocks influence the relationship between them. To explore the influence of a\nthird stock on the relationship between two stocks, we use a partial\ncorrelation measurement to determine the underlying relationships between\nfinancial assets. Building on previous work, we present a statistically robust\napproach to extract the underlying relationships between stocks from four\ndifferent financial markets: the United States, the United Kingdom, Japan, and\nIndia. This methodology provides new insights into financial market dynamics\nand uncovers implicit influences in play between stocks. To demonstrate the\ncapabilities of this methodology, we (i) quantify the influence of different\ncompanies and, by studying market similarity across time, present new insights\ninto market structure and market stability, and (ii) we present a practical\napplication, which provides information on the how a company is influenced by\ndifferent economic sectors, and how the sectors interact with each other. These\nexamples demonstrate the effectiveness of this methodology in uncovering\ninformation valuable for a range of individuals, including not only investors\nand traders but also regulators and policy makers.\n"
    },
    {
        "paper_id": 1402.144,
        "authors": "Enrico Onali, John Goddard",
        "title": "Are European equity markets efficient? New evidence from fractal\n  analysis",
        "comments": null,
        "journal-ref": "International Review of Financial Analysis(2011), Vol 20, pp 59 67",
        "doi": "10.1016/j.irfa.2011.02.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fractal analysis is carried out on the stock market indices of seven European\ncountries and the US. We find evidence of long range dependence in the log\nreturn series of the Mibtel (Italy) and the PX Glob (Czech Republic). Long\nrange dependence implies that predictable patterns in the log returns do not\ndissipate quickly, and may therefore produce potential arbitrage opportunities.\nTherefore, these results are in contravention of the Efficient Market\nHypothesis. We show that correcting for short range dependence, or\nprefiltering, may dispose of genuine long range dependence, suggesting that the\nmarket is efficient in cases when it is not. Prefiltering does not reduce\nsignificantly the power of the tests only for cases for which the Hurst\nexponent (a measure of the long range dependence) lies well outside the\nboundaries of no long range dependence. For borderline cases, the prefiltering\nprocedure reduces the power of the test. On the other hand, the absence of\nprefiltering does not result in a test that is significantly oversized.\n"
    },
    {
        "paper_id": 1402.1552,
        "authors": "Ashadun Nobi, Sungmin Lee, Doo Hwan Kim, and Jae Woo Lee",
        "title": "Correlation and Network Topologies in Global and Local Stock Indices",
        "comments": "11 pages,4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physleta.2014.07.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examined how the correlation and network structure of 30 global\nindices and 145 local Korean indices belonging to the KOSPI 200 have changed\nduring the 13-year period, 2000-2012. The correlations among the indices were\ncalculated. The results showed that although the average correlations of the\nglobal indices increased with time, the local indices showed a decreasing trend\nexcept for drastic changes during crises. The average correlation of the local\nindices exceeded the global indices during the crises from 2000-2002, implying\na strong correlation structure among the local indices during this period due\nto the detrimental effect of the dot-com bubble. The threshold networks (TN)\nwere constructed in the observation time window by assigning a threshold value\nand determining the network topologies. A significant change in the network\ntopologies was observed due to the financial crises in both markets. The\nJaccard similarities were also determined using the common links of TNs. The\nTNs of the financial network were not consistent with the evolution of the\ntime, and the successive TNs of the global indices were more similar than those\nof the successive local indices. Finally, the Jaccard similarities identified\nthe change in the market state due to a crisis in both markets.\n"
    },
    {
        "paper_id": 1402.1554,
        "authors": "Kais Hamza, Fima C. Klebaner, Zinoviy Landsman and Ying-Oon Tan",
        "title": "Option Pricing for Symmetric L\\'evy Returns with Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers options pricing when the assumption of normality is\nreplaced with that of the symmetry of the underlying distribution. Such a\nmarket affords many equivalent martingale measures (EMM). However we argue (as\nin the discrete-time setting of Klebaner and Landsman, 2007) that an EMM that\nkeeps distributions within the same family is a \"natural\" choice. We obtain\nBlack-Scholes type option pricing formulae for symmetric Variance-Gamma and\nsymmetric Normal Inverse Gaussian models.\n"
    },
    {
        "paper_id": 1402.1624,
        "authors": "Dietmar Janetzko",
        "title": "Using Twitter to Model the EUR/USD Exchange Rate",
        "comments": "35 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fast, global, and sensitively reacting to political, economic and social\nevents of any kind, these are attributes that social media like Twitter share\nwith foreign exchange markets. The leading assumption of this paper is that\ninformation which can be distilled from public debates on Twitter has\npredictive content for exchange rate movements. This assumption prompted a\nTwitter-based exchange rate model that harnesses regARIMA analyses for\nshort-term out-of-sample ex post forecasts of the daily closing prices of\nEUR/USD spot exchange rates. The analyses used Tweet counts collected from\nJanuary 1, 2012 - September 27, 2013. To identify concepts mentioned on Twitter\nwith a predictive potential the analysis followed a 2-step selection. Firstly,\na heuristic qualitative analysis assembled a long list of 594 concepts, e.g.,\nMerkel, Greece, Cyprus, crisis, chaos, growth, unemployment expected to covary\nwith the ups and downs of the EUR/USD exchange rate. Secondly, cross-validation\nusing window averaging with a fixed-sized rolling origin was deployed to select\nconcepts and corresponding univariate time series that had error scores below\nchance level as defined by the random walk model. With regard to a short list\nof 17 concepts (covariates), in particular SP (Standard & Poor's) and risk, the\nout-of-sample predictive accuracy of the Twitter-based regARIMA model was found\nto be repeatedly better than that obtained from both the random walk model and\na random noise covariate in 1-step ahead forecasts of the EUR/USD exchange\nrate. This advantage was evident on the level of forecast error metrics (MSFE,\nMAE) when a majority vote over different estimation windows was conducted. The\nresults challenge the semi-strong form of the efficient market hypothesis\n(Fama, 1970, 1991) which when applied to the FX market maintains that all\npublicly available information is already integrated into exchange rates.\n"
    },
    {
        "paper_id": 1402.1809,
        "authors": "Erhan Bayraktar and Yuchong Zhang",
        "title": "Minimizing the Probability of Lifetime Ruin Under Ambiguity Aversion",
        "comments": "Final version. To apper in SIAM Journal on Control and Optimization.\n  Keywords: Probability of lifetime ruin, ambiguity aversion, drift\n  uncertainty, viscosity solutions, Perron's method, regularity. 34 pages; 6\n  figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine the optimal robust investment strategy of an individual who\ntargets at a given rate of consumption and seeks to minimize the probability of\nlifetime ruin when she does not have perfect confidence in the drift of the\nrisky asset. Using stochastic control, we characterize the value function as\nthe unique classical solution of an associated Hamilton-Jacobi-Bellman (HJB)\nequation, obtain feedback forms for the optimal investment and drift\ndistortion, and discuss their dependence on various model parameters. In\nanalyzing the HJB equation, we establish the existence and uniqueness of\nviscosity solution using Perron's method, and then upgrade regularity by\nworking with an equivalent convex problem obtained via the Cole-Hopf\ntransformation. We show the original value function may lose convexity for a\nclass of parameters and the Isaacs condition may fail. Numerical examples are\nalso included to illustrate our results.\n"
    },
    {
        "paper_id": 1402.1953,
        "authors": "Anatoliy Swishchuk, Maksym Tertychnyi, Robert Elliott",
        "title": "Pricing Currency Derivatives with Markov-modulated Levy Dynamics",
        "comments": "25 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a Levy process we generalize formulas in Bo et al.(2010) for the\nEsscher transform parameters for the log-normal distribution which ensure the\nmartingale condition holds for the discounted foreign exchange rate. Using\nthese values of the parameters we find a risk-neural measure and provide new\nformulas for the distribution of jumps, the mean jump size, and the Poisson\nprocess intensity with respect to to this measure. The formulas for a European\ncall foreign exchange option are also derived. We apply these formulas to the\ncase of the log-double exponential distribution of jumps. We provide numerical\nsimulations for the European call foreign exchange option prices with different\nparameters.\n"
    },
    {
        "paper_id": 1402.2046,
        "authors": "Sandrine Jacob Leal, Mauro Napoletano, Andrea Roventini and Giorgio\n  Fagiolo",
        "title": "Rock around the Clock: An Agent-Based Model of Low- and High-Frequency\n  Trading",
        "comments": "11 pages, 10 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build an agent-based model to study how the interplay between low- and\nhigh-frequency trading affects asset price dynamics. Our main goal is to\ninvestigate whether high-frequency trading exacerbates market volatility and\ngenerates flash crashes. In the model, low-frequency agents adopt trading rules\nbased on chronological time and can switch between fundamentalist and chartist\nstrategies. On the contrary, high-frequency traders activation is event-driven\nand depends on price fluctuations. High-frequency traders use directional\nstrategies to exploit market information produced by low-frequency traders.\nMonte-Carlo simulations reveal that the model replicates the main stylized\nfacts of financial markets. Furthermore, we find that the presence of\nhigh-frequency trading increases market volatility and plays a fundamental role\nin the generation of flash crashes. The emergence of flash crashes is explained\nby two salient characteristics of high-frequency traders, i.e. their ability to\ni) generate high bid-ask spreads and ii) synchronize on the sell side of the\nlimit order book. Finally, we find that higher rates of order cancellation by\nhigh-frequency traders increase the incidence of flash crashes but reduce their\nduration.\n"
    },
    {
        "paper_id": 1402.2198,
        "authors": "Anton Golub, Gregor Chliamovitch, Alexandre Dupuis and Bastien Chopard",
        "title": "Multi-scale Representation of High Frequency Market Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an event based framework of directional changes and overshoots\nto map continuous financial data into the so-called Intrinsic Network - a state\nbased discretisation of intrinsically dissected time series. Defining a method\nfor state contraction of Intrinsic Network, we show that it has a consistent\nhierarchical structure that allows for multi-scale analysis of financial data.\nWe define an information theoretic measurement termed Liquidity that\ncharacterises the unlikeliness of price trajectories and argue that the new\nmetric has the ability to detect and predict stress in financial markets. We\nshow empirical examples within the Foreign Exchange market where the new\nmeasure not only quantifies liquidity but also acts as an early warning signal.\n"
    },
    {
        "paper_id": 1402.2273,
        "authors": "Anatoliy Swishchuk, Maksym Tertychnyi, Winsor Hoang",
        "title": "Currency Derivatives Pricing for Markov-modulated Merton Jump-diffusion\n  Spot Forex Rate",
        "comments": "17 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1402.1953",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derived similar to Bo et al. (2010) results but in the case when the\ndynamics of the FX rate is driven by a general Merton jump-diffusion process.\nThe main results of our paper are as follows: 1) formulas for the Esscher\ntransform parameters which ensure that the martingale condition for the\ndiscounted foreign exchange rate is a martingale for a general Merton\njump--diffusion process are derived; using the values of these parameters we\nproceeded to a risk-neural measure and provide new formulas for the\ndistribution of jumps, the mean jump size, and the Poisson process intensity\nwith respect to the measure; pricing formulas for European call foreign\nexchange options have been given as well; 2) obtained formulas are applied to\nthe case of the exponential processes; 3) numerical simulations of European\ncall foreign exchange option prices for different parameters are also provided;\n4) codes for Matlab functions used in numerical simulations of option prices\nare given.\n"
    },
    {
        "paper_id": 1402.2492,
        "authors": "Alice X.D. Dong, Jennifer S.K. Chan, Gareth W. Peters",
        "title": "Risk Margin Quantile Function Via Parametric and Non-Parametric Bayesian\n  Quantile Regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop quantile regression models in order to derive risk margin and to\nevaluate capital in non-life insurance applications. By utilizing the entire\nrange of conditional quantile functions, especially higher quantile levels, we\ndetail how quantile regression is capable of providing an accurate estimation\nof risk margin and an overview of implied capital based on the historical\nvolatility of a general insurers loss portfolio. Two modelling frameworks are\nconsidered based around parametric and nonparametric quantile regression models\nwhich we develop specifically in this insurance setting.\n  In the parametric quantile regression framework, several models including the\nflexible generalized beta distribution family, asymmetric Laplace (AL)\ndistribution and power Pareto distribution are considered under a Bayesian\nregression framework. The Bayesian posterior quantile regression models in each\ncase are studied via Markov chain Monte Carlo (MCMC) sampling strategies.\n  In the nonparametric quantile regression framework, that we contrast to the\nparametric Bayesian models, we adopted an AL distribution as a proxy and\ntogether with the parametric AL model, we expressed the solution as a scale\nmixture of uniform distributions to facilitate implementation. The models are\nextended to adopt dynamic mean, variance and skewness and applied to analyze\ntwo real loss reserve data sets to perform inference and discuss interesting\nfeatures of quantile regression for risk margin calculations.\n"
    },
    {
        "paper_id": 1402.2494,
        "authors": "Ludvig Bohlin and Martin Rosvall",
        "title": "Stock portfolio structure of individual investors infers future trading\n  behavior",
        "comments": "9 pages, 4 figures, 1 table",
        "journal-ref": "PLoS ONE (2014) 9(7): e103006",
        "doi": "10.1371/journal.pone.0103006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although the understanding of and motivation behind individual trading\nbehavior is an important puzzle in finance, little is known about the\nconnection between an investor's portfolio structure and her trading behavior\nin practice. In this paper, we investigate the relation between what stocks\ninvestors hold, and what stocks they buy, and show that investors with similar\nportfolio structures to a great extent trade in a similar way. With data from\nthe central register of shareholdings in Sweden, we model the market in a\nsimilarity network, by considering investors as nodes, connected with links\nrepresenting portfolio similarity. From the network, we find groups of\ninvestors that not only identify different investment strategies, but also\nrepresent groups of individual investors trading in a similar way. These\nfindings suggest that the stock portfolios of investors hold meaningful\ninformation, which could be used to earn a better understanding of stock market\ndynamics.\n"
    },
    {
        "paper_id": 1402.2596,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "On Arbitrage and Duality under Model Uncertainty and Portfolio\n  Constraints",
        "comments": "Final version. To appear in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the fundamental theorem of asset pricing (FTAP) and hedging\nprices of options under non-dominated model uncertainty and portfolio\nconstrains in discrete time. We first show that no arbitrage holds if and only\nif there exists some family of probability measures such that any admissible\nportfolio value process is a local super-martingale under these measures. We\nalso get the non-dominated optional decomposition with constraints. From this\ndecomposition, we get duality of the super-hedging prices of European options,\nas well as the sub- and super-hedging prices of American options. Finally, we\nget the FTAP and duality of super-hedging prices in a market where stocks are\ntraded dynamically and options are traded statically.\n"
    },
    {
        "paper_id": 1402.2599,
        "authors": "Arash Fahim and Yu-Jui Huang",
        "title": "Model-independent Superhedging under Portfolio Constraints",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a discrete-time market, we study model-independent superhedging, while the\nsemi-static superhedging portfolio consists of {\\it three} parts: static\npositions in liquidly traded vanilla calls, static positions in other tradable,\nyet possibly less liquid, exotic options, and a dynamic trading strategy in\nrisky assets under certain constraints. By considering the limit order book of\neach tradable exotic option and employing the Monge-Kantorovich theory of\noptimal transport, we establish a general superhedging duality, which admits a\nnatural connection to convex risk measures. With the aid of this duality, we\nderive a model-independent version of the fundamental theorem of asset pricing.\nThe notion \"finite optimal arbitrage profit\", weaker than no-arbitrage, is also\nintroduced. It is worth noting that our method covers a large class of Delta\nconstraints as well as Gamma constraint.\n"
    },
    {
        "paper_id": 1402.303,
        "authors": "Fernando F. Ferreira, A. Christian Silva, Ju-Yi Yen",
        "title": "Information ratio analysis of momentum strategies",
        "comments": "23 pages, 8 figures;v2:revised text",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the past 20 years, momentum or trend following strategies have become an\nestablished part of the investor toolbox. We introduce a new way of analyzing\nmomentum strategies by looking at the information ratio (IR, average return\ndivided by standard deviation). We calculate the theoretical IR of a momentum\nstrategy, and show that if momentum is mainly due to the positive\nautocorrelation in returns, IR as a function of the portfolio formation period\n(look-back) is very different from momentum due to the drift (average return).\nThe IR shows that for look-back periods of a few months, the investor is more\nlikely to tap into autocorrelation. However, for look-back periods closer to 1\nyear, the investor is more likely to tap into the drift. We compare the\nhistorical data to the theoretical IR by constructing stationary periods. The\nempirical study finds that there are periods/regimes where the autocorrelation\nis more important than the drift in explaining the IR (particularly pre-1975)\nand others where the drift is more important (mostly after 1975). We conclude\nour study by applying our momentum strategy to 100 plus years of the Dow-Jones\nIndustrial Average. We report damped oscillations on the IR for look-back\nperiods of several years and model such oscilations as a reversal to the mean\ngrowth rate.\n"
    },
    {
        "paper_id": 1402.3424,
        "authors": "Teycir Abdelghani Goucha",
        "title": "Reference Vectors in Economic Choice",
        "comments": null,
        "journal-ref": "Theoretical and Applied Economics, Volume XX (2013), No. 7(587),\n  pp. 109-118",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the introduction of notion of reference vector paves the way\nfor a combination of classical and social approaches in the framework of\nreferential preferences given by matrix groups. It is shown that individual\ndemand issue from rational decision does not depend on that reference.\n"
    },
    {
        "paper_id": 1402.3464,
        "authors": "Jianjun Gao, Ke Zhou, Duan Li and Xiren Cao",
        "title": "Dynamic Mean-LPM and Mean-CVaR Portfolio Optimization in Continuous-time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Instead of controlling \"symmetric\" risks measured by central moments of\ninvestment return or terminal wealth, more and more portfolio models have\nshifted their focus to manage \"asymmetric\" downside risks that the investment\nreturn is below certain threshold. Among the existing downside risk measures,\nthe lower-partial moments (LPM) and conditional value-at-risk (CVaR) are\nprobably most promising. In this paper we investigate the dynamic mean-LPM and\nmean-CVaR portfolio optimization problems in continuous-time, while the current\nliterature has only witnessed their static versions. Our contributions are\ntwo-fold, in both building up tractable formulations and deriving corresponding\nanalytical solutions. By imposing a limit funding level on the terminal wealth,\nwe conquer the ill-posedness exhibited in the class of mean-downside risk\nportfolio models. The limit funding level not only enables us to solve both\ndynamic mean-LPM and mean-CVaR portfolio optimization problems, but also offers\na flexibility to tame the aggressiveness of the portfolio policies generated\nfrom such mean - downside risk models. More specifically, for a general market\nsetting, we prove the existence and uniqueness of the Lagrangian multiplies,\nwhich is a key step in applying the martingale approach, and establish a\ntheoretical foundation for developing efficient numerical solution approaches.\nMoreover, for situations where the opportunity set of the market setting is\ndeterministic, we derive analytical portfolio policies for both dynamic\nmean-LPM and mean-CVaR formulations.\n"
    },
    {
        "paper_id": 1402.3483,
        "authors": "Matija Pi\\v{s}korec, Nino Antulov-Fantulin, Petra Kralj Novak, Igor\n  Mozeti\\v{c}, Miha Gr\\v{c}ar, Irena Vodenska, Tomislav \\v{S}muc",
        "title": "News Cohesiveness: an Indicator of Systemic Risk in Financial Markets",
        "comments": null,
        "journal-ref": "Scientific Reports 4: 5038 (2014)",
        "doi": "10.1038/srep05038",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by recent financial crises significant research efforts have been\nput into studying contagion effects and herding behaviour in financial markets.\nMuch less has been said about influence of financial news on financial markets.\nWe propose a novel measure of collective behaviour in financial news on the\nWeb, News Cohesiveness Index (NCI), and show that it can be used as a systemic\nrisk indicator. We evaluate the NCI on financial documents from large Web news\nsources on a daily basis from October 2011 to July 2013 and analyse the\ninterplay between financial markets and financially related news. We\nhypothesized that strong cohesion in financial news reflects movements in the\nfinancial markets. Cohesiveness is more general and robust measure of systemic\nrisk expressed in news, than measures based on simple occurrences of specific\nterms. Our results indicate that cohesiveness in the financial news is highly\ncorrelated with and driven by volatility on the financial markets.\n"
    },
    {
        "paper_id": 1402.356,
        "authors": "Bin Zou and Abel Cadenillas",
        "title": "Optimal Investment and Risk Control Problem for an Insurer: Expected\n  Utility Maximization",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the AIG bailout case in the financial crisis of 2007-2008, we\nconsider an insurer who wants to maximize the expected utility of the terminal\nwealth by selecting optimal investment and risk control strategies. The\ninsurer's risk process is modelled by a jump-diffusion process and is\nnegatively correlated with the capital gains in the financial market. We obtain\nexplicit solution to optimal strategies for various utility functions.\n"
    },
    {
        "paper_id": 1402.3562,
        "authors": "Bin Zou and Abel Cadenillas",
        "title": "Explicit Solutions of Optimal Consumption, Investment and Insurance\n  Problem with Regime Switching",
        "comments": "42 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor who wants to select her/his optimal consumption,\ninvestment and insurance policies. Motivated by new insurance products, we\nallow not only the financial marke but also the insurable loss to depend on the\nregime of the economy. The objective of the investor is to maximize her/his\nexpected total discounted utility of consumption over an infinite time horizon.\nFor the case of hyperbolic absolute risk aversion (HARA) utility functions, we\nobtain the first explicit solutions for simultaneous optimal consumption,\ninvestment, and insurance problems when there is regime switching. We determine\nthat the optimal insurance contract is either no-insurance or deductible\ninsurance, and calculate when it is optimal to buy insurance. The optimal\npolicy depends strongly on the regime of the economy. Through an economic\nanalysis, we calculate the advantage of buying insurance.\n"
    },
    {
        "paper_id": 1402.3688,
        "authors": "Annika Birch and Tomaso Aste",
        "title": "Systemic Losses Due to Counter Party Risk in a Stylized Banking System",
        "comments": "31 pages 13 figures",
        "journal-ref": null,
        "doi": "10.1007/s10955-014-1040-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report a study of a stylized banking cascade model investigating systemic\nrisk caused by counter party failure using liabilities and assets to define\nbanks' balance sheet. In our stylized system, banks can be in two states:\nnormally operating or distressed and the state of a bank changes from normally\noperating to distressed whenever its liabilities are larger than the banks'\nassets. The banks are connected through an interbank lending network and,\nwhenever a bank is distressed, its creditor cannot expect the loan from the\ndistressed bank to be repaid, potentially becoming distressed themselves. We\nsolve the problem analytically for a homogeneous system and test the robustness\nand generality of the results with simulations of more complex systems. We\ninvestigate the parameter space and the corresponding distribution of operating\nbanks mapping the conditions under which the whole system is stable or\nunstable. This allows us to determine how financial stability of a banking\nsystem is influenced by regulatory decisions, such as leverage; we discuss the\neffect of central bank actions, such as quantitative easing and we determine\nthe cost of rescuing a distressed banking system using re-capitalisation.\nFinally, we estimate the stability of the UK and US banking systems in the\nyears 2007 and 2012 showing that both banking systems were more unstable in\n2007 and connectedness on the interbank market partly caused the banking\ncrisis.\n"
    },
    {
        "paper_id": 1402.372,
        "authors": "Soumik Pal, Ting-Kam Leonard Wong",
        "title": "The geometry of relative arbitrage",
        "comments": "31 pages, 5 figures; substantially revised; Section 4 illustrates the\n  optiaml transport approach with empirical examples",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider an equity market with $n$ stocks. The vector of proportions of the\ntotal market capitalizations that belong to each stock is called the market\nweight. The market weight defines the market portfolio which is a buy-and-hold\nportfolio representing the performance of the entire stock market. Consider a\nfunction that assigns a portfolio vector to each possible value of the market\nweight, and we perform self-financing trading using this portfolio function. We\nstudy the problem of characterizing functions such that the resulting portfolio\nwill outperform the market portfolio in the long run under the conditions of\ndiversity and sufficient volatility. No other assumption on the future behavior\nof stock prices is made. We prove that the only solutions are functionally\ngenerated portfolios in the sense of Fernholz. A second characterization is\ngiven as the optimal maps of a remarkable optimal transport problem. Both\ncharacterizations follow from a novel property of portfolios called\nmultiplicative cyclical monotonicity.\n"
    },
    {
        "paper_id": 1402.3725,
        "authors": "Micha{\\l} Barski",
        "title": "On the shortfall risk control -- a refinement of the quantile hedging\n  method",
        "comments": "24 pages in Statistics & Risk Modeling. ISSN (Online) 2196-7040, ISSN\n  (Print) 2193-1402",
        "journal-ref": null,
        "doi": "10.1515/strm-2014-1169",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The issue of constructing a risk minimizing hedge under an additional\nalmost-surely type constraint on the shortfall profile is examined. Several\nclassical risk minimizing problems are adapted to the new setting and solved.\nIn particular, the bankruptcy threat of optimal strategies appearing in the\nclassical risk minimizing setting is ruled out. The existence and concrete\nforms of optimal strategies in a general semimartingale market model with the\nuse of conditional statistical tests are proven. The well known quantile\nhedging method as well as the classical Neyman-Pearson lemma are generalized.\nOptimal hedging strategies with shortfall constraints in the Black-Scholes and\nexponential Poisson model are explicitly determined.\n"
    },
    {
        "paper_id": 1402.382,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Information-theoretic approach to lead-lag effect on financial markets",
        "comments": "9 pages, 19 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2014-50108-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently the interest of researchers has shifted from the analysis of\nsynchronous relationships of financial instruments to the analysis of more\nmeaningful asynchronous relationships. Both of those analyses are concentrated\nonly on Pearson's correlation coefficient and thus intraday lead-lag\nrelationships associated with such. Under Efficient Market Hypothesis such\nrelationships are not possible as all information is embedded in the prices. In\nthis paper we analyse lead-lag relationships of financial instruments and\nextend known methodology by using mutual information instead of Pearson's\ncorrelation coefficient, which not only is a more general measure, sensitive to\nnon-linear dependencies, but also can lead to a simpler procedure of\nstatistical validation of links between financial instruments. We analyse\nlagged relationships using NYSE 100 data not only on intraday level but also\nfor daily stock returns, which has usually been ignored.\n"
    },
    {
        "paper_id": 1402.4047,
        "authors": "M. Koz{\\l}owska, T. Gubiec, T. R. Werner, M. Denys, A. Sienkiewicz, R.\n  Kutner, Z. Struzik",
        "title": "Empirical symptoms of catastrophic bifurcation transitions on financial\n  markets: A phenomenological approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The principal aim of this work is the evidence on empirical way that\ncatastrophic bifurcation breakdowns or transitions, proceeded by flickering\nphenomenon, are present on notoriously significant and unpredictable financial\nmarkets. Overall, in this work we developed various metrics associated with\ncatastrophic bifurcation transitions, in particular, the catastrophic slowing\ndown (analogous to the critical slowing down). All these things were considered\non a well-defined example of financial markets of small and middle to large\ncapitalization. The catastrophic bifurcation transition seems to be connected\nwith the question of whether the early-warning signals are present in financial\nmarkets. This question continues to fascinate both the research community and\nthe general public. Interestingly, such early-warning signals have recently\nbeen identified and explained to be a consequence of a catastrophic bifurcation\ntransition phenomenon observed in multiple physical systems, e.g. in\necosystems, climate dynamics and in medicine (epileptic seizure and asthma\nattack). In the present work we provide an analogical, positive identification\nof such phenomenon by examining its several different indicators in the context\nof a well-defined daily bubble; this bubble was induced by the recent worldwide\nfinancial crisis on typical financial markets of small and middle to large\ncapitalization.\n"
    },
    {
        "paper_id": 1402.415,
        "authors": "A.O. Glekin, A. Lykov and K.L. Vaninsky",
        "title": "On Simulation of Various Effects in Consolidated Order Book",
        "comments": "20 figures, 18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper consists of two parts. The first part is devoted to empirical\nanalysis of consolidated order book (COB) for the index RTS futures. In the\nsecond part we consider Poissonian multi--agent model of the COB. By varying\nparameters of different groups of agents submitting orders to the book we are\nable to model various real life phenomenons. In particular we model the spread,\nthe profile of the book and large price changes. Two different mechanisms of\nlarge price changes are considered in detail. One is the disbalance of\nliquidity in the COB and another is the disbalance of sell and buy orders in\nthe order flow.\n"
    },
    {
        "paper_id": 1402.4171,
        "authors": "Rossana Mastrandrea, Tiziano Squartini, Giorgio Fagiolo, Diego\n  Garlaschelli",
        "title": "Reconstructing the world trade multiplex: the role of intensive and\n  extensive biases",
        "comments": null,
        "journal-ref": "Phys. Rev. E 90, 062804 (2014)",
        "doi": "10.1103/PhysRevE.90.062804",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In economic and financial networks, the strength of each node has always an\nimportant economic meaning, such as the size of supply and demand, import and\nexport, or financial exposure. Constructing null models of networks matching\nthe observed strengths of all nodes is crucial in order to either detect\ninteresting deviations of an empirical network from economically meaningful\nbenchmarks or reconstruct the most likely structure of an economic network when\nthe latter is unknown. However, several studies have proved that real economic\nnetworks and multiplexes are topologically very different from configurations\ninferred only from node strengths. Here we provide a detailed analysis of the\nWorld Trade Multiplex by comparing it to an enhanced null model that\nsimultaneously reproduces the strength and the degree of each node. We study\nseveral temporal snapshots and almost one hundred layers (commodity classes) of\nthe multiplex and find that the observed properties are systematically well\nreproduced by our model. Our formalism allows us to introduce the (static)\nconcept of extensive and intensive bias, defined as a measurable tendency of\nthe network to prefer either the formation of extra links or the reinforcement\nof link weights, with respect to a reference case where only strengths are\nenforced. Our findings complement the existing economic literature on (dynamic)\nintensive and extensive trade margins. More in general, they show that\nreal-world multiplexes can be strongly shaped by layer-specific local\nconstraints.\n"
    },
    {
        "paper_id": 1402.4551,
        "authors": "Wenjun Zhang and John Holt",
        "title": "A debt behaviour model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A stochastic model with hidden discrete Markov processes is constructed to\nunderstand the behavior of debtors.\n"
    },
    {
        "paper_id": 1402.4683,
        "authors": "Peter Tankov",
        "title": "Tails of weakly dependent random vectors",
        "comments": "Replaced with revised version",
        "journal-ref": "Journal of Multivariate Analysis, Volume 145, March 2016, Pages\n  73-86",
        "doi": "10.1016/j.jmva.2015.12.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new functional measure of tail dependence for weakly dependent\n(asymptotically independent) random vectors, termed weak tail dependence\nfunction. The new measure is defined at the level of copulas and we compute it\nfor several copula families such as the Gaussian copula, copulas of a class of\nGaussian mixture models, certain Archimedean copulas and extreme value copulas.\nThe new measure allows to quantify the tail behavior of certain functionals of\nweakly dependent random vectors at the log scale.\n"
    },
    {
        "paper_id": 1402.4783,
        "authors": "Matteo Smerlak, Brady Stoll, Agam Gupta, James S. Magdanz",
        "title": "Mapping systemic risk: critical degree and failures distribution in\n  financial networks",
        "comments": "19 pages, 4 figures",
        "journal-ref": "PLoS ONE 10(7): e0130948 (2015)",
        "doi": "10.1371/journal.pone.0130948",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2008 financial crisis illustrated the need for a thorough, functional\nunderstanding of systemic risk in strongly interconnected financial structures.\nDynamic processes on complex networks being intrinsically difficult, most\nrecent studies of this problem have relied on numerical simulations. Here we\nreport analytical results in a network model of interbank lending based on\ndirectly relevant financial parameters, such as interest rates and leverage\nratios. Using a mean-field approach, we obtain a closed-form formula for the\n\"critical degree\", viz. the number of creditors per bank below which an\nindividual shock can propagate throughout the network. We relate the failures\ndistribution (probability that a single shock induces $F$ failures) to the\ndegree distribution (probability that a bank has $k$ creditors), showing in\nparticular that the former is fat-tailed whenever the latter is. Our criterion\nfor the onset of contagion turns out to be isomorphic to the condition for\ncooperation to evolve on graphs and social networks, as recently formulated in\nevolutionary game theory. This remarkable connection supports recent calls for\na methodological rapprochement between finance and ecology.\n"
    },
    {
        "paper_id": 1402.5094,
        "authors": "Samuel Palmer",
        "title": "Accelerating Implicit Finite Difference Schemes Using a Hardware\n  Optimized Tridiagonal Solver for FPGAs",
        "comments": "Preliminary paper, awaiting final implementation results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a design and implementation of the Thomas algorithm optimized for\nhardware acceleration on an FPGA, the Thomas Core. The hardware-based algorithm\ncombined with the custom data flow and low level parallelism available in an\nFPGA reduces the overall complexity from 8N down to 5N serial arithmetic\noperations, and almost halves the overall latency by parallelizing the two\ncostly divisions. Combining this with a data streaming interface, we reduce\nmemory overheads to 2 N-length vectors per N-tridiagonal system to be solved.\nThe Thomas Core allows for multiple independent tridiagonal systems to be\ncontinuously solved in parallel, providing an efficient and scalable\naccelerator for many numerical computations. Finally we present applications\nfor derivatives pricing problems using implicit finite difference schemes on an\nFPGA accelerated system and we investigate the use and limitations of\nfixed-point arithmetic in our algorithm.\n"
    },
    {
        "paper_id": 1402.5208,
        "authors": "Bhaskar DasGupta and Lakshmi Kaligounder",
        "title": "Densely Entangled Financial Systems",
        "comments": "to appear in Network Models in Economics and Finance, V. Kalyagin, P.\n  M. Pardalos and T. M. Rassias (editors), Springer Optimization and Its\n  Applications series, Springer, 2014",
        "journal-ref": "in Network Models in Economics and Finance, V. Kalyagin, P. M.\n  Pardalos and Th. M. Rassias (eds.), Springer Optimization and Its\n  Applications series, 100, 85-105, Springer, 2014",
        "doi": "10.1007/978-3-319-09683-4__5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In [1] Zawadoski introduces a banking network model in which the asset and\ncounter-party risks are treated separately and the banks hedge their assets\nrisks by appropriate OTC contracts. In his model, each bank has only two\ncounter-party neighbors, a bank fails due to the counter-party risk only if at\nleast one of its two neighbors default, and such a counter-party risk is a low\nprobability event. Informally, the author shows that the banks will hedge their\nasset risks by appropriate OTC contracts, and, though it may be socially\noptimal to insure against counter-party risk, in equilibrium banks will {\\em\nnot} choose to insure this low probability event.\n  In this paper, we consider the above model for more general network\ntopologies, namely when each node has exactly 2r counter-party neighbors for\nsome integer r>0. We extend the analysis of [1] to show that as the number of\ncounter-party neighbors increase the probability of counter-party risk also\nincreases, and in particular the socially optimal solution becomes privately\nsustainable when each bank hedges its risk to at least n/2 banks, where n is\nthe number of banks in the network, i.e., when 2r is at least n/2, banks not\nonly hedge their asset risk but also hedge its counter-party risk.\n"
    },
    {
        "paper_id": 1402.53,
        "authors": "Erhan Bayraktar, David Promislow, Virginia Young",
        "title": "Purchasing Life Insurance to Reach a Bequest Goal",
        "comments": "Final version. To appear in in \"Insurance: Mathematics and\n  Economics\". Keywords: Term life insurance, whole life insurance, bequest\n  motive, deterministic control",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We determine how an individual can use life insurance to meet a bequest goal.\nWe assume that the individual's consumption is met by an income, such as a\npension, life annuity, or Social Security. Then, we consider the wealth that\nthe individual wants to devote towards heirs (separate from any wealth related\nto the afore-mentioned income) and find the optimal strategy for buying life\ninsurance to maximize the probability of reaching a given bequest goal. We\nconsider life insurance purchased by a single premium, with and without cash\nvalue available. We also consider irreversible and reversible life insurance\npurchased by a continuously paid premium; one can view the latter as\n(instantaneous) term life insurance.\n"
    },
    {
        "paper_id": 1402.5304,
        "authors": "Ludovic Moreau, Johannes Muhle-Karbe, H. Mete Soner",
        "title": "Trading with Small Price Impact",
        "comments": "46 pages, to appear in \"Mathematical Finance\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An investor trades a safe and several risky assets with linear price impact\nto maximize expected utility from terminal wealth. In the limit for small\nimpact costs, we explicitly determine the optimal policy and welfare, in a\ngeneral Markovian setting allowing for stochastic market, cost, and preference\nparameters. These results shed light on the general structure of the problem at\nhand, and also unveil close connections to optimal execution problems and to\nother market frictions such as proportional and fixed transaction costs.\n"
    },
    {
        "paper_id": 1402.5306,
        "authors": "Ren Liu, Johannes Muhle-Karbe, Marko H. Weber",
        "title": "Rebalancing with Linear and Quadratic Costs",
        "comments": "30 pages, 3 figures, to appear in \"SIAM Journal on Control and\n  Optimization\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market consisting of one safe and one risky asset, which offer\nconstant investment opportunities. Taking into account both proportional\ntransaction costs and linear price impact, we derive optimal rebalancing\npolicies for representative investors with constant relative risk aversion and\na long horizon.\n"
    },
    {
        "paper_id": 1402.5352,
        "authors": "Konstantinos Spiliopoulos",
        "title": "Systemic Risk and Default Clustering for Large Financial Systems",
        "comments": "in Large Deviations and Asymptotic Methods in Finance, (Editors: P.\n  Friz, J. Gatheral, A. Gulisashvili, A. Jacqier, J. Teichmann) , Springer\n  Proceedings in Mathematics and Statistics, Vol. 110 2015,",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As it is known in the finance risk and macroeconomics literature,\nrisk-sharing in large portfolios may increase the probability of creation of\ndefault clusters and of systemic risk. We review recent developments on\nmathematical and computational tools for the quantification of such phenomena.\nLimiting analysis such as law of large numbers and central limit theorems allow\nto approximate the distribution in large systems and study quantities such as\nthe loss distribution in large portfolios. Large deviations analysis allow us\nto study the tail of the loss distribution and to identify pathways to default\nclustering. Sensitivity analysis allows to understand the most likely ways in\nwhich different effects, such as contagion and systematic risks, combine to\nlead to large default rates. Such results could give useful insights into how\nto optimally safeguard against such events.\n"
    },
    {
        "paper_id": 1402.5373,
        "authors": "Anna V. Vilisova, Qiang Fu",
        "title": "Technology Parks Potential for Small and Medium Enterprises",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Being one of the most important factors of economic growth of the country,\ninnovations became one of the key vectors in Russian economic policy. In this\nfield technology parks are one of the most effective instruments which can\nprovide growth of innovative activity in sectors, regions and economies. In\nthis paper, we made a model that allows us to evaluate the effect of technology\nparks in the economy of the country and its potential for small and medium\nenterprises. The model is based on a system of coupled equations, whose\nparameters are estimated on the statistical data that reflect the activity of\nthe economic entity, in an environment of this entity the technology parks are\nacting. Typically, there are regression equations linking a number of economic\nfactors with some output indicators. We analyzed the property of increasing the\nshare of surviving small and medium enterprises for Russian conditions as one\nof the effect of technology parks and built a working model for estimating the\nmaximum (limit) values of the effect.\n"
    },
    {
        "paper_id": 1402.5534,
        "authors": "Imre Kondor",
        "title": "Estimation Error of Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of estimation error of Expected Shortfall is analyzed, with a\nview of its introduction as a global regulatory risk measure.\n"
    },
    {
        "paper_id": 1402.5679,
        "authors": "G. S. Vasilev",
        "title": "Time-dependent Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents an exact solution to the generalized Heston model, where\nthe model parameters are assumed to have linear time dependence The solution\nfor the model in expressed in terms of confluent hypergeometric functions.\n"
    },
    {
        "paper_id": 1402.6204,
        "authors": "F. Bagarello, E. Haven",
        "title": "The role of information in a two-traders market",
        "comments": "in press in Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.02.052",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a very simple stock market, made by only two \\emph{initially equivalent}\ntraders, we discuss how the information can affect the performance of the\ntraders. More in detail, we first consider how the portfolios of the traders\nevolve in time when the market is \\emph{closed}. After that, we discuss two\nmodels in which an interaction with the outer world is allowed. We show that,\nin this case, the two traders behave differently, depending on \\textbf{i)} the\namount of information which they receive from outside; and \\textbf{ii)}the\nquality of this information.\n"
    },
    {
        "paper_id": 1402.6313,
        "authors": "Abdelali Gabih, Hakam Kondakji, J\\\"orn Sass, Ralf Wunderlich",
        "title": "Expert Opinions and Logarithmic Utility Maximization in a Market with\n  Gaussian Drift",
        "comments": "21 pages",
        "journal-ref": "Communications on Stochastic Analysis , Vol. 8, No. 1, 27-47, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates optimal portfolio strategies in a financial market\nwhere the drift of the stock returns is driven by an unobserved Gaussian mean\nreverting process. Information on this process is obtained from observing stock\nreturns and expert opinions. The latter provide at discrete time points an\nunbiased estimate of the current state of the drift. Nevertheless, the drift\ncan only be observed partially and the best estimate is given by the\nconditional expectation given the available information, i.e., by the filter.\nWe provide the filter equations in the model with expert opinion and derive in\ndetail properties of the conditional variance. For an investor who maximizes\nexpected logarithmic utility of his portfolio, we derive the optimal strategy\nexplicitly in different settings for the available information. The optimal\nexpected utility, the value function of the control problem, depends on the\nconditional variance. The bounds and asymptotic results for the conditional\nvariances are used to derive bounds and asymptotic properties for the value\nfunctions. The results are illustrated with numerical examples.\n"
    },
    {
        "paper_id": 1402.6393,
        "authors": "Yang-Yu Liu, Jose C. Nacher, Tomoshiro Ochiai, Mauro Martino, Yaniv\n  Altshuler",
        "title": "Prospect Theory for Online Financial Trading",
        "comments": "23 pages, 4 figures. *: These authors contributed equally to this\n  work",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0109458",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prospect theory is widely viewed as the best available descriptive model of\nhow people evaluate risk in experimental settings. According to prospect\ntheory, people are risk-averse with respect to gains and risk-seeking with\nrespect to losses, a phenomenon called \"loss aversion\". Despite of the fact\nthat prospect theory has been well developed in behavioral economics at the\ntheoretical level, there exist very few large-scale empirical studies and most\nof them have been undertaken with micro-panel data. Here we analyze over 28.5\nmillion trades made by 81.3 thousand traders of an online financial trading\ncommunity over 28 months, aiming to explore the large-scale empirical aspect of\nprospect theory. By analyzing and comparing the behavior of winning and losing\ntrades and traders, we find clear evidence of the loss aversion phenomenon, an\nessence in prospect theory. This work hence demonstrates an unprecedented\nlarge-scale empirical evidence of prospect theory, which has immediate\nimplication in financial trading, e.g., developing new trading strategies by\nminimizing the effect of loss aversion. Moreover, we introduce three\nrisk-adjusted metrics inspired by prospect theory to differentiate winning and\nlosing traders based on their historical trading behavior. This offers us\npotential opportunities to augment online social trading, where traders are\nallowed to watch and follow the trading activities of others, by predicting\npotential winners statistically based on their historical trading behavior\nrather than their trading performance at any given point in time.\n"
    },
    {
        "paper_id": 1402.6444,
        "authors": "Christian Bender and Nikolai Dokuchaev",
        "title": "A First-Order BSPDE for Swing Option Pricing: Classical Solutions",
        "comments": null,
        "journal-ref": "Mathematical Finance, Vol. 27, 902-925, 2017",
        "doi": "10.1111/mafi.12096",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Bender and Dokuchaev (2013), we studied a control problem related to swing\noption pricing in a general non-Markovian setting. The main result there shows\nthat the value process of this control problem can be uniquely characterized in\nterms of a first order backward SPDE and a pathwise differential inclusion. In\nthe present paper we additionally assume that the cashflow process of the swing\noption is left-continuous in expectation (LCE). Under this assumption we show\nthat the value process is continuously differentiable in the space variable\nthat represents the volume which the holder of the option can still exercise\nuntil maturity. This gives rise to an existence and uniqueness result for the\ncorresponding backward SPDE in a classical sense. We also explicitly represent\nthe space derivative of the value process in terms of a nonstandard optimal\nstopping problem over a subset of predictable stopping times. This\nrepresentation can be applied to derive a dual minimization problem in terms of\nmartingales.\n"
    },
    {
        "paper_id": 1402.6583,
        "authors": "Lyudmila A. Glik, Oleg L. Kritski",
        "title": "Finding informed traders in futures and their inderlying assets in\n  intraday trading",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a mathematical procedure for finding informed traders in\nultra-high frequency trading. We wrote it as Vector ARMA and found condition of\nits stationarity. For the price exposure complied with ARMA(1,2) we proved that\nunderlying asset price difference can be derived as ARMA(1,1) process. For\nvalidation of the model, we test an influence of informed traders in EUR/USD,\nGBP/USD, USD/RUB pairs and futures, in gold and futures prices, in Russian\nTrade System share index (RTS) and futures trading. We found some evidence of\nsuch influence in gold and currency pair USD/RUB pricing, in RTS index in the\nperiod from Dec 16 till Dec 20, 2013 and from Jan 28 till Jan 30.\n"
    },
    {
        "paper_id": 1402.676,
        "authors": "Hanqing Jin and Yimin Yang",
        "title": "Time-Inconsistent Mean-Utility Portfolio Selection with Moving Target",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we solve the time inconsistent portfolio selection problem by\nusing different utility functions with a moving target as our constraint. We\nsolve this problem by finding an equilibrium control under the given definition\nas our optimal control. We firstly derive a sufficient equilibrium condition\nfor second-order continuously differentiable utility funtions. Then we use\npower functions of order two, three and four in our problem and find the\nrespective condtions for obtaining an equilibrium for our different problems.\nIn the last part of the paper, we consider using another definition of\nequilibrium to solve our problem when the utility function that we use in our\nproblem is the negative part of x and also find the condtions for obtaining an\nequilibrium.\n"
    },
    {
        "paper_id": 1402.7027,
        "authors": "Florian Ziel, Rick Steinert and Sven Husmann",
        "title": "Efficient Modeling and Forecasting of the Electricity Spot Price",
        "comments": null,
        "journal-ref": "Energy Economics, 47 (2015) 98-111",
        "doi": "10.1016/j.eneco.2014.10.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing importance of renewable energy, especially solar and wind\npower, has led to new forces in the formation of electricity prices. Hence,\nthis paper introduces an econometric model for the hourly time series of\nelectricity prices of the European Power Exchange (EPEX) which incorporates\nspecific features like renewable energy. The model consists of several\nsophisticated and established approaches and can be regarded as a periodic\nVAR-TARCH with wind power, solar power, and load as influences on the time\nseries. It is able to map the distinct and well-known features of electricity\nprices in Germany. An efficient iteratively reweighted lasso approach is used\nfor the estimation. Moreover, it is shown that several existing models are\noutperformed by the procedure developed in this paper.\n"
    },
    {
        "paper_id": 1403.0015,
        "authors": "Maria Letizia Bertotti, Giovanni Modanese",
        "title": "Micro to macro models for income distribution in the absence and in the\n  presence of tax evasion",
        "comments": "21 pages, 9 figures. Submitted to Appl. Math. Comput",
        "journal-ref": "Applied Mathematics and Computation (2014), pp. 836-846",
        "doi": "10.1016/j.amc.2014.07.055",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the effect of tax evasion on the income distribution and the\ninequality index of a society through a kinetic model described by a set of\nnonlinear ordinary differential equations. The model allows to compute the\nglobal outcome of binary and multiple microscopic interactions between\nindividuals. When evasion occurs, both individuals involved in a binary\ninteraction take advantage of it, while the rest of the society is deprived of\na part of the planned redistribution. In general, the effect of evasion on the\nincome distribution is to decrease the population of the middle classes and\nincrease that of the poor and rich classes. We study the dependence of the Gini\nindex on several parameters (mainly taxation rates and evasion rates), also in\nthe case when the evasion rate increases proportionally to a taxation rate\nwhich is perceived by citizens as unfair. Finally, we evaluate the relative\nprobability of class advancement of individuals due to direct interactions and\nwelfare provisions, and some typical temporal rates of convergence of the\nincome distribution to its equilibrium state.\n"
    },
    {
        "paper_id": 1403.0064,
        "authors": "Ladislav Kristoufek",
        "title": "Leverage effect in energy futures",
        "comments": "19 pages, 2 figures, 5 tables",
        "journal-ref": "Energy Economics 42, pp. 50-57, 2014",
        "doi": "10.1016/j.eneco.2013.12.001",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We propose a comprehensive treatment of the leverage effect, i.e. the\nrelationship between returns and volatility of a specific asset, focusing on\nenergy commodities futures, namely Brent and WTI crude oils, natural gas and\nheating oil. After estimating the volatility process without assuming any\nspecific form of its behavior, we find the volatility to be long-term dependent\nwith the Hurst exponent on a verge of stationarity and non-stationarity.\nBypassing this using by using the detrended cross-correlation and the\ndetrending moving-average cross-correlation coefficients, we find the standard\nleverage effect for both crude oil. For heating oil, the effect is not\nstatistically significant, and for natural gas, we find the inverse leverage\neffect. Finally, we also show that none of the effects between returns and\nvolatility is detected as the long-term cross-correlated one. These findings\ncan be further utilized to enhance forecasting models and mainly in the risk\nmanagement and portfolio diversification.\n"
    },
    {
        "paper_id": 1403.0202,
        "authors": "Moritz Duembgen and L.C.G. Rogers",
        "title": "Investing and Stopping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we solve the hedge fund manager's optimization problem in a\nmodel that allows for investors to enter and leave the fund over time depending\non its performance. The manager's payoff at the end of the year will then\ndepend not just on the terminal value of the fund level, but also on the lowest\nand the highest value reached over that time. We establish equivalence to an\noptimal stopping problem for Brownian motion; by approximating this problem\nwith the corresponding optimal stopping problem for a random walk we are led to\na simple and efficient numerical scheme to find the solution, which we then\nillustrate with some examples.\n"
    },
    {
        "paper_id": 1403.0333,
        "authors": "Truc Le",
        "title": "Intrinsic Prices Of Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review the nature of some well-known phenomena such as volatility smiles,\nconvexity adjustments and parallel derivative markets. We propose that the\nmarket is incomplete and postulate the existence of intrinsic risks in every\ncontingent claim as a basis for understanding these phenomena. In a continuous\ntime framework, we bring together the notion of intrinsic risk and the theory\nof change of measures to derive a probability measure, namely risk-subjective\nmeasure, for evaluating contingent claims. This paper is a modest attempt to\nprove that measure of intrinsic risk is a crucial ingredient for explaining\nthese phenomena, and in consequence proposes a new approach to pricing and\nhedging financial derivatives. By adapting theoretical knowledge to practical\napplications, we show that our approach is consistent and robust, compared with\nthe standard risk-neutral approach.\n"
    },
    {
        "paper_id": 1403.0527,
        "authors": "Matyas Barczy, Gyula Pap, Tamas T. Szabo",
        "title": "Parameter estimation for the subcritical Heston model based on discrete\n  time observations",
        "comments": "22 pages, mistakes in the proof of Theorem 3.2 are corrected",
        "journal-ref": "Acta Scientiarum Mathematicarum (Szeged) 82, (2016), 313-338",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study asymptotic properties of some (essentially conditional least\nsquares) parameter estimators for the subcritical Heston model based on\ndiscrete time observations derived from conditional least squares estimators of\nsome modified parameters.\n"
    },
    {
        "paper_id": 1403.0627,
        "authors": "Joseph Byrne, Dimitris Korobilis, Pinho Ribeiro",
        "title": "Exchange Rate Predictability in a Changing World",
        "comments": "84 pages including additional appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An expanding literature articulates the view that Taylor rules are helpful in\npredicting exchange rates. In a changing world however, Taylor rule parameters\nmay be subject to structural instabilities, for example during the Global\nFinancial Crisis. This paper forecasts exchange rates using such Taylor rules\nwith Time Varying Parameters (TVP) estimated by Bayesian methods. In core\nout-of-sample results, we improve upon a random walk benchmark for at least\nhalf, and for as many as eight out of ten, of the currencies considered. This\ncontrasts with a constant parameter Taylor rule model that yields a more\nlimited improvement upon the benchmark. In further results, Purchasing Power\nParity and Uncovered Interest Rate Parity TVP models beat a random walk\nbenchmark, implying our methods have some generality in exchange rate\nprediction.\n"
    },
    {
        "paper_id": 1403.0648,
        "authors": "Jinli Hu and Amos Storkey",
        "title": "Multi-period Trading Prediction Markets with Connections to Machine\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new model for prediction markets, in which we use risk measures\nto model agents and introduce a market maker to describe the trading process.\nThis specific choice on modelling tools brings us mathematical convenience. The\nanalysis shows that the whole market effectively approaches a global objective,\ndespite that the market is designed such that each agent only cares about its\nown goal. Additionally, the market dynamics provides a sensible algorithm for\noptimising the global objective. An intimate connection between machine\nlearning and our markets is thus established, such that we could 1) analyse a\nmarket by applying machine learning methods to the global objective, and 2)\nsolve machine learning problems by setting up and running certain markets.\n"
    },
    {
        "paper_id": 1403.0718,
        "authors": "Xiangyu Cui, Duan Li, Xun Li",
        "title": "Mean-Variance Policy for Discrete-time Cone Constrained Markets: The\n  Consistency in Efficiency and Minimum-Variance Signed Supermartingale Measure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The discrete-time mean-variance portfolio selection formulation, a\nrepresentative of general dynamic mean-risk portfolio selection problems, does\nnot satisfy time consistency in efficiency (TCIE) in general, i.e., a truncated\npre-committed efficient policy may become inefficient when considering the\ncorresponding truncated problem, thus stimulating investors' irrational\ninvestment behavior. We investigate analytically effects of portfolio\nconstraints on time consistency of efficiency for convex cone constrained\nmarkets. More specifically, we derive the semi-analytical expressions for the\npre-committed efficient mean-variance policy and the minimum-variance signed\nsupermartingale measure (VSSM) and reveal their close relationship. Our\nanalysis shows that the pre-committed discrete-time efficient mean-variance\npolicy satisfies TCIE if and only if the conditional expectation of VSSM's\ndensity (with respect to the original probability measure) is nonnegative, or\nonce the conditional expectation becomes negative, it remains at the same\nnegative value until the terminal time. Our findings indicate that the property\nof time consistency in efficiency only depends on the basic market setting,\nincluding portfolio constraints, and this fact motivates us to establish a\ngeneral solution framework in constructing TCIE dynamic portfolio selection\nproblem formulations by introducing suitable portfolio constraints.\n"
    },
    {
        "paper_id": 1403.0842,
        "authors": "Damian Eduardo Taranto, Giacomo Bormetti, Fabrizio Lillo",
        "title": "The adaptive nature of liquidity taking in limit order books",
        "comments": "40 pages, 14 figures, and 2 tables; old figure 12 removed. Accepted\n  for publication on JSTAT",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2014/06/P06002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial markets, the order flow, defined as the process assuming value\none for buy market orders and minus one for sell market orders, displays a very\nslowly decaying autocorrelation function. Since orders impact prices,\nreconciling the persistence of the order flow with market efficiency is a\nsubtle issue. A possible solution is provided by asymmetric liquidity, which\nstates that the impact of a buy or sell order is inversely related to the\nprobability of its occurrence. We empirically find that when the order flow\npredictability increases in one direction, the liquidity in the opposite side\ndecreases, but the probability that a trade moves the price decreases\nsignificantly. While the last mechanism is able to counterbalance the\npersistence of order flow and restore efficiency and diffusivity, the first\nacts in opposite direction. We introduce a statistical order book model where\nthe persistence of the order flow is mitigated by adjusting the market order\nvolume to the predictability of the order flow. The model reproduces the\ndiffusive behaviour of prices at all time scales without fine-tuning the values\nof parameters, as well as the behaviour of most order book quantities as a\nfunction of the local predictability of order flow.\n"
    },
    {
        "paper_id": 1403.0848,
        "authors": "Andreas Joseph, Irena Vodenska, Eugene Stanley, Guanrong Chen",
        "title": "Netconomics: Novel Forecasting Techniques from the Combination of Big\n  Data, Network Science and Economics",
        "comments": "18 pages, 8 figure, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The combination of the network theoretic approach with recently available\nabundant economic data leads to the development of novel analytic and\ncomputational tools for modelling and forecasting key economic indicators. The\nmain idea is to introduce a topological component into the analysis, taking\ninto account consistently all higher-order interactions. We present three basic\nmethodologies to demonstrate different approaches to harness the resulting\nnetwork gain. First, a multiple linear regression optimisation algorithm is\nused to generate a relational network between individual components of national\nbalance of payment accounts. This model describes annual statistics with a high\naccuracy and delivers good forecasts for the majority of indicators. Second, an\nearly-warning mechanism for global financial crises is presented, which\ncombines network measures with standard economic indicators. From the analysis\nof the cross-border portfolio investment network of long-term debt securities,\nthe proliferation of a wide range of over-the-counter-traded financial\nderivative products, such as credit default swaps, can be described in terms of\ngross-market values and notional outstanding amounts, which are associated with\nincreased levels of market interdependence and systemic risk. Third,\nconsidering the flow-network of goods traded between G-20 economies, network\nstatistics provide better proxies for key economic measures than conventional\nindicators. For example, it is shown that a country's gate-keeping potential,\nas a measure for local power, projects its annual change of GDP generally far\nbetter than the volume of its imports or exports.\n"
    },
    {
        "paper_id": 1403.0851,
        "authors": "Dominique Pepin (CRIEF)",
        "title": "Asset Prices and Risk Aversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The standard asset pricing models (the CCAPM and the Epstein-Zin non-expected\nutility model) counterintuitively predict that equilibrium asset prices can\nrise if the representative agent's risk aversion increases. If the income\neffect, which implies enhanced saving as a result of an increase in risk\naversion, dominates the substitution effect, which causes the representative\nagent to reallocate his portfolio in favour of riskless assets, the demand for\nsecurities increases. Thus, asset prices are forced to rise when the\nrepresentative agent is more risk adverse. By disentangling risk aversion and\nintertemporal substituability, we demonstrate that the risky asset price is an\nincreasing function of the coefficient of risk aversion only if the elasticity\nof intertemporal substitution (EIS) exceeds unity. This result, which was first\nproved par Epstein (1988) in a stationary economy setting with a constant risk\naversion, is shown to hold true for non-stationary economies with a variable or\nconstant risk aversion coefficient. The conclusion is that the EIS probably\nexceeds unity.\n"
    },
    {
        "paper_id": 1403.0994,
        "authors": "Behzad Mehrdad, Lingjiong Zhu",
        "title": "On the Hawkes Process with Different Exciting Functions",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Hawkes process is a simple point process, whose intensity function\ndepends on the entire past history and is self-exciting and has the clustering\nproperty. The Hawkes process is in general non-Markovian. The linear Hawkes\nprocess has immigration-birth representation. Based on that, Fierro et al.\nrecently introduced a generalized linear Hawkes model with different exciting\nfunctions. In this paper, we study the convergence to equilibrium, large\ndeviation principle, and moderate deviation principle for this generalized\nmodel. This model also has connections to the multivariate linear Hawkes\nprocess. Some applications to finance are also discussed.\n"
    },
    {
        "paper_id": 1403.1086,
        "authors": "Johan Gunnesson, Alberto Fern\\'andez Mu\\~noz de Morales",
        "title": "Recovering from Derivatives Funding: A consistent approach to DVA, FVA\n  and Hedging",
        "comments": "v2: Expanded discussion on fair-value of FVA and added additional\n  references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The inclusion of DVA in the fair-value of derivative transactions has now\nbecome standard accounting practice in most parts of the world. Furthermore,\nsome sophisticated banks are including an FVA (Funding Valuation Adjustment),\nbut since DVA can be interpreted as a funding benefit the oft-debated issue\nregarding a possible double-counting of funding benefits arises, with little\nconsensus as to its resolution. One possibility is to price the derivative by\nreplication, guaranteeing a consistent inclusion of costs and benefits.\nHowever, as has recently been noted, DVA is (at least partially) unhedgeable,\nhaving no exact market hedge. Furthermore, current frameworks shed little light\non the controversial question, raised by Hull (2012), of whether the effect a\nderivative has on the riskiness of an institution's debt should be taken into\naccount when calculating FVA.\n  In this paper we propose a solution to these two problems by identifying an\ninstrument, a fictitious CDS written on the hedging counterparty which is\nimplicitly contained in any given derivatives transaction. This allows us to\nshow that the hedger's unhedged jump-to-default risk has, despite not being\nactively managed, a well defined value associated to a funding benefit.\nCarrying out the replication including such a CDS, we obtain a price for the\nderivative consisting of its collateralized equivalent, a contingent CVA, a\ncontingent DVA, and an FVA, coupled to the price via the hedger's short-term\nbond-CDS basis.\n  The resulting funding cost is non-zero, but substantially smaller than what\nis obtained in alternative approaches due to the effect the derivative has on\nthe recovery of the hedger's liabilities. Also, price agreement is possible for\ntwo sophisticated counterparties entering a deal if their bond-CDS bases obey a\ncertain relationship, similar to what was first obtained by Morini and\nPrampolini (2010).\n"
    },
    {
        "paper_id": 1403.1183,
        "authors": "David Landriault and Bin Li and Hongzhong Zhang",
        "title": "On the Frequency of Drawdowns for Brownian Motion Processes",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": "10.1239/jap/1429282615",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Drawdowns measuring the decline in value from the historical running maxima\nover a given period of time, are considered as extremal events from the\nstandpoint of risk management. To date, research on the topic has mainly focus\non the side of severity by studying the first drawdown over certain\npre-specified size. In this paper, we extend the discussion by investigating\nthe frequency of drawdowns, and some of their inherent characteristics. We\nconsider two types of drawdown time sequences depending on whether a historical\nrunning maximum {is reset or not}. For each type, we study the frequency rate\nof drawdowns, the Laplace transform of the $n$-th drawdown time, the\ndistribution of the running maximum and the value process at the $n$-th\ndrawdown time, as well as some other quantities of interest. Interesting\nrelationships between these two drawdown time sequences are also established.\nFinally, insurance policies protecting against the risk of frequent drawdowns\nare also proposed and priced.\n"
    },
    {
        "paper_id": 1403.1363,
        "authors": "Xiaobing Feng, Woo Seong Jo and Beom Jun Kim",
        "title": "International Transmission of Shocks and Fragility of a Bank Network",
        "comments": "9 pages, 4 figures",
        "journal-ref": "Physica A 403 (2014) 120--129",
        "doi": "10.1016/j.physa.2014.02.030",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The weighted and directed network of countries based on the number of\noverseas banks is analyzed in terms of its fragility to the banking crisis of\none country. We use two different models to describe transmission of shocks,\none local and the other global. Depending on the original source of the crisis,\nthe overall size of crisis impacts is found to differ country by country. For\nthe two-step local spreading model, it is revealed that the scale of the first\nimpact is determined by the out-strength, the total number of overseas branches\nof the country at the origin of the crisis, while the second impact becomes\nmore serious if the in-strength at the origin is increased. For the global\nspreading model, some countries named \"triggers\" are found to play important\nroles in shock transmission, and the importance of the feed-forward-loop\nmechanism is pointed out. We also discuss practical policy implications of the\npresent work.\n"
    },
    {
        "paper_id": 1403.1509,
        "authors": "Michael B. Walker",
        "title": "Modelling the Bid and Ask Prices of Illiquid CDSs",
        "comments": "37 pages, 11 figures. http://www.worldscinet.com/ijtaf/",
        "journal-ref": "IJTAF Vol. 15, Iss. 06, 2012, Page 1250045",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  CDS (credit default swap) contracts that were initiated some time ago\nfrequently have spreads and/or maturities that are not available on the current\nmarket of CDSs, and are thus illiquid. This article introduces an\nincomplete-market approach to valuing illiquid CDSs that, in contrast to the\nrisk-neutral approach of current market practice, allows a dealer who buys an\nilliquid CDS from an investor to determine ask and bid prices (which differ) in\nsuch a way as to guarantee a minimum positive expected rate of return on the\ndeal. An alternative procedure, which replaces the expected rate of return by\nan analogue of the Sharpe ratio, is also discussed. The approach to pricing\njust described belongs to the good-deal category of approaches, since the\ndealer decides what it would take to make an appropriate expected rate of\nreturn, and sets the bid and ask prices accordingly. A number of different\nhedges are discussed and compared within the general framework developed in the\narticle. The approach is implemented numerically, and example plots of\nimportant quantities are given. The paper also develops a useful result in\nlinear programming theory in the case that the cost vector is random.\n"
    },
    {
        "paper_id": 1403.1548,
        "authors": "Peter Klimek, Sebastian Poledna, J. Doyne Farmer, Stefan Thurner",
        "title": "To bail-out or to bail-in? Answers from an agent-based model",
        "comments": "10 pages, 3 figures",
        "journal-ref": "Journal of Economic Dynamics and Control 50, 144-154, (2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since beginning of the 2008 financial crisis almost half a trillion euros\nhave been spent to financially assist EU member states in taxpayer-funded\nbail-outs. These crisis resolutions are often accompanied by austerity programs\ncausing political and social friction on both domestic and international\nlevels. The question of how to resolve failing financial institutions under\nwhich economic preconditions is therefore a pressing and controversial issue of\nvast political importance. In this work we employ an agent-based model to study\nthe economic and financial ramifications of three highly relevant crisis\nresolution mechanisms. To establish the validity of the model we show that it\nreproduces a series of key stylized facts if the financial and real economy.\nThe distressed institution can either be closed via a purchase & assumption\ntransaction, it can be bailed-out using taxpayer money, or it may be bailed-in\nin a debt-to-equity conversion. We find that for an economy characterized by\nlow unemployment and high productivity the optimal crisis resolution with\nrespect to financial stability and economic productivity is to close the\ndistressed institution. For economies in recession with high unemployment the\nbail-in tool provides the most efficient crisis resolution mechanism. Under no\ncircumstances do taxpayer-funded bail-out schemes outperform bail-ins with\nprivate sector involvement.\n"
    },
    {
        "paper_id": 1403.1574,
        "authors": "V. Gontis and A. Kononovicius",
        "title": "Consentaneous agent-based and stochastic model of the financial markets",
        "comments": "17 pages, 6 figures, Gontis V, Kononovicius A (2014) Consentaneous\n  Agent-Based and Stochastic Model of the Financial Markets. PLoS ONE 9(7):\n  e102201. doi: 10.1371/journal.pone.0102201",
        "journal-ref": "PLoS ONE 9(7), e102201, 2014",
        "doi": "10.1371/journal.pone.0102201",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We are looking for the agent-based treatment of the financial markets\nconsidering necessity to build bridges between microscopic, agent based, and\nmacroscopic, phenomenological modeling. The acknowledgment that agent-based\nmodeling framework, which may provide qualitative and quantitative\nunderstanding of the financial markets, is very ambiguous emphasizes the\nexceptional value of well defined analytically tractable agent systems. Herding\nas one of the behavior peculiarities considered in the behavioral finance is\nthe main property of the agent interactions we deal with in this contribution.\nLooking for the consentaneous agent-based and macroscopic approach we combine\ntwo origins of the noise: exogenous one, related to the information flow, and\nendogenous one, arising form the complex stochastic dynamics of agents. As a\nresult we propose a three state agent-based herding model of the financial\nmarkets. From this agent-based model we derive a set of stochastic differential\nequations, which describes underlying macroscopic dynamics of agent population\nand log price in the financial markets. The obtained solution is then subjected\nto the exogenous noise, which shapes instantaneous return fluctuations. We test\nboth Gaussian and q-Gaussian noise as a source of the short term fluctuations.\nThe resulting model of the return in the financial markets with the same set of\nparameters reproduces empirical probability and spectral densities of absolute\nreturn observed in New York, Warsaw and NASDAQ OMX Vilnius Stock Exchanges. Our\nresult confirms the prevalent idea in behavioral finance that herding\ninteractions may be dominant over agent rationality and contribute towards\nbubble formation.\n"
    },
    {
        "paper_id": 1403.1637,
        "authors": "Charles D. Brummitt, Rajiv Sethi, Duncan J. Watts",
        "title": "Inside Money, Procyclical Leverage, and Banking Catastrophes",
        "comments": "31 pages, 10 figures",
        "journal-ref": "PLoS ONE 9(8): e104219",
        "doi": "10.1371/journal.pone.0104219",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore a model of the interaction between banks and outside investors in\nwhich the ability of banks to issue inside money (short-term liabilities\nbelieved to be convertible into currency at par) can generate a collapse in\nasset prices and widespread bank insolvency. The banks and investors share a\ncommon belief about the future value of certain long-term assets, but they have\ndifferent objective functions; changes to this common belief result in\nportfolio adjustments and trade. Positive belief shocks induce banks to buy\nrisky assets from investors, and the banks finance those purchases by issuing\nnew short-term liabilities. Negative belief shocks induce banks to sell assets\nin order to reduce their chance of insolvency to a tolerably low level, and\nthey supply more assets at lower prices, which can result in multiple\nmarket-clearing prices. A sufficiently severe negative shock causes the set of\nequilibrium prices to contract (in a manner given by a cusp catastrophe),\ncausing prices to plummet discontinuously and banks to become insolvent.\nSuccessive positive and negative shocks of equal magnitude do not cancel;\nrather, a banking catastrophe can occur even if beliefs simply return to their\ninitial state. Capital requirements can prevent crises by curtailing the\nexpansion of balance sheets when beliefs become more optimistic, but they can\nalso force larger price declines. Emergency asset price supports can be\nunderstood as attempts by a central bank to coordinate expectations on an\nequilibrium with solvency.\n"
    },
    {
        "paper_id": 1403.1715,
        "authors": "Damien Challet and Ahmed Bel Hadj Ayed",
        "title": "Do Google Trend data contain more predictability than price returns?",
        "comments": "15 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using non-linear machine learning methods and a proper backtest procedure, we\ncritically examine the claim that Google Trends can predict future price\nreturns. We first review the many potential biases that may influence backtests\nwith this kind of data positively, the choice of keywords being by far the\ngreatest culprit. We then argue that the real question is whether such data\ncontain more predictability than price returns themselves: our backtest yields\na performance of about 17bps per week which only weakly depends on the kind of\ndata on which predictors are based, i.e. either past price returns or Google\nTrends data, or both.\n"
    },
    {
        "paper_id": 1403.1804,
        "authors": "Andrey Itkin",
        "title": "High-Order Splitting Methods for Forward PDEs and PIDEs",
        "comments": "25 pages, 2 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is dedicated to the construction of high-order (in both space and\ntime) finite-difference schemes for both forward and backward PDEs and PIDEs,\nsuch that option prices obtained by solving both the forward and backward\nequations are consistent. This approach is partly inspired by Andreasen & Huge,\n2011 who reported a pair of consistent finite-difference schemes of first-order\napproximation in time for an uncorrelated local stochastic volatility model. We\nextend their approach by constructing schemes that are second-order in both\nspace and time and that apply to models with jumps and discrete dividends.\nTaking correlation into account in our approach is also not an issue.\n"
    },
    {
        "paper_id": 1403.1822,
        "authors": "Asim Ghosh, Arnab Chatterjee, Anindya S. Chakrabarti, Bikas K\n  Chakrabarti",
        "title": "Zipf's law in city size from a resource utilization model",
        "comments": "7 pages, 5 figs; accepted in Phys Rev E",
        "journal-ref": "Phys. Rev. E 90 (2014) 042815",
        "doi": "10.1103/PhysRevE.90.042815",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a resource utilization scenario characterized by intrinsic fitness.\nTo describe the growth and organization of different cities, we consider a\nmodel for resource utilization where many restaurants compete, as in a game, to\nattract customers using an iterative learning process. Results for the case of\nrestaurants with uniform fitness are reported. When fitness is uniformly\ndistributed, it gives rise to a Zipf law for the number of customers. We\nperform an exact calculation for the utilization fraction for the case when\nchoices are made independent of fitness. A variant of the model is also\nintroduced where the fitness can be treated as an ability to stay in the\nbusiness. When a restaurant loses customers, its fitness is replaced by a\nrandom fitness. The steady state fitness distribution is characterized by a\npower law, while the distribution of the number of customers still follows the\nZipf law, implying the robustness of the model. Our model serves as a paradigm\nfor the emergence of Zipf law in city size distribution.\n"
    },
    {
        "paper_id": 1403.1889,
        "authors": "Thierry Roncalli",
        "title": "Introduction to Risk Parity and Budgeting",
        "comments": "151 pages, 32 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although portfolio management didn't change much during the 40 years after\nthe seminal works of Markowitz and Sharpe, the development of risk budgeting\ntechniques marked an important milestone in the deepening of the relationship\nbetween risk and asset management. Risk parity then became a popular financial\nmodel of investment after the global financial crisis in 2008. Today, pension\nfunds and institutional investors are using this approach in the development of\nsmart indexing and the redefinition of long-term investment policies.\n  Introduction to Risk Parity and Budgeting provides an up-to-date treatment of\nthis alternative method to Markowitz optimization. It builds financial exposure\nto equities and commodities, considers credit risk in the management of bond\nportfolios, and designs long-term investment policy.\n  This book contains the solutions of tutorial exercices which are included in\nIntroduction to Risk Parity and Budgeting.\n"
    },
    {
        "paper_id": 1403.205,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Partial Mutual Information Analysis of Financial Networks",
        "comments": "6 pages, 4 figures, 1 table, submitted to EPL",
        "journal-ref": null,
        "doi": "10.12693/APhysPolA.127.863",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The econophysics approach to socio-economic systems is based on the\nassumption of their complexity. Such assumption inevitably lead to another\nassumption, namely that underlying interconnections within socio-economic\nsystems, particularly financial markets, are nonlinear, which is shown to be\ntrue even in mainstream economic literature. Thus it is surprising to see that\nnetwork analysis of financial markets is based on linear correlation and its\nderivatives. An analysis based on partial correlation is of particular interest\nas it leading to the vicinity of causality detection in time series analysis.\nIn this paper we generalise the Planar Maximally Filtered Graphs and Partial\nCorrelation Planar Graphs to incorporate nonlinearity using partial mutual\ninformation.\n"
    },
    {
        "paper_id": 1403.206,
        "authors": "Michael B. Walker",
        "title": "Modelling Credit Default Swaps: Market-Standard Vs Incomplete-Market\n  Models",
        "comments": "19 pages, 5 figures, submitted for consideration in International\n  Journal of Theoretical and Applied Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, incomplete-market techniques have been used to develop a model\napplicable to credit default swaps (CDSs) with results obtained that are quite\ndifferent from those obtained using the market-standard model. This article\nmakes use of the new incomplete-market model to further study CDS hedging and\nextends the model so that it is capable treating single-name CDS portfolios.\nAlso, a hedge called the vanilla hedge is described, and with it, analytic\nresults are obtained explaining the striking features of the plot of\nno-arbitrage bounds versus CDS maturity for illiquid CDSs. The valuation\nprocess that follows from the incomplete-market model is an integrated\nmodelling and risk management procedure, that first uses the model to find the\narbitrage-free range of fair prices, and then requires risk management\nprofessionals for both the buyer and the seller to find, as a basis for\nnegotiation, prices that both respect the range of fair prices determined by\nthe model, and also benefit their firms. Finally, in a section on numerical\nresults, the striking behavior of the no-arbitrage bounds as a function of CDS\nmaturity is illustrated, and several examples describe the reduction in risk by\nthe hedging of single-name CDS portfolios.\n"
    },
    {
        "paper_id": 1403.2229,
        "authors": "Dieter Hendricks, Diane Wilcox",
        "title": "A reinforcement learning extension to the Almgren-Chriss model for\n  optimal trade execution",
        "comments": "6 pages, 2014 IEEE Computational Intelligence for Financial\n  Engineering and Economics conference (accepted)",
        "journal-ref": null,
        "doi": "10.1109/CIFEr.2014.6924109",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reinforcement learning is explored as a candidate machine learning technique\nto enhance existing analytical solutions for optimal trade execution with\nelements from the market microstructure. Given a volume-to-trade, fixed time\nhorizon and discrete trading periods, the aim is to adapt a given volume\ntrajectory such that it is dynamic with respect to favourable/unfavourable\nconditions during realtime execution, thereby improving overall cost of\ntrading. We consider the standard Almgren-Chriss model with linear price impact\nas a candidate base model. This model is popular amongst sell-side institutions\nas a basis for arrival price benchmark execution algorithms. By training a\nlearning agent to modify a volume trajectory based on the market's prevailing\nspread and volume dynamics, we are able to improve post-trade implementation\nshortfall by up to 10.3% on average compared to the base model, based on a\nsample of stocks and trade sizes in the South African equity market.\n"
    },
    {
        "paper_id": 1403.273,
        "authors": "M. Nabil Kazi-Tani, Dylan Possama\\\"i, Chao Zhou",
        "title": "Quadratic BSDEs with jumps: related non-linear expectations",
        "comments": "28 pages. Formerly part of arXiv:1208.5581",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we follow the study of quadratic backward SDEs with\njumps,that is to say for which the generator has quadratic growth in the\nvariables (z; u), started in our accompanying paper [15]. Relying on the\nexistence and uniqueness result of [15], we define the corresponding\ng-expectations and study some of their properties. We obtain in particular a\nnon-linear Doob-Meyer decomposition for g-submartingales and a downcrossing\ninequality which implies their regularity in time. As a consequence of these\nresults, we also obtain a converse comparison theorem for our class of BSDEs.\nFinally, we provide a dual representation for the corresponding dynamic risk\nmeasures, and study the properties of their inf-convolution, giving several\nexplicit examples\n"
    },
    {
        "paper_id": 1403.3138,
        "authors": "Dong Han Kim, Stefano Marmi",
        "title": "Distribution of the asset price movement and market potential",
        "comments": "6 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we discuss the distribution of asset price movements by the\nmarket potential function. From the principle of free energy minimization we\nanalyze two different kinds of market potentials. We obtain a U-shaped\npotential when market reversion (i.e. contrarian investors) is dominant. On the\nother hand, if there are more trend followers, flat and logarithmic--like\npotentials appeared. By using the Cyclical Adjusted Price-to-Earning ratio,\nwhich is a common valuation tool, we empirically investigate the market data.\nBy studying long term data we observe the historical change of the market\npotential of the US stock market. Recent US data shows that the market\npotential looks more likely as the trending followers' potential. Next, we\ncompare the market potentials for 12 different countries. Though some countries\nhave similar market potentials, there are specific examples like Japan which\nexhibits very flat potential.\n"
    },
    {
        "paper_id": 1403.3212,
        "authors": "Jakub Trybu{\\l}a and Dariusz Zawisza",
        "title": "Continuous-Time Portfolio Choice Under Monotone Mean-Variance\n  Preferences-Stochastic Factor Case",
        "comments": "Major revision, the same model but the main result is strenghtened,\n  the square root factor model added (Heston model)",
        "journal-ref": "Mathematics of Operations Research 44 (2019), 966-987",
        "doi": "10.1287/moor.2018.0952",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an incomplete market with a nontradable stochastic factor and a\ncontinuous time investment problem with an optimality criterion based on\nmonotone mean-variance preferences. We formulate it as a stochastic\ndifferential game problem and use Hamilton-Jacobi-Bellman-Isaacs equations to\nfind an optimal investment strategy and the value function. What is more, we\nshow that our solution is also optimal for the classical Markowitz problem and\nevery optimal solution for the classical Markowitz problem is optimal also for\nthe monotone mean-variance preferences. These results are interesting because\nthe original Markowitz functional is not monotone, and it was observed that in\nthe case of a static one-period optimization problem the solutions for those\ntwo functionals are different. In addition, we determine explicit Markowitz\nstrategies in the square root factor models.\n"
    },
    {
        "paper_id": 1403.3223,
        "authors": "Jakub Trybu{\\l}a",
        "title": "Merton problem with one additional indivisible asset",
        "comments": null,
        "journal-ref": "Universitatis Iagellonicae Acta Mathematica, 52 (2014), 45-56",
        "doi": "10.4467/20843828AM.15.005.3909",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a modification of the classical Merton portfolio\noptimization problem. Namely, an investor can trade in financial asset and\nconsume his capital. He is additionally endowed with a one unit of an\nindivisible asset which he can sell at any time. We give a numerical example of\ncalculating the optimal time to sale the indivisible asset, the optimal\nconsumption rate and the value function.\n"
    },
    {
        "paper_id": 1403.3294,
        "authors": "Lyudmila A. Glik, Oleg L. Kritski",
        "title": "Detecting informed activities in European-style option tradings",
        "comments": "9 pages. arXiv admin note: substantial text overlap with\n  arXiv:1402.6583",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a mathematical procedure for finding informed trader activities in\nEuropean-style options and their underlying asset. The regression model (9)\nwith moving average component was written. Being added to it ARMA-process for\nlog-price differences of underlying asset, the generalized model is written as\nVector ARMA, stable at abs(ro)<1. We also constructed an informed trader\nactivity presence criterion. Using TAIFEX option prices we investigate whether\nsuch activity was at the market. We found that there is no significant\ninfluence for pricing process made by major market players.\n"
    },
    {
        "paper_id": 1403.3362,
        "authors": "Dorje C. Brody and Stala Hadjipetri",
        "title": "Coherent Chaos Interest Rate Models",
        "comments": "26 pages",
        "journal-ref": "International Journal of Theoretical and Applied Finance 18,\n  1550016 (2015)",
        "doi": "10.1142/S0219024915500168",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Wiener chaos approach to interest rate modelling arises from the\nobservation that the pricing kernel admits a representation in terms of the\nconditional variance of a square-integrable random variable, which in turn\nadmits a chaos expansion. When the expansion coefficients factorise into\nmultiple copies of a single function, then the resulting interest rate model is\ncalled coherent, whereas a generic interest rate model will necessarily be\nincoherent. Coherent representations are nevertheless of fundamental importance\nbecause incoherent ones can always be expressed as a linear superposition of\ncoherent elements. This property is exploited to derive general expressions for\nthe pricing kernel and the associated bond price and short rate processes in\nthe case of an n-th order chaos model for each $n$. The pricing formulae for\nbond options and swaptions are obtained in closed forms for a number of\nexamples. An explicit representation for the pricing kernel of a\ngeneric---incoherent---model is then obtained by use of the underlying coherent\nelements. Finally, finite-dimensional realisations of the coherent chaos models\nare investigated in detail. In particular, it is shown that a class of highly\ntractable models can be constructed having the characteristic feature that the\ndiscount bond price is given by a piecewise flat (simple) process.\n"
    },
    {
        "paper_id": 1403.3459,
        "authors": "Tahir Choulli and Jun Deng",
        "title": "Structure conditions under progressively added information",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been understood that the \"local\" existence of the Markowitz' optimal\nportfolio or the solution to the local-risk minimization problem is guaranteed\nby some specific mathematical structures on the underlying assets price\nprocesses known in the literature as \"{\\it Structure Conditions}\". In this\npaper, we consider a semi-martingale market model, and an arbitrary random time\nthat is not adapted to the information flow of the market model. This random\ntime may model the default time of a firm, the death time of an insured, or any\nthe occurrence time of an event that might impact the market model somehow. By\nadding additional uncertainty to the market model, via this random time, the\n{\\it structures conditions} may fail and hence the Markowitz's optimal\nportfolio and other quadratic-optimal portfolios might fail to exist. Our aim\nis to investigate the impact of this random time on the structures conditions\nfrom different perspectives. Our analysis allows us to conclude that under some\nmild assumptions on the market model and the random time, these structures\nconditions will remain valid on the one hand. Furthermore, we provide two\nexamples illustrating the importance of these assumptions. On the other hand,\nwe describe the random time models for which these structure conditions are\npreserved for any market model. These results are elaborated separately for the\ntwo contexts of stopping with the random time and incorporating totally a\nspecific class of random times respectively.\n"
    },
    {
        "paper_id": 1403.3478,
        "authors": "Gao-Feng Gu, Xiong Xiong, Wei Zhang, Yong-Jie Zhang and Wei-Xing Zhou",
        "title": "Empirical properties of inter-cancellation durations in the Chinese\n  stock market",
        "comments": "14 pages, 7 figures and 5 tables",
        "journal-ref": "Frontiers in Physics 2, 16 (2014)",
        "doi": "10.3389/fphy.2014.00016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Order cancellation process plays a crucial role in the dynamics of price\nformation in order-driven stock markets and is important in the construction\nand validation of computational finance models. Based on the order flow data of\n18 liquid stocks traded on the Shenzhen Stock Exchange in 2003, we investigate\nthe empirical statistical properties of inter-cancellation durations in units\nof events defined as the waiting times between two consecutive cancellations.\nThe inter-cancellation durations for both buy and sell orders of all the stocks\nfavor a $q$-exponential distribution when the maximum likelihood estimation\nmethod is adopted; In contrast, both cancelled buy orders of 6 stocks and\ncancelled sell orders of 3 stocks prefer Weibull distribution when the\nnonlinear least-square estimation is used. Applying detrended fluctuation\nanalysis (DFA), centered detrending moving average (CDMA) and multifractal\ndetrended fluctuation analysis (MF-DFA) methods, we unveil that the\ninter-cancellation duration time series process long memory and multifractal\nnature for both buy and sell cancellations of all the stocks. Our findings show\nthat order cancellation processes exhibit long-range correlated bursty\nbehaviors and are thus not Poissonian.\n"
    },
    {
        "paper_id": 1403.3571,
        "authors": "Iacopo Mastromatteo, Bence Toth and Jean-Philippe Bouchaud",
        "title": "Anomalous impact in reaction-diffusion models",
        "comments": "5 pages, 2 figures",
        "journal-ref": "Phys. Rev. Lett. 113, 268701 (2014)",
        "doi": "10.1103/PhysRevLett.113.268701",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize the reaction-diffusion model A + B -> 0 in order to study the\nimpact of an excess of A (or B) at the reaction front. We provide an exact\nsolution of the model, which shows that linear response breaks down: the\naverage displacement of the reaction front grows as the square-root of the\nimbalance. We argue that this model provides a highly simplified but generic\nframework to understand the square-root impact of large orders in financial\nmarkets.\n"
    },
    {
        "paper_id": 1403.3584,
        "authors": "Rudolf Fiebig and David Musgrove",
        "title": "Testing for Detailed Balance in a Financial Market",
        "comments": "7 pages, 11 figures, pdflatex",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test a historical price time series in a financial market (the NASDAQ 100\nindex) for a statistical property known as detailed balance. The presence of\ndetailed balance would imply that the market can be modeled by a stochastic\nprocess based on a Markov chain, thus leading to equilibrium. In economic\nterms, a positive outcome of the test would support the efficient market\nhypothesis, a cornerstone of neo-classical economic theory. In contrast to the\nusage in prevalent economic theory the term equilibrium here is tied to the\nreturns, rather than the price time series. The test is based on an action\nfunctional $S$ constructed from the elements of the detailed balance condition\nand the historical data set, and then analyzing $S$ by means of simulated\nannealing. Checks are performed to verify the validity of the analysis method.\nWe discuss the outcome of this analysis.\n"
    },
    {
        "paper_id": 1403.3627,
        "authors": "Claudiu Tiberiu Albulescu (CRIEF), Dominique Pepin (CRIEF), Aviral\n  Kumar Tiwari",
        "title": "A re-examination of real interest parity in CEECs using old and new\n  generations of panel unit root tests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study applies old and new generations of panel unit root tests to test\nthe validity of long-run real interest rate parity (RIP) hypothesis for ten\nCentral and Eastern European Countries (CEECs) with respect to the Euro area\nand an average of the CEECs' real interest rates, respectively. When the panel\nunit root tests are carried out with respect to the Euro area rate, we confirm\nthe results of previous studies which support the RIP hypothesis. Nevertheless,\nwhen the test is performed using the average of the CEECs' rate, our results\nare mitigated, revealing that the hypothesis of CEECs' interest rates\nconvergence cannot be taken for granted. From a robustness analysis\nperspective, our findings indicate that the RIP hypothesis for CEECs should be\nconsidered with cautions, being sensitive to the benchmark.\n"
    },
    {
        "paper_id": 1403.3638,
        "authors": "Giulia Iori, Rosario N. Mantegna, Luca Marotta, Salvatore Micciche',\n  James Porter, Michele Tumminello",
        "title": "Networked relationships in the e-MID Interbank market: A trading model\n  with memory",
        "comments": "37 pages, 10 figures",
        "journal-ref": "JEDC, 50, 98-116, (2015)",
        "doi": "10.1016/j.jedc.2014.08.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interbank markets are fundamental for bank liquidity management. In this\npaper, we introduce a model of interbank trading with memory. Our model\nreproduces features of preferential trading patterns in the e-MID market\nrecently empirically observed through the method of statistically validated\nnetworks. The memory mechanism is used to introduce a proxy of trust in the\nmodel. The key idea is that a lender, having lent many times to a borrower in\nthe past, is more likely to lend to that borrower again in the future than to\nother borrowers, with which the lender has never (or has in- frequently)\ninteracted. The core of the model depends on only one parameter representing\nthe initial attractiveness of all the banks as borrowers. Model outcomes and\nreal data are compared through a variety of measures that describe the\nstructure and properties of trading networks, including number of statistically\nvalidated links, bidirectional links, and 3-motifs. Refinements of the pairing\nmethod are also proposed, in order to capture finite memory and reciprocity in\nthe model. The model is implemented within the Mason framework in Java.\n"
    },
    {
        "paper_id": 1403.3756,
        "authors": "D.J. Manuge, P.T. Kim",
        "title": "A fast Fourier transform method for Mellin-type option pricing",
        "comments": "12 pages, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Analytical pricing formulas and Greeks are obtained for European and American\nbasket put options using Mellin transforms. We assume assets are driven by\ngeometric Brownian motion which exhibit correlation and pay a continuous\ndividend rate. A novel approach to numerical Mellin inversion is achieved via\nthe fast Fourier transform, enabling the computation of option values at\nequidistant log asset prices. Numerical accuracy is verified among existing\nmethods for American call options.\n"
    },
    {
        "paper_id": 1403.4069,
        "authors": "Tung-Lam Dao",
        "title": "Momentum Strategies with L1 Filter",
        "comments": "22 pages, 15 figures. Submitted to The Journal of Investment\n  Strategies, reference code: JOIS140227TD",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we discuss various implementation of L1 filtering in order\nto detect some properties of noisy signals. This filter consists of using a L1\npenalty condition in order to obtain the filtered signal composed by a set of\nstraight trends or steps. This penalty condition, which determines the number\nof breaks, is implemented in a constrained least square problem and is\nrepresented by a regularization parameter ? which is estimated by a\ncross-validation procedure. Financial time series are usually characterized by\na long-term trend (called the global trend) and some short-term trends (which\nare named local trends). A combination of these two time scales can form a\nsimple model describing the process of a global trend process with some\nmean-reverting properties. Explicit applications to momentum strategies are\nalso discussed in detail with appropriate uses of the trend configurations.\n"
    },
    {
        "paper_id": 1403.4099,
        "authors": "Dieter Hendricks, Diane Wilcox, Tim Gebbie",
        "title": "High-speed detection of emergent market clustering via an unsupervised\n  parallel genetic algorithm",
        "comments": "10 pages, 5 figures, 4 tables, More thorough discussion of\n  implementation",
        "journal-ref": "S Afr J Sci. 2016;112(1/2), Art. #2014-0340, 9 pages",
        "doi": "10.17159/sajs.2016/20140340",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We implement a master-slave parallel genetic algorithm (PGA) with a bespoke\nlog-likelihood fitness function to identify emergent clusters within price\nevolutions. We use graphics processing units (GPUs) to implement a PGA and\nvisualise the results using disjoint minimal spanning trees (MSTs). We\ndemonstrate that our GPU PGA, implemented on a commercially available general\npurpose GPU, is able to recover stock clusters in sub-second speed, based on a\nsubset of stocks in the South African market. This represents a pragmatic\nchoice for low-cost, scalable parallel computing and is significantly faster\nthan a prototype serial implementation in an optimised C-based\nfourth-generation programming language, although the results are not directly\ncomparable due to compiler differences. Combined with fast online intraday\ncorrelation matrix estimation from high frequency data for cluster\nidentification, the proposed implementation offers cost-effective,\nnear-real-time risk assessment for financial practitioners.\n"
    },
    {
        "paper_id": 1403.4111,
        "authors": "Fred Espen Benth and Paul Kr\\\"uhner",
        "title": "Representation of infinite dimensional forward price models in commodity\n  markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the forward price dynamics in commodity markets realized as a\nprocess with values in a Hilbert space of absolutely continuous functions\ndefined by Filipovi\\'c. The forward dynamics are defined as the mild solution\nof a certain stochastic partial differential equation driven by an infinite\ndimensional L\\'evy process. It is shown that the associated spot price dynamics\ncan be expressed as a sum of Ornstein-Uhlenbeck processes, or more generally,\nas a sum of certain stationary processes. These results link the possibly\ninfinite dimensional forward dynamics to classical commodity spot models. We\ncontinue with a detailed analysis of multiplication and integral operators on\nthe Hilbert spaces and show that Hilbert-Schmidt operators are essentially\nintegral operators. The covariance operator of the L\\'evy process driving the\nforward dynamics and the diffusion term can both be specified in terms of such\noperators, and we analyse in several examples the consequences on model\ndynamics and their probabilistic properties. Also, we represent the forward\nprice for contracts delivering over a period in terms of an integral operator,\na case being relevant for power and gas markets. In several examples we reduce\nour general model to existing commodity spot and forward dynamics.\n"
    },
    {
        "paper_id": 1403.4171,
        "authors": "Giuseppe arbia",
        "title": "Least quartic Regression Criterion with Application to Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article proposes a new method for the estimation of the parameters of a\nsimple linear regression model which accounts for the role of co-moments in\nnon-Gaussian distributions being based on the minimization of a quartic loss\nfunction. Although the proposed method is very general, we examine its\napplication to finance. In fact, in this field the contribution of the\nco-moments in explaining the return-generating process is of paramount\nimportance when evaluating the systematic risk of an asset within the framework\nof the Capital Asset Pricing Model (CAPM). The suggested new method contributes\nto this literature by showing that, in the presence of non-normality, the\nregression slope can be expressed as a function of the co-kurtosis between the\nreturns of a risky asset and the market proxy. The paper provides an\nillustration of the method based on some empirical financial data referring to\n40 industrial sector assets rates of the Italian stock market.\n"
    },
    {
        "paper_id": 1403.4291,
        "authors": "Philipp Arbenz, Mathieu Cambou and Marius Hofert",
        "title": "An importance sampling approach for copula models in insurance",
        "comments": "24 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An importance sampling approach for sampling copula models is introduced. We\npropose two algorithms that improve Monte Carlo estimators when the functional\nof interest depends mainly on the behaviour of the underlying random vector\nwhen at least one of the components is large. Such problems often arise from\ndependence models in finance and insurance. The importance sampling framework\nwe propose is general and can be easily implemented for all classes of copula\nmodels from which sampling is feasible. We show how the proposal distribution\nof the two algorithms can be optimized to reduce the sampling error. In a case\nstudy inspired by a typical multivariate insurance application, we obtain\nvariance reduction factors between 10 and 30 in comparison to standard Monte\nCarlo estimators.\n"
    },
    {
        "paper_id": 1403.4305,
        "authors": "Yaya Li, Yongli Li, Yulin Zhao, Fang Wang",
        "title": "Which factor dominates the industry evolution? A synergy analysis based\n  on China's ICT industry",
        "comments": "10 pages, 5 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Industry evolution caused by various reasons, among which technology progress\ndriving industry development has been approved, but with the new trend of\nindustry convergence, inter-industry convergence also plays an increasing\nimportant role. This paper plans to probe the industry synergetic evolution\nmechanism based on industry convergence and technology progress. Firstly, we\nuse self-organization method and Haken Model to establish synergetic evolution\nequations, select technology progress and industry convergence as the key\nvariables of industry evolution system; then use patent licensing data of\nchina's listed ICT companies to measure industry convergence rate and apply DEA\nMalmquist index method to calculate technology progress level; furthermore\napply simultaneous equation estimation method to investigate the synergetic\nindustry evolution process. From 2002 to 2012, China's ICT industry develops\nrapidly; it has the most obvious convergence and powerful technology progress\ncompared with other industries. We choose china's listed ICT industry to make\nempirical analysis. Our main findings are: a) technology progress is the order\nparameter which dominates industry system evolution. Moreover, industry\nconvergence is the control parameter which is influenced by technology\nprogress; b) Development of technology progress is the core factor for causing\nevolution of industry system, and industry convergence is the outcome of\ntechnology progress; c) Especially, it is important that the dominated role of\ntechnology progress will be sustained, even though in the environment of\nconvergence, companies also need focus on self-innovation, rather than only\nadapt to the new industry evolution trend.\n"
    },
    {
        "paper_id": 1403.4329,
        "authors": "Alexandra Rodkina and Nikolai Dokuchaev",
        "title": "On asymptotic optimality of Merton's myopic portfolio strategies for\n  discrete time market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the properties of discrete time stochastic optimal control\nproblems associated with portfolio selection. We investigate if optimal\ncontinuous time strategies can be used effectively for a discrete time market\nafter a straightforward discretization. We found that Merton's strategy\napproximates the performance of the optimal strategy in a discrete time model\nwith the sufficiently small time steps\n"
    },
    {
        "paper_id": 1403.446,
        "authors": "Tiziano Squartini, Diego Garlaschelli",
        "title": "Stationarity, non-stationarity and early warning signals in economic\n  networks",
        "comments": "12 pages, 9 figures. Extended version of the paper \"Economic networks\n  in and out of equilibrium\" (arXiv:1309.1875)",
        "journal-ref": "J. Complex Netw. 3 (1), 1-21 (2015)",
        "doi": "10.1093/comnet/cnu012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic integration, globalization and financial crises represent examples\nof processes whose understanding requires the analysis of the underlying\nnetwork structure. Of particular interest is establishing whether a real\neconomic network is in a state of (quasi)stationary equilibrium, i.e.\ncharacterized by smooth structural changes rather than abrupt transitions.\nWhile in the former case the behaviour of the system can be reasonably\ncontrolled and predicted, in the latter case this is generally impossible.\nHere, we propose a method to assess whether a real economic network is in a\nquasi-stationary state by checking the consistency of its structural evolution\nwith appropriate quasi-equilibrium maximum-entropy ensembles of graphs. As\nillustrative examples, we consider the International Trade Network (ITN) and\nthe Dutch Interbank Network (DIN). We find that the ITN is an almost perfect\nexample of quasi-equilibrium network, while the DIN is clearly\nout-of-equilibrium. In the latter, the entity of the deviation from\nquasi-stationarity contains precious information that allows us to identify\nremarkable early warning signals of the interbank crisis of 2008. These early\nwarning signals involve certain dyadic and triadic topological properties,\nincluding dangerous 'debt loops' with different levels of interbank\nreciprocity.\n"
    },
    {
        "paper_id": 1403.5179,
        "authors": "Thomas Bury",
        "title": "Collective behaviours in the stock market -- A maximum entropy approach",
        "comments": "146 pages, PhD Thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scale invariance, collective behaviours and structural reorganization are\ncrucial for portfolio management (portfolio composition, hedging, alternative\ndefinition of risk, etc.). This lack of any characteristic scale and such\nelaborated behaviours find their origin in the theory of complex systems. There\nare several mechanisms which generate scale invariance but maximum entropy\nmodels are able to explain both scale invariance and collective behaviours. The\nstudy of the structure and collective modes of financial markets attracts more\nand more attention. It has been shown that some agent based models are able to\nreproduce some stylized facts. Despite their partial success, there is still\nthe problem of rules design. In this work, we used a statistical inverse\napproach to model the structure and co-movements in financial markets. Inverse\nmodels restrict the number of assumptions. We found that a pairwise maximum\nentropy model is consistent with the data and is able to describe the complex\nstructure of financial systems. We considered the existence of a critical state\nwhich is linked to how the market processes information, how it responds to\nexogenous inputs and how its structure changes. The considered data sets did\nnot reveal a persistent critical state but rather oscillations between order\nand disorder. In this framework, we also showed that the collective modes are\nmostly dominated by pairwise co-movements and that univariate models are not\ngood candidates to model crashes. The analysis also suggests a genuine adaptive\nprocess since both the maximum variance of the log-likelihood and the accuracy\nof the predictive scheme vary through time. This approach may provide some clue\nto crash precursors and may provide highlights on how a shock spreads in a\nfinancial network and if it will lead to a crash. The natural continuation of\nthe present work could be the study of such a mechanism.\n"
    },
    {
        "paper_id": 1403.5193,
        "authors": "Zeyu Zheng, Zhi Qiao, Joel N. Tenenbaum, H. Eugene Stanley and Baowen\n  Li",
        "title": "Predicting market instability: New dynamics between volume and\n  volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Econophysics and econometrics agree that there is a correlation between\nvolume and volatility in a time series. Using empirical data and their\ndistributions, we further investigate this correlation and discover new ways\nthat volatility and volume interact, particularly when the levels of both are\nhigh. We find that the distribution of the volume-conditional volatility is\nwell fit by a power-law function with an exponential cutoff. We find that the\nvolume-conditional volatility distribution scales with volume, and collapses\nthese distributions to a single curve. We exploit the characteristics of the\nvolume-volatility scatter plot to find a strong correlation between logarithmic\nvolume and a quantity we define as local maximum volatility (LMV), which\nindicates the largest volatility observed in a given range of trading volumes.\nThis finding supports our empirical analysis showing that volume is an\nexcellent predictor of the maximum value of volatility for both same-day and\nnear-future time periods. We also use a joint conditional probability that\nincludes both volatility and volume to demonstrate that invoking both allows us\nto better predict the largest next-day volatility than invoking either one\nalone.\n"
    },
    {
        "paper_id": 1403.5227,
        "authors": "Stephen J. Hardiman and Jean-Philippe Bouchaud",
        "title": "Branching ratio approximation for the self-exciting Hawkes process",
        "comments": "7 pages, 7 figures",
        "journal-ref": "Phys. Rev. E 90, 062807 (2014)",
        "doi": "10.1103/PhysRevE.90.062807",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model-independent approximation for the branching ratio of\nHawkes self-exciting point processes. Our estimator requires knowing only the\nmean and variance of the event count in a sufficiently large time window,\nstatistics that are readily obtained from empirical data. The method we propose\ngreatly simplifies the estimation of the Hawkes branching ratio, recently\nproposed as a proxy for market endogeneity and formerly estimated using\nnumerical likelihood maximisation. We employ our new method to support recent\ntheoretical and experimental results indicating that the best fitting Hawkes\nmodel to describe S&P futures price changes is in fact critical (now and in the\nrecent past) in light of the long memory of financial market activity.\n"
    },
    {
        "paper_id": 1403.5236,
        "authors": "Fred Espen Benth and Salvador Ortiz-Latorre",
        "title": "A change of measure preserving the affine structure in the BNS model for\n  commodity markets",
        "comments": "30 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1308.3378",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a commodity spot price dynamics given by an Ornstein-Uhlenbeck process\nwith Barndorff-Nielsen and Shephard stochastic volatility, we price forwards\nusing a class of pricing measures that simultaneously allow for change of level\nand speed in the mean reversion of both the price and the volatility. The risk\npremium is derived in the case of arithmetic and geometric spot price\nprocesses, and it is demonstrated that we can provide flexible shapes that is\ntypically observed in energy markets. In particular, our pricing measure\npreserves the affine model structure and decomposes into a price and volatility\nrisk premium, and in the geometric spot price model we need to resort to a\ndetailed analysis of a system of Riccati equations, for which we show existence\nand uniqueness of solution and asymptotic properties that explains the possible\nrisk premium profiles. Among the typical shapes, the risk premium allows for a\nstochastic change of sign, and can attain positive values in the short end of\nthe forward market and negative in the long end.\n"
    },
    {
        "paper_id": 1403.5247,
        "authors": "Marcos Escobar, Daniela Neykova, Rudi Zagst",
        "title": "Portfolio Optimization in Affine Models with Markov Switching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic factor financial model where the asset price process\nand the process for the stochastic factor depend on an observable Markov chain\nand exhibit an affine structure. We are faced with a finite time investment\nhorizon and derive optimal dynamic investment strategies that maximize the\ninvestor's expected utility from terminal wealth. To this aim we apply Merton's\napproach, as we are dealing with an incomplete market. Based on the\nsemimartingale characterization of Markov chains we first derive the HJB\nequations, which in our case correspond to a system of coupled non-linear PDEs.\nExploiting the affine structure of the model, we derive simple expressions for\nthe solution in the case with no leverage, i.e. no correlation between the\nBrownian motions driving the asset price and the stochastic factor. In the\npresence of leverage we propose a separable ansatz, which leads to explicit\nsolutions in this case as well. General verification results are also proved.\nThe results are illustrated for the special case of a Markov modulated Heston\nmodel.\n"
    },
    {
        "paper_id": 1403.5302,
        "authors": "Archil Gulisashvili and Josep Vives",
        "title": "Asymptotic analysis of stock price densities and implied volatilities in\n  mixed stochastic models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we obtain sharp asymptotic formulas with error estimates for\nthe Mellin convolution of functions, and use these formulas to characterize the\nasymptotic behavior of marginal distribution densities of stock price processes\nin mixed stochastic models. Special examples of mixed models are jump-diffusion\nmodels and stochastic volatility models with jumps. We apply our general\nresults to the Heston model with double exponential jumps, and make a detailed\nanalysis of the asymptotic behavior of the stock price density, the call option\npricing function, and the implied volatility in this model. We also obtain\nsimilar results for the Heston model with jumps distributed according to the\nNIG law.\n"
    },
    {
        "paper_id": 1403.5309,
        "authors": "Mike Giles, Yuan Xia",
        "title": "Multilevel Monte Carlo For Exponential L\\'{e}vy Models",
        "comments": "32 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply multilevel Monte Carlo for option pricing problems using exponential\nL\\'{e}vy models with a uniform timestep discretisation to monitor the running\nmaximum required for lookback and barrier options. The numerical results\ndemonstrate the computational efficiency of this approach. We derive estimates\nof the convergence rate for the error introduced by the discrete monitoring of\nthe running supremum of a broad class of L\\'{e}vy processes. We use these to\nobtain upper bounds on the multilevel Monte Carlo variance convergence rate for\nthe Variance Gamma, NIG and $\\alpha$-stable processes used in the numerical\nexperiments. We also show numerical results and analysis of a trapezoidal\napproximation for Asian options.\n"
    },
    {
        "paper_id": 1403.5402,
        "authors": "Rafael Mendoza-Arriaga, Vadim Linetsky",
        "title": "Time-changed CIR default intensities with two-sided mean-reverting jumps",
        "comments": "Published in at http://dx.doi.org/10.1214/13-AAP936 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2014, Vol. 24, No. 2, 811-856",
        "doi": "10.1214/13-AAP936",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper introduces a jump-diffusion extension of the classical\ndiffusion default intensity model by means of subordination in the sense of\nBochner. We start from the bi-variate process $(X,D)$ of a diffusion state\nvariable $X$ driving default intensity and a default indicator process $D$ and\ntime change it with a L\\'{e}vy subordinator ${\\mathcal{T}}$. We characterize\nthe time-changed process\n$(X^{\\phi}_t,D^{\\phi}_t)=(X({\\mathcal{T}}_t),D({\\mathcal{T}}_t))$ as a\nMarkovian--It\\^{o} semimartingale and show from the Doob--Meyer decomposition\nof $D^{\\phi}$ that the default time in the time-changed model has a\njump-diffusion or a pure jump intensity. When $X$ is a CIR diffusion with\nmean-reverting drift, the default intensity of the subordinate model (SubCIR)\nis a jump-diffusion or a pure jump process with mean-reverting jumps in both\ndirections that stays nonnegative. The SubCIR default intensity model is\nanalytically tractable by means of explicitly computed eigenfunction expansions\nof relevant semigroups, yielding closed-form pricing of credit-sensitive\nsecurities.\n"
    },
    {
        "paper_id": 1403.5599,
        "authors": "Nguyet Nguyen and Giray \\\"Okten",
        "title": "The acceptance-rejection method for low-discrepancy sequences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generation of pseudorandom numbers from different probability distributions\nhas been studied extensively in the Monte Carlo simulation literature. Two\nstandard generation techniques are the acceptance-rejection and inverse\ntransformation methods. An alternative approach to Monte Carlo simulation is\nthe quasi-Monte Carlo method, which uses low-discrepancy sequences, instead of\npseudorandom numbers, in simulation. Low-discrepancy sequences from different\ndistributions can be obtained by the inverse transformation method, just like\nfor pseudorandom numbers. In this paper, we will present an\nacceptance-rejection algorithm for low-discrepancy sequences. We will prove a\nconvergence result, and present error bounds. We will then use this\nacceptance-rejection algorithm to develop quasi-Monte Carlo versions of some\nwell known algorithms to generate beta and gamma distributions, and investigate\nthe efficiency of these algorithms numerically. We will also consider the\nsimulation of the variance gamma model, a model used in computational finance,\nwhere the generation of these probability distributions are needed. Our results\nshow that the acceptance-rejection technique can result in significant\nimprovements in computing time over the inverse transformation method in the\ncontext of low-discrepancy sequences.\n"
    },
    {
        "paper_id": 1403.5623,
        "authors": "B. Podobnik, D. Horvatic, M. Bertella, L. Feng, X. Huang, and B. Li",
        "title": "Systemic risk in dynamical networks with stochastic failure criterion",
        "comments": "7 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1209/0295-5075/106/68003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex non-linear interactions between banks and assets we model by two\ntime-dependent Erd\\H{o}s Renyi network models where each node, representing\nbank, can invest either to a single asset (model I) or multiple assets (model\nII). We use dynamical network approach to evaluate the collective financial\nfailure---systemic risk---quantified by the fraction of active nodes. The\nsystemic risk can be calculated over any future time period, divided on\nsub-periods, where within each sub-period banks may contiguously fail due to\nlinks to either (i) assets or (ii) other banks, controlled by two parameters,\nprobability of internal failure $p$ and threshold $T_h$ (\"solvency\" parameter).\nThe systemic risk non-linearly increases with $p$ and decreases with average\nnetwork degree faster when all assets are equally distributed across banks than\nif assets are randomly distributed. The more inactive banks each bank can\nsustain (smaller $T_h$), the smaller the systemic risk---for some $T_h$ values\nin I we report a discontinuity in systemic risk. When contiguous spreading\nbecomes stochastic (ii) controlled by probability $p_2$---a condition for the\nbank to be solvent (active) is stochastic---the systemic risk decreases with\ndecreasing $p_2$. We analyse asset allocation for the U.S. banks.\n"
    },
    {
        "paper_id": 1403.5685,
        "authors": "Alexander Alvarez and Sebastian Ferrando",
        "title": "Trajectory Based Models, Arbitrage and Continuity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper develops no arbitrage results for trajectory based models by\nimposing general constraints on the trading portfolios. The main condition\nimposed, in order to avoid arbitrage opportunities, is a local continuity\nrequirement on the final portfolio value considered as a functional on the\ntrajectory space. The paper shows this to be a natural requirement by proving\nthat a large class of practical trading strategies, defined by means of\ntrajectory based stopping times, give rise to locally continuous functionals.\nThe theory is illustrated, with some detail, for two specific trajectory models\nof practical interest. The implications for stochastic models which are not\nsemimartingales are described. The present paper extends some of the results in\n[1] by incorporating in the formalism a larger set of trading portfolios.\n"
    },
    {
        "paper_id": 1403.5833,
        "authors": "Salil Mehta",
        "title": "Sophisticated gamblers ruin and survival chances",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note explores the mathematical theory to solve modern gamblers ruin\nproblems. We establish a ruin framework and solve for the probability of\nbankruptcy. We also show how this relates to the expected time to bankruptcy\nand review the risk neutral probabilities associated an adjustment to\nasymmetrical views.\n"
    },
    {
        "paper_id": 1403.5965,
        "authors": "Romuald N. Kenmoe S and Carine D. Tafou",
        "title": "The Implied Volatility Analysis: The South African Experience",
        "comments": "14,2",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyse the South African implied volatility in various\nsetting. We assess the information content in SAVI implied volatility using\ndaily markets data. Our empirical application is focused on the FTSE/JSE Top 40\nindex and we emphasize our models performance in distinct sub-periods. Our\nresults are compared with VIX/VXN and S&P 500/NASDAQ 100 data in some points\nwhich are taken as our benchmark. We find a significant negative relationship\nbetween returns and volatility, in line with the results found in other\nmarkets. Finally, the link between SAVI, VIX and VXN are undertaken to examine\nthe equity market transmission with respect to uncertainty.\n"
    },
    {
        "paper_id": 1403.6093,
        "authors": "Jaehyung Choi, Young Shin Kim, Ivan Mitov",
        "title": "Reward-risk momentum strategies using classical tempered stable\n  distribution",
        "comments": "38 pages, 6 subfigures, Published version",
        "journal-ref": "Journal of Banking and Finance 58 (2015), pp. 194-213",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We implement momentum strategies using reward-risk measures as ranking\ncriteria based on classical tempered stable distribution. Performances and risk\ncharacteristics for the alternative portfolios are obtained in various asset\nclasses and markets. The reward-risk momentum strategies with lower volatility\nlevels outperform the traditional momentum strategy regardless of asset class\nand market. Additionally, the alternative portfolios are not only less riskier\nin risk measures such as VaR, CVaR and maximum drawdown but also characterized\nby thinner downside tails. Similar patterns in performance and risk profile are\nalso found at the level of each ranking basket in the reward-risk portfolios.\nHigher factor-neutral returns achieved by the reward-risk momentum strategies\nare statistically significant and large portions of the performances are not\nexplained by the Carhart four-factor model.\n"
    },
    {
        "paper_id": 1403.6112,
        "authors": "Dominique Pepin (CRIEF)",
        "title": "The role of the \"Maximizing Output Growth Inflation Rate\" in monetary\n  policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper discusses the role of monetary policy when potential output depends\non the inflation rate. If the intention of the central bank is to maximize\nactual output growth, then it has to be credibly committed to a strict\ninflation targeting rule, and to take the MOGIR (the Maximizing Output Growth\nInflation Rate) as the target.\n"
    },
    {
        "paper_id": 1403.6175,
        "authors": "Oleksii Mostovyi",
        "title": "Utility maximization in the large markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the large financial market, which is described by a model with countably\nmany traded assets, we formulate the problem of the expected utility\nmaximization. Assuming that the preferences of an economic agent are modeled\nwith a stochastic utility and that the consumption occurs according to a\nstochastic clock, we obtain the \"usual\" conclusions of the utility maximization\ntheory. We also give a characterization of the value function in the large\nmarket in terms of a sequence of the value functions in the finite-dimensional\nmodels.\n"
    },
    {
        "paper_id": 1403.6342,
        "authors": "Benedikt Fuchs and Stefan Thurner",
        "title": "Behavioral and Network Origins of Wealth Inequality: Insights from a\n  Virtual World",
        "comments": "22 pages, 8 figures, 8 pages SI",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0103503",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Almost universally, wealth is not distributed uniformly within societies or\neconomies. Even though wealth data have been collected in various forms for\ncenturies, the origins for the observed wealth-disparity and social inequality\nare not yet fully understood. Especially the impact and connections of human\nbehavior on wealth could so far not be inferred from data. Here we study wealth\ndata from the virtual economy of the massive multiplayer online game (MMOG)\nPardus. This data not only contains every player's wealth at every point in\ntime, but also all actions of every player over a timespan of almost a decade.\nWe find that wealth distributions in the virtual world are very similar to\nthose in western countries. In particular we find an approximate exponential\nfor low wealth and a power-law tail. The Gini index is found to be $g=0.65$,\nwhich is close to the indices of many Western countries. We find that\nwealth-increase rates depend on the time when players entered the game. Players\nthat entered the game early on tend to have remarkably higher wealth-increase\nrates than those who joined later. Studying the players' positions within their\nsocial networks, we find that the local position in the trade network is most\nrelevant for wealth. Wealthy people have high in- and out-degree in the trade\nnetwork, relatively low nearest-neighbor degree and a low clustering\ncoefficient. Wealthy players have many mutual friendships and are socially well\nrespected by others, but spend more time on business than on socializing. We\nfind that players that are not organized within social groups with at least\nthree members are significantly poorer on average. We observe that high\n`political' status and high wealth go hand in hand. Wealthy players have few\npersonal enemies, but show animosity towards players that behave as public\nenemies.\n"
    },
    {
        "paper_id": 1403.6378,
        "authors": "Stefan Bornholdt, Kim Sneppen",
        "title": "Do Bitcoins make the world go round? On the dynamics of competing\n  crypto-currencies",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoins have emerged as a possible competitor to usual currencies, but other\ncrypto-currencies have likewise appeared as competitors to the Bitcoin\ncurrency. The expanding market of crypto-currencies now involves capital\nequivalent to $10^{10}$ US Dollars, providing academia with an unusual\nopportunity to study the emergence of value. Here we show that the Bitcoin\ncurrency in itself is not special, but may rather be understood as the\ncontemporary dominating crypto-currency that may well be replaced by other\ncurrencies. We suggest that perception of value in a social system is generated\nby a voter-like dynamics, where fashions form and disperse even in the case\nwhere information is only exchanged on a pairwise basis between agents.\n"
    },
    {
        "paper_id": 1403.6531,
        "authors": "Karol Przanowski",
        "title": "Credit acceptance process strategy case studies - the power of Credit\n  Scoring",
        "comments": "38 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  The paper is aware of the importance of certain figures that are essential to\nan understanding of Credit Scoring models in credit acceptance process\noptimization, namely if the power of discrimination measured by Gini value is\nincreased by 5% then the profit of the process can be increased monthly by\nabout 1 500 kPLN (300 kGBP, 500 kUSD, 350 kEUR). Simple business models of\ncredit loans are also presented: acquisition - installment loan (low price) and\ncross-sell - cash loans (high price). Scoring models are used to optimize\nprocess, to become profitable. Various acceptance strategies with different\ncutoffs are presented, some are profitable and some are not. Moreover, in a\ntime of prosperity some are preferable whilst the inverse is true during a\nperiod of high risk or crisis. To optimize the process four models are\nemployed: three risk models, to predict the probability of default and one\ntypical propensity model to predict the probability of response. It is a simple\nbut very important example of the Customer Lifetime Value (CLTV or CLV) model\nbusiness, where risk and response models are working together to become a\nprofitable process.\n"
    },
    {
        "paper_id": 1403.7021,
        "authors": "Bradly Alicea",
        "title": "Contextual and Structural Representations of Market-mediated Economic\n  Value",
        "comments": "15 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do we assign value to economic transactions? To answer this question, we\nmust consider whether the value of objects is inherent, is a product of social\ninteraction, or involves other mechanisms. Economic theory predicts that there\nis an optimal price for any market transaction, and can be observed during\nauctions or other bidding processes. However, there are also social, cultural,\nand cognitive components to the assignation of value, which can be observed in\nboth human and non-human Primate societies. While behaviors related to these\nfactors are embedded in market interactions, they also involve a biological\nsubstrate for the assignation of value (valuation). To synthesize this\ndiversity of perspectives, we will propose that the process of valuation can be\nmodeled computationally and conceived of as a set of interrelated cultural\nevolutionary, cognitive, and neural processes. To do this, contextual geometric\nstructures (CGS) will be placed in an agent-based context (minimal and\ncompositional markets). Objects in the form of computational propositions can\nbe acquired and exchanged, which will determine the value of both singletons\nand linked propositions. Expected results of this model will be evaluated in\nterms of their contribution to understanding human economic phenomena. The\npaper will focus on computational representations and how they correspond to\nreal-world concepts. The implications for evolutionary economics and our\ncontemporary understanding of valuation and market dynamics will also be\ndiscussed.\n"
    },
    {
        "paper_id": 1403.7179,
        "authors": "Menelaos Karanasos, Alexandros Paraskevopoulos, Faek Menla Ali,\n  Michail Karoglou, Stavroula Yfanti",
        "title": "Modelling Returns and Volatilities During Financial Crises: a Time\n  Varying Coefficient Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine how the most prevalent stochastic properties of key financial time\nseries have been affected during the recent financial crises. In particular we\nfocus on changes associated with the remarkable economic events of the last two\ndecades in the mean and volatility dynamics, including the underlying\nvolatility persistence and volatility spillovers structure. Using daily data\nfrom several key stock market indices we find that stock market returns exhibit\ntime varying persistence in their corresponding conditional variances.\nFurthermore, the results of our bivariate GARCH models show the existence of\ntime varying correlations as well as time varying shock and volatility\nspillovers between the returns of FTSE and DAX, and those of NIKKEI and Hang\nSeng, which became more prominent during the recent financial crisis. Our\ntheoretical considerations on the time varying model which provides the\nplatform upon which we integrate our multifaceted empirical approaches are also\nof independent interest. In particular, we provide the general solution for low\norder time varying specifications, which is a long standing research topic.\nThis enables us to characterize these models by deriving, first, their\nmultistep ahead predictors, second, the first two time varying unconditional\nmoments, and third, their covariance structure.\n"
    },
    {
        "paper_id": 1403.7269,
        "authors": "Zuo Quan Xu",
        "title": "A Note on the Quantile Formulation",
        "comments": "to appear in Mathematical Finance",
        "journal-ref": "Mathematical Finance, Vol.26, No. 3 (2016), 589-601",
        "doi": "10.1111/mafi.12072",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many investment models in discrete or continuous-time settings boil down to\nmaximizing an objective of the quantile function of the decision variable. This\nquantile optimization problem is known as the quantile formulation of the\noriginal investment problem. Under certain monotonicity assumptions, several\nschemes to solve such quantile optimization problems have been proposed in the\nliterature. In this paper, we propose a change-of-variable and relaxation\nmethod to solve the quantile optimization problems without using the calculus\nof variations or making any monotonicity assumptions. The method is\ndemonstrated through a portfolio choice problem under rank-dependent utility\ntheory (RDUT). We show that this problem is equivalent to a classical Merton's\nportfolio choice problem under expected utility theory with the same utility\nfunction but a different pricing kernel explicitly determined by the given\npricing kernel and probability weighting function. With this result, the\nfeasibility, well-posedness, attainability and uniqueness issues for the\nportfolio choice problem under RDUT are solved. It is also shown that solving\nfunctional optimization problems may reduce to solving probabilistic\noptimization problems. The method is applicable to general models with\nlaw-invariant preference measures including portfolio choice models under\ncumulative prospect theory (CPT) or RDUT, Yaari's dual model, Lopes' SP/A\nmodel, and optimal stopping models under CPT or RDUT.\n"
    },
    {
        "paper_id": 1403.7628,
        "authors": "Thomas Conlon, John Cotter",
        "title": "Anatomy of a Bail-In",
        "comments": null,
        "journal-ref": "Journal of Financial Stability, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To mitigate potential contagion from future banking crises, the European\nCommission recently proposed a framework which would provide for the\n$\\textit{bail-in}$ of bank creditors in the event of failure. In this study, we\nexamine this framework retrospectively in the context of failed European banks\nduring the global financial crisis. Empirical findings suggest that equity and\nsubordinated bond holders would have been the main losers from the 535 billion\neuro impairment losses realized by failed European banks. Losses attributed to\nsenior debt holders would, on aggregate, have been proportionally small, while\nno losses would have been imposed on depositors. Cross-country analysis,\nincorporating stress-tests, reveals a divergence of outcomes with subordinated\ndebt holders wiped out in a number of countries, while senior debt holders of\nGreek, Austrian and Irish banks would have required bail-in.\n"
    },
    {
        "paper_id": 1403.768,
        "authors": "Zhenyu Cui",
        "title": "Omega risk model with tax",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  In this paper we study the Omega risk model with surplus-dependent tax\npayments in a time-homogeneous diffusion setting. The new model incorporates\npractical features from both the Omega risk model(Albrecher and Gerber and Shiu\n(2011)) and the risk model with tax(Albrecher and Hipp (2007)). We explicitly\ncharacterize the Laplace transform of the occupation time of an Azema-Yor\nprocess(e.g. a process refracted by functionals of its running maximum) below a\nconstant level until the first hitting time of another Azema-Yor process or\nuntil an independent exponential time. This result unifies and extends recent\nliterature(Li and Zhou (2013) and Zhang (2014)) incorporating some of their\nresults as special cases. We explicitly characterize the Laplace transform of\nthe time of bankruptcy in the Omega risk model with tax and discuss an\nextension to integral functionals. Finally we present examples using a Brownian\nmotion with drift.\n"
    },
    {
        "paper_id": 1403.7799,
        "authors": "Gabriele Sarais and Damiano Brigo",
        "title": "Inflation securities valuation with macroeconomic-based no-arbitrage\n  dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model to price inflation and interest rates derivatives using\ncontinuous-time dynamics that have some links with macroeconomic monetary DSGE\nmodels equipped with a Taylor rule: in particular, the reaction function of the\ncentral bank, the bond market liquidity, inflation and growth expectations play\nan important role. The model can explain the effects of non-standard monetary\npolicies (like quantitative easing or its tapering) and shed light on how\ncentral bank policy can affect the value of inflation and interest rates\nderivatives.\n  The model is built under standard no-arbitrage assumptions. Interestingly,\nthe model yields short rate dynamics that are consistent with a time-varying\nHull-White model, therefore making the calibration to the nominal interest\ncurve and options straightforward. Further, we obtain closed forms for both\nzero-coupon and year-on-year inflation swap and options. The calibration\nstrategy we propose is fully separable, which means that the calibration can be\ncarried out in subsequent simple steps that do not require heavy computation. A\nmarket calibration example is provided.\n  The advantages of such structural inflation modelling become apparent when\none starts doing risk analysis on an inflation derivatives book: because the\nmodel explicitly takes into account economic variables, a trader can easily\nassess the impact of a change in central bank policy on a complex book of fixed\nincome instruments, which is normally not straightforward if one is using\nstandard inflation pricing models.\n"
    },
    {
        "paper_id": 1403.78,
        "authors": "Pierre Degond, Jian-Guo Liu, Christian Ringhofer",
        "title": "Evolution of wealth in a nonconservative economy driven by local Nash\n  equilibria",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1098/rsta.2013.0394",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model for the evolution of wealth in a non-conservative economic\nenvironment, extending a theory developed earlier by the authors. The model\nconsiders a system of rational agents interacting in a game theoretical\nframework. This evolution drives the dynamic of the agents in both wealth and\neconomic configuration variables. The cost function is chosen to represent a\nrisk averse strategy of each agent. That is, the agent is more likely to\ninteract with the market, the more predictable the market, and therefore the\nsmaller its individual risk. This yields a kinetic equation for an effective\nsingle particle agent density with a Nash equilibrium serving as the local\nthermodynamic equilibrium. We consider a regime of scale separation where the\nlarge scale dynamics is given by a hydrodynamic closure with this local\nequilibrium. A class of generalized collision invariants (GCIs) is developed to\novercome the difficulty of the non-conservative property in the hydrodynamic\nclosure derivation of the large scale dynamics for the evolution of wealth\ndistribution. The result is a system of gas dynamics-type equations for the\ndensity and average wealth of the agents on large scales. We recover the\ninverse Gamma distribution, which has been previously considered in the\nliterature, as a local equilibrium for particular choices of the cost function.\n"
    },
    {
        "paper_id": 1403.783,
        "authors": "Vicky Henderson, Gechun Liang",
        "title": "Pseudo Linear Pricing Rule for Utility Indifference Valuation",
        "comments": "21 pages. arXiv admin note: text overlap with arXiv:1111.3856",
        "journal-ref": "Finance and Stochastics, 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers exponential utility indifference pricing for a\nmultidimensional non-traded assets model, and provides two linear\napproximations for the utility indifference price. The key tool is a\nprobabilistic representation for the utility indifference price by the solution\nof a functional differential equation, which is termed \\emph{pseudo linear\npricing rule}. We also provide an alternative derivation of the quadratic BSDE\nrepresentation for the utility indifference price.\n"
    },
    {
        "paper_id": 1403.8018,
        "authors": "Pedro Lencastre, Frank Raischel, Pedro G. Lind, Tim Rogers",
        "title": "Are credit ratings time-homogeneous and Markov?",
        "comments": "11 pages, Fig 5, for 2014 conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a simple approach for testing the reliability of homogeneous\ngenerators and the Markov property of the stochastic processes underlying\nempirical time series of credit ratings. We analyze open access data provided\nby Moody's and show that the validity of these assumptions - existence of a\nhomogeneous generator and Markovianity - is not always guaranteed. Our analysis\nis based on a comparison between empirical transition matrices aggregated over\nfixed time windows and candidate transition matrices generated from\nmeasurements taken over shorter periods. Ratings are widely used in credit\nrisk, and are a key element in risk assessment; our results provide a tool for\nquantifying confidence in predictions extrapolated from these time series.\n"
    },
    {
        "paper_id": 1403.8125,
        "authors": "Jaehyung Choi",
        "title": "Maximum drawdown, recovery, and momentum",
        "comments": "29 pages, 6 subfigures; minor revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We empirically test predictability on asset price by using stock selection\nrules based on maximum drawdown and its consecutive recovery. In various equity\nmarkets, monthly momentum- and weekly contrarian-style portfolios constructed\nfrom these alternative selection criteria are superior not only in forecasting\ndirections of asset prices but also in capturing cross-sectional return\ndifferentials. In monthly periods, the alternative portfolios ranked by maximum\ndrawdown measures exhibit outperformance over other alternative momentum\nportfolios including traditional cumulative return-based momentum portfolios.\nIn weekly time scales, recovery-related stock selection rules are the best\nranking criteria for detecting mean-reversion. For the alternative portfolios\nand their ranking baskets, improved risk profiles in various reward-risk\nmeasures also imply more consistent prediction on the direction of assets in\nfuture. In the Carhart four-factor analysis, higher factor-neutral intercepts\nfor the alternative strategies are another evidence for the robust prediction\nby the alternative stock selection rules.\n"
    },
    {
        "paper_id": 1404.0243,
        "authors": "D. Sornette (ETH Zurich)",
        "title": "Physics and Financial Economics (1776-2014): Puzzles, Ising and\n  Agent-Based models",
        "comments": "76 pages, in press in Reports on Progress in Physics (2014)",
        "journal-ref": "Rep. Prog. Phys. 77, 062001 (2014)",
        "doi": "10.1088/0034-4885/77/6/062001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This short review presents a selected history of the mutual fertilization\nbetween physics and economics, from Isaac Newton and Adam Smith to the present.\nThe fundamentally different perspectives embraced in theories developed in\nfinancial economics compared with physics are dissected with the examples of\nthe volatility smile and of the excess volatility puzzle. The role of the Ising\nmodel of phase transitions to model social and financial systems is reviewed,\nwith the concepts of random utilities and the logit model as the analog of the\nBoltzmann factor in statistic physics. Recent extensions in term of quantum\ndecision theory are also covered. A wealth of models are discussed briefly that\nbuild on the Ising model and generalize it to account for the many stylized\nfacts of financial markets. A summary of the relevance of the Ising model and\nits extensions is provided to account for financial bubbles and crashes. The\nreview would be incomplete if it would not cover the dynamical field of agent\nbased models (ABMs), also known as computational economic models, of which the\nIsing-type models are just special ABM implementations. We formulate the\n``Emerging Market Intelligence hypothesis'' to reconcile the pervasive presence\nof ``noise traders'' with the near efficiency of financial markets. Finally, we\nnote that evolutionary biology, more than physics, is now playing a growing\nrole to inspire models of financial markets.\n"
    },
    {
        "paper_id": 1404.034,
        "authors": "Areski Cousin (SAF), Ibrahima Niang (SAF)",
        "title": "On the range of admissible term-structures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze the diversity of term structure functions (e.g.,\nyield curves, swap curves, credit curves) constructed in a process which\ncomplies with some admissible properties: arbitrage-freeness, ability to fit\nmarket quotes and a certain degree of smooth- ness. When present values of\nbuilding instruments are expressed as linear combinations of some primary\nquantities such as zero-coupon bonds, discount factor, or survival probabilit-\nies, arbitrage-free bounds can be derived for those quantities at the most\nliquid maturities. As a matter of example, we present an iterative procedure\nthat allows to compute model-free bounds for OIS-implied discount rates and\nCDS-implied default probabilities. We then show how mean-reverting term\nstructure models can be used as generators of admissible curves. This framework\nis based on a particular specification of the mean-reverting level which al-\nlows to perfectly reproduce market quotes of standard vanilla interest-rate and\ndefault-risky securities while preserving a certain degree of smoothness. The\nnumerical results suggest that, for both OIS discounting curves and CDS credit\ncurves, the operational task of term- structure construction may be associated\nwith a significant degree of uncertainty.\n"
    },
    {
        "paper_id": 1404.0375,
        "authors": "Vitor V. Lopes, Teresa Scholz, Frank Raischel, Pedro G. Lind",
        "title": "Principal wind turbines for a conditional portfolio approach to wind\n  farms",
        "comments": "9 pages, conference proceedings of \"The science of making torque from\n  wind\"",
        "journal-ref": null,
        "doi": "10.1088/1742-6596/524/1/012183",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a measure for estimating the best risk-return relation of power\nproduction in wind farms within a given time-lag, conditioned to the velocity\nfield. The velocity field is represented by a scalar that weighs the influence\nof the velocity at each wind turbine at present and previous time-steps for the\npresent \"state\" of the wind field. The scalar measure introduced is a linear\ncombination of the few turbines, that most influence the overall power\nproduction. This quantity is then used as the condition for computing a\nconditional expected return and corresponding risk associated to the future\ntotal power output.\n"
    },
    {
        "paper_id": 1404.041,
        "authors": "Tahir Choulli, Anna Aksamit, Jun Deng and Monique Jeanblanc",
        "title": "Non-Arbitrage under a Class of Honest Times",
        "comments": "31 pages. arXiv admin note: text overlap with arXiv:1310.1142",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper quantifies the interplay between the non-arbitrage notion of\nNo-Unbounded-Profit-with-Bounded-Risk (NUPBR hereafter) and additional\ninformation generated by a random time. This study complements the one of\nAksamit/Choulli/Deng/Jeanblanc [1] in which the authors studied similar topics\nfor the case of stopping at the random time instead, while herein we are\nconcerned with the part after the occurrence of the random time. Given that all\nthe literature -up to our knowledge- proves that the NUPBR notion is always\nviolated after honest times that avoid stopping times in a continuous\nfiltration, herein we propose a new class of honest times for which the NUPBR\nnotion can be preserved for some models. For this family of honest times, we\nelaborate two principal results. The first main result characterizes the pairs\nof initial market and honest time for which the resulting model preserves the\nNUPBR property, while the second main result characterizes the honest times\nthat preserve the NUPBR property for any quasi-left continuous model.\nFurthermore, we construct explicitly \"the-after-tau\" local martingale deflators\nfor a large class of initial models (i.e. models in the small filtration) that\nare already risk-neutralized.\n"
    },
    {
        "paper_id": 1404.0601,
        "authors": "Jos\\'e E. Figueroa-L\\'opez and Sveinn \\'Olafsson",
        "title": "Short-time expansions for close-to-the-money options under a L\\'evy jump\n  model with stochastic volatility",
        "comments": "corrected some typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Figueroa-L\\'opez et al. (2013), a second order approximation for\nat-the-money (ATM) option prices is derived for a large class of exponential\nL\\'evy models, with or without a Brownian component. The purpose of this\narticle is twofold. First, we relax the regularity conditions imposed in\nFigueroa-L\\'opez et al. (2013) on the L\\'evy density to the weakest possible\nconditions for such an expansion to be well defined. Second, we show that the\nformulas extend both to the case of \"close-to-the-money\" strikes and to the\ncase where the continuous Brownian component is replaced by an independent\nstochastic volatility process with leverage.\n"
    },
    {
        "paper_id": 1404.0648,
        "authors": "Aur\\'elien Alfonsi and Pierre Blanc",
        "title": "Dynamic optimal execution in a mixed-market-impact Hawkes price model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a linear price impact model including other liquidity takers, whose\nflow of orders either follows a Poisson or a Hawkes process. The optimal\nexecution problem is solved explicitly in this context, and the closed-formula\noptimal strategy describes in particular how one should react to the orders of\nother traders. This result enables us to discuss the viability of the market.\nIt is shown that Poissonian arrivals of orders lead to quite robust Price\nManipulation Strategies in the sense of Huberman and Stanzl. Instead, a\nparticular set of conditions on the Hawkes model balances the self-excitation\nof the order flow with the resilience of the price, excludes Price Manipulation\nStrategies and gives some market stability.\n"
    },
    {
        "paper_id": 1404.0651,
        "authors": "Fabian Dunker and Thorsten Hohage",
        "title": "On parameter identification in stochastic differential equations by\n  penalized maximum likelihood",
        "comments": null,
        "journal-ref": "Inverse Problems, 2014, 30, 095001",
        "doi": "10.1088/0266-5611/30/9/095001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present nonparametric estimators for coefficients in\nstochastic differential equation if the data are described by independent,\nidentically distributed random variables. The problem is formulated as a\nnonlinear ill-posed operator equation with a deterministic forward operator\ndescribed by the Fokker-Planck equation. We derive convergence rates of the\nrisk for penalized maximum likelihood estimators with convex penalty terms and\nfor Newton-type methods. The assumptions of our general convergence results are\nverified for estimation of the drift coefficient. The advantages of\nlog-likelihood compared to quadratic data fidelity terms are demonstrated in\nMonte-Carlo simulations.\n"
    },
    {
        "paper_id": 1404.0746,
        "authors": "Zura Kakushadze and Jim Kyung-Soo Liew",
        "title": "Is It Possible to OD on Alpha?",
        "comments": "12 pages; preprint version",
        "journal-ref": "The Journal of Alternative Investments 18(2) (2015) 39-49",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that combining multiple hedge fund alpha streams yields\ndiversification benefits to the resultant portfolio. Additionally, crossing\ntrades between different alpha streams reduces transaction costs. As the number\nof alpha streams increases, the relative turnover of the portfolio decreases as\nmore trades are crossed. However, we argue, under reasonable assumptions, that\nas the number of alphas increases, the turnover does not decrease indefinitely;\ninstead, the turnover approaches a non-vanishing limit related to the\ncorrelation structure of the portfolio's alphas. We also point out that, more\ngenerally, computational simplifications can arise when the number of alphas is\nlarge.\n"
    },
    {
        "paper_id": 1404.0879,
        "authors": "Gunther Leobacher and Philip Ngare",
        "title": "Utility indifference pricing of derivatives written on industrial loss\n  indexes",
        "comments": "(Re-)Insurance, catastrophe derivatives, jump process, random\n  thinning, utility indifference price",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of pricing derivatives written on some industrial\nloss index via utility indifference pricing. The industrial loss index is\nmodelled by a compound Poisson process and the insurer can adjust her portfolio\nby choosing the risk loading, which in turn determines the demand. We compute\nthe price of a CAT(spread) option written on that index using utility\nindifference pricing.\n"
    },
    {
        "paper_id": 1404.1051,
        "authors": "Jian Zhou (ECUST), Gao-Feng Gu (ECUST), Zhi-Qiang Jiang (ECUST), Xiong\n  Xiong (TJU), Wei Chen (SZSE), Wei Zhang (TJU), Wei-Xing Zhou (ECUST)",
        "title": "Computational experiments successfully predict the emergence of\n  autocorrelations in ultra-high-frequency stock returns",
        "comments": null,
        "journal-ref": "Computational Economics 50 (4), 579-594 (2017)",
        "doi": "10.1007/s10614-016-9612-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social and economic systems are complex adaptive systems, in which\nheterogenous agents interact and evolve in a self-organized manner, and\nmacroscopic laws emerge from microscopic properties. To understand the\nbehaviors of complex systems, computational experiments based on physical and\nmathematical models provide a useful tools. Here, we perform computational\nexperiments using a phenomenological order-driven model called the modified\nMike-Farmer (MMF) to predict the impacts of order flows on the autocorrelations\nin ultra-high-frequency returns, quantified by Hurst index $H_r$. Three\npossible determinants embedded in the MMF model are investigated, including the\nHurst index $H_s$ of order directions, the Hurst index $H_x$ and the power-law\ntail index $\\alpha_x$ of the relative prices of placed orders. The\ncomputational experiments predict that $H_r$ is negatively correlated with\n$\\alpha_x$ and $H_x$ and positively correlated with $H_s$. In addition, the\nvalues of $\\alpha_x$ and $H_x$ have negligible impacts on $H_r$, whereas $H_s$\nexhibits a dominating impact on $H_r$. The predictions of the MMF model on the\ndependence of $H_r$ upon $H_s$ and $H_x$ are verified by the empirical results\nobtained from the order flow data of 43 Chinese stocks.\n"
    },
    {
        "paper_id": 1404.1052,
        "authors": "Hai-Chuan Xu (TJU), Wei Zhang (TJU), Xiong Xiong (TJU), Wei-Xing Zhou\n  (ECUST)",
        "title": "An agent-based computational model for China's stock market and stock\n  index futures market",
        "comments": null,
        "journal-ref": "Mathematical Problems in Engineering 2014, 563912 (2014)",
        "doi": "10.1155/2014/563912",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents an agent-based computational cross-market model for\nChinese equity market structure, which includes both stocks and CSI 300 index\nfutures. In this model, we design several stocks and one index futures to\nsimulate this structure. This model allows heterogeneous investors to make\ninvestment decisions with restrictions including wealth, market trading\nmechanism, and risk management. Investors' demands and order submissions are\nendogenously determined. Our model successfully reproduces several key features\nof the Chinese financial markets including spot-futures basis distribution,\nbid-ask spread distribution, volatility clustering and long memory in absolute\nreturns. Our model can be applied in cross-market risk control, market\nmechanism design and arbitrage strategies analysis.\n"
    },
    {
        "paper_id": 1404.1164,
        "authors": "Mikio Ito, Kiyotaka Maeda, Akihiko Noda",
        "title": "Market Efficiency and Government Interventions in Prewar Japanese Rice\n  Futures Markets",
        "comments": "22 pages, 3 tables, 3 figures, Financial History Review, 23(3),\n  pp.325-346, 2017",
        "journal-ref": null,
        "doi": "10.1017/S0968565017000014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study analyzes how colonial rice trade in prewar Japan affected its rice\nmarket, considering several government interventions in the two rice futures\nexchanges in Tokyo and Osaka. We explore the interventions in the futures\nmarkets using two procedures. First, we measure the joint degree of efficiency\nin the markets using a time-varying vector autoregression model. Second, we\nexamine many historical events that possibly affected the markets and focus on\none event at a time. The degree varies over time within our sample period\n(1881-1932). The observation, together with historical analysis, leads to the\nfollowing conclusions. (1) The two major markets in Tokyo and Osaka were nearly\nefficient. (2) Government interventions involving the delivery of imported rice\nfrom Taiwan and Korea often reduced futures market efficiency. Finally, (3)\nthis relationship continued as long as the quality difference between imported\nand domestic rice existed. The government interventions that promoted domestic\ndistributions of the colonial goods resulted in confusion in the commodity\nmarkets, and decreased efficiency of the markets in the metropole.\n"
    },
    {
        "paper_id": 1404.118,
        "authors": "Calypso Herrera and Louis Paulot",
        "title": "Parallel American Monte Carlo",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new algorithm for American Monte Carlo that can\nbe used either for American-style options, callable structured products or for\ncomputing counterparty credit risk (e.g. CVA or PFE computation). Leveraging\nleast squares regressions, the main novel feature of our algorithm is that it\ncan be fully parallelized. Moreover, there is no need to store the paths and\nthe payoff computation can be done forwards: this allows to price structured\nproducts with complex path and exercise dependencies. The key idea of our\nalgorithm is to split the set of paths in several subsets which are used\niteratively. We give the convergence rate of the algorithm. We illustrate our\nmethod on an American put option and compare the results with the\nLongstaff-Schwartz algorithm.\n"
    },
    {
        "paper_id": 1404.1351,
        "authors": "Carol Alexander and Johannes Rauch",
        "title": "Model-Free Discretisation-Invariant Swaps and S&P 500 Higher-Moment Risk\n  Premia",
        "comments": "This paper was divided and developed into two current papers:\n  arXiv:1602.00235 and arXiv:1602.00865",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2420051",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a general multivariate theory for realised characteristics of\n`model-free discretisation-invariant swaps', so-called because the standard\nno-arbitrage assumption of martingale forward prices is sufficient to derive\nfair-value swap rates for such characteristics which have no jump or\ndiscretisation errors. This theory underpins specific examples for swaps based\non higher moments of a single log return distribution where exact replication\nis possible via option-implied `fundamental contracts' like the log contact.\nThe common factors determining the S&P 500 risk premia associated with these\nhigher-moment characteristics are investigated empirically at the daily, weekly\nand monthly frequencies.\n"
    },
    {
        "paper_id": 1404.1367,
        "authors": "A. Agreda and K. Tucci",
        "title": "Emergence of communities on a coevolutive model of wealth interchange",
        "comments": "5 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1140/epjst/e2014-02305-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a model in which we investigate the structure and evolution of a\nrandom network that connects agents capable of exchanging wealth. Economic\ninteractions between neighbors can occur only if the difference between their\nwealth is less than a threshold value that defines the width of the economic\nclasses. If the interchange of wealth cannot be done, agents are reconnected\nwith another randomly selected agent, allowing the network to evolve in time.\nOn each interaction there is a probability of favoring the poorer agent,\nsimulating the action of the government. We measure the Gini index, having real\nworld values attached to reality. Besides the network structure showed a very\nclose connection with the economic dynamic of the system.\n"
    },
    {
        "paper_id": 1404.1441,
        "authors": "Boualem Djehiche and Hamidou Tembine and Raul Tempone",
        "title": "A Stochastic Maximum Principle for Risk-Sensitive Mean-Field Type\n  Control",
        "comments": "20 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study mean-field type control problems with risk-sensitive\nperformance functionals. We establish a stochastic maximum principle (SMP) for\noptimal control of stochastic differential equations (SDEs) of mean-field type,\nin which the drift and the diffusion coefficients as well as the performance\nfunctional depend not only on the state and the control but also on the mean of\nthe distribution of the state. Our result extends the risk-sensitive SMP\n(without mean-field coupling) of Lim and Zhou (2005), derived for feedback (or\nMarkov) type optimal controls, to optimal control problems for non-Markovian\ndynamics which may be time-inconsistent in the sense that the Bellman\noptimality principle does not hold. In our approach to the risk-sensitive SMP,\nthe smoothness assumption on the value-function imposed in Lim and Zhou (2005)\nneed not to be satisfied. For a general action space a Peng's type SMP is\nderived, specifying the necessary conditions for optimality. Two examples are\ncarried out to illustrate the proposed risk-sensitive mean-field type SMP under\nlinear stochastic dynamics with exponential quadratic cost function. Explicit\nsolutions are given for both mean-field free and mean-field models.\n"
    },
    {
        "paper_id": 1404.1516,
        "authors": "Y. Dolinsky and H. M. Soner",
        "title": "Martingale optimal transport in the Skorokhod space",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dual representation of the martingale optimal transport problem in the\nSkorokhod space of multi dimensional cadlag processes is proved. The dual is a\nminimization problem with constraints involving stochastic integrals and is\nsimilar to the Kantorovich dual of the standard optimal transport problem. The\nconstraints are required to hold for very path in the Skorokhod space. This\nproblem has the financial interpretation as the robust hedging of path\ndependent European options.\n  In this second version, we included the multi-marginal case.\n"
    },
    {
        "paper_id": 1404.173,
        "authors": "Paulo Rocha, Frank Raischel, Jo\\~ao P. da Cruz, Pedro G. Lind",
        "title": "Stochastic Evolution of Stock Market Volume-Price Distributions",
        "comments": "9 pages, 5 figuras, Conference Proceedings SMDTA 2014, accepted\n  (2014)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using available data from the New York stock market (NYSM) we test four\ndifferent bi-parametric models to fit the correspondent volume-price\ndistributions at each $10$-minute lag: the Gamma distribution, the inverse\nGamma distribution, the Weibull distribution and the log-normal distribution.\nThe volume-price data, which measures market capitalization, appears to follow\na specific statistical pattern, other than the evolution of prices measured in\nsimilar studies. We find that the inverse Gamma model gives a superior fit to\nthe volume-price evolution than the other models. We then focus on the inverse\nGamma distribution as a model for the NYSM data and analyze the evolution of\nthe pair of distribution parameters as a stochastic process. Assuming that the\nevolution of these parameters is governed by coupled Langevin equations, we\nderive the corresponding drift and diffusion coefficients, which then provide\ninsight for understanding the mechanisms underlying the evolution of the stock\nmarket.\n"
    },
    {
        "paper_id": 1404.1761,
        "authors": "Lokman A. Abbas-Turki, Ioannis Karatzas and Qinghua Li",
        "title": "Impulse Control of a Diffusion with a Change Point",
        "comments": "29 pages; 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper solves a Bayes sequential impulse control problem for a diffusion,\nwhose drift has an unobservable parameter with a change point. The\npartially-observed problem is reformulated into one with full observations, via\na change of probability measure which removes the drift. The optimal impulse\ncontrols can be expressed in terms of the solutions and the current values of a\nMarkov process adapted to the observation filtration. We shall illustrate the\napplication of our results using the Longstaff-Schwartz algorithm for multiple\noptimal stopping times in a geometric Brownian motion stock price model with\ndrift uncertainty.\n"
    },
    {
        "paper_id": 1404.1773,
        "authors": "Robert Stelzer and Jovana Zavi\\v{s}in",
        "title": "Derivative pricing under the possibility of long memory in the supOU\n  stochastic volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the supOU stochastic volatility model which is able to exhibit\nlong-range dependence. For this model we give conditions for the discounted\nstock price to be a martingale, calculate the characteristic function, give a\nstrip where it is analytic and discuss the use of Fourier pricing techniques.\n  Finally, we present a concrete specification with polynomially decaying\nautocorrelations and calibrate it to observed market prices of plain vanilla\noptions.\n"
    },
    {
        "paper_id": 1404.1895,
        "authors": "Nicole El Karoui (LPMA), Caroline Hillairet (CMAP), Mohamed Mrad\n  (LAGA)",
        "title": "Ramsey Rule with Progressive Utility in Long Term Yield Curves Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper relies on the study of long term yield curves\nmodeling. Inspired by the economic litterature, it provides a financial\ninterpretation of the Ramsey rule that links discount rate and marginal utility\nof aggregate optimal consumption. For such a long maturity modelization, the\npossibility of adjusting preferences to new economic information is crucial.\nThus, after recalling some important properties on progressive utility, this\npaper first provides an extension of the notion of a consistent progressive\nutility to a consistent pair of progressive utilities of investment and\nconsumption. An optimality condition is that the utility from the wealth\nsatisfies a second order SPDE of HJB type involving the Fenchel-Legendre\ntransform of the utility from consumption. This SPDE is solved in order to give\na full characterization of this class of consistent progressive pair of\nutilities. An application of this results is to revisit the classical backward\noptimization problem in the light of progressive utility theory, emphasizing\nintertemporal-consistency issue. Then we study the dynamics of the marginal\nutility yield curve, and give example with backward and progressive power\nutilities.\n"
    },
    {
        "paper_id": 1404.1913,
        "authors": "Nicole El Karoui (LPMA), Mohamed Mrad (LAGA), Caroline Hillairet\n  (CMAP)",
        "title": "Ramsey Rule with Progressive utility and Long Term Affine Yields Curves",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1404.1895",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper relies on the study of long term affine yield\ncurves modeling. It is inspired by the Ramsey rule of the economic literature,\nthat links discount rate and marginal utility of aggregate optimal consumption.\nFor such a long maturity modelization, the possibility of adjusting preferences\nto new economic information is crucial, justifying the use of progressive\nutility. This paper studies, in a framework with affine factors, the yield\ncurve given from the Ramsey rule. It first characterizes consistent progressive\nutility of investment and consumption, given the optimal wealth and consumption\nprocesses. A special attention is paid to utilities associated with linear\noptimal processes with respect to their initial conditions, which is for\nexample the case of power progressive utilities. Those utilities are the basis\npoint to construct other progressive utilities generating non linear optimal\nprocesses but leading yet to still tractable computations. This is of\nparticular interest to study the impact of initial wealth on yield curves.\n"
    },
    {
        "paper_id": 1404.205,
        "authors": "Maciej Kostrzewski",
        "title": "Bayesian DEJD model and detection of asymmetric jumps",
        "comments": "20 pages, 5 figures, working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  News might trigger jump arrivals in financial time series. The \"bad\" and\n\"good\" news seems to have distinct impact. In the research, a double\nexponential jump distribution is applied to model downward and upward jumps.\nBayesian double exponential jump-diffusion model is proposed. Theorems stated\nin the paper enable estimation of the model's parameters, detection of jumps\nand analysis of jump frequency. The methodology, founded upon the idea of\nlatent variables, is illustrated with two empirical studies, employing both\nsimulated and real-world data (the KGHM index). News might trigger jump\narrivals in financial time series. The \"bad\" and \"good\" news seems to have\ndistinct impact. In the research, a double exponential jump distribution is\napplied to model downward and upward jumps. Bayesian double exponential\njump-diffusion model is proposed. Theorems stated in the paper enable\nestimation of the model's parameters, detection of jumps and analysis of jump\nfrequency. The methodology, founded upon the idea of latent variables, is\nillustrated with two empirical studies, employing both simulated and real-world\ndata (the KGHM index).\n"
    },
    {
        "paper_id": 1404.214,
        "authors": "Didier Sornette and Peter Cauwels (ETH Zurich)",
        "title": "Financial bubbles: mechanisms and diagnostics",
        "comments": "24 pages, 10 figures. Notenstein white paper series (2014)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define a financial bubble as a period of unsustainable growth, when the\nprice of an asset increases ever more quickly, in a series of accelerating\nphases of corrections and rebounds. More technically, during a bubble phase,\nthe price follows a faster-than-exponential power law growth process, often\naccompanied by log-periodic oscillations. This dynamic ends abruptly in a\nchange of regime that may be a crash or a substantial correction. Because they\nleave such specific traces, bubbles may be recognised in advance, that is,\nbefore they burst. In this paper, we will explain the mechanism behind\nfinancial bubbles in an intuitive way. We will show how the log-periodic power\nlaw emerges spontaneously from the complex system that financial markets are,\nas a consequence of feedback mechanisms, hierarchical structure and specific\ntrading dynamics and investment styles. We argue that the risk of a major\ncorrection, or even a crash, becomes substantial when a bubble develops towards\nmaturity, and that it is therefore very important to find evidence of bubbles\nand to follow their development from as early a stage as possible. The tools\nthat are explained in this paper actually serve that purpose. They are at the\ncore of the Financial Crisis Observatory at the ETH Zurich, where tens of\nthousands of assets are monitored on a daily basis. This allow us to have a\ncontinuous overview of emerging bubbles in the global financial markets. The\ncompanion report available as part of the Notenstein white paper series (2014)\nwith the title ``Financial bubbles: mechanism, diagnostic and state of the\nWorld (Feb. 2014)'' presents a practical application of the methodology\noutlines in this article and describes our view of the status concerning\npositive and negative bubbles in the financial markets, as of the end of\nJanuary 2014.\n"
    },
    {
        "paper_id": 1404.2227,
        "authors": "Kasper Larsen, H. Mete Soner, Gordan Zitkovic",
        "title": "Facelifting in Utility Maximization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish the existence and characterization of a primal and a dual\nfacelift - discontinuity of the value function at the terminal time - for\nutility-maximization in incomplete semimartingale-driven financial markets.\nUnlike in the lower- and upper-hedging problems, and somewhat unexpectedly, a\nfacelift turns out to exist in utility-maximization despite strict convexity in\nthe objective function. In addition to discussing our results in their natural,\nMarkovian environment, we also use them to show that the dual optimizer cannot\nbe found in the set of countably-additive (martingale) measures in a wide\nvariety of situations.\n"
    },
    {
        "paper_id": 1404.3153,
        "authors": "Matthew Lorig, Stefano Pagliarani, Andrea Pascucci",
        "title": "Asymptotics for $d$-dimensional L\\'evy-type processes",
        "comments": "20 Pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general d-dimensional Levy-type process with killing. Combining\nthe classical Dyson series approach with a novel polynomial expansion of the\ngenerator A(t) of the Levy-type process, we derive a family of asymptotic\napproximations for transition densities and European-style options prices.\nExamples of stochastic volatility models with jumps are provided in order to\nillustrate the numerical accuracy of our approach. The methods described in\nthis paper extend the results from Corielli et al. (2010), Pagliarani and\nPascucci (2013) and Lorig et al. (2013a) for Markov diffusions to Markov\nprocesses with jumps.\n"
    },
    {
        "paper_id": 1404.316,
        "authors": "Pablo Olivares",
        "title": "Pricing of Basket Options Using Polynomial Approximations",
        "comments": "6 figures, 18 page",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we use Bernstein and Chebyshev polynomials to approximate the\nprice of some basket options under a bivariate Black-Scholes model. The method\nconsists in expanding the price of a univariate related contract after\nconditioning on the remaining underlying assets and calculating the mixed\nexponential-power moments of a Gaussian distribution that arise as a\nconsequence of such approximation. Our numerical implementation on spread\ncontracts shows the method is as accurate as a standard Monte Carlo approach at\nconsiderable lesser computational effort.\n"
    },
    {
        "paper_id": 1404.3167,
        "authors": "Christopher J.K. Knight, Alexandra S. Penn and Rebecca B. Hoyle",
        "title": "A Dynamical Model of the Industrial Economy of the Humber Region",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Humber region in the UK is a large and diverse industrial area centred\naround oil refining, chemical industries and energy production. However there\nis currently a desire to see the region transition towards a more bio-based\neconomy. New bio-related industries are being situated in the region as a\nconsequence of policy and economic incentives. Many of these industries are\nconnected through their supply chains, either directly, or by sharing common\nsuppliers or customers and the growth or decline of one industry can hence have\nimpacts on many others. Therefore an important question to consider is what\neffect this movement towards bio-based industry will actually have on the\nregional economy as a whole. In this paper we develop a general abstract\ndynamical model for the metabolic interactions of firms or industries. This\ndynamical model has been applied to the Humber region in order to gain a deeper\nunderstanding of how the region may develop. The model suggests that the\ntransition to a bio-based economy will occur with oil refining losing its\ndominance to bioethanol production and biological chemical production, whilst\nanaerobic digestion grows as a major source of electricity, in turn driving up\nthe value of regional waste aggregators and arable farming in the overall\neconomy.\n"
    },
    {
        "paper_id": 1404.3219,
        "authors": "Hong Pi and Carsten Peterson",
        "title": "Estimating nonlinear regression errors without doing regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A method for estimating nonlinear regression errors and their distributions\nwithout performing regression is presented. Assuming continuity of the modeling\nfunction the variance is given in terms of conditional probabilities extracted\nfrom the data. For N data points the computational demand is N2. Comparing the\npredicted residual errors with those derived from a linear model assumption\nprovides a signal for nonlinearity. The method is successfully illustrated with\ndata generated by the Ikeda and Lorenz maps augmented with noise. As a\nby-product the embedding dimensions of these maps are also extracted.\n"
    },
    {
        "paper_id": 1404.3229,
        "authors": "Pablo Olivares and Alexander Alvarez",
        "title": "A Note on the Pricing of Basket Options Using Taylor Approximations",
        "comments": "13 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a closed-form approximation for the price of basket\noptions under a multivariate Black-Scholes model, based on Taylor expansions\nand the calculation of mixed exponential-power moments of a Gaussian\ndistribution. Our numerical results show that a second order expansion provides\naccurate prices of spread options with low computational costs, even for\nout-of-the-money contracts.\n"
    },
    {
        "paper_id": 1404.3258,
        "authors": "Sourish Das, Aritra Halder and Dipak K. Dey",
        "title": "Regularizing Portfolio Risk Analysis: A Bayesian Approach",
        "comments": "14 pages, 3 figures",
        "journal-ref": "Methodology and Computing in Applied Probability, 2017, Vol 19,\n  Issue 3,pp: 865--889",
        "doi": "10.1007/s11009-016-9524-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is important for a portfolio manager to estimate and analyze recent\nportfolio volatility to keep the portfolio's risk within limit. Though the\nnumber of financial instruments in the portfolio can be very large, sometimes\nmore than thousands, daily returns considered for analysis are only for a month\nor even less. In this case rank of portfolio covariance matrix is less than\nfull, hence solution is not unique. It is typically known as the ``ill-posed\"\nproblem. In this paper we discuss a Bayesian approach to regularize the\nproblem. One of the additional advantages of this approach is to analyze the\nsource of risk by estimating the probability of positive `conditional\ncontribution to total risk' (CCTR). Each source's CCTR would sum up to the\nportfolio's total volatility risk. Existing methods only estimate CCTR of a\nsource, and does not estimate the probability of CCTR to be significantly\ngreater (or less) than zero. This paper presents Bayesian methodology to do so.\nWe use a parallelizable and easy to use Monte Carlo (MC) approach to achieve\nour objective. Estimation of various risk measures, such as Value at Risk and\nExpected Shortfall, becomes a by-product of this Monte-Carlo approach.\n"
    },
    {
        "paper_id": 1404.3274,
        "authors": "Y. Lemp\\'eri\\`ere, C. Deremble, P. Seager, M. Potters, J. P. Bouchaud",
        "title": "Two centuries of trend following",
        "comments": "17 pages, 9 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish the existence of anomalous excess returns based on trend\nfollowing strategies across four asset classes (commodities, currencies, stock\nindices, bonds) and over very long time scales. We use for our studies both\nfutures time series, that exist since 1960, and spot time series that allow us\nto go back to 1800 on commodities and indices. The overall t-stat of the excess\nreturns is $\\approx 5$ since 1960 and $\\approx 10$ since 1800, after accounting\nfor the overall upward drift of these markets. The effect is very stable, both\nacross time and asset classes. It makes the existence of trends one of the most\nstatistically significant anomalies in financial markets. When analyzing the\ntrend following signal further, we find a clear saturation effect for large\nsignals, suggesting that fundamentalist traders do not attempt to resist \"weak\ntrends\", but step in when their own signal becomes strong enough. Finally, we\nstudy the performance of trend following in the recent period. We find no sign\nof a statistical degradation of long trends, whereas shorter trends have\nsignificantly withered.\n"
    },
    {
        "paper_id": 1404.3347,
        "authors": "Jean-Bernard Chatelain, Kirsten Ralf",
        "title": "Stability and Identification with Optimal Macroprudential Policy Rules",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the identification, the determinacy and the stability\nof ad hoc, \"quasi-optimal\" and optimal policy rules augmented with financial\nstability indicators (such as asset prices deviations from their fundamental\nvalues) and minimizing the volatility of the policy interest rates, when the\ncentral bank precommits to financial stability. Firstly, ad hoc and\nquasi-optimal rules parameters of financial stability indicators cannot be\nidentified. For those rules, non zero policy rule parameters of financial\nstability indicators are observationally equivalent to rule parameters set to\nzero in another rule, so that they are unable to inform monetary policy.\nSecondly, under controllability conditions, optimal policy rules parameters of\nfinancial stability indicators can all be identified, along with a bounded\nsolution stabilizing an unstable economy as in Woodford (2003), with\ndeterminacy of the initial conditions of non- predetermined variables.\n"
    },
    {
        "paper_id": 1404.3555,
        "authors": "Adam Aleksander Majewski, Giacomo Bormetti, Fulvio Corsi",
        "title": "Smile from the Past: A general option pricing framework with multiple\n  volatility and leverage components",
        "comments": "33 pages, 2 figures, 3 tables; Forthcoming on Journal of Econometrics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the current literature, the analytical tractability of discrete time\noption pricing models is guaranteed only for rather specific types of models\nand pricing kernels. We propose a very general and fully analytical option\npricing framework, encompassing a wide class of discrete time models featuring\nmultiple-component structure in both volatility and leverage, and a flexible\npricing kernel with multiple risk premia. Although the proposed framework is\ngeneral enough to include either GARCH-type volatility, Realized Volatility or\na combination of the two, in this paper we focus on realized volatility option\npricing models by extending the Heterogeneous Autoregressive Gamma (HARG) model\nof Corsi, Fusari, La Vecchia (2012) to incorporate heterogeneous leverage\nstructures with multiple components, while preserving closed-form solutions for\noption prices. Applying our analytically tractable asymmetric HARG model to a\nlarge sample of S&P 500 index options, we demonstrate its superior ability to\nprice out-of-the-money options compared to existing benchmarks.\n"
    },
    {
        "paper_id": 1404.3678,
        "authors": "Vadim Borokhov",
        "title": "On the properties of nodal price response matrix in electricity markets",
        "comments": "Paper shortened and reformatted, some explanations added",
        "journal-ref": "IEEE Transactions on Power Systems, 30(6): 3286-3294 November 2015",
        "doi": "10.1109/TPWRS.2014.2376494",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish sufficient conditions for nodal price response matrix in\nelectric power system to be symmetric and negative (semi-)definite. The results\nare applicable for electricity markets with nonlinear and intertemporal\nconstraints.\n"
    },
    {
        "paper_id": 1404.4014,
        "authors": "Robert Azencott, Yutheeka Gadhyan, Roland Glowinski",
        "title": "Option Pricing Accuracy for Estimated Heston Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider assets for which price $X_t$ and squared volatility $Y_t$ are\njointly driven by Heston joint stochastic differential equations (SDEs). When\nthe parameters of these SDEs are estimated from $N$ sub-sampled data $(X_{nT},\nY_{nT})$, estimation errors do impact the classical option pricing PDEs. We\nestimate these option pricing errors by combining numerical evaluation of\nestimation errors for Heston SDEs parameters with the computation of option\nprice partial derivatives with respect to these SDEs parameters. This is\nachieved by solving six parabolic PDEs with adequate boundary conditions. To\nimplement this approach, we also develop an estimator $\\hat \\lambda$ for the\nmarket price of volatility risk, and we study the sensitivity of option pricing\nto estimation errors affecting $\\hat \\lambda$. We illustrate this approach by\nfitting Heston SDEs to 252 daily joint observations of the S\\&P 500 index and\nof its approximate volatility VIX, and by numerical applications to European\noptions written on the S\\&P 500 index.\n"
    },
    {
        "paper_id": 1404.4028,
        "authors": "Mark Higgins",
        "title": "Stochastic Spot/Volatility Correlation in Stochastic Volatility Models\n  and Barrier Option Pricing",
        "comments": "23 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most models for barrier pricing are designed to let a market maker tune the\nmodel-implied covariance between moves in the asset spot price and moves in the\nimplied volatility skew. This is often implemented with a local\nvolatility/stochastic volatility mixture model, where the mixture parameter\ntunes that covariance. This paper defines an alternate model where the\nspot/volatility correlation is a separate mean-reverting stochastic variable\nwhich is itself correlated with spot. We also develop an efficient\napproximation for barrier option and one touch pricing in the model based on\nsemi-static vega replication and compare it with Monte Carlo pricing. The\napproximation works well in markets where the risk neutral drift is modest.\n"
    },
    {
        "paper_id": 1404.404,
        "authors": "Fabio Caccioli, Imre Kondor, Matteo Marsili, Susanne Still",
        "title": "$L_p$ regularized portfolio optimization",
        "comments": "27 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investors who optimize their portfolios under any of the coherent risk\nmeasures are naturally led to regularized portfolio optimization when they take\ninto account the impact their trades make on the market. We show here that the\nimpact function determines which regularizer is used. We also show that any\nregularizer based on the norm $L_p$ with $p>1$ makes the sensitivity of\ncoherent risk measures to estimation error disappear, while regularizers with\n$p<1$ do not. The $L_1$ norm represents a border case: its \"soft\"\nimplementation does not remove the instability, but rather shifts its locus,\nwhereas its \"hard\" implementation (equivalent to a ban on short selling)\neliminates it. We demonstrate these effects on the important special case of\nExpected Shortfall (ES) that is on its way to becoming the next global\nregulatory market risk measure.\n"
    },
    {
        "paper_id": 1404.4068,
        "authors": "Guy Katriel",
        "title": "Directed Random Market: the equilibrium distribution",
        "comments": null,
        "journal-ref": "Acta Applicandae Mathematicae, 139 (2015), 95-103",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find the explicit expression for the equilibrium wealth distribution of\nthe Directed Random Market process, recently introduced by\nMart\\'inez-Mart\\'inez and L\\'opez-Ruiz, which turns out to be a Gamma\ndistribution with shape parameter $\\frac{1}{2}$. We also prove the convergence\nof the discrete-time process describing the evolution of the distribution of\nwealth to the equilibrium distribution.\n"
    },
    {
        "paper_id": 1404.415,
        "authors": "Alain Bensoussan, Jens Frehse, Phillip Yam",
        "title": "The Master Equation in Mean Field Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In his lectures at College de France, P.L. Lions introduced the concept of\nMaster equation, see [5] for Mean Field Games. It is introduced in a heuristic\nfashion, from the system of partial differential equations, associated to a\nNash equilibrium for a large, but finite, number of players. The method, also\nexplained in[2], consists in a formal analogy of terms. The interest of this\nequation is that it contains interesting particular cases, which can be studied\ndirectly, in particular the system of HJB-FP (Hamilton-Jacobi-Bellman,\nFokker-Planck) equations obtained as the limit of the finite Nash equilibrium\ngame, when the trajectories are independent, see [4]. Usually, in mean field\ntheory, one can bypass the large Nash equilibrium, by introducing the concept\nof representative agent, whose action is influenced by a distribution of\nsimilar agents, and obtains directly the system of HJB-FP equations of\ninterest, see for instance [1]. Apparently, there is no such approach for the\nMaster equation. We show here that it is possible. We first do it for the Mean\nField type control problem, for which we interpret completely the Master\nequation. For the Mean Field Games itself, we solve a related problem, and\nobtain again the Master equation.\n"
    },
    {
        "paper_id": 1404.4275,
        "authors": "Xiaochao Qian",
        "title": "A Bitcoin system with no mining and no history transactions: Build a\n  compact Bitcoin system",
        "comments": "Call for collaborators",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit definition of decentralization and show you that\ndecentralization is almost impossible for the current stage and Bitcoin is the\nfirst truly noncentralized currency in the currency history. We propose a new\nframework of noncentralized cryptocurrency system with an assumption of the\nexistence of a weak adversary for a bank alliance. It abandons the mining\nprocess and blockchain, and removes history transactions from data\nsynchronization. We propose a consensus algorithm named Converged Consensus for\na noncentralized cryptocurrency system.\n"
    },
    {
        "paper_id": 1404.4464,
        "authors": "Giovanni Conforti, Stefano De Marco, Jean-Dominique Deuschel",
        "title": "On small-noise equations with degenerate limiting system arising from\n  volatility models",
        "comments": "21 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The one-dimensional SDE with non Lipschitz diffusion coefficient $dX_{t} =\nb(X_{t})dt + \\sigma X_{t}^{\\gamma} dB_{t}, \\ X_{0}=x, \\ \\gamma<1$ is widely\nstudied in mathematical finance. Several works have proposed asymptotic\nanalysis of densities and implied volatilities in models involving instances of\nthis equation, based on a careful implementation of saddle-point methods and\n(essentially) the explicit knowledge of Fourier transforms. Recent research on\ntail asymptotics for heat kernels [J-D. Deuschel, P.~Friz, A.~Jacquier, and\nS.~Violante. Marginal density expansions for diffusions and stochastic\nvolatility, part II: Applications. 2013, arxiv:1305.6765] suggests to work with\nthe rescaled variable $X^{\\varepsilon}:=\\varepsilon^{1/(1-\\gamma)} X$: while\nallowing to turn a space asymptotic problem into a small-$\\varepsilon$ problem\nwith fixed terminal point, the process $X^{\\varepsilon}$ satisfies a SDE in\nWentzell--Freidlin form (i.e. with driving noise $\\varepsilon dB$). We prove a\npathwise large deviation principle for the process $X^{\\varepsilon}$ as\n$\\varepsilon \\to 0$. As it will become clear, the limiting ODE governing the\nlarge deviations admits infinitely many solutions, a non-standard situation in\nthe Wentzell--Freidlin theory. As for applications, the $\\varepsilon$-scaling\nallows to derive exact log-asymptotics for path functionals of the process:\nwhile on the one hand the resulting formulae are confirmed by the CIR-CEV\nbenchmarks, on the other hand the large deviation approach (i) applies to\nequations with a more general drift term and (ii) potentially opens the way to\nheat kernel analysis for higher-dimensional diffusions involving such an SDE as\na component.\n"
    },
    {
        "paper_id": 1404.455,
        "authors": "Peter Sarlin",
        "title": "Macroprudential oversight, risk communication and visualization",
        "comments": "Supplementary interactive applications: http://vis.risklab.fi/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the role of risk communication in macroprudential\noversight and of visualization in risk communication. Beyond the soar in data\navailability and precision, the transition from firm-centric to system-wide\nsupervision imposes vast data needs. Moreover, except for internal\ncommunication as in any organization, broad and effective external\ncommunication of timely information related to systemic risks is a key mandate\nof macroprudential supervisors, further stressing the importance of simple\nrepresentations of complex data. This paper focuses on the background and\ntheory of information visualization and visual analytics, as well as techniques\nwithin these fields, as potential means for risk communication. We define the\ntask of visualization in risk communication, discuss the structure of\nmacroprudential data, and review visualization techniques applied to systemic\nrisk. We conclude that two essential, yet rare, features for supporting the\nanalysis of big data and communication of risks are analytical visualizations\nand interactive interfaces. For visualizing the so-called macroprudential data\ncube, we provide the VisRisk platform with three modules: plots, maps and\nnetworks. While VisRisk is herein illustrated with five web-based interactive\nvisualizations of systemic risk indicators and models, the platform enables and\nis open to the visualization of any data from the macroprudential data cube.\n"
    },
    {
        "paper_id": 1404.4659,
        "authors": "Jan Kuklinski, Doinita Negru and Pawel Pliszka",
        "title": "Modelling the skew and smile of SPX and DAX index options using the\n  Shifted Log-Normal and SABR stochastic models",
        "comments": "5 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We discuss modelling of SPX and DAX index option prices using the Shifted\nLog-Normal (SLN) model, (also known as Displaced Diffusion), and the SABR\nmodel. We found out that for SPX options, an example of strongly skewed option\nprices, SLN can produce a quite accurate fit. Moreover, for both types of index\noptions, the SLN model is giving a good fit of near-at-the-forward strikes.\nSuch a near-at-the-money fit allows us to calculate precisely the skew\nparameter without involving directly the 3rd moment of the related probability\ndistribution. Eventually, we can follow with a procedure in which the skew is\ncalculated using the SLN model and further smile effects are added as a next\niteration/perturbation. Furthermore, we point out that the SLN trajectories are\nexact solutions of the SABR model for rho = +/-1.\n"
    },
    {
        "paper_id": 1404.4665,
        "authors": "Karsten Chipeniuk, Nets Hawk Katz, Todd Walker",
        "title": "Approximate aggregation in the neoclassical growth model with\n  ideosyncratic shocks",
        "comments": "24 pages, preliminary version, not yet submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an explicit aggregation in the neoclassical growth model with\naggregate shocks and uninsurable employment risk. We show there are two\nrestrictions on the unemployment shock for approximate aggregation to occur.\nFirst the probability of unemployment must be positive for each agent in each\ntime period. That ensures a strong precautionary savings motive. Second, we\nmust have like agents having similar future prospects. That is agents with\nsimilar employment status and wealth must have similar employment paths. The\nsolution of the model must have distribution of wealth as a state variable and\nhence the curse of dimensionality must be confronted. We sidestep this thorny\nissue by introducing a Walrassian auctioneer that communicates the optimal\namount of invested in every period for every outcome of the shocks to the\nagents.\n"
    },
    {
        "paper_id": 1404.4798,
        "authors": "Bruno Durin",
        "title": "Signal-wise performance attribution for constrained portfolio\n  optimisation",
        "comments": "Working paper, 24 pages Reference added, typos corrected, wording\n  improved",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Performance analysis, from the external point of view of a client who would\nonly have access to returns and holdings of a fund, evolved towards exact\nattribution made in the context of portfolio optimisation, which is the\ninternal point of view of a manager controlling all the parameters of this\noptimisation. Attribution is exact, that-is-to-say no residual \"interaction\"\nterm remains, and various contributions to the optimal portfolio can be\nidentified: predictive signals, constraints, benchmark. However constraints are\nidentified as a separate portfolio and attribution for each signal that are\nused to predict future returns thus corresponds to unconstrained signal\nportfolios. We propose a novel attribution method that put predictive signals\nat the core of attribution and allows to include the effect of constraints in\nportfolios attributed to every signal. We show how this can be applied to\nvarious trading models and portfolio optimisation frameworks and explain what\nkind of insights such an attribution provides.\n"
    },
    {
        "paper_id": 1404.495,
        "authors": "Magomet Yandiev",
        "title": "Expected Cash Flow: A Novel Model Of Evaluating Financial Assets",
        "comments": "7 pages, 7 equations, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper provides the basis for a novel financial asset pricing\nmodel that could avoid the shortcomings of, or even completely replace the\ntraditional DCF model. The model is based on Brownian motion logic and expected\nfuture cash flow values. It can be very useful for Islamic Finance.\n"
    },
    {
        "paper_id": 1404.505,
        "authors": "Zura Kakushadze",
        "title": "A Spectral Model of Turnover Reduction",
        "comments": "15 pages; a trivial typo corrected in Eq. (43), no other changes",
        "journal-ref": "Econometrics 3(3) (2015) 577-589",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a simple explicit formula for turnover reduction when a large number\nof alphas are traded on the same execution platform and trades are crossed\ninternally. We model turnover reduction via alpha correlations. Then, for a\nlarge number of alphas, turnover reduction is related to the largest eigenvalue\nand the corresponding eigenvector of the alpha correlation matrix.\n"
    },
    {
        "paper_id": 1404.5138,
        "authors": "Bertram D\\\"uring, Michel Fourni\\'e, Christof Heuer",
        "title": "High-order compact finite difference schemes for option pricing in\n  stochastic volatility models on non-uniform grids",
        "comments": "21 pages, to appear in J. Comput. Appl. Math",
        "journal-ref": "J. Comput. Appl. Math. 271 (2014), 247-266",
        "doi": "10.1016/j.cam.2014.04.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive high-order compact finite difference schemes for option pricing in\nstochastic volatility models on non-uniform grids. The schemes are fourth-order\naccurate in space and second-order accurate in time for vanishing correlation.\nIn our numerical study we obtain high-order numerical convergence also for\nnon-zero correlation and non-smooth payoffs which are typical in option\npricing. In all numerical experiments a comparative standard second-order\ndiscretisation is significantly outperformed. We conduct a numerical stability\nstudy which indicates unconditional stability of the scheme.\n"
    },
    {
        "paper_id": 1404.514,
        "authors": "Bertram D\\\"uring, Michel Fourni\\'e",
        "title": "High-order compact finite difference scheme for option pricing in\n  stochastic volatility models",
        "comments": "23 pages",
        "journal-ref": "J. Comput. Appl. Math. 236(17) (2012), 4462-4473",
        "doi": "10.1016/j.cam.2012.04.017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a new high-order compact finite difference scheme for option\npricing in stochastic volatility models. The scheme is fourth-order accurate in\nspace and second-order accurate in time. Under some restrictions, theoretical\nresults like unconditional stability in the sense of von Neumann are presented.\nWhere the analysis becomes too involved we validate our findings by a numerical\nstudy. Numerical experiments for the European option pricing problem are\npresented. We observe fourth-order convergence for non-smooth payoff.\n"
    },
    {
        "paper_id": 1404.5203,
        "authors": "Ventura Charlin and Arturo Cifuentes",
        "title": "Towards a Monotonicity-Compliant Price Index for the Art Market",
        "comments": "15 pages 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Notwithstanding almost forty years of efforts, the market for paintings still\nlacks a widely accepted price index. In this paper, we introduce a simple and\nintuitive metric to construct such index. Our metric is based on the price of a\npainting divided by its area. This formulation rests on a solid mathematical\nfoundation as it corresponds to a particular type of hedonic model. However,\nunlike indexes based on the time-dummy coefficients of conventional hedonic\nmodels, this index satisfies the monotonicity condition. We demonstrate with a\nsimple example the advantages of our metric. We also show the dangers of\nrelying on the time-dummy coefficients of conventional hedonic models to\nestimate returns and generate price indexes.\n"
    },
    {
        "paper_id": 1404.5222,
        "authors": "Takashi Shinzato",
        "title": "Self-Averaging Property of Minimal Investment Risk of Mean-Variance\n  Model",
        "comments": "37 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0133846",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In portfolio optimization problems, the minimum expected investment risk is\nnot always smaller than the expected minimal investment risk. That is, using a\nwell-known approach from operations research, it is possible to derive a\nstrategy that minimizes the expected investment risk, but this strategy does\nnot always result in the best rate of return on assets. Prior to making\ninvestment decisions, it is important to an investor to know the potential\nminimal investment risk (or the expected minimal investment risk) and to\ndetermine the strategy that will maximize the return on assets. We use the\nself-averaging property to analyze the potential minimal investment risk and\nthe concentrated investment level for the strategy that gives the best rate of\nreturn. We compare the results from our method with the results obtained by the\noperations research approach and with those obtained by a numerical simulation\nusing the optimal portfolio. The results of our method and the numerical\nsimulation are in agreement, but they differ from that of the operations\nresearch approach.\n"
    },
    {
        "paper_id": 1404.5271,
        "authors": "A. Kushpel and J. Levesley",
        "title": "Reconstruction of density functions by sk-splines",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reconstruction of density functions and their characteristic functions by\nradial basis functions with scattered data points is a popular topic in the\ntheory of pricing of basket options. Such functions are usually entire or admit\nan analytic extension into an appropriate tube and \"bell-shaped\" with rapidly\ndecaying tails. Unfortunately, the domain of such functions is not compact\nwhich creates various technical difficulties. We solve interpolation problem on\nan infinite rectangular grid for a wide range of kernel functions and calculate\nexplicitly their Fourier transform to obtain representations for the respective\ndensity functions.\n"
    },
    {
        "paper_id": 1404.5381,
        "authors": "Mikio Ito, Kiyotaka Maeda, Akihiko Noda",
        "title": "The Futures Premium and Rice Market Efficiency in Prewar Japan",
        "comments": "36 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the interrelation between spot and futures prices in the\ntwo major rice markets in prewar Japan from the perspective of market\nefficiency. Applying a non-Bayesian time-varying model approach to the\nfundamental equation for spot returns and the futures premium, we detect when\nefficiency reductions in the two major rice markets occurred. We also examine\nhow government interventions affected the rice markets in Japan, which\ncolonized Taiwan and Korea before World War II, and argue that the function of\nrice futures markets crucially depended on the differences in rice spot\nmarket's structure. The increased volume of imported rice of a different\nvariety from domestic rice first disrupted the rice futures. Then, government\nintervention in the rice futures markets failed to improve the disruption.\nChanges in colonial rice cropping successfully improved the disruption, and\ncolonial rice was promoted to unify the different varieties of inland and\ncolonial rice.\n"
    },
    {
        "paper_id": 1404.5408,
        "authors": "Jakub Trybu{\\l}a, Dariusz Zawisza",
        "title": "Continuous time portfolio choice under monotone preferences with\n  quadratic penalty - stochastic interest rate case",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is a follow up of our previous paper - Trybu{\\l}a and Zawisza\n\\cite{TryZaw}, where we considered a modification of a monotone mean-variance\nfunctional in continuous time in stochastic factor model. In this article we\naddress the problem of optimizing the mentioned functional in a market with a\nstochastic interest rate. We formulate it as a stochastic differential game\nproblem and use Hamilton-Jacobi-Bellman-Isaacs equations to derive the optimal\ninvestment strategy and the value function.\n"
    },
    {
        "paper_id": 1404.5689,
        "authors": "Xiaobing Feng, Haibo Hu",
        "title": "Measurement and Internalization of Systemic Risk in a Global Banking\n  Network",
        "comments": null,
        "journal-ref": "Int. J. Mod. Phys. C 24, 1250093 (2013)",
        "doi": "10.1142/S0129183112500933",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The negative externalities from an individual bank failure to the whole\nsystem can be huge. One of the key purposes of bank regulation is to\ninternalize the social costs of potential bank failures via capital charges.\nThis study proposes a method to evaluate and allocate the systemic risk to\ndifferent countries/regions using a SIR type of epidemic spreading model and\nthe Shapley value in game theory. The paper also explores features of a\nconstructed bank network using real globe-wide banking data.\n"
    },
    {
        "paper_id": 1404.612,
        "authors": "Feijia Wang",
        "title": "Incorporating a Volatility Smile into the Markov-Functional Model",
        "comments": "55 figures, 18 tables, Master thesis at the University of Twente by\n  F. Wang, thesis supervisors: A. Bagchi, M.H. Vellekoop and D. Kandhai, 22\n  December 2006",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a Markov-Functional (MF) interest-rate model with Uncertain\nVolatility Displaced Diffusion (UVDD) digital mapping, which is consistent with\nthe volatility-smile phenomenon observed in the option market. We first check\nthe impact of pricing Bermudan swaptions by the model. Next, we also\ninvestigate the future smiles implied by the MF models and the smile dynamics\nimplied by the UVDD model. Finally, we conduct hedging simulations against\nBermudan swaptions to test extensively the hedge performance of this\nsmile-consistenet MF model.\n"
    },
    {
        "paper_id": 1404.619,
        "authors": "Si Cheng and Michael R. Tehranchi",
        "title": "Polynomial Term Structure Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article discuss a class of tractable model in the form of polynomial\ntype.\n"
    },
    {
        "paper_id": 1404.6227,
        "authors": "Reza Farrahi Moghaddam, Fereydoun Farrahi Moghaddam, Mohamed Cheriet",
        "title": "A Multi-Entity Input Output (MEIO) Approach to Sustainability -\n  Water-Energy-GHG (WEG) Footprint Statements in Use Cases from Auto and Telco\n  Industries",
        "comments": "48 pages, 8 figures, and 27 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new Input-Output model, called the Multi-Entity Input-Output (MEIO) model,\nis introduced to estimate the responsibility of entities of an ecosystem on the\nfootprint of each other. It assumed that the ecosystem is comprised of end\nusers, service providers, and utilities. The proposed MEIO modeling approach\ncan be seen as a realization of the Everybody-in-the-Loop (EitL) framework,\nwhich promotes a sustainable future using behaviors and actions that are aware\nof their ubiquitous eco-socio-environment impacts. In this vision, the\nbehavioral changes could be initiated by providing all actors with their\nfootprint statement, which would be estimated using the MEIO models. First, a\nnaive MEIO model is proposed in the form of a graph of actions and\nresponsibility by considering interactions and goods transfers among the\nentities and actors along four channels. Then, the unnormalized responsibility\nand also the final responsibility among the actors are introduced, and then are\nused to re-allocate immediate footprint of actors among themselves. The\nfootprint in the current model is limited to three major impacts: Water,\nEnergy, and GHG emissions. The naive model is then generalized to\nProvider-perspective (P-perspective) and End User-perspective (E-perspective)\nMEIO models in order to make it more suitable to cases where a large number of\nend users are served by a provider. The E-perspective modeling approach\nparticularly allows estimating the footprint associated to a specific end user.\nIn two use cases from the auto and Telco industries, it has been observed that\nthe proposed MEIO models are practical and dependable in allocating footprint\nto the provider and also to the end user, while i) avoiding footprint leakage\nto the end users and ii) handling the large numbers end users. In addition, it\nwill be shown that the MEIO models could be sued to integrate Scope-3 and LCA\napproaches.\n"
    },
    {
        "paper_id": 1404.6637,
        "authors": "Ovidiu Racorean",
        "title": "Braided and Knotted Stocks in the Stock Market: Anticipating the flash\n  crashes",
        "comments": "23 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A simple and elegant arrangement of stock components of a portfolio (market\nindex-DJIA) in a recent paper [1], has led to the construction of crossing of\nstocks diagram. The crossing stocks method revealed hidden remarkable algebraic\nand geometrical aspects of stock market. The present paper continues to uncover\nnew mathematical structures residing from crossings of stocks diagram by\nintroducing topological properties stock market is endowed with. The crossings\nof stocks are categorized as overcrossings and undercrossings and interpreted\nas generators of braid that stocks form in the process of prices quotations in\nthe market. Topological structure of the stock market is even richer if the\nclosure of stocks braid is considered, such that it forms a knot. To\ndistinguish the kind of knot that stock market forms, Alexander-Conway\npolynomial and the Jones polynomials are calculated for some knotted stocks.\nThese invariants of knots are important for the future practical applications\ntopological stock market might have. Such application may account of the\nrelation between Jones polynomial and phase transition statistical models to\nprovide a clear way to anticipate the transition of financial markets to the\nphase that leads to crisis. The resemblance between braided stocks and logic\ngates of topological quantum computers could quantum encode the stock market\nbehavior.\n"
    },
    {
        "paper_id": 1404.6792,
        "authors": "Tim Leung, Matthew Lorig, Andrea Pascucci",
        "title": "Leveraged {ETF} implied volatilities from {ETF} dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The growth of the exhange-traded fund (ETF) industry has given rise to the\ntrading of options written on ETFs and their leveraged counterparts {(LETFs)}.\nWe study the relationship between the ETF and LETF implied volatility surfaces\nwhen the underlying ETF is modeled by a general class of local-stochastic\nvolatility models. A closed-form approximation for prices is derived for\nEuropean-style options whose payoff depends on the terminal value of the ETF\nand/or LETF. Rigorous error bounds for this pricing approximation are\nestablished. A closed-form approximation for implied volatilities is also\nderived. We also discuss a scaling procedure for comparing implied volatilities\nacross leverage ratios. The implied volatility expansions and scalings are\ntested in three well-known settings: CEV, Heston and SABR.\n"
    },
    {
        "paper_id": 1404.7314,
        "authors": "Damiano Brigo, Qing Liu, Andrea Pallavicini, David Sloth",
        "title": "Nonlinear Valuation under Collateral, Credit Risk and Funding Costs: A\n  Numerical Case Study Extending Black-Scholes",
        "comments": "An updated version of this report will appear in the volume:\n  Veronesi, P. (Editor), \\Handbook in Fixed-Income Securities, Wiley, 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an arbitrage-free framework for consistent valuation of derivative\ntrades with collateralization, counterparty credit gap risk, and funding costs,\nfollowing the approach first proposed by Pallavicini and co-authors in 2011.\nBased on the risk-neutral pricing principle, we derive a general pricing\nequation where Credit, Debit, Liquidity and Funding Valuation Adjustments (CVA,\nDVA, LVA and FVA) are introduced by simply modifying the payout cash-flows of\nthe deal. Funding costs and specific close-out procedures at default break the\nbilateral nature of the deal price and render the valuation problem a\nnon-linear and recursive one. CVA and FVA are in general not really additive\nadjustments, and the risk for double counting is concrete. We introduce a new\nadjustment, called a Non-linearity Valuation Adjustment (NVA), to address\ndouble-counting. The theoretical risk free rate disappears from our final\nequations. The framework can be tailored also to CCP trading under initial and\nvariation margins, as explained in detail in Brigo and Pallavicini (2014). In\nparticular, we allow for asymmetric collateral and funding rates, replacement\nclose-out and re-hypothecation. The valuation equation takes the form of a\nbackward stochastic differential equation or semi-linear partial differential\nequation, and can be cast as a set of iterative equations that can be solved by\nleast-squares Monte Carlo. We propose such a simulation algorithm in a case\nstudy involving a generalization of the benchmark model of Black and Scholes\nfor option pricing. Our numerical results confirm that funding risk has a\nnon-trivial impact on the deal price, and that double counting matters too. We\nconclude the article with an analysis of large scale implications of\nnon-linearity of the pricing equations.\n"
    },
    {
        "paper_id": 1404.732,
        "authors": "Qinghua Li",
        "title": "Facilitation and Internalization Optimal Strategy in a Multilateral\n  Trading Context",
        "comments": "40 pages; 7 figures; 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies four trading algorithms of a professional trader at a\nmultilateral trading facility, observing a realistic two-sided limit order book\nwhose dynamics are driven by the order book events. The identity of the trader\ncan be either internalizing or regular, either a hedge fund or a brokery\nagency. The speed and cost of trading can be balanced by properly choosing\nactive strategies on the displayed orders in the book and passive strategies on\nthe hidden orders within the spread. We shall show that the price switching\nalgorithms provide lower and upper bounds of the mixed trading algorithms.\nEspecially, when the internalization premium is zero, an internalizing trader's\noptimal mixed trading strategy can be achieved among the set of price switching\nstrategies. For both an internalizing trader and a regular trader, the optimal\nprice switching strategy exists and is expressed in terms of the value\nfunction. A parallelizable algorithm to numerically compute the value function\nand optimal price switching strategy for the discretized state process is\nprovided.\n"
    },
    {
        "paper_id": 1404.7356,
        "authors": "Daniel C. Wagner, Thilo A. Schmitt, Rudi Sch\\\"afer, Thomas Guhr,\n  Dietrich E. Wolf",
        "title": "Analysis of a decision model in the context of equilibrium pricing and\n  order book pricing",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.08.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An agent-based model for financial markets has to incorporate two aspects:\ndecision making and price formation. We introduce a simple decision model and\nconsider its implications in two different pricing schemes. First, we study its\nparameter dependence within a supply-demand balance setting. We find realistic\nbehavior in a wide parameter range. Second, we embed our decision model in an\norder book setting. Here we observe interesting features which are not present\nin the equilibrium pricing scheme. In particular, we find a nontrivial behavior\nof the order book volumes which reminds of a trend switching phenomenon. Thus,\nthe decision making model alone does not realistically represent the trading\nand the stylized facts. The order book mechanism is crucial.\n"
    },
    {
        "paper_id": 1404.7364,
        "authors": "Maxim Gusev, Dimitri Kroujiline, Boris Govorkov, Sergey V. Sharov,\n  Dmitry Ushanov and Maxim Zhilyaev",
        "title": "Predictable markets? A news-driven model of the stock market",
        "comments": "This is the version accepted for publication in a new journal\n  Algorithmic Finance (http://algorithmicfinance.org). A draft was posted here\n  on 29 April",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We attempt to explain stock market dynamics in terms of the interaction among\nthree variables: market price, investor opinion and information flow. We\npropose a framework for such interaction and apply it to build a model of stock\nmarket dynamics which we study both empirically and theoretically. We\ndemonstrate that this model replicates observed market behavior on all relevant\ntimescales (from days to years) reasonably well. Using the model, we obtain and\ndiscuss a number of results that pose implications for current market theory\nand offer potential practical applications.\n"
    },
    {
        "paper_id": 1404.7377,
        "authors": "Stefano Olgiati, Gilberto Bronzini, Alessandro Danovi",
        "title": "The Italian Crisis and Producer Households Debt: a Source of Stability?\n  A Reproducible Research",
        "comments": "Accepted at the Risk, Banking and Finance Society, University of\n  Florence, New York University Stern Salomon Center, and Warsaw School of\n  Economics International Credit Risk Management Conference 2014, June 23,24 in\n  Warsaw, Poland",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European Credit Research Institute Research Report 2013 identifies\nHouseholds debt \"rapid increase and abrupt retrenchment\" among the causes of\nmacroeconomic instability in the European Union after 2008. In our research: i)\nwe accessed the Bank of Italy Online Statistical Database on Customers and Risk\nfor Producer Households and Non-Financial Corporations with R Sweave open\naccess statistical software, which makes our analysis freely reproducible by\nother researchers; ii) we subset the European System of Accounts sector\nHouseholds into the Bank of Italy sub-sectors Households and Producer\nHouseholds, which are market producing entities limited to informal\npartnerships, de facto companies and sole proprietorships with up to five\nemployees and iii) we tested the hypothesis of \"rapid increase and abrupt\nretrenchment\" of debt for this subset in Italy for the period 1996-2013. We\nfound that PH debt (bad debt) has been more stable with a lower Variation\nCoefficient of 10.3% (14.2%) versus 13.2% (20.1%) in NFC. We also found that\nthe time series of the ratio of debt granted to NFC (numerator) versus PH\n(denominator) is best described (Multiple Squared 0.95) by the concavity of the\n5th degree coefficient (slope -1.22; 95% CI -1.52 - -0.91) of a 5th order\npolynomial linear regression and by the convexity of the 2nd degree coefficient\n(slope 4.26; 95% CI 2.53 - 5.99) for bad debt (Multiple R Squared 0.47), with\nthis concavity of debt and convexity of bad debt beginning with the Italian\ncrisis in the second trimester of 2008. We reject the hypothesis (p < 0.01) of\n\"rapid increase and abrupt retrenchment\" of debt for the subset Producer\nHouseholds during the Italian Crisis. We generate the hypothesis that this\nsubset could represent a prospective source of stability relative to\nNon-Financial Corporation.\n"
    },
    {
        "paper_id": 1404.7406,
        "authors": "Erhan Bayraktar and Yuchong Zhang",
        "title": "Stochastic Perron's Method for the Probability of lifetime ruin problem\n  under transaction costs",
        "comments": "Final version: To appear in SIAM Journal on Control and Optimization.\n  Keywords: Stochastic Perron's method, singular control, probability of\n  lifetime ruin, transaction costs, viscosity solutions, comparison principle.\n  24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply stochastic Perron's method to a singular control problem where an\nindividual targets at a given consumption rate, invests in a risky financial\nmarket in which trading is subject to proportional transaction costs, and seeks\nto minimize her probability of lifetime ruin. Without relying on the dynamic\nprogramming principle (DPP), we characterize the value function as the unique\nviscosity solution of an associated Hamilton-Jacobi-Bellman (HJB) variational\ninequality. We also provide a complete proof of the comparison principle which\nis the main assumption of stochastic Perron's method.\n"
    },
    {
        "paper_id": 1404.7438,
        "authors": "Maciej Klimek, Marcin Pitera",
        "title": "The least squares method for option pricing revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is shown that the the popular least squares method of option pricing\nconverges even under very general assumptions. This substantially increases the\nfreedom of creating different implementations of the method, with varying\nlevels of computational complexity and flexible approach to regression. It is\nalso argued that in many practical applications even modest non-linear\nextensions of standard regression may produce satisfactory results. This claim\nis illustrated with examples.\n"
    },
    {
        "paper_id": 1404.7493,
        "authors": "Lisa R. Goldberg and Ola Mahmoud",
        "title": "Drawdown: From Practice to Theory and Back Again",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Maximum drawdown, the largest cumulative loss from peak to trough, is one of\nthe most widely used indicators of risk in the fund management industry, but\none of the least developed in the context of measures of risk. We formalize\ndrawdown risk as Conditional Expected Drawdown (CED), which is the tail mean of\nmaximum drawdown distributions. We show that CED is a degree one positive\nhomogenous risk measure, so that it can be linearly attributed to factors; and\nconvex, so that it can be used in quantitative optimization. We empirically\nexplore the differences in risk attributions based on CED, Expected Shortfall\n(ES) and volatility. An important feature of CED is its sensitivity to serial\ncorrelation. In an empirical study that fits AR(1) models to US Equity and US\nBonds, we find substantially higher correlation between the autoregressive\nparameter and CED than with ES or with volatility.\n"
    },
    {
        "paper_id": 1404.7632,
        "authors": "Mario Bonino, Matteo Camelia, Paolo Pigato",
        "title": "A multivariate model for financial indices and an algorithm for\n  detection of jumps in the volatility",
        "comments": "20 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a mean-reverting stochastic volatility model which satisfies some\nrelevant stylized facts of financial markets. We introduce an algorithm for the\ndetection of peaks in the volatility profile, that we apply to the time series\nof Dow Jones Industrial Average and Financial Times Stock Exchange 100 in the\nperiod 1984-2013. Based on empirical results, we propose a bivariate version of\nthe model, for which we find an explicit expression for the decay over time of\ncross-asset correlations between absolute returns. We compare our theoretical\npredictions with empirical estimates on the same financial time series, finding\nan excellent agreement.\n"
    },
    {
        "paper_id": 1404.7642,
        "authors": "Fukang Zhu, Zongwu Cai, Liang Peng",
        "title": "Predictive regressions for macroeconomic data",
        "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS708 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 577-594",
        "doi": "10.1214/13-AOAS708",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Researchers have constantly asked whether stock returns can be predicted by\nsome macroeconomic data. However, it is known that macroeconomic data may\nexhibit nonstationarity and/or heavy tails, which complicates existing testing\nprocedures for predictability. In this paper we propose novel empirical\nlikelihood methods based on some weighted score equations to test whether the\nmonthly CRSP value-weighted index can be predicted by the log dividend-price\nratio or the log earnings-price ratio. The new methods work well both\ntheoretically and empirically regardless of the predicting variables being\nstationary or nonstationary or having an infinite variance.\n"
    },
    {
        "paper_id": 1404.7653,
        "authors": "Hajo Holzmann, Matthias Eulert",
        "title": "The role of the information set for forecasting - with applications to\n  risk management",
        "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS709 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 595-621",
        "doi": "10.1214/13-AOAS709",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predictions are issued on the basis of certain information. If the\nforecasting mechanisms are correctly specified, a larger amount of available\ninformation should lead to better forecasts. For point forecasts, we show how\nthe effect of increasing the information set can be quantified by using\nstrictly consistent scoring functions, where it results in smaller average\nscores. Further, we show that the classical Diebold-Mariano test, based on\nstrictly consistent scoring functions and asymptotically ideal forecasts, is a\nconsistent test for the effect of an increase in a sequence of information sets\non $h$-step point forecasts. For the value at risk (VaR), we show that the\naverage score, which corresponds to the average quantile risk, directly relates\nto the expected shortfall. Thus, increasing the information set will result in\nVaR forecasts which lead on average to smaller expected shortfalls. We\nillustrate our results in simulations and applications to stock returns for\nunconditional versus conditional risk management as well as univariate modeling\nof portfolio returns versus multivariate modeling of individual risk factors.\nThe role of the information set for evaluating probabilistic forecasts by using\nstrictly proper scoring rules is also discussed.\n"
    },
    {
        "paper_id": 1404.7698,
        "authors": "Zuo Quan Xu and Fahuai Yi",
        "title": "An Optimal Consumption-Investment Model with Constraint on Consumption",
        "comments": null,
        "journal-ref": "Mathematical Control and Related Fields, Vol.6, No. 3 (2016),\n  517-534",
        "doi": "10.3934/mcrf.2016014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A continuous-time consumption-investment model with constraint is considered\nfor a small investor whose decisions are the consumption rate and the\nallocation of wealth to a risk-free and a risky asset with logarithmic Brownian\nmotion fluctuations. The consumption rate is subject to an upper bound\nconstraint which linearly depends on the investor's wealth and bankruptcy is\nprohibited. The investor's objective is to maximize total expected discounted\nutility of consumption over an infinite trading horizon. It is shown that the\nvalue function is (second order) smooth everywhere but a unique possibility of\n(known) exception point and the optimal consumption-investment strategy is\nprovided in a closed feedback form of wealth, which in contrast to the existing\nwork does not involve the value function. According to this model, an investor\nshould take the same optimal investment strategy as in Merton's model\nregardless his financial situation. By contrast, the optimal consumption\nstrategy does depend on the investor's financial situation: he should use a\nsimilar consumption strategy as in Merton's model when he is in a bad\nsituation, and consume as much as possible when he is in a good situation.\n"
    },
    {
        "paper_id": 1405.0378,
        "authors": "Masaaki Fujii",
        "title": "A Polynomial Scheme of Asymptotic Expansion for Backward SDEs and Option\n  pricing",
        "comments": "Revised version. To appear in QF",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new asymptotic expansion scheme for backward SDEs (BSDEs) is proposed.The\nperturbation parameter is introduced just to scale the forward stochastic\nvariables within a BSDE. In contrast to the standard small-diffusion asymptotic\nexpansion method, the dynamics of variables given by the forward SDEs is\ntreated exactly. Although it requires a special form of the quadratic\ncovariation terms of the continuous part, it allows rather generic drift as\nwell as jump components to exist. The resultant approximation is given by a\npolynomial function in terms of the unperturbed forward variables whose\ncoefficients are uniquely specified by the solution of the recursive system of\nlinear ODEs. Applications to a jump-extended Heston and lambda-SABR models for\nEuropean contingent claims, as well as the utility-optimization problem in the\npresence of a terminal liability are discussed.\n"
    },
    {
        "paper_id": 1405.0508,
        "authors": "Andrew Green and Chris Kenyon",
        "title": "MVA: Initial Margin Valuation Adjustment by Replication and Regression",
        "comments": "15 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Initial margin requirements are becoming an increasingly common feature of\nderivative markets. However, while the valuation of derivatives under\ncollateralisation (Piterbarg 2010, Piterbarg2012), under counterparty risk with\nunsecured funding costs (FVA) (Burgard2011, Burgard2011, Burgard2013) and in\nthe presence of regulatory capital (KVA) (Green2014) are established through\nvaluation adjustments, hitherto initial margin has not been considered. This\npaper further extends the semi-replication framework of (Burgard2013a), itself\nlater extended by (Green2014), to cover the cost of initial margin, leading to\nMargin Valuation Adjustment (MVA). Initial margin requirements are typically\ngenerated through the use of VAR or CVAR models. Given the form of MVA as an\nintegral over the expected initial margin profile this would lead to excessive\ncomputational costs if a brute force calculation were to be used. Hence we also\npropose a computationally efficient approach to the calculation of MVA through\nthe use of regression techniques, Longstaff-Schwartz Augmented Compression\n(LSAC).\n"
    },
    {
        "paper_id": 1405.0515,
        "authors": "Andrew Green and Chris Kenyon",
        "title": "KVA: Capital Valuation Adjustment",
        "comments": "25 pages, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit (CVA), Debit (DVA) and Funding Valuation Adjustments (FVA) are now\nfamiliar valuation adjustments made to the value of a portfolio of derivatives\nto account for credit risks and funding costs. However, recent changes in the\nregulatory regime and the increases in regulatory capital requirements has led\nmany banks to include the cost of capital in derivative pricing. This paper\nformalises the addition of cost of capital by extending the Burgard-Kjaer\n(2013) semi-replication approach to CVA and FVA to include an addition capital\nterm, Capital Valuation Adjustment (KVA, i.e. Kapital Valuation Adjustment to\ndistinguish from CVA.) The utilization of the capital for funding purposes is\nalso considered. The use of the semi-replication approach means that the\nflexibility around the treatment of self-default is carried over into this\nanalysis. The paper further considers the practical calculation of KVA with\nreference to the Basel II (BCBS-128) and Basel III (BCBS-189) capital regimes\nand their implementation via CRD IV. The paper also assesses how KVA may be\nhedged, given that any hedging transactions themselves lead to regulatory\ncapital requirements and hence capital costs. Finally a number of numerical\nexamples are presented to gauge the cost impact of KVA on vanilla derivative\nproducts.\n"
    },
    {
        "paper_id": 1405.0585,
        "authors": "Ole Peters and Murray Gell-Mann",
        "title": "Evaluating gambles using dynamics",
        "comments": "11 pages, 2 figures",
        "journal-ref": "Chaos 26, 023103 (2016)",
        "doi": "10.1063/1.4940236",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gambles are random variables that model possible changes in monetary wealth.\nClassic decision theory transforms money into utility through a utility\nfunction and defines the value of a gamble as the expectation value of utility\nchanges. Utility functions aim to capture individual psychological\ncharacteristics, but their generality limits predictive power. Expectation\nvalue maximizers are defined as rational in economics, but expectation values\nare only meaningful in the presence of ensembles or in systems with ergodic\nproperties, whereas decision-makers have no access to ensembles and the\nvariables representing wealth in the usual growth models do not have the\nrelevant ergodic properties. Simultaneously addressing the shortcomings of\nutility and those of expectations, we propose to evaluate gambles by averaging\nwealth growth over time. No utility function is needed, but a dynamic must be\nspecified to compute time averages. Linear and logarithmic \"utility functions\"\nappear as transformations that generate ergodic observables for purely additive\nand purely multiplicative dynamics, respectively. We highlight inconsistencies\nthroughout the development of decision theory, whose correction clarifies that\nour perspective is legitimate. These invalidate a commonly cited argument for\nbounded utility functions.\n"
    },
    {
        "paper_id": 1405.0732,
        "authors": "Klusik Przemyslaw",
        "title": "Hedging of equity-linked with maximal success factor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an equity-linked contract whose payoff depends on the lifetime of\npolicy holder and the stock price. We assume the limited capital for hedging\nand we provide with the best strategy for an insurance company in the meaning\nof so called succes factor $\\IE^\\IP\\left[{\\mathbf 1}_{\\{V_T \\geq D)}+{\\mathbf\n1}_{\\{V_T < D\\}}\\frac{V_T}{D}\\right ]$, where $V_T$ denotes the end value of\nstrategy and $D$ is the payoff of the contract. The work is a genaralisation of\nthe work of F\\\"{o}llmer and Schied \\cite{FS2004} and Klusik and Palmowski\n\\cite{KluPal}, but it considers much more general \"incompletness\" of the\nmarket, among others midterm nonmarket information signals and infitite\nnonmarket scenarios.\n"
    },
    {
        "paper_id": 1405.0733,
        "authors": "Marcel Ausloos (Liege & Amsterdam), Herbert Dawid (Bielefeld), and Ugo\n  Merlone (Torino)",
        "title": "Spatial interactions in agent-based modeling",
        "comments": "26 pages, 5 figures, 105 references; a chapter prepared for the book\n  \"Complexity and Geographical Economics - Topics and Tools\", P. Commendatore,\n  S.S. Kayam and I. Kubin, Eds. (Springer, in press, 2014)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent Based Modeling (ABM) has become a widespread approach to model complex\ninteractions. In this chapter after briefly summarizing some features of ABM\nthe different approaches in modeling spatial interactions are discussed.\n  It is stressed that agents can interact either indirectly through a shared\nenvironment and/or directly with each other. In such an approach, higher-order\nvariables such as commodity prices, population dynamics or even institutions,\nare not exogenously specified but instead are seen as the results of\ninteractions. It is highlighted in the chapter that the understanding of\npatterns emerging from such spatial interaction between agents is a key problem\nas much as their description through analytical or simulation means.\n  The chapter reviews different approaches for modeling agents' behavior,\ntaking into account either explicit spatial (lattice based) structures or\nnetworks. Some emphasis is placed on recent ABM as applied to the description\nof the dynamics of the geographical distribution of economic activities, - out\nof equilibrium. The Eurace@Unibi Model, an agent-based macroeconomic model with\nspatial structure, is used to illustrate the potential of such an approach for\nspatial policy analysis.\n"
    },
    {
        "paper_id": 1405.0878,
        "authors": "Grzegorz Orynczak, Marcin Jakubek, Karol Wawrzyniak, Michal Klos",
        "title": "Market Coupling as the Universal Algorithm to Assess Zonal Divisions",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adopting a zonal structure of electricity market requires specification of\nzones' borders. In this paper we use social welfare as the measure to assess\nquality of various zonal divisions. The social welfare is calculated by Market\nCoupling algorithm. The analyzed divisions are found by the usage of extended\nLocational Marginal Prices (LMP) methodology presented in paper [1], which\ntakes into account variable weather conditions. The offered method of\nassessment of a proposed division of market into zones is however not limited\nto LMP approach but can evaluate the social welfare of divisions obtained by\nany methodology.\n"
    },
    {
        "paper_id": 1405.1212,
        "authors": "Przemys{\\l}aw Klusik",
        "title": "Market risk modelling in Solvency II regime and hedging options not\n  using underlying",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper we develop mathematical tools of quantile hedging in incomplete\nmarket. Those could be used for two significant applications:\n  o calculating the \\textbf{optimal capital requirement imposed by Solvency II}\n(Directive 2009/138/EC of the European Parliament and of the Council) when the\nmarket and non-market risk is present in insurance company. We show hot to find\nthe minimal capital $V_0$ to provide with the one-year hedging strategy for\ninsurance company satisfying $E\\left[{\\mathbf 1}_{\\{V_1 \\geq\nD\\}}\\right]=0.995$, where $V_1$ denotes the value of insurance company in one\nyear time and $D$ is the payoff of the contract.\n  o finding a hedging strategy for derivative not using underlying but an asset\nwith dynamics correlated or in some other way dependent (no deterministically)\non underlying. The work is a generalisation of the work of Klusik and Palmowski\n\\cite{KluPal}.\n  Keywords: quantile hedging, solvency II, capital modelling, hedging options\non nontradable asset.\n"
    },
    {
        "paper_id": 1405.1247,
        "authors": "Gao-Feng Gu (ECUST), Xiong Xiong (TJU), Yong-Jie Zhang (TJU), Wei Chen\n  (SZSE), Wei Zhang (TJU), Wei-Xing Zhou (ECUST)",
        "title": "Stylized facts of price gaps in limit order books: Evidence from Chinese\n  stocks",
        "comments": "13 pages",
        "journal-ref": "Chaos, Solitons & Fractals 88, 48-58 (2016)",
        "doi": "10.1016/j.chaos.2015.10.031",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price gap, defined as the logarithmic price difference between the first two\noccupied price levels on the same side of a limit order book (LOB), is a key\ndeterminant of market depth, which is one of the dimensions of liquidity.\nHowever, the properties of price gaps have not been thoroughly studied due to\nthe less availability of ultrahigh frequency data. In the paper, we rebuild the\nLOB dynamics based on the order flow data of 26 A-share stocks traded on the\nShenzhen Stock Exchange in 2003. Three key empirical statistical properties of\nprice gaps are investigated. We find that the distribution of price gaps has a\npower-law tail for all stocks with an average tail exponent close to 3.2.\nApplying modern statistical methods, we confirm that the gap time series are\nlong-range correlated and possess multifractal nature. These three features\nvary from stock to stock and are not universal. Furthermore, we also unveil\nbuy-sell asymmetry phenomena in the properties of price gaps on the buy and\nsell sides of the LOBs for individual stocks. These findings deepen our\nunderstanding of the dynamics of liquidity of common stocks and can be used to\ncalibrate agent-based computational financial models.\n"
    },
    {
        "paper_id": 1405.1266,
        "authors": "Walter Schachermayer",
        "title": "The super-replication theorem under proportional transaction costs\n  revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial market with one riskless and one risky asset. The\nsuper-replication theorem states that there is no duality gap in the problem of\nsuper-replicating a contingent claim under transaction costs and the associated\ndual problem. We give two versions of this theorem. The first theorem relates a\nnum\\'eraire-based admissibility condition in the primal problem to the notion\nof a local martingale in the dual problem. The second theorem relates a\nnum\\'eraire -free admissibility condition in the primal problem to the notion\nof a uniformly integrable martingale in the dual problem.\n"
    },
    {
        "paper_id": 1405.1309,
        "authors": "Luciana Dalla Valle, Maria Elena De Giuli, Claudia Tarantola, Claudio\n  Manelli",
        "title": "Default Probability Estimation via Pair Copula Constructions",
        "comments": "40 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a novel approach for firm default probability\nestimation. The methodology is based on multivariate contingent claim analysis\nand pair copula constructions. For each considered firm, balance sheet data are\nused to assess the asset value, and to compute its default probability. The\nasset pricing function is expressed via a pair copula construction, and it is\napproximated via Monte Carlo simulations. The methodology is illustrated\nthrough an application to the analysis of both operative and defaulted firms.\n"
    },
    {
        "paper_id": 1405.1326,
        "authors": "Edward Furman, Jianxi Su, and Ri\\v{c}ardas Zitikis",
        "title": "Paths and indices of maximal tail dependence",
        "comments": "ASTIN Bulletin: The Journal of the International Actuarial\n  Association, 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate both analytically and numerically that the existing methods\nfor measuring tail dependence in copulas may sometimes underestimate the extent\nof extreme co-movements of dependent risks and, therefore, may not always\ncomply with the new paradigm of prudent risk management. This phenomenon holds\nin the context of both symmetric and asymmetric copulas with and without\nsingularities. As a remedy, we introduce a notion of paths of maximal (tail)\ndependence and utilize it to propose several new indices of tail dependence.\nThe suggested new indices are conservative, conform with the basic concepts of\nmodern quantitative risk management, and are able to distinguish between\ndistinct risky positions in situations when the existing indices fail to do so.\n"
    },
    {
        "paper_id": 1405.1791,
        "authors": "Nassim N Taleb, Raphael Douady",
        "title": "On the Super-Additivity and Estimation Biases of Quantile Contributions",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 429,\n  252-260, 2015",
        "doi": "10.1016/j.physa.2015.02.038",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sample measures of top centile contributions to the total (concentration) are\ndownward biased, unstable estimators, extremely sensitive to sample size and\nconcave in accounting for large deviations. It makes them particularly unfit in\ndomains with power law tails, especially for low values of the exponent. These\nestimators can vary over time and increase with the population size, as shown\nin this article, thus providing the illusion of structural changes in\nconcentration. They are also inconsistent under aggregation and mixing\ndistributions, as the weighted average of concentration measures for A and B\nwill tend to be lower than that from A U B. In addition, it can be shown that\nunder such fat tails, increases in the total sum need to be accompanied by\nincreased sample size of the concentration measurement. We examine the\nestimation superadditivity and bias under homogeneous and mixed distributions.\n"
    },
    {
        "paper_id": 1405.1948,
        "authors": "Zura Kakushadze",
        "title": "Phynance",
        "comments": "111 pages; minor misprints corrected",
        "journal-ref": "Univ. J. Phys. Appl. 9(2) (2015) 64-133",
        "doi": "10.13189/ujpa.2015.090203",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  These are the lecture notes for an advanced Ph.D. level course I taught in\nSpring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.\nThe course primarily focused on an introduction to stochastic calculus and\nderivative pricing with various stochastic computations recast in the language\nof path integral, which is used in theoretical physics, hence \"Phynance\". I\nalso included several \"quiz\" problems (with solutions) comprised of\n(pre-)interview questions quantitative finance job candidates were sometimes\nasked back in those days. The course to a certain extent follows an excellent\nbook \"Financial Calculus: An Introduction to Derivative Pricing\" by M. Baxter\nand A. Rennie.\n"
    },
    {
        "paper_id": 1405.2023,
        "authors": "M. Alessandra Crisafi, Andrea Macrina",
        "title": "Simultaneous Trading in 'Lit' and Dark Pools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an optimal trading problem over a finite period of time during\nwhich an investor has access to both a standard exchange and a dark pool. We\ntake the exchange to be an order-driven market and propose a continuous-time\nsetup for the best bid price and the market spread, both modelled by L\\'evy\nprocesses. Effects on the best bid price arising from the arrival of limit buy\norders at more favourable prices, the incoming market sell orders potentially\nwalking the book, and deriving from the cancellations of limit sell orders at\nthe best ask price are incorporated in the proposed price dynamics. A permanent\nimpact that occurs when 'lit' pool trades cannot be avoided is built in, and an\ninstantaneous impact that models the slippage, to which all 'lit' exchange\ntrades are subject, is also considered. We assume that the trading price in the\ndark pool is the mid-price and that no fees are due for posting orders. We\nallow for partial trade executions in the dark pool, and we find the optimal\ntrading strategy in both venues. Since the mid-price is taken from the\nexchange, the dynamics of the limit order book also affects the optimal\nallocation of shares in the dark pool. We propose a general objective function\nand we show that, subject to suitable technical conditions, the value function\ncan be characterised by the unique continuous viscosity solution to the\nassociated partial integro differential equation. We present two explicit\nexamples of the price and the spread models, and derive the associated optimal\ntrading strategy numerically. We discuss the various degrees of the agent's\nrisk aversion and further show that roundtrips, i.e. posting the remaining\ninventory in the dark pool at every point in time, are not necessarily\nbeneficial.\n"
    },
    {
        "paper_id": 1405.2051,
        "authors": "Laurent Fournier",
        "title": "Merchant Sharing Towards a Zero Marginal Cost Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is the first attempt to formalize a new field of economics;\nstudding the Intangibles Goods available on the Internet. We are taking\nadvantage of the digital world's specific rules, in particular the zero\nmarginal cost, to propose a theory of trading & sharing unified. A function\nbased money is created as a world-wide currency; \"cup\". We argue that our\nsystem discourage speculation activities while it makes easy captured taxes for\ngovernments. The implementation removes the today's paywall on the Internet and\nprovides a simple-to-use, open-source, free-of-charge, highly-secure,\nperson-to-person, privacy-respectful, digital payment tool for citizens, using\nstandard smart-phones with a strong authentication. Next step will be the\npropagation of the network application and we expect many shared benefits for\nthe whole economics development.\n"
    },
    {
        "paper_id": 1405.222,
        "authors": "Li-Xin Wang",
        "title": "Gaussian-Chain Filters for Heavy-Tailed Noise with Application to\n  Detecting Big Buyers and Big Sellers in Stock Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new heavy-tailed distribution --- Gaussian-Chain (GC)\ndistribution, which is inspirited by the hierarchical structures prevailing in\nsocial organizations. We determine the mean, variance and kurtosis of the\nGaussian-Chain distribution to show its heavy-tailed property, and compute the\ntail distribution table to give specific numbers showing how heavy is the\nheavy-tails. To filter out the heavy-tailed noise, we construct two filters ---\n2nd and 3rd-order GC filters --- based on the maximum likelihood principle.\nSimulation results show that the GC filters perform much better than the\nbenchmark least-squares algorithm when the noise is heavy-tail distributed.\nUsing the GC filters, we propose a trading strategy, named Ride-the-Mood, to\nfollow the mood of the market by detecting the actions of the big buyers and\nthe big sellers in the market based on the noisy, heavy-tailed price data.\nApplication of the Ride-the-Mood strategy to five blue-chip Hong Kong stocks\nover the recent two-year period from April 2, 2012 to March 31, 2014 shows that\ntheir returns are higher than the returns of the benchmark Buy-and-Hold\nstrategy and the Hang Seng Index Fund.\n"
    },
    {
        "paper_id": 1405.224,
        "authors": "Denis Belomestny and Volker Kraetschmer",
        "title": "Optimal stopping under model uncertainty: randomized stopping times\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we consider optimal stopping problems with conditional convex\nrisk measures called optimised certainty equivalents. Without assuming any kind\nof time-consistency for the underlying family of risk measures, we derive a\nnovel representation for the solution of the optimal stopping problem. In\nparticular, we generalise the additive dual representation of Rogers (2002) to\nthe case of optimal stopping under uncertainty. Finally, we develop several\nMonte Carlo algorithms and illustrate their power for optimal stopping under\nAverage Value at Risk.\n"
    },
    {
        "paper_id": 1405.2384,
        "authors": "Wenbin Zhang, Zhen Dai, Bindu Pan, Milan Djabirov",
        "title": "A Multi-factor Adaptive Statistical Arbitrage Model",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the implementation of a statistical arbitrage trading\nstrategy based on co-integration relationships where we discover candidate\nportfolios using multiple factors rather than just price data. The portfolio\nselection methodologies include K-means clustering, graphical lasso and a\ncombination of the two. Our results show that clustering appears to yield\nbetter candidate portfolios on average than naively using graphical lasso over\nthe entire equity pool. A hybrid approach of using the combination of graphical\nlasso and clustering yields better results still. We also examine the effects\nof an adaptive approach during the trading period, by re-computing potential\nportfolios once to account for change in relationships with passage of time.\nHowever, the adaptive approach does not produce better results than the one\nwithout re-learning. Our results managed to pass the test for the presence of\nstatistical arbitrage test at a statistically significant level. Additionally\nwe were able to validate our findings over a separate dataset for formation and\ntrading periods.\n"
    },
    {
        "paper_id": 1405.2442,
        "authors": "Tiziano De Angelis, Giorgio Ferrari, John Moriarty",
        "title": "A Non Convex Singular Stochastic Control Problem and its Related Optimal\n  Stopping Boundaries",
        "comments": "24 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Equivalences are known between problems of singular stochastic control (SSC)\nwith convex performance criteria and related questions of optimal stopping, see\nfor example Karatzas and Shreve [SIAM J. Control Optim. 22 (1984)]. The aim of\nthis paper is to investigate how far connections of this type generalise to a\nnon convex problem of purchasing electricity. Where the classical equivalence\nbreaks down we provide alternative connections to optimal stopping problems.\n  We consider a non convex infinite time horizon SSC problem whose state\nconsists of an uncontrolled diffusion representing a real-valued commodity\nprice, and a controlled increasing bounded process representing an inventory.\nWe analyse the geometry of the action and inaction regions by characterising\ntheir (optimal) boundaries. Unlike the case of convex SSC problems we find that\nthe optimal boundaries may be both reflecting and repelling and it is natural\nto interpret the problem as one of SSC with discretionary stopping.\n"
    },
    {
        "paper_id": 1405.2445,
        "authors": "Jozef Barunik and Evzen Kocenda and Lukas Vacha",
        "title": "How does bad and good volatility spill over across petroleum markets?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We detect and quantify asymmetries in volatility spillovers using the\nrealized semivariances of petroleum commodities: crude oil, gasoline, and\nheating oil. During the 1987--2014 period we document increasing spillovers\nfrom volatility among petroleum commodities that substantially change after the\n2008 financial crisis. The increase in volatility spillovers correlates with\nthe progressive financialization of the commodities. In terms of asymmetries in\nspillovers we show that periods of increasing crude oil prices strongly\ncorrelate with dominating spillovers due to bad volatility. Overall, bad\nvolatility due to negative returns spills over among petroleum commodities to a\nmuch larger extent than good volatility due to positive returns. After the 2008\nfinancial crisis the asymmetries in spillovers markedly declined in terms of\ntotal as well as directional spillovers. An analysis of directional spillovers\nfurther reveals that no commodity dominates other commodities in terms of\nspillover transmission in general.\n"
    },
    {
        "paper_id": 1405.245,
        "authors": "Zorana Grbac, Antonis Papapantoleon, John Schoenmakers, David Skovmand",
        "title": "Affine LIBOR models with multiple curves: theory, examples and\n  calibration",
        "comments": "42 pages, 11 figures. Updated version, added section on negative\n  rates and positive spreads",
        "journal-ref": "SIAM Journal on Financial Mathematics 6, 984-1025, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a multiple curve framework that combines tractable dynamics and\nsemi-analytic pricing formulas with positive interest rates and basis spreads.\nNegatives rates and positive spreads can also be accommodated in this\nframework. The dynamics of OIS and LIBOR rates are specified following the\nmethodology of the affine LIBOR models and are driven by the wide and flexible\nclass of affine processes. The affine property is preserved under forward\nmeasures, which allows us to derive Fourier pricing formulas for caps,\nswaptions and basis swaptions. A model specification with dependent LIBOR rates\nis developed, that allows for an efficient and accurate calibration to a system\nof caplet prices.\n"
    },
    {
        "paper_id": 1405.2459,
        "authors": "Dmitry Muravey",
        "title": "Interest rate models and Whittaker functions",
        "comments": "19 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I present the technique which can analyse some interest rate models:\nConstantinides-Ingersoll, CIR-model, geometric CIR and Geometric Brownian\nMotion. All these models have the unified structure of Whittaker function. The\nmain focus of this text is closed-form solutions of the zero-coupon bond value\nin these models. In text I emphasize the specific details of mathematical\nmethods of their determination such as Laplace transform and hypergeometric\nfunctions.\n"
    },
    {
        "paper_id": 1405.2609,
        "authors": "Nassim N. Taleb",
        "title": "Risk Neutral Option Pricing With Neither Dynamic Hedging nor Complete\n  Markets",
        "comments": null,
        "journal-ref": "European Financial Management 21 (2), 228-235,2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Proof that under simple assumptions, such as constraints of Put-Call Parity,\nthe probability measure for the valuation of a European option has the mean\nderived from the forward price which can, but does not have to be the\nrisk-neutral one, under any general probability distribution, bypassing the\nBlack-Scholes-Merton dynamic hedging argument, and without the requirement of\ncomplete markets and other strong assumptions. We confirm that the heuristics\nused by traders for centuries are both more robust, more consistent, and more\nrigorous than held in the economics literature. We also show that options can\nbe priced using infinite variance (finite mean) distributions.\n"
    },
    {
        "paper_id": 1405.2669,
        "authors": "Martin Keller-Ressel",
        "title": "Simple examples of pure-jump strict local martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present simple new examples of pure-jump strict local martingales. The\nexamples are constructed as exponentials of self-exciting affine Markov\nprocesses. We characterize the strict local martingale property of these\nprocesses by an integral criterion and by non-uniqueness of an associated\nordinary differential equation. Finally we show an alternative construction for\nour examples by an absolutely continuous measure change in the spirit of\n(Delbaen and Schachermayer, PTRF 1995).\n"
    },
    {
        "paper_id": 1405.2718,
        "authors": "Ivan Guo and Marek Rutkowski",
        "title": "Arbitrage Pricing of Multi-person Game Contingent Claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a class of financial contracts involving several parties by\nextending the notion of a two-person game option (see Kifer (2000)) to a\ncontract in which an arbitrary number of parties is involved and each of them\nis allowed to make a wide array of decisions at any time, not restricted to\nsimply `exercising the option'. The collection of decisions by all parties then\ndetermines the contract's settlement date as well as the terminal payoff for\neach party. We provide sufficient conditions under which a multi-person game\noption has a unique arbitrage price, which is additive with respect to any\npartition of the contract.\n"
    },
    {
        "paper_id": 1405.3202,
        "authors": "Hyejin Youn, Lu\\'is M. A. Bettencourt, Jos\\'e Lobo, Deborah Strumsky,\n  Horacio Samaniego, and Geoffrey B. West",
        "title": "The systematic structure and predictability of urban business diversity",
        "comments": "Press embargo in place until publication",
        "journal-ref": "J. R. Soc. Interface 13: 20150937 (2016)",
        "doi": "10.1098/rsif.2015.0937",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Understanding cities is central to addressing major global challenges from\nclimate and health to economic resilience. Although increasingly perceived as\nfundamental socio-economic units, the detailed fabric of urban economic\nactivities is only now accessible to comprehensive analyses with the\navailability of large datasets. Here, we study abundances of business\ncategories across U.S. metropolitan statistical areas to investigate how\ndiversity of economic activities depends on city size. A universal structure\ncommon to all cities is revealed, manifesting self-similarity in internal\neconomic structure as well as aggregated metrics (GDP, patents, crime). A\nderivation is presented that explains universality and the observed empirical\ndistribution. The model incorporates a generalized preferential attachment\nprocess with ceaseless introduction of new business types. Combined with\nscaling analyses for individual categories, the theory quantitatively predicts\nhow individual business types systematically change rank with city size,\nthereby providing a quantitative means for estimating their expected abundances\nas a function of city size. These results shed light on processes of economic\ndifferentiation with scale, suggesting a general structure for the growth of\nnational economies as integrated urban systems.\n"
    },
    {
        "paper_id": 1405.3225,
        "authors": "Ivan Medovikov",
        "title": "Can Analysts Predict Rallies Better Than Crashes?",
        "comments": "15 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the copula approach to study the structure of dependence between\nsell-side analysts' consensus recommendations and subsequent security returns,\nwith a focus on asymmetric tail dependence. We match monthly vintages of\nI/B/E/S recommendations for the period January to December 2011 with excess\nsecurity returns during six months following recommendation issue. Using a\nsymmetrized Joe-Clayton Copula (SJC) model we find evidence to suggest that\nanalysts can identify stocks that will substantially outperform, but not\nunderperform relative to the market, and that their predictive ability is\nconditional on recommendation changes.\n"
    },
    {
        "paper_id": 1405.3512,
        "authors": "Xiangyi Meng, Jian-Wei Zhang, Hong Guo",
        "title": "Quantum Brownian motion model for the stock market",
        "comments": null,
        "journal-ref": "Physica A 452 (2016) 281-288",
        "doi": "10.1016/j.physa.2016.02.026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is believed by the majority today that the efficient market hypothesis is\nimperfect because of market irrationality. Using the physical concepts and\nmathematical structures of quantum mechanics, we construct an econophysics\nframework for the stock market, based on which we analogously map massive\nnumbers of single stocks into a reservoir consisting of many quantum harmonic\noscillators and their stock index into a typical quantum open system--a quantum\nBrownian particle. In particular, the irrationality of stock transactions is\nquantitatively considered as the Planck constant within Heisenberg's\nuncertainty relationship of quantum mechanics in an analogous manner. We\nanalyze real stock data of Shanghai Stock Exchange of China and investigate\nfat-tail phenomena and non-Markovian behaviors of the stock index with the\nassistance of the quantum Brownian motion model, thereby interpreting and\nstudying the limitations of the classical Brownian motion model for the\nefficient market hypothesis from a new perspective of quantum open system\ndynamics.\n"
    },
    {
        "paper_id": 1405.3561,
        "authors": "Jean-Francois Chassagneux, Antoine Jacquier, Ivo Mihaylov",
        "title": "An explicit Euler scheme with strong rate of convergence for financial\n  SDEs with non-Lipschitz coefficients",
        "comments": "36 pages, 17 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the approximation of stochastic differential equations (SDEs)\nwith non-Lipschitz drift or diffusion coefficients. We present a modified\nexplicit Euler-Maruyama discretisation scheme that allows us to prove strong\nconvergence, with a rate. Under some regularity and integrability conditions,\nwe obtain the optimal strong error rate. We apply this scheme to SDEs widely\nused in the mathematical finance literature, including the\nCox-Ingersoll-Ross~(CIR), the 3/2 and the Ait-Sahalia models, as well as a\nfamily of mean-reverting processes with locally smooth coefficients. We\nnumerically illustrate the strong convergence of the scheme and demonstrate its\nefficiency in a multilevel Monte Carlo setting.\n"
    },
    {
        "paper_id": 1405.3566,
        "authors": "Yal\\c{c}in Aktar, Erik Taflin",
        "title": "A remark on smooth solutions to a stochastic control problem with a\n  power terminal cost function and stochastic volatilities",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Incomplete financial markets are considered, defined by a multi-dimensional\nnon-homogeneous diffusion process, being the direct sum of an It\\^{o} process\n(the price process), and another non-homogeneous diffusion process (the\nexogenous process, representing exogenous stochastic sources). The drift and\nthe diffusion matrix of the price process are functions of the time, the price\nprocess itself and the exogenous process.\n  In the context of such markets and for power utility functions, it is proved\nthat the stochastic control problem consisting of optimizing the expected\nutility of the terminal wealth, has a classical solution (i.e. $C^{1,2}$).\n  This result paves the way to a study of the optimal portfolio problem in\nincomplete forward variance stochastic volatility models, along the lines of\nRef: Ekeland et al.\n"
    },
    {
        "paper_id": 1405.3767,
        "authors": "Xin Dong and Harry Zheng",
        "title": "Intensity Process for a Pure Jump L\\'evy Structural Model with\n  Incomplete Information",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we discuss a credit risk model with a pure jump L\\'evy process\nfor the asset value and an unobservable random barrier. The default time is the\nfirst time when the asset value falls below the barrier. Using the\nindistinguishability of the intensity process and the likelihood process, we\nprove the existence of the intensity process of the default time and find its\nexplicit representation in terms of the distance between the asset value and\nits running minimal value. We apply the result to find the instantaneous credit\nspread process and illustrate it with a numerical example.\n"
    },
    {
        "paper_id": 1405.3769,
        "authors": "Ruodu Wang and Johanna F. Ziegel",
        "title": "Distortion Risk Measures and Elicitability",
        "comments": "The paper has been withdrawn by the authors because it is not\n  properly written to be cited by the research community",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss equivalent axiomatic characterizations of distortion risk\nmeasures, and give a novel and concise proof of the characterization of\nelicitable distortion risk measures. Elicitability has recently been discussed\nas a desirable criterion for risk measures, motivated by statistical\nconsiderations of forecasting. We reveal the mathematical conflict between the\nrequirements of elicitability and comonotonic additivity which intuitively\nexplains why only Value-at-Risk and the mean are elicitable distortion risk\nmeasures in a general sense.\n"
    },
    {
        "paper_id": 1405.3812,
        "authors": "Mikl\\'os R\\'asonyi and Jos\\'e G. Rodr\\'iguez-Villarreal",
        "title": "Optimal investment under behavioural criteria -- a dual approach",
        "comments": "Forthcoming in Banach Center Publications. Some errors have been\n  corrected, in particular in Assumption 2.3b",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete-time, generically incomplete market model and a\nbehavioural investor with power-like utility and distortion functions. The\nexistence of optimal strategies in this setting has been shown in a previous\npaper under certain conditions on the parameters of these power functions.\n  In the present paper we prove the existence of optimal strategies under a\ndifferent set of conditions on the parameters, identical to the ones which were\nshown to be necessary and sufficient in the Black-Scholes model.\n  Although there exists no natural dual problem for optimisation under\nbehavioural criteria (due to the lack of concavity), we will rely on techniques\nbased on the usual duality between attainable contingent claims and equivalent\nmartingale measures.\n"
    },
    {
        "paper_id": 1405.4079,
        "authors": "Tomasz R. Bielecki, Marek Rutkowski",
        "title": "Valuation and Hedging of Contracts with Funding Costs and\n  Collateralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The research presented in this work is motivated by recent papers by Brigo et\nal. (2011), Burgard and Kjaer (2009), Cr\\'epey (2012), Fujii and Takahashi\n(2010), Piterbarg (2010) and Pallavicini et al. (2012). Our goal is to provide\na sound theoretical underpinning for some results presented in these papers by\ndeveloping a unified framework for the non-linear approach to hedging and\npricing of OTC financial contracts. We introduce a systematic approach to\nvaluation and hedging in nonlinear markets, that is, in markets where cash\nflows of the financial contracts may depend on the hedging strategies. Our\nsystematic approach allows to identify primary sources of and quantify various\nadjustment to valuation and hedging, primarily the funding and liquidity\nadjustment and credit risk adjustment. We propose a way to define no-arbitrage\nin such nonlinear markets, and we provide conditions that imply absence of\narbitrage in some specific market trading models. Accordingly, we formulate a\nconcept of no-arbitrage price, and we provide relevant (non-linear) BSDE that\nproduces the no-arbitrage price in case when the contract's cash flows can be\nreplicated.\n"
    },
    {
        "paper_id": 1405.4301,
        "authors": "Stanislav Sobolevsky, Izabela Sitko, Sebastian Grauwin, Remi Tachet\n  des Combes, Bartosz Hawelka, Juan Murillo Arias, Carlo Ratti",
        "title": "Mining Urban Performance: Scale-Independent Classification of Cities\n  Based on Individual Economic Transactions",
        "comments": "10 pages, 7 figures, to be published in the proceedings of ASE\n  BigDataScience 2014 conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intensive development of urban systems creates a number of challenges for\nurban planners and policy makers in order to maintain sustainable growth.\nRunning efficient urban policies requires meaningful urban metrics, which could\nquantify important urban characteristics including various aspects of an actual\nhuman behavior. Since a city size is known to have a major, yet often\nnonlinear, impact on the human activity, it also becomes important to develop\nscale-free metrics that capture qualitative city properties, beyond the effects\nof scale. Recent availability of extensive datasets created by human activity\ninvolving digital technologies creates new opportunities in this area. In this\npaper we propose a novel approach of city scoring and classification based on\nquantitative scale-free metrics related to economic activity of city residents,\nas well as domestic and foreign visitors. It is demonstrated on the example of\nSpain, but the proposed methodology is of a general character. We employ a new\nsource of large-scale ubiquitous data, which consists of anonymized countrywide\nrecords of bank card transactions collected by one of the largest Spanish\nbanks. Different aspects of the classification reveal important properties of\nSpanish cities, which significantly complement the pattern that might be\ndiscovered with the official socioeconomic statistics.\n"
    },
    {
        "paper_id": 1405.4421,
        "authors": "Nicolas Perkowski, David J. Pr\\\"omel",
        "title": "Local times for typical price paths and pathwise Tanaka formulas",
        "comments": null,
        "journal-ref": "Electron. J. Probab. 20 (2015), no. 46, 1-15",
        "doi": "10.1214/EJP.v20-3534",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following a hedging based approach to model free financial mathematics, we\nprove that it should be possible to make an arbitrarily large profit by\ninvesting in those one-dimensional paths which do not possess local times. The\nlocal time is constructed from discrete approximations, and it is shown that it\nis $\\alpha$-H\\\"older continuous for all $\\alpha<1/2$. Additionally, we provide\nvarious generalizations of F\\\"ollmer's pathwise It\\^o formula.\n"
    },
    {
        "paper_id": 1405.4474,
        "authors": "Shiqi Song",
        "title": "Local martingale deflators for asset processes stopped at a default time\n  $S^\\tau$ or right before $S^{\\tau-}$",
        "comments": "The predictable multiplicative decompositions of Az\\'ema\n  supermartingales are reconsidered in this new version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\mathbb{F}\\subset \\mathbb{G}$ be two filtrations and $S$ be a\n$\\mathbb{F}$ semimartingale possessing a $\\mathbb{F}$ local martingale\ndeflator. Consider $\\tau$ a $\\mathbb{G}$ stopping time. We study the problem\nwhether $S^{\\tau-}$ or $S^{\\tau}$ can have $\\mathbb{G}$ local martingale\ndeflators. A suitable theoretical framework is set up in this paper, within\nwhich necessary/sufficient conditions for the problem to be solved have been\nproved. Under these conditions, we will construct $\\mathbb{G}$ local martingale\ndeflators for $S^{\\tau-}$ or for $S^{\\tau}$. Among others, it is proved that\n$\\mathbb{G}$ local martingale deflators are multiples of $\\mathbb{F}$ local\nmartingale deflators, with a multiplicator coming from the multiplicative\ndecomposition of the Az\\'ema supermartingale of $\\tau$. The proofs of the\nnecessary/sufficient conditions require various results to be established about\nAz\\'ema supermartingale, about local martingale deflator, about filtration\nenlargement, which are interesting in themselves.\n  Our study is based on a filtration enlargement setting. For applications, it\nis important to have a method to infer the existence of such setting from the\nknowledge of the market information. This question is discussed at the end of\nthe paper.\n"
    },
    {
        "paper_id": 1405.449,
        "authors": "Xiangyi Meng, Jian-Wei Zhang, Jingjing Xu, Hong Guo",
        "title": "Quantum spatial-periodic harmonic model for daily price-limited stock\n  markets",
        "comments": "8 pages, 9 figures",
        "journal-ref": "Physica A 438 (2015) 154-160",
        "doi": "10.1016/j.physa.2015.06.041",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the behavior of stocks in daily price-limited stock markets by\npurposing a quantum spatial-periodic harmonic model. The stock price is\npresumed to oscillate and damp in a quantum spatial-periodic harmonic\noscillator potential well. Complicated non-linear relations including\ninter-band positive correlation and intra-band negative correlation between the\nvolatility and the trading volume of stocks are derived by considering the\nenergy band structure of the model. The validity of price limitation is then\nexamined and abnormal phenomena of a price-limited stock market (Shanghai Stock\nExchange) of China are studied by applying our quantum model.\n"
    },
    {
        "paper_id": 1405.4498,
        "authors": "Pavel Ciaian, Miroslava Rajcaniova, d'Artis Kancs",
        "title": "The Economics of BitCoin Price Formation",
        "comments": "Key words: BitCoin, exchange rate, supply-demand fundamentals,\n  financial indicators, attractiveness JEL classification: E31; E42; G12",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyses the relationship between BitCoin price and supply-demand\nfundamentals of BitCoin, global macro-financial indicators and BitCoin\nattractiveness for investors. Using daily data for the period 2009-2014 and\napplying time-series analytical mechanisms, we find that BitCoin market\nfundamentals and BitCoin attractiveness for investors have a significant impact\non BitCoin price. Our estimates do not support previous findings that the\nmacro-financial developments are driving BitCoin price.\n"
    },
    {
        "paper_id": 1405.4537,
        "authors": "Terry Lyons",
        "title": "Rough paths, Signatures and the modelling of functions on streams",
        "comments": "To appear in the Proceedings of the International Congress of\n  Mathematicians 2014, Korea",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Rough path theory is focused on capturing and making precise the interactions\nbetween highly oscillatory and non-linear systems. It draws on the analysis of\nLC Young and the geometric algebra of KT Chen. The concepts and the uniform\nestimates, have widespread application and have simplified proofs of basic\nquestions from the large deviation theory and extended Ito's theory of SDEs;\nthe recent applications contribute to (Graham) automated recognition of Chinese\nhandwriting and (Hairer) formulation of appropriate SPDEs to model randomly\nevolving interfaces. At the heart of the mathematics is the challenge of\ndescribing a smooth but potentially highly oscillatory and vector valued path\n$x_{t}$ parsimoniously so as to effectively predict the response of a nonlinear\nsystem such as $dy_{t}=f(y_{t})dx_{t}$, $y_{0}=a$. The Signature is a\nhomomorphism from the monoid of paths into the grouplike elements of a closed\ntensor algebra. It provides a graduated summary of the path $x$. Hambly and\nLyons have shown that this non-commutative transform is faithful for paths of\nbounded variation up to appropriate null modifications. Among paths of bounded\nvariation with given Signature there is always a unique shortest\nrepresentative. These graduated summaries or features of a path are at the\nheart of the definition of a rough path; locally they remove the need to look\nat the fine structure of the path. Taylor's theorem explains how any smooth\nfunction can, locally, be expressed as a linear combination of certain special\nfunctions (monomials based at that point). Coordinate iterated integrals form a\nmore subtle algebra of features that can describe a stream or path in an\nanalogous way; they allow a definition of rough path and a natural linear\n\"basis\" for functions on streams that can be used for machine learning.\n"
    },
    {
        "paper_id": 1405.4716,
        "authors": "Zura Kakushadze",
        "title": "Combining Alpha Streams with Costs",
        "comments": "21 pages; minor misprints corrected; to appear in The Journal of Risk",
        "journal-ref": "The Journal of Risk 17(3) (2015) 57-78",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss investment allocation to multiple alpha streams traded on the same\nexecution platform with internal crossing of trades and point out differences\nwith allocating investment when alpha streams are traded on separate execution\nplatforms with no crossing. First, in the latter case allocation weights are\nnon-negative, while in the former case they can be negative. Second, the\neffects of both linear and nonlinear (impact) costs are different in these two\ncases due to turnover reduction when the trades are crossed. Third, the\nturnover reduction depends on the universe of traded alpha streams, so if some\nalpha streams have zero allocations, turnover reduction needs to be recomputed,\nhence an iterative procedure. We discuss an algorithm for finding allocation\nweights with crossing and linear costs. We also discuss a simple approximation\nwhen nonlinear costs are added, making the allocation problem tractable while\nstill capturing nonlinear portfolio capacity bound effects. We also define\n\"regression with costs\" as a limit of optimization with costs, useful in\noften-occurring cases with singular alpha covariance matrix.\n"
    },
    {
        "paper_id": 1405.4905,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Andreas H. Hamel, Birgit Rudloff",
        "title": "Set-valued shortfall and divergence risk measures",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance 20 (5)\n  1750026 (2017)",
        "doi": "10.1142/S0219024917500261",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk measures for multivariate financial positions are studied in a\nutility-based framework. Under a certain incomplete preference relation,\nshortfall and divergence risk measures are defined as the optimal values of\nspecific set minimization problems. The dual relationship between these two\nclasses of multivariate risk measures is constructed via a recent Lagrange\nduality for set optimization. In particular, it is shown that a shortfall risk\nmeasure can be written as an intersection over a family of divergence risk\nmeasures indexed by a scalarization parameter. Examples include set-valued\nversions of the entropic risk measure and the average value at risk. As a\nsecond step, the minimization of these risk measures subject to trading\nopportunities is studied in a general convex market in discrete time. The\noptimal value of the minimization problem, called the market risk measure, is\nalso a set-valued risk measure. A dual representation for the market risk\nmeasure that decomposes the effects of the original risk measure and the\nfrictions of the market is proved.\n"
    },
    {
        "paper_id": 1405.5,
        "authors": "Yue-Hua Dai (ECUST), Wen-Jie Xie (ECUST), Zhi-Qiang Jiang (ECUST),\n  George J. Jiang (WSU), Wei-Xing Zhou (ECUST)",
        "title": "Correlation structure and principal components in global crude oil\n  market",
        "comments": "13 Latex pages including 7 figures and 1 table",
        "journal-ref": "Empirical Economics 51 (4), 1501-1519 (2016)",
        "doi": "10.1007/s00181-015-1057-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates the correlation structure of the global crude oil\nmarket using the daily returns of 71 oil price time series across the world\nfrom 1992 to 2012. We identify from the correlation matrix six clusters of time\nseries exhibiting evident geographical traits, which supports Weiner's (1991)\nregionalization hypothesis of the global oil market. We find that intra-cluster\npairs of time series are highly correlated while inter-cluster pairs have\nrelatively low correlations. Principal component analysis shows that most\neigenvalues of the correlation matrix locate outside the prediction of the\nrandom matrix theory and these deviating eigenvalues and their corresponding\neigenvectors contain rich economic information. Specifically, the largest\neigenvalue reflects a collective effect of the global market, other four\nlargest eigenvalues possess a partitioning function to distinguish the six\nclusters, and the smallest eigenvalues highlight the pairs of time series with\nthe largest correlation coefficients. We construct an index of the global oil\nmarket based on the eigenfortfolio of the largest eigenvalue, which evolves\nsimilarly as the average price time series and has better performance than the\nbenchmark $1/N$ portfolio under the buy-and-hold strategy.\n"
    },
    {
        "paper_id": 1405.523,
        "authors": "Christian Bayer and Ulrich Horst and Jinniao Qiu",
        "title": "A Functional Limit Theorem for Limit Order Books with State Dependent\n  Price Dynamics",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic model for the dynamics of the two-sided limit order\nbook (LOB). Our model is flexible enough to allow for a dependence of the price\ndynamics on volumes. For the joint dynamics of best bid and ask prices and the\nstanding buy and sell volume densities, we derive a functional limit theorem,\nwhich states that our LOB model converges in distribution to a fully coupled\nSDE-SPDE system when the order arrival rates tend to infinity and the impact of\nan individual order arrival on the book as well as the tick size tends to zero.\nThe SDE describes the bid/ask price dynamics while the SPDE describes the\nvolume dynamics.\n"
    },
    {
        "paper_id": 1405.5294,
        "authors": "Pavel V. Shevchenko and Pierre Del Moral",
        "title": "Valuation of Barrier Options using Sequential Monte Carlo",
        "comments": "Journal of Computational Finance (2015)",
        "journal-ref": "Journal of Computational Finance 20(4), pp. 107-135, 2017",
        "doi": "10.21314/JCF.2016.324",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sequential Monte Carlo (SMC) methods have successfully been used in many\napplications in engineering, statistics and physics. However, these are seldom\nused in financial option pricing literature and practice. This paper presents\nSMC method for pricing barrier options with continuous and discrete monitoring\nof the barrier condition. Under the SMC method, simulated asset values rejected\ndue to barrier condition are re-sampled from asset samples that do not breach\nthe barrier condition improving the efficiency of the option price estimator;\nwhile under the standard Monte Carlo many simulated asset paths can be rejected\nby the barrier condition making it harder to estimate option price accurately.\nWe compare SMC with the standard Monte Carlo method and demonstrate that the\nextra effort to implement SMC when compared with the standard Monte Carlo is\nvery little while improvement in price estimate can be significant. Both\nmethods result in unbiased estimators for the price converging to the true\nvalue as $1/\\sqrt{M}$, where $M$ is the number of simulations (asset paths).\nHowever, the variance of SMC estimator is smaller and does not grow with the\nnumber of time steps when compared to the standard Monte Carlo. In this paper\nwe demonstrate that SMC can successfully be used for pricing barrier options.\nSMC can also be used for pricing other exotic options and also for cases with\nmany underlying assets and additional stochastic factors such as stochastic\nvolatility; we provide general formulas and references.\n"
    },
    {
        "paper_id": 1405.5695,
        "authors": "Rickard Nyman and Paul Ormerod",
        "title": "Big Data, Socio-Psychological Theory, Algorithmic Text Analysis and\n  Predicting the Michigan Consumer Sentiment Index",
        "comments": "10 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an exercise of using Big Data to predict the Michigan Consumer\nSentiment Index, a widely used indicator of the state of confidence in the US\neconomy. We carry out the exercise from a pure ex ante perspective. We use the\nmethodology of algorithmic text analysis of an archive of brokers' reports over\nthe period June 2010 through June 2013. The search is directed by the\nsocial-psychological theory of agent behaviour, namely conviction narrative\ntheory. We compare one month ahead forecasts generated this way over a 15 month\nperiod with the forecasts reported for the consensus predictions of Wall Street\neconomists. The former give much more accurate predictions, getting the\ndirection of change correct on 12 of the 15 occasions compared to only 7 for\nthe consensus predictions. We show that the approach retains significant\npredictive power even over a four month ahead horizon.\n"
    },
    {
        "paper_id": 1405.5805,
        "authors": "Alessio Emanuele Biondo, Alessandro Pluchino, Andrea Rapisarda",
        "title": "Micro and Macro Benefits of Random Investments in Financial Markets",
        "comments": "23 pages, 14 figures, This manuscript was prepared for submission to\n  Contemporary Physics",
        "journal-ref": "Contemporary Physics, Volume 55, Issue 4, Pages 318-334 (2014)",
        "doi": "10.1080/00107514.2014.929308",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, making use of recent statistical physics techniques and\nmodels, we address the specific role of randomness in financial markets, both\nat the micro and the macro level. In particular, we review some recent results\nobtained about the effectiveness of random strategies of investment, compared\nwith some of the most used trading strategies for forecasting the behavior of\nreal financial indexes. We also push forward our analysis by means of a\nSelf-Organized Criticality model, able to simulate financial avalanches in\ntrading communities with different network topologies, where a Pareto-like\npower law behavior of wealth spontaneously emerges. In this context, we present\nnew findings and suggestions for policies based on the effects that random\nstrategies can have in terms of reduction of dangerous financial extreme\nevents, i.e. bubbles and crashes.\n"
    },
    {
        "paper_id": 1405.5842,
        "authors": "Angelos Dassios, Xin Dong",
        "title": "Stationarity of Bivariate Dynamic Contagion Processes",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Bivariate Dynamic Contagion Processes (BDCP) are a broad class of\nbivariate point processes characterized by the intensities as a general class\nof piecewise deterministic Markov processes. The BDCP describes a rich dynamic\nstructure where the system is under the influence of both external and internal\nfactors modelled by a shot-noise Cox process and a generalized Hawkes process\nrespectively. In this paper we mainly address the stationarity issue for the\nBDCP, which is important in applications. We investigate the stationary\ndistribution by applying the the Markov theory on the branching system\napproximation representation of the BDCP. We find the condition under which\nthere exists a unique stationary distribution of the BDCP intensity and the\nresulting BDCP has stationary increments. Moments of the stationary intensity\nare provided by using the Markov property.\n"
    },
    {
        "paper_id": 1405.5939,
        "authors": "Hai-Chuan Xu (TJU), Wei Zhang (TJU), Xiong Xiong (TJU), Wei-Xing Zhou\n  (ECUST)",
        "title": "Wealth share analysis with \"fundamentalist/chartist\" heterogeneous\n  agents",
        "comments": null,
        "journal-ref": "Abstract and Applied Analysis 2014, 328498 (2014)",
        "doi": "10.1155/2014/328498",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build a multiassets heterogeneous agents model with fundamentalists and\nchartists, who make investment decisions by maximizing the constant relative\nrisk aversion utility function. We verify that the model can reproduce the main\nstylized facts in real markets, such as fat-tailed return distribution and\nlong-term memory in volatility. Based on the calibrated model, we study the\nimpacts of the key strategies' parameters on investors' wealth shares. We find\nthat, as chartists' exponential moving average periods increase, their wealth\nshares also show an increasing trend. This means that higher memory length can\nhelp to improve their wealth shares. This effect saturates when the exponential\nmoving average periods are sufficiently long. On the other hand, the mean\nreversion parameter has no obvious impacts on wealth shares of either type of\ntraders. It suggests that no matter whether fundamentalists take moderate\nstrategy or aggressive strategy on the mistake of stock prices, it will have no\ndifferent impact on their wealth shares in the long run.\n"
    },
    {
        "paper_id": 1405.6027,
        "authors": "James B. Glattfelder, Thomas Bisig and Richard B. Olsen",
        "title": "R&D Strategy Document",
        "comments": "December 2010 paper by the Olsen Ltd. research group, 22 pages, 8\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We outline what we believe are the prerequisites and building-blocks for\nsuccessfully devising trading models and other financial applications based on\na complex systems perspective.\n"
    },
    {
        "paper_id": 1405.6047,
        "authors": "Marcello Rambaldi, Paris Pennesi, Fabrizio Lillo",
        "title": "Modeling FX market activity around macroeconomic news: a Hawkes process\n  approach",
        "comments": "16 pages, 12 figures. Accepted for publication in Physical Review E\n  (2015)",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.91.012819",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a Hawkes model approach to foreign exchange market in which the\nhigh frequency price dynamics is affected by a self exciting mechanism and an\nexogenous component, generated by the pre-announced arrival of macroeconomic\nnews. By focusing on time windows around the news announcement, we find that\nthe model is able to capture the increase of trading activity after the news,\nboth when the news has a sizeable effect on volatility and when this effect is\nnegligible, either because the news in not important or because the\nannouncement is in line with the forecast by analysts. We extend the model by\nconsidering non-causal effects, due to the fact that the existence of the news\n(but not its content) is known by the market before the announcement.\n"
    },
    {
        "paper_id": 1405.6111,
        "authors": "Andrey Itkin",
        "title": "Splitting and Matrix Exponential approach for jump-diffusion models with\n  Inverse Normal Gaussian, Hyperbolic and Meixner jumps",
        "comments": "32 pages, 7 tables. arXiv admin note: substantial text overlap with\n  arXiv:1304.3159",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is a further extension of the method proposed in Itkin, 2014 as\napplied to another set of jump-diffusion models: Inverse Normal Gaussian,\nHyperbolic and Meixner. To solve the corresponding PIDEs we accomplish few\nsteps. First, a second-order operator splitting on financial processes\n(diffusion and jumps) is applied to these PIDEs. To solve the diffusion\nequation, we use standard finite-difference methods. For the jump part, we\ntransform the jump integral into a pseudo-differential operator and construct\nits second order approximation on a grid which supersets the grid that we used\nfor the diffusion part. The proposed schemes are unconditionally stable in time\nand preserve positivity of the solution which is computed either via a matrix\nexponential, or via P'ade approximation of the matrix exponent. Various\nnumerical experiments are provided to justify these results.\n"
    },
    {
        "paper_id": 1405.64,
        "authors": "Matthew O. Jackson and Stephen M. Nei",
        "title": "Networks of Military Alliances, Wars, and International Trade",
        "comments": "39 pages, 14 figures",
        "journal-ref": null,
        "doi": "10.1073/pnas.1520970112",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the role of networks of alliances in preventing (multilateral)\ninterstate wars. We first show that, in the absence of international trade, no\nnetwork of alliances is peaceful and stable. We then show that international\ntrade induces peaceful and stable networks: trade increases the density of\nalliances so that countries are less vulnerable to attack and also reduces\ncountries' incentives to attack an ally. We present historical data on wars and\ntrade, noting that the dramatic drop in interstate wars since 1950, and\naccompanying densification and stabilization of alliances, are consistent with\nthe model but not other prominent theories.\n"
    },
    {
        "paper_id": 1405.6514,
        "authors": "Martino Bardi, Annalisa Cesaroni, Andrea Scotti",
        "title": "Convergence in Multiscale Financial Models with Non-Gaussian Stochastic\n  Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider stochastic control systems affected by a fast mean reverting\nvolatility $Y(t)$ driven by a pure jump L\\'evy process. Motivated by a large\nliterature on financial models, we assume that $Y(t)$ evolves at a faster time\nscale $\\frac{t}{\\varepsilon}$ than the assets, and we study the asymptotics as\n$\\varepsilon\\to 0$. This is a singular perturbation problem that we study\nmostly by PDE methods within the theory of viscosity solutions.\n"
    },
    {
        "paper_id": 1405.6677,
        "authors": "Tatiana Labopin-Richard (IMT), Fabrice Gamboa (IMT), Aur\\'elien\n  Garivier (IMT), Bertrand Iooss (GdR MASCOT-NUM)",
        "title": "Bregman superquantiles. Estimation methods and applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we extend some quantities introduced in \"Optimization of\nconditional value-at-risk\" of R.T Rockafellar and S. Uryasev to the case where\nthe proximity between real numbers is measured by using a Bregman divergence.\nThis leads to the definition of the Bregman superquantile. Axioms of a coherent\nmeasure of risk discussed in \"Coherent approches to risk in optimization under\nuncertainty\" of R.T Rockafellar are studied in the case of Bregman\nsuperquantile. Furthermore, we deal with asymptotic properties of a Monte Carlo\nestimator of the Bregman superquantile.\n"
    },
    {
        "paper_id": 1405.6905,
        "authors": "Jean-David Fermanian, Hassan Malongo",
        "title": "On the stationarity of Dynamic Conditional Correlation models",
        "comments": "Revised version: correction of typos, reduction of the number of\n  figures, etc",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide conditions for the existence and the unicity of strictly\nstationary solutions of the usual Dynamic Conditional Correlation GARCH models\n(DCC-GARCH). The proof is based on Tweedie's (1988) criteria, after having\nrewritten DCC-GARCH models as nonlinear Markov chains. Moreover, we study the\nexistence of their finite moments.\n"
    },
    {
        "paper_id": 1405.699,
        "authors": "Sergey A. Kamenshchikov",
        "title": "Transport catastrophe analysis as an alternative to a fractal\n  description: theory and application to financial crisis time series",
        "comments": "13 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  The goal of this investigation was to overcome limitations of a persistency\nanalysis, introduced by Benoit Mandelbrot for fractal Brownian processes:\nnondifferentiability, Brownian nature of process and a linear memory measure.\nWe have extended a sense of a Hurst factor by consideration of a phase\ndiffusion power law. It was shown that pre-catastrophic stabilization as an\nindicator of bifurcation leads to a new minimum of momentary phase diffusion,\nwhile bifurcation causes an increase of the momentary transport. Basic\nconclusions of a diffusive analysis have been compared to the Lyapunov\nstability model. An extended Reynolds parameter has been introduces as an\nindicator of phase transition. A combination of diffusive and Reynolds analysis\nhas been applied for a description of a time series of Dow Jones Industrial\nweekly prices for a world financial crisis of 2007-2009. Diffusive and Reynolds\nparameters shown an extreme values in October 2008 when a mortgage crisis was\nfixed. A combined R/D description allowed distinguishing of short-memory and\nlong memory shifts of a market evolution. It was stated that a systematic large\nscale failure of a financial system has begun in October 2008 and started\nfading in February 2009.\n"
    },
    {
        "paper_id": 1405.7342,
        "authors": "Lorenzo Mercuri and Fabio Bellini",
        "title": "Option Pricing in a Dynamic Variance-Gamma Model",
        "comments": null,
        "journal-ref": "Journal of Financial Decision Making (2011) vol. 7, n.1 pp. 37-51\n  - ISSN: 1790-4870",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a discrete time stochastic volatility model in which the\nconditional distribution of the logreturns is a Variance-Gamma, that is a\nnormal variance-mean mixture with Gamma mixing density. We assume that the\nGamma mixing density is time varying and follows an affine Garch model, trying\nto capture persistence of volatility shocks and also higher order conditional\ndynamics in a parsimonious way. We select an equivalent martingale measure by\nmeans of the conditional Esscher transform as in Buhlmann et al. (1996) and\nshow that this change of measure leads to a similar dynamics of the mixing\ndistribution. The model admits a recursive procedure for the computation of the\ncharacteristic function of the terminal logprice, thus allowing semianalytical\npricing as in Heston and Nandi (2000). From an empirical point of view, we\ncheck the ability of this model to calibrate SPX option data and we compare it\nwith the Heston and Nandi (2000) model and with the Christoffersen, Heston and\nJacobs (2006) model, that is based on Inverse Gaussian innovations. Moreover,\nwe provide a detailed comparison with several variants of the Heston and Nandi\nmodel that shows the superiority of the Variance-Gamma innovations also from\nthe point of view of historical MLE estimation.\n"
    },
    {
        "paper_id": 1405.7603,
        "authors": "Edit Rroji, Lorenzo Mercuri",
        "title": "Mixed Tempered Stable distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new parametric distribution, the Mixed Tempered\nStable. It has the same structure of the Normal Variance Mean Mixtures but the\nnormality assumption leaves place to a semi-heavy tailed distribution. We show\nthat, by choosing appropriately the parameters of the distribution and under\nthe concrete specification of the mixing random variable, it is possible to\nobtain some well-known distributions as special cases.\n  We employ the Mixed Tempered Stable distribution which has many attractive\nfeatures for modeling univariate returns. Our results suggest that it is enough\nflexible to accomodate different density shapes. Furthermore, the analysis\napplied to statistical time series shows that our approach provides a better\nfit than competing distributions that are common in the practice of finance.\n"
    },
    {
        "paper_id": 1405.7611,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "VAR and ES/CVAR Dependence on data cleaning and Data Models: Analysis\n  and Resolution",
        "comments": "22 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical (Stressed-) Value-at-Risk ((S)VAR), and Expected Shortfall (ES),\nare widely used risk measures in regulatory capital and Initial Margin, i.e.\nfunding, computations. However, whilst the definitions of VAR and ES are\nunambiguous, they depend on input distributions that are data-cleaning- and\nData-Model-dependent. We quantify the scale of these effects from USD CDS\n(2004--2014), and from USD interest rates (1989--2014, single-curve setup\nbefore 2004, multi-curve setup after 2004), and make two standardisation\nproposals: for data; and for Data-Models. VAR and ES are required for lifetime\nportfolio calculations, i.e. collateral calls, which cover a wide range of\nmarket states. Hence we need standard, i.e. clean, complete, and common (i.e.\nidentical for all banks), market data also covering this wide range of market\nstates. This data is historically incomplete and not clean hence data\nstandardization is required. Stressed VAR and ES require moving market\nmovements during a past (usually not recent) window to current, and future,\nmarket states. All choices (e.g. absolute difference, relative, relative scaled\nby some function of market states) implicitly define a Data Model for\ntransformation of extreme market moves (recall that 99th percentiles are\ntypical, and the behaviour of the rest is irrelevant). Hence we propose\nstandard Data Models. These are necessary because different banks have\ndifferent stress windows. Where there is no data, or a requirement for\nsimplicity, we propose standard lookup tables (one per window, etc.). Without\nthis standardization of data and Data Models we demonstrate that VAR and ES are\ncomplex derivatives of subjective choices.\n"
    },
    {
        "paper_id": 1405.7747,
        "authors": "Fabio Dercole and Davide Radi",
        "title": "Does the \"uptick rule\" stabilize the stock market? Insights from\n  Adaptive Rational Equilibrium Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the effects of the \"uptick rule\" (a short selling\nregulation formally known as rule 10a-1) by means of a simple stock market\nmodel, based on the ARED (adaptive rational equilibrium dynamics) modeling\nframework, where heterogeneous and adaptive beliefs on the future prices of a\nrisky asset were first shown to be responsible for endogenous price\nfluctuations.\n  The dynamics of stock prices generated by the model, with and without the\nuptick-rule restriction, are analyzed by pairing the classical fundamental\nprediction with beliefs based on both linear and nonlinear technical analyses.\nThe comparison shows a reduction of downward price movements of undervalued\nshares when the short selling restriction is imposed. This gives evidence that\nthe uptick rule meets its intended objective. However, the effects of the short\nselling regulation fade when the intensity of choice to switch trading\nstrategies is high. The analysis suggests possible side effects of the\nregulation on price dynamics.\n"
    },
    {
        "paper_id": 1405.7801,
        "authors": "Han Feng, David Hobson",
        "title": "Gambling in contests with random initial law",
        "comments": "Published at http://dx.doi.org/10.1214/14-AAP1088 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2016, Vol. 26, No. 1, 186-215",
        "doi": "10.1214/14-AAP1088",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a variant of the contest model introduced in Seel and\nStrack [J. Econom. Theory 148 (2013) 2033-2048]. In the Seel-Strack contest,\neach agent or contestant privately observes a Brownian motion, absorbed at\nzero, and chooses when to stop it. The winner of the contest is the agent who\nstops at the highest value. The model assumes that all the processes start from\na common value $x_0>0$ and the symmetric Nash equilibrium is for each agent to\nutilise a stopping rule which yields a randomised value for the stopped\nprocess. In the two-player contest, this randomised value has a uniform\ndistribution on $[0,2x_0]$. In this paper, we consider a variant of the problem\nwhereby the starting values of the Brownian motions are independent,\nnonnegative random variables that have a common law $\\mu$. We consider a\ntwo-player contest and prove the existence and uniqueness of a symmetric Nash\nequilibrium for the problem. The solution is that each agent should aim for the\ntarget law $\\nu$, where $\\nu$ is greater than or equal to $\\mu$ in convex\norder; $\\nu$ has an atom at zero of the same size as any atom of $\\mu$ at zero,\nand otherwise is atom free; on $(0,\\infty)$ $\\nu$ has a decreasing density; and\nthe density of $\\nu$ only decreases at points where the convex order constraint\nis binding.\n"
    },
    {
        "paper_id": 1406.0044,
        "authors": "Zura Kakushadze",
        "title": "Can Turnover Go to Zero?",
        "comments": "28 pages; minor misprints corrected; to appear in Journal of\n  Derivatives & Hedge Funds",
        "journal-ref": "Journal of Derivatives & Hedge Funds 20(3) (2014) 157-176",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Internal crossing of trades between multiple alpha streams results in\nportfolio turnover reduction. Turnover reduction can be modeled using the\ncorrelation structure of the alpha streams. As more and more alphas are added,\ngenerally turnover reduces. In this note we use a factor model approach to\naddress the question of whether the turnover goes to zero or a finite limit as\nthe number of alphas N goes to infinity. We argue that the limiting turnover\nvalue is determined by the number of alpha clusters F, not the number of alphas\nN. This limiting value behaves according to the \"power law\" ~ F^(-3/2). So, to\nachieve zero limiting turnover, the number of alpha clusters must go to\ninfinity along with the number of alphas. We further argue on general grounds\nthat, if the number of underlying tradable instruments is finite, then the\nturnover cannot go to zero, which implies that the number of alpha clusters\nalso appears to be finite.\n"
    },
    {
        "paper_id": 1406.0055,
        "authors": "Ren\\'e Aid, Salvatore Federico, Huy\\^en Pham, Bertrand Villeneuve",
        "title": "Explicit investment rules with time-to-build and uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish explicit socially optimal rules for an irreversible investment\ndeci- sion with time-to-build and uncertainty. Assuming a price sensitive\ndemand function with a random intercept, we provide comparative statics and\neconomic interpreta- tions for three models of demand (arithmetic Brownian,\ngeometric Brownian, and the Cox-Ingersoll-Ross). Committed capacity, that is,\nthe installed capacity plus the in- vestment in the pipeline, must never drop\nbelow the best predictor of future demand, minus two biases. The discounting\nbias takes into account the fact that investment is paid upfront for future\nuse; the precautionary bias multiplies a type of risk aversion index by the\nlocal volatility. Relying on the analytical forms, we discuss in detail the\neconomic effects.\n"
    },
    {
        "paper_id": 1406.007,
        "authors": "X.F. Jiang, T.T. Chen, and B. Zheng",
        "title": "Structure of local interactions in complex financial dynamics",
        "comments": "15 pages, 10 figures, accepted by Scientific Reports",
        "journal-ref": "Scientific Reports,4 (2014) 5321",
        "doi": "10.1038/srep05321",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  With the network methods and random matrix theory, we investigate the\ninteraction structure of communities in financial markets. In particular, based\non the random matrix decomposition, we clarify that the local interactions\nbetween the business sectors (subsectors) are mainly contained in the sector\nmode. In the sector mode, the average correlation inside the sectors is\npositive, while that between the sectors is negative. Further, we explore the\ntime evolution of the interaction structure of the business sectors, and\nobserve that the local interaction structure changes dramatically during a\nfinancial bubble or crisis.\n"
    },
    {
        "paper_id": 1406.0077,
        "authors": "Johan GB Beumee, Chris Cormack, Peyman Khorsand, Manish Patel",
        "title": "Path Diffusion, Part I",
        "comments": "28 pages, 10 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the position (state) distribution of the single step\nbinomial (multi-nomial) process on a discrete state / time grid under the\nassumption that the velocity process rather than the state process is\nMarkovian. In this model the particle follows a simple multi-step process in\nvelocity space which also preserves the proper state equation of motion. Many\nnumerical numerical examples of this process are provided. For a smaller grid\nthe probability construction converges into a correlated set of probabilities\nof hyperbolic functions for each velocity at each state point. It is shown that\nthe two dimensional process can be transformed into a Telegraph equation and\nvia transformation into a Klein-Gordon equation if the transition rates are\nconstant. In the last Section there is an example of multi-dimensional\nhyperbolic partial differential equation whose numerical average satisfies\nNewton's equation. There is also a momentum measure provided both for the\ntwo-dimensional case as for the multi-dimensional rate matrix.\n"
    },
    {
        "paper_id": 1406.0209,
        "authors": "Thomas Kruse and Philipp Strack",
        "title": "An inverse optimal stopping problem for diffusion processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $X$ be a one-dimensional diffusion and let\n$g\\colon[0,T]\\times\\mathbb{R}\\to\\mathbb{R}$ be a payoff function depending on\ntime and the value of $X$. The paper analyzes the inverse optimal stopping\nproblem of finding a time-dependent function $\\pi:[0,T]\\to\\mathbb{R}$ such that\na given stopping time $\\tau^{\\star}$ is a solution of the stopping problem\n$\\sup_{\\tau}\\mathbb{E}\\left[g(\\tau,X_{\\tau})+\\pi(\\tau)\\right]\\,.$ Under\nregularity and monotonicity conditions, there exists a solution $\\pi$ if and\nonly if $\\tau^{\\star}$ is the first time when $X$ exceeds a time-dependent\nbarrier $b$, i.e. $\\tau^{\\star}=\\inf\\left\\{ t\\ge0\\,|\\,X_{t}\\ge b(t)\\right\\}\n\\,.$ We prove uniqueness of the solution $\\pi$ and derive a closed form\nrepresentation. The representation is based on an auxiliary process which is a\nversion of the original diffusion $X$ reflected at $b$ towards the continuation\nregion. The results lead to a new integral equation characterizing the stopping\nboundary $b$ of the stopping problem\n$\\sup_{\\tau}\\mathbb{E}\\left[g(\\tau,X_{\\tau})\\right]$.\n"
    },
    {
        "paper_id": 1406.0268,
        "authors": "Ladislav Kristoufek",
        "title": "What are the main drivers of the Bitcoin price? Evidence from wavelet\n  coherence analysis",
        "comments": "19 pages, 5 figures",
        "journal-ref": "PLoS ONE 10(4): e0123923, 2015",
        "doi": "10.1371/journal.pone.0123923",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Bitcoin has emerged as a fascinating phenomenon of the financial markets.\nWithout any central authority issuing the currency, it has been associated with\ncontroversy ever since its popularity and public interest reached high levels.\nHere, we contribute to the discussion by examining potential drivers of Bitcoin\nprices ranging from fundamental to speculative and technical sources as well as\na potential influence of the Chinese market. The evolution of the relationships\nis examined in both time and frequency domains utilizing the continuous\nwavelets framework so that we comment on development of the interconnections in\ntime but we can also distinguish between short-term and long-term connections.\n"
    },
    {
        "paper_id": 1406.0389,
        "authors": "J.D. Opdyke",
        "title": "Estimating Operational Risk Capital with Greater Accuracy, Precision,\n  and Robustness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The largest US banks are required by regulatory mandate to estimate the\noperational risk capital they must hold using an Advanced Measurement Approach\n(AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution\nApproach (LDA) which defines the aggregate loss distribution as the convolution\nof a frequency and a severity distribution representing the number and\nmagnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th\npercentile) estimate of this annual loss distribution. In practice, the\nseverity distribution drives the capital estimate, which is essentially a very\nhigh quantile of the estimated severity distribution. Unfortunately, because\nthe relevant severities are heavy-tailed AND the quantiles being estimated are\nso high, VaR always appears to be a convex function of the severity parameters,\ncausing all widely-used estimators to generate biased capital estimates\n(apparently) due to Jensen's Inequality. The observed capital inflation is\nsometimes enormous, even at the unit-of-measure (UoM) level (even billions\nUSD). Herein I present an estimator of capital that essentially eliminates this\nupward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with\nthe regulatory intent of the LDA framework than implementations that fail to\nmitigate this bias. RCE also notably increases the precision of the capital\nestimate and consistently increases its robustness to violations of the i.i.d.\ndata presumption (which are endemic to operational risk loss event data). So\nwith greater capital accuracy, precision, and robustness, RCE lowers capital\nrequirements at both the UoM and enterprise levels, increases capital stability\nfrom quarter to quarter, ceteris paribus, and does both while more accurately\nand precisely reflecting regulatory intent. RCE is straightforward to implement\nusing any major statistical software package.\n"
    },
    {
        "paper_id": 1406.0394,
        "authors": "Archil Gulisashvili and Peter Tankov",
        "title": "Implied volatility of basket options at extreme strikes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, we characterize the asymptotic behavior of the implied\nvolatility of a basket call option at large and small strikes in a variety of\nsettings with increasing generality. First, we obtain an asymptotic formula\nwith an error bound for the left wing of the implied volatility, under the\nassumption that the dynamics of asset prices are described by the\nmultidimensional Black-Scholes model. Next, we find the leading term of\nasymptotics of the implied volatility in the case where the asset prices follow\nthe multidimensional Black-Scholes model with time change by an independent\nincreasing stochastic process. Finally, we deal with a general situation in\nwhich the dependence between the assets is described by a given copula\nfunction. In this setting, we obtain a model-free tail-wing formula that links\nthe implied volatility to a special characteristic of the copula called the\nweak lower tail dependence function.\n"
    },
    {
        "paper_id": 1406.0412,
        "authors": "Gianluca Cassese",
        "title": "Option Pricing in an Imperfect World",
        "comments": "The paper has been withdrawn because in the newer version it was\n  split into two different papers, each of which have been uploaded into Arxiv",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a model with no given probability measure, we consider asset pricing in\nthe presence of frictions and other imperfections and characterize the property\nof coherent pricing, a notion related to (but much weaker than) the no\narbitrage property. We show that prices are coherent if and only if the set of\npricing measures is non empty, i.e. if pricing by expectation is possible. We\nthen obtain a decomposition of coherent prices highlighting the role of\nbubbles. eventually we show that under very weak conditions the coherent\npricing of options allows for a very clear representation from which it is\npossible, as in the original work of Breeden and Litzenberger, to extract the\nimplied probability. Eventually we test this conclusion empirically via a new\nnon parametric approach.\n"
    },
    {
        "paper_id": 1406.0437,
        "authors": "Taras Bodnar, Nestor Parolya and Wolfgang Schmid",
        "title": "Estimation of the Global Minimum Variance Portfolio in High Dimensions",
        "comments": "38 pages inc. 16 figures. Revised and corrected version",
        "journal-ref": "European Journal of Operational Research, Volume 266, Issue 1,\n  2018, 371-390",
        "doi": "10.1016/j.ejor.2017.09.028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We estimate the global minimum variance (GMV) portfolio in the\nhigh-dimensional case using results from random matrix theory. This approach\nleads to a shrinkage-type estimator which is distribution-free and it is\noptimal in the sense of minimizing the out-of-sample variance. Its asymptotic\nproperties are investigated assuming that the number of assets $p$ depends on\nthe sample size $n$ such that $\\frac{p}{n}\\rightarrow c\\in (0,+\\infty)$ as $n$\ntends to infinity. The results are obtained under weak assumptions imposed on\nthe distribution of the asset returns, namely it is only required the fourth\nmoments existence. Furthermore, we make no assumption on the upper bound of the\nspectrum of the covariance matrix. As a result, the theoretical findings are\nalso valid if the dependencies between the asset returns are described by a\nfactor model which appears to be very popular in financial literature nowadays.\nThis is also well-documented in a numerical study where the small- and\nlarge-sample behavior of the derived estimator are compared with existing\nestimators of the GMV portfolio. The resulting estimator shows significant\nimprovements and it turns out to be robust to the deviations from normality.\n"
    },
    {
        "paper_id": 1406.0455,
        "authors": "Cheng Chen, Lan Zheng, Venkatesh Srinivasan, Alex Thomo, Kui Wu,\n  Anthony Sukow",
        "title": "Buyer to Seller Recommendation under Constraints",
        "comments": "9 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The majority of recommender systems are designed to recommend items (such as\nmovies and products) to users. We focus on the problem of recommending buyers\nto sellers which comes with new challenges: (1) constraints on the number of\nrecommendations buyers are part of before they become overwhelmed, (2)\nconstraints on the number of recommendations sellers receive within their\nbudget, and (3) constraints on the set of buyers that sellers want to receive\n(e.g., no more than two people from the same household). We propose the\nfollowing critical problems of recommending buyers to sellers: Constrained\nRecommendation (C-REC) capturing the first two challenges, and Conflict-Aware\nConstrained Recommendation (CAC-REC) capturing all three challenges at the same\ntime. We show that C-REC can be modeled using linear programming and can be\nefficiently solved using modern solvers. On the other hand, we show that\nCAC-REC is NP-hard. We propose two approximate algorithms to solve CAC-REC and\nshow that they achieve close to optimal solutions via comprehensive experiments\nusing real-world datasets.\n"
    },
    {
        "paper_id": 1406.0496,
        "authors": "Nicolo Musmeci, Tomaso Aste and Tiziana Di Matteo",
        "title": "Relation between Financial Market Structure and the Real Economy:\n  Comparison between Clustering Methods",
        "comments": "31 pages, 17 figures",
        "journal-ref": "Journal of Network Theory in Finance, VOLUME 4, NUMBER 2 (2018)",
        "doi": "10.1371/journal.pone.0116201",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify the amount of information filtered by different hierarchical\nclustering methods on correlations between stock returns comparing it with the\nunderlying industrial activity structure. Specifically, we apply, for the first\ntime to financial data, a novel hierarchical clustering approach, the Directed\nBubble Hierarchical Tree and we compare it with other methods including the\nLinkage and k-medoids. In particular, by taking the industrial sector\nclassification of stocks as a benchmark partition, we evaluate how the\ndifferent methods retrieve this classification. The results show that the\nDirected Bubble Hierarchical Tree can outperform other methods, being able to\nretrieve more information with fewer clusters. Moreover, we show that the\neconomic information is hidden at different levels of the hierarchical\nstructures depending on the clustering method. The dynamical analysis on a\nrolling window also reveals that the different methods show different degrees\nof sensitivity to events affecting financial markets, like crises. These\nresults can be of interest for all the applications of clustering methods to\nportfolio optimization and risk hedging.\n"
    },
    {
        "paper_id": 1406.0551,
        "authors": "Alexander M.G. Cox, Zhaoxu Hou and Jan Obloj",
        "title": "Robust pricing and hedging under trading restrictions and the emergence\n  of local martingale models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the pricing of derivatives in a setting with trading\nrestrictions, but without any probabilistic assumptions on the underlying\nmodel, in discrete and continuous time. In particular, we assume that European\nput or call options are traded at certain maturities, and the forward price\nimplied by these option prices may be strictly decreasing in time. In discrete\ntime, when call options are traded, the short-selling restrictions ensure no\narbitrage, and we show that classical duality holds between the smallest\nsuper-replication price and the supremum over expectations of the payoff over\nall supermartingale measures. More surprisingly in the case where the only\nvanilla options are put options, we show that there is a duality gap. Embedding\nthe discrete time model into a continuous time setup, we make a connection with\n(strict) local-martingale models, and derive framework and results often seen\nin the literature on financial bubbles. This connection suggests a certain\nnatural interpretation of many existing results in the literature on financial\nbubbles.\n"
    },
    {
        "paper_id": 1406.0824,
        "authors": "Sercan Arik, Sukru Burc Eryilmaz, Adam Goldberg",
        "title": "Supervised classification-based stock prediction and portfolio\n  optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the number of publicly traded companies as well as the amount of their\nfinancial data grows rapidly, it is highly desired to have tracking, analysis,\nand eventually stock selections automated. There have been few works focusing\non estimating the stock prices of individual companies. However, many of those\nhave worked with very small number of financial parameters. In this work, we\napply machine learning techniques to address automated stock picking, while\nusing a larger number of financial parameters for individual companies than the\nprevious studies. Our approaches are based on the supervision of prediction\nparameters using company fundamentals, time-series properties, and correlation\ninformation between different stocks. We examine a variety of supervised\nlearning techniques and found that using stock fundamentals is a useful\napproach for the classification problem, when combined with the high\ndimensional data handling capabilities of support vector machine. The portfolio\nour system suggests by predicting the behavior of stocks results in a 3% larger\ngrowth on average than the overall market within a 3-month time period, as the\nout-of-sample test suggests.\n"
    },
    {
        "paper_id": 1406.0968,
        "authors": "Christopher S Kirk",
        "title": "Integration of a Predictive, Continuous Time Neural Network into\n  Securities Market Trading Operations",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes recent development and test implementation of a\ncontinuous time recurrent neural network that has been configured to predict\nrates of change in securities. It presents outcomes in the context of popular\ntechnical analysis indicators and highlights the potential impact of continuous\npredictive capability on securities market trading operations.\n"
    },
    {
        "paper_id": 1406.1149,
        "authors": "Ahmad Reza Yazdanian, T A Pirvu",
        "title": "Numerical analysis for Spread option pricing model in illiquid\n  underlying asset market: full feedback model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper performs the numerical analysis and the computation of a Spread\noption in a market with imperfect liquidity. The number of shares traded in the\nstock market has a direct impact on the stock's price. Thus, we consider a\nfull-feedback model in which price impact is fully incorporated into the model.\nThe price of a Spread option is characterize by a nonlinear partial\ndifferential equation. This is reduced to linear equations by asymptotic\nexpansions. The Peaceman-Rachford scheme as an alternating direction implicit\nmethod is employed to solve the linear equations numerically. We discuss the\nstability and the convergence of the numerical scheme. Illustrative examples\nare included to demonstrate the validity and applicability of the presented\nmethod. Finally we provide a numerical analysis of the illiquidity effect in\nreplicating an European Spread option; compared to the Black-Scholes case, a\ntrader generally buys more stock to replicate this option.\n"
    },
    {
        "paper_id": 1406.1249,
        "authors": "Zura Kakushadze",
        "title": "Notes on Alpha Stream Optimization",
        "comments": "42 pages; clarifying remarks added, minor misprints corrected; to\n  appear in The Journal of Investment Strategies",
        "journal-ref": "The Journal of Investment Strategies 4(3) (2015) 37-81",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In these notes we discuss investment allocation to multiple alpha streams\ntraded on the same execution platform, including when trades are crossed\ninternally resulting in turnover reduction. We discuss approaches to alpha\nweight optimization where one maximizes P&L subject to bounds on volatility (or\nSharpe ratio). The presence of negative alpha weights, which are allowed when\nalpha streams are traded on the same execution platform, complicates the\noptimization problem. By using factor model approach to alpha covariance\nmatrix, the original optimization problem can be viewed as a 1-dimensional root\nsearching problem plus an optimization problem that requires a finite number of\niterations. We discuss this approach without costs and with linear costs, and\nalso with nonlinear costs in a certain approximation, which makes the\nallocation problem tractable without forgoing nonlinear portfolio capacity\nbound effects.\n"
    },
    {
        "paper_id": 1406.1547,
        "authors": "Stan Palasek",
        "title": "Arbitrage-free exchange rate ensembles over a general trade network",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is assumed that under suitable economic and information-theoretic\nconditions, market exchange rates are free from arbitrage. Commodity markets in\nwhich trades occur over a complete graph are shown to be trivial. We therefore\nexamine the vector space of no-arbitrage exchange rate ensembles over an\narbitrary connected undirected graph. Consideration is given for the minimal\ninformation for determination of an exchange rate ensemble. We conclude with a\ntopical discussion of exchanges in which our analyses may be relevant,\nincluding the emergent but highly-regulated (and therefore not a complete\ngraph) market for digital currencies.\n"
    },
    {
        "paper_id": 1406.1733,
        "authors": "Vasileios Barmpoutis",
        "title": "The Naive Extrapolation Hypothesis and the Rosy-Gloomy Forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the behavior and the performance of the long-term forecasts issued by\nfinancial analysts with respect to the Extrapolation Hypothesis. That\nhypothesis states that investors, extrapolating from the firms' recent\nperformances, are too optimistic about growth and large firms and too\npessimistic about value and small firms. I find that the forecasting errors are\nhigher for the growth firms and large firms, thus providing support for the\nExtrapolation Hypothesis. However, in addition to the rosy picture of the\ngrowth and large firms, the forecasts of the value and small firms are not so\ngloomy in many cases. My analysis also reveals that expectations move together\nfor all categories of book-to-market and all sizes of firms. I proceed by\ninvestigating some common factors that may influence analysts' long-term\nforecasts, including co-movement and excessive optimism. I find that macro\nfactors beyond a firm's recent performance may influence the formation of\nexpectations.\n"
    },
    {
        "paper_id": 1406.1811,
        "authors": "Eduard Gim\\'enez, Alberto Elices and Giovanna Villani",
        "title": "A heuristic pricing and hedging framework for multi-currency fixed\n  income desks",
        "comments": "19 pages, without figures, this version is in review process in\n  Wilmott magazine",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that traded foreign exchange forwards and cross currency\nswaps (CCS) cannot be priced applying overnight cash and carry arguments as\nthey imply absence of funding advantage of one currency to the other. This\npaper proposes a heuristic present value concept for multi-currency pricing and\nhedging which allows taking into account the funding and therefore the\ncollateral currency and its pricing impact. For uncollateralized operations, it\nprovides more funding optionality to achieve either cheaper or more connected\nfunding to the hedging instruments. When foreign exchange forwards get aligned\nwith overnight cash and carry arguments, this method naturally converges to the\nwell established OIS discounting where each leg is funded in its own currency.\nA worked example compares this approach with a benchmark.\n"
    },
    {
        "paper_id": 1406.1936,
        "authors": "Andrew Papanicolaou",
        "title": "Stochastic Analysis Seminar on Filtering Theory",
        "comments": "94 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  These notes were originally written for the Stochastic Analysis Seminar in\nthe Department of Operations Research and Financial Engineering at Princeton\nUniversity, in February of 2011. The seminar was attended and supported by\nmembers of the Research Training Group, with the author being partially\nsupported by NSF grant DMS-0739195.\n"
    },
    {
        "paper_id": 1406.2053,
        "authors": "Hyong-chol O, Yong-hwa Ro, Ning Wan",
        "title": "A Method of Reducing Dimension of Space Variables in Multi-dimensional\n  Black-Scholes Equations",
        "comments": "14 pages",
        "journal-ref": "MATEMATIKA, Vol.30, 2014, no.1",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a method of reducing space dimension in multi-dimensional\nBlack-Scholes partial differential equations as well as in multi-dimensional\nparabolic equations. We prove that a multiplicative transformation of space\nvariables in the Black-Scholes partial differential equation reserves the form\nof Black-Scholes partial differential equation and reduces the space dimension.\nWe show that this transformation can reduce the number of sources of risks by\ntwo or more in some cases by giving remarks and several examples of financial\npricing problems. We also present that the invariance of the form of\nBlack-Scholes equations is based on the invariance of the form of parabolic\nequation under a change of variables with the linear combination of variables.\n"
    },
    {
        "paper_id": 1406.2133,
        "authors": "Timothy G. Ling, Pavel V. Shevchenko",
        "title": "Historical Backtesting of Local Volatility Model using AUD/USD Vanilla\n  Options",
        "comments": null,
        "journal-ref": "ANZIAM J. 57 (2016) 319-338",
        "doi": "10.1017/S1446181115000310",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Local Volatility model is a well-known extension of the Black-Scholes\nconstant volatility model whereby the volatility is dependent on both time and\nthe underlying asset. This model can be calibrated to provide a perfect fit to\na wide range of implied volatility surfaces. The model is easy to calibrate and\nstill very popular in FX option trading. In this paper we address a question of\nvalidation of the Local Volatility model. Different stochastic models for the\nunderlying can be calibrated to provide a good fit to the current market data\nbut should be recalibrated every trading date. A good fit to the current market\ndata does not imply that the model is appropriate and historical backtesting\nshould be performed for validation purposes. We study delta hedging errors\nunder the Local Volatility model using historical data from 2005 to 2011 for\nthe AUD/USD implied volatility. We performed backtests for a range of option\nmaturities and strikes using sticky delta and theoretically correct delta\nhedging. The results show that delta hedging errors under the standard\nBlack-Scholes model are no worse than that of the Local Volatility model.\nMoreover, for the case of in and at the money options, the hedging error for\nthe Back-Scholes model is significantly better.\n"
    },
    {
        "paper_id": 1406.2292,
        "authors": "A.Canale, R.M. Mininni, A.Rhandi",
        "title": "Analitic approach to solve a degenerate parabolic PDE for the Heston\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an analytic approach to solve a degenerate parabolic problem\nassociated to the Heston model, which is widely used in mathematical finance to\nderive the price of an European option on an risky asset with stochastic\nvolatility. We give a variational formulation, involving weighted Sobolev\nspaces, of the second order degenerate elliptic operator of the parabolic PDE.\nWe use this approach to prove, under appropriate assumptions on some involved\nunknown parameters, the existence and uniqueness of weak solutions to the\nparabolic problem on unbounded subdomains of the half-plane.\n"
    },
    {
        "paper_id": 1406.2581,
        "authors": "Denis Belomestny, Tigran Nagapetyan",
        "title": "Multilevel path simulation for weak approximation schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we discuss the possibility of using multilevel Monte Carlo\n(MLMC) methods for weak approximation schemes. It turns out that by means of a\nsimple coupling between consecutive time discretisation levels, one can achieve\nthe same complexity gain as under the presence of a strong convergence. We\nexemplify this general idea in the case of weak Euler scheme for L\\'evy driven\nstochastic differential equations, and show that, given a weak convergence of\norder $\\alpha\\geq 1/2,$ the complexity of the corresponding \"weak\" MLMC\nestimate is of order $\\varepsilon^{-2}\\log ^{2}(\\varepsilon).$ The numerical\nperformance of the new \"weak\" MLMC method is illustrated by several numerical\nexamples.\n"
    },
    {
        "paper_id": 1406.295,
        "authors": "Hirbod Assa",
        "title": "On Optimal Reinsurance Policy with Distortion Risk Measures and Premiums",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of optimal reinsurance design, when\nthe risk is measured by a distortion risk measure and the premium is given by a\ndistortion risk premium. First, we show how the optimal reinsurance design for\nthe ceding company, the reinsurance company and the social planner can be\nformulated in the same way. Second, by introducing the marginal indemnification\nfunctions, we characterize the optimal reinsurance contracts. We show that, for\nan optimal policy, the associated marginal indemnification function only takes\nthe values zero and one. We will see how the roles of the market preferences\nand premiums and that of the total risk are separated.\n"
    },
    {
        "paper_id": 1406.3064,
        "authors": "Andrzej Jarynowski, Andrzej Buda",
        "title": "Hierarchical representation of socio-economic complex systems according\n  to minimal sapnning trees",
        "comments": "Biotech Conference, Gdansk 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate hierarchical structure in various complex systems according to\nMinimum Spanning Tree methods. Firstly, we investigate stock markets where the\ngraphis obtained from the matrix of correlations coefficient computed between\nall pairs of assets by considering the synchronous time evolution of the\ndifference of the logarithm of daily stock price. The hierarchical tree\nprovides information useful to investigate the number and nature of economic\nfactors that have associated a meaningful economic taxonomy. We continue to use\nthis method in social systems (sport, political parties and pharmacy) to\ninvestigate collective effects and detect how single element of the system\ninfluences on the other ones. The level of correlations and Minimum Spanning\nTrees in various complex systems is also discussed.\n"
    },
    {
        "paper_id": 1406.3112,
        "authors": "Oscar Lopez and Rafael Serrano",
        "title": "Martingale approach to optimal portfolio-consumption problems in\n  Markov-modulated pure-jump models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal investment strategies that maximize expected utility from\nconsumption and terminal wealth in a pure-jump asset price model with\nMarkov-modulated (regime switching) jump-size distributions. We give sufficient\nconditions for existence of optimal policies and find closed-form expressions\nfor the optimal value function for agents with logarithmic and fractional power\n(CRRA) utility in the case of two-state Markov chains. The main tools are\nconvex duality techniques, stochastic calculus for pure-jump processes and\nexplicit formulae for the moments of telegraph processes with Markov-modulated\nrandom jumps.\n"
    },
    {
        "paper_id": 1406.3396,
        "authors": "Zura Kakushadze",
        "title": "Factor Models for Alpha Streams",
        "comments": "27 pages; discussion section and references added; to appear in The\n  Journal of Investment Strategies",
        "journal-ref": "The Journal of Investment Strategies 4(1) (2014) 83-109",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a framework for constructing factor models for alpha streams. Our\nmotivation is threefold. 1) When the number of alphas is large, the sample\ncovariance matrix is singular. 2) Its out-of-sample stability is challenging.\n3) Optimization of investment allocation into alpha streams can be tractable\nfor a factor model alpha covariance matrix. We discuss various risk factors for\nalphas such as: style risk factors; cluster risk factors based on alpha\ntaxonomy; principal components; and also using the underlying tradables\n(stocks) as alpha risk factors, for which computing the factor loadings and\nfactor covariance matrices does not involve any correlations with alphas, and\ntheir number is much larger than that of the relevant principal components. We\ndraw insight from stock factor models, but also point out substantial\ndifferences.\n"
    },
    {
        "paper_id": 1406.3531,
        "authors": "Ovidiu Racorean",
        "title": "Decoding Stock Market Behavior with the Topological Quantum Computer",
        "comments": "24 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A surprising image of the stock market arises if the price time series of all\nDow Jones Industrial Average stock components are represented in one chart at\nonce. The chart evolves into a braid representation of the stock market by\ntaking into account only the crossing of stocks and fixing a convention\ndefining overcrossings and undercrossings. The braid of stocks prices has a\nremarkable connection with the topological quantum computer. Using pairs of\nquasi-particles, called non-abelian anyons, having their trajectories braided\nin time, topological quantum computer can effectively simulate the stock market\nbehavior encoded in the braiding of stocks. In a typically topological quantum\ncomputation process the trajectories of non-abelian anyons are manipulated\naccording to the braiding of stocks and the outcome reflects the probability of\nthe future state of stock market. The probability depends only on the Jones\npolynomial of the knot formed by plat closing the quantum computation. The\nJones polynomial of the knotted stock market acts, making a parallel with the\ncommon financial literature, in a topological quantum computation as a\ncounterpart of a classical technical indicator in trading the stock market. The\ntype of knot stock market formed is an indicator of its future tendencies.\n"
    },
    {
        "paper_id": 1406.3716,
        "authors": "Archil Gulisashvili and Josef Teichmann",
        "title": "The G\\\"{a}rtner-Ellis theorem, homogenization, and affine processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain a first order extension of the large deviation estimates in the\nG\\\"{a}rtner-Ellis theorem. In addition, for a given family of measures, we find\na special family of functions having a similar Laplace principle expansion up\nto order one to that of the original family of measures. The construction of\nthe special family of functions mentioned above is based on heat kernel\nexpansions. Some of the ideas employed in the paper come from the theory of\naffine stochastic processes. For instance, we provide an explicit expansion\nwith respect to the homogenization parameter of the rescaled cumulant\ngenerating function in the case of a generic continuous affine process. We also\ncompute the coefficients in the homogenization expansion for the Heston model\nthat is one of the most popular stock price models with stochastic volatility.\n"
    },
    {
        "paper_id": 1406.3967,
        "authors": "Mehdi Lallouache, Damien Challet",
        "title": "The limits of statistical significance of Hawkes processes fitted to\n  financial data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many fits of Hawkes processes to financial data look rather good but most of\nthem are not statistically significant. This raises the question of what part\nof market dynamics this model is able to account for exactly. We document the\naccuracy of such processes as one varies the time interval of calibration and\ncompare the performance of various types of kernels made up of sums of\nexponentials. Because of their around-the-clock opening times, FX markets are\nideally suited to our aim as they allow us to avoid the complications of the\nlong daily overnight closures of equity markets. One can achieve statistical\nsignificance according to three simultaneous tests provided that one uses\nkernels with two exponentials for fitting an hour at a time, and two or three\nexponentials for full days, while longer periods could not be fitted within\nstatistical satisfaction because of the non-stationarity of the endogenous\nprocess. Fitted timescales are relatively short and endogeneity factor is high\nbut sub-critical at about 0.8.\n"
    },
    {
        "paper_id": 1406.4114,
        "authors": "Alessandro Chieppa, Andrea Ricca, Gianluca Rosso",
        "title": "Climate Events and Insurance Demand - The effect of potentially\n  catastrophic events on insurance demand in Italy",
        "comments": "Chiara Daniela Pronzato (Research coordinator), Isabella Pecetto\n  (Research collaborator)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate extreme events are constantly increasing. What is the effect of these\npotentially catastrophic events on insurance demand in Italy, with particular\nreference to the economic activities? Extreme precipitation events over most of\nthe midlatitude land masses and over wet tropical regions will very likely\nbecome more intense and more frequent by the end of this century, as global\nmean surface temperature increases. If we look to Italy, examination of the\nprecipitation time series shows a sensitive and highly significant decrease in\nthe total number of precipitation events in Italy, with a trend of events\nintense dissimilar as regards to low and high intensity, with a decline of\nfirsts and an increase of seconds. The risk related to hydrological natural\ndisasters is in Italy one of the most important problem for both damage and\nnumber of victims. How evolves the ability to pay for damages, with a view to\nsafeguarding work and economic activities, and employment protection?\n"
    },
    {
        "paper_id": 1406.4222,
        "authors": "Zuo Quan Xu",
        "title": "Investment under Duality Risk Measure",
        "comments": "17 pages, 1 figure",
        "journal-ref": "European Journal of Operational Research, Vol.239 (2014), 786-793",
        "doi": "10.1016/j.ejor.2014.06.022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One index satisfies the duality axiom if one agent, who is uniformly more\nrisk-averse than another, accepts a gamble, the latter accepts any less risky\ngamble under the index. Aumann and Serrano (2008) show that only one index\ndefined for so-called gambles satisfies the duality and positive homogeneity\naxioms. We call it a duality index. This paper extends the definition of\nduality index to all outcomes including all gambles, and considers a portfolio\nselection problem in a complete market, in which the agent's target is to\nminimize the index of the utility of the relative investment outcome. By\nlinking this problem to a series of Merton's optimum consumption-like problems,\nthe optimal solution is explicitly derived. It is shown that if the prior\nbenchmark level is too high (which can be verified), then the investment risk\nwill be beyond any agent's risk tolerance. If the benchmark level is\nreasonable, then the optimal solution will be the same as that of one of the\nMerton's series problems, but with a particular value of absolute risk\naversion, which is given by an explicit algebraic equation as a part of the\noptimal solution. According to our result, it is riskier to achieve the same\nsurplus profit in a stable market than in a less-stable market, which is\nconsistent with the common financial intuition.\n"
    },
    {
        "paper_id": 1406.4275,
        "authors": "Takashi Kato, Jun Sekine and Hiromitsu Yamamoto",
        "title": "A One-Factor Conditionally Linear Commodity Pricing Model under Partial\n  Information",
        "comments": "21 pages",
        "journal-ref": "Asia-Pacific Financial Markets, May 2014, Volume 21, Issue 2, pp\n  151-174",
        "doi": "10.1007/s10690-014-9182-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A one-factor asset pricing model with an Ornstein--Uhlenbeck process as its\nstate variable is studied under partial information: the mean-reverting level\nand the mean-reverting speed parameters are modeled as hidden/unobservable\nstochastic variables. No-arbitrage pricing formulas for derivative securities\nwritten on a liquid asset and exponential utility indifference pricing formulas\nfor derivative securities written on an illiquid asset are presented. Moreover,\na conditionally linear filtering result is introduced to compute the\npricing/hedging formulas and the Bayesian estimators of the hidden variables.\n"
    },
    {
        "paper_id": 1406.4297,
        "authors": "Tiziano De Angelis, Salvatore Federico and Giorgio Ferrari",
        "title": "Optimal Boundary Surface for Irreversible Investment with Stochastic\n  Costs",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines a Markovian model for the optimal irreversible investment\nproblem of a firm aiming at minimizing total expected costs of production. We\nmodel market uncertainty and the cost of investment per unit of production\ncapacity as two independent one-dimensional regular diffusions, and we consider\na general convex running cost function. The optimization problem is set as a\nthree-dimensional degenerate singular stochastic control problem.\n  We provide the optimal control as the solution of a Skorohod reflection\nproblem at a suitable boundary surface. Such boundary arises from the analysis\nof a family of two-dimensional parameter-dependent optimal stopping problems\nand it is characterized in terms of the family of unique continuous solutions\nto parameter-dependent nonlinear integral equations of Fredholm type.\n"
    },
    {
        "paper_id": 1406.4301,
        "authors": "Christa Cuchiero, Claudio Fontana, Alessandro Gnoatto",
        "title": "A general HJM framework for multiple yield curve modeling",
        "comments": "(40 pages, 4 figures)",
        "journal-ref": "Finance and Stochastics, 2016, vol. 20(2): 267-320",
        "doi": "10.1007/s00780-016-0291-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a general framework for modeling multiple yield curves which have\nemerged after the last financial crisis. In a general semimartingale setting,\nwe provide an HJM approach to model the term structure of multiplicative\nspreads between FRA rates and simply compounded OIS risk-free forward rates. We\nderive an HJM drift and consistency condition ensuring absence of arbitrage\nand, in addition, we show how to construct models such that multiplicative\nspreads are greater than one and ordered with respect to the tenor's length.\nWhen the driving semimartingale is specified as an affine process, we obtain a\nflexible Markovian structure. Finally, we show that the proposed framework\nallows to unify and extend several recent approaches to multiple yield curve\nmodeling.\n"
    },
    {
        "paper_id": 1406.4322,
        "authors": "Matthew Ames, Gareth W. Peters, Guillaume Bagnarosa and Ioannis\n  Kosmidis",
        "title": "Upside and Downside Risk Exposures of Currency Carry Trades via Tail\n  Dependence",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1303.4314",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Currency carry trade is the investment strategy that involves selling low\ninterest rate currencies in order to purchase higher interest rate currencies,\nthus profiting from the interest rate differentials. This is a well known\nfinancial puzzle to explain, since assuming foreign exchange risk is\nuninhibited and the markets have rational risk-neutral investors, then one\nwould not expect profits from such strategies. That is, according to uncovered\ninterest rate parity (UIP), changes in the related exchange rates should offset\nthe potential to profit from such interest rate differentials. However, it has\nbeen shown empirically, that investors can earn profits on average by borrowing\nin a country with a lower interest rate, exchanging for foreign currency, and\ninvesting in a foreign country with a higher interest rate, whilst allowing for\nany losses from exchanging back to their domestic currency at maturity. This\npaper explores the financial risk that trading strategies seeking to exploit a\nviolation of the UIP condition are exposed to with respect to multivariate tail\ndependence present in both the funding and investment currency baskets. It will\noutline in what contexts these portfolio risk exposures will benefit\naccumulated portfolio returns and under what conditions such tail exposures\nwill reduce portfolio returns.\n"
    },
    {
        "paper_id": 1406.4329,
        "authors": "Samuel N. Cohen and Victor Fedyashov",
        "title": "Ergodic BSDEs with jumps and time dependence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we look at ergodic BSDEs in the case where the forward dynamics\nare given by the solution to a non-autonomous (time-periodic coefficients)\nOrnstein-Uhlenbeck SDE with L\\'evy noise, taking values in a separable Hilbert\nspace. We establish the existence of a unique bounded solution to an infinite\nhorizon discounted BSDE. We then use the vanishing discount approach, together\nwith coupling techniques, to obtain a Markovian solution to the EBSDE. We also\nprove uniqueness under certain growth conditions. Applications are then given,\nin particular to risk-averse ergodic optimal control and power plant evaluation\nunder uncertainty.\n"
    },
    {
        "paper_id": 1406.4783,
        "authors": "A. M. Avdeenko",
        "title": "Advisors and indicators based on the SSA models and non-linear\n  generalizations",
        "comments": "6 pages, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers method of creation of an advisor and indicator based on\nthe spectral stochastic analysis model, both with linear and non-linear\napproximation. The problem of entrance to one or another trade position is\nsolved on the basis of combined analysis of dynamics of quotations of all\ncurrency pairs, what allows to actively hedge open positions.\n"
    },
    {
        "paper_id": 1406.5022,
        "authors": "Julius Bonart, Jean-Philippe Bouchaud, Augustin Landier, David Thesmar",
        "title": "Instabilities in large economies: aggregate volatility without\n  idiosyncratic shocks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2014/10/P10040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a dynamical model of interconnected firms which allows for certain\nmarket imperfections and frictions, restricted here to be myopic price\nforecasts and slow adjustment of production. Whereas the standard rational\nequilibrium is still formally a stationary solution of the dynamics, we show\nthat this equilibrium becomes linearly unstable in a whole region of parameter\nspace. When agents attempt to reach the optimal production target too quickly,\ncoordination breaks down and the dynamics becomes chaotic. In the unstable,\n\"turbulent\" phase, the aggregate volatility of the total output remains\nsubstantial even when the amplitude of idiosyncratic shocks goes to zero or\nwhen the size of the economy becomes large. In other words, crises become\nendogenous. This suggests an interesting resolution of the \"small shocks, large\nbusiness cycles\" puzzle.\n"
    },
    {
        "paper_id": 1406.5083,
        "authors": "Jos\\'e Mar\\'ia Sarabia, Faustino Prieto, Vanesa Jord\\'a",
        "title": "A variation of the Dragulescu-Yakovenko income model",
        "comments": "This is a preprint (7 pages, 4 tables, 2 figures)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of the Dragulescu-Yakovenko (2000) model, we show that\nempirical income distribution with truncated datasets, cannot be properly\nmodeled by the one-parameter exponential distribution. However, a truncated\nversion characterized by an exponential distribution with two parameters gives\nan accurate fit.\n"
    },
    {
        "paper_id": 1406.512,
        "authors": "Ernesto Savaglio and Stefano Vannucci",
        "title": "Strategy-proofness and single-peackedness in bounded distributive\n  lattices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two distinct specifications of single peakedness as currently met in the\nrelevant literature are singled out and discussed. Then, it is shown that,\nunder both of those specifications, a voting rule as defined on a bounded\ndistributive lattice is strategy-proof on the set of all profiles of single\npeaked total preorders if and only if it can be represented as an iterated\nmedian of projections and constants, or equivalently as the behaviour of a\ncertain median tree-automaton. The equivalence of individual and coalitional\nstrategy-proofness that is known to hold for single peaked domains in bounded\nlinear orders fails in such a general setting. A related impossibility result\non anonymous coalitionally strategy-proof voting rules is also obtained.\n"
    },
    {
        "paper_id": 1406.5276,
        "authors": "Ryo Murakami, Tomomichi Nakamura, Shin Kimura, Masashi Manabe, and\n  Toshihiro Tanizawa",
        "title": "On possible origins of trends in financial market price changes",
        "comments": "23 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.11.021",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate possible origins of trends using a deterministic threshold\nmodel, where we refer to long-term variabilities of price changes (price\nmovements) in financial markets as trends. From the investigation we find two\nphenomena. One is that the trend of monotonic increase and decrease can be\ngenerated by dealers' minuscule change in mood, which corresponds to the\npossible fundamentals. The other is that the emergence of trends is all but\ninevitable in the realistic situation because of the fact that dealers cannot\nalways obtain accurate information about deals, even if there is no influence\nfrom fundamentals and technical analyses.\n"
    },
    {
        "paper_id": 1406.5312,
        "authors": "Martin Le Doux Mbele Bidima and Mikl\\'os R\\'asonyi",
        "title": "Asymptotic Exponential Arbitrage and Utility-based Asymptotic Arbitrage\n  in Markovian Models of Financial Markets",
        "comments": "Forthcoming in Acta Applicandae Mathematicae",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider a discrete-time infinite horizon financial market model in which the\nlogarithm of the stock price is a time discretization of a stochastic\ndifferential equation. Under conditions different from those given in a\nprevious paper of ours, we prove the existence of investment opportunities\nproducing an exponentially growing profit with probability tending to $1$\ngeometrically fast. This is achieved using ergodic results on Markov chains and\ntools of large deviations theory.\n  Furthermore, we discuss asymptotic arbitrage in the expected utility sense\nand its relationship to the first part of the paper.\n"
    },
    {
        "paper_id": 1406.5386,
        "authors": "Desislava Chetalova, Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Zooming into market states",
        "comments": "13 pages, 9 figures",
        "journal-ref": "J. Stat. Mech., P01029 (2015)",
        "doi": "10.1088/1742-5468/2015/01/P01029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the daily stock data of the Nasdaq Composite index in the 22-year\nperiod 1992-2013 and identify market states as clusters of correlation matrices\nwith similar correlation structures. We investigate the stability of the\ncorrelation structure of each state by estimating the statistical fluctuations\nof correlations due to their non-stationarity. Our study is based on a random\nmatrix approach recently introduced to model the non-stationarity of\ncorrelations by an ensemble of random matrices. This approach reduces the\ncomplexity of the correlated market to a single parameter which characterizes\nthe fluctuations of the correlations and can be determined directly from the\nempirical return distributions. This parameter provides an insight into the\nstability of the correlation structure of each market state as well as into the\ncorrelation structure dynamics in the whole observation period. The analysis\nreveals an intriguing relationship between average correlation and correlation\nfluctuations. The strongest fluctuations occur during periods of high average\ncorrelation which is the case particularly in times of crisis.\n"
    },
    {
        "paper_id": 1406.5414,
        "authors": "Christa Cuchiero and Josef Teichmann",
        "title": "A convergence result for the Emery topology and a variant of the proof\n  of the fundamental theorem of asset pricing",
        "comments": "slightly extended version and list of references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that \\emph{No unbounded profit with bounded risk} (NUPBR) implies\n\\emph{predictable uniform tightness} (P-UT), a boundedness property in the\nEmery topology which has been introduced by C. Stricker \\cite{S:85}. Combining\nthis insight with well known results from J. M\\'emin and L. S{\\l}ominski\n\\cite{MS:91} leads to a short variant of the proof of the fundamental theorem\nof asset pricing initially proved by F. Delbaen and W. Schachermayer\n\\cite{DS:94}. The results are formulated in the general setting of admissible\nportfolio wealth processes as laid down by Y. Kabanov in \\cite{kab:97}.\n"
    },
    {
        "paper_id": 1406.543,
        "authors": "Jingtang Ma, Dongya Deng, Harry Zheng",
        "title": "A robust algorithm and convergence analysis for static replications of\n  nonlinear payoffs",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a new robust algorithm to find the optimal static\nreplicating portfolios for general nonlinear payoff functions and give the\nestimate of the rate of convergence that is absent in the literature. We choose\nthe static replication by minimizing the error bound between the nonlinear\npayoff function and the linear spline approximation and derive the\nequidistribution equation for selecting the optimal strike prices. The\nnumerical tests for variance swaps and swaptions and also for the static\nquadratic replication and the model with counterparty risk show that the\nproposed algorithm is simple, fast and accurate. The paper has generalized and\nimproved the results of the static replication and approximation in the\nliterature.\n"
    },
    {
        "paper_id": 1406.5486,
        "authors": "Efstathios Panayi, Gareth Peters and Ioannis Kosmidis",
        "title": "Liquidity commonality does not imply liquidity resilience commonality: A\n  functional characterisation for ultra-high frequency cross-sectional LOB data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a large-scale study of commonality in liquidity and resilience\nacross assets in an ultra high-frequency (millisecond-timestamped) Limit Order\nBook (LOB) dataset from a pan-European electronic equity trading facility. We\nfirst show that extant work in quantifying liquidity commonality through the\ndegree of explanatory power of the dominant modes of variation of liquidity\n(extracted through Principal Component Analysis) fails to account for heavy\ntailed features in the data, thus producing potentially misleading results. We\nemploy Independent Component Analysis, which both decorrelates the liquidity\nmeasures in the asset cross-section, but also reduces higher-order statistical\ndependencies.\n  To measure commonality in liquidity resilience, we utilise a novel\ncharacterisation as the time required for return to a threshold liquidity\nlevel. This reflects a dimension of liquidity that is not captured by the\nmajority of liquidity measures and has important ramifications for\nunderstanding supply and demand pressures for market makers in electronic\nexchanges, as well as regulators and HFTs. When the metric is mapped out across\na range of thresholds, it produces the daily Liquidity Resilience Profile (LRP)\nfor a given asset. This daily summary of liquidity resilience behaviour from\nthe vast LOB dataset is then amenable to a functional data representation. This\nenables the comparison of liquidity resilience in the asset cross-section via\nfunctional linear sub-space decompositions and functional regression. The\nfunctional regression results presented here suggest that market factors for\nliquidity resilience (as extracted through functional principal components\nanalysis) can explain between 10 and 40% of the variation in liquidity\nresilience at low liquidity thresholds, but are less explanatory at more\nextreme levels, where individual asset factors take effect.\n"
    },
    {
        "paper_id": 1406.5487,
        "authors": "Efstathios Panayi and Gareth Peters",
        "title": "Survival Models for the Duration of Bid-Ask Spread Deviations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many commonly used liquidity measures are based on snapshots of the state of\nthe limit order book (LOB) and can thus only provide information about\ninstantaneous liquidity, and not regarding the local liquidity regime. However,\ntrading in the LOB is characterised by many intra-day liquidity shocks, where\nthe LOB generally recovers after a short period of time. In this paper, we\ncapture this dynamic aspect of liquidity using a survival regression framework,\nwhere the variable of interest is the duration of the deviations of the spread\nfrom a pre-specified level. We explore a large number of model structures using\na branch-and-bound subset selection algorithm and illustrate the explanatory\nperformance of our model.\n"
    },
    {
        "paper_id": 1406.5646,
        "authors": "Ahmet Goncu",
        "title": "Statistical Arbitrage in the Black-Scholes Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  In this study we prove the existence of statistical arbitrage opportunities\nin the Black-Scholes framework by considering trading strategies that consists\nof borrowing from the risk free rate and taking a long position in the stock\nuntil it hits a deterministic barrier level. We derive analytical formulas for\nthe expected value, variance, and probability of loss for the discounted\ncumulative trading profits. No-statistical arbitrage condition is derived for\nthe Black-Scholes framework, which imposes a constraint on the Sharpe ratio of\nthe stock. Furthermore, we verify our theoretical results via extensive Monte\nCarlo simulations.\n"
    },
    {
        "paper_id": 1406.5718,
        "authors": "Denis M. Filatov, Maksim A. Vanyarkho",
        "title": "An Unconventional Attempt to Tame Mandelbrot's Grey Swans",
        "comments": "37 pages (paper 13 pages, supplementary materials 24 pages), 6\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest an original physical approach to describe the mechanism of market\npricing. The core of our approach is to consider pricing at different time\nscales separately, using independent equations of motion. Such an approach\nleads to a pricing model that not only allows estimating the volatility of\nfuture market prices, but also permits forecasting the direction of the price\nmove. Alongside with that, it is crucial that our model implies no calibration\non historical market data. And last but not least, properties of the model's\nsolution are consistent with those of real markets: it has fat tails, possesses\nscaling and evinces nonlinear market memory. As our model has been derived with\nthe tip of the pen, it may be not a yet another confirmation of the known\nempirical facts, but a theoretical justification thereto. Tests on real\nfinancial instruments prove the competence of our approach.\n"
    },
    {
        "paper_id": 1406.5755,
        "authors": "Johan Gunnesson, Alberto Fern\\'andez Mu\\~noz de Morales",
        "title": "A Bond Consistent Derivative Fair Value",
        "comments": "Minor changes. Additional comments and reference added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a rigorously motivated pricing equation for\nderivatives, including general cash collateralization schemes, which is\nconsistent with quoted market bond prices. Traditionally, there have been\ndifferences in how instruments with similar cash flow structures have been\npriced if their definition falls under that of a financial derivative versus if\nthey correspond to bonds, leading to possibilities such as funding through\nderivatives transactions. Furthermore, the problem has not been solved with the\nrecent introduction of Funding Valuation Adjustments in derivatives pricing,\nand in some cases has even been made worse.\n  In contrast, our proposed equation is not only consistent with fixed income\nassets and liabilities, but is also symmetric, implying a well-defined exit\nprice, independent of the entity performing the valuation. Also, we provide\nsome practical proxies, such as first-order approximations or basing\ncalculations of CVA and DVA on bond curves, rather than Credit Default Swaps.\n"
    },
    {
        "paper_id": 1406.5817,
        "authors": "Vinko Zlati\\'c, Giampaolo Gabbi, Hrvoje Abraham",
        "title": "Reduction of systemic risk by means of Pigouvian taxation",
        "comments": "19 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0114928",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the possibility of reduction of systemic risk in financial markets\nthrough Pigouvian taxation of financial institutions which is used to support\nthe rescue fund. We introduce the concept of the cascade risk with a clear\noperational definition as a subclass and a network related measure of the\nsystemic risk. Using financial networks constructed from real Italian money\nmarket data and using realistic parameters, we show that the cascade risk can\nbe substantially reduced by a small rate of taxation and by means of a simple\nstrategy of the money transfer from the rescue fund to interbanking market\nsubjects. Furthermore, we show that while negative effects on the return on\ninvestment ($ROI$) are direct and certain, an overall positive effect on risk\nadjusted return on investments ($ROI^{RA}$) is visible. Please note that\n\\emph{the taxation} is introduced as a monetary/regulatory, not as a fiscal\nmeasure, as the term could suggest. \\emph{The rescue fund} is implemented in a\nform of a common reserve fund.\n"
    },
    {
        "paper_id": 1406.5852,
        "authors": "Jak\\v{s}a Cvitani\\'c and Dylan Possama\\\"i and Nizar Touzi",
        "title": "Moral Hazard in Dynamic Risk Management",
        "comments": "36 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a contracting problem in which a principal hires an agent to\nmanage a risky project. When the agent chooses volatility components of the\noutput process and the principal observes the output continuously, the\nprincipal can compute the quadratic variation of the output, but not the\nindividual components. This leads to moral hazard with respect to the risk\nchoices of the agent. We identify a family of admissible contracts for which\nthe optimal agent's action is explicitly characterized, and, using the recent\ntheory of singular changes of measures for It\\^o processes, we study how\nrestrictive this family is. In particular, in the special case of the standard\nHomlstr\\\"om-Milgrom model with fixed volatility, the family includes all\npossible contracts. We solve the principal-agent problem in the case of CARA\npreferences, and show that the optimal contract is linear in these factors: the\ncontractible sources of risk, including the output, the quadratic variation of\nthe output and the cross-variations between the output and the contractible\nrisk sources. Thus, like sample Sharpe ratios used in practice, path-dependent\ncontracts naturally arise when there is moral hazard with respect to risk\nmanagement. In a numerical example, we show that the loss of efficiency can be\nsignificant if the principal does not use the quadratic variation component of\nthe optimal contract.\n"
    },
    {
        "paper_id": 1406.6038,
        "authors": "Dirk Tasche",
        "title": "Exact fit of simple finite mixture models",
        "comments": "16 pages, 2 tables, some corrections, section on cost quantification\n  inserted",
        "journal-ref": "Journal of Risk and Financial Management 7(4), 150-164, 2014",
        "doi": "10.3390/jrfm7040150",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How to forecast next year's portfolio-wide credit default rate based on last\nyear's default observations and the current score distribution? A classical\napproach to this problem consists of fitting a mixture of the conditional score\ndistributions observed last year to the current score distribution. This is a\nspecial (simple) case of a finite mixture model where the mixture components\nare fixed and only the weights of the components are estimated. The optimum\nweights provide a forecast of next year's portfolio-wide default rate. We point\nout that the maximum-likelihood (ML) approach to fitting the mixture\ndistribution not only gives an optimum but even an exact fit if we allow the\nmixture components to vary but keep their density ratio fix. From this\nobservation we can conclude that the standard default rate forecast based on\nlast year's conditional default rates will always be located between last\nyear's portfolio-wide default rate and the ML forecast for next year. As an\napplication example, then cost quantification is discussed. We also discuss how\nthe mixture model based estimation methods can be used to forecast total loss.\nThis involves the reinterpretation of an individual classification problem as a\ncollective quantification problem.\n"
    },
    {
        "paper_id": 1406.6084,
        "authors": "Henry Lam and Zhenming Liu",
        "title": "From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial\n  Environments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a non-stochastic online learning approach to price financial\noptions by modeling the market dynamic as a repeated game between the nature\n(adversary) and the investor. We demonstrate that such framework yields\nanalogous structure as the Black-Scholes model, the widely popular option\npricing model in stochastic finance, for both European and American options\nwith convex payoffs. In the case of non-convex options, we construct\napproximate pricing algorithms, and demonstrate that their efficiency can be\nanalyzed through the introduction of an artificial probability measure, in\nparallel to the so-called risk-neutral measure in the finance literature, even\nthough our framework is completely adversarial. Continuous-time convergence\nresults and extensions to incorporate price jumps are also presented.\n"
    },
    {
        "paper_id": 1406.609,
        "authors": "Sakda Chaiworawitkul, Patrick S. Hagan, and Andrew Lesniewski",
        "title": "Semiclassical approximation in stochastic optimal control I. Portfolio\n  construction problem",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is the first in a series of papers in which we study an efficient\napproximation scheme for solving the Hamilton-Jacobi-Bellman equation for\nmulti-dimensional problems in stochastic control theory. The method is a\ncombination of a WKB style asymptotic expansion of the value function, which\nreduces the second order HJB partial differential equation to a hierarchy of\nfirst order PDEs, followed by a numerical algorithm to solve the first few of\nthe resulting first order PDEs. This method is applicable to stochastic systems\nwith a relatively large number of degrees of freedom, and does not seem to\nsuffer from the curse of dimensionality. Computer code implementation of the\nmethod using modest computational resources runs essentially in real time. We\napply the method to solve a general portfolio construction problem.\n"
    },
    {
        "paper_id": 1406.61,
        "authors": "Takao Hishikawa, Jun-ichi Inoue",
        "title": "Probabilistic flows of inhabitants in urban areas and self-organization\n  in housing markets",
        "comments": "23 pages, 19 figures, using svmult.cls, Proceedings of\n  Econophysics-Kolkata VIII",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple probabilistic model to explain the spatial structure of\nthe rent distribution of housing market in city of Sapporo. Here we modify the\nmathematical model proposed by Gauvin et. al. Especially, we consider the\ncompetition between two distances, namely, the distance between house and\ncenter, and the distance between house and office. Computer simulations are\ncarried out to reveal the self-organized spatial structure appearing in the\nrent distribution. We also compare the resulting distribution with empirical\nrent distribution in Sapporo as an example of cities designated by ordinance.\nWe find that the lowest ranking agents (from the viewpoint of the lowest\n`willing to pay') are swept away from relatively attractive regions and make\nseveral their own `communities' at low offering price locations in the city.\n"
    },
    {
        "paper_id": 1406.6142,
        "authors": "Andreas Lager{\\aa}s",
        "title": "How to hedge extrapolated yield curves",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a framework on how to hedge the interest rate sensitivity of\nliabilities discounted by an extrapolated yield curve. The framework is based\non functional analysis in that we consider the extrapolated yield curve as a\nfunctional of an observed yield curve and use its G\\^ateaux variation to\nunderstand the sensitivity to any possible yield curve shift. We apply the\nframework to analyse the Smith-Wilson method of extrapolation that is proposed\nby the European Insurance and Occupational Pensions Authority (EIOPA) in the\ncoming EU legislation Solvency II, and the method recently introduced, and\ncurrently prescribed, by the Swedish Financial Supervisory Authority.\n"
    },
    {
        "paper_id": 1406.6245,
        "authors": "Christoph Belak, An Chen, Carla Mereu, Robert Stelzer",
        "title": "Optimal investment with time-varying stochastic endowments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers a utility maximization and optimal asset allocation\nproblem in the presence of a stochastic endowment that cannot be fully hedged\nthrough trading in the financial market. After studying continuity properties\nof the value function for general utility functions, we rely on the dynamic\nprogramming approach to solve the optimization problem for power utility\ninvestors including the empirically relevant and mathematically challenging\ncase of relative risk aversion larger than one. For this, we argue that the\nvalue function is the unique viscosity solution of the Hamilton-Jacobi-Bellman\n(HJB) equation. The homogeneity of the value function is then used to reduce\nthe HJB equation by one dimension, which allows us to prove that the value\nfunction is even a classical solution thereof. Using this, an optimal strategy\nis derived and its asymptotic behavior in the large wealth regime is discussed.\n"
    },
    {
        "paper_id": 1406.6441,
        "authors": "Matteo Smerlak",
        "title": "Thermodynamics of inequalities: from precariousness to economic\n  stratification",
        "comments": "18 pages",
        "journal-ref": "Physica A 441, 40-50 (2016)",
        "doi": "10.1016/j.physa.2015.09.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Growing economic inequalities are observed in several countries throughout\nthe world. Following Pareto, the power-law structure of these inequalities has\nbeen the subject of much theoretical and empirical work. But their\nnonequilibrium dynamics, e.g. after a policy change, remains incompletely\nunderstood. Here we introduce a thermodynamical theory of inequalities based on\nthe analogy between economic stratification and statistical entropy. Within\nthis framework we identify the combination of upward mobility with\nprecariousness as a fundamental driver of inequality. We formalize this\nstatement by a \"second-law\" inequality displaying upward mobility and\nprecariousness as thermodynamic conjugate variables. We estimate the time scale\nfor the \"relaxation\" of the wealth distribution after a sudden change of the\nafter-tax return on capital. Our method can be generalized to gain insight into\nthe dynamics of inequalities in any Markovian model of socioeconomic\ninteractions.\n"
    },
    {
        "paper_id": 1406.6496,
        "authors": "Luisanna Cocco, Giulio Concas and Michele Marchesi",
        "title": "Using an Artificial Financial Market for studying a Cryptocurrency\n  Market",
        "comments": "13 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an agent-based artificial cryptocurrency market in which\nheterogeneous agents buy or sell cryptocurrencies, in particular Bitcoins. In\nthis market, there are two typologies of agents, Random Traders and Chartists,\nwhich interact with each other by trading Bitcoins. Each agent is initially\nendowed with a finite amount of crypto and/or fiat cash and issues buy and sell\norders, according to her strategy and resources. The number of Bitcoins\nincreases over time with a rate proportional to the real one, even if the\nmining process is not explicitly modelled.\n  The model proposed is able to reproduce some of the real statistical\nproperties of the price absolute returns observed in the Bitcoin real market.\nIn particular, it is able to reproduce the autocorrelation of the absolute\nreturns, and their cumulative distribution function. The simulator has been\nimplemented using object-oriented technology, and could be considered a valid\nstarting point to study and analyse the cryptocurrency market and its future\nevolutions.\n"
    },
    {
        "paper_id": 1406.6559,
        "authors": "Ersin Kantar, Bayram Deviren and Mustafa Keskin",
        "title": "Hierarchical structure of the European countries based on debts as a\n  percentage of GDP during the 2000-2011 period",
        "comments": "7 pages, 9 figures, and 1 table",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.07.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate hierarchical structures of the European countries by using\ndebt as a percentage of Gross Domestic Product (GDP) of the countries as they\nchange over a certain period of time. We obtain the topological properties\namong the countries based on debt as a percentage of GDP of European countries\nover the period 2000-2011 by using the concept of hierarchical structure\nmethods (minimal spanning tree, (MST) and hierarchical tree, (HT)). This period\nis also divided into two sub-periods related to 2004 enlargement of the\nEuropean Union, namely 2000-2004 and 2005-2011, in order to test various\ntime-window and observe the temporal evolution. The bootstrap techniques is\napplied to see a value of statistical reliability of the links of the MSTs and\nHTs. The clustering linkage procedure is also used to observe the cluster\nstructure more clearly. From the structural topologies of these trees, we\nidentify different clusters of countries according to their level of debts and\neconomic ties. Our results show that by the debt crisis, the less and most\naffected Eurozones economies are formed as a cluster with each other in the\nMSTs and hierarchical trees.\n"
    },
    {
        "paper_id": 1406.6562,
        "authors": "Ersin Kantar, Alper Aslan, Bayram Deviren and Mustafa Keskin",
        "title": "Hierarchical structure of the countries based on electricity consumption\n  and economic growth",
        "comments": "9 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the hierarchical structures of countries based on electricity\nconsumption and economic growth by using the real amounts of their consumption\nover a certain time period. We use of electricity consumption data to detect\nthe topological properties of 60 countries from 1971 to 2008. These countries\nare divided into three subgroups: low income group, middle income group and\nhigh income group countries. Firstly, a relationship between electricity\nconsumption and economic growth is investigated by using the concept of\nhierarchical structure methods (minimal spanning tree (MST) and hierarchical\ntree (HT)). Secondly, we perform bootstrap techniques to investigate a value of\nthe statistical reliability to the links of the MST. Finally, we use a\nclustering linkage procedure in order to observe the cluster structure more\nclearly. The results of the structural topologies of these trees are as\nfollows: i) we identified different clusters of countries according to their\ngeographical location and economic growth, ii) we found a strong relation\nbetween energy consumption and economic growth for all the income groups\nconsidered in this study and iii) the results are in good agreement with the\ncausal relationship between electricity consumption and economic growth.\n"
    },
    {
        "paper_id": 1406.6575,
        "authors": "Oliver Kley and Claudia Kl\\\"uppelberg and Lukas Reichel",
        "title": "Systemic risk through contagion in a core-periphery structured banking\n  network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We contribute to the understanding of how systemic risk arises in a network\nof credit-interlinked agents. Motivated by empirical studies we formulate a\nnetwork model which, despite its simplicity, depicts the nature of interbank\nmarkets better than a homogeneous model. The components of a vector\nOrnstein-Uhlenbeck process living on the vertices of the network describe the\nfinancial robustnesses of the agents. For this system, we prove a LLN for\ngrowing network size leading to a propagation of chaos result. We state\nproperties, which arise from such a structure, and examine the effect of\ninhomogeneity on several risk management issues and the possibility of\ncontagion.\n"
    },
    {
        "paper_id": 1406.662,
        "authors": "Venkat Venkatasubramanian, Yu Luo and Jay Sethuraman",
        "title": "Game Theory, Statistical Mechanics and Income Inequality",
        "comments": "Corresponding author: Venkat@columbia.edu",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The widening inequality in income distribution in recent years, and the\nassociated excessive pay packages of CEOs in the U.S. and elsewhere, is of\ngrowing concern among policy makers as well as the common person. However,\nthere seems to be no satisfactory answer, in conventional economic theories and\nmodels, to the fundamental question of what kind of pay distribution we ought\nto see, at least under ideal conditions, in a free market environment and\nwhether this distribution is fair. We propose a game theoretic framework that\naddresses these questions and show that the lognormal distribution is the\nfairest inequality of pay in an organization comprising of homogenous agents,\nachieved at equilibrium, under ideal free market conditions. We also show that\nfor a population of two different classes of agents, the final distribution is\na combination of two different lognormal distributions where one of them,\ncorresponding to the top 3-5% of the population, can be misidentified as a\nPareto distribution. Our theory also shows the deep and direct connection\nbetween potential game theory and statistical mechanics through entropy, which\nis a measure of fairness in a distribution. This leads us to propose the fair\nmarket hypothesis, that the self-organizing dynamics of the ideal free market,\ni.e., Adam Smith's \"invisible hand\", not only promotes efficiency but also\nmaximizes fairness under the given constraints.\n"
    },
    {
        "paper_id": 1406.6651,
        "authors": "Ishanu Chattopadhyay",
        "title": "Causality Networks",
        "comments": "22 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While correlation measures are used to discern statistical relationships\nbetween observed variables in almost all branches of data-driven scientific\ninquiry, what we are really interested in is the existence of causal\ndependence. Designing an efficient causality test, that may be carried out in\nthe absence of restrictive pre-suppositions on the underlying dynamical\nstructure of the data at hand, is non-trivial. Nevertheless, ability to\ncomputationally infer statistical prima facie evidence of causal dependence may\nyield a far more discriminative tool for data analysis compared to the\ncalculation of simple correlations. In the present work, we present a new\nnon-parametric test of Granger causality for quantized or symbolic data streams\ngenerated by ergodic stationary sources. In contrast to state-of-art binary\ntests, our approach makes precise and computes the degree of causal dependence\nbetween data streams, without making any restrictive assumptions, linearity or\notherwise. Additionally, without any a priori imposition of specific dynamical\nstructure, we infer explicit generative models of causal cross-dependence,\nwhich may be then used for prediction. These explicit models are represented as\ngeneralized probabilistic automata, referred to crossed automata, and are shown\nto be sufficient to capture a fairly general class of causal dependence. The\nproposed algorithms are computationally efficient in the PAC sense; $i.e.$, we\nfind good models of cross-dependence with high probability, with polynomial\nrun-times and sample complexities. The theoretical results are applied to\nweekly search-frequency data from Google Trends API for a chosen set of\nsocially \"charged\" keywords. The causality network inferred from this dataset\nreveals, quite expectedly, the causal importance of certain keywords. It is\nalso illustrated that correlation analysis fails to gather such insight.\n"
    },
    {
        "paper_id": 1406.6805,
        "authors": "Simone Farinelli and Hideyuki Takada",
        "title": "Credit Bubbles in Arbitrage Markets: The Geometric Arbitrage Approach to\n  Credit Risk",
        "comments": "arXiv admin note: text overlap with arXiv:0910.1671",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply Geometric Arbitrage Theory to obtain results in mathematical finance\nfor credit markets, which do not need stochastic differential geometry in their\nformulation. We obtain closed form equations involving default intensities and\nloss given defaults characterizing the no-free-lunch-with-vanishing-risk\ncondition for corporate bonds, as well as the generic dynamics for credit\nmarket allowing for arbitrage possibilities. Moreover, arbitrage credit bubbles\nfor both base credit assets and credit derivatives are explicitly computed for\nthe market dynamics minimizing the arbitrage.\n"
    },
    {
        "paper_id": 1406.6862,
        "authors": "Egil Ferkingstad and Anders L{\\o}land",
        "title": "Coping with area price risk in electricity markets: Forecasting\n  Contracts for Difference in the Nordic power market",
        "comments": "29 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Contracts for Difference (CfDs) are forwards on the spread between an area\nprice and the system price. Together with the system price forwards, these\nproducts are used to hedge the area price risk in the Nordic electricity\nmarket. The CfDs are typically available for the next two months, three\nquarters and three years. This is fine, except that CfDs are not traded at\nNASDAQ OMX Commodities for every Nord Pool Spot price area. We therefore ask\nthe hypothetical question: What would the CfD market price have been, say in\nthe price area NO2, if it had been traded? We build regression models for each\nobservable price area, and use Bayesian elicitation techniques to obtain prior\ninformation on how similar the different price areas are to forecast the price\nin an area where CfDs are not traded.\n"
    },
    {
        "paper_id": 1406.6902,
        "authors": "Claudia Ceci, Katia Colaneri, Alessandra Cretarola",
        "title": "Hedging of unit-linked life insurance contracts with unobservable\n  mortality hazard rate via local risk-minimization",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2014.10.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the local risk-minimization approach for a\ncombined financial-insurance model where there are restrictions on the\ninformation available to the insurance company. In particular we assume that,\nat any time, the insurance company may observe the number of deaths from a\nspecific portfolio of insured individuals but not the mortality hazard rate. We\nconsider a financial market driven by a general semimartingale and we aim to\nhedge unit-linked life insurance contracts via the local risk-minimization\napproach under partial information. The F\\\"ollmer-Schweizer decomposition of\nthe insurance claim and explicit formulas for the optimal strategy for pure\nendowment and term insurance contracts are provided in terms of the projection\nof the survival process on the information flow. Moreover, in a Markovian\nframework, we reduce to solve a filtering problem with point process\nobservations.\n"
    },
    {
        "paper_id": 1406.694,
        "authors": "Xiongfei Jian and Xun Li and Fahuai Yi",
        "title": "Optimal Investment with Stopping in Finite Horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate dynamic optimization problems featuring both\nstochastic control and optimal stopping in a finite time horizon. The paper\naims to develop new methodologies, which are significantly different from those\nof mixed dynamic optimal control and stopping problems in the existing\nliterature, to study a manager's decision. We formulate our model to a free\nboundary problem of a fully nonlinear equation. Furthermore, by means of a dual\ntransformation for the above problem, we convert the above problem to a new\nfree boundary problem of a linear equation. Finally, we apply the theoretical\nresults to challenging, yet practically relevant and important, risk-sensitive\nproblems in wealth management to obtain the properties of the optimal strategy\nand the right time to achieve a certain level over a finite time investment\nhorizon.\n"
    },
    {
        "paper_id": 1406.6951,
        "authors": "Luciano Campi, Ismail Laachir, Claude Martini",
        "title": "Change of numeraire in the two-marginals martingale transport problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we apply change of numeraire techniques to the optimal\ntransport approach for computing model-free prices of derivatives in a two\nperiods model. In particular, we consider the optimal transport plan\nconstructed in \\cite{HobsonKlimmek2013} as well as the one introduced in\n\\cite{BeiglJuil} and further studied in \\cite{BrenierMartingale}. We show that,\nin the case of positive martingales, a suitable change of numeraire applied to\n\\cite{HobsonKlimmek2013} exchanges forward start straddles of type I and type\nII, so that the optimal transport plan in the subhedging problems is the same\nfor both types of options. Moreover, for \\cite{BrenierMartingale}'s\nconstruction, the right monotone transference plan can be viewed as a mirror\ncoupling of its left counterpart under the change of numeraire. An application\nto stochastic volatility models is also provided.\n"
    },
    {
        "paper_id": 1406.6952,
        "authors": "Zied Ben-Salah, H\\'el\\`ene Gu\\'erin, Manuel Morales and Hassan Omidi\n  Firouzi",
        "title": "On the Depletion Problem for an Insurance Risk Process: New Non-ruin\n  Quantities in Collective Risk Theory",
        "comments": "23 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The field of risk theory has traditionally focused on ruin-related\nquantities. In particular, the socalled Expected Discounted Penalty Function\nhas been the object of a thorough study over the years. Although interesting in\ntheir own right, ruin related quantities do not seem to capture path-dependent\nproperties of the reserve. In this article we aim at presenting the\nprobabilistic properties of drawdowns and the speed at which an insurance\nreserve depletes as a consequence of the risk exposure of the company. These\nnew quantities are not ruin related yet they capture important features of an\ninsurance position and we believe it can lead to the design of a meaningful\nrisk measures. Studying drawdowns and speed of depletion for L\\'evy insurance\nrisk processes represent a novel and challenging concept in insurance\nmathematics. In this paper, all these concepts are formally introduced in an\ninsurance setting. Moreover, using recent results in fluctuation theory for\nL\\'evy processes, we derive expressions for the distribution of several\nquantities related to the depletion problem. Of particular interest are the\ndistribution of drawdowns and the Laplace transform for the speed of depletion.\nThese expressions are given for some examples of L\\'evy insurance risk\nprocesses for which they can be calculated, in particular for the classical\nCramer-Lundberg model.\n"
    },
    {
        "paper_id": 1406.704,
        "authors": "Hassan Omidi Firouzi and Andrew Luong",
        "title": "Optimal Portfolio Problem Using Entropic Value at Risk: When the\n  Underlying Distribution is Non-Elliptical",
        "comments": "13 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to study the optimal portfolio problem. Harry\nMarkowitz's Ph.D. thesis prepared the ground for the mathematical theory of\nfinance. In modern portfolio theory, we typically find asset returns that are\nmodeled by a random variable with an elliptical distribution and the notion of\nportfolio risk is described by an appropriate risk measure. In this paper, we\npropose new stochastic models for the asset returns that are based on Jumps-\nDiffusion (J-D) distributions. This family of distributions are more compatible\nwith stylized features of asset returns. On the other hand, in the past\ndecades, we find attempts in the literature to use well-known risk measures,\nsuch as Value at Risk and Expected Shortfall, in this context. Unfortunately,\none drawback with these previous approaches is that no explicit formulas are\navailable and numerical approximations are used to solve the optimization\nproblem. In this paper, we propose to use a new coherent risk measure,\nso-called, Entropic Value at Risk(EVaR), in the optimization problem. For\ncertain models, including a jump-diffusion distribution, this risk measure\nyields an explicit formula for the objective function so that the optimization\nproblem can be solved without resorting to numerical approximations.\n"
    },
    {
        "paper_id": 1406.7064,
        "authors": "Ersin Kantar",
        "title": "Hierarchical Structure of the Foreign Trade: The Case of the United\n  State",
        "comments": "7 pages, 6 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:1010.5653, arXiv:1406.6559; and text overlap with\n  arXiv:1406.6562 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study uses hierarchical structure methods (minimal spanning tree, (MST)\nand hierarchical tree, (HT)) to examine the hierarchical structures of the\nUnited State (US) foreign trade by using the real prices of their commodity\nexport and import move together over time. We obtain the topological properties\namong the countries based on US foreign trade over the periods of 1985-2011. We\nalso perform the bootstrap techniques to investigate a value of the statistical\nreliability to the links of the MSTs. Finally, we use a clustering linkage\nprocedure in order to observe the cluster structure much better. The results of\nthe topologies structural of these trees are as follows: i) We identified\ndifferent clusters of countries according to their geographical location and\neconomic growth. ii) Our results show that the European Union and Asian\ncountries are more important within the network, due to a tighter connection\nwith other countries. The country's most important trading partners are the\nCanada, China, Mexico, Japan, Germany, United Kingdom, South Korea, France,\nTaiwan, India, Singapore and Netherlands iii) We have also found that these\ncountries play a significance role for US foreign trade and have important\nimplications for the design of portfolio and investment strategies.\n"
    },
    {
        "paper_id": 1406.7115,
        "authors": "Dietrich Stauffer",
        "title": "Income Inequality in the 21st Century -- A biased summary of Piketty's\n  Capital in the Twenty-First Century",
        "comments": "Two pages plus many figures. Revision corrects typos and enlarges\n  captions 1 and 4",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Capital usually leads to income, and income is more accurately and easily\nmeasured. Thus we summarize income distributions in USA, Germany, etc.\n"
    },
    {
        "paper_id": 1406.733,
        "authors": "Felix Ming Fai Wong, Zhenming Liu, Mung Chiang",
        "title": "Stock Market Prediction from WSJ: Text Mining via Sparse Matrix\n  Factorization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the problem of predicting directional movements of stock prices\nbased on news articles: here our algorithm uses daily articles from The Wall\nStreet Journal to predict the closing stock prices on the same day. We propose\na unified latent space model to characterize the \"co-movements\" between stock\nprices and news articles. Unlike many existing approaches, our new model is\nable to simultaneously leverage the correlations: (a) among stock prices, (b)\namong news articles, and (c) between stock prices and news articles. Thus, our\nmodel is able to make daily predictions on more than 500 stocks (most of which\nare not even mentioned in any news article) while having low complexity. We\ncarry out extensive backtesting on trading strategies based on our algorithm.\nThe result shows that our model has substantially better accuracy rate (55.7%)\ncompared to many widely used algorithms. The return (56%) and Sharpe ratio due\nto a trading strategy based on our model are also much higher than baseline\nindices.\n"
    },
    {
        "paper_id": 1406.7526,
        "authors": "Pawe{\\l} Fiedor and Odd Magnus Trondrud",
        "title": "Predictability of Volatility Homogenised Financial Time Series",
        "comments": "8 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modelling financial time series as a time change of a simpler process has\nbeen proposed in various forms over the years. One of such recent approaches is\ncalled volatility homogenisation decomposition, and has been designed\nspecifically to aid the forecasting of price changes on financial markets. The\nauthors of this method have attempted to prove the its usefulness by applying a\nspecific forecasting procedure and determining the effectiveness of this\nprocedure on the decomposed time series, as compared with the original time\nseries. This is problematic in at least two ways. First, the choice of the\nforecasting procedure obviously has an effect on the results, rendering them\nnon-exhaustive. Second, the results obtained were not completely convincing,\nwith some values falling under 50% guessing rate. Additionally, only nine\nAustralian stocks were being investigated, which further limits the scope of\nthis proof. In this study we propose to find the usefulness of volatility\nhomogenisation by calculating the predictability of the decomposed time series\nand comparing it to the predictability of the original time series. We are\napplying information-theoretic notion of entropy rate to quantify\npredictability, which guarantees the result is not tied to a specific method of\nprediction, and additionally we base our calculations on a large number of\nstocks from the Warsaw Stock Exchange.\n"
    },
    {
        "paper_id": 1406.7604,
        "authors": "Xiaoxiao Zheng and Xin Zhang",
        "title": "Optimal investment-reinsurance policy under a long-term perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we assume an insure is allowed to purchase proportional\nreinsurance and can invest his or her wealth into the financial market where a\nsavings account, stocks and bonds are available. Different from classical\noptimal investment and reinsurance problem, this paper studies the insurer's\nlong-term investment decision. Under this setting, our model consider the\ninterest risk and the inflation risk. Specifically, we suppose the interest\nrate follows a stochastic process, while price index is described by a\nclassical model. By solving Hamilton-Jacobi-Bellman equation, the closed-form\nexpression of the optimal policy is obtained. Further, we prove the\ncorresponding verification theorem without the usual Lipschitz condition. In\nthe end, numerical examples are made to illustrate the difference of the\noptimal polices under Ho-lee model and Vasicek model.\n"
    },
    {
        "paper_id": 1406.7606,
        "authors": "Xiaoxiao Zheng and Xin Zhang",
        "title": "Optimal Hybrid Dividend Strategy Under The Markovian Regime-Switching\n  Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the optimal dividend problem for a company. We\ndescribe the surplus process of the company by a diffusion model with regime\nswitching. The aim of the company is to choose a dividend policy to maximize\nthe expected total discounted payments until ruin. In this article, we consider\na hybrid dividend strategy, that is, the company is allowed to conduct\ncontinuous dividend strategy as well as impulsive dividend strategy. In\naddition, we consider the change of economy, which is characterized by a\nmarkovian regime-switching, and under the setting of two regimes, we solve the\nproblem and obtain the analytical solution for the value function.\n"
    },
    {
        "paper_id": 1406.7723,
        "authors": "Ronald Hochreiter and Christoph Waldhauser",
        "title": "Active extension portfolio optimization with non-convex risk measures\n  using metaheuristics",
        "comments": null,
        "journal-ref": "Proceedings of MENDEL 2014: 1-6. 2014",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimization of active extension portfolios. For this\npurpose, the optimization problem is rewritten as a stochastic programming\nmodel and solved using a clever multi-start local search heuristic, which turns\nout to provide stable solutions. The heuristic solutions are compared to\noptimization results of convex optimization solvers where applicable.\nFurthermore, the approach is applied to solve problems with non-convex risk\nmeasures, most notably to minimize Value-at-Risk. Numerical results using data\nfrom both the Dow Jones Industrial Average as well as the DAX 30 are shown.\n"
    },
    {
        "paper_id": 1406.7752,
        "authors": "Samuel R\\\"onnqvist and Peter Sarlin",
        "title": "Bank Networks from Text: Interrelations, Centrality and Determinants",
        "comments": "Quantitative Finance, forthcoming in 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the wake of the still ongoing global financial crisis, bank\ninterdependencies have come into focus in trying to assess linkages among banks\nand systemic risk. To date, such analysis has largely been based on numerical\ndata. By contrast, this study attempts to gain further insight into bank\ninterconnections by tapping into financial discourse. We present a\ntext-to-network process, which has its basis in co-occurrences of bank names\nand can be analyzed quantitatively and visualized. To quantify bank importance,\nwe propose an information centrality measure to rank and assess trends of bank\ncentrality in discussion. For qualitative assessment of bank networks, we put\nforward a visual, interactive interface for better illustrating network\nstructures. We illustrate the text-based approach on European Large and Complex\nBanking Groups (LCBGs) during the ongoing financial crisis by quantifying bank\ninterrelations and centrality from discussion in 3M news articles, spanning\n2007Q1 to 2014Q3.\n"
    },
    {
        "paper_id": 1406.7775,
        "authors": "Maria Rocha Sousa, Jo\\~ao Gama, Manuel J. Silva Gon\\c{c}alves",
        "title": "A two-stage model for dealing with temporal degradation of credit\n  scoring",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is attached to the BRICS 2013 competition. We propose a two-stage\nmodel for dealing with the temporal degradation of credit scoring models. This\nmethodology produced motivating results in a 1-year horizon. We anticipate that\nit can be extended to other applications of risk assessment with great success.\nFuture extensions should cover predictions in larger time frames and consider\nlagged periods. This methodology can be further improved if more information\nabout the economic cycles is integrated in the forecasting of default.\n"
    },
    {
        "paper_id": 1407.0108,
        "authors": "Ulrich Horst and Jinniao Qiu and Qi Zhang",
        "title": "A Constrained Control Problem with Degenerate Coefficients and\n  Degenerate Backward SPDEs with Singular Terminal Condition",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a constrained optimal control problem with possibly degenerate\ncoefficients arising in models of optimal portfolio liquidation under market\nimpact. The coefficients can be random in which case the value function is\ndescribed by a degenerate backward stochastic partial differential equation\n(BSPDE) with singular terminal condition. For this degenerate BSPDE, we prove\nexistence and uniqueness of a nonnegative solution. Our existence result\nrequires a novel gradient estimate for degenerate BSPDEs.\n"
    },
    {
        "paper_id": 1407.0225,
        "authors": "Federica Cerina, Zhen Zhu, Alessandro Chessa, Massimo Riccaboni",
        "title": "World Input-Output Network",
        "comments": "24 pages, 16 figures",
        "journal-ref": "PLoS ONE 10(7): e0134025 (2015)",
        "doi": "10.1371/journal.pone.0134025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic systems, traditionally analyzed as almost independent national\nsystems, are increasingly connected on a global scale. Only recently becoming\navailable, the World Input-Output Database (WIOD) is one of the first efforts\nto construct the multi-regional input-output (MRIO) tables at the global level.\nBy viewing the world input-output system as an interdependent network where the\nnodes are the individual industries in different economies and the edges are\nthe monetary goods flows between industries, we study the network properties of\nthe so-called world input-output network (WION) and document its evolution over\ntime. We are able to quantify not only some global network properties such as\nassortativity, clustering coefficient, and degree and strength distributions,\nbut also its subgraph structure and dynamics by using community detection\ntechniques. Over time, we detect a marked increase in cross-country\nconnectivity of the production system, only temporarily interrupted by the\n2008-2009 crisis. Moreover, we find a growing input-output regional community\nin Europe led by Germany and the rise of China in the global production system.\nFinally, we use the network-based PageRank centrality and community coreness\nmeasure to identify the key industries and economies in the WION and the\nresults are different from the one obtained by the traditional\nfinal-demand-weighted backward linkage measure.\n"
    },
    {
        "paper_id": 1407.0256,
        "authors": "Andrey Itkin",
        "title": "To sigmoid-based functional description of the volatility smile",
        "comments": "32 pages, 18 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new static parameterization of the implied volatility surface\nwhich is constructed by using polynomials of sigmoid functions combined with\nsome other terms. This parameterization is flexible enough to fit market\nimplied volatilities which demonstrate smile or skew. An arbitrage-free\ncalibration algorithm is considered that constructs the implied volatility\nsurface as a grid in the strike-expiration space and guarantees a lack of\narbitrage at every node of this grid. We also demonstrate how to construct an\narbitrage-free interpolation and extrapolation in time, as well as build a\nlocal volatility and implied pdf surfaces. Asymptotic behavior of this\nparameterization is discussed, as well as results on stability of the\ncalibrated parameters are presented. Numerical examples show robustness of the\nproposed approach in building all these surfaces as well as demonstrate a\nbetter quality of the fit as compared with some known models.\n"
    },
    {
        "paper_id": 1407.0433,
        "authors": "Reza Arghandeh, Jeremy Woyak, Ahmet Onen, Jaesung Jung, Robert P.\n  Broadwater",
        "title": "Economic Optimal Operation of Community Energy Storage Systems in\n  Competitive Energy Markets",
        "comments": "17 Pages, submitted to Applied Energy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distributed, controllable energy storage devices offer several benefits to\nelectric power system operation. Three such benefits include reducing peak\nload, providing standby power, and enhancing power quality. These benefits,\nhowever, are only realized during peak load or during an outage, events that\nare infrequent. This paper presents a means of realizing additional benefits by\ntaking advantage of the fluctuating costs of energy in competitive energy\nmarkets. An algorithm for optimal charge/discharge scheduling of community\nenergy storage (CES) devices as well as an analysis of several of the key\ndrivers of the optimization are discussed.\n"
    },
    {
        "paper_id": 1407.0517,
        "authors": "Paz Grimberg and Zeev Schuss",
        "title": "Stochastic model of a pension plan",
        "comments": "41 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Structuring a viable pension plan is a problem that arises in the study of\nfinancial contracts pricing and bears special importance these days.\nDeterministic pension models often rely on projections that are based on\nseveral assumptions concerning the \"average\" long-time behavior of the stock\nmarket. Our aim here is to examine some of the popular \"average\" assumptions in\na more realistic setting of a stochastic model. Thus, we examine the contention\nthat investment in the stock market is similar to gambling in a casino, while\npurchasing companies, after due diligence, is safer under the premise that\nacting as a holding company that wholly owns other companies avoids some of the\nstock market risks. We show that the stock market index faithfully reflects its\ncompanies' profits at the time of their publication. We compare the shifted\nhistorical dynamics of the S\\&P500's aggregated financial earnings to its\nvalue, and find a high degree of correlation. We conclude that there is no\nbenefit to a pension fund in wholly owning a super trust. We verify, by\nexamining historical data, that stock earnings follow an exponential\n(geometric) Brownian motion and estimate its parameters. The robustness of this\nmodel is examined by an estimate of a pensioner's accumulated assets over a\nsaving period. We also estimate the survival probability and mean survival time\nof the accumulated individual fund with pension consumption over the residual\nlife of the pensioner.\n"
    },
    {
        "paper_id": 1407.0787,
        "authors": "Ekaterina Svetlova (Universit\\\"at Konstanz, Karlshochschule\n  International University), Henk van Elst (Karlshochschule International\n  University)",
        "title": "Decision-theoretic approaches to non-knowledge in economics",
        "comments": "13 pages, LaTeX2e, hyperlinked references. To appear in \"Routledge\n  International Handbook of Ignorance Studies,\" edited by Matthias Gro{\\ss} and\n  Linsey McGoey (London: Routledge), due to be published in February 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review two strands of conceptual approaches to the formal representation\nof a decision maker's non-knowledge at the initial stage of a static\none-person, one-shot decision problem in economic theory. One focuses on\nrepresentations of non-knowledge in terms of probability measures over sets of\nmutually exclusive and exhaustive consequence-relevant states of Nature, the\nother deals with unawareness of potentially important events by means of sets\nof states that are less complete than the full set of consequence-relevant\nstates of Nature. We supplement our review with a brief discussion of\nunresolved matters in both approaches.\n"
    },
    {
        "paper_id": 1407.0948,
        "authors": "Matteo Burzoni, Marco Frittelli and Marco Maggis",
        "title": "Universal Arbitrage Aggregator in Discrete Time Markets under\n  Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a model independent discrete time financial market, we discuss the\nrichness of the family of martingale measures in relation to different notions\nof Arbitrage, generated by a class $\\mathcal{S}$ of significant sets, which we\ncall Arbitrage de la classe $\\mathcal{S}$. The choice of $\\mathcal{S}$ reflects\ninto the intrinsic properties of the class of polar sets of martingale\nmeasures. In particular: for S=${\\Omega}$ absence of Model Independent\nArbitrage is equivalent to the existence of a martingale measure; for\n$\\mathcal{S}$ being the open sets, absence of Open Arbitrage is equivalent to\nthe existence of full support martingale measures. These results are obtained\nby adopting a technical filtration enlargement and by constructing a universal\naggregator of all arbitrage opportunities. We further introduce the notion of\nmarket feasibility and provide its characterization via arbitrage conditions.\nWe conclude providing a dual representation of Open Arbitrage in terms of\nweakly open sets of probability measures, which highlights the robust nature of\nthis concept.\n"
    },
    {
        "paper_id": 1407.1072,
        "authors": "Alessandro Ramponi",
        "title": "On a Transform Method for the Efficient Computation of Conditional VaR\n  (and VaR) with Application to Loss Models with Jumps and Stochastic\n  Volatility",
        "comments": "23 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1007/s11009-015-9446-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider Fourier transform techniques to efficiently compute\nthe Value-at-Risk and the Conditional Value-at-Risk of an arbitrary loss random\nvariable, characterized by having a computable generalized characteristic\nfunction. We exploit the property of these risk measures of being the solution\nof an elementary optimization problem of convex type in one dimension for which\nFast and Fractional Fourier transform can be implemented. An application to\nunivariate loss models driven by L\\'{e}vy or stochastic volatility risk factors\ndynamic is finally reported.\n"
    },
    {
        "paper_id": 1407.1343,
        "authors": "Federico De Olivera and Ernesto Mordecki",
        "title": "Computing Greeks for L\\'evy Models: The Fourier Transform Approach",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The computation of Greeks for exponential L\\'evy models are usually\napproached by Malliavin Calculus and other methods, as the Likelihood Ratio and\nthe finite difference method. In this paper we obtain exact formulas for Greeks\nof European options based on the Lewis formula for the option value. Therefore,\nit is possible to obtain accurate approximations using Fast Fourier Transform.\nWe will present an exhaustive development of Greeks for Call options. The error\nis shown for all Greeks in the Black-Scholes model, where Greeks can be exactly\ncomputed. Other models used in the literature are compared, such as the Merton\nand Variance Gamma models. The presented formulas can reach desired accuracy\nbecause our approach generates error only by approximation of the integral.\n"
    },
    {
        "paper_id": 1407.1453,
        "authors": "Tahir Choulli and Jun Deng",
        "title": "Non-arbitrage for Informational Discrete Time Market Models",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the stability of the non-arbitrage condition in\ndiscrete time market models when some unknown information $\\tau$ is\npartially/fully incorporated into the market. Our main conclusions are twofold.\nOn the one hand, for a fixed market $S$, we prove that the non-arbitrage\ncondition is preserved under a mild condition. On the other hand, we give the\nnecessary and sufficient equivalent conditions on the unknown information\n$\\tau$ to ensure the validity of the non-arbitrage condition for any market.\nTwo concrete examples are presented to illustrate the importance of these\nconditions, where we calculate explicitly the arbitrage opportunities when they\nexist.\n"
    },
    {
        "paper_id": 1407.1595,
        "authors": "Dalia Ibrahim, Fr\\'ed\\'eric Abergel (MAS, FiQuant)",
        "title": "Non-linear filtering and optimal investment under partial information\n  for stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the question of filtering and maximizing terminal wealth\nfrom expected utility in a partially information stochastic volatility models.\nThe special features is that the only information available to the investor is\nthe one generated by the asset prices, and the unobservable processes will be\nmodeled by a stochastic differential equations. Using the change of measure\ntechniques, the partial observation context can be transformed into a full\ninformation context such that coefficients depend only on past history of\nobserved prices (filters processes). Adapting the stochastic non-linear\nfiltering, we show that under some assumptions on the model coefficients, the\nestimation of the filters depend on a priorimodels for the trend and the\nstochastic volatility. Moreover, these filters satisfy a stochastic partial\ndifferential equations named \"Kushner-Stratonovich equations\". Using the\nmartingale duality approach in this partially observed incomplete model, we can\ncharacterize the value function and the optimal portfolio. The main result here\nis that the dual value function associated to the martingale approach can be\nexpressed, via the dynamic programmingapproach, in terms of the solution to a\nsemilinear partial differential equation. We illustrate our results with some\nexamples of stochastic volatility models popular in the financial literature.\n"
    },
    {
        "paper_id": 1407.1674,
        "authors": "Marcel Nutz",
        "title": "Robust Superhedging with Jumps and Diffusion",
        "comments": "Forthcoming in 'Stochastic Processes and their Applications'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a nondominated version of the optional decomposition theorem in\na setting that includes jump processes with nonvanishing diffusion as well as\ngeneral continuous processes. This result is used to derive a robust\nsuperhedging duality and the existence of an optimal superhedging strategy for\ngeneral contingent claims. We illustrate the main results in the framework of\nnonlinear L\\'evy processes.\n"
    },
    {
        "paper_id": 1407.1715,
        "authors": "Alexander Gairat and Vadim Shcherbakov",
        "title": "Density of Skew Brownian motion and its functionals with application in\n  finance",
        "comments": "1 section and two figures are added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive the joint density of a Skew Brownian motion, its last visit to the\norigin, local and occupation times. The result is applied to option pricing in\na two valued local volatility model and in a displaced diffusion model with\nconstrained volatility.\n"
    },
    {
        "paper_id": 1407.1726,
        "authors": "Eiji Yamamura and Fabio Sabatini",
        "title": "Superstars in politics: the role of the media in the rise and success of\n  Junichiro Koizumi",
        "comments": "Keywords mass media, television, newspapers, elections, Koizumi,\n  Japan, superstar effect",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the role of mass media in people perceptions of\ncharismatic leaders, focusing on the case of Junichiro Koizumi, Prime Minister\nof Japan from 2001 to 2006. Using survey data collected immediately after his\n2005 landslide electoral victory, this study empirically assesses the influence\nof television and newspapers on support for Koizumi and for the most\ndistinctive policy action he announced during his campaign, the privatization\nof the postal service.\n"
    },
    {
        "paper_id": 1407.1769,
        "authors": "Sebastian E. Ferrando, Alfredo L. Gonzalez, Ivan L. Degano and\n  Massoome Rahsepar",
        "title": "Discrete, Non Probabilistic Market Models. Arbitrage and Pricing\n  Intervals",
        "comments": "Version 2, from June 12, 2015, supersedes the version of July 7 2014.\n  The changes are numerous and substantial. Version3, November 4, 2015, much\n  polished version, notation and notions consistent with sequel paper\n  (appearing as reference [12] in this version)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper develops general, discrete, non-probabilistic market models and\nminmax price bounds leading to price intervals for European options. The\napproach provides the trajectory based analogue of martingale-like properties\nas well as a generalization that allows a limited notion of arbitrage in the\nmarket while still providing coherent option prices. Several properties of the\nprice bounds are obtained, in particular a connection with risk neutral pricing\nis established for trajectory markets associated to a continuous-time\nmartingale model.\n"
    },
    {
        "paper_id": 1407.2031,
        "authors": "Paolo Barucca",
        "title": "Localization in covariance matrices of coupled heterogenous\n  Ornstein-Uhlenbeck processes",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.90.062129",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define a random-matrix ensemble given by the infinite-time covariance\nmatrices of Ornstein-Uhlenbeck processes at different temperatures coupled by a\nGaussian symmetric matrix. The spectral properties of this ensemble are shown\nto be in qualitative agreement with some stylized facts of financial markets.\nThrough the presented model formulas are given for the analysis of\nheterogeneous time-series. Furthermore evidence for a localization transition\nin eigenvectors related to small and large eigenvalues in cross-correlations\nanalysis of this model is found and a simple explanation of localization\nphenomena in financial time-series is provided. Finally we identify both in our\nmodel and in real financial data an inverted-bell effect in correlation between\nlocalized components and their local temperature: high and low\ntemperature/volatility components are the most localized ones.\n"
    },
    {
        "paper_id": 1407.242,
        "authors": "Umut \\c{C}etin, Albina Danilova",
        "title": "Markovian Nash equilibrium in financial markets with asymmetric\n  information and related forward-backward systems",
        "comments": "Published at http://dx.doi.org/10.1214/15-AAP1138 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2016, Vol. 26, No. 4, 1996-2029",
        "doi": "10.1214/15-AAP1138",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a new methodology for studying continuous-time Nash\nequilibrium in a financial market with asymmetrically informed agents. This\napproach allows us to lift the restriction of risk neutrality imposed on market\nmakers by the current literature. It turns out that, when the market makers are\nrisk averse, the optimal strategies of the agents are solutions of a\nforward-backward system of partial and stochastic differential equations. In\nparticular, the price set by the market makers solves a nonstandard \"quadratic\"\nbackward stochastic differential equation. The main result of the paper is the\nexistence of a Markovian solution to this forward-backward system on an\narbitrary time interval, which is obtained via a fixed-point argument on the\nspace of absolutely continuous distribution functions. Moreover, the\nequilibrium obtained in this paper is able to explain several stylized facts\nwhich are not captured by the current asymmetric information models.\n"
    },
    {
        "paper_id": 1407.2514,
        "authors": "Friedrich Hubalek, Martin Keller-Ressel, Carlo Sgarra",
        "title": "Geometric Asian Option Pricing in General Affine Stochastic Volatility\n  Models with Jumps",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present some results on Geometric Asian option valuation for\naffine stochastic volatility models with jumps. We shall provide a general\nframework into which several different valuation problems based on some average\nprocess can be cast, and we shall obtain close-form solutions for some relevant\naffine model classes.\n"
    },
    {
        "paper_id": 1407.2642,
        "authors": "Nick Polson and Jan Hendrik Witte",
        "title": "A Bellman View of Jesse Livermore",
        "comments": "4 Pages, 0 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Richard Bellman's Principle of Optimality, formulated in 1957, is the heart\nof dynamic programming, the mathematical discipline which studies the optimal\nsolution of multi-period decision problems. In this paper, we look at the main\ntrading principles of Jesse Livermore, the legendary stock operator whose\nmethod was published in 1923, from a Bellman point of view.\n"
    },
    {
        "paper_id": 1407.2677,
        "authors": "H.M.S.P. Herath, Cao Liang, Chen Yongbing",
        "title": "Impacts of Regional Trade Agreements(RTAs) on Food Security: A Case of\n  ASEAN Free Trade Agreement",
        "comments": null,
        "journal-ref": "International Journal of Social Science & Interdisciplinary\n  Research, Vol.3(3) 2014, ISSN:2277-3630",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Discriminatory trade liberalization policies are becoming more popular among\nworld economies. Countries are motivated to enter for regional trade agreements\nto capture faster economic growth for alleviating poverty. In developing\neconomies like most of the member countries of the Association of South East\nAsian Nations (ASEAN), a sizeable portion of people are suffering from poverty\nby exposing them to food insecurity. Low level of income and low productivity\nof agricultural sector have augmented the severity of food insecurity of those\npeople. Discriminatory trade liberalization policies are expected to reduce\npoverty and strengthen the food security. The objective of this paper is to\nexamine the effect of ASEAN Free Trade Agreement (AFTA) on food security of its\nmember countries. The multiple regression analysis in panel data was employed\nto disentangle the impacts of trade liberalization on food securit y with use\nof regional trade agreement dummy variable. The finding of the study supports\nthat AFTA has influenced positively on food security of its member nations.\nAfter the formation of AFTA, the level of per-capita daily dietary energy\nsupply of the member countries has been increased moderately over time.\n"
    },
    {
        "paper_id": 1407.3154,
        "authors": "Ljudmila A. Bordag, Ivan P. Yamshchikov and Dmitry Zhelezov",
        "title": "Portfolio optimization in the case of an asset with a given liquidation\n  time distribution",
        "comments": "30 pages, 1 figure",
        "journal-ref": "Int. J. Eng. Math. Model, vol. 2, no. 2, pp.31-50, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Management of the portfolios containing low liquidity assets is a tedious\nproblem. The buyer proposes the price that can differ greatly from the paper\nvalue estimated by the seller, the seller, on the other hand, can not liquidate\nhis portfolio instantly and waits for a more favorable offer. To minimize\nlosses in this case we need to develop new methods. One of the steps moving the\ntheory towards practical needs is to take into account the time lag of the\nliquidation of an illiquid asset. This task became especially significant for\nthe practitioners in the time of the global financial crises. Working in the\nMerton's optimal consumption framework with continuous time we consider an\noptimization problem for a portfolio with an illiquid, a risky and a risk-free\nasset. While a standard Black-Scholes market describes the liquid part of the\ninvestment the illiquid asset is sold at a random moment with prescribed\nliquidation time distribution. In the moment of liquidation it generates\nadditional liquid wealth dependent on illiquid assets paper value. The investor\nhas the logarithmic utility function as a limit case of a HARA-type utility.\nDifferent distributions of the liquidation time of the illiquid asset are under\nconsideration - a classical exponential distribution and Weibull distribution\nthat is more practically relevant. Under certain conditions we show the\nexistence of the viscosity solution in both cases. Applying numerical methods\nwe compare classical Merton's strategies and the optimal consumption-allocation\nstrategies for portfolios with different liquidation-time distributions of an\nilliquid asset.\n"
    },
    {
        "paper_id": 1407.318,
        "authors": "F. Pedroche, R. Criado, E. Garcia, M. Romance and V.E. Sanchez",
        "title": "Comparing series of rankings with ties by using complex networks: An\n  analysis of the spanish stock market (IBEX-35 index)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend the concept of Competitivity Graph to compare series\nof rankings with ties ({\\em partial rankings}). We extend the usual method used\nto compute Kendall's coefficient for two partial rankings to the concept of\nevolutive Kendall's coefficient for a series of partial rankings. The\ntheoretical framework consists of a four-layer multiplex network. Regarding the\ntreatment of ties, our approach allows to define a tie between two values when\nthey are close {\\em enough}, depending on a threshold. We show an application\nusing data from the Spanish Stock Market; we analyse the series of rankings\ndefined by $25$ companies that have contributed to the IBEX-35 return and\nvolatility values over the period 2003 to 2013.\n"
    },
    {
        "paper_id": 1407.3201,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Warehousing Credit (CVA) Risk, Capital (KVA) and Tax (TVA) Consequences",
        "comments": "15 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk may be warehoused by choice, or because of limited hedging\npossibilities. Credit risk warehousing increases capital requirements and\nleaves open risk. Open risk must be priced in the physical measure, rather than\nthe risk neutral measure, and implies profits and losses. Furthermore the rate\nof return on capital that shareholders require must be paid from profits.\nProfits are taxable and losses provide tax credits. Here we extend the\nsemi-replication approach of Burgard and Kjaer (2013) and the capital formalism\n(KVA) of Green, Kenyon, and Dennis (2014) to cover credit risk warehousing and\ntax, formalized as double-semi-replication and TVA (Tax Valuation Adjustment)\nto enable quantification.\n"
    },
    {
        "paper_id": 1407.3372,
        "authors": "Przemys{\\l}aw Rola",
        "title": "Arbitrage in markets with bid-ask spreads",
        "comments": "18 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper a finite discrete time market with an arbitrary state space and\nbid-ask spreads is considered. The notion of an equivalent bid-ask martingale\nmeasure (EBAMM) is introduced and the fundamental theorem of asset pricing is\nproved using (EBAMM) as an equivalent condition for no-arbitrage. The\nCox-Ross-Rubinstein model with bid-ask spreads is presented as an application\nof our results.\n"
    },
    {
        "paper_id": 1407.339,
        "authors": "X. Brokmann, E. Serie, J. Kockelkoren, J.-P. Bouchaud",
        "title": "Slow decay of impact in equity markets",
        "comments": "6 pages, submitted to \"Market Microstructure & Liquidity\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a proprietary dataset of meta-orders and prediction signals, and\nassuming a quasi-linear impact model, we deconvolve market impact from past\ncorrelated trades and a predictable return component to elicit the temporal\ndependence of the market impact of a single daily meta-order, over a ten day\nhorizon in various equity markets. We find that the impact of single\nmeta-orders is to a first approximation universal and slowly decays to zero (or\nto a small value), possibly as a power-law. We show that auto-correlated\norder-flows and trade information contents fully accounts for the apparent\nplateau observed in the raw data. We discuss the possible bias introduced by\nthe quasi-linear assumption.\n"
    },
    {
        "paper_id": 1407.3652,
        "authors": "Lucas Fievet, Zal\\`an Forr\\`o, Peter Cauwels, Didier Sornette",
        "title": "Forecasting future oil production in Norway and the UK: a general\n  improved methodology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new Monte-Carlo methodology to forecast the crude oil production\nof Norway and the U.K. based on a two-step process, (i) the nonlinear\nextrapolation of the current/past performances of individual oil fields and\n(ii) a stochastic model of the frequency of future oil field discoveries.\nCompared with the standard methodology that tends to underestimate remaining\noil reserves, our method gives a better description of future oil production,\nas validated by our back-tests starting in 2008. Specifically, we predict\nremaining reserves extractable until 2030 to be 188 +/- 10 million barrels for\nNorway and 98 +/- 10 million barrels for the UK, which are respectively 45% and\n66% above the predictions using the standard methodology.\n"
    },
    {
        "paper_id": 1407.3742,
        "authors": "Behlool Sabir and M. S. Santhanam",
        "title": "Record statistics of financial time series and geometric random walks",
        "comments": "4 pages, 5 figures",
        "journal-ref": "Phys. Rev. E 90, 032126 (2014)",
        "doi": "10.1103/PhysRevE.90.032126",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study of record statistics of correlated series is gaining momentum. In\nthis work, we study the records statistics of the time series of select stock\nmarket data and the geometric random walk, primarily through simulations. We\nshow that the distribution of the age of records is a power law with the\nexponent $\\alpha$ lying in the range $1.5 \\le \\alpha \\le 1.8$. Further, the\nlongest record ages follow the Fr\\'{e}chet distribution of extreme value\ntheory. The records statistics of geometric random walk series is in good\nagreement with that from the empirical stock data.\n"
    },
    {
        "paper_id": 1407.3749,
        "authors": "Maria Letizia Bertotti, Giovanni Modanese",
        "title": "Microscopic Models for Welfare Measures Addressing a Reduction of\n  Economic Inequality",
        "comments": "15 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1403.0015",
        "journal-ref": "Complexity 21 (2016) 89-98",
        "doi": "10.1002/cplx.21669",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate a flexible micro-to-macro kinetic model which is able to explain\nthe emergence of income profiles out of a whole of individual economic\ninteractions. The model is expressed by a system of several nonlinear\ndifferential equations which involve parameters defined by probabilities.\nSociety is described as an ensemble of individuals divided into income classes;\nthe individuals exchange money through binary and ternary interactions, leaving\nthe total wealth unchanged. The ternary interactions represent taxation and\nredistribution effects. Dynamics is investigated through computational\nsimulations, the focus being on the effects that different fiscal policies and\ndifferently weighted welfare policies have on the long-run income\ndistributions. The model provides a tool which may contribute to the\nidentification of the most effective actions towards a reduction of economic\ninequality. We find for instance that, under certain hypotheses, the Gini index\nis more affected by a policy of reduction of the welfare and subsidies for the\nrich classes than by an increase of the upper tax rate. Such a policy also has\nthe effect of slightly increasing the total tax revenue.\n"
    },
    {
        "paper_id": 1407.4452,
        "authors": "Nick Laskin",
        "title": "New Pricing Framework: Options and Bonds",
        "comments": "V2 includes 62 pages and 4 Tables. Generalized Greeks have been\n  introduced and calculated in Sec.7.3. Table 4 has been added to display the\n  generalized Greeks. Appendix B has been added to show that in diffusion\n  approximation the equation for the new Greek theta goes into the well-known\n  Black-Scholes Greek theta. Typos have been corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A unified analytical pricing framework with involvement of the shot noise\nrandom process has been introduced and elaborated. Two exactly solvable new\nmodels have been developed. The first model has been designed to value options.\nIt is assumed that asset price stochastic dynamics follows a Geometric Shot\nNoise motion. A new arbitrage-free integro-differential option pricing equation\nhas been found and solved. The put-call parity has been proved and the Greeks\nhave been calculated. Three additional new Greeks associated with market model\nparameters have been introduced and evaluated. It has been shown that in\ndiffusion approximation the developed option pricing model incorporates the\nwell-known Black-Scholes equation and its solution. The stochastic dynamic\norigin of the Black-Scholes volatility has been uncovered. The new option\npricing model has been generalized based on asset price dynamics modeled by the\nsuperposition of Geometric Brownian motion and Geometric Shot Noise. To model\nstochastic dynamics of a short term interest rate, the second model has been\nintroduced and developed based on Langevin type equation with shot noise. A new\nbond pricing formula has been obtained. It has been shown that in diffusion\napproximation the developed bond pricing formula goes into the well-known\nVasicek solution. The stochastic dynamic origin of the long-term mean and\ninstantaneous volatility of the Vasicek model has been uncovered. A generalized\nbond pricing model has been introduced and developed based on short term\ninterest rate stochastic dynamics modeled by superposition of a standard Wiener\nprocess and shot noise. Despite the non-Gaussianity of probability\ndistributions involved, all newly elaborated models have the same degree of\nanalytical tractability as the Black-Scholes model and the Vasicek model.\n"
    },
    {
        "paper_id": 1407.4512,
        "authors": "Ioane Muni Toke",
        "title": "Exact and asymptotic solutions of the call auction problem",
        "comments": "24 pages, 7 figures",
        "journal-ref": "Market microstructure and liquidity, 1(01), 1550001. (2015)",
        "doi": "10.1142/S238262661550001X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The call auction is a widely used trading mechanism, especially during the\nopening and closing periods of financial markets. In this paper, we study a\nstandard call auction problem where orders are submitted according to Poisson\nprocesses, with random prices distributed according to a general distribution,\nand may be cancelled at any time. We compute the analytical expressions of the\ndistributions of the traded volume, of the lower and upper bounds of the\nclearing prices, and of the price range of these possible clearing prices of\nthe call auction. Using results from the theory of order statistics and a\ntheorem on the limit of sequences of random variables with independent random\nindices, we derive the weak limits of all these distributions. In this setting,\ntraded volume and bounds of the clearing prices are found to be asymptotically\nnormal, while the clearing price range is asymptotically exponential. All the\nparameters of these distributions are explicitly derived as functions of the\nparameters of the incoming orders' flows.\n"
    },
    {
        "paper_id": 1407.4614,
        "authors": "Olivier Gu\\'eant, Jean-Michel Lasry, Jiang Pu",
        "title": "A convex duality method for optimal liquidation with participation\n  constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In spite of the growing consideration for optimal execution in the financial\nmathematics literature, numerical approximations of optimal trading curves are\nalmost never discussed. In this article, we present a numerical method to\napproximate the optimal strategy of a trader willing to unwind a large\nportfolio. The method we propose is very general as it can be applied to\nmulti-asset portfolios with any form of execution costs, including a bid-ask\nspread component, even when participation constraints are imposed. Our method,\nbased on convex duality, only requires Hamiltonian functions to have $C^{1,1}$\nregularity while classical methods require additional regularity and cannot be\napplied to all cases found in practice.\n"
    },
    {
        "paper_id": 1407.4702,
        "authors": "Michal Sawa, Dariusz Grech",
        "title": "Identification of cross and autocorrelations in time series within an\n  approach based on Wigner eigenspectrum of random matrices",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an original and novel method based on random matrix approach that\nenables to distinguish the respective role of temporal autocorrelations inside\ngiven time series and cross correlations between various time series. The\nproposed algorithm is based on properties of Wigner eigenspectrum of random\nmatrices instead of commonly used Wishart eigenspectrum methodology. The\nproposed approach is then qualitatively and quantitatively applied to financial\ndata in stocks building WIG (Warsaw Stock Exchange Index).\n"
    },
    {
        "paper_id": 1407.4864,
        "authors": "Leunglung Chan and Song-Ping Zhu",
        "title": "An exact and explicit formula for pricing lookback options with regime\n  switching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the pricing of European-style lookback options when\nthe price dynamics of the underlying risky asset are assumed to follow a\nMarkov-modulated Geo-metric Brownian motion; that is, the appreciation rate and\nthe volatility of the underlying risky asset depend on unobservable states of\nthe economy described by a continuous-time hidden Markov chain process. We\nderive an exact, explicit and closed-form solution for European-style lookback\noptions in a two-state regime switching model.\n"
    },
    {
        "paper_id": 1407.502,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Causal Non-Linear Financial Networks",
        "comments": "11 pages, 9 figures, submitted to Financial Risk & Network Theory\n  seminar",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our previous study we have presented an approach to studying lead--lag\neffect in financial markets using information and network theories. Methodology\npresented there, as well as previous studies using Pearson's correlation for\nthe same purpose, approached the concept of lead--lag effect in a naive way. In\nthis paper we further investigate the lead--lag effect in financial markets,\nthis time treating them as causal effects. To incorporate causality in a manner\nconsistent with our previous study, that is including non-linear\ninterdependencies, we base this study on a generalisation of Granger causality\nin the form of transfer entropy, or equivalently a special case of conditional\n(partial) mutual information. This way we are able to produce networks of\nstocks, where directed links represent causal relationships for a specific time\nlag. We apply this procedure to stocks belonging to the NYSE 100 index for\nvarious time lags, to investigate the short-term causality on this market, and\nto comment on the resulting Bonferroni networks.\n"
    },
    {
        "paper_id": 1407.5037,
        "authors": "Vladimir Filimonov, Didier Sornette",
        "title": "Power law scaling and \"Dragon-Kings\" in distributions of intraday\n  financial drawdowns",
        "comments": null,
        "journal-ref": "Chaos, Solitons & Fractals, 2015, 74 (5), 27-45",
        "doi": "10.1016/j.chaos.2014.12.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the distributions of epsilon-drawdowns and epsilon-drawups of\nthe most liquid futures financial contracts of the world at time scales of 30\nseconds. The epsilon-drawdowns (resp. epsilon- drawups) generalise the notion\nof runs of negative (resp. positive) returns so as to capture the risks to\nwhich investors are arguably the most concerned with. Similarly to the\ndistribution of returns, we find that the distributions of epsilon-drawdowns\nand epsilon-drawups exhibit power law tails, albeit with exponents\nsignificantly larger than those for the return distributions. This paradoxical\nresult can be attributed to (i) the existence of significant transient\ndependence between returns and (ii) the presence of large outliers\n(dragon-kings) characterizing the extreme tail of the drawdown/drawup\ndistributions deviating from the power law. The study of the tail dependence\nbetween the sizes, speeds and durations of drawdown/drawup indicates a clear\nrelationship between size and speed but none between size and duration. This\nimplies that the most extreme drawdown/drawup tend to occur fast and are\ndominated by a few very large returns. We discuss both the endogenous and\nexogenous origins of these extreme events.\n"
    },
    {
        "paper_id": 1407.5091,
        "authors": "Leunglung Chan and Song-Ping Zhu",
        "title": "An exact and explicit formula for pricing Asian options with regime\n  switching",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1407.4864",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the pricing of European-style Asian options when the price\ndynamics of the underlying risky asset are assumed to follow a Markov-\nmodulated geometric Brownian motion; that is, the appreciation rate and the\nvolatility of the underlying risky asset depend on unobservable states of the\neconomy described by a continuous-time hidden Markov process. We derive the\nexact, explicit and closed-form solutions for European-style Asian options in a\ntwo-state regime switching model.\n"
    },
    {
        "paper_id": 1407.5139,
        "authors": "Erhan Bayraktar and Alexander Munk",
        "title": "Comparing the $G$-Normal Distribution to its Classical Counterpart",
        "comments": "Final version. To appear in Communications on Stochastic Analysis.\n  Title has changed. Keywords: sublinear expectation, multidimensional\n  $G$-normal distribution, independence",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In one dimension, the theory of the $G$-normal distribution is\nwell-developed, and many results from the classical setting have a nonlinear\ncounterpart. Significant challenges remain in multiple dimensions, and some of\nwhat has already been discovered is quite nonintuitive. By answering several\nclassically-inspired questions concerning independence, covariance uncertainty,\nand behavior under certain linear operations, we continue to highlight the\nfascinating range of unexpected attributes of the multidimensional $G$-normal\ndistribution.\n"
    },
    {
        "paper_id": 1407.5254,
        "authors": "Cina Aghamohammadi, Mehran Ebrahimian, and Hamed Tahmooresi",
        "title": "Permutation approach, high frequency trading and variety of micro\n  patterns in financial time series",
        "comments": "10 pages, 4 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 413 (2014),\n  pp. 25-30",
        "doi": "10.1016/j.physa.2014.06.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Permutation approach is suggested as a method to investigate financial time\nseries in micro scales. The method is used to see how high frequency trading in\nrecent years has affected the micro patterns which may be seen in financial\ntime series. Tick to tick exchange rates are considered as examples. It is seen\nthat variety of patterns evolve through time; and that the scale over which the\ntarget markets have no dominant patterns, have decreased steadily over time\nwith the emergence of higher frequency trading.\n"
    },
    {
        "paper_id": 1407.5258,
        "authors": "Jun-jie Chen, Bo Zheng, Lei Tan",
        "title": "Agent-based model with asymmetric trading and herding for complex\n  financial systems",
        "comments": "17 pages, 6 figures",
        "journal-ref": "PLoS ONE 8(11): e79531",
        "doi": "10.1371/journal.pone.0079531",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Background: For complex financial systems, the negative and positive\nreturn-volatility correlations, i.e., the so-called leverage and anti-leverage\neffects, are particularly important for the understanding of the price\ndynamics. However, the microscopic origination of the leverage and\nanti-leverage effects is still not understood, and how to produce these effects\nin agent-based modeling remains open. On the other hand, in constructing\nmicroscopic models, it is a promising conception to determine model parameters\nfrom empirical data rather than from statistical fitting of the results.\n  Methods: To study the microscopic origination of the return-volatility\ncorrelation in financial systems, we take into account the individual and\ncollective behaviors of investors in real markets, and construct an agent-based\nmodel. The agents are linked with each other and trade in groups, and\nparticularly, two novel microscopic mechanisms, i.e., investors' asymmetric\ntrading and herding in bull and bear markets, are introduced. Further, we\npropose effective methods to determine the key parameters in our model from\nhistorical market data.\n  Results: With the model parameters determined for six representative\nstock-market indices in the world respectively, we obtain the corresponding\nleverage or anti-leverage effect from the simulation, and the effect is in\nagreement with the empirical one on amplitude and duration. At the same time,\nour model produces other features of the real markets, such as the fat-tail\ndistribution of returns and the long-term correlation of volatilities.\n  Conclusions: We reveal that for the leverage and anti-leverage effects, both\nthe investors' asymmetric trading and herding are essential generation\nmechanisms. These two microscopic mechanisms and the methods for the\ndetermination of the key parameters can be applied to other complex systems\nwith similar asymmetries.\n"
    },
    {
        "paper_id": 1407.5278,
        "authors": "Grzegorz Andruszkiewicz and Mark H.A. Davis and S\\'ebastien Lleo",
        "title": "Risk-sensitive investment in a finite-factor model",
        "comments": "23 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new jump diffusion regime-switching model is introduced, which allows for\nlinking jumps in asset prices with regime changes. We prove the existence and\nuniqueness of the solution to the risk-sensitive asset management criterion\nmaximisation problem in this setting. We provide an ODE for the optimal value\nfunction, which may be efficiently solved numerically. Relevant probability\nmeasure changes are discussed in the appendix. The approach of Klebaner and\nLipster (2014) is used to prove the martingale property of the relevant density\nprocesses.\n"
    },
    {
        "paper_id": 1407.5305,
        "authors": "Christoph Aymanns, J. Doyne Farmer",
        "title": "The dynamics of the leverage cycle",
        "comments": "35 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple agent-based model of a financial system composed of\nleveraged investors such as banks that invest in stocks and manage their risk\nusing a Value-at-Risk constraint, based on historical observations of asset\nprices. The Value-at-Risk constraint implies that when perceived risk is low,\nleverage is high and vice versa, a phenomenon that has been dubbed pro-cyclical\nleverage. We show that this leads to endogenous irregular oscillations, in\nwhich gradual increases in stock prices and leverage are followed by drastic\nmarket collapses, i.e. a leverage cycle. This phenomenon is studied using\nsimplified models that give a deeper understanding of the dynamics and the\nnature of the feedback loops and instabilities underlying the leverage cycle.\nWe introduce a flexible leverage regulation policy in which it is possible to\ncontinuously tune from pro-cyclical to countercyclical leverage. When the\npolicy is sufficiently countercyclical and bank risk is sufficiently low the\nendogenous oscillation disappears and prices go to a fixed point. While there\nis always a leverage ceiling above which the dynamics are unstable,\ncountercyclical leverage can be used to raise the ceiling. We also study the\nimpact on leverage cycles of direct, temporal control of the bank's riskiness\nvia the bank's required Value-at-Risk quantile. Under such a rule the regulator\nrelaxes the Value-at-Risk quantile following a negative stock price shock and\ntightens it following a positive shock. While such a policy rule can reduce the\namplitude of leverage cycles, its effectiveness is highly dependent on the\nchoice of parameters. Finally, we investigate fixed limits on leverage and show\nhow they can control the leverage cycle.\n"
    },
    {
        "paper_id": 1407.5429,
        "authors": "Luca Marotta, Salvatore Miccich\\`e, Yoshi Fujiwara, Hiroshi Iyetomi,\n  Hideaki Aoyama, Mauro Gallegati, Rosario N. Mantegna",
        "title": "Bank-firm credit network in Japan. An analysis of a bipartite network",
        "comments": "9 pages, 4 figures, 2 Tables",
        "journal-ref": "PLOS, 10 (5), e0123079, (2015)",
        "doi": "10.1371/journal.pone.0123079",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an analysis of the credit market of Japan. The analysis is\nperformed by investigating the bipartite network of banks and firms which is\nobtained by setting a link between a bank and a firm when a credit relationship\nis present in a given time window. In our investigation we focus on a community\ndetection algorithm which is identifying communities composed by both banks and\nfirms. We show that the clusters obtained by directly working on the bipartite\nnetwork carry information about the networked nature of the Japanese credit\nmarket. Our analysis is performed for each calendar year during the time period\nfrom 1980 to 2011. Specifically, we obtain communities of banks and networks\nfor each of the 32 investigated years, and we introduce a method to track the\ntime evolution of these communities on a statistical basis. We then\ncharacterize communities by detecting the simultaneous over-expression of\nattributes of firms and banks. Specifically, we consider as attributes the\neconomic sector and the geographical location of firms and the type of banks.\nIn our 32 year long analysis we detect a persistence of the over-expression of\nattributes of clusters of banks and firms together with a slow dynamics of\nchanges from some specific attributes to new ones. Our empirical observations\nshow that the credit market in Japan is a networked market where the type of\nbanks, geographical location of firms and banks and economic sector of the firm\nplay a role in shaping the credit relationships between banks and firms.\n"
    },
    {
        "paper_id": 1407.5466,
        "authors": "Ladislav Kristoufek and Petra Lunackova",
        "title": "Rockets and feathers meet Joseph: Reinvestigating the oil-gasoline\n  asymmetry on the international markets",
        "comments": "20 pages, 2 figures, 4 tables",
        "journal-ref": "Energy Economics, Volume 49, May 2015, Pages 1-8",
        "doi": "10.1016/j.eneco.2015.01.013",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We reinvestigate the \"rockets and feathers\" effect between retail gasoline\nand crude oil prices in a new framework of fractional integration, long-term\nmemory and borderline (non-)stationarity. The most frequently used\nerror-correction model is examined in detail and we find that the prices return\nto their equilibrium value much more slowly than would be typical for the\nerror-correction model. Such dynamics is usually referred to as \"the Joseph\neffect\". The standard procedure is shown to be troublesome and we introduce\nthree new tests to investigate possible asymmetry in the price adjustment to\nequilibrium under these complicated time series characteristics. On the dataset\nof seven national gasoline prices, we report that apart from Belgium, there is\nno asymmetry found. The proposed methodology is not limited to the gasoline and\ncrude oil case but it can be utilized for any asymmetric adjustment to\nequilibrium analysis.\n"
    },
    {
        "paper_id": 1407.5528,
        "authors": "Petros Dellaportas, Aleksandar Mijatovi\\'c",
        "title": "Arbitrage-free prediction of the implied volatility smile",
        "comments": "18 pages, 2 figures; a shorter version of this paper has appeared as\n  a Technical Paper in Risk (30 April 2014) under the title \"Smile\n  transformation for price prediction\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper gives an arbitrage-free prediction for future prices of an\narbitrary co-terminal set of options with a given maturity, based on the\nobserved time series of these option prices. The statistical analysis of such a\nmulti-dimensional time series of option prices corresponding to $n$ strikes\n(with $n$ large, e.g. $n\\geq 40$) and the same maturity, is a difficult task\ndue to the fact that option prices at any moment in time satisfy non-linear and\nnon-explicit no-arbitrage restrictions. Hence any $n$-dimensional time series\nmodel also has to satisfy these implicit restrictions at each time step, a\ncondition that is impossible to meet since the model innovations can take\narbitrary values. We solve this problem for any $n\\in\\NN$ in the context of\nForeign Exchange (FX) by first encoding the option prices at each time step in\nterms of the parameters of the corresponding risk-neutral measure and then\nperforming the time series analysis in the parameter space. The option price\npredictions are obtained from the predicted risk-neutral measure by effectively\nintegrating it against the corresponding option payoffs. The non-linear\ntransformation between option prices and the risk-neutral parameters applied\nhere is \\textit{not} arbitrary: it is the standard mapping used by market\nmakers in the FX option markets (the SABR parameterisation) and is given\nexplicitly in closed form. Our method is not restricted to the FX asset class\nnor does it depend on the type of parameterisation used. Statistical analysis\nof FX market data illustrates that our arbitrage-free predictions outperform\nthe naive random walk forecasts, suggesting a potential for building management\nstrategies for portfolios of derivative products, akin to the ones widely used\nin the underlying equity and futures markets.\n"
    },
    {
        "paper_id": 1407.5684,
        "authors": "Jonathan A. Ch\\'avez-Casillas and Jos\\'e E. Figueroa-L\\'opez",
        "title": "One-level limit order book models with memory and variable spread",
        "comments": "This is a revision of the former manuscript \"One-level limit order\n  books with sparsity and memory\". The first model (related to sparsity) has\n  been removed from this version and the discussion and results regarding the\n  second model have been expanded. This version also corrects some typos and\n  mistakes from the first version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model for the level I of a Limit Order Book (LOB), which\nincorporates the information about the standing orders at the opposite side of\nthe book after each price change and the arrivals of new orders within the\nspread. Our main result gives a diffusion approximation for the mid-price\nprocess. To illustrate the applicability of the considered framework, we also\npropose a feasible method to compute several quantities of interest}, such as\nthe distribution of the time span between price changes and the probability of\nconsecutive price increments conditioned on the current state of the book. The\nproposed method is used to develop an efficient simulation scheme for the price\ndynamics, which is then applied to assess numerically the accuracy of the\ndiffusion approximation.\n"
    },
    {
        "paper_id": 1407.5877,
        "authors": "Alet Roux and Tomasz Zastawniak",
        "title": "Linear vector optimization and European option pricing under\n  proportional transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A method for pricing and superhedging European options under proportional\ntransaction costs based on linear vector optimisation and geometric duality\ndeveloped by Lohne & Rudloff (2014) is compared to a special case of the\nalgorithms for American type derivatives due to Roux & Zastawniak (2014). An\nequivalence between these two approaches is established by means of a general\nresult linking the support function of the upper image of a linear vector\noptimisation problem with the lower image of the dual linear optimisation\nproblem.\n"
    },
    {
        "paper_id": 1407.6222,
        "authors": "Jean-Bernard Chatelain, Kirsten Ralf",
        "title": "A finite set of equilibria for the indeterminacy of linear rational\n  expectations models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper demonstrates the existence of a finite set of equilibria in the\ncase of the indeterminacy of linear rational expectations models. The number of\nequilibria corresponds to the number of ways to select n eigenvectors among a\nlarger set of eigenvectors related to stable eigenvalues. A finite set of\nequilibria is a substitute to continuous (uncountable) sets of sunspots\nequilibria, when the number of independent eigenvectors for each stable\neigenvalue is equal to one.\n"
    },
    {
        "paper_id": 1407.6334,
        "authors": "Heribert Genreith",
        "title": "Field Theory of Macroeconomics",
        "comments": "94 pages, 39 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we will show that the Macro-Economy and its growth can be\nmodelled and explained exactly in principle by commonly known Field Theory from\ntheoretical physics. We will show the main concepts and calculations needed and\nshow that calculation and prediction of economic growth then gets indeed\npossible in Dollars and Cents. As every field theory it is based on an equation\nof continuity, which in economic terms means the full balance of all sources\nand sinks of Capital (Assets) and real Goods (GDP) in the bulk. Uniqueness of\nfield theory of macroeconomics then can be derived from adapting Noether's\nTheorems, which is based on the notion of invariants to derive unique field\nequations. We will show that the only assumption which is needed for a\nself-consistent non-linear macro-economic theory is that the well known\nQuantity Equation, used in corrected formulation, holds at least locally in\ntime.\n"
    },
    {
        "paper_id": 1407.6649,
        "authors": "Amogh Deshpande",
        "title": "On the role of F\\\"ollmer-Schweizer minimal martingale measure in Risk\n  Sensitive control Asset Management",
        "comments": "A.Deshpande (2015), On the role of F\\\"ollmer-Schweizer minimal\n  martingale measure in Risk Sensitive control Asset Management,Vol. 52, No. 3,\n  Journal of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kuroda and Nagai \\cite{KN} state that the factor process in the Risk\nSensitive control Asset Management (RSCAM) is stable under the\nF\\\"ollmer-Schweizer minimal martingale measure . Fleming and Sheu \\cite{FS} and\nmore recently F\\\"ollmer and Schweizer \\cite{FoS} have observed that the role of\nthe minimal martingale measure in this portfolio optimization is yet to be\nestablished. In this article we aim to address this question by explicitly\nconnecting the optimal wealth allocation to the minimal martingale measure. We\nachieve this by using a \"trick\" of observing this problem in the context of\nmodel uncertainty via a two person zero sum stochastic differential game\nbetween the investor and an antagonistic market that provides a probability\nmeasure. We obtain some startling insights. Firstly, if short-selling is not\npermitted and if the factor process evolves under the minimal martingale\nmeasure then the investor's optimal strategy can only be to invest in the\nriskless asset (i.e. the no-regret strategy). Secondly, if the factor process\nand the stock price process have independent noise, then even if the market\nallows short selling, the optimal strategy for the investor must be the\nno-regret strategy while the factor process will evolve under the minimal\nmartingale measure .\n"
    },
    {
        "paper_id": 1407.6851,
        "authors": "Bruce M. Boghosian",
        "title": "Fokker-Planck Description of Wealth Dynamics and the Origin of Pareto's\n  Law",
        "comments": "6 pages",
        "journal-ref": "International Journal of Modern Physics C, Vol. 25, No. 11 (2014)\n  1441008",
        "doi": "10.1142/S0129183114410083",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The so-called \"Yard-Sale Model\" of wealth distribution posits that wealth is\ntransferred between economic agents as a result of transactions whose size is\nproportional to the wealth of the less wealthy agent. In recent work [B.M.\nBoghosian, \"Kinetics of Wealth and the Pareto Law,\" {\\it Phys. Rev. E} {\\bf 89}\n(2014) 042804], it was shown that this results in a Fokker-Planck equation\ngoverning the distribution of wealth. With the addition of a mechanism for\nwealth redistribution, it was further shown that this model results in\nstationary wealth distributions that are very similar in form to Pareto's well\nknown law. In this paper, a much simpler derivation of that Fokker-Planck\nequation is presented.\n"
    },
    {
        "paper_id": 1407.686,
        "authors": "Tiziano De Angelis and Yerkin Kitapbayev",
        "title": "On the optimal exercise boundaries of swing put options",
        "comments": "30 pages, 4 figures, added a figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use probabilistic methods to characterise time dependent optimal stopping\nboundaries in a problem of multiple optimal stopping on a finite time horizon.\nMotivated by financial applications we consider a payoff of immediate stopping\nof \"put\" type and the underlying dynamics follows a geometric Brownian motion.\nThe optimal stopping region relative to each optimal stopping time is described\nin terms of two boundaries which are continuous, monotonic functions of time\nand uniquely solve a system of coupled integral equations of Volterra-type.\nFinally we provide a formula for the value function of the problem.\n"
    },
    {
        "paper_id": 1407.714,
        "authors": "Gaurab Aryal, Maria Florencia Gabrielli and Quang Vuong",
        "title": "Semiparametric Estimation of First-Price Auction Models",
        "comments": "66 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a semiparametric method to estimate the density of private values\nin first-price auctions. Specifically, we model private values through a set of\nconditional moment restrictions and use a two-step procedure. In the first step\nwe recover a sample of pseudo private values using Local Polynomial Estimator.\nIn the second step we use a GMM procedure to estimate the parameter(s) of\ninterest. We show that the proposed semiparametric estimator is consistent, has\nan asymptotic normal distribution, and attains the parametric (\"root-n\") rate\nof convergence.\n"
    },
    {
        "paper_id": 1407.7153,
        "authors": "N. Bagatella-Flores, M. Rodriguez-Achach, H.F. Coronel-Brizio and A.R.\n  Hernandez-Montoya",
        "title": "Wealth distribution of simple exchange models coupled with extremal\n  dynamics",
        "comments": "15 pages, 5 figures. An Econophysics paper",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.07.081",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Punctuated Equilibrium (PE) states that after long periods of evolutionary\nquiescence, species evolution can take place in short time intervals, where\nsudden differentiation makes new species emerge and some species extinct. In\nthis paper, we introduce and study the effect of punctuated equilibrium on two\ndifferent asset exchange models: The yard sale model (YS, winner gets a random\nfraction of a poorer player's wealth) and the theft and fraud model (TF, winner\ngets a random fraction of the loser's wealth). The resulting wealth\ndistribution is characterized using the Gini index. In order to do this, we\nconsider PE as a perturbation with probability $\\rho$ of being applied. We\ncompare the resulting values of the Gini index at different increasing values\nof $\\rho$ in both models. We found that in the case of the TF model, the Gini\nindex reduces as the perturbation $\\rho$ increases, not showing dependence with\nthe agents number. While for YS we observe a phase transition which happens\naround $\\rho_c=0.79$. For perturbations $\\rho<\\rho_c$ the Gini index reaches\nthe value of one as time increases (an extreme wealth condensation state),\nwhereas for perturbations bigger or equal than $\\rho_c$ the Gini index becomes\ndifferent to one, avoiding the system reaches this extreme state. We show that\nboth simple exchange models coupled with PE dynamics give more realistic\nresults. In particular for YS, we observe a power low decay of wealth\ndistribution.\n"
    },
    {
        "paper_id": 1407.7237,
        "authors": "Jonas M\\\"uller, Marcus Hildmann, Andreas Ulbig and G\\\"oran Andersson",
        "title": "Grid Integration Costs of Fluctuating Renewable Energy Sources",
        "comments": "Accepted for SUSTECH 2014, Portland, Oregon, USA, July 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The grid integration of intermittent Renewable Energy Sources (RES) causes\ncosts for grid operators due to forecast uncertainty and the resulting\nproduction schedule mismatches. These so-called profile service costs are\nmarginal cost components and can be understood as an insurance fee against RES\nproduction schedule uncertainty that the system operator incurs due to the\nobligation to always provide sufficient control reserve capacity for power\nimbalance mitigation. This paper studies the situation for the German power\nsystem and the existing German RES support schemes. The profile service costs\nincurred by German Transmission System Operators (TSOs) are quantified and\nmeans for cost reduction are discussed. In general, profile service costs are\ndependent on the RES prediction error and the specific workings of the power\nmarkets via which the prediction error is balanced. This paper shows both how\nthe prediction error can be reduced in daily operation as well as how profile\nservice costs can be reduced via optimization against power markets and/or\nactive curtailment of RES generation.\n"
    },
    {
        "paper_id": 1407.7315,
        "authors": "Alexander Buryak and Ivan Guo",
        "title": "Effective and simple VWAP option pricing model",
        "comments": "International Journal of Theoretical and Applied Finance (2014 or\n  2015)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volume weighted average price (VWAP) options are a popular security type in\nmany countries, but despite their popularity very few pricing models have been\ndeveloped so far for VWAP options. This can be explained by the fact that the\nVWAP pricing problem is set in an incomplete market since there is no\nunderlying with which to hedge the volume risk, and hence there is no uniquely\ndefined price. Any price, which is obtained will include a market price of\nvolume risk which must be determined from the corresponding volume statistics.\nOur analysis strongly supports the hypothesis that the empirical volume\nstatistics of ASX equities can be described reasonably well by fitted gamma\ndistributions. Based on this observation we suggest a simple gamma\nprocess-based model that allows for the exact analytic pricing of VWAP options\nin a rather straightforward way.\n"
    },
    {
        "paper_id": 1407.7328,
        "authors": "Alexander Buryak and Ivan Guo",
        "title": "New analytic approach to address Put - Call parity violation due to\n  discrete dividends",
        "comments": null,
        "journal-ref": "Applied Mathematical Finance, v. 191, No 1, pp. 37-58, 2012",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The issue of developing simple Black-Scholes type approximations for pricing\nEuropean options with large discrete dividends was popular since early 2000's\nwith a few different approaches reported during the last 10 years. Moreover, it\nhas been claimed that at least some of the resulting expressions represent\nhigh-quality approximations which closely match results obtained by the use of\nnumerics.\n  In this paper we review, on the one hand, these previously suggested\nBlack-Scholes type approximations and, on the other hand, different versions of\nthe corresponding Crank-Nicolson numerical schemes with a primary focus on\ntheir boundary condition variations. Unexpectedly we often observe substantial\ndeviations between the analytical and numerical results which may be especially\npronounced for European Puts. Moreover, our analysis demonstrates that any\nBlack-Scholes type approximation which adjusts Put parameters identically to\nCall parameters has an inherent problem of failing to detect a little known\nPut-Call Parity violation phenomenon. To address this issue we derive a new\nanalytic approximation which is in a better agreement with the corresponding\nnumerical results in comparison with any of the previously known analytic\napproaches for European Calls and Puts with large discrete dividends.\n"
    },
    {
        "paper_id": 1407.7447,
        "authors": "Yves Pomeau and Ricardo Lopez-Ruiz",
        "title": "Study of a model for the distribution of wealth",
        "comments": "12 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An equation for the evolution of the distribution of wealth in a population\nof economic agents making binary transactions with a constant total amount of\n\"money\" has recently been proposed by one of us (RLR). This equation takes the\nform of an iterated nonlinear map of the distribution of wealth. The\nequilibrium distribution is known and takes a rather simple form. If this\ndistribution is such that, at some time, the higher momenta of the distribution\nexist, one can find exactly their law of evolution. A seemingly simple\nextension of the laws of exchange yields also explicit iteration formulae for\nthe higher momenta, but with a major difference with the original iteration\nbecause high order momenta grow indefinitely. This provides a quantitative\nmodel where the spreading of wealth, namely the difference between the rich and\nthe poor, tends to increase with time.\n"
    },
    {
        "paper_id": 1407.7717,
        "authors": "Peter Bank and Helena Kauppila",
        "title": "Convex duality for stochastic singular control problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general theory of convex duality for certain singular control\nproblems, taking the abstract results by Kramkov and Schachermayer (1999) for\noptimal expected utility from nonnegative random variables to the level of\noptimal expected utility from increasing, adapted controls. The main\ncontributions are the formulation of a suitable duality framework, the\nidentification of the problem's dual functional as well as the full duality for\nthe primal and dual value functions and their optimizers. The scope of our\nresults is illustrated by an irreversible investment problem and the\nHindy-Huang-Kreps utility maximization problem for incomplete financial\nmarkets.\n"
    },
    {
        "paper_id": 1407.7725,
        "authors": "Giorgia Callegaro, Luciano Campi, Valeria Giusto, Tiziano Vargiolu",
        "title": "Utility indifference pricing and hedging for structured contracts in\n  energy markets",
        "comments": "32 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the pricing and hedging of structured products in\nenergy markets, such as swing and virtual gas storage, using the exponential\nutility indifference pricing approach in a general incomplete multivariate\nmarket model driven by finitely many stochastic factors. The buyer of such\ncontracts is allowed to trade in the forward market in order to hedge the risk\nof his position. We fully characterize the buyer's utility indifference price\nof a given product in terms of continuous viscosity solutions of suitable\nnonlinear PDEs. This gives a way to identify reasonable candidates for the\noptimal exercise strategy for the structured product as well as for the\ncorresponding hedging strategy. Moreover, in a model with two correlated\nassets, one traded and one nontraded, we obtain a representation of the price\nas the value function of an auxiliary simpler optimization problem under a risk\nneutral probability, that can be viewed as a perturbation of the minimal\nentropy martingale measure. Finally, numerical results are provided.\n"
    },
    {
        "paper_id": 1407.7738,
        "authors": "Peter Martey Addo",
        "title": "Multivariate Self-Exciting Threshold Autoregressive Models with\n  eXogenous Input",
        "comments": "This is a preliminary version of the paper-- please do not quote",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study defines a multivariate Self--Exciting Threshold Autoregressive\nwith eXogenous input (MSETARX) models and present an estimation procedure for\nthe parameters. The conditions for stationarity of the nonlinear MSETARX models\nis provided. In particular, the efficiency of an adaptive parameter estimation\nalgorithm and LSE (least squares estimate) algorithm for this class of models\nis then provided via simulations.\n"
    },
    {
        "paper_id": 1407.8024,
        "authors": "Yuhong Xu",
        "title": "Robust valuation and risk measurement under model uncertainty",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Model uncertainty is a type of inevitable financial risk. Mistakes on the\nchoice of pricing model may cause great financial losses. In this paper we\ninvestigate financial markets with mean-volatility uncertainty. Models for\nstock markets and option markets with uncertain prior distribution are\nestablished by Peng's G-stochastic calculus. The process of stock price is\ndescribed by generalized geometric G-Brownian motion in which the mean\nuncertainty may move together with or regardless of the volatility uncertainty.\nOn the hedging market, the upper price of an (exotic) option is derived\nfollowing the Black-Scholes-Barenblatt equation. It is interesting that the\ncorresponding Barenblatt equation does not depend on the risk preference of\ninvestors and the mean-uncertainty of underlying stocks. Hence under some\nappropriate sublinear expectation, neither the risk preference of investors nor\nthe mean-uncertainty of underlying stocks pose effects on our super and\nsubhedging strategies. Appropriate definitions of arbitrage for super and\nsub-hedging strategies are presented such that the super and sub-hedging prices\nare reasonable. Especially the condition of arbitrage for sub-hedging strategy\nfills the gap of the theory of arbitrage under model uncertainty. Finally we\nshow that the term $K$ of finite-variance arising in the super-hedging strategy\nis interpreted as the max Profit\\&Loss of being short a delta-hedged option.\nThe ask-bid spread is in fact the accumulation of summation of the superhedging\n$P\\&L$ and the subhedging $P\\&L $.\n"
    },
    {
        "paper_id": 1407.8068,
        "authors": "Fernando Cordero and Lavinia Perez-Ostafe",
        "title": "Critical transaction costs and 1-step asymptotic arbitrage in fractional\n  binary markets",
        "comments": "21 pages",
        "journal-ref": "Int. J. Theor. Appl. Finan., Volume 18, Issue 5 (2015)",
        "doi": "10.1142/S0219024915500296",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the arbitrage opportunities in the presence of transaction costs in\na sequence of binary markets approximating the fractional Black-Scholes model.\nThis approximating sequence was constructed by Sottinen and named fractional\nbinary markets. Since, in the frictionless case, these markets admit arbitrage,\nwe aim to determine the size of the transaction costs needed to eliminate the\narbitrage from these models. To gain more insight, we first consider only\n1-step trading strategies and we prove that arbitrage opportunities appear when\nthe transaction costs are of order $o(1/\\sqrt{N})$. Next, we characterize the\nasymptotic behavior of the smallest transaction costs $\\lambda_c^{(N)}$, called\n\"critical\" transaction costs, starting from which the arbitrage disappears.\nSince the fractional Black-Scholes model is arbitrage-free under arbitrarily\nsmall transaction costs, one could expect that $\\lambda_c^{(N)}$ converges to\nzero. However, the true behavior of $\\lambda_c^{(N)}$ is opposed to this\nintuition. More precisely, we show, with the help of a new family of trading\nstrategies, that $\\lambda_c^{(N)}$ converges to one. We explain this apparent\ncontradiction and conclude that it is appropriate to see the fractional binary\nmarkets as a large financial market and to study its asymptotic arbitrage\nopportunities. Finally, we construct a $1$-step asymptotic arbitrage in this\nlarge market when the transaction costs are of order $o(1/N^H)$, whereas for\nconstant transaction costs, we prove that no such opportunity exists.\n"
    },
    {
        "paper_id": 1407.83,
        "authors": "Ting-Kam Leonard Wong",
        "title": "Optimization of relative arbitrage",
        "comments": "33 pages, 5 figures, 2 tables; revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In stochastic portfolio theory, a relative arbitrage is an equity portfolio\nwhich is guaranteed to outperform a benchmark portfolio over a finite horizon.\nWhen the market is diverse and sufficiently volatile, and the benchmark is the\nmarket or a buy-and-hold portfolio, functionally generated portfolios\nintroduced by Fernholz provide a systematic way of constructing relative\narbitrages. In this paper we show that if the market portfolio is replaced by\nthe equal or entropy weighted portfolio among many others, no relative\narbitrages can be constructed under the same conditions using functionally\ngenerated portfolios. We also introduce and study a shaped-constrained\noptimization problem for functionally generated portfolios in the spirit of\nmaximum likelihood estimation of a log-concave density.\n"
    },
    {
        "paper_id": 1408.0308,
        "authors": "Marco D'Errico, Gulnur Muradoglu, Silvana Stefani, Giovanni Zambruno",
        "title": "Opinion Dynamics and Price Formation: a Nonlinear Network Model",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Opinions and beliefs determine the evolution of social systems. This is of\nparticular interest in finance, as the increasing complexity of financial\nsystems is coupled with information overload. Opinion formation, therefore, is\nnot always the result of optimal information processing. On the contrary,\nagents are boundedly rational and naturally tend to observe and imitate others\nin order to gain further insights. Hence, a certain degree of interaction,\nwhich can be envisioned as a network, occurs within the system. Opinions, the\ninteraction network and prices in financial markets are then heavily\nintertwined and influence one another. We build on previous contributions on\nadaptive systems, where agents have hetereogenous beliefs, and introduce a\ndynamic confidence network that captures the interaction and shapes the opinion\npatterns. The analytical framework we adopt for modeling the interaction is\nrooted in the opinion dynamics problem. This will allow us to introduce a\nnonlinear model where the confidence network, opinion dynamics and price\nformation coevolve in time. A key aspect of the model is the classification of\nagents according to their topological role in the network, therefore showing\nthat topology matters in determining how of opinions and prices will coevolve.\nWe illustrate the dynamics via simulations, discussing the stylized facts in\nfinance that the model is able to capture. Last, we propose an empirical\nvalidation and calibration scheme that makes use of social network data.\n"
    },
    {
        "paper_id": 1408.044,
        "authors": "Christoph Aymanns, Co-Pierre Georg",
        "title": "Contagious Synchronization and Endogenous Network Formation in Financial\n  Networks",
        "comments": "41 pages, 10 figures, Journal of Banking & Finance 2014",
        "journal-ref": null,
        "doi": "10.1016/j.jbankfin.2014.06.030",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When banks choose similar investment strategies the financial system becomes\nvulnerable to common shocks. We model a simple financial system in which banks\ndecide about their investment strategy based on a private belief about the\nstate of the world and a social belief formed from observing the actions of\npeers. Observing a larger group of peers conveys more information and thus\nleads to a stronger social belief. Extending the standard model of Bayesian\nupdating in social networks, we show that the probability that banks\nsynchronize their investment strategy on a state non-matching action critically\ndepends on the weighting between private and social belief. This effect is\nalleviated when banks choose their peers endogenously in a network formation\nprocess, internalizing the externalities arising from social learning.\n"
    },
    {
        "paper_id": 1408.0443,
        "authors": "Wei Li, Dror Y. Kenett, Kazuko Yamasaki, H. Eugene Stanley, Shlomo\n  Havlin",
        "title": "Ranking the Economic Importance of Countries and Industries",
        "comments": "17 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  In the current era of worldwide stock market interdependencies, the global\nfinancial village has become increasingly vulnerable to systemic collapse. The\nrecent global financial crisis has highlighted the necessity of understanding\nand quantifying interdependencies among the world's economies, developing new\neffective approaches to risk evaluation, and providing mitigating solutions. We\npresent a methodological framework for quantifying interdependencies in the\nglobal market and for evaluating risk levels in the world-wide financial\nnetwork. The resulting information will enable policy and decision makers to\nbetter measure, understand, and maintain financial stability. We use the\nmethodology to rank the economic importance of each industry and country\naccording to the global damage that would result from their failure. Our\nquantitative results shed new light on China's increasing economic dominance\nover other economies, including that of the USA, to the global economy.\n"
    },
    {
        "paper_id": 1408.0916,
        "authors": "Dmitry Kramkov, Sergio Pulido",
        "title": "A system of quadratic BSDEs arising in a price impact model",
        "comments": "Published at http://dx.doi.org/10.1214/15-AAP1103 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2016, Vol. 26, No. 2, 794-817",
        "doi": "10.1214/15-AAP1103",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial model where the prices of risky assets are quoted by\na representative market maker who takes into account an exogenous demand. We\ncharacterize these prices in terms of a system of BSDEs with quadratic growth.\nWe show that this system admits a unique solution for every bounded demand if\nand only if the market maker's risk-aversion is sufficiently small. The\nuniqueness is established in the natural class of solutions, without any\nadditional norm restrictions. To the best of our knowledge, this is the first\nstudy that proves such (global) uniqueness result for a system of fully coupled\nquadratic BSDEs.\n"
    },
    {
        "paper_id": 1408.0981,
        "authors": "Tetsuya Takaishi",
        "title": "Bayesian estimation of realized stochastic volatility model by Hybrid\n  Monte Carlo algorithm",
        "comments": "4 pages, 2 figures",
        "journal-ref": "Journal of Physics: Conference Series 490 (2014) 012092",
        "doi": "10.1088/1742-6596/490/1/012092",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The hybrid Monte Carlo algorithm (HMCA) is applied for Bayesian parameter\nestimation of the realized stochastic volatility (RSV) model. Using the 2nd\norder minimum norm integrator (2MNI) for the molecular dynamics (MD) simulation\nin the HMCA, we find that the 2MNI is more efficient than the conventional\nleapfrog integrator. We also find that the autocorrelation time of the\nvolatility variables sampled by the HMCA is very short. Thus it is concluded\nthat the HMCA with the 2MNI is an efficient algorithm for parameter estimations\nof the RSV model.\n"
    },
    {
        "paper_id": 1408.1022,
        "authors": "Gaurab Aryal and Ronald Stauber",
        "title": "A Note on Kuhn's Theorem with Ambiguity Averse Players",
        "comments": "7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.econlet.2014.08.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kuhn's Theorem shows that extensive games with perfect recall can\nequivalently be analyzed using mixed or behavioral strategies, as long as\nplayers are expected utility maximizers. This note constructs an example that\nillustrate the limits of Kuhn's Theorem in an environment with ambiguity averse\nplayers who use maxmin decision rule and full Bayesian updating.\n"
    },
    {
        "paper_id": 1408.1159,
        "authors": "Peter P. Carr, Marcos Lopez de Prado",
        "title": "Determining Optimal Trading Rules without Backtesting",
        "comments": "Working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Calibrating a trading rule using a historical simulation (also called\nbacktest) contributes to backtest overfitting, which in turn leads to\nunderperformance. In this paper we propose a procedure for determining the\noptimal trading rule (OTR) without running alternative model configurations\nthrough a backtest engine. We present empirical evidence of the existence of\nsuch optimal solutions for the case of prices following a discrete\nOrnstein-Uhlenbeck process, and show how they can be computed numerically.\nAlthough we do not derive a closed-form solution for the calculation of OTRs,\nwe conjecture its existence on the basis of the empirical evidence presented.\n"
    },
    {
        "paper_id": 1408.1352,
        "authors": "Krzysztof Urbanowicz, Peter Richmond and Janusz A. Ho{\\l}yst",
        "title": "A simple model of local prices and associated risk evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A simple spin system is constructed to simulate dynamics of asset prices and\nstudied numerically. The outcome for the distribution of prices is shown to\ndepend both on the dimension of the system and the introduction of price into\nthe link measure. For dimensions below 2, the associated risk is high and the\nprice distribution is bimodal. For higher dimensions, the price distribution is\nGaussian and the associated risk is much lower. It is suggested that the\nresults are relevant to rare assets or situations where few players are\ninvolved in the deal making process.\n"
    },
    {
        "paper_id": 1408.1365,
        "authors": "Sanchari Goswami and Anirban Chakraborti",
        "title": "Kinetic Exchange Models in Economics and Sociology",
        "comments": "19 pages, 3 figures. To appear in Eds. R. Lopez-Ruiz, D.\n  Fournier-Prunaret, Y. Nishio, C. Gracio, Nonlinear Maps and their\n  Applications, Springer Proceedings in Mathematics & Statistics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we briefly review the different aspects and applications of\nkinetic exchange models in economics and sociology. Our main aim is to show in\nwhat manner the kinetic exchange models for closed economic systems were\ninspired by the kinetic theory of gas molecules. The simple yet powerful\nframework of kinetic theory, first proposed in 1738, led to the successful\ndevelopment of statistical physics of gases towards the end of the 19th\ncentury. This framework was successfully adapted to modeling of wealth\ndistributions in the early 2000's. In later times, it was applied to other\nareas like firm dynamics and opinion formation in the society, as well. We have\ntried to present the flavour of the several models proposed and their\napplications, intentionally leaving out the intricate mathematical and\ntechnical details.\n"
    },
    {
        "paper_id": 1408.1382,
        "authors": "Xiang Yu",
        "title": "Optimal Consumption under Habit Formation In Markets with Transaction\n  Costs and Random Endowments",
        "comments": "Final version. To appear in The Annals of Applied Probability.\n  Keywords: Proportional Transaction Costs, Unbounded Random Endowments,\n  Acceptable Portfolios, Consumption Budget Constraint, Consumption Habit\n  Formation, Convex Duality, Market Isomorphism",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal consumption under the addictive habit\nformation preference in markets with transaction costs and unbounded random\nendowments. To model the proportional transaction costs, we adopt the Kabanov's\nmulti-asset framework with a cash account. At the terminal time T, the investor\ncan receive unbounded random endowments for which we propose a new definition\nof acceptable portfolios based on the strictly consistent price system (SCPS).\nWe prove a type of super-hedging theorem using the acceptable portfolios which\nenables us to obtain the consumption budget constraint condition under market\nfrictions. Applying the path dependence reduction and the embedding approach,\nwe obtain the existence and uniqueness of the optimal consumption using some\nauxiliary processes and the duality analysis. As an application of the duality\ntheory, the market isomorphism with special discounting factors is also\ndiscussed in the sense that the original optimal consumption with habit\nformation is equivalent to the standard optimal consumption problem without the\nhabits impact, however, in a modified isomorphic market model.\n"
    },
    {
        "paper_id": 1408.1494,
        "authors": "David Garcia, Claudio Juan Tessone, Pavlin Mavrodiev and Nicolas\n  Perony",
        "title": "The digital traces of bubbles: feedback cycles between socio-economic\n  signals in the Bitcoin economy",
        "comments": "16 pages, 3 figures, supplementary material",
        "journal-ref": "Journal of the Royal Society Interface, pp. 20140623, vol. 11\n  (2014)",
        "doi": "10.1098/?rsif.2014.0623",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What is the role of social interactions in the creation of price bubbles?\nAnswering this question requires obtaining collective behavioural traces\ngenerated by the activity of a large number of actors. Digital currencies offer\na unique possibility to measure socio-economic signals from such digital\ntraces. Here, we focus on Bitcoin, the most popular cryptocurrency. Bitcoin has\nexperienced periods of rapid increase in exchange rates (price) followed by\nsharp decline; we hypothesise that these fluctuations are largely driven by the\ninterplay between different social phenomena. We thus quantify four\nsocio-economic signals about Bitcoin from large data sets: price on on-line\nexchanges, volume of word-of-mouth communication in on-line social media,\nvolume of information search, and user base growth. By using vector\nautoregression, we identify two positive feedback loops that lead to price\nbubbles in the absence of exogenous stimuli: one driven by word of mouth, and\nthe other by new Bitcoin adopters. We also observe that spikes in information\nsearch, presumably linked to external events, precede drastic price declines.\nUnderstanding the interplay between the socio-economic signals we measured can\nlead to applications beyond cryptocurrencies to other phenomena which leave\ndigital footprints, such as on-line social network usage.\n"
    },
    {
        "paper_id": 1408.1671,
        "authors": "Damiano Fiorillo and Fabio Sabatini",
        "title": "Structural social capital and health in Italy",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the first empirical assessment of the causal relationship\nbetween social capital and health in Italy. The analysis draws on the 2000 wave\nof the Multipurpose Survey on Household conducted by the Italian Institute of\nStatistics on a representative sample of the population (n = 46,868). Our\nmeasure of social capital is the frequency of meetings with friends. Based on\nIV and bivariate probit estimates, we find that individuals who meet friends\nevery day or at least two times a week are approximately 11% to 16% more likely\nto report good health.\n"
    },
    {
        "paper_id": 1408.1728,
        "authors": "Leonidas Sandoval Junior",
        "title": "Dynamics in two networks based on stocks of the US stock market",
        "comments": "For the complete version, with the 22 figures, please contact the\n  author or download from\n  https://insper.academia.edu/LeonidasSandoval/Papers?s=nav#add",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We follow the main stocks belonging to the New York Stock Exchange and to\nNasdaq from 2003 to 2012, through years of normality and of crisis, and study\nthe dynamics of networks built on two measures expressing relations between\nthose stocks: correlation, which is symmetric and measures how similar two\nstocks behave, and Transfer Entropy, which is non-symmetric and measures the\ninfluence of the time series of one stock onto another in terms of the\ninformation that the time series of one stock transmits to the time series of\nanother stock. The two measures are used in the creation of two networks that\nevolve in time, revealing how the relations between stocks and industrial\nsectors changed in times of crisis. The two networks are also used in\nconjunction with a dynamic model of the spreading of volatility in order to\ndetect which are the stocks that are most likely to spread crises, according to\nthe model. This information may be used in the building of policies aiming to\nreduce the effect of financial crises.\n"
    },
    {
        "paper_id": 1408.2138,
        "authors": "Andrea Zaccaria, Matthieu Cristelli, Andrea Tacchella, and Luciano\n  Pietronero",
        "title": "How the Taxonomy of Products Drives the Economic Development of\n  Countries",
        "comments": "16 pages, 8 figures",
        "journal-ref": "PLoS ONE 9(12): e113770 (2014)",
        "doi": "10.1371/journal.pone.0113770",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an algorithm able to reconstruct the relevant network structure\non which the time evolution of country-product bipartite networks takes place.\nThe significant links are obtained by selecting the largest values of the\nprojected matrix. We first perform a number of tests of this filtering\nprocedure on synthetic cases and a toy model. Then we analyze the bipartite\nnetwork constituted by countries and exported products, using two databases for\na total of almost 50 years. It is then possible to build a hierarchically\ndirected network, in which the taxonomy of products emerges in a natural way.\nWe study the influence of the structure of this taxonomy network on countries'\ndevelopment; in particular, guided by an example taken from the\nindustrialization of South Korea, we link the structure of the taxonomy network\nto the empirical temporal connections between product activations, finding that\nthe most relevant edges for countries' development are the ones suggested by\nour network. These results suggest paths in the product space which are easier\nto achieve, and so can drive countries' policies in the industrialization\nprocess.\n"
    },
    {
        "paper_id": 1408.2217,
        "authors": "Zura Kakushadze",
        "title": "Mean-Reversion and Optimization",
        "comments": "41 pages; a trivial typo corrected",
        "journal-ref": "Journal of Asset Management 16(1) (2015) 14-40",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of these notes is to provide a systematic quantitative framework\n- in what is intended to be a \"pedagogical\" fashion - for discussing\nmean-reversion and optimization. We start with pair trading and add complexity\nby following the sequence \"mean-reversion via demeaning -> regression ->\nweighted regression -> (constrained) optimization -> factor models\". We discuss\nin detail how to do mean-reversion based on this approach, including common\npitfalls encountered in practical applications, such as the difference between\nmaximizing the Sharpe ratio and minimizing an objective function when trading\ncosts are included. We also discuss explicit algorithms for optimization with\nlinear costs, constraints and bounds.\n"
    },
    {
        "paper_id": 1408.2324,
        "authors": "Sanchari Goswami and Parongama Sen",
        "title": "Agent based models for wealth distribution with preference in\n  interaction",
        "comments": "8 pages, 18 figures (To appear in Physica A)",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2014.08.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a set of conservative models in which agents exchange wealth with\na preference in the choice of interacting agents in different ways. The common\nfeature in all the models is that the temporary values of financial status of\nagents is a deciding factor for interaction. Other factors which may play\nimportant role are past interactions and wealth possessed by individuals.\nWealth distribution, network properties and activity are the main quantities\nwhich have been studied. Evidence of phase transitions and other interesting\nfeatures are presented. The results show that certain observations of real\neconomic system can be reproduced by the models.\n"
    },
    {
        "paper_id": 1408.2462,
        "authors": "Luca Spadafora, Marco Dubrovich and Marcello Terraneo",
        "title": "Value-at-Risk time scaling for long-term risk estimation",
        "comments": "Pre-Print version, submitted to The Journal of Risk. 18 pages, 17\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we discuss a general methodology to compute the market risk\nmeasure over long time horizons and at extreme percentiles, which are the\ntypical conditions needed for estimating Economic Capital. The proposed\napproach extends the usual market-risk measure, ie, Value-at-Risk (VaR) at a\nshort-term horizon and 99% confidence level, by properly applying a scaling on\nthe short-term Profit-and-Loss (P&L) distribution. Besides the standard\nsquare-root-of-time scaling, based on normality assumptions, we consider two\nleptokurtic probability density function classes for fitting empirical P&L\ndatasets and derive accurately their scaling behaviour in light of the Central\nLimit Theorem, interpreting time scaling as a convolution problem. Our analyses\nresult in a range of possible VaR-scaling approaches depending on the\ndistribution providing the best fit to empirical data, the desired percentile\nlevel and the time horizon of the Economic Capital calculation. After assessing\nthe different approaches on a test equity trading portfolio, it emerges that\nthe choice of the VaR-scaling approach can affect substantially the Economic\nCapital calculation. In particular, the use of a convolution-based approach\ncould lead to significantly larger risk measures (by up to a factor of four)\nthan those calculated using Normal assumptions on the P&L distribution.\n"
    },
    {
        "paper_id": 1408.2794,
        "authors": "Angela Gu, Patrick Zeng",
        "title": "Sector-Based Factor Models for Asset Returns",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Factor analysis is a statistical technique employed to evaluate how observed\nvariables correlate through common factors and unique variables. While it is\noften used to analyze price movement in the unstable stock market, it does not\nalways yield easily interpretable results. In this study, we develop improved\nfactor models by explicitly incorporating sector information on our studied\nstocks. We add eleven sectors of stocks as defined by the IBES, represented by\nrespective sector-specific factors, to non-specific market factors to revise\nthe factor model. We then develop an expectation maximization (EM) algorithm to\ncompute our revised model with 15 years' worth of S&P 500 stocks' daily close\nprices. Our results in most sectors show that nearly all of these factor\ncomponents have the same sign, consistent with the intuitive idea that stocks\nin the same sector tend to rise and fall in coordination over time. Results\nobtained by the classic factor model, in contrast, had a homogeneous blend of\npositive and negative components. We conclude that results produced by our\nsector-based factor model are more interpretable than those produced by the\nclassic non-sector-based model for at least some stock sectors.\n"
    },
    {
        "paper_id": 1408.2805,
        "authors": "Georg Hofmann",
        "title": "Accelerated Portfolio Optimization with Conditional Value-at-Risk\n  Constraints using a Cutting-Plane Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial portfolios are often optimized for maximum profit while subject to\na constraint formulated in terms of the Conditional Value-at-Risk (CVaR). This\namounts to solving a linear problem. However, in its original formulation this\nlinear problem has a very large number of linear constraints, too many to be\nenforced in practice. In the literature this is addressed by a reformulation of\nthe problem using so-called dummy variables. This reduces the large number of\nconstraints in the original linear problem at the cost of increasing the number\nof variables. In the context of reinsurance portfolio optimization we observe\nthat the increase in variable count can lead to situations where solving the\nreformulated problem takes a long time. Therefore we suggest a different\napproach. We solve the original linear problem with cutting-plane method: The\nproposed algorithm starts with the solution of a relaxed problem and then\niteratively adds cuts until the solution is approximated within a preset\nthreshold. This is a new approach. For a reinsurance case study we show that a\nsignificant reduction of necessary computer resources can be achieved.\n"
    },
    {
        "paper_id": 1408.2859,
        "authors": "Jonathan E. Ingersoll Jr., Lawrence J. Jin",
        "title": "Realization Utility with Reference-Dependent Preferences",
        "comments": "appears in The Review of Financial Studies (2013)",
        "journal-ref": null,
        "doi": "10.1093/rfs/hhs116",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a tractable model of realization utility that studies the role of\nreference-dependent S-shaped preferences in a dynamic investment setting with\nreinvestment. Our model generates both voluntarily realized gains and losses.\nIt makes specific predictions about the volume of gains and losses, the holding\nperiods, and the sizes of both realized and paper gains and losses that can be\ncalibrated to a variety of statistics, including the Odean measure of the\ndisposition effect. Our model also predicts several anomalies including, among\nothers, the flattening of the capital market line and a negative price for\nidiosyncratic risk.\n"
    },
    {
        "paper_id": 1408.2985,
        "authors": "Tom\\'a\\v{s} V\\'yrost, \\v{S}tefan Ly\\'ocsa, Eduard Baum\\\"ohl",
        "title": "Granger Causality Stock Market Networks: Temporal Proximity and\n  Preferential Attachment",
        "comments": "This work was supported by the Slovak Research and Development Agency\n  under the contract No. APVV-0666-11",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2015.02.017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The structure of return spillovers is examined by constructing Granger\ncausality networks using daily closing prices of 20 developed markets from 2nd\nJanuary 2006 to 31st December 2013. The data is properly aligned to take into\naccount non-synchronous trading effects. The study of the resulting networks of\nover 94 sub-samples revealed three significant findings. First, after the\nrecent financial crisis the impact of the US stock market has declined. Second,\nspatial probit models confirmed the role of the temporal proximity between\nmarket closing times for return spillovers, i.e. the time distance between\nnational stock markets matters. Third, preferential attachment between stock\nmarkets exists, i.e. spillover from market j to market i is more likely if A)\nmarket j influences other markets other than i, or when B) market i is\ninfluenced by other markets other than j.\n"
    },
    {
        "paper_id": 1408.3086,
        "authors": "Mauro R. Oliveira, Armando Chinelatto Neto",
        "title": "Downturn LGD: A More Conservative Approach for Economic Decline Periods",
        "comments": null,
        "journal-ref": "Revista Tecnologia de Cr\\'edito, Edi\\c{c}\\~ao 86, Editora Serasa\n  Experian, 2014 pages 42-51",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to identify a relevant statistical correlation\nbetween rate of default, RD, and loss given default, LGD, in a major Brazilian\nfinancial institution Retail Home Equity exposure rated using the IRB approach,\nso that we may find a causal relationship between the two risk parameters.\nTherefore, according to Central Bank of Brazil requirements, a methodology is\napplied to add conservatism to the estimation of the Loss Given Default\nparameter at times of economic decline, reflected as increased rates of\ndefault.\n"
    },
    {
        "paper_id": 1408.3387,
        "authors": "Hassan A. Fallahgoul and Young S. Kim",
        "title": "Elliptical Tempered Stable Distribution and Fractional Calculus",
        "comments": "16 pages, working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A definition for elliptical tempered stable distribution, based on the\ncharacteristic function, have been explained which involve a unique spectral\nmeasure. This definition provides a framework for creating a connection between\ninfinite divisible distribution, and particularly elliptical tempered stable\ndistribution, with fractional calculus. Finally, some analytical approximations\nfor the probability density function of tempered infinite divisible\ndistribution, which elliptical tempered stable distributions are a subclass of\nthem, are considered.\n"
    },
    {
        "paper_id": 1408.365,
        "authors": "Eric M. Aldrich, Indra Heckenbach, Gregory Laughlin",
        "title": "The Random Walk of High Frequency Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper builds a model of high-frequency equity returns by separately\nmodeling the dynamics of trade-time returns and trade arrivals. Our main\ncontributions are threefold. First, we characterize the distributional behavior\nof high-frequency asset returns both in ordinary clock time and in trade time.\nWe show that when controlling for pre-scheduled market news events, trade-time\nreturns of the highly liquid near-month E-mini S&P 500 futures contract are\nwell characterized by a Gaussian distribution at very fine time scales. Second,\nwe develop a structured and parsimonious model of clock-time returns by\nsubordinating a trade-time Gaussian distribution with a trade arrival process\nthat is associated with a modified Markov-Switching Multifractal Duration\n(MSMD) model. This model provides an excellent characterization of\nhigh-frequency inter-trade durations. Over-dispersion in this distribution of\ninter-trade durations leads to leptokurtosis and volatility clustering in\nclock-time returns, even when trade-time returns are Gaussian. Finally, we use\nour model to extrapolate the empirical relationship between trade rate and\nvolatility in an effort to understand conditions of market failure. Our model\nsuggests that the 1,200 km physical separation of financial markets in Chicago\nand New York/New Jersey provides a natural ceiling on systemic volatility and\nmay contribute to market stability during periods of extremely heavy trading.\n"
    },
    {
        "paper_id": 1408.3692,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "On Zero-sum Optimal Stopping Games",
        "comments": "Final version. To appear in Applied Mathematics and Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On a filtered probability space\n$(\\Omega,\\mathcal{F},P,\\mathbb{F}=(\\mathcal{F}_t)_{t=0,\\dotso,T})$, we consider\nstopper-stopper games $\\overline\nV:=\\inf_{\\Rho\\in\\bT^{ii}}\\sup_{\\tau\\in\\T}\\E[U(\\Rho(\\tau),\\tau)]$ and\n$\\underline V:=\\sup_{\\Tau\\in\\bT^i}\\inf_{\\rho\\in\\T}\\E[U(\\Rho(\\tau),\\tau)]$ in\ndiscrete time, where $U(s,t)$ is $\\mathcal{F}_{s\\vee t}$-measurable instead of\n$\\mathcal{F}_{s\\wedge t}$-measurable as is often assumed in the literature,\n$\\T$ is the set of stopping times, and $\\bT^i$ and $\\bT^{ii}$ are sets of\nmappings from $\\T$ to $\\T$ satisfying certain non-anticipativity conditions. We\nconvert the problems into a corresponding Dynkin game, and show that $\\overline\nV=\\underline V=V$, where $V$ is the value of the Dynkin game. We also get the\noptimal $\\Rho\\in\\bT^{ii}$ and $\\Tau\\in\\bT^i$ for $\\overline V$ and $\\underline\nV$ respectively.\n"
    },
    {
        "paper_id": 1408.3728,
        "authors": "Pawe{\\l} Fiedor",
        "title": "Maximum Entropy Production Principle for Stock Returns",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our previous studies we have investigated the structural complexity of\ntime series describing stock returns on New York's and Warsaw's stock\nexchanges, by employing two estimators of Shannon's entropy rate based on\nLempel-Ziv and Context Tree Weighting algorithms, which were originally used\nfor data compression. Such structural complexity of the time series describing\nlogarithmic stock returns can be used as a measure of the inherent (model-free)\npredictability of the underlying price formation processes, testing the\nEfficient-Market Hypothesis in practice. We have also correlated the estimated\npredictability with the profitability of standard trading algorithms, and found\nthat these do not use the structure inherent in the stock returns to any\nsignificant degree. To find a way to use the structural complexity of the stock\nreturns for the purpose of predictions we propose the Maximum Entropy\nProduction Principle as applied to stock returns, and test it on the two\nmentioned markets, inquiring into whether it is possible to enhance prediction\nof stock returns based on the structural complexity of these and the mentioned\nprinciple.\n"
    },
    {
        "paper_id": 1408.3774,
        "authors": "Yan Dolinsky and Yuri Kifer",
        "title": "Risk Minimization for Game Options in Markets Imposing Minimal\n  Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study partial hedging for game options in markets with transaction costs\nbounded from below. More precisely, we assume that the investor's transaction\ncosts for each trade are the maximum between proportional transaction costs and\na fixed transaction costs. We prove that in the continuous time Black--Scholes\n(BS) model, there exists a trading strategy which minimizes the shortfall risk.\nFurthermore, we use binomial models in order to provide numerical schemes for\nthe calculation of the shortfall risk and the corresponding optimal portfolio\nin the BS model.\n"
    },
    {
        "paper_id": 1408.4618,
        "authors": "Jean-Cyprien H\\'eam and Erwan Koch",
        "title": "Diversification and Endogenous Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test the hypothesis that interconnections across financial institutions\ncan be explained by a diversification motive. This idea stems from the\nempirical evidence of the existence of long-term exposures that cannot be\nexplained by a liquidity motive (maturity or currency mismatch). We model\nendogenous interconnections of heterogenous financial institutions facing\nregulatory constraints using a maximization of their expected utility. Both\ntheoretical and simulation-based results are compared to a stylized genuine\nfinancial network. The diversification motive appears to plausibly explain\ninterconnections among key players. Using our model, the impact of regulation\non interconnections between banks -currently discussed at the Basel Committee\non Banking Supervision- is analyzed.\n"
    },
    {
        "paper_id": 1408.4746,
        "authors": "Amelia Carolina Sparavigna",
        "title": "Recurrence plots of exchange rates of currencies",
        "comments": "Keywords: Recurrence Plots, Texture Transitions, Currencies, Euro, US\n  Dollar, GB Pound, Japanese Yen, Exchange Rates, Autoregressive Models,\n  Econometrics",
        "journal-ref": "The International Journal of Sciences, 2014, Volume 3, Issue 7,\n  Pages 87-95",
        "doi": "10.18483/ijSci.545",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Used to investigate the presence of distinctive recurrent behaviours in\nnatural processes, the recurrence plots can be applied to the analysis of\neconomic data, and, in particular, to the characterization of exchange rates of\ncurrencies too. In this paper, we will show that these plots are able to\ncharacterize the periods of oscillation and random walk of currencies and\nenhance their reply to news and events, by means of texture transitions. The\nexamples of recurrence plots given here are obtained from time series of\nexchange rates of Euro.\n"
    },
    {
        "paper_id": 1408.4848,
        "authors": "Erhan Bayraktar and Gu Wang",
        "title": "Quantile Hedging in a Semi-Static Market with Model Uncertainty",
        "comments": "Final version. To appear in the Mathematical Methods of Operations\n  Research. Keywords: Quantile hedging, expected success ratio, model\n  uncertainty, semi-static hedging, Neyman-Pearson Lemma",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With model uncertainty characterized by a convex, possibly non-dominated set\nof probability measures, the agent minimizes the cost of hedging a path\ndependent contingent claim with given expected success ratio, in a\ndiscrete-time, semi-static market of stocks and options. Based on duality\nresults which link quantile hedging to a randomized composite hypothesis test,\nan arbitrage-free discretization of the market is proposed as an approximation.\nThe discretized market has a dominating measure, which guarantees the existence\nof the optimal hedging strategy and helps numerical calculation of the quantile\nhedging price. As the discretization becomes finer, the approximate quantile\nhedging price converges and the hedging strategy is asymptotically optimal in\nthe original market.\n"
    },
    {
        "paper_id": 1408.5266,
        "authors": "Anindya Goswami, Jeeten Patel, Poorva Sevgaonkar",
        "title": "The optimal hedging in a semi-Markov modulated market",
        "comments": "23 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper includes an original self contained proof of well-posedness of an\ninitial-boundary value problem involving a non-local parabolic PDE which\nnaturally arises in the study of derivative pricing in a generalized market\nmodel. We call this market model a semi-Markov modulated market. Although a\nwellposedness result of that problem is available in the literature, but this\nrecent paper has a different proof. Here the existence of solution is\nestablished without invoking mild solution technique. We study the\nwell-posedness of the initial-boundary value problem via a Volterra integral\nequation of second kind. The method of conditioning on stopping times was used\nonly for showing uniqueness. Furthermore, in the present study we find an\nintegral representation of the PDE problem which enables us to find a robust\nnumerical scheme to compute derivative of the solution. This study paves for\naddressing many other interesting problems involving this new set of PDEs. Some\nderivations of external cash flow corresponding to an optimal strategy are\npresented. These quantities are extremely important when dealing with an\nincomplete market. Apart from these, the risk measures for discrete trading are\nformulated which may be of interest to the practitioners.\n"
    },
    {
        "paper_id": 1408.551,
        "authors": "Bruno Bouchard, Marcel Nutz",
        "title": "Consistent Price Systems under Model Uncertainty",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a version of the fundamental theorem of asset pricing for\ndiscrete-time markets with proportional transaction costs and model\nuncertainty. A robust notion of no-arbitrage of the second kind is defined and\nshown to be equivalent to the existence of a collection of strictly consistent\nprice systems.\n"
    },
    {
        "paper_id": 1408.5526,
        "authors": "Linlin Xu and Giray \\\"Okten",
        "title": "High Performance Financial Simulation Using Randomized Quasi-Monte Carlo\n  Methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  GPU computing has become popular in computational finance and many financial\ninstitutions are moving their CPU based applications to the GPU platform. Since\nmost Monte Carlo algorithms are embarrassingly parallel, they benefit greatly\nfrom parallel implementations, and consequently Monte Carlo has become a focal\npoint in GPU computing. GPU speed-up examples reported in the literature often\ninvolve Monte Carlo algorithms, and there are software tools commercially\navailable that help migrate Monte Carlo financial pricing models to GPU.\n  We present a survey of Monte Carlo and randomized quasi-Monte Carlo methods,\nand discuss existing (quasi) Monte Carlo sequences in GPU libraries. We discuss\nspecific features of GPU architecture relevant for developing efficient (quasi)\nMonte Carlo methods. We introduce a recent randomized quasi-Monte Carlo method,\nand compare it with some of the existing implementations on GPU, when they are\nused in pricing caplets in the LIBOR market model and mortgage backed\nsecurities.\n"
    },
    {
        "paper_id": 1408.5585,
        "authors": "Diane Wilcox and Tim Gebbie",
        "title": "Hierarchical causality in financial economics",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2544327",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hierarchical analysis is considered and a multilevel model is presented in\norder to explore causality, chance and complexity in financial economics. A\ncoupled system of models is used to describe multilevel interactions,\nconsistent with market data: the lowest level is occupied by agents generating\nthe prices of individual traded assets; the next level entails aggregation of\nstocks into markets; the third level combines shared risk factors with\ninformation variables and bottom-up, agent-generated structure, consistent with\nconditions for no-arbitrage pricing theory; the fourth level describes market\nfactors which originate in the greater economy and the highest levels are\ndescribed by regulated market structure and the customs and ethics which define\nthe nature of acceptable transactions. A mechanism for emergence or innovation\nis considered and causal sources are discussed in terms of five causation\nclasses.\n"
    },
    {
        "paper_id": 1408.5618,
        "authors": "Hao Meng (ECUST), Hai-Chuan Xu (ECUST), Wei-Xing Zhou (ECUST), Didier\n  Sornette (ETH Zurich)",
        "title": "Symmetric thermal optimal path and time-dependent lead-lag relationship:\n  Novel statistical tests and application to UK and US real-estate and monetary\n  policies",
        "comments": null,
        "journal-ref": "Quantitative Finance 17 (6), 959-977 (2017)",
        "doi": "10.1080/14697688.2016.1241424",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the symmetric thermal optimal path (TOPS) method to determine the\ntime-dependent lead-lag relationship between two stochastic time series. This\nnovel version of the previously introduced TOP method alleviates some\ninconsistencies by imposing that the lead-lag relationship should be invariant\nwith respect to a time reversal of the time series after a change of sign. This\nmeans that, if `$X$ comes before $Y$', this transforms into `$Y$ comes before\n$X$' under a time reversal. We show that previously proposed bootstrap test\nlacks power and leads too often to a lack of rejection of the null that there\nis no lead-lag correlation when it is present. We introduce instead two novel\ntests. The first the free energy p-value $\\rho$ criterion quantifies the\nprobability that a given lead-lag structure could be obtained from random time\nseries with similar characteristics except for the lead-lag information. The\nsecond self-consistent test embodies the idea that, for the lead-lag path to be\nsignificant, synchronizing the two time series using the time varying lead-lag\npath should lead to a statistically significant correlation. We perform\nintensive synthetic tests to demonstrate their performance and limitations.\nFinally, we apply the TOPS method with the two new tests to the time dependent\nlead-lag structures of house price and monetary policy of the United Kingdom\n(UK) and United States (US) from 1991 to 2011. The TOPS approach stresses the\nimportance of accounting for change of regimes, so that similar pieces of\ninformation or policies may have drastically different impacts and\ndevelopments, conditional on the economic, financial and geopolitical\nconditions. This study reinforces the view that the hypothesis of statistical\nstationarity is highly questionable.\n"
    },
    {
        "paper_id": 1408.5673,
        "authors": "Beata Stehlikova",
        "title": "Approximating the zero-coupon bond price in a general one-factor model\n  with constant coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general one-factor short rate model, in which the instantaneous\ninterest rate is driven by a univariate diffusion with time independent drift\nand volatility. We construct recursive formula for the coefficients of the\nTaylor expansion of the bond price and its logarithm around $\\tau=0$, where\n$\\tau$ is time to maturity. We provide numerical examples of convergence of the\npartial sums of the series and compare them with the known exact values in the\ncase of Cox-Ingersoll-Ross and Dothan model.\n"
    },
    {
        "paper_id": 1408.5677,
        "authors": "Jiatu Cai and Masaaki Fukasawa",
        "title": "Asymptotic replication with modified volatility under small transaction\n  costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic hedging of an European option under a general local volatility model\nwith small linear transaction costs is studied. A continuous control version of\nLeland's strategy that asymptotically replicates the payoff is constructed. An\nassociated central limit theorem of hedging error is proved. The asymptotic\nerror variance is minimized by an explicit trading strategy.\n"
    },
    {
        "paper_id": 1408.5951,
        "authors": "Ashish R. Hota, Siddharth Garg, Shreyas Sundaram",
        "title": "Fragility of the Commons under Prospect-Theoretic Risk Attitudes",
        "comments": "Accepted for publication in Games and Economic Behavior, 2016",
        "journal-ref": null,
        "doi": "10.1016/j.geb.2016.06.003",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study a common-pool resource game where the resource experiences failure\nwith a probability that grows with the aggregate investment in the resource. To\ncapture decision making under such uncertainty, we model each player's risk\npreference according to the value function from prospect theory. We show the\nexistence and uniqueness of a pure Nash equilibrium when the players have\nheterogeneous risk preferences and under certain assumptions on the rate of\nreturn and failure probability of the resource. Greater competition, vis-a-vis\nthe number of players, increases the failure probability at the Nash\nequilibrium; we quantify this effect by obtaining bounds on the ratio of the\nfailure probability at the Nash equilibrium to the failure probability under\ninvestment by a single user. We further show that heterogeneity in attitudes\ntowards loss aversion leads to higher failure probability of the resource at\nthe equilibrium.\n"
    },
    {
        "paper_id": 1408.5989,
        "authors": "Christoph Czichowsky and Walter Schachermayer",
        "title": "Duality Theory for Portfolio Optimisation under Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For portfolio optimisation under proportional transaction costs, we provide a\nduality theory for general cadlag price processes. In this setting, we prove\nthe existence of a dual optimiser as well as a shadow price process in a\ngeneralised sense. This shadow price is defined via a \"sandwiched\" process\nconsisting of a predictable and an optional strong supermartingale and pertains\nto all strategies which remain solvent under transaction costs. We provide\nexamples showing that in the present general setting the shadow price process\nhas to be of this generalised form.\n"
    },
    {
        "paper_id": 1408.6043,
        "authors": "Giovanni Fasano",
        "title": "A Framework of Conjugate Direction Methods for Symmetric Linear Systems\n  in Optimization",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": "10.1007/s10957-014-0600-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a parameter dependent class of Krylov-based\nmethods, namely CD, for the solution of symmetric linear systems. We give\nevidence that in our proposal we generate sequences of conjugate directions,\nextending some properties of the standard Conjugate Gradient (CG) method, in\norder to preserve the conjugacy. For specific values of the parameters in our\nframework we obtain schemes equivalent to both the CG and the scaled-CG. We\nalso prove the finite convergence of the algorithms in CD, and we provide some\nerror analysis. Finally, preconditioning is introduced for CD, and we show that\nstandard error bounds for the preconditioned CG also hold for the\npreconditioned CD.\n"
    },
    {
        "paper_id": 1408.6065,
        "authors": "Christoph Czichowsky, Walter Schachermayer, Junjian Yang",
        "title": "Shadow prices for continuous processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a financial market with a continuous price process and proportional\ntransaction costs we investigate the problem of utility maximization of\nterminal wealth. We give sufficient conditions for the existence of a shadow\nprice process, i.e.~a least favorable frictionless market leading to the same\noptimal strategy and utility as in the original market under transaction costs.\nThe crucial ingredients are the continuity of the price process and the\nhypothesis of \"no unbounded profit with bounded risk\". A counter-example\nreveals that these hypotheses cannot be relaxed.\n"
    },
    {
        "paper_id": 1408.607,
        "authors": "Xiangyu Cui, Xun Li, Duan Li, Yun Shi",
        "title": "Time Consistent Behavior Portfolio Policy for Dynamic Mean-Variance\n  Formulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When we implement a portfolio selection methodology under a mean-risk\nformulation, it is essential to correctly model investors' risk aversion which\nmay be time-dependent, or even state-dependent during the investment procedure.\nIn this paper, we propose a behavior risk aversion model, which is a piecewise\nlinear function of the current wealth level with a reference point at a preset\ninvestment target. Due to the time inconsistency of the resulting multi-period\nmean-variance model with an adaptive risk aversion, we investigate in this\npaper the time consistent behavior portfolio policy by solving a nested\nmean-variance game formulation. We derive semi-analytical time consistent\nbehavior portfolio policy which takes a piecewise linear feedback form of the\ncurrent wealth level with respect to the discounted investment target.\n"
    },
    {
        "paper_id": 1408.6118,
        "authors": "Takashi Kato",
        "title": "VWAP Execution as an Optimal Strategy",
        "comments": "13 pages, 3 figures, long version of the paper \"VWAP execution as an\n  optimal strategy\" in JSIAM Letters, Vol. 7 (2015), pp.33-36",
        "journal-ref": "JSIAM Letters, Vol. 7 (2015), pp.33-36",
        "doi": "10.14495/jsiaml.7.33",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The volume weighted average price (VWAP) execution strategy is well known and\nwidely used in practice. In this study, we explicitly introduce a trading\nvolume process into the Almgren-Chriss model, which is a standard model for\noptimal execution. We then show that the VWAP strategy is the optimal execution\nstrategy for a risk-neutral trader. Moreover, we examine the case of a\nrisk-averse trader and derive the first-order asymptotic expansion of the\noptimal strategy for a mean-variance optimization problem.\n"
    },
    {
        "paper_id": 1408.6122,
        "authors": "Mireille Bossy, Nadia Maizi, Odile Pourtallier",
        "title": "Game theory analysis for carbon auction market through electricity\n  market coupling",
        "comments": "arXiv admin note: text overlap with arXiv:1311.1535",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze Nash equilibria between electricity producers\nselling their production on an electricity market and buying CO2 emission\nallowances on an auction carbon market. The producers' strategies integrate the\ncoupling of the two markets via the cost functions of the electricity\nproduction. We set out a clear Nash equilibrium on the power market that can be\nused to compute equilibrium prices on both markets as well as the related\nelectricity produced and CO2 emissions released.\n"
    },
    {
        "paper_id": 1408.6255,
        "authors": "T. Gubiec and M. Wili\\'nski",
        "title": "Intra-day variability of the stock market activity versus stationarity\n  of the financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2015.03.033",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe the impact of the intra-day activity pattern on the\nautocorrelation function estimator. We obtain an exact formula relating\nestimators of the autocorrelation functions of non-stationary process to its\nstationary counterpart. Hence, we proved that the day seasonality of\ninter-transaction times extends the memory of as well the process itself as its\nabsolute value. That is, both processes relaxation to zero is longer.\n"
    },
    {
        "paper_id": 1408.6279,
        "authors": "Marcio Laurini and Alberto Ohashi",
        "title": "A Noisy Principal Component Analysis for Forward Rate Curves",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Principal Component Analysis (PCA) is the most common nonparametric method\nfor estimating the volatility structure of Gaussian interest rate models. One\nmajor difficulty in the estimation of these models is the fact that forward\nrate curves are not directly observable from the market so that non-trivial\nobservational errors arise in any statistical analysis. In this work, we point\nout that the classical PCA analysis is not suitable for estimating factors of\nforward rate curves due to the presence of measurement errors induced by market\nmicrostructure effects and numerical interpolation. Our analysis indicates that\nthe PCA based on the long-run covariance matrix is capable to extract the true\ncovariance structure of the forward rate curves in the presence of\nobservational errors. Moreover, it provides a significant reduction in the\npricing errors due to noisy data typically founded in forward rate curves.\n"
    },
    {
        "paper_id": 1408.6455,
        "authors": "Huyen Pham (LPMA, CREST)",
        "title": "Long time asymptotics for optimal investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey reviews portfolio selection problem for long-term horizon. We\nconsider two objectives: (i) maximize the probability for outperforming a\ntarget growth rate of wealth process (ii) minimize the probability of falling\nbelow a target growth rate. We study the asymptotic behavior of these criteria\nformulated as large deviations control pro\\-blems, that we solve by duality\nmethod leading to ergodic risk-sensitive portfolio optimization problems.\nSpecial emphasis is placed on linear factor models where explicit solutions are\nobtained.\n"
    },
    {
        "paper_id": 1408.6513,
        "authors": "Andrey Itkin and Alexander Lipton",
        "title": "Efficient solution of structural default models with correlated jumps\n  and mutual obligations",
        "comments": "43 pages, 18 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The structural default model of Lipton and Sepp, 2009 is generalized for a\nset of banks with mutual interbank liabilities whose assets are driven by\ncorrelated Levy processes with idiosyncratic and common components. The\nmulti-dimensional problem is made tractable via a novel computational method,\nwhich generalizes the one-dimensional fractional partial differential equation\nmethod of Itkin, 2014 to the two- and three-dimensional cases. This method is\nunconditionally stable and of the second order of approximation in space and\ntime; in addition, for many popular Levy models it has linear complexity in\neach dimension. Marginal and joint survival probabilities for two and three\nbanks with mutual liabilities are computed. The effects of mutual liabilities\nare discussed, and numerical examples are given to illustrate these effects.\n"
    },
    {
        "paper_id": 1408.6637,
        "authors": "Ladislav Kristoufek",
        "title": "Spectrum-based estimators of the bivariate Hurst exponent",
        "comments": "15 pages, 4 figures",
        "journal-ref": "Physical Review E 90, 062802 (2014)",
        "doi": "10.1103/PhysRevE.90.062802",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We introduce two new estimators of the bivariate Hurst exponent in the\npower-law cross-correlations setting -- the cross-periodogram and local\n$X$-Whittle estimators -- as generalizations of their univariate counterparts.\nAs the spectrum-based estimators are dependent on a part of the spectrum taken\ninto consideration during estimation, a simulation study showing performance of\nthe estimators under varying bandwidth parameter as well as correlation between\nprocesses and their specification is provided as well. The newly introduced\nestimators are less biased than the already existent averaged periodogram\nestimator which, however, has slightly lower variance. The spectrum-based\nestimators can serve as a good complement to the popular time domain\nestimators.\n"
    },
    {
        "paper_id": 1408.6639,
        "authors": "Jaroslav Pavlicek and Ladislav Kristoufek",
        "title": "Can Google searches help nowcast and forecast unemployment rates in the\n  Visegrad Group countries?",
        "comments": "22 pages, 2 figures, 3 tables",
        "journal-ref": "PLoS ONE 10(5): e0127084, 2015",
        "doi": "10.1371/journal.pone.0127084",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Online activity of the Internet users has been repeatedly shown to provide a\nrich information set for various research fields. We focus on the job-related\nsearches on Google and their possible usefulness in the region of the Visegrad\nGroup -- the Czech Republic, Hungary, Poland and Slovakia. Even for rather\nsmall economies, the online searches of their inhabitants can be successfully\nutilized for macroeconomic predictions. Specifically, we study the unemployment\nrates and their interconnection to the job-related searches. We show that the\nGoogle searches strongly enhance both nowcasting and forecasting models of the\nunemployment rates.\n"
    },
    {
        "paper_id": 1408.6673,
        "authors": "Maciej J. Capi\\'nski",
        "title": "Hedging Conditional Value at Risk with Options",
        "comments": "10 pages, 0 figures",
        "journal-ref": "European Journal of Operational Research 242 (2015) 688-691",
        "doi": "10.1016/j.ejor.2014.11.011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method of hedging Conditional Value at Risk of a position in\nstock using put options. The result leads to a linear programming problem that\ncan be solved to optimise risk hedging.\n"
    },
    {
        "paper_id": 1408.6799,
        "authors": "Erhan Bayraktar, Jiaqi Li",
        "title": "Stochastic Perron for stochastic target games",
        "comments": "Published at http://dx.doi.org/10.1214/15-AAP1112 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2016, Vol. 26, No. 2, 1082-1110",
        "doi": "10.1214/15-AAP1112",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the stochastic Perron method to analyze the framework of stochastic\ntarget games, in which one player tries to find a strategy such that the state\nprocess almost surely reaches a given target no matter which action is chosen\nby the other player. Within this framework, our method produces a viscosity\nsub-solution (super-solution) of a Hamilton-Jacobi-Bellman (HJB) equation. We\nthen characterize the value function as a viscosity solution to the HJB\nequation using a comparison result and a byproduct to obtain the dynamic\nprogramming principle.\n"
    },
    {
        "paper_id": 1408.6938,
        "authors": "Xiaolin Luo and Pavel V. Shevchenko",
        "title": "Fast and Simple Method for Pricing Exotic Options using Gauss-Hermite\n  Quadrature on a Cubic Spline Interpolation",
        "comments": null,
        "journal-ref": "Journal of Financial Engineering, Vol. 1, No. 4 (December 2014)",
        "doi": "10.1142/S2345768614500330",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a vast literature on numerical valuation of exotic options using\nMonte Carlo, binomial and trinomial trees, and finite difference methods. When\ntransition density of the underlying asset or its moments are known in closed\nform, it can be convenient and more efficient to utilize direct integration\nmethods to calculate the required option price expectations in a backward\ntime-stepping algorithm. This paper presents a simple, robust and efficient\nalgorithm that can be applied for pricing many exotic options by computing the\nexpectations using Gauss-Hermite integration quadrature applied on a cubic\nspline interpolation. The algorithm is fully explicit but does not suffer the\ninherent instability of the explicit finite difference counterpart. A `free'\nbonus of the algorithm is that it already contains the function for fast and\naccurate interpolation of multiple solutions required by many discretely\nmonitored path dependent options. For illustrations, we present examples of\npricing a series of American options with either Bermudan or continuous\nexercise features, and a series of exotic path-dependent options of target\naccumulation redemption note (TARN). Results of the new method are compared\nwith Monte Carlo and finite difference methods, including some of the most\nadvanced or best known finite difference algorithms in the literature. The\ncomparison shows that, despite its simplicity, the new method can rival with\nsome of the best finite difference algorithms in accuracy and at the same time\nit is significantly faster. Virtually the same algorithm can be applied to\nprice other path-dependent financial contracts such as Asian options and\nvariable annuities.\n"
    },
    {
        "paper_id": 1408.6973,
        "authors": "Serguei Saavedra, Rudolf P. Rohr, Luis J. Gilarranz, Jordi Bascompte",
        "title": "How structurally stable are global socioeconomic systems?",
        "comments": null,
        "journal-ref": "J. R. Soc. Interface 11: 20140693 (2014)",
        "doi": "10.1098/?rsif.2014.0693",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stability analysis of socioeconomic systems has been centered on\nanswering whether small perturbations when a system is in a given quantitative\nstate will push the system permanently to a different quantitative state.\nHowever, typically the quantitative state of socioeconomic systems is subject\nto constant change. Therefore, a key stability question that has been\nunder-investigated is how strong the conditions of a system itself can change\nbefore the system moves to a qualitatively different behavior, i.e., how\nstructurally stable the systems is. Here, we introduce a framework to\ninvestigate the structural stability of socioeconomic systems formed by the\nnetwork of interactions among agents competing for resources. We measure the\nstructural stability of the system as the range of conditions in the\ndistribution and availability of resources compatible with the qualitative\nbehavior in which all the constituent agents can be self-sustained across time.\nTo illustrate our framework, we study an empirical representation of the global\nsocioeconomic system formed by countries sharing and competing for\nmultinational companies used as proxy for resources. We demonstrate that the\nstructural stability of the system is inversely associated with the level of\ncompetition and the level of heterogeneity in the distribution of resources.\nImportantly, we show that the qualitative behavior of the observed global\nsocioeconomic system is highly sensitive to changes in the distribution of\nresources. We believe this work provides a methodological basis to develop\nsustainable strategies for socioeconomic systems subject to constantly changing\nconditions.\n"
    },
    {
        "paper_id": 1408.701,
        "authors": "Scott Robertson, Hao Xing",
        "title": "Long Term Optimal Investment in Matrix Valued Factor Models",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long term optimal investment problems are studied in a factor model with\nmatrix valued state variables. Explicit parameter restrictions are obtained\nunder which, for an isoelastic investor, the finite horizon value function and\noptimal strategy converge to their long-run counterparts as the investment\nhorizon approaches infinity. This convergence also yields portfolio turnpikes\nfor general utilities. By using results on large time behaviour of semi-linear\npartial differential equations, our analysis extends affine models, where the\nWishart process drives investment opportunities, to a non-affine setting.\nFurthermore, in the affine setting, an example is constructed where the value\nfunction is not exponentially affine, in contrast to models with vector-valued\nstate variables.\n"
    },
    {
        "paper_id": 1409.0002,
        "authors": "Atif Ansar, Bent Flyvbjerg, Alexander Budzier, Daniel Lunn",
        "title": "Should we build more large dams? The actual costs of hydropower\n  megaproject development",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.enpol.2013.10.069",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A brisk building boom of hydropower mega-dams is underway from China to\nBrazil. Whether benefits of new dams will outweigh costs remains unresolved\ndespite contentious debates. We investigate this question with the \"outside\nview\" or \"reference class forecasting\" based on literature on decision-making\nunder uncertainty in psychology. We find overwhelming evidence that budgets are\nsystematically biased below actual costs of large hydropower dams - excluding\ninflation, substantial debt servicing, environmental, and social costs. Using\nthe largest and most reliable reference data of its kind and multilevel\nstatistical techniques applied to large dams for the first time, we were\nsuccessful in fitting parsimonious models to predict cost and schedule\noverruns. The outside view suggests that in most countries large hydropower\ndams will be too costly in absolute terms and take too long to build to deliver\na positive risk-adjusted return unless suitable risk management measures\noutlined in this paper can be affordably provided. Policymakers, particularly\nin developing countries, are advised to prefer agile energy alternatives that\ncan be built over shorter time horizons to energy megaprojects.\n"
    },
    {
        "paper_id": 1409.0003,
        "authors": "Bent Flyvbjerg",
        "title": "What You Should Know About Megaprojects, and Why: An Overview",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/pmj.21409",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper takes stock of megaproject management, an emerging and hugely\ncostly field of study. First, it answers the question of how large megaprojects\nare by measuring them in the units mega, giga, and tera, concluding we are\npresently entering a new \"tera era\" of trillion-dollar projects. Second, total\nglobal megaproject spending is assessed, at USD 6-9 trillion annually, or 8\npercent of total global GDP, which denotes the biggest investment boom in human\nhistory. Third, four \"sublimes\" - political, technological, economic, and\naesthetic - are identified to explain the increased size and frequency of\nmegaprojects. Fourth, the \"iron law of megaprojects\" is laid out and\ndocumented: Over budget, over time, over and over again. Moreover, the\n\"break-fix model\" of megaproject management is introduced as an explanation of\nthe iron law. Fifth, Albert O. Hirschman's theory of the Hiding Hand is\nrevisited and critiqued as unfounded and corrupting for megaproject thinking in\nboth the academy and policy. Sixth, it is shown how megaprojects are\nsystematically subject to \"survival of the unfittest,\" explaining why the worst\nprojects get built instead of the best. Finally, it is argued that the\nconventional way of managing megaprojects has reached a \"tension point,\" where\ntradition is challenged and reform is emerging.\n"
    },
    {
        "paper_id": 1409.0118,
        "authors": "Tetsuya Takaishi",
        "title": "Analysis of Spin Financial Market by GARCH Model",
        "comments": "10 pages",
        "journal-ref": "Journal of Physics: Conference Series 454 (2013) 012041",
        "doi": "10.1088/1742-6596/454/1/012041",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A spin model is used for simulations of financial markets. To determine\nreturn volatility in the spin financial market we use the GARCH model often\nused for volatility estimation in empirical finance. We apply the Bayesian\ninference performed by the Markov Chain Monte Carlo method to the parameter\nestimation of the GARCH model. It is found that volatility determined by the\nGARCH model exhibits \"volatility clustering\" also observed in the real\nfinancial markets. Using volatility determined by the GARCH model we examine\nthe mixture-of-distribution hypothesis (MDH) suggested for the asset return\ndynamics. We find that the returns standardized by volatility are approximately\nstandard normal random variables. Moreover we find that the absolute\nstandardized returns show no significant autocorrelation. These findings are\nconsistent with the view of the MDH for the return dynamics.\n"
    },
    {
        "paper_id": 1409.0407,
        "authors": "Chuancun Yin, Kam Chuen Yuen",
        "title": "Optimal dividend problems for a jump-diffusion model with capital\n  injections and proportional transaction costs",
        "comments": "Journal of Industrial and Management Optimization (to appear)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the optimal control problem for a company whose\nsurplus process evolves as an upward jump diffusion with random return on\ninvestment. Three types of practical optimization problems faced by a company\nthat can control its liquid reserves by paying dividends and injecting capital.\nIn the first problem, we consider the classical dividend problem without\ncapital injections. The second problem aims at maximizing the expected\ndiscounted dividend payments minus the expected discounted costs of capital\ninjections over strategies with positive surplus at all times. The third\nproblem has the same objective as the second one, but without the constraints\non capital injections. Under the assumption of proportional transaction costs,\nwe identify the value function and the optimal strategies for any distribution\nof gains.\n"
    },
    {
        "paper_id": 1409.0636,
        "authors": "V.I. Yukalov and D. Sornette",
        "title": "Manipulating decision making of typical agents",
        "comments": "Latex file, 14 pages",
        "journal-ref": "IEEE Trans. Syst. Man Cybern. Syst. 44 (2014) 1155-1168",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate how the choice of decision makers can be varied under the\npresence of risk and uncertainty. Our analysis is based on the approach we have\npreviously applied to individual decision makers, which we now generalize to\nthe case of decision makers that are members of a society. The approach employs\nthe mathematical techniques that are common in quantum theory, justifying our\nnaming as Quantum Decision Theory. However, we do not assume that decision\nmakers are quantum objects. The techniques of quantum theory are needed only\nfor defining the prospect probabilities taking into account such hidden\nvariables as behavioral biases and other subconscious feelings. The approach\ndescribes an agent's choice as a probabilistic event occurring with a\nprobability that is the sum of a utility factor and of an attraction factor.\nThe attraction factor embodies subjective and unconscious dimensions in the\nmind of the decision maker. We show that the typical aggregate amplitude of the\nattraction factor is $1/4$, and it can be either positive or negative depending\non the relative attraction of the competing choices. The most efficient way of\nvarying the decision makers choice is realized by influencing the attraction\nfactor. This can be done in two ways. One method is to arrange in a special\nmanner the payoff weights, which induces the required changes of the values of\nattraction factors. We show that a slight variation of the payoff weights can\ninvert the sign of the attraction factors and reverse the decision preferences,\neven when the prospect utilities remain unchanged. The second method of\ninfluencing the decision makers choice is by providing information to decision\nmakers. The methods of influencing decision making are illustrated by several\nexperiments, whose outcomes are compared quantitatively with the predictions of\nour approach.\n"
    },
    {
        "paper_id": 1409.0665,
        "authors": "Maria B. Chiarolla, Giorgio Ferrari and Gabriele Stabile",
        "title": "Optimal Dynamic Procurement Policies for a Storable Commodity with\n  L\\'evy Prices and Convex Holding Costs",
        "comments": "28 pages, 3 figures; improved presentation, added new results and\n  sections",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a continuous time stochastic inventory model for a\ncommodity traded in the spot market and whose supply purchase is affected by\nprice and demand uncertainty. A firm aims at meeting a random demand of the\ncommodity at a random time by maximizing total expected profits. We model the\nfirm's optimal procurement problem as a singular stochastic control problem in\nwhich controls are nondecreasing processes and represent the cumulative\ninvestment made by the firm in the spot market (a so-called stochastic\n\"monotone follower problem\"). We assume a general exponential L\\'evy process\nfor the commodity's spot price, rather than the commonly used geometric\nBrownian motion, and general convex holding costs.\n  We obtain necessary and sufficient first order conditions for optimality and\nwe provide the optimal procurement policy in terms of a \"base inventory\"\nprocess; that is, a minimal time-dependent desirable inventory level that the\nfirm's manager must reach at any time. In particular, in the case of linear\nholding costs and exponentially distributed demand, we are also able to obtain\nthe explicit analytic form of the optimal policy and a probabilistic\nrepresentation of the optimal revenue. The paper is completed by some computer\ndrawings of the optimal inventory when spot prices are given by a geometric\nBrownian motion and by an exponential jump-diffusion process. In the first case\nwe also make a numerical comparison between the value function and the revenue\nassociated to the classical static \"newsvendor\" strategy.\n"
    },
    {
        "paper_id": 1409.0697,
        "authors": "Bowei Chen and Jun Wang",
        "title": "A lattice framework for pricing display advertisement options with the\n  stochastic volatility underlying model",
        "comments": "Bowei Chen and Jun Wang. A lattice framework for pricing display\n  advertisement options with the stochastic volatility underlying model.\n  Electronic Commerce Research and Applications, 2015, Volume 14, Issue 6,\n  pages 465-479, ISSN: 1567-4223",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Advertisement (abbreviated ad) options are a recent development in online\nadvertising. Simply, an ad option is a first look contract in which a publisher\nor search engine grants an advertiser a right but not obligation to enter into\ntransactions to purchase impressions or clicks from a specific ad slot at a\npre-specified price on a specific delivery date. Such a structure provides\nadvertisers with more flexibility of their guaranteed deliveries. The valuation\nof ad options is an important topic and previous studies on ad options pricing\nhave been mostly restricted to the situations where the underlying prices\nfollow a geometric Brownian motion (GBM). This assumption is reasonable for\nsponsored search; however, some studies have also indicated that it is not\nvalid for display advertising. In this paper, we address this issue by\nemploying a stochastic volatility (SV) model and discuss a lattice framework to\napproximate the proposed SV model in option pricing. Our developments are\nvalidated by experiments with real advertising data: (i) we find that the SV\nmodel has a better fitness over the GBM model; (ii) we validate the proposed\nlattice model via two sequential Monte Carlo simulation methods; (iii) we\ndemonstrate that advertisers are able to flexibly manage their guaranteed\ndeliveries by using the proposed options, and publishers can have an increased\nrevenue when some of their inventories are sold via ad options.\n"
    },
    {
        "paper_id": 1409.0789,
        "authors": "Rosario N. Mantegna",
        "title": "Sicily and the development of Econophysics: the pioneering work of\n  Ettore Majorana and the Econophysics Workshop in Palermo",
        "comments": "4 pages, Proceedings of the XXXIII Congress of the Italian Society\n  for the History of Physics and Astronomy (SISFA 2013 Acireale September\n  4th-7th 2013)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sicily has played an important role in the development of the new research\narea named \"Econophysics\". In fact some key ideas supporting this new hybrid\ndiscipline were originally formulated in a pioneering work of the Sicilian born\nphysicist Ettore Majorana. The article he wrote was entitled \"The value of\nstatistical laws in physics and social sciences\". I will discuss its origin and\nhistory that has been recently discovered in the study of Stefano Roncoroni.\nThis recent study documents the true reasons and motivations that triggered the\npioneering work of Majorana. It also shows that the description of this work\nprovided by Edoardo Amaldi was shallow and misleading. In the second part of\nthe talk I will recollect the first years of development of econophysics and in\nparticular the role of the \"International Workshop on Econophysics and\nStatistical Finance\" held in Palermo on 28-30 September 1998 and the setting in\n1999 of the \"Observatory of Complex Systems\" the research group on Econophysics\nof Palermo University and Istituto Nazionale di Fisica della Materia.\n"
    },
    {
        "paper_id": 1409.1071,
        "authors": "A.V. Leonidov, E.L. Rumyantsev",
        "title": "Default contagion risks in Russian interbank market",
        "comments": "Final version, to appear in Physica A",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risks of default contagion in the Russian interbank market are\ninvestigated. The analysis is based on considering the bow-tie structure of the\nweighted oriented graph describing the structure of the interbank loans. A\nprobabilistic model of interbank contagion explicitly taking into account the\nempirical bow-tie structure reflecting functionality of the corresponding nodes\n(borrowers, lenders, borrowers and lenders simultaneously), degree\ndistributions and disassortativity of the interbank network under consideration\nbased on empirical data is developed. The characteristics of contagion-related\nsystemic risk calculated with this model are shown to be in agreement with\nthose of explicit stress tests.\n"
    },
    {
        "paper_id": 1409.1175,
        "authors": "Pablo Olivares and Matthew Cane",
        "title": "Pricing Spread Options under Stochastic Correlation and Jump-Diffusion\n  Models",
        "comments": "9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the problem of pricing spread options under some models\nwith jumps driven by Compound Poisson Processes and stochastic volatilities in\nthe form of Cox-Ingersoll-Ross(CIR) processes. We derive the characteristic\nfunction for two market models featuring joint normally distributed jumps,\nstochastic volatility, and different stochastic dependence structures. With the\nuse of Fast Fourier Transform(FFT) we accurately compute spread option prices\nacross a variety of strikes and initial price vectors at a very low\ncomputational cost when compared to Monte Carlo pricing methods. We also look\nat the sensitivities of the prices to the model specifications and find strong\ndependence on the selection of the jump and stochastic volatility parameters.\nOur numerical implementation is based on the method developed by Hurd and Zhou\n(2009).\n"
    },
    {
        "paper_id": 1409.1393,
        "authors": "Wai-Ki Ching, Jia-Wen Gu and Harry Zheng",
        "title": "On Correlated Defaults and Incomplete Information",
        "comments": "24 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a continuous time structural asset value model for\ntwo correlated firms using a two-dimensional Brownian motion. We consider the\nsituation of incomplete information, where the information set available to the\nmarket participants includes the default time of each firm and the periodic\nasset value reports. In this situation, the default time of each firm becomes a\ntotally inaccessible stopping time to the market participants. The original\nstructural model is first transformed to a reduced-form model. Then the\nconditional distribution of the default time together with the asset value of\neach name are derived. We prove the existence of the intensity processes of\ndefault times and also give the explicit form of the intensity processes.\nNumerical studies on the intensities of the two correlated names are conducted\nfor some special cases. We also indicate the possible future research extension\ninto three names case by considering a special correlation structure.\n"
    },
    {
        "paper_id": 1409.1441,
        "authors": "Vladimir Markov, Slava Mazur, and David Saltz",
        "title": "Design and Implementation of Schedule-Based Trading Strategies Based on\n  Uncertainty Bands",
        "comments": null,
        "journal-ref": "The Journal of Trading, Fall 2011, Vol. 6, No. 4, pp. 45-52",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a design for schedule-based execution trading strategies based on\nuncertainty bands. This formulation: 1) simplifies strategy specification and\nimplementation; 2) provides for flexible allocation among passive,\nopportunistic, aggressive, and dark pool crossing execution tactics; 3) allows\nfor rapid enhancements as new optimization methods, scheduling techniques,\nalpha models, and execution tactics are developed; and 4) yields information at\nmacroscopic (strategic) and microscopic (tactical) levels that is easily\npublished to trading databases and front-end applications.\n"
    },
    {
        "paper_id": 1409.1442,
        "authors": "Vladimir Markov",
        "title": "On the design of sell-side limit and market order tactics",
        "comments": null,
        "journal-ref": "The Journal of Trading, Summer 2012, Vol. 7, No. 3: pp. 29-39",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article provides a novel framework to evaluate limit order tactics that\nhighlights expected fill price, adverse price selection cost, and opportunity\ncost. We formulate the problem of optimal execution of market orders with\nnonlinear market impact, power law decay kernel, and stochastic and\ndeterministic liquidity constraints. We demonstrate how these tactics can be\nincorporated in the uncertainty bands framework.\n"
    },
    {
        "paper_id": 1409.1451,
        "authors": "Gareth W. Peters, Ariane Chapelle, Efstathios Panayi",
        "title": "Opening discussion on banking sector risk exposures and vulnerabilities\n  from virtual currencies: An operational risk perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop the first basic Operational Risk perspective on key risk\nmanagement issues associated with the development of new forms of electronic\ncurrency in the real economy. In particular, we focus on understanding the\ndevelopment of new risks types and the evolution of current risk types as new\ncomponents of financial institutions arise to cater for an increasing demand\nfor electronic money, micro-payment systems, Virtual money and cryptographic\n(Crypto) currencies. In particular, this paper proposes a framework of risk\nidentification and assessment applied to Virtual and Crypto currencies from a\nbanking regulation perspective. In doing so, it addresses the topical issues of\nunderstanding important key Operational Risk vulnerabilities and exposure risk\ndrivers under the framework of the Basel II/III banking regulation,\nspecifically associated with Virtual and Crypto currencies. This is critical to\nconsider should such alternative currencies continue to grow in utilisation to\nthe point that they enter into the banking sector, through commercial banks and\nfinancial institutions who are beginning to contemplate their recognition in\nterms of deposits, transactions and exchangeability for fiat currencies.\n  We highlight how some of the features of Virtual and Crypto currencies are\nimportant drivers of Operational Risk, posing both management and regulatory\nchallenges that must start to be considered and addressed both by regulators,\ncentral banks and security exchanges. In this paper we focus purely on the\nOperational Risk perspective of banks operating in an environment where such\nelectronic Virtual currencies are available. Some aspects of this discussion\nare directly relevant now, whilst others can be understood as discussions to\nraise awareness of issues in Operational Risk that will arise as Virtual\ncurrency start to interact more widely in the real economy.\n"
    },
    {
        "paper_id": 1409.162,
        "authors": "Yevgeniy Kovchegov, Nese Yildiz",
        "title": "Orthogonal Polynomials for Seminonparametric Instrumental Variables\n  Model",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an approach that resolves a {\\it polynomial basis problem} for a\nclass of models with discrete endogenous covariate, and for a class of\neconometric models considered in the work of Newey and Powell (2003), where the\nendogenous covariate is continuous. Suppose $X$ is a $d$-dimensional endogenous\nrandom variable, $Z_1$ and $Z_2$ are the instrumental variables (vectors), and\n$Z=\\left(\\begin{array}{c}Z_1 \\\\Z_2\\end{array}\\right)$. Now, assume that the\nconditional distributions of $X$ given $Z$ satisfy the conditions sufficient\nfor solving the identification problem as in Newey and Powell (2003) or as in\nProposition 1.1 of the current paper. That is, for a function $\\pi(z)$ in the\nimage space there is a.s. a unique function $g(x,z_1)$ in the domain space such\nthat $$E[g(X,Z_1)~|~Z]=\\pi(Z) \\qquad Z-a.s.$$ In this paper, for a class of\nconditional distributions $X|Z$, we produce an orthogonal polynomial basis\n$Q_j(x,z_1)$ such that for a.e. $Z_1=z_1$, and for all $j \\in \\mathbb{Z}_+^d$,\nand a certain $\\mu(Z)$, $$P_j(\\mu(Z))=E[Q_j(X, Z_1)~|~Z ],$$ where $P_j$ is a\npolynomial of degree $j$. This is what we call solving the {\\it polynomial\nbasis problem}.\n  Assuming the knowledge of $X|Z$ and an inference of $\\pi(z)$, our approach\nprovides a natural way of estimating the structural function of interest\n$g(x,z_1)$. Our polynomial basis approach is naturally extended to Pearson-like\nand Ord-like families of distributions.\n"
    },
    {
        "paper_id": 1409.1748,
        "authors": "Bulcsu Sandor and Zoltan Neda",
        "title": "A spring-block analogy for the dynamics of stock indexes",
        "comments": "11 pages 9 figures",
        "journal-ref": "Physica A, 427, 122-131 (2015)",
        "doi": "10.1016/j.physa.2015.01.079",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A spring-block chain placed on a running conveyor belt is considered for\nmodeling stylized facts observed in the dynamics of stock indexes. Individual\nstocks are modeled by the blocks, while the stock-stock correlations are\nintroduced via simple elastic forces acting in the springs. The dragging effect\nof the moving belt corresponds to the expected economic growth. The\nspring-block system produces collective behavior and avalanche like phenomena,\nsimilar to the ones observed in stock markets. An artificial index is defined\nfor the spring-block chain, and its dynamics is compared with the one measured\nfor the Dow Jones Industrial Average. For certain parameter regions the model\nreproduces qualitatively well the dynamics of the logarithmic index, the\nlogarithmic returns, the distribution of the logarithmic returns, the\navalanche-size distribution and the distribution of the investment horizons. A\nnoticeable success of the model is that it is able to account for the gain-loss\nasymmetry observed in the inverse statistics. Our approach has mainly a\npedagogical value, bridging between a complex socio-economic phenomena and a\nbasic (mechanical) model in physics.\n"
    },
    {
        "paper_id": 1409.1786,
        "authors": "Jin-Li Guo",
        "title": "Zero-determinant strategies in iterated multi-strategy games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Self-serving, rational agents sometimes cooperate to their mutual benefit.\nThe two-player iterated prisoner's dilemma game is a model for including the\nemergence of cooperation. It is generally believed that there is no simple\nultimatum strategy which a player can control the return of the other\nparticipants. The recent discovery of the powerful class of zero-determinant\nstrategies in the iterated prisoner's dilemma dramatically expands our\nunderstanding of the classic game by uncovering strategies that provide a\nunilateral advantage to sentient players pitted against unwitting opponents.\nHowever, strategies in the prisoner's dilemma game are only two strategies. Are\nthere these results for general multi-strategy games? To address this question,\nthe paper develops a theory for zero-determinant strategies for multi-strategy\ngames, with any number of strategies. The analytical results exhibit a similar\nyet different scenario to the case of two-strategy games. Zero-determinant\nstrategies in iterated prisoner's dilemma can be seen as degenerate case of our\nresults. The results are also applied to the snowdrift game, the hawk-dove game\nand the chicken game.\n"
    },
    {
        "paper_id": 1409.183,
        "authors": "Anja Richter and Josef Teichmann",
        "title": "Discrete Time Term Structure Theory and Consistent Recalibration Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop theory and applications of forward characteristic processes in\ndiscrete time following a seminal paper of Jan Kallsen and Paul Kr\\\"uhner.\nParticular emphasis is placed on the dynamics of volatility surfaces which can\nbe easily formulated and implemented from the chosen discrete point of view. In\nmathematical terms we provide an algorithmic answer to the following question:\ndescribe a rich, still tractable class of discrete time stochastic processes,\nwhose marginal distributions are given at initial time and which are free of\narbitrage. In terms of mathematical finance we can construct models with\npre-described (implied) volatility surface and quite general volatility surface\ndynamics. In terms of the works of Rene Carmona and Sergey Nadtochiy, we\nanalyze the dynamics of tangent affine models. We believe that the discrete\napproach due to its technical simplicity will be important in term structure\nmodelling.\n"
    },
    {
        "paper_id": 1409.1858,
        "authors": "Eberhard Mayerhofer",
        "title": "Affine Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We put forward a complete theory on moment explosion for fairly general\nstate-spaces. This includes a characterization of the validity of the affine\ntransform formula in terms of minimal solutions of a system of generalized\nRiccati differential equations.\n  Also, we characterize the class of positive semidefinite processes, and\nprovide existence of weak and strong solutions for Wishart SDEs. As an\napplication, we answer a conjecture of M.L. Eaton on the maximal parameter\ndomain of non-central Wishart distributions.\n  The last chapter of this thesis comprises three individual works on affine\nmodels, such as a characterization of the martingale property of exponentially\naffine processes, an investigation of the jump-behaviour of processes on\npositive semidefinite cones, and an existence result for transition densities\nof multivariate affine jump-diffusions and their approximation theory in\nweighted Hilbert spaces.\n"
    },
    {
        "paper_id": 1409.1956,
        "authors": "Roberto Casarin and Fabrizio Leisen and German Molina and Enrique ter\n  Horst",
        "title": "A Bayesian Beta Markov Random Field Calibration of the Term Structure of\n  Implied Risk Neutral Densities",
        "comments": "27 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build on the work in Fackler and King 1990, and propose a more general\ncalibration model for implied risk neutral densities. Our model allows for the\njoint calibration of a set of densities at different maturities and dates\nthrough a Bayesian dynamic Beta Markov Random Field. Our approach allows for\npossible time dependence between densities with the same maturity, and for\ndependence across maturities at the same point in time. This approach to the\nproblem encompasses model flexibility, parameter parsimony and, more\nimportantly, information pooling across densities.\n"
    },
    {
        "paper_id": 1409.2023,
        "authors": "Miklos Rasonyi",
        "title": "Optimal investment with bounded above utilities in discrete time markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an arbitrage-free, discrete time and frictionless market. We\nprove that an investor maximising the expected utility of her terminal wealth\ncan always find an optimal investment strategy provided that her\ndissatisfaction of infinite losses is infinite and her utility function is\nnon-decreasing, continuous and bounded above. The same result is shown for\ncumulative prospect theory preferences, under additional assumptions.\n"
    },
    {
        "paper_id": 1409.2214,
        "authors": "Nien-Lin Liu and Hoang-Long Ngo",
        "title": "Approximation of eigenvalues of spot cross volatility matrix with a view\n  toward principal component analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to study the geometry of interest rates market dynamics, Malliavin,\nMancino and Recchioni [A non-parametric calibration of the HJM geometry: an\napplication of It\\^o calculus to financial statistics, {\\it Japanese Journal of\nMathematics}, 2, pp.55--77, 2007] introduced a scheme, which is based on the\nFourier Series method, to estimate eigenvalues of a spot cross volatility\nmatrix. In this paper, we present another estimation scheme based on the\nQuadratic Variation method. We first establish limit theorems for each scheme\nand then we use a stochastic volatility model of Heston's type to compare the\neffectiveness of these two schemes.\n"
    },
    {
        "paper_id": 1409.2226,
        "authors": "Erik J. Baurdoux, Nan Chen, Budhi A. Surya and Kazutoshi Yamazaki",
        "title": "Optimal double stopping of a Brownian bridge",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal double stopping problems driven by a Brownian bridge. The\nobjective is to maximize the expected spread between the payoffs achieved at\nthe two stopping times. We study several cases where the solutions can be\nsolved explicitly by strategies of threshold type.\n"
    },
    {
        "paper_id": 1409.2575,
        "authors": "Zura Kakushadze and Jim Kyung-Soo Liew",
        "title": "Custom v. Standardized Risk Models",
        "comments": "30 pages; minor improvements, more source code added; to appear in\n  Risks",
        "journal-ref": "Risks 3(2) (2015) 112-138",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss when and why custom multi-factor risk models are warranted and\ngive source code for computing some risk factors. Pension/mutual funds do not\nrequire customization but standardization. However, using standardized risk\nmodels in quant trading with much shorter holding horizons is suboptimal: 1)\nlonger horizon risk factors (value, growth, etc.) increase noise trades and\ntrading costs; 2) arbitrary risk factors can neutralize alpha; 3)\n\"standardized\" industries are artificial and insufficiently granular; 4)\nnormalization of style risk factors is lost for the trading universe; 5)\ndiversifying risk models lowers P&L correlations, reduces turnover and market\nimpact, and increases capacity. We discuss various aspects of custom risk model\nbuilding.\n"
    },
    {
        "paper_id": 1409.2618,
        "authors": "Kyle Bechler and Mike Ludkovski",
        "title": "Optimal Execution with Dynamic Order Flow Imbalance",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine optimal execution models that take into account both market\nmicrostructure impact and informational costs. Informational footprint is\nrelated to order flow and is represented by the trader's influence on the flow\nimbalance process, while microstructure influence is captured by instantaneous\nprice impact. We propose a continuous-time stochastic control problem that\nbalances between these two costs. Incorporating order flow imbalance leads to\nthe consideration of the current market state and specifically whether one's\norders lean with or against the prevailing order flow, key components often\nignored by execution models in the literature. In particular, to react to\nchanging order flow, we endogenize the trading horizon $T$. After developing\nthe general indefinite-horizon formulation, we investigate several tractable\napproximations that sequentially optimize over price impact and over $T$. These\napproximations, especially a dynamic version based on receding horizon control,\nare shown to be very accurate and connect to the prevailing Almgren-Chriss\nframework. We also discuss features of empirical order flow and links between\nour model and \"Optimal Execution Horizon\" by Easley et al (Mathematical\nFinance, 2013).\n"
    },
    {
        "paper_id": 1409.2625,
        "authors": "Pierre Paga and Reimer K\\\"uhn",
        "title": "Contagion in an interacting economy",
        "comments": "21 pages, 6 figures",
        "journal-ref": "J. Stat. Mech. (2015) P03008",
        "doi": "10.1088/1742-5468/2015/03/P03008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the credit risk model defined in Hatchett & K\\\"{u}hn under\nmore general assumptions, in particular using a general degree distribution for\nsparse graphs. Expanding upon earlier results, we show that the model is\nexactly solvable in the $N\\rightarrow \\infty$ limit and demonstrate that the\nexact solution is described by the message-passing approach outlined by Karrer\nand Newman, generalized to include heterogeneous agents and couplings. We\nprovide comparisons with simulations of graph ensembles with power-law degree\ndistributions.\n"
    },
    {
        "paper_id": 1409.2661,
        "authors": "P. Lencastre, F. Raischel, P.G. Lind",
        "title": "The effect of the number of states on the validity of credit ratings",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-6596/574/1/012151",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explicitly test if the reliability of credit ratings depends on the total\nnumber of admissible states. We analyse open access credit rating data and show\nthat the effect of the number of states in the dynamical properties of ratings\nchange with time, thus giving supportive evidence that the ideal number of\nadmissible states changes with time. We use matrix estimation methods that\nexplicitly assume the hypothesis needed for the process to be a valid rating\nprocess. By comparing with the likelihood maximization method of matrix\nestimation, we quantify the \"likelihood-loss\" of assuming that the process is a\nwell grounded rating process.\n"
    },
    {
        "paper_id": 1409.276,
        "authors": "Inga Ivanova, Oivind Strand, and Loet Leydesdorff",
        "title": "Synergy cycles in the Norwegian innovation system: The relation between\n  synergy and cycle values",
        "comments": "31 pages, 16 figures",
        "journal-ref": "Foresight and STI Governance, 13(1)(2019), 48-6",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The knowledge base of an economy measured in terms of Triple Helix relations\ncan be analyzed in terms of mutual information among geographical, sectorial,\nand size distributions of firms as dimensions of the probabilistic entropy. The\nresulting synergy values of a TH system provide static snapshots. In this\nstudy, we add the time dimension and analyze the synergy dynamics using the\nNorwegian innovation system as an example. The synergy among the three\ndimensions can be mapped as a set of partial time series and spectrally\nanalyzed. The results suggest that the synergy at the level of both the country\nand its 19 counties shoe non-chaotic oscillatory behavior and resonates in a\nset of natural frequencies. That is, synergy surges and drops are non-random\nand can be analyzed and predicted. There is a proportional dependence between\nthe amplitudes of oscillations and synergy values and an inverse proportional\ndependence between the oscillation frequencies' relative inputs and synergy\nvalues. This analysis of the data informs us that one can expect\nfrequency-related synergy-volatility growth in relation to the synergy value\nand a shift in the synergy volatility towards the long-term fluctuations with\nthe synergy growth.\n"
    },
    {
        "paper_id": 1409.3296,
        "authors": "Stanislao Gualdi, Jean-Philippe Bouchaud, Giulia Cencetti, Marco\n  Tarzia, Francesco Zamponi",
        "title": "Endogenous crisis waves: a stochastic model with synchronized collective\n  behavior",
        "comments": "5 pages, 3 figures. This paper is part of the CRISIS project,\n  http://www.crisis-economics.eu",
        "journal-ref": "Phys. Rev. Lett. 114, 088701 (2015)",
        "doi": "10.1103/PhysRevLett.114.088701",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple framework to understand commonly observed crisis waves in\nmacroeconomic Agent Based models, that is also relevant to a variety of other\nphysical or biological situations where synchronization occurs. We compute\nexactly the phase diagram of the model and the location of the synchronization\ntransition in parameter space. Many modifications and extensions can be\nstudied, confirming that the synchronization transition is extremely robust\nagainst various sources of noise or imperfections.\n"
    },
    {
        "paper_id": 1409.3394,
        "authors": "David Hobson and Yeqi Zhu",
        "title": "Optimal consumption and sale strategies for a risk averse agent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider a special case of an optimal consumption/optimal\nportfolio problem first studied by Constantinides and Magill and by Davis and\nNorman, in which an agent with constant relative risk aversion seeks to\nmaximise expected discounted utility of consumption over the infinite horizon,\nin a model comprising a risk-free asset and a risky asset with proportional\ntransaction costs. The special case that we consider is that the cost of\npurchases of the risky asset is infinite, or equivalently the risky asset can\nonly be sold and not bought.\n  In this special setting new solution techniques are available, and we can\nmake considerable progress towards an analytical solution. This means we are\nable to consider the comparative statics of the problem. There are some\nsurprising conclusions, such as consumption rates are not monotone increasing\nin the return of the asset, nor are the certainty equivalent values of the\nrisky positions monotone in the risk aversion.\n"
    },
    {
        "paper_id": 1409.3738,
        "authors": "Benjamin Vandermarliere, Alexei Karas, Jan Ryckebusch, Koen Schoors",
        "title": "Beyond the Power Law: Uncovering Stylized Facts in Interbank Networks",
        "comments": "17 pages, 9 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications Volume 428,\n  (2015), pp. 443-457",
        "doi": "10.1016/j.physa.2015.01.058",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use daily data on bilateral interbank exposures and monthly bank balance\nsheets to study network characteristics of the Russian interbank market over\nAug 1998 - Oct 2004. Specifically, we examine the distributions of (un)directed\n(un)weighted degree, nodal attributes (bank assets, capital and\ncapital-to-assets ratio) and edge weights (loan size and counterparty\nexposure). We search for the theoretical distribution that fits the data best\nand report the \"best\" fit parameters. We observe that all studied distributions\nare heavy tailed. The fat tail typically contains 20% of the data and can be\nmostly described well by a truncated power law. Also the power law, stretched\nexponential and log-normal provide reasonably good fits to the tails of the\ndata. In most cases, however, separating the bulk and tail parts of the data is\nhard, so we proceed to study the full range of the events. We find that the\nstretched exponential and the log-normal distributions fit the full range of\nthe data best. These conclusions are robust to 1) whether we aggregate the data\nover a week, month, quarter or year; 2) whether we look at the \"growth\" versus\n\"maturity\" phases of interbank market development; and 3) with minor\nexceptions, whether we look at the \"normal\" versus \"crisis\" operation periods.\nIn line with prior research, we find that the network topology changes greatly\nas the interbank market moves from a \"normal\" to a \"crisis\" operation period.\n"
    },
    {
        "paper_id": 1409.3799,
        "authors": "Paolo Sgrignoli",
        "title": "The World Trade Web: A Multiple-Network Perspective",
        "comments": "PhD thesis at IMT Institute for Advanced Studies Lucca. 114 pages, 12\n  figures, 18 tables",
        "journal-ref": null,
        "doi": "10.6092/imtlucca/e-theses/137",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  International Trade (IT) plays a fundamental role in today's economy: by\nconnecting world countries production and consumption processes, it radically\ncontributes in shaping their economy and development path. Although its\nevolving structure and determinants have been widely analyzed in the\nliterature, much less has been done to understand its interplay with other\ncomplex phenomena. The aim of this work is, precisely in this direction, to\nstudy the relations of IT with International Migration (IM) and Foreign Direct\nInvestments (FDI). In both cases the procedure used is to first approach the\nproblem in a multiple-networks perspective and than deepen the analysis by\nusing ad hoc econometrics techniques. With respect to IM, a general positive\ncorrelation with IT is highlighted and product categories for which this effect\nis stronger are identified and cross-checked with previous classifications.\nNext, employing spatial econometric techniques and proposing a new way to\ndefine country neighbors based on the most intense IM flows, direct/indirect\nnetwork effects are studied and a stronger competitive effect of third country\nmigrants is identified for a specific product class. In the case of FDI, first\ncorrelations between the two networks are identified, highlighting how they can\nbe mostly explained by countries economic/demographic size and geographical\ndistance. Then, using the Heckman selection model with a gravity equation,\n(non-linear) components arising from distance, position in the Global Supply\nChain and presence of Regional Trade Agreements are studied. Finally, it is\nshown how IT and FDI correlation changes with sectors: they are complements in\nmanufacturing, but substitutes in services.\n"
    },
    {
        "paper_id": 1409.3837,
        "authors": "Paolo Sgrignoli, Elena Agliari, Raffaella Burioni, Augusto Schianchi",
        "title": "Instability and network effects in innovative markets",
        "comments": "20 pages, 6 figures. 7th workshop on \"Dynamic Models in Economics and\n  Finance\" - MDEF2012 (COST Action IS1104), Urbino (2012)",
        "journal-ref": "Mathematics and Computers in Simulation, Volume 108, February\n  2015, Pages 260-271",
        "doi": "10.1016/j.matcom.2014.05.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a network of interacting agents and we model the process of\nchoice on the adoption of a given innovative product by means of\nstatistical-mechanics tools. The modelization allows us to focus on the effects\nof direct interactions among agents in establishing the success or failure of\nthe product itself. Mimicking real systems, the whole population is divided\ninto two sub-communities called, respectively, Innovators and Followers, where\nthe former are assumed to display more influence power. We study in detail and\nvia numerical simulations on a random graph two different scenarios:\nno-feedback interaction, where innovators are cohesive and not sensitively\naffected by the remaining population, and feedback interaction, where the\ninfluence of followers on innovators is non negligible. The outcomes are\nmarkedly different: in the former case, which corresponds to the creation of a\nniche in the market, Innovators are able to drive and polarize the whole\nmarket. In the latter case the behavior of the market cannot be definitely\npredicted and become unstable. In both cases we highlight the emergence of\ncollective phenomena and we show how the final outcome, in terms of the number\nof buyers, is affected by the concentration of innovators and by the\ninteraction strengths among agents.\n"
    },
    {
        "paper_id": 1409.3969,
        "authors": "Jiacheng Feng",
        "title": "Portfolio Selection with Mandatory Bequest",
        "comments": "This paper contains both exposition and original work related to the\n  optimal consumption-portfolio problem",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, optimal consumption and investment decisions are studied for\nan investor who can invest in a fixed interest rate bank account and a stock\nwhose price is a log normal diffusion. We present the method of the HJB\nequation in order to explicitly solve problems of this type with modifications\nsuch as a fixed percentage transaction cost and a mandatory bequest function.\nIt is shown that the investor treats the mandatory bequest as an expense that\nshe factors into her personal wealth when making consumption and transaction\ndecisions. Furthermore, the investor keeps her portfolio proportions inside a\nfixed boundary relating to Merton's optimal proportion and the transaction\ncosts.\n"
    },
    {
        "paper_id": 1409.3979,
        "authors": "Yong Tao, Xiangjun Wu, Changshuai Li",
        "title": "Rawls' Fairness, Income Distribution and Alarming Level of Gini\n  Coefficient",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The argument that the alarming level of Gini coefficient is 0.4 is very\npopular, especially in the media industry, all around the world for a long\ntime. Although the 0.4 standard is widely accepted, the derivation of the value\nlacks rigid theoretical foundations. In fact, to the best of our knowledge, it\nis not based on any prevalent and convincing economic theories. In this paper,\nwe incorporate Rawls' principle of fair equality of opportunity into\nArrow-Debreu's framework of general equilibrium theory with heterogeneous\nagents, and derive the alarming level of Gini coefficient formally. Our theory\nreveals that the exponential distribution of income not only satisfies Pareto\noptimality, but also obeys social fairness in Rawls' sense. Therefore, we\nspecify the maximal value of the Gini coefficient when income follows\nexponential distribution as a possible alarming level. Our computations show\nthat the alarming level should be specified at least equal or larger than 0.5\nrather than 0.4. We empirically investigate if our model receives support from\na large data set of all kinds of countries all over the world from Word Bank in\n1990, 1995, 2000 and 2005 using the distribution fitting and statistical\ndecision methodology. The results suggest that the value of 0.4 is around the\nmean of the Gini coefficients, corresponding to the most probable event in a\npeaceful world, rather than the alarming level, while the two-sigma rule shows\nthat in our sample the alarming levels are all larger than 0.5, conforming to\nthe predictions of our theory.\n"
    },
    {
        "paper_id": 1409.4387,
        "authors": "Valery Tabakov",
        "title": "Indicators of availability of non-market relations in the sphere of\n  labor market in Ukraine",
        "comments": "16 pages in Ukrainian",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are identified indicators of availability a non-market relations in the\nsphere of labor market in Ukraine. It is concluded that illegal tax money paid\nby legally working in Ukraine, as insurance premiums in the event of\nunemployment. It is concluded that increased pressure from the government on\nlabor market regulators Ukraine established on a parity basis. There are\nformulated recommendations for the implementation of the principle of a free\nmarket economy in the regulation of the labor market of Ukraine.\n"
    },
    {
        "paper_id": 1409.4541,
        "authors": "Peter G. Fennell, David O'Sullivan, Antoine Godin and Stephen Kinsella",
        "title": "Visualising stock flow consistent models as directed acyclic graphs",
        "comments": "11 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how every stock-flow consistent model of the macroeconomy can be\nrepresented as a directed acyclic graph. The advantages of representing the\nmodel in this way include graphical clarity, causal inference, and model\nspecification. We provide many examples implemented with a new software\npackage.\n"
    },
    {
        "paper_id": 1409.4857,
        "authors": "Ricardo P\\'erez-Marco",
        "title": "A simple dynamical model leading to Pareto wealth distribution and\n  stability",
        "comments": "10 pages. Formulas corrected from version 1. Results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple dynamical model of wealth evolution. The invariant\ndistributions are of Pareto type and are dynamically stable as conjectured by\nPareto.\n"
    },
    {
        "paper_id": 1409.489,
        "authors": "Matteo Formenti",
        "title": "Can Market Risk Perception Drive Inefficient Prices? Theory and Evidence",
        "comments": "38 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents an asset pricing model that under rational expectation\nequilibrium perspective shows how, depending on risk aversion and noise\nvolatility, a risky-asset has one equilibrium price that differs in term of\nefficiency: an informational efficient one (similar to Campbell and Kyle\n(1993)), and another one where price diverges from its informational efficient\nlevel. The former Pareto dominates (is dominated by) the latter in presence of\nlow (high) market risk perception. The estimates of the model using S&P 500\nIndex support the theoretical findings, and the estimated inefficient\nequilibrium price captures the higher risk premium and higher volatility\nobserved during the Dot.com bubble 1995--2000.\n"
    },
    {
        "paper_id": 1409.4894,
        "authors": "Matteo Formenti",
        "title": "The Credibility Theory applied to backtesting Counterparty Credit Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credibility theory provides tools to obtain better estimates by combining\nindividual data with sample information. We apply the Credibility theory to a\nUniform distribution that is used in testing the reliability of forecasting an\ninterest rate for long term horizons. Such empirical exercise is asked by\nRegulators (CRR, 2013) in validating an Internal Model Method for Counterparty\nCredit Risk. The main results is that risk managers consider more reliable the\noutput of a test with limited sample size when the Credibility is applied to\ndefine a confidence interval.\n"
    },
    {
        "paper_id": 1409.4896,
        "authors": "Matteo Formenti",
        "title": "Mean of Ratios or Ratio of Means: statistical uncertainty applied to\n  estimate Multiperiod Probability of Defaul",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The estimate of a Multiperiod probability of default applied to residential\nmortgages can be obtained using the mean of the observed default, so called the\nMean of ratios estimator, or aggregating the default and the issued mortgages\nand computing the ratio of their sum, that is the Ratio of means. This work\nstudies the statistical properties of the two estimators with the result that\nthe Ratio of means has a lower statistical uncertainty. The application on a\nprivate residential mortgage portfolio leads to a lower probability of default\non the overall portfolio by eleven basis points.\n"
    },
    {
        "paper_id": 1409.5142,
        "authors": "Jos\\'e Da Fonseca and Claude Martini",
        "title": "The $\\alpha$-Hypergeometric Stochastic Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this work is to introduce a new stochastic volatility model for\nequity derivatives. To overcome some of the well-known problems of the Heston\nmodel, and more generally of the affine models, we define a new specification\nfor the dynamics of the stock and its volatility. Within this framework we\ndevelop all the key elements to perform the pricing of vanilla European options\nas well as of volatility derivatives. We clarify the conditions under which the\nstock price is a martingale and illustrate how the model can be implemented.\n"
    },
    {
        "paper_id": 1409.5321,
        "authors": "Yasemin Hafizogullari, Stanislaus Maier-Paape, Andreas Platen",
        "title": "Empirical Study of the 1-2-3 Trend Indicator",
        "comments": "21 pages, 13 figures, 5 tables; Keywords: market technical trends,\n  automatic trend analysis, wavelength of time series, autocorrelation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study automatically recognized trends and investigate their\nstatistics. To do that we introduce the notion of a wavelength for time series\nvia cross correlation and use this wavelength to calibrate the 1-2-3 trend\nindicator of Maier-Paape [Automatic One Two Three, Quantitative Finance, 2013]\nto automatically find trends. Extensive statistics are reported for EUR-USD,\nDAX-Future, Gold and Crude Oil regarding e.g. the dynamic, duration and\nextension of trends on different time scales.\n"
    },
    {
        "paper_id": 1409.5801,
        "authors": "Fred Espen Benth, Hanna Zdanowicz",
        "title": "Pricing and hedging of energy spread options and volatility modulated\n  Volterra processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive the price of a spread option based on two assets which follow a\nbivariate volatility modulated Volterra process dynamics. Such a price dynamics\nis particularly relevant in energy markets, modelling for example the spot\nprice of power and gas. Volatility modulated Volterra processes are in general\nnot semimartingales, but contain several special cases of interest in energy\nmarkets like for example continuous-time autoregressive moving average\nprocesses. Based on a change of measure, we obtain a pricing expression based\non a univariate Fourier transform of the payoff function and the characteristic\nfunction of the price dynamics. Moreover, the spread option price can be\nexpressed in terms of the forward prices on the underlying dynamics assets. We\ncompute a linear system of equations for the quadratic hedge for the spread\noption in terms of a portfolio of underlying forward contracts.\n"
    },
    {
        "paper_id": 1409.5936,
        "authors": "Steven E. Pav",
        "title": "Bounds on Portfolio Quality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The signal-noise ratio of a portfolio of p assets, its expected return\ndivided by its risk, is couched as an estimation problem on the sphere. When\nthe portfolio is built using noisy data, the expected value of the signal-noise\nratio is bounded from above via a Cramer-Rao bound, for the case of Gaussian\nreturns. The bound holds for `biased' estimators, thus there appears to be no\nbias-variance tradeoff for the problem of maximizing the signal-noise ratio. An\napproximate distribution of the signal-noise ratio for the Markowitz portfolio\nis given, and shown to be fairly accurate via Monte Carlo simulations, for\nGaussian returns as well as more exotic returns distributions. These findings\nimply that if the maximal population signal-noise ratio grows slower than the\nuniverse size to the 1/4 power, there may be no diversification benefit, rather\nexpected signal-noise ratio can decrease with additional assets. As a practical\nmatter, this may explain why the Markowitz portfolio is typically applied to\nsmall asset universes. Finally, the theorem is expanded to cover more general\nmodels of returns and trading schemes, including the conditional expectation\ncase where mean returns are linear in some observable features, subspace\nconstraints (i.e., dimensionality reduction), and hedging constraints.\n"
    },
    {
        "paper_id": 1409.5963,
        "authors": "Mariusz Karpiarz and Piotr Fronczak and Agata Fronczak",
        "title": "International trade network: fractal properties and globalization puzzle",
        "comments": "5 pages, 7 figures",
        "journal-ref": "Phys. Rev. Lett. 113, 248701 (2014)",
        "doi": "10.1103/PhysRevLett.113.248701",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Globalization is one of the central concepts of our age. The common\nperception of the process is that, due to declining communication and transport\ncosts, distance becomes less and less important. However, the distance\ncoefficient in the gravity model of trade, which grows in time, indicates that\nthe role of distance increases rather than decreases. This, in essence,\ncaptures the notion of the globalization puzzle. Here, we show that the\nfractality of the international trade system (ITS) provides a simple solution\nfor the puzzle. We argue, that the distance coefficient corresponds to the\nfractal dimension of ITS. We provide two independent methods, box counting\nmethod and spatial choice model, which confirm this statement. Our results\nallow us to conclude that the previous approaches to solving the puzzle\nmisinterpreted the meaning of the distance coefficient in the gravity model of\ntrade.\n"
    },
    {
        "paper_id": 1409.6027,
        "authors": "Archil Gulisashvili",
        "title": "Distance to the line in the Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main object of study in the paper is the distance from a point to a line\nin the Riemannian manifold associated with the Heston model. We reduce the\nproblem of computing such a distance to certain minimization problems for\nfunctions of one variable over finite intervals. One of the main ideas in this\npaper is to use a new system of coordinates in the Heston manifold and the\nlevel sets associated with this system. In the case of a vertical line, the\nformulas for the distance to the line are rather simple. For slanted lines, the\nformulas are more complicated, and a more subtle analysis of the level sets\nintersecting the given line is needed. We also find simple formulas for the\nHeston distance from a point to a level set. As a natural application, we use\nthe formulas obtained in the present paper to compute the small maturity limit\nof the implied volatility in the correlated Heston model.\n"
    },
    {
        "paper_id": 1409.6042,
        "authors": "Krzysztof Turek",
        "title": "Option pricing in constant elasticity of variance model with liquidity\n  costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Paper is based on \"The cost of illiquidity and its effects on hedging\", L. C.\nG. Rogers and Surbjeet Singh, 2010. We generalize its thesis to constant\nelasticity model, which own previously used Black-Schoels model as a special\ncase. The Goal of this article is to find optimal hedging strategy of European\ncall/put option in illiquid environment. We understand illiquidity as a non\nlinear transaction cost function depending only on rate of change of our\nportfolio. In case this function is quadratic, optimal policy is given by\nsystem of 3 PDE. In addition we show, that for small $\\epsilon$ costs of\nselling portfolio in time $T$ be important ($O(\\epsilon)$) and shouldn't be\nneglected in Value function ($o(\\epsilon^k)$- our result).\n"
    },
    {
        "paper_id": 1409.6093,
        "authors": "Lorenzo Cornalba",
        "title": "Funding Value Adjustment and Incomplete Markets",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value adjustment of uncollateralized trades is determined within a\nrisk-neutral pricing framework. When hedging such trades, investors cannot\nfreely trade protection on their own name, thus facing an incomplete market.\nThis fact is reflected in the non-uniqueness of the pricing measure, which is\nonly constrained by the values of the hedging instruments tradable by the\ninvestor. Uncollateralized trades should then be considered not as derivatives\nbut as new primary assets in the investor's economy. Different choices of the\nrisk-neutral measure correspond to different completions of the market, based\non the risk appetite of the investor, leading to different levels of value\nadjustments. We recover, in limiting cases, results well known in the\nliterature.\n"
    },
    {
        "paper_id": 1409.6193,
        "authors": "Giulio Cimini, Tiziano Squartini, Andrea Gabrielli, Diego Garlaschelli",
        "title": "Estimating topological properties of weighted networks from limited\n  information",
        "comments": null,
        "journal-ref": "Phys. Rev. E 92, 040802 (2015)",
        "doi": "10.1103/PhysRevE.92.040802",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A fundamental problem in studying and modeling economic and financial systems\nis represented by privacy issues, which put severe limitations on the amount of\naccessible information. Here we introduce a novel, highly nontrivial method to\nreconstruct the structural properties of complex weighted networks of this kind\nusing only partial information: the total number of nodes and links, and the\nvalues of the strength for all nodes. The latter are used as fitness to\nestimate the unknown node degrees through a standard configuration model. Then,\nthese estimated degrees and the strengths are used to calibrate an enhanced\nconfiguration model in order to generate ensembles of networks intended to\nrepresent the real system. The method, which is tested on real economic and\nfinancial networks, while drastically reducing the amount of information needed\nto infer network properties, turns out to be remarkably effective$-$thus\nrepresenting a valuable tool for gaining insights on privacy-protected\nsocioeconomic systems.\n"
    },
    {
        "paper_id": 1409.6257,
        "authors": "Paulo Rocha, Frank Raischel, Jo\\~ao Pedro Boto and Pedro G. Lind",
        "title": "Optimal models of extreme volume-prices are time-dependent",
        "comments": "5 pages, 4 figures, for conference paper of IC-MSQUARE, August 2014,\n  Madrid, Spain",
        "journal-ref": null,
        "doi": "10.1088/1742-6596/574/1/012148",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present evidence that the best model for empirical volume-price\ndistributions is not always the same and it strongly depends in (i) the region\nof the volume-price spectrum that one wants to model and (ii) the period in\ntime that is being modelled. To show these two features we analyze stocks of\nthe New York stock market with four different models: Gamma, inverse-gamma,\nlog-normal, and Weibull distributions. To evaluate the accuracy of each model\nwe use standard relative deviations as well as the Kullback-Leibler distance\nand introduce an additional distance particularly suited to evaluate how\naccurate are the models for the distribution tails (large volume-price).\nFinally we put our findings in perspective and discuss how they can be extended\nto other situations in finance engineering.\n"
    },
    {
        "paper_id": 1409.6443,
        "authors": "Paul Gaskell, Frank McGroarty, Thanassis Tiropanis",
        "title": "Signal Diffusion Mapping: Optimal Forecasting with Time Varying Lags",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new methodology for forecasting which we call Signal Diffusion\nMapping. Our approach accommodates features of real world financial data which\nhave been ignored historically in existing forecasting methodologies. Our\nmethod builds upon well-established and accepted methods from other areas of\nstatistical analysis. We develop and adapt those models for use in forecasting.\nWe also present tests of our model on data in which we demonstrate the efficacy\nof our approach.\n"
    }
]