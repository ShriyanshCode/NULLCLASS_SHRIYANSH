[
    {
        "paper_id": 2105.10019,
        "authors": "Daniel Poh, Bryan Lim, Stefan Zohren and Stephen Roberts",
        "title": "Enhancing Cross-Sectional Currency Strategies by Context-Aware Learning\n  to Rank with Self-Attention",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The performance of a cross-sectional currency strategy depends crucially on\naccurately ranking instruments prior to portfolio construction. While this\nranking step is traditionally performed using heuristics, or by sorting the\noutputs produced by pointwise regression or classification techniques,\nstrategies using Learning to Rank algorithms have recently presented themselves\nas competitive and viable alternatives. Although the rankers at the core of\nthese strategies are learned globally and improve ranking accuracy on average,\nthey ignore the differences between the distributions of asset features over\nthe times when the portfolio is rebalanced. This flaw renders them susceptible\nto producing sub-optimal rankings, possibly at important periods when accuracy\nis actually needed the most. For example, this might happen during critical\nrisk-off episodes, which consequently exposes the portfolio to substantial,\nunwanted drawdowns. We tackle this shortcoming with an analogous idea from\ninformation retrieval: that a query's top retrieved documents or the local\nranking context provide vital information about the query's own\ncharacteristics, which can then be used to refine the initial ranked list. In\nthis work, we use a context-aware Learning-to-rank model that is based on the\nTransformer architecture to encode top/bottom ranked assets, learn the context\nand exploit this information to re-rank the initial results. Backtesting on a\nslate of 31 currencies, our proposed methodology increases the Sharpe ratio by\naround 30% and significantly enhances various performance metrics.\nAdditionally, this approach also improves the Sharpe ratio when separately\nconditioning on normal and risk-off market states.\n"
    },
    {
        "paper_id": 2105.10099,
        "authors": "Rui (Aruhan) Shi",
        "title": "Learning from zero: how to make consumption-saving decisions in a\n  stochastic environment with an AI algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This exercise proposes a learning mechanism to model economic agent's\ndecision-making process using an actor-critic structure in the literature of\nartificial intelligence. It is motivated by the psychology literature of\nlearning through reinforcing good or bad decisions. In a model of an\nenvironment, to learn to make decisions, this AI agent needs to interact with\nits environment and make explorative actions. Each action in a given state\nbrings a reward signal to the agent. These interactive experience is saved in\nthe agent's memory, which is then used to update its subjective belief of the\nworld. The agent's decision-making strategy is formed and adjusted based on\nthis evolving subjective belief. This agent does not only take an action that\nit knows would bring a high reward, it also explores other possibilities. This\nis the process of taking explorative actions, and it ensures that the agent\nnotices changes in its environment and adapt its subjective belief and\ndecisions accordingly. Through a model of stochastic optimal growth, I\nillustrate that the economic agent under this proposed learning structure is\nadaptive to changes in an underlying stochastic process of the economy. AI\nagents can differ in their levels of exploration, which leads to different\nexperience in the same environment. This reflects on to their different\nlearning behaviours and welfare obtained. The chosen economic structure\npossesses the fundamental decision making problems of macroeconomic models,\ni.e., how to make consumption-saving decisions in a lifetime, and it can be\ngeneralised to other decision-making processes and economic models.\n"
    },
    {
        "paper_id": 2105.10237,
        "authors": "P. A. Gloor, A. Fronzetti Colladon, F. Grippa, B. M. Hadley, S.\n  Woerner",
        "title": "The impact of social media presence and board member composition on new\n  venture success: Evidences from VC-backed U.S. startups",
        "comments": null,
        "journal-ref": "Technological Forecasting & Social Change 157, 120098 (2020)",
        "doi": "10.1016/j.techfore.2020.120098",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of this study is to examine the impact of board member\ncomposition and board members' social media presence on the performance of\nstartups. Using multiple sources, we compile a unique dataset of about 500\nUS-based technology startups. We find that startups with more venture\ncapitalists on the board and whose board members are active on Twitter attract\nadditional funding over the years, though they do not generate additional\nsales. By contrast, startups which have no venture capitalists on the board and\nwhose board members are not on Twitter show an increased ability to translate\nassets into sales. Consistent with other research, our results indicate that\nstartups potentially benefit from working with VCs because of the opportunity\nto access additional funding, although their presence does not necessarily\ntranslate into sales growth and operational efficiency. We use a number of\ncontrol variables, including board gender representation and board members'\nposition in the interlocking directorates' network.\n"
    },
    {
        "paper_id": 2105.10252,
        "authors": "Andreas Krause",
        "title": "A note on the CAPM with endogenously consistent market returns",
        "comments": "4 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I demonstrate that with the market return determined by the equilibrium\nreturns of the CAPM, expected returns of an asset are affected by the risks of\nall assets jointly. Another implication is that the range of feasible market\nreturns will be limited and dependent on the distribution of weights in the\nmarket portfolio. A large and well diversified market with no dominating asset\nwill only return zero while a market dominated by a small number of assets will\nonly return the risk-free rate. In the limiting case of atomistic assets, we\nrecover the properties of the standard CAPM.\n"
    },
    {
        "paper_id": 2105.10265,
        "authors": "Ahmed Awwad",
        "title": "The impact of Over The Top service providers on the Global Mobile\n  Telecom Industry: A quantified analysis and recommendations for recovery",
        "comments": "28 Pages, 18 figures",
        "journal-ref": "World_Journal_of_Advanced_Research_and_Reviews,2024,21(01),1638-1669",
        "doi": "10.30574/wjarr.2024.21.1.0113",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Telecom industry is significantly evolving all over the globe than ever.\nMobile users number is increasing remarkably. Telecom operators are investing\nto get more users connected and to improve user experience, however, they are\nfacing various challenges. Decrease of main revenue streams of voice calls, SMS\n(Short Message Service) and LDC (Long distance calls) with a significant\nincrease in data traffic. In contrary, with free cost, OTT (Over the top)\nproviders such as WhatsApp and Facebook communication services rendered over\nnetworks that built and owned by MNOs. Recently, OTT services gradually\nsubstituting the traditional MNOs` services and became ubiquitous with the help\nof the underlying data services provided by MNOs. The OTTs` services massive\npenetration into telecom industry is driving the MNOs to reconsider their\nstrategies and revenue sources.\n"
    },
    {
        "paper_id": 2105.10306,
        "authors": "Feng Zhang, Xi Wang and Honggao Cao",
        "title": "Turnover-Adjusted Information Ratio",
        "comments": "16 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study the behavior of information ratio (IR) as determined\nby the fundamental law of active investment management. We extend the classic\nrelationship between IR and its two determinants (i.e., information coefficient\nand investment \"breadth\") by explicitly and simultaneously taking into account\nthe volatility of IC and the cost from portfolio turnover. Through mathematical\nderivations and simulations, we show that - for both mean-variance and quintile\nportfolios - a turnover-adjusted IR is always lower than an IR that ignores the\ncost from turnover; more importantly, we find that, contrary to the implication\nfrom the fundamental low but consistent with available empirical evidence,\ninvestment managers may improve their investment performance or IR by\nlimiting/optimizing trade or portfolio turnover.\n"
    },
    {
        "paper_id": 2105.1043,
        "authors": "Zihao Zhang, Stefan Zohren",
        "title": "Multi-Horizon Forecasting for Limit Order Books: Novel Deep Learning\n  Approaches and Hardware Acceleration using Intelligent Processing Units",
        "comments": "18 pages, 7 figures, and 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We design multi-horizon forecasting models for limit order book (LOB) data by\nusing deep learning techniques. Unlike standard structures where a single\nprediction is made, we adopt encoder-decoder models with sequence-to-sequence\nand Attention mechanisms to generate a forecasting path. Our methods achieve\ncomparable performance to state-of-art algorithms at short prediction horizons.\nImportantly, they outperform when generating predictions over long horizons by\nleveraging the multi-horizon setup. Given that encoder-decoder models rely on\nrecurrent neural layers, they generally suffer from slow training processes. To\nremedy this, we experiment with utilising novel hardware, so-called Intelligent\nProcessing Units (IPUs) produced by Graphcore. IPUs are specifically designed\nfor machine intelligence workload with the aim to speed up the computation\nprocess. We show that in our setup this leads to significantly faster training\ntimes when compared to training models with GPUs.\n"
    },
    {
        "paper_id": 2105.10459,
        "authors": "Jiongyan Zhang",
        "title": "Research on Regional Urban Economic Development by Nightlight-time\n  Remote Sensing",
        "comments": "in Chinese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In order to study the phenomenon of regional economic development and urban\nexpansion from the perspective of night-light remote sensing images,\nresearchers use NOAA-provided night-light remote sensing image data (data from\n1992 to 2013) along with ArcGIS software to process image information, obtain\nthe basic pixel information data of specific areas of the image, and analyze\nthese data from the space-time domain for presentation of the trend of regional\neconomic development in China in recent years, and tries to explore the\nurbanization effect brought by the rapid development of China's economy.\nThrough the analysis and study of the data, the results show that the\nurbanization development speed in China is still at its peak, and has great\ndevelopment potential and space. But at the same time, people also need to pay\nattention to the imbalance of regional development.\n"
    },
    {
        "paper_id": 2105.10464,
        "authors": "David Cerezo S\\'anchez",
        "title": "Pravuil: Global Consensus for a United World",
        "comments": null,
        "journal-ref": "FinTech 2022, 1(4), 325-344",
        "doi": "10.3390/fintech1040025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pravuil is a robust, secure, and scalable consensus protocol for a\npermissionless blockchain suitable for deployment in an adversarial environment\nsuch as the Internet. Pravuil circumvents previous shortcomings of other\nblockchains:\n  - Bitcoin's limited adoption problem: as transaction demand grows, payment\nconfirmation times grow much lower than other PoW blockchains\n  - higher transaction security at a lower cost\n  - more decentralisation than other permissionless blockchains\n  - impossibility of full decentralisation and the blockchain scalability\ntrilemma: decentralisation, scalability, and security can be achieved\nsimultaneously\n  - Sybil-resistance for free implementing the social optimum\n  - Pravuil goes beyond the economic limits of Bitcoin or other PoW/PoS\nblockchains, leading to a more valuable and stable crypto-currency\n"
    },
    {
        "paper_id": 2105.10467,
        "authors": "Haozhe Su, M.V. Tretyakov, David P. Newton",
        "title": "Deep learning of transition probability densities for stochastic asset\n  models with applications in option pricing",
        "comments": "updated version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transition probability density functions (TPDFs) are fundamental to\ncomputational finance, including option pricing and hedging. Advancing recent\nwork in deep learning, we develop novel neural TPDF generators through solving\nbackward Kolmogorov equations in parametric space for cumulative probability\nfunctions. The generators are ultra-fast, very accurate and can be trained for\nany asset model described by stochastic differential equations. These are\n\"single solve\", so they do not require retraining when parameters of the\nstochastic model are changed (e.g. recalibration of volatility). Once trained,\nthe neural TDPF generators can be transferred to less powerful computers where\nthey can be used for e.g. option pricing at speeds as fast as if the TPDF were\nknown in a closed form.\n  We illustrate the computational efficiency of the proposed neural\napproximations of TPDFs by inserting them into numerical option pricing\nmethods. We demonstrate a wide range of applications including the\nBlack-Scholes-Merton model, the standard Heston model, the SABR model, and\njump-diffusion models. These numerical experiments confirm the ultra-fast speed\nand high accuracy of the developed neural TPDF generators.\n"
    },
    {
        "paper_id": 2105.10567,
        "authors": "Scott W. Hegerty",
        "title": "Deprivation, Crime, and Abandonment: Do Other Midwestern Cities Have\n  'Little Detroits'?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Both within the United States and worldwide, the city of Detroit has become\nsynonymous with economic decline, depopulation, and crime. Is Detroit's\nsituation unique, or can similar neighborhoods be found elsewhere? This study\nexamines Census block group data, as well as local crime statistics for 2014,\nfor a set of five Midwestern cities. Roughly three percent of Chicago's and\nMilwaukee's block groups--all of which are in majority nonwhite areas--exceed\nDetroit's median values for certain crimes, vacancies, and a poverty measure.\nThis figure rises to 11 percent for St. Louis, while Minneapolis has only a\nsingle \"Detroit-like\" block group. Detroit's selected areas are more likely to\nbe similar to the entire city itself, both spatially and statistically, while\nthese types of neighborhoods for highly concentrated \"pockets\" of poverty\nelsewhere. Development programs that are targeted in one city, therefore, must\ntake these differences into account and should be targeted to appropriate\nneighborhoods.\n"
    },
    {
        "paper_id": 2105.10599,
        "authors": "Hassane Abba Mallam, Diakarya Barro, Yameogo WendKouni and Bisso Saley",
        "title": "Pricing multivariate european equity option using gaussian mixture\n  distributions and evt-based copulas",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this article, we present an approach which allows to take into account the\neffect of extreme values in the modeling of financial asset returns and in the\nvalorisation of associeted options. Specifically, the marginal distribution of\nassets returns is modeled by a mixture of two gaussiens distributions.\nMoreover, we model the joint dependence structure of the returns using an\nextremal copula which is suitable for our financial data. Applications are made\non the Atos and Dassault Systems actions of the CAC40 index. Monte-Carlo method\nis used to compute the values of some equity options: the call on maximum, the\ncall on minimum, the digital option and the spreads option with the basket\n(Atos, Dassault systems).\n"
    },
    {
        "paper_id": 2105.10623,
        "authors": "Christian Bender, Sebastian Ferrando and Alfredo Gonzalez",
        "title": "Model-Free Finance and Non-Lattice Integration",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Starting solely with a set of possible prices for a traded asset $S$ (in\ninfinite discrete time) expressed in units of a numeraire, we explain how to\nconstruct a Daniell type of integral representing prices of integrable\nfunctions depending on the asset. Such functions include the values of simple\ndynamic portfolios obtained by trading with $S$ and the numeraire. The space of\nelementary integrable functions, i.e. the said portfolio values, is not a\nvector lattice. It then follows that the integral is not classical, i.e. it is\nnot associated to a measure. The essential ingredient in constructing the\nintegral is a weak version of the no-arbitrage condition but here expressed in\nterms of properties of the trajectory space. We also discuss the continuity\nconditions imposed by Leinert (Archiv der Mathematik, 1982) and K\\\"onig\n(Mathematische Annalen, 1982) in the abstract theory of non-lattice integration\nfrom a financial point of view and establish some connections between these\ncontinuity conditions and the existence of martingale measures\n"
    },
    {
        "paper_id": 2105.10865,
        "authors": "Ryan P. Badman, Yunxin Wu, Keigo Inukai, Rei Akaishi",
        "title": "Blessing or Curse of Democracy?: Current Evidence from the Covid-19\n  Pandemic",
        "comments": "21 pages, 2 main figures, 1 main table, 1 supplementary data table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Background: A major question in Covid-19 research is whether democracies\nhandled the Covid-19 pandemic crisis better or worse than authoritarian\ncountries. However, it is important to consider the issues of democracy versus\nauthoritarianism, and state fragility, when examining official Covid-19 death\ncounts in research, because these factors can influence the accurate reporting\nof pandemic deaths by governments. In contrast, excess deaths are less prone to\nvariability in differences in definitions of Covid-19 deaths and testing\ncapacities across countries. Here we use excess pandemic deaths to explore\npotential relationships between political systems and public health outcomes.\n  Methods: We address these issues by comparing the official government\nCovid-19 death counts in a well-established John Hopkins database to the\ngenerally more reliable excess mortality measure of Covid-19 deaths, taken from\nthe recently released World Mortality Dataset. We put the comparison in the\ncontext of the political and fragile state dimensions.\n  Findings: We find (1) significant potential underreporting of Covid-19 deaths\nby authoritarian governments and governments with high state fragility and (2)\nsubstantial geographic variation among countries and regions with regard to\nstandard democracy indices. Additionally, we find that more authoritarian\ngovernments are (weakly) associated with more excess deaths during the pandemic\nthan democratic governments.\n  Interpretations: The inhibition and censorship of information flows, inherent\nto authoritarian states, likely results in major inaccuracies in pandemic\nstatistics that confound global public health analyses. Thus, both excess\npandemic deaths and official Covid-19 death counts should be examined in\nstudies using death as an outcome variable.\n"
    },
    {
        "paper_id": 2105.10866,
        "authors": "Zeinab Rouhollahi",
        "title": "Towards Artificial Intelligence Enabled Financial Crime Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, financial institutes have been dealing with an increase in\nfinancial crimes. In this context, financial services firms started to improve\ntheir vigilance and use new technologies and approaches to identify and predict\nfinancial fraud and crime possibilities. This task is challenging as\ninstitutions need to upgrade their data and analytics capabilities to enable\nnew technologies such as Artificial Intelligence (AI) to predict and detect\nfinancial crimes. In this paper, we put a step towards AI-enabled financial\ncrime detection in general and money laundering detection in particular to\naddress this challenge. We study and analyse the recent works done in financial\ncrime detection and present a novel model to detect money laundering cases with\nminimum human intervention needs.\n"
    },
    {
        "paper_id": 2105.10871,
        "authors": "Tim Leung, Theodore Zhao",
        "title": "Financial Time Series Analysis and Forecasting with HHT Feature\n  Generation and Machine Learning",
        "comments": "28 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:2105.08133",
        "journal-ref": "Appl Stochastic Models Bus Ind. 2021; 1 - 24",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present the method of complementary ensemble empirical mode decomposition\n(CEEMD) and Hilbert-Huang transform (HHT) for analyzing nonstationary financial\ntime series. This noise-assisted approach decomposes any time series into a\nnumber of intrinsic mode functions, along with the corresponding instantaneous\namplitudes and instantaneous frequencies. Different combinations of modes allow\nus to reconstruct the time series using components of different timescales. We\nthen apply Hilbert spectral analysis to define and compute the associated\ninstantaneous energy-frequency spectrum to illustrate the properties of various\ntimescales embedded in the original time series. Using HHT, we generate a\ncollection of new features and integrate them into machine learning models,\nsuch as regression tree ensemble, support vector machine (SVM), and long\nshort-term memory (LSTM) neural network. Using empirical financial data, we\ncompare several HHT-enhanced machine learning models in terms of forecasting\nperformance.\n"
    },
    {
        "paper_id": 2105.10965,
        "authors": "Marina Dias and Demian Pouzo",
        "title": "Inference for multi-valued heterogeneous treatment effects when the\n  number of treated units is small",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a method for conducting asymptotically valid inference for\ntreatment effects in a multi-valued treatment framework where the number of\nunits in the treatment arms can be small and do not grow with the sample size.\nWe accomplish this by casting the model as a semi-/non-parametric conditional\nquantile model and using known finite sample results about the law of the\nindicator function that defines the conditional quantile. Our framework allows\nfor structural functions that are non-additively separable, with flexible\nfunctional forms and heteroskedasticy in the residuals, and it also encompasses\ncommonly used designs like difference in difference. We study the finite sample\nbehavior of our test in a Monte Carlo study and we also apply our results to\nassessing the effect of weather events on GDP growth.\n"
    },
    {
        "paper_id": 2105.10991,
        "authors": "Esther Yusuf Enoch and Abubakar Mahmud Digil and Usman Abubakar Arabo",
        "title": "Evaluating the Effect of Credit Collection Policy on Portfolio Quality\n  of Micro-Finance Bank",
        "comments": "11 pages",
        "journal-ref": "International Journal of Business and Management Invention\n  (IJBMI), vol. 10(05), 2021, pp. 16-26",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study evaluates the effect of collection policy on portfolio quality of\nmicrofinance banks in Adamawa State, Nigeria. Real data were collected from 51\ncredit officers, then a multi-stage sampling method was used to select a sample\nof 21 respondents from the population (i.e., 51 credit officers). In addition,\nwe used regression analysis and descriptive statistics to analyze the data\ncollected and to also test our proposed hypothesis. Based on the evaluation\nperformed, the results showed that collection policy has a higher effect on\nportfolio quality. Hence, the study showed that microfinance banks should\nadhere to strict or stiff debt collection policy as strictness in collection\npolicy help the banks to recover their loans, thereby improving the portfolio\nquality of the bank.\n"
    },
    {
        "paper_id": 2105.10995,
        "authors": "Abubakar Bala, Esther Yusuf Enoch, Salisu Yakubu",
        "title": "The Problems of Personal Income Tax on Revenue Generation in Gombe State",
        "comments": "9 pages",
        "journal-ref": "International Journal of Business and Management Invention (IJBMI)\n  , vol. 07, no. 07, 2018, pp.20-28",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examined the problems of personal income tax on revenue generation\nin Gombe state. The methodology used in data collection is survey, which\nutilized both primary and secondary types of data. Purposive sampling technique\nwas adopted in selecting a sample of 150 respondents from both employees of\nstate board of internal revenue service and tax payers in the state. The chi\nsquare statistics test was used in testing the hypotheses. The study found that\ntax avoidance/evasion and complete absences of information technology are\nserious problems affecting revenue generation in the state. It recommends that\ngovernment should device strict measures in dealing and punishing individuals\nengage in tax avoidance and evasion. It should also employ the use of\ninformation technology as it is the only way problems experience in personal\nincome tax collection can be reduced drastically.\n"
    },
    {
        "paper_id": 2105.11053,
        "authors": "Samuel N. Cohen and Christoph Reisinger and Sheng Wang",
        "title": "Arbitrage-free neural-SDE market models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modelling joint dynamics of liquid vanilla options is crucial for\narbitrage-free pricing of illiquid derivatives and managing risks of option\ntrade books. This paper develops a nonparametric model for the European options\nbook respecting underlying financial constraints and while being practically\nimplementable. We derive a state space for prices which are free from static\n(or model-independent) arbitrage and study the inference problem where a model\nis learnt from discrete time series data of stock and option prices. We use\nneural networks as function approximators for the drift and diffusion of the\nmodelled SDE system, and impose constraints on the neural nets such that\nno-arbitrage conditions are preserved. In particular, we give methods to\ncalibrate \\textit{neural SDE} models which are guaranteed to satisfy a set of\nlinear inequalities. We validate our approach with numerical experiments using\ndata generated from a Heston stochastic local volatility model.\n"
    },
    {
        "paper_id": 2105.11077,
        "authors": "Fen Li, Cunyi Yang, Zhenghui Li, and Pierre Failler",
        "title": "Does Geopolitics Have an Impact on Energy Trade? Empirical Research on\n  Emerging Countries",
        "comments": "23 Pages,16 Tables, 8 Figures",
        "journal-ref": "Sustainability. 2021, 13 (9), 5199",
        "doi": "10.3390/su13095199",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The energy trade is an important pillar of each country's development, making\nup for the imbalance in the production and consumption of fossil fuels.\nGeopolitical risks affect the energy trade of various countries to a certain\nextent, but the causes of geopolitical risks are complex, and energy trade also\ninvolves many aspects, so the impact of geopolitics on energy trade is also\ncomplex. Based on the monthly data from 2000 to 2020 of 17 emerging economies,\nthis paper employs the fixed-effect model and the regression-discontinuity (RD)\nmodel to verify the negative impact of geopolitics on energy trade first and\nthen analyze the mechanism and heterogeneity of the impact. The following\nconclusions are drawn: First, geopolitics has a significant negative impact on\nthe import and export of the energy trade, and the inhibition on the export is\ngreater than that on the import. Second, the impact mechanism of geopolitics on\nthe energy trade is reflected in the lagging effect and mediating effect on the\nimports and exports; that is, the negative impact of geopolitics on energy\ntrade continued to be significant 10 months later. Coal and crude oil prices,\nas mediating variables, decreased to reduce the imports and exports, whereas\nnatural gas prices showed an increase. Third, the impact of geopolitics on\nenergy trade is heterogeneous in terms of national attribute characteristics\nand geo-event types.\n"
    },
    {
        "paper_id": 2105.1108,
        "authors": "Cunyi Yang, Tinghui Li, Khaldoon Albitar",
        "title": "Does energy efficiency affect ambient PM2.5? The moderating role of\n  energy investment",
        "comments": "27 Pages, 11 Tables, 2 Figures",
        "journal-ref": "Frontiers in Environmental Science. 2021, 9",
        "doi": "10.3389/fenvs.2021.707751",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The difficulty of balance between environment and energy consumption makes\ncountries and enterprises face a dilemma, and improving energy efficiency has\nbecome one of the ways to solve this dilemma. Based on data of 158 countries\nfrom 1980 to 2018, the dynamic TFP of different countries is calculated by\nmeans of the Super-SBM-GML model. The TFP is decomposed into indexes of EC\n(Technical Efficiency Change), TC (Technological Change) and EC has been\nextended to PEC (Pure Efficiency Change) and SEC (Scale Efficiency Change).\nThen the fixed effect model and fixed effect panel quantile model are used to\nanalyze the moderating effect and exogenous effect of energy efficiency on\nPM2.5 concentration on the basis of verifying that energy efficiency can reduce\nPM2.5 concentration. We conclude, first, the global energy efficiency has been\ncontinuously improved during the sample period, and both of technological\nprogress and technical efficiency have been improved. Second, the impact of\nenergy efficiency on PM2.5 is heterogeneous which is reflected in the various\nelements of energy efficiency decomposition. The increase of energy efficiency\ncan inhibit PM2.5 concentration and the inhibition effect mainly comes from TC\nand PEC but SEC promotes PM2.5 emission. Third, energy investment plays a\nmoderating role in the environmental protection effect of energy efficiency.\nFourth, the impact of energy efficiency on PM2.5 concentration is heterogeneous\nin terms of national attribute, which is embodied in the differences of\nnational development, science & technology development level, new energy\nutilization ratio and the role of international energy trade.\n"
    },
    {
        "paper_id": 2105.11184,
        "authors": "Esther Enoch Yusuf, Abubakar Bala",
        "title": "Empirical Analysis of Service Quality, Reliability and End-User\n  Satisfaction on Electronic Banking in Nigeria",
        "comments": "7 pages",
        "journal-ref": "IOSR Journal of Business and Management (IOSR-JBM) e-ISSN:\n  2278-487X, p-ISSN: 2319-7668. Volume 17, Issue 10 .Ver. II (Oct. 2015), PP\n  28-34",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Today, almost all banks have adopted ICT as a means of enhancing their\nbanking service quality. These banks provide ICT based electronic service which\nis also called electronic banking, internet banking or online banking etc to\ntheir customers. Despite the increasing adoption of electronic banking and it\nrelevance towards end users satisfaction, few investigations has been conducted\non factors that enhanced end users satisfaction perception. In this research,\nan empirical analysis has been conducted on factors that influence electronic\nbanking user's satisfaction perception and the relationship between these\nfactors and the customer's satisfaction. The study will help bank industries in\nimproving the level of their customer's satisfaction and increase the bond\nbetween a bank and its customer.\n"
    },
    {
        "paper_id": 2105.11195,
        "authors": "Fritz Braeuer, Max Kleinebrahm, Elias Naber, Fabian Scheller, Russell\n  McKenna",
        "title": "Optimal system design for energy communities in multi-family buildings:\n  the case of the German Tenant Electricity Law",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Involving residential actors in the energy transition is crucial for its\nsuccess. Local energy generation, consumption and trading are identified as\ndesirable forms of involvement, especially in energy communities. The\npotentials for energy communities in the residential building stock are high\nbut are largely untapped in multi-family buildings. In many countries, rapidly\nevolving legal frameworks aim at overcoming related barriers, e.g. ownership\nstructures, principal-agent problems and system complexity. But academic\nliterature is scarce regarding the techno-economic and environmental\nimplications of such complex frameworks. This paper develops a mixed-integer\nlinear program (MILP) optimisation model for assessing the implementation of\nmulti-energy systems in an energy community in multi-family buildings with a\nspecial distinction between investor and user. The model is applied to the\nGerman Tenant Electricity Law. Based on hourly demands from appliances, heating\nand electric vehicles, the optimal energy system layout and dispatch are\ndetermined. The results contain a rich set of performance indicators that\ndemonstrate how the legal framework affects the technologies' interdependencies\nand economic viability of multi-energy system energy communities. Certain\neconomic technology combinations may fail to support national emissions\nmitigation goals and lead to lock-ins in Europe's largest residential building\nstock. The subsidies do not lead to the utilisation of a battery storage.\nDespite this, self-sufficiency ratios of more than 90% are observable for\nsystems with combined heat and power plants and heat pumps. Public CO2\nmitigation costs range between 147.5-272.8 EUR/tCO2. Finally, the results show\nthe strong influence of the heat demand on the system layout.\n"
    },
    {
        "paper_id": 2105.11376,
        "authors": "Xin Jin",
        "title": "Can we imitate the principal investor's behavior to learn option price?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a framework of imitating the principal investor's\nbehavior for optimal pricing and hedging options. We construct a\nnon-deterministic Markov decision process for modeling stock price change\ndriven by the principal investor's decision making. However, low\nsignal-to-noise ratio and instability that are inherent in equity markets pose\nchallenges to determine the state transition (stock price change) after\nexecuting an action (the principal investor's decision) as well as decide an\naction based on current state (spot price). In order to conquer these\nchallenges, we resort to a Bayesian deep neural network for computing the\npredictive distribution of the state transition led by an action. Additionally,\ninstead of exploring a state-action relationship to formulate a policy, we seek\nfor an episode based visible-hidden state-action relationship to\nprobabilistically imitate the principal investor's successive decision making.\nUnlike conventional option pricing that employs analytical stochastic processes\nor utilizes time series analysis to model and sample underlying stock price\nmovements, our algorithm simulates stock price paths by imitating the principal\ninvestor's behavior which requires no preset probability distribution and fewer\npredetermined parameters. Eventually the optimal option price is learned by\nreinforcement learning to maximize the cumulative risk-adjusted return of a\ndynamically hedged portfolio over simulated price paths.\n"
    },
    {
        "paper_id": 2105.11405,
        "authors": "C. Seri and A. de Juan Fernandez",
        "title": "The relationship between economic growth and environment. Testing the\n  EKC hypothesis for Latin American countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We employ an ARDL bounds testing approach to cointegration and Unrestricted\nError Correction Models (UECMs) to estimate the relationship between income and\nCO2 emissions per capita in 21 Latin American Countries (LACs) over 1960-2017.\nUsing time series we estimate six different specifications of the model to take\ninto account the independent effect on CO2 emissions per capita of different\nfactors considered as drivers of different dynamics of CO2 emissions along the\ndevelopment path. This approach allows to address two concerns. First, the\nestimation of the model controlling for different variables serves to assess if\nthe EKC hypothesis is supported by evidence in any of the LACs considered and\nto evaluate if this evidence is robust to different model specifications.\nSecond, the inclusion of control variables accounting for the effect on CO2\nemissions is directed at increasing our understanding of CO2 emissions drivers\nin different countries. The EKC hypothesis effectively describes the long term\nincome-emissions relationship only in a minority of LACs and, in many cases,\nthe effect on CO2 emissions of different factors depends on the individual\ncountry experience and on the type and quantity of environmental policies\nadopted. Overall, these results call for increased environmental action in the\nregion.\n"
    },
    {
        "paper_id": 2105.11679,
        "authors": "Dami\\'an H. Zanette and Susanna Manrubia",
        "title": "Fat Tails and Black Swans: Exact Results for Multiplicative Processes\n  with Resets",
        "comments": "10 pages, 6 figures",
        "journal-ref": "Chaos 30, 033104 (2020)",
        "doi": "10.1063/1.5141837",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of multiplicative processes which, added with stochastic\nreset events, give origin to stationary distributions with power-law tails --\nubiquitous in the statistics of social, economic, and ecological systems. Our\nmain goal is to provide a series of exact results on the dynamics and\nasymptotic behaviour of increasingly complex versions of a basic multiplicative\nprocess with resets, including discrete and continuous-time variants and\nseveral degrees of randomness in the parameters that control the process. In\nparticular, we show how the power-law distributions are built up as time\nelapses, how their moments behave with time, and how their stationary profiles\nbecome quantitatively determined by those parameters. Our discussion emphasizes\nthe connection with financial systems, but these stochastic processes are also\nexpected to be fruitful in modeling a wide variety of social and biological\nphenomena.\n"
    },
    {
        "paper_id": 2105.11716,
        "authors": "Manohar Serrao and Aloysius Sequeira and K. V. M. Varambally",
        "title": "Impact of Financial Inclusion on the Socio-Economic Status of Rural and\n  Urban Households of Vulnerable Sections in Karnataka",
        "comments": "28 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial inclusion and inclusive growth are the buzzwords today. Inclusive\ngrowth empowers people belonging to vulnerable sections. This in turn depends\nupon a variety of factors, the most important being financial inclusion, which\nplays a strategic role in promoting inclusive growth and helps in reducing\npoverty by providing regular and reliable sources of finance to the vulnerable\nsections. In this direction, the Government of India in its drive for financial\ninclusion has taken several measures to increase the access to and availing of\nformal financial services by unbanked households. The purpose of this paper is\nto assess the nature and extent of financial inclusion and its impact on the\nsocio-economic status of households belonging to vulnerable sections focusing\non inclusive growth. This has been analyzed with the theoretical background on\nfinancial access and economic growth, and by analyzing the primary data\ncollected from the Revenue Divisions of Karnataka. The results show that there\nis a disparity in nature and extent of financial inclusion. Access to, availing\nof formal banking services pave the way to positive changes in the\nsocio-economic status of households belonging to vulnerable sections which are\ncorrelated, leading to inclusive growth based on which the paper proposes a\nmodel to make the financial system more inclusive and pro-poor.\n"
    },
    {
        "paper_id": 2105.11756,
        "authors": "Elvan Ece Satici and Bayram Cakir",
        "title": "Environmental Kuznets Curve & Effectiveness of International Policies:\n  Evidence from Cross Country Carbon Emission Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, we are presenting the relationship between environmental\npollution and the income level of the selected twenty-four countries. We\nimplemented a data-based research analysis where, for each country, we analyzed\nthe related data for fifty-six years, from 1960 to 2016, to assess the\nrelationship between the carbon emission and income level. After performing the\nrelated data analysis for each country, we concluded whether the results for\nthat country were in line with the Environmental Kuznets Curve (EKC)\nhypothesis. The EKC hypothesis suggests that the carbon emission per capita\nstarts a declining trend when the country-specific high level of income is\nreached. The results of our data analyses show that the EKC hypothesis is valid\nfor high-income countries and the declining trends of carbon emission are\nclearly observed when the income level reaches a specific high enough level. On\nthe other hand, for the non-high income countries, our analysis results show\nthat it is too early to make an assessment at this growth stage of their\neconomies because they have not reached their related high-enough income per\ncapita levels yet. Furthermore, we performed two more additional analyses on\nhigh-income countries. First, we analyzed the related starting years of their\ncarbon emission declining trends. The big variance in the starting years of the\ncarbon emission declining trends shows that the international policies are\nclearly ineffective in initiating the declining trend in carbon emission. In\naddition, for the high-income countries, we explained the differences in their\ncarbon emission per capita levels in 2014 with their SGI indices and their\ndependence on high-carbon emission energy production.\n"
    },
    {
        "paper_id": 2105.11829,
        "authors": "F. Javier Sanchez-Vidal and Camino Ramon-Llorens",
        "title": "Perception of corruption influences entrepreneurship inside established\n  companies",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the Global Entrepreneurship Monitor (GEM) surveys and conducting a\npanel data estimation to test our hypothesis, this paper examines whether\ncorruption perceptions might sand or grease the wheels for entrepreneurship\ninside companies or intrapreneurship in a sample of 92 countries for the period\n2012 to 2019. Our results find that the corruption perception sands the wheel\nfor intrapreneurship. There is evidence of a quadratic relation, but this\nrelation is only clear for the less developed countries, which sort of moderate\nthe very negative effect of corruption for these countries. The results also\nconfirm that corruption influences differently on intrapreneurship depending on\nthe level of development of the country.\n"
    },
    {
        "paper_id": 2105.1184,
        "authors": "Berno Buechel, Selina Gangl, Martin Huber",
        "title": "How residence permits affect the labor market attachment of foreign\n  workers: Evidence from a migration lottery in Liechtenstein",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze the impact of obtaining a residence permit on foreign workers'\nlabor market and residential attachment. To overcome the usually severe\nselection issues, we exploit a unique migration lottery that randomly assigns\naccess to otherwise restricted residence permits in Liechtenstein (situated\nbetween Austria and Switzerland). Using an instrumental variable approach, our\nresults show that lottery compliers (whose migration behavior complies with the\nassignment in their first lottery) raise their employment probability in\nLiechtenstein by on average 24 percentage points across outcome periods (2008\nto 2018) as a result of receiving a permit. Relatedly, their activity level and\nemployment duration in Liechtenstein increase by on average 20 percentage\npoints and 1.15 years, respectively, over the outcome window. These substantial\nand statistically significant effects are mainly driven by individuals not\n(yet) working in Liechtenstein prior to the lottery rather than by previous\ncross-border commuters. Moreover, we find both the labor market and residential\neffects to be persistent even several years after the lottery with no sign of\nfading out. These results suggest that granting resident permits to foreign\nworkers can be effective to foster labor supply even beyond the effect of\ncross-border commuting from adjacent regions.\n"
    },
    {
        "paper_id": 2105.12044,
        "authors": "Ariel Ortiz-Bobea",
        "title": "Climate, Agriculture and Food",
        "comments": "100 pages, 14 figures, handbook chapter",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Agriculture is arguably the most climate-sensitive sector of the economy.\nGrowing concerns about anthropogenic climate change have increased research\ninterest in assessing its potential impact on the sector and in identifying\npolicies and adaptation strategies to help the sector cope with a changing\nclimate. This chapter provides an overview of recent advancements in the\nanalysis of climate change impacts and adaptation in agriculture with an\nemphasis on methods. The chapter provides an overview of recent research\nefforts addressing key conceptual and empirical challenges. The chapter also\ndiscusses practical matters about conducting research in this area and provides\nreproducible R code to perform common tasks of data preparation and model\nestimation in this literature. The chapter provides a hands-on introduction to\nnew researchers in this area.\n"
    },
    {
        "paper_id": 2105.12072,
        "authors": "Christian Bender, Sebastian E. Ferrando and Alfredo L. Gonzalez",
        "title": "Conditional Non-Lattice Integration, Pricing and Superhedging",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Closely motivated by financial considerations, we develop an integration\ntheory which is not classical i.e. it is not necessarily associated to a\nmeasure. The base space, denoted by $\\mathcal{S}$ and called a trajectory\nspace, substitutes the set $\\Omega$ in probability theory and provides a\nfundamental structure via conditional subsets $\\mathcal{S}_{(S,j)}$ that allows\nthe definition of conditional integrals. The setting is a natural by-product of\nno arbitrage assumptions that are used to model financial markets and games of\nchance (in a discrete infinite time framework). The constructed conditional\nintegrals can be interpreted as required investments, at the conditioning node,\nfor hedging an integrable function, the latter characterized a.e. and in the\nlimit as we increase the number of portfolios used. The integral is not\nclassical due to the fact that the original vector space of portfolio payoffs\nis not a vector lattice. In contrast to a classical stochastic setting, where\nprice processes are associated to conditional expectations (with respect to\nrisk neutral measures), we uncover a theory where prices are naturally given by\nconditional non-lattice integrals. One could then study analogues of classical\nprobabilistic notions in such non-classical setting, the paper stops after\ndefining trajectorial martingales the study of which is deferred to future\nwork.\n"
    },
    {
        "paper_id": 2105.12087,
        "authors": "Javier Alcazar, Andrea Cadarso, Amara Katabarwa, Marta Mauri, Borja\n  Peropadre, Guoming Wang, Yudong Cao",
        "title": "Quantum algorithm for credit valuation adjustments",
        "comments": "23 pages, 16 figures",
        "journal-ref": null,
        "doi": "10.1088/1367-2630/ac5003",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum mechanics is well known to accelerate statistical sampling processes\nover classical techniques. In quantitative finance, statistical samplings arise\nbroadly in many use cases. Here we focus on a particular one of such use cases,\ncredit valuation adjustment (CVA), and identify opportunities and challenges\ntowards quantum advantage for practical instances. To improve the depths of\nquantum circuits for solving such problem, we draw on various heuristics that\nindicate the potential for significant improvement over well-known techniques\nsuch as reversible logical circuit synthesis. In minimizing the resource\nrequirements for amplitude amplification while maximizing the speedup gained\nfrom the quantum coherence of a noisy device, we adopt a recently developed\nBayesian variant of quantum amplitude estimation using engineered likelihood\nfunctions (ELF). We perform numerical analyses to characterize the prospect of\nquantum speedup in concrete CVA instances over classical Monte Carlo\nsimulations.\n"
    },
    {
        "paper_id": 2105.12267,
        "authors": "Maximilian Vierlboeck, Roshanak Rose Nilchiani",
        "title": "Effects of COVID-19 Vaccine Developments and Rollout on the Capital\n  Market -- A Case Study",
        "comments": "12 pages, 8 figures, 4 tables, full reference list",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Various companies have developed vaccines to combat the pandemic caused 2020\nby the virus COVID-19. Such vaccines and the distribution can have a major\nimpact on the success of pharmaceutical companies, which in turn can show\nitself in their valuation and stock price. This poses the question if and how\nthe trends or popularity of the companies might be connected to the value and\nstock price of said entities. To gain some insight into these questions, the\nwork at hand looks at five COVID vaccine development companies and evaluates\ntheir correlations over the development of the vaccine as well as after the\nrollout start. The process was conducted by using python including various\nlibraries. The result of this analysis was that there is a significant\ncorrelation between the Google Trend data and the respective stock prices\n(retrieved from yahoo! Finance) of the companies on average, where the time\nduring the development of the drugs is more positively correlated and the\npost-rollout periods show a shift to a slightly negative inclining correlation.\nFurthermore, it was found that the smaller companies based on their market cap\nshow a higher price volatility overall. In addition, higher average trend\nscores and thus popularity values were found after the rollout of the\nrespective companies. In conclusion, a correlations between the trend data and\nthe financial values have been found and corroborate the plots of the data. Due\nto the small size of the sample, the result cannot yet be considered\nstatistically significant, but possibility for expansion exists and is already\nbeing worked on.\n"
    },
    {
        "paper_id": 2105.12292,
        "authors": "Eiji Yamamura",
        "title": "The Effect of Providing Peer Information on Evaluation for Gender\n  Equalized and ESG Oriented Firms: An Internet Survey Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Internet survey experiment is conducted to examine how providing peer\ninformation of evaluation about progressive firms changed individual's\nevaluations. Using large sample including over 13,000 observations collected by\ntwo-step experimental surveys, I found; (1) provision of the information leads\nindividuals to expect higher probability of rising of stocks and be more\nwilling to buy it. (2) the effect on willingness to buy is larger than the\nexpected probability of stock price rising, (3) The effect for woman is larger\nthan for man. (4) individuals who prefer environment (woman's empowerment)\nbecome more willing to buy stock of pro-environment (gender-balanced) firms\nthan others if they have the information. (5) The effect of the peer\ninformation is larger for individuals with \"warm -glow\" motivation.\n"
    },
    {
        "paper_id": 2105.12293,
        "authors": "Yong Shi, Wei Dai, Wen Long, Bo Li",
        "title": "Deep Kernel Gaussian Process Based Financial Market Predictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Gaussian Process with a deep kernel is an extension of the classic GP\nregression model and this extended model usually constructs a new kernel\nfunction by deploying deep learning techniques like long short-term memory\nnetworks. A Gaussian Process with the kernel learned by LSTM, abbreviated as\nGP-LSTM, has the advantage of capturing the complex dependency of financial\nsequential data, while retaining the ability of probabilistic inference.\nHowever, the deep kernel Gaussian Process has not been applied to forecast the\nconditional returns and volatility in financial market to the best of our\nknowledge. In this paper, grid search algorithm, used for performing\nhyper-parameter optimization, is integrated with GP-LSTM to predict both the\nconditional mean and volatility of stock returns, which are then combined\ntogether to calculate the conditional Sharpe Ratio for constructing a\nlong-short portfolio. The experiments are performed on a dataset covering all\nconstituents of Shenzhen Stock Exchange Component Index. Based on empirical\nresults, we find that the GP-LSTM model can provide more accurate forecasts in\nstock returns and volatility, which are jointly evaluated by the performance of\nconstructed portfolios. Further sub-period analysis of the experiment results\nindicates that the superiority of GP-LSTM model over the benchmark models stems\nfrom better performance in highly volatile periods.\n"
    },
    {
        "paper_id": 2105.12304,
        "authors": "Rahul Deb and Anne-Katrin Roesler",
        "title": "Multi-Dimensional Screening: Buyer-Optimal Learning and Informational\n  Robustness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A monopolist seller of multiple goods screens a buyer whose type is initially\nunknown to both but drawn from a commonly known distribution. The buyer\nprivately learns about his type via a signal. We derive the seller's optimal\nmechanism in two different information environments. We begin by deriving the\nbuyer-optimal outcome. Here, an information designer first selects a signal,\nand then the seller chooses an optimal mechanism in response; the designer's\nobjective is to maximize consumer surplus. Then, we derive the optimal\ninformationally robust mechanism. In this case, the seller first chooses the\nmechanism, and then nature picks the signal that minimizes the seller's\nprofits. We derive the relation between both problems and show that the optimal\nmechanism in both cases takes the form of pure bundling.\n"
    },
    {
        "paper_id": 2105.12334,
        "authors": "Christoph J. B\\\"orner, Ingo Hoffmann, Jonas Krettek, Lars M.\n  K\\\"urzinger, Tim Schmitz",
        "title": "On the Return Distributions of a Basket of Cryptocurrencies and\n  Subsequent Implications",
        "comments": "33 pages, 3 Figures, 11 Tables JEL Classification: C12, C13, C43, E22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper evaluates and assesses the risk associated with capital allocation\nin cryptocurrencies (CCs). In this regard, we take a basket of 27 CCs and the\nCC index EWCI$^-$ into account. After considering a series of statistical tests\nwe find the stable distribution (SDI) to be the most appropriate to model the\nbody of CCs returns. However, as we find the SDI to possess less favorable\nproperties in the tail area for high quantiles, the generalized Pareto\ndistribution is adapted for a more precise risk assessment. We use a\ncombination of both distributions to calculate the Value at Risk and the\nConditional Value at Risk, indicating two subgroups of CCs with differing risk\ncharacteristics.\n"
    },
    {
        "paper_id": 2105.12336,
        "authors": "Christoph J. B\\\"orner, Ingo Hoffmann, Jonas Krettek, Lars M.\n  K\\\"urzinger, Tim Schmitz",
        "title": "Bitcoin: Like a Satellite or Always Hardcore? A Core-Satellite\n  Identification in the Cryptocurrency Market",
        "comments": "16 Pages, 2 Figures, 3 Tables, JEL Classification: C14, C46, C55,\n  E22, G10",
        "journal-ref": null,
        "doi": "10.1057/s41260-022-00267-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies (CCs) become more interesting for institutional investors'\nstrategic asset allocation and will be a fixed component of professional\nportfolios in future. This asset class differs from established assets\nespecially in terms of the severe manifestation of statistical parameters. The\nquestion arises whether CCs with similar statistical key figures exist. On this\nbasis, a core market incorporating CCs with comparable properties enables the\nimplementation of a tracking error approach. A prerequisite for this is the\nsegmentation of the CC market into a core and a satellite, the latter\ncomprising the accumulation of the residual CCs remaining in the complement.\nUsing a concrete example, we segment the CC market into these components, based\non modern methods from image / pattern recognition.\n"
    },
    {
        "paper_id": 2105.12432,
        "authors": "Patrick Cheridito, John Ery and Mario V. W\\\"uthrich",
        "title": "Assessing asset-liability risk with neural networks",
        "comments": null,
        "journal-ref": "Risks 2020, 8, 16",
        "doi": "10.3390/risks8010016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a neural network approach for assessing the risk of a portfolio\nof assets and liabilities over a given time period. This requires a conditional\nvaluation of the portfolio given the state of the world at a later time, a\nproblem that is particularly challenging if the portfolio contains structured\nproducts or complex insurance contracts which do not admit closed form\nvaluation formulas. We illustrate the method on different examples from banking\nand insurance. We focus on value-at-risk and expected shortfall, but the\napproach also works for other risk measures.\n"
    },
    {
        "paper_id": 2105.12469,
        "authors": "Patrick Reinwald, Stephan Leitner and Friederike Wall",
        "title": "Effects of limited and heterogeneous memory in hidden-action situations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Limited memory of decision-makers is often neglected in economic models,\nalthough it is reasonable to assume that it significantly influences the\nmodels' outcomes. The hidden-action model introduced by Holmstr\\\"om also\nincludes this assumption. In delegation relationships between a principal and\nan agent, this model provides the optimal sharing rule for the outcome that\noptimizes both parties' utilities. This paper introduces an agent-based model\nof the hidden-action problem that includes limitations in the cognitive\ncapacity of contracting parties. Our analysis mainly focuses on the sensitivity\nof the principal's and the agent's utilities to the relaxed assumptions. The\nresults indicate that the agent's utility drops with limitations in the\nprincipal's cognitive capacity. Also, we find that the agent's cognitive\ncapacity limitations affect neither his nor the principal's utility. Thus, the\nagent bears all adverse effects resulting from limitations in cognitive\ncapacity.\n"
    },
    {
        "paper_id": 2105.12699,
        "authors": "Andreea Avramescu, Richard Allmendinger, Manuel L\\'opez-Ib\\'a\\~nez",
        "title": "Managing Manufacturing and Delivery of Personalised Medicine: Current\n  and Future Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With almost 50% of annual commercial drug approvals being Personalised\nMedicine (PM) and its huge potential to improve quality of life, this emerging\nmedical sector has received increased attention from the industry and medical\nresearch, driven by health and care services, and us, the patients.\nNotwithstanding the power of Advanced Therapy Medicinal Products (ATMPs) to\ntreat progressive illnesses and rare genetic conditions, their delivery on\nlarge scale is still problematic. The biopharmaceutical companies are currently\nstruggling to meet timely delivery and, given high prices of up to $2 million\nper patient, prove the cost-effectiveness of their ATMP. The fragility of ATMPs\ncombined with the impossibility for replacements due to the nature of the\ntreatment and the advanced stages of the patient's condition are some of the\nbottlenecks added to a generally critical supply chain. As a consequence, ATMPs\nare currently used in most cases only as a last resort. ATMPs are at the\nintersection of multiple healthcare logistic networks and, due to their\nnovelty, research around their commercialisation is still in its infancy from\nan operations research perspective. To accelerate technology adoption in this\ndomain, we characterize pertinent practical challenges in a PM supply chain and\nthen capture them in a holistic mathematical model ready for optimisation. The\nidentified challenges and derived model will be contrasted with literature of\nrelated supply chains in terms of model formulations and suitable optimisation\nmethods. Finally, needed technological advancements are discussed to pave the\nway to affordable commercialisation of PM.\n"
    },
    {
        "paper_id": 2105.12825,
        "authors": "Zhihan Zhou, Liqian Ma, Han Liu",
        "title": "Trade the Event: Corporate Events Detection for News-Based Event-Driven\n  Trading",
        "comments": "Accepted to publish in Findings of ACL 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce an event-driven trading strategy that predicts\nstock movements by detecting corporate events from news articles. Unlike\nexisting models that utilize textual features (e.g., bag-of-words) and\nsentiments to directly make stock predictions, we consider corporate events as\nthe driving force behind stock movements and aim to profit from the temporary\nstock mispricing that may occur when corporate events take place. The core of\nthe proposed strategy is a bi-level event detection model. The low-level event\ndetector identifies events' existences from each token, while the high-level\nevent detector incorporates the entire article's representation and the\nlow-level detected results to discover events at the article-level. We also\ndevelop an elaborately-annotated dataset EDT for corporate event detection and\nnews-based stock prediction benchmark. EDT includes 9721 news articles with\ntoken-level event labels as well as 303893 news articles with minute-level\ntimestamps and comprehensive stock price labels. Experiments on EDT indicate\nthat the proposed strategy outperforms all the baselines in winning rate,\nexcess returns over the market, and the average return on each transaction.\n"
    },
    {
        "paper_id": 2105.12878,
        "authors": "Sajad Rahimian",
        "title": "Corruption Determinants, Geography, and Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper aims to identify the robust determinants of corruption after\nintegrating out the effects of spatial spillovers in corruption levels between\ncountries. In other words, we want to specify which variables play the most\ncritical role in determining the corruption levels after accounting for the\neffects that neighbouring countries have on each other. We collected the annual\ndata of 115 countries over the 1985-2015 period and used the averaged values to\nconduct our empirical analysis. Among 39 predictors of corruption, our spatial\nBMA models identify Rule of Law as the most persistent determinant of\ncorruption.\n"
    },
    {
        "paper_id": 2105.12915,
        "authors": "Xi Zhi Lim",
        "title": "Ordered Reference Dependent Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies how violations of structural assumptions like expected\nutility and exponential discounting can be connected to basic rationality\nviolations, even though these assumptions are typically regarded as independent\nbuilding blocks in decision theory. A reference-dependent generalization of\nbehavioral postulates captures preference shifts in various choice domains.\nWhen reference points are fixed, canonical models hold; otherwise,\nreference-dependent preference parameters (e.g., CARA coefficients, discount\nfactors) give rise to \"non-standard\" behavior. The framework allows us to study\nrisk, time, and social preferences collectively, where seemingly independent\nanomalies are interconnected through the lens of reference-dependent choice.\n"
    },
    {
        "paper_id": 2105.13126,
        "authors": "Thomas Orton",
        "title": "An Introduction To Regret Minimization In Algorithmic Trading: A Survey\n  of Universal Portfolio Techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial investing, universal portfolios are a means of constructing\nportfolios which guarantee a certain level of performance relative to a\nbaseline, while making no statistical assumptions about the future market data.\nThey fall under the broad category of regret minimization algorithms. This\ndocument covers an introduction and survey to universal portfolio techniques,\ncovering some of the basic concepts and proofs in the area. Topics include:\nConstant Rebalanced Portfolios, Cover's Algorithm, Incorporating Transaction\nCosts, Efficient Computation of Portfolios, Including Side Information, and\nFollow The Leader Algorithm.\n"
    },
    {
        "paper_id": 2105.1332,
        "authors": "Timothy DeLise",
        "title": "Neural Options Pricing",
        "comments": "9 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research investigates pricing financial options based on the traditional\nmartingale theory of arbitrage pricing applied to neural SDEs. We treat neural\nSDEs as universal It\\^o process approximators. In this way we can lift all\nassumptions on the form of the underlying price process, and compute\ntheoretical option prices numerically. We propose a variation of the SDE-GAN\napproach by implementing the Wasserstein distance metric as a loss function for\ntraining. Furthermore, it is conjectured that the error of the option price\nimplied by the learnt model can be bounded by the very Wasserstein distance\nmetric that was used to fit the empirical data.\n"
    },
    {
        "paper_id": 2105.13401,
        "authors": "Xiao Chen, Jin Hyuk Choi, Kasper Larsen, Duane J. Seppi",
        "title": "Learning about latent dynamic trading demand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an equilibrium model of dynamic trading, learning, and\npricing by strategic investors with trading targets and price impact. Since\ntrading targets are private, rebalancers and liquidity providers filter the\nchild order flow over time to estimate the latent underlying parent trading\ndemand imbalance and its expected impact on subsequent price pressure dynamics.\nWe prove existence of the equilibrium and solve for equilibrium trading\nstrategies and prices in terms of the solution to a system of coupled ODEs. We\nshow that trading strategies are combinations of trading towards investor\ntargets, liquidity provision for other investors' demands, and front-running\nbased on learning about latent underlying trading demand imbalances and future\nprice pressure.\n"
    },
    {
        "paper_id": 2105.1351,
        "authors": "Robin Fritsch and Roger Wattenhofer",
        "title": "A Note on Optimal Fees for Constant Function Market Makers",
        "comments": "11 pages, 5 figures",
        "journal-ref": "Proceedings of the 2021 ACM CCS Workshop on Decentralized Finance\n  and Security (2021) 9-14",
        "doi": "10.1145/3464967.3488589",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest a framework to determine optimal trading fees for constant\nfunction market makers (CFMMs) in order to maximize liquidity provider returns.\nIn a setting of multiple competing liquidity pools, we show that no race to the\nbottom occurs, but instead pure Nash equilibria of optimal fees exist. We\ntheoretically prove the existence of these equilibria for pools using the\nconstant product trade function used in popular CFMMs like Uniswap. We also\nnumerically compute the equilibria for a number of examples and discuss the\neffects the equilibrium fees have on capital allocation among pools. Finally,\nwe use our framework to compute optimal fees for real world pools using past\ntrade data.\n"
    },
    {
        "paper_id": 2105.13556,
        "authors": "Carlos Carrion, Zenan Wang, Harikesh Nair, Xianghong Luo, Yulin Lei,\n  Xiliang Lin, Wenlong Chen, Qiyu Hu, Changping Peng, Yongjun Bao and Weipeng\n  Yan",
        "title": "Blending Advertising with Organic Content in E-Commerce: A Virtual Bids\n  Optimization Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In e-commerce platforms, sponsored and non-sponsored content are jointly\ndisplayed to users and both may interactively influence their engagement\nbehavior. The former content helps advertisers achieve their marketing goals\nand provides a stream of ad revenue to the platform. The latter content\ncontributes to users' engagement with the platform, which is key to its\nlong-term health. A burning issue for e-commerce platform design is how to\nblend advertising with content in a way that respects these interactions and\nbalances these multiple business objectives. This paper describes a system\ndeveloped for this purpose in the context of blending personalized sponsored\ncontent with non-sponsored content on the product detail pages of JD.COM, an\ne-commerce company. This system has three key features: (1) Optimization of\nmultiple competing business objectives through a new virtual bids approach and\nthe expressiveness of the latent, implicit valuation of the platform for the\nmultiple objectives via these virtual bids. (2) Modeling of users' click\nbehavior as a function of their characteristics, the individual characteristics\nof each sponsored content and the influence exerted by other sponsored and\nnon-sponsored content displayed alongside through a deep learning approach; (3)\nConsideration of externalities in the allocation of ads, thereby making it\ndirectly compatible with a Vickrey-Clarke-Groves (VCG) auction scheme for the\ncomputation of payments in the presence of these externalities. The system is\ncurrently deployed and serving all traffic through JD.COM's mobile application.\nExperiments demonstrating the performance and advantages of the system are\npresented.\n"
    },
    {
        "paper_id": 2105.13652,
        "authors": "Adam Sulich, Malgorzata Rutkowska, Agnieszka Krawczyk-Jezierska,\n  Jaroslaw Jezierski, Tomasz Zema",
        "title": "Cybersecurity and Sustainable Development",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.16633.60001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Growing interdependencies between organizations lead them towards the\ncreation of inter-organizational networks where cybersecurity and sustainable\ndevelopment have become one of the most important issues. The Environmental\nGoods and Services Sector (EGSS) is one of the fastest developing sectors of\nthe economy fueled by the growing relationships between network entities based\non ICT usage. In this sector, Green Cybersecurity is an emerging issue because\nit secures processes related directly and indirectly to environmental\nmanagement and protection. In the future, the multidimensional development of\nthe EGSS can help European Union to overcome the upcoming crises. At the same\ntime, computer technologies and cybersecurity can contribute to the\nimplementation of the concept of sustainable development. The development of\nenvironmental technologies along with their cybersecurity is one of the aims of\nthe realization of sustainable production and domestic security concepts among\nthe EU countries. Hence, the aim of this article is a theoretical discussion\nand research on the relationships between cybersecurity and sustainable\ndevelopment in inter-organizational networks. Therefore, the article is an\nattempt to give an answer to the question about the current state of the\nimplementation of cybersecurity in relation to the EGSS part of the economy in\ndifferent EU countries.\n"
    },
    {
        "paper_id": 2105.13727,
        "authors": "Kieran Wood, Stephen Roberts, Stefan Zohren",
        "title": "Slow Momentum with Fast Reversion: A Trading Strategy Using Deep\n  Learning and Changepoint Detection",
        "comments": "minor changes made to methodology to match implementation",
        "journal-ref": "The Journal of Financial Data Science Winter 2022, jfds.2021.1.081",
        "doi": "10.3905/jfds.2021.1.081",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Momentum strategies are an important part of alternative investments and are\nat the heart of commodity trading advisors (CTAs). These strategies have,\nhowever, been found to have difficulties adjusting to rapid changes in market\nconditions, such as during the 2020 market crash. In particular, immediately\nafter momentum turning points, where a trend reverses from an uptrend\n(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies\nare prone to making bad bets. To improve the response to regime change, we\nintroduce a novel approach, where we insert an online changepoint detection\n(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which\nuses an LSTM deep-learning architecture to simultaneously learn both trend\nestimation and position sizing. Furthermore, our model is able to optimise the\nway in which it balances 1) a slow momentum strategy which exploits persisting\ntrends, but does not overreact to localised price moves, and 2) a fast\nmean-reversion strategy regime by quickly flipping its position, then swapping\nit back again to exploit localised price moves. Our CPD module outputs a\nchangepoint location and severity score, allowing our model to learn to respond\nto varying degrees of disequilibrium, or smaller and more localised\nchangepoints, in a data driven manner. Back-testing our model over the period\n1995-2020, the addition of the CPD module leads to an improvement in Sharpe\nratio of one-third. The module is especially beneficial in periods of\nsignificant nonstationarity, and in particular, over the most recent years\ntested (2015-2020) the performance boost is approximately two-thirds. This is\ninteresting as traditional momentum strategies have been underperforming in\nthis period.\n"
    },
    {
        "paper_id": 2105.13822,
        "authors": "Lioba Heimbach, Ye Wang, Roger Wattenhofer",
        "title": "Behavior of Liquidity Providers in Decentralized Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Decentralized exchanges (DEXes) have introduced an innovative trading\nmechanism, where it is not necessary to match buy-orders and sell-orders to\nexecute a trade. DEXes execute each trade individually, and the exchange rate\nis automatically determined by the ratio of assets reserved in the market.\nTherefore, apart from trading, financial players can also liquidity providers,\nbenefiting from transaction fees from trades executed in DEXes. Although\nliquidity providers are essential for the functionality of DEXes, it is not\nclear how liquidity providers behave in such markets. In this paper, we aim to\nunderstand how liquidity providers react to market information and how they\nbenefit from providing liquidity in DEXes. We measure the operations of\nliquidity providers on Uniswap and analyze how they determine their investment\nstrategy based on market changes. We also reveal their returns and risks of\ninvestments in different trading pair categories, i.e., stable pairs, normal\npairs, and exotic pairs. Further, we investigate the movement of liquidity\nbetween trading pools. To the best of our knowledge, this is the first work\nthat systematically studies the behavior of liquidity providers in DEXes.\n"
    },
    {
        "paper_id": 2105.13891,
        "authors": "Simon Cousaert, Jiahua Xu, Toshiko Matsui",
        "title": "SoK: Yield Aggregators in DeFi",
        "comments": null,
        "journal-ref": "2022 IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC)",
        "doi": "10.1109/ICBC54727.2022.9805523",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Yield farming has been an immensely popular activity for cryptocurrency\nholders since the explosion of Decentralized Finance (DeFi) in the summer of\n2020. In this Systematization of Knowledge (SoK), we study a general framework\nfor yield farming strategies with empirical analysis. First, we summarize the\nfundamentals of yield farming by focusing on the protocols and tokens used by\naggregators. We then examine the sources of yield and translate those into\nthree example yield farming strategies, followed by the simulations of yield\nfarming performance, based on these strategies. We further compare four major\nyield aggregrators -- Idle, Pickle, Harvest and Yearn -- in the ecosystem,\nalong with brief introductions of others. We systematize their strategies and\nrevenue models, and conduct an empirical analysis with on-chain data from\nexample vaults, to find a plausible connection between data anomalies and\nhistorical events. Finally, we discuss the benefits and risks of yield\naggregators.\n"
    },
    {
        "paper_id": 2105.13898,
        "authors": "Jaydip Sen, Sidra Mehtab, Abhishek Dutta",
        "title": "Volatility Modeling of Stocks from Selected Sectors of the Indian\n  Economy Using GARCH",
        "comments": "This paper is the accepted version of our paper in the IEEE Asian\n  Conference on Innovation Technology (IEEE ASIANCON'2021) which will be\n  organized in Pune, INDIA during August 28 - 29, 2021. The paper consists of 8\n  pages and it contains 13 figures and 22 tables",
        "journal-ref": null,
        "doi": "10.1109/ASIANCON51346.2021.9544977",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility clustering is an important characteristic that has a significant\neffect on the behavior of stock markets. However, designing robust models for\naccurate prediction of future volatilities of stock prices is a very\nchallenging research problem. We present several volatility models based on\ngeneralized autoregressive conditional heteroscedasticity (GARCH) framework for\nmodeling the volatility of ten stocks listed in the national stock exchange\n(NSE) of India. The stocks are selected from the auto sector and the banking\nsector of the Indian economy, and they have a significant impact on the\nsectoral index of their respective sectors in the NSE. The historical stock\nprice records from Jan 1, 2010, to Apr 30, 2021, are scraped from the Yahoo\nFinance website using the DataReader API of the Pandas module in the Python\nprogramming language. The GARCH modules are built and fine-tuned on the\ntraining data and then tested on the out-of-sample data to evaluate the\nperformance of the models. The analysis of the results shows that asymmetric\nGARCH models yield more accurate forecasts on the future volatility of stocks.\n"
    },
    {
        "paper_id": 2105.13903,
        "authors": "Victor Olkhov",
        "title": "Three Remarks On Asset Pricing",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the consumption-based asset pricing model, derive a new modified\nbasic pricing equation, and present its successive approximations using the\nTaylor series expansions of the investor's utility during the averaging time\ninterval. For linear and quadratic Taylor approximations, we derive new\nexpressions for the mean price, mean payoff, volatility, skewness, and the\nasset's amount that define the maximum of the investor's utility. We discuss\nthe market-based origin of price probability. We use volume weighted average\nprice (VWAP) as a market-based average price and introduce market-based price\nvolatility. The use of VWAP results in zero correlations between the price p\nand trade volume U. We derive a correlation between price p and squares of\ntrade volume {U^2} and between squares of price {p^2} and volume {U^2}. To\npredict market-based price volatility, one should forecast the 2-d statistical\nmoments of the market trade values and volumes at the same horizon T.\n"
    },
    {
        "paper_id": 2105.14193,
        "authors": "Laurence F Lacey",
        "title": "Characterization of the probability and information entropy of a process\n  with an exponentially increasing sample space and its application to the\n  Broad Money Supply",
        "comments": "26 pages, 17 figures, 1 appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There is a random variable (X) with a determined outcome (i.e., X = x0),\np(x0) = 1. Consider x0 to have a discrete uniform distribution over the integer\ninterval [1, s], where the size of the sample space (s) = 1, in the initial\nstate, such that p(x0) = 1. What is the probability of x0 and the associated\ninformation entropy (H), as s increases exponentially? If the sample space\nexpansion occurs at an exponential rate (rate constant = lambda) with time (t)\nand applying time scaling, such that T = lambda x t, gives: p(x0|T)=exp(-T) and\nH(T)=T. The characterization has also been extended to include exponential\nexpansion by means of simultaneous, independent processes, as well as the more\ngeneral multi-exponential case. The methodology was applied to the expansion of\nthe broad money supply of US$ over the period 2001-2019, as a real-world\nexample. At any given time, the information entropy is related to the rate at\nwhich the sample space is expanding. In the context of the expansion of the\nbroad money supply, the information entropy could be considered to be related\nto the \"velocity\" of the expansion of the money supply.\n"
    },
    {
        "paper_id": 2105.14194,
        "authors": "Nikolay Ivanov and Qiben Yan",
        "title": "Constraint-Based Inference of Heuristics for Foreign Exchange Trade\n  Model Optimization",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Foreign Exchange (Forex) is a large decentralized market, on which\ntrading analysis and algorithmic trading are popular. Research efforts have\nbeen focusing on proof of efficiency of certain technical indicators. We\ndemonstrate, however, that the values of indicator functions are not\nreproducible and often reduce the number of trade opportunities, compared to\nprice-action trading.\n  In this work, we develop two dataset-agnostic Forex trading heuristic\ntemplates with high rate of trading signals. In order to determine most optimal\nparameters for the given heuristic prototypes, we perform a machine learning\nsimulation of 10 years of Forex price data over three low-margin instruments\nand 6 different OHLC granularities. As a result, we develop a specific and\nreproducible list of most optimal trade parameters found for each\ninstrument-granularity pair, with 118 pips of average daily profit for the\noptimized configuration.\n"
    },
    {
        "paper_id": 2105.14198,
        "authors": "Md Saimum Hossain, Faruque Ahamed",
        "title": "Comprehensive Analysis On Determinants Of Bank Profitability In\n  Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study investigates the relationship between bank profitability and a\ncomprehensive list of bank specific, industry specific and macroeconomic\nvariables using unique panel data from 23 Bangladeshi banks with large market\nshares from 2005 to 2019 employing the Pooled Ordinary Least Square (POLS)\nMethod for regression estimation. The random Effect model has been used to\ncheck for robustness. Three variables, namely, Return on Asset (ROA), Return on\nEquity (ROE), and Net Interest Margin (NIM), have been used as profitability\nproxies. Non-interest income, capital ratio, and GDP growth have been found to\nhave a significant relationship with ROA. In addition to non-interest income,\nmarket share, bank size, and real exchange rates are significant explaining\nvariables if profitability is measured as NIM. The only significant determinant\nof profitability measured by ROE is market share. The primary contribution of\nthis study to the existing knowledge base is an extensive empirical analysis by\ncovering the entire gamut of independent variables (bank specific, industry\nrelated, and macroeconomic) to explain the profitability of the banks in\nBangladesh. It also covers an extensive and recent data set. Banking sector\nstakeholders may find great value from the outputs of this paper. Regulators\nand policymakers may find this useful in undertaking analyses in setting policy\nrates, banking industry stability, and impact assessment of critical policy\nmeasures before and after the enactment, etc. Investors and the bank management\nare to use the findings of this paper in analyzing the real drivers of\nprofitability of the banks they are contemplating to invest and managing on a\ndaily basis.\n"
    },
    {
        "paper_id": 2105.14199,
        "authors": "Faruque Ahamed",
        "title": "Impact of Public and Private Investments on Economic Growth of\n  Developing Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper aims to study the impact of public and private investments on the\neconomic growth of developing countries. The study uses the panel data of 39\ndeveloping countries covering the periods 1990-2019. The study was based on the\nneoclassical growth models or exogenous growth models state in which land,\nlabor, capital accumulation, etc., and technology proved substantial for\neconomic growth. The paper finds that public investment has a strong positive\nimpact on economic growth than private investment. Gross capital formation,\nlabor growth, and government final consumption expenditure were found\nsignificant in explaining the economic growth. Overall, both public and private\ninvestments are substantial for the economic growth and development of\ndeveloping countries.\n"
    },
    {
        "paper_id": 2105.14318,
        "authors": "Da Zhang, Qingyi Wang, Shaojie Song, Simiao Chen, Mingwei Li, Lu Shen,\n  Siqi Zheng, Bofeng Cai, Shenhao Wang",
        "title": "Estimating air quality co-benefits of energy transition using machine\n  learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating health benefits of reducing fossil fuel use from improved air\nquality provides important rationales for carbon emissions abatement.\nSimulating pollution concentration is a crucial step of the estimation, but\ntraditional approaches often rely on complicated chemical transport models that\nrequire extensive expertise and computational resources. In this study, we\ndevelop a novel and succinct machine learning framework that is able to provide\nprecise and robust annual average fine particle (PM2.5) concentration\nestimations directly from a high-resolution fossil energy use data set. The\naccessibility and applicability of this framework show great potentials of\nmachine learning approaches for integrated assessment studies. Applications of\nthe framework with Chinese data reveal highly heterogeneous health benefits of\nreducing fossil fuel use in different sectors and regions in China with a mean\nof \\$34/tCO2 and a standard deviation of \\$84/tCO2. Reducing rural and\nresidential coal use offers the highest co-benefits with a mean of \\$360/tCO2.\nOur findings prompt careful policy designs to maximize cost-effectiveness in\nthe transition towards a carbon-neutral energy system.\n"
    },
    {
        "paper_id": 2105.14325,
        "authors": "Paolo Bartesaghi, Gian Paolo Clemente and Rosanna Grassi",
        "title": "A tensor-based unified approach for clustering coefficients in financial\n  multiplex networks",
        "comments": null,
        "journal-ref": "Information Sciences (2022)",
        "doi": "10.1016/j.ins.2022.04.021",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Big data and the use of advanced technologies are relevant topics in the\nfinancial market. In this context, complex networks became extremely useful in\ndescribing the structure of complex financial systems. In particular, the time\nevolution property of the stock markets have been described by temporal\nnetworks. However, these approaches fail to consider the interactions over time\nbetween assets. To overcome this drawback, financial markets can be described\nby multiplex networks where the different relations between nodes can be\nconveniently expressed structuring the network through different layers. To\ncatch this kind of interconnections we provide new local clustering\ncoefficients for multiplex networks, looking at the network from different\nperspectives depending on the node position, as well as a global clustering\ncoefficient for the whole network. We also prove that all the well-known\nexpressions for clustering coefficients existing in the literature, suitably\nextended to the multiplex framework, may be unified into our proposal. By means\nof an application to the multiplex temporal financial network, based on the\nreturns of the S\\&P100 assets, we show that the proposed measures prove to be\neffective in describing dependencies between assets over time.\n"
    },
    {
        "paper_id": 2105.14587,
        "authors": "Mikhail Kunavin, Tatiana Kozitsina (Babkina), Mikhail Myagkov, Irina\n  Kozhevnikova, Mikhail Pankov, Ludmila Sokolova",
        "title": "Bioelectrical brain activity can predict prosocial behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generally, people behave in social dilemmas such as proself and prosocial.\nHowever, inside social groups, people have a tendency to choose prosocial\nalternatives due to in-group favoritism. The bioelectrical activity of the\nhuman brain shows the differences between proself and prosocial exist even out\nof a socialized group. Moreover, a group socialization strengthens these\ndifferences. We used EEG System, \"Neuron-Spectrum-4/EPM\" (16 channels), to\ntrack the brain bioelectrical activity during decision making in laboratory\nexperiments with the Prisoner's dilemma game and the short-term socialization\nstage. We compared the spatial distribution of the spectral density during the\ndifferent experimental parts. The noncooperative decision was characterized by\nthe increased values of spectral the beta rhythm in the orbital regions of\nprefrontal cortex. The cooperative choice, on the contrary, was accompanied by\nthe theta-rhythm activation in the central cortex regions in both hemispheres\nand the high-frequency alpha rhythm in the medial regions of the prefrontal\ncortex. People who increased the cooperation level after the socialization\nstage was initially different from the ones who decreased the cooperation in\nterms of the bioelectrical activity. Well-socialized participants differed by\nincreased values of spectral density of theta-diapason and decreased values of\nspectral density of beta-diapason in the middle part of frontal lobe. People\nwho decreased the cooperation level after the socialization stage was\ncharacterized by decreased values of spectral density of alpha rhythm in the\nmiddle and posterior convex regions of both hemispheres.\n"
    },
    {
        "paper_id": 2105.14959,
        "authors": "Dragan Tevdovski, Petar Jolakoski and Viktor Stojkoski",
        "title": "Determinants of budget deficits: Focus on the effects from the COVID-19\n  crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper revisits the discussion on determinants of budget balances and\ninvestigates the change in their effect in light of the COVID-19 crisis by\nutilizing data on 43 countries and a system generalized method of moments\napproach. The results show that the overall impact of the global pandemic led\nto a disproportionate increase in the magnitude of the estimated effects of the\nmacroeconomic determinants on the budget balance. However, we also find that\nmore developed economies were able to undertake higher stimulus packages for\nthe relatively same level of primary balance. We believe that one of the\nfactors affecting this outcome is that that they exhibit a higher government\ndebt position in domestic currency denomination.\n"
    },
    {
        "paper_id": 2105.14996,
        "authors": "Valdemar J. Wesz Junior, Simone Piras, Catia Grisa, Stefano Ghinoi",
        "title": "Assessing Brazilian agri-food policies: what impact on family farms?",
        "comments": "1 figure, 5 tables, supplementary material available",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Since the beginning of the 1990s, Brazil has introduced different policies\nfor increasing agricultural production among family farms, such as the National\nProgram for Strengthening Family Farming (Pronaf), the technical assistance and\nrural extension programmes (ATER), and seeds distribution. Despite the\nimportance of these policies for the development of family farming, there is a\nlack of empirical studies investigating their impact on commercialization of\nfood products. By considering household-level data from the 2014 Brazilian\nNational Household Sample Survey, we use propensity score matching techniques\naccounting for the interaction effects between policies to compare the\ncommercialisation behaviour of recipients with non recipients. We find that\nPronaf has a significant positive impact on family farmers propensity to engage\nin commercialisation, and this effect increases if farmers have also access to\nATER. Receiving technical assistance alone has a positive effect, but this is\nmostly limited to smaller farms. In turn, seed distribution appears not to\nincrease commercialization significantly. A well balanced policy mix could\nensure that, in a country subject to the pressure of international food\nmarkets, increased commercialisation does not result in reduced food security\nfor rural dwellers.\n"
    },
    {
        "paper_id": 2105.15008,
        "authors": "Hangsuck Lee, Gaeun Lee and Seongjoo Song",
        "title": "Multi-step Reflection Principle and Barrier Options",
        "comments": "31 pages, submitted to Journal of Futures Markets",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines a class of barrier options-multi-step barrier options,\nwhich can have any finite number of barriers of any level. We obtain a general,\nexplicit expression of option prices of this type under the Black-Scholes\nmodel. Multi-step barrier options are not only useful in that they can handle\nbarriers of different levels and time steps, but can also approximate options\nwith arbitrary barriers. Moreover, they can be embedded in financial products\nsuch as deposit insurances based on jump models with simple barriers. Along the\nway, we derive multi-step reflection principle, which generalizes the\nreflection principle of Brownian motion.\n"
    },
    {
        "paper_id": 2105.15175,
        "authors": "Mikhail Freer and Cesar Martinelli",
        "title": "An algebraic approach to revealed preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and develop an algebraic approach to revealed preference. Our\napproach dispenses with non algebraic structure, such as topological\nassumptions. We provide algebraic axioms of revealed preference that subsume\nprevious, classical revealed preference axioms, as well as generate new axioms\nfor behavioral theories, and show that a data set is rationalizable if and only\nif it is consistent with an algebraic axiom.\n"
    },
    {
        "paper_id": 2106.00123,
        "authors": "Tidor-Vlad Pricope",
        "title": "Deep Reinforcement Learning in Quantitative Algorithmic Trading: A\n  Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Algorithmic stock trading has become a staple in today's financial market,\nthe majority of trades being now fully automated. Deep Reinforcement Learning\n(DRL) agents proved to be to a force to be reckon with in many complex games\nlike Chess and Go. We can look at the stock market historical price series and\nmovements as a complex imperfect information environment in which we try to\nmaximize return - profit and minimize risk. This paper reviews the progress\nmade so far with deep reinforcement learning in the subdomain of AI in finance,\nmore precisely, automated low-frequency quantitative stock trading. Many of the\nreviewed studies had only proof-of-concept ideals with experiments conducted in\nunrealistic settings and no real-time trading applications. For the majority of\nthe works, despite all showing statistically significant improvements in\nperformance compared to established baseline strategies, no decent\nprofitability level was obtained. Furthermore, there is a lack of experimental\ntesting in real-time, online trading platforms and a lack of meaningful\ncomparisons between agents built on different types of DRL or human traders. We\nconclude that DRL in stock trading has showed huge applicability potential\nrivalling professional traders under strong assumptions, but the research is\nstill in the very early stages of development.\n"
    },
    {
        "paper_id": 2106.00125,
        "authors": "Steven Diamond, Stephen Boyd, David Greenberg, Mykel Kochenderfer,\n  Andrew Ang",
        "title": "Optimal Claiming of Social Security Benefits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a lifecycle framework with Epstein-Zin (1989) utility and a\nmixed-integer optimization approach, we compute the optimal age to claim Social\nSecurity benefits. Taking advantage of homogeneity, a sufficient statistic is\nthe ratio of wealth to the primary insurance amount (PIA). If the investor's\nwealth to PIA ratio exceeds a certain threshold, individuals should defer\nSocial Security for at least a year. The optimal threshold depends on mortality\nassumptions and an individual's utility preferences, but is less sensitive to\ncapital market assumptions. The threshold wealth to PIA ratio increases from\n5.5 for men and 5.2 for women at age 62 to 11.1 for men and 10.4 for women at\nage 69. Below the threshold wealth to PIA ratio, individuals claim Social\nSecurity to raise consumption. Above this level, investors can afford to fund\nconsumption out of wealth for at least one year, and then claim a higher\nbenefit.\n"
    },
    {
        "paper_id": 2106.00213,
        "authors": "Craig McIntosh and Andrew Zeitlin",
        "title": "Cash versus Kind: Benchmarking a Child Nutrition Program against\n  Unconditional Cash Transfers in Rwanda",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We benchmark a multi-dimensional child nutrition intervention against an\nunconditional cash transfer of equal cost. Randomized variation in transfer\namounts allows us to estimate impacts of cash transfers at expenditure levels\nequivalent to the in-kind program, as well as to estimate the return to\nincreasing cash transfer values. While neither the in-kind program nor a\ncost-equivalent transfer costing \\$124 per household moves core child outcomes\nwithin a year, cash transfers create significantly greater consumption than the\nin-kind alternative. A larger cash transfer costing \\$517 substantially\nimproves consumption and investment outcomes and drives modest improvements in\ndietary diversity and child growth.\n"
    },
    {
        "paper_id": 2106.00288,
        "authors": "Chao Wang, Richard Gerlach",
        "title": "A Bayesian realized threshold measurement GARCH framework for financial\n  tail risk forecasting",
        "comments": "28 pages, 6 Tables, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes an innovative threshold measurement equation to be\nemployed in a Realized-GARCH framework. The proposed framework incorporates a\nnonlinear threshold regression specification to consider the leverage effect\nand model the contemporaneous dependence between the observed realized measure\nand hidden volatility. A Bayesian Markov Chain Monte Carlo method is adapted\nand employed for model estimation, with its validity assessed via a simulation\nstudy. The validity of incorporating the proposed measurement equation in\nRealized-GARCH type models is evaluated via an empirical study, forecasting the\n1% and 2.5% Value-at-Risk and Expected Shortfall on six market indices with two\ndifferent out-of-sample sizes. The proposed framework is shown to be capable of\nproducing competitive tail risk forecasting results in comparison to the GARCH\nand Realized-GARCH type models.\n"
    },
    {
        "paper_id": 2106.00348,
        "authors": "Lindgren Erik, Per Pettersson-Lidbom and Bjorn Tyrefors",
        "title": "The Causal Effect of Transport Infrastructure: Evidence from a New\n  Historical Database",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we analyze the effect of transport infrastructure investments\nin railways. As a testing ground, we use data from a new historical database\nthat includes annual panel data on approximately 2,400 Swedish rural\ngeographical areas during the period 1860-1917. We use a staggered event study\ndesign that is robust to treatment effect heterogeneity. Importantly, we find\nextremely large reduced-form effects of having access to railways. For real\nnonagricultural income, the cumulative treatment effect is approximately 120%\nafter 30 years. Equally important, we also show that our reduced-form effect is\nlikely to reflect growth rather than a reorganization of existing economic\nactivity since we find no spillover effects between treated and untreated\nregions. Specifically, our results are consistent with the big push hypothesis,\nwhich argues that simultaneous/coordinated investment, such as large\ninfrastructure investment in railways, can generate economic growth if there\nare strong aggregate demand externalities (e.g., Murphy et al. 1989). We used\nplant-level data to further corroborate this mechanism. Indeed, we find that\ninvestments in local railways dramatically, and independent of initial\nconditions, increase local industrial production and employment on the order of\n100-300% across almost all industrial sectors.\n"
    },
    {
        "paper_id": 2106.0035,
        "authors": "Lindgren Erik, Per Pettersson-Lidbom and Bjorn Tyrefors",
        "title": "The causal effect of political power on the provision of public\n  education: Evidence from a weighted voting system",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we estimate the causal effect of political power on the\nprovision of public education. We use data from a historical nondemocratic\nsociety with a weighted voting system where eligible voters received votes in\nproportion to their taxable income and without any limit on the maximum of\nvotes, i.e., the political system used in Swedish local governments during the\nperiod 1862-1909. We use a novel identification strategy where we combine two\ndifferent identification strategies, i.e., a threshold regression analysis and\na generalized event-study design, both of which exploit nonlinearities or\ndiscontinuities in the effect of political power between two opposing local\nelites: agricultural landowners and emerging industrialists. The results\nsuggest that school spending is approximately 90-120% higher if the nonagrarian\ninterest controls all of the votes compared to when landowners have more than a\nmajority of votes. Moreover, we find no evidence that the concentration of\nlandownership affected this relationship\n"
    },
    {
        "paper_id": 2106.0046,
        "authors": "Michele Vespe, Umberto Minora, Stefano Maria Iacus, Spyridon Spyratos,\n  Francesco Sermi, Matteo Fontana, Biagio Ciuffo, Panayotis Christidis",
        "title": "Mobility and Economic Impact of COVID-19 Restrictions in Italy using\n  Mobile Network Operator Data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2760/241286",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work presents the analysis of the impact of restrictions on mobility in\nItaly, with a focus on the period from 6 November 2020 to 31 January 2021, when\na three-tier system based on different levels of risk was adopted and applied\nat regional level to contrast the second wave of COVID-19. The impact is first\nevaluated on mobility using Mobile Network Operator anonymised and aggregate\ndata shared in the framework of a Business-to-Government initiative with the\nEuropean Commission. Mobility data, alongside additional information about\nelectricity consuption, are then used to assess the impacts on an economic\nlevel of the three-tier system in different areas of the country.\n"
    },
    {
        "paper_id": 2106.00464,
        "authors": "Malgorzata Rutkowska, Adam Sulich",
        "title": "The Green Management Towards a Green Industrial Revolution",
        "comments": "arXiv admin note: text overlap with arXiv:2105.13652",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.10997.50401",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Green Management (GM) is now one of many methods proposed to achieve new,\nmore ecological, and sustainable economic models. The paper is focused on the\nimpact of the developing human population on the environment measured by\nresearched variables. Anthropopressure can have both a positive and a negative\ndimension. This paper aims to present an econometric model of the Green\nIndustrial Revolution (GIR) impact on the Labour Market. The GIR is similar to\nthe Fourth Industrial Revolution (FIR) and takes place as the next stage in the\ndevelopment of humanity in the perception of both machines and devices and the\nnatural environment. The processes of the GIR in the European Union can be\nidentified based on selected indicators of Sustainable Development (SD), in\nparticular with the use of indicators of the Green Economy (GE) using taxonomic\nmethods and regression analysis. The GM strives to implement the idea of the SD\nin many areas, to transform the whole economy, and elements of this process are\nvisible Green Labour Market (GLM). The adopted direction of economic\ndevelopment depends on the as-sumptions of strategic management, which can be\ndefined, for example, with green management, which is mainly manifested in the\ncreation of green jobs.\n"
    },
    {
        "paper_id": 2106.00465,
        "authors": "Adam Sulich, Malgorzata Rutkowska, Uma Shankar Singh",
        "title": "Decision Towards Green Careers and Sustainable Development",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.14326.73280/1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The graduates careers are the most spectacular and visible outcome of\nexcellent university education. This is also important for the university\nperformance assessment when its graduates can easily find jobs in the labor\nmarket. The information about graduates matching their qualifications and\nfields of studies versus undertaken employment, creates an important set of\ndata for future students and employers to be analyzed. Additionally, there is\nbusiness environment pressure to transform workplaces and whole organizations\ntowards a more green and sustainable form. Green Jobs (GJ) are the elements of\nthe whole economic transformation. This change is based on the green\nqualifications and green careers which translate theoretical assumptions into\nbusiness language. Therefore, the choice of future career path is based on\nspecified criteria, which were examined by surveys performed among graduates by\nthe career office at Wroclaw University of Technology (WUT) in Poland. The aim\nof this article was to address the question about the most significant criteria\nof green career paths among graduates of WUT in 2019. Special attention was\npaid to the GJ understood as green careers. In this article, the multi-criteria\nBellinger method was explained, presented, and then used to analyze chosen\nfactors of choice graduates career paths and then compared with Gale-Shapley\nalgorithm results in a comparative analysis. Future research can develop a\ngraduate profile willing to be employed in GJ.\n"
    },
    {
        "paper_id": 2106.00581,
        "authors": "Ruimeng Hu, Thaleia Zariphopoulou",
        "title": "$N$-player and Mean-field Games in It\\^{o}-diffusion Markets with\n  Competitive or Homophilous Interaction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In It\\^{o}-diffusion environments, we introduce and analyze $N$-player and\ncommon-noise mean-field games in the context of optimal portfolio choice in a\ncommon market. The players invest in a finite horizon and also interact, driven\neither by competition or homophily. We study an incomplete market model in\nwhich the players have constant individual risk tolerance coefficients (CARA\nutilities). We also consider the general case of random individual risk\ntolerances and analyze the related games in a complete market setting. This\nrandomness makes the problem substantially more complex as it leads to ($N$ or\na continuum of) auxiliary ''individual'' It\\^{o}-diffusion markets. For all\ncases, we derive explicit or closed-form solutions for the equilibrium\nstochastic processes, the optimal state processes, and the values of the games.\n"
    },
    {
        "paper_id": 2106.00647,
        "authors": "Matthieu Nadini, Laura Alessandretti, Flavio Di Giacinto, Mauro\n  Martino, Luca Maria Aiello, Andrea Baronchelli",
        "title": "Mapping the NFT revolution: market trends, trade networks and visual\n  features",
        "comments": "Working paper, comments welcome",
        "journal-ref": "Scientific Reports 11, 20902 (2021)",
        "doi": "10.1038/s41598-021-00053-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Non Fungible Tokens (NFTs) are digital assets that represent objects like\nart, collectible, and in-game items. They are traded online, often with\ncryptocurrency, and are generally encoded within smart contracts on a\nblockchain. Public attention towards NFTs has exploded in 2021, when their\nmarket has experienced record sales, but little is known about the overall\nstructure and evolution of its market. Here, we analyse data concerning 6.1\nmillion trades of 4.7 million NFTs between June 23, 2017 and April 27, 2021,\nobtained primarily from Ethereum and WAX blockchains. First, we characterize\nstatistical properties of the market. Second, we build the network of\ninteractions, show that traders typically specialize on NFTs associated with\nsimilar objects and form tight clusters with other traders that exchange the\nsame kind of objects. Third, we cluster objects associated to NFTs according to\ntheir visual features and show that collections contain visually homogeneous\nobjects. Finally, we investigate the predictability of NFT sales using simple\nmachine learning algorithms and find that sale history and, secondarily, visual\nfeatures are good predictors for price. We anticipate that these findings will\nstimulate further research on NFT production, adoption, and trading in\ndifferent contexts.\n"
    },
    {
        "paper_id": 2106.00788,
        "authors": "Jason Poulos, Andrea Albanese, Andrea Mercatanti, Fan Li",
        "title": "Retrospective causal inference via matrix completion, with an evaluation\n  of the effect of European integration on cross-border employment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method of retrospective counterfactual imputation in panel data\nsettings with later-treated and always-treated units, but no never-treated\nunits. We use the observed outcomes to impute the counterfactual outcomes of\nthe later-treated using a matrix completion estimator. We propose a novel\npropensity-score and elapsed-time weighting of the estimator's objective\nfunction to correct for differences in the observed covariate and unobserved\nfixed effects distributions, and elapsed time since treatment between groups.\nOur methodology is motivated by studying the effect of two milestones of\nEuropean integration -- the Free Movement of persons and the Schengen Agreement\n-- on the share of cross-border workers in sending border regions. We apply the\nproposed method to the European Labour Force Survey (ELFS) data and provide\nevidence that opening the border almost doubled the probability of working\nbeyond the border in Eastern European regions.\n"
    },
    {
        "paper_id": 2106.00839,
        "authors": "Dimitris Bertsimas, Agni Orfanoudaki",
        "title": "Algorithmic Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As machine learning algorithms start to get integrated into the\ndecision-making process of companies and organizations, insurance products are\nbeing developed to protect their owners from liability risk. Algorithmic\nliability differs from human liability since it is based on a single model\ncompared to multiple heterogeneous decision-makers and its performance is known\na priori for a given set of data. Traditional actuarial tools for human\nliability do not take these properties into consideration, primarily focusing\non the distribution of historical claims. We propose, for the first time, a\nquantitative framework to estimate the risk exposure of insurance contracts for\nmachine-driven liability, introducing the concept of algorithmic insurance.\nSpecifically, we present an optimization formulation to estimate the risk\nexposure of a binary classification model given a pre-defined range of\npremiums. We adjust the formulation to account for uncertainty in the resulting\nlosses using robust optimization. Our approach outlines how properties of the\nmodel, such as accuracy, interpretability, and generalizability, can influence\nthe insurance contract evaluation. To showcase a practical implementation of\nthe proposed framework, we present a case study of medical malpractice in the\ncontext of breast cancer detection. Our analysis focuses on measuring the\neffect of the model parameters on the expected financial loss and identifying\nthe aspects of algorithmic performance that predominantly affect the risk of\nthe contract.\n"
    },
    {
        "paper_id": 2106.012,
        "authors": "Karel in 't Hout and Jacob Snoeijer",
        "title": "Numerical valuation of American basket options via partial differential\n  complementarity problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the principal component analysis based approach introduced by\nReisinger & Wittum (2007) and the comonotonic approach considered by Hanbali &\nLinders (2019) for the approximation of American basket option values via\nmultidimensional partial differential complementarity problems (PDCPs). Both\napproximation approaches require the solution of just a limited number of\nlow-dimensional PDCPs. It is demonstrated by ample numerical experiments that\nthey define approximations that lie close to each other. Next, an efficient\ndiscretisation of the pertinent PDCPs is presented that leads to a favourable\nconvergence behaviour.\n"
    },
    {
        "paper_id": 2106.01281,
        "authors": "Felix-Benedikt Liebrich and Cosimo Munari",
        "title": "Law-invariant functionals that collapse to the mean: Beyond convexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We establish general \"collapse to the mean\" principles that provide\nconditions under which a law-invariant functional reduces to an expectation. In\nthe convex setting, we retrieve and sharpen known results from the literature.\nHowever, our results also apply beyond the convex setting. We illustrate this\nby providing a complete account of the \"collapse to the mean\" for quasiconvex\nfunctionals. In the special cases of consistent risk measures and Choquet\nintegrals, we can even dispense with quasiconvexity. In addition, we relate the\n\"collapse to the mean\" to the study of solutions of a broad class of\noptimisation problems with law-invariant objectives that appear in mathematical\nfinance, insurance, and economics. We show that the corresponding quantile\nformulations studied in the literature are sometimes illegitimate and require\nfurther analysis.\n"
    },
    {
        "paper_id": 2106.01787,
        "authors": "Valeri Lipunov, Vladislav Shirshikov, Jonathan Lewis",
        "title": "Theoretical and methodological approaches to the study of the problem of\n  corruption",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article reveals the main theoretical approaches to the analysis and study\nof the phenomenon of corruption. Special attention is paid to the consideration\nof the index approach to the analysis of corruption.\n"
    },
    {
        "paper_id": 2106.01815,
        "authors": "Shreya Biswas, Upasak Das",
        "title": "Adding fuel to human capital: Exploring the educational effects of\n  cooking fuel choice from rural India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The study examines the effect of cooking fuel choice on educational outcomes\nof adolescent children in rural India. Using multiple large-scale nationally\nrepresentative datasets, we observe household solid fuel usage to adversely\nimpact school attendance, years of schooling and age-appropriate grade\nprogression among children. This inference is robust to alternative ways of\nmeasuring educational outcomes, other datasets, specifications and estimation\ntechniques. Importantly, the effect is found to be more pronounced for females\nin comparison to the males highlighting the gendered nature of the impact. On\nexploring possible pathways, we find that the direct time substitution on\naccount of solid fuel collection and preparation can explain the detrimental\neducational outcomes that include learning outcomes as well, even though we are\nunable to reject the health channel. In the light of the micro and macro level\nvulnerabilities posed by the COVID-19 outbreak, the paper recommends\ninterventions that have the potential to fasten the household energy transition\ntowards clean fuel in the post-covid world.\n"
    },
    {
        "paper_id": 2106.01936,
        "authors": "Gianluca Teza, Michele Caraglio and Attilio L. Stella",
        "title": "Entropic measure unveils country competitiveness and product\n  specialization in the World trade web",
        "comments": null,
        "journal-ref": "Sci Rep 11, 10189 (2021)",
        "doi": "10.1038/s41598-021-89519-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how the Shannon entropy function can be used as a basis to set up\ncomplexity measures weighting the economic efficiency of countries and the\nspecialization of products beyond bare diversification. This entropy function\nguarantees the existence of a fixed point which is rapidly reached by an\niterative scheme converging to our self-consistent measures. Our approach\nnaturally allows to decompose into inter-sectorial and intra-sectorial\ncontributions the country competitivity measure if products are partitioned\ninto larger categories. Besides outlining the technical features and advantages\nof the method, we describe a wide range of results arising from the analysis of\nthe obtained rankings and we benchmark these observations against those\nestablished with other economical parameters. These comparisons allow to\npartition countries and products into various main typologies, with\nwell-revealed characterizing features. Our methods have wide applicability to\ngeneral problems of ranking in bipartite networks.\n"
    },
    {
        "paper_id": 2106.01952,
        "authors": "Minou Ghaffari, Maxime Kaniewicz, Stephan Stricker",
        "title": "Personalized Communication Strategies: Towards A New Debtor Typology\n  Framework",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Based on debt collection agency (PAIR Finance) data, we developed a novel\ndebtor typology framework by expanding previous approaches to 4 behavioral\ndimensions. The 4 dimensions we identified were willingness to pay, ability to\npay, financial organization, and rational behavior. Using these dimensions,\ndebtors could be classified into 16 different typologies. We identified 5 main\ntypologies, which account for 63% of the debtors in our data set. Further, we\nobserved that each debtor typology reacted differently to the content and\ntiming of reminder messages, allowing us to define an optimal debt collection\nstrategy for each typology. For example, sending a reciprocity message at 8\np.m. in the evening is the most successful strategy to get a reaction from a\ndebtor who is willing to pay their debt, able to pay their debt, chaotic in\nterms of their financial organization, and emotional when communicating and\nhandling their finances. In sum, our findings suggest that each debtor type\nshould be approached in a personalized way using different tonalities and\ntiming schedules.\n"
    },
    {
        "paper_id": 2106.02131,
        "authors": "Taras Bodnar, Nestor Parolya and Erik Thorsen",
        "title": "Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance\n  Portfolio",
        "comments": "27 pages, 7 figures, update1: minor fixes",
        "journal-ref": null,
        "doi": "10.1109/TSP.2023.3263950",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, new results in random matrix theory are derived which allow us\nto construct a shrinkage estimator of the global minimum variance (GMV)\nportfolio when the shrinkage target is a random object. More specifically, the\nshrinkage target is determined as the holding portfolio estimated from previous\ndata. The theoretical findings are applied to develop theory for dynamic\nestimation of the GMV portfolio, where the new estimator of its weights is\nshrunk to the holding portfolio at each time of reconstruction. Both cases with\nand without overlapping samples are considered in the paper. The\nnon-overlapping samples corresponds to the case when different data of the\nasset returns are used to construct the traditional estimator of the GMV\nportfolio weights and to determine the target portfolio, while the overlapping\ncase allows intersections between the samples. The theoretical results are\nderived under weak assumptions imposed on the data-generating process. No\nspecific distribution is assumed for the asset returns except from the\nassumption of finite $4+\\varepsilon$, $\\varepsilon>0$, moments. Also, the\npopulation covariance matrix with unbounded spectrum can be considered. The\nperformance of new trading strategies is investigated via an extensive\nsimulation. Finally, the theoretical findings are implemented in an empirical\nillustration based on the returns on stocks included in the S\\&P 500 index.\n"
    },
    {
        "paper_id": 2106.02187,
        "authors": "Roberto Mota Navarro, Paulino Monroy Castillero, Francois Leyvraz",
        "title": "Time-dependent relations between gaps and returns in a Bitcoin order\n  book",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Several studies have shown that large changes in the returns of an asset are\nassociated with the sized of the gaps present in the order book In general,\nthese associations have been studied without explicitly considering the\ndynamics of either gaps or returns. Here we present a study of these\nrelationships. Our results suggest that the causal relationship between gaps\nand returns is limited to instantaneous causation.\n"
    },
    {
        "paper_id": 2106.02263,
        "authors": "Zhengqing Zhou, Guanyang Wang, Jose Blanchet, Peter W. Glynn",
        "title": "Unbiased Optimal Stopping via the MUSE",
        "comments": "39 pages, add several numerical experiments and technical results,\n  accepted by Stochastic Processes and their Applications",
        "journal-ref": null,
        "doi": "10.1016/j.spa.2022.12.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new unbiased estimator for estimating the utility of the optimal\nstopping problem. The MUSE, short for Multilevel Unbiased Stopping Estimator,\nconstructs the unbiased Multilevel Monte Carlo (MLMC) estimator at every stage\nof the optimal stopping problem in a backward recursive way. In contrast to\ntraditional sequential methods, the MUSE can be implemented in parallel. We\nprove the MUSE has finite variance, finite computational complexity, and\nachieves $\\epsilon$-accuracy with $O(1/\\epsilon^2)$ computational cost under\nmild conditions. We demonstrate MUSE empirically in an option pricing problem\ninvolving a high-dimensional input and the use of many parallel processors.\n"
    },
    {
        "paper_id": 2106.02371,
        "authors": "Alfred Galichon and Bernard Salani\\'e",
        "title": "Cupid's Invisible Hand: Social Surplus and Identification in Matching\n  Models",
        "comments": "85 pages, 8 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate a model of one-to-one matching with transferable utility and\ngeneral unobserved heterogeneity. Under a separability assumption that\ngeneralizes Choo and Siow (2006), we first show that the equilibrium matching\nmaximizes a social gain function that trades off exploiting complementarities\nin observable characteristics and matching on unobserved characteristics. We\nuse this result to derive simple closed-form formulae that identify the joint\nmatching surplus and the equilibrium utilities of all participants, given any\nknown distribution of unobserved heterogeneity. We provide efficient algorithms\nto compute the stable matching and to estimate parametric versions of the\nmodel. Finally, we revisit Choo and Siow's empirical application to illustrate\nthe potential of our more general approach.\n"
    },
    {
        "paper_id": 2106.02418,
        "authors": "Claude Martini, Arianna Mingone",
        "title": "Explicit no arbitrage domain for sub-SVIs via reparametrization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The no Butterfly arbitrage domain of Gatheral SVI 5-parameters formula for\nthe volatility smile has been recently described. It requires in general a\nnumerical minimization of 2 functions altogether with a few root finding\nprocedures. We study here the case of some sub-SVIs (all with 3 parameters):\nthe Symmetric SVI, the Vanishing Upward/Downward SVI, and SSVI, for which we\nprovide an explicit domain, with no numerical procedure required.\n"
    },
    {
        "paper_id": 2106.02446,
        "authors": "Kartik Sethi, Siddhartha P. Chakrabarty",
        "title": "Modeling premiums of non-life insurance companies in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We undertake an empirical analysis for the premium data of non-life insurance\ncompanies operating in India, in the paradigm of fitting the data for the\nparametric distribution of Lognormal and the extreme value based distributions\nof Generalized Extreme Value and Generalized Pareto. The best fit to the data\nfor ten companies considered, is obtained for the Generalized Extreme Value\ndistribution.\n"
    },
    {
        "paper_id": 2106.02522,
        "authors": "Junran Wu, Ke Xu, Xueyuan Chen, Shangzhe Li and Jichang Zhao",
        "title": "Price graphs: Utilizing the structural information of financial time\n  series for stock prediction",
        "comments": "Code and data can be accessed through\n  https://github.com/BUAA-WJR/PriceGraph",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Great research efforts have been devoted to exploiting deep neural networks\nin stock prediction. While long-range dependencies and chaotic property are\nstill two major issues that lower the performance of state-of-the-art deep\nlearning models in forecasting future price trends. In this study, we propose a\nnovel framework to address both issues. Specifically, in terms of transforming\ntime series into complex networks, we convert market price series into graphs.\nThen, structural information, referring to associations among temporal points\nand the node weights, is extracted from the mapped graphs to resolve the\nproblems regarding long-range dependencies and the chaotic property. We take\ngraph embeddings to represent the associations among temporal points as the\nprediction model inputs. Node weights are used as a priori knowledge to enhance\nthe learning of temporal attention. The effectiveness of our proposed framework\nis validated using real-world stock data, and our approach obtains the best\nperformance among several state-of-the-art benchmarks. Moreover, in the\nconducted trading simulations, our framework further obtains the highest\ncumulative profits. Our results supplement the existing applications of complex\nnetwork methods in the financial realm and provide insightful implications for\ninvestment applications regarding decision support in financial markets.\n"
    },
    {
        "paper_id": 2106.02546,
        "authors": "Kerim Eser Af\\c{s}ar, Mehmet \\\"Ozyi\\~git, Yusuf Y\\\"uksel, \\\"Umit\n  Ak{\\i}nc{\\i}",
        "title": "Testing the Goodwin Growth Cycles with Econophysics Approach in\n  2002-2019 Period in Turkey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The Turkish economy between 2002-2019 period has been investigated within the\neconophysical approach. From the individual income data obtained from the\nHousehold Budget Survey, the Gompertz-Pareto distribution for each year and\nGoodwin cycle for the mentioned period have been obtained. For this period, in\nwhich thirteen elections were held under the single-party rule, it has been\nobserved that the income distribution fits well with the Gompertz-Pareto\ndistribution which shows the two-class structure of the Turkish economy. The\nvariation of the threshold value $x_t$ (which separates these two classes) as\nwell as Pareto coefficient have been obtained. Besides, Goodwin cycle has been\nobserved within this period, centered at $(u,v)\\cong (66.30,83.40)$ and a\nperiod of $T=18.30$ years. It has been concluded that these observations are\nconsistent with the economic and social events experienced in the mentioned\nperiod.\n"
    },
    {
        "paper_id": 2106.02861,
        "authors": "Nicolaus Tideman and Thomas Mecherikunnel",
        "title": "Optimal Taxation of Assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The optimal taxation of assets requires attention to two concerns: 1) the\nelasticity of the supply of assets and 2) the impact of taxing assets on\ndistributional objectives. The most efficient way to attend to these two\nconcerns is to tax assets of different types separately, rather than having one\ntax on all assets. When assets are created by specialized effort rather than by\nsaving, as with innovations, discoveries of mineral deposits and development of\nunregulated natural monopolies, it is interesting to consider a regime in which\nthe government awards a prize for the creation of the asset and then collects\nthe remaining value of the asset in taxes. Analytically, the prize is like a\nwage after taxes. In this perspective, prizes are awarded based on a variation\non optimal taxation theory, while assets of different types are taxed in\ndivergent ways, depending on their characteristics. Some categories of assets\nare abolished.\n"
    },
    {
        "paper_id": 2106.02916,
        "authors": "Muyang Ge, Shen Zhou, Shijun Luo and Boping Tian",
        "title": "3D Tensor-based Deep Learning Models for Predicting Option Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Option pricing is a significant problem for option risk management and\ntrading. In this article, we utilize a framework to present financial data from\ndifferent sources. The data is processed and represented in a form of 2D\ntensors in three channels. Furthermore, we propose two deep learning models\nthat can deal with 3D tensor data. Experiments performed on the Chinese market\noption dataset prove the practicability of the proposed strategies over\ncommonly used ways, including B-S model and vector-based LSTM.\n"
    },
    {
        "paper_id": 2106.02917,
        "authors": "Vikram Govindan, Wei Xie",
        "title": "On the Stratification of Product Portfolios",
        "comments": "13 pages, 5 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stratifying commercial product portfolios into multiple classes of decreasing\npriority, ABCD analysis, is a common supply chain tool. Key planning parameters\nthat drive strategic and execution priorities are tied to the resulting\nsegmentation. These priorities in turn drive supply chain performance. For\nlarge product assortments, manual segmentation is infeasible so an automated\nalgorithm is needed. We therefore advocate that careful attention be paid to\nthe design of such an ABCD algorithm and present three key features that can be\nincorporated into such a calculation to improve its quality and commercial\nutility.\n"
    },
    {
        "paper_id": 2106.02945,
        "authors": "Sarwar J. Minar and Abdul Halim",
        "title": "The Rohingyas of Rakhine State: Social Evolution and History in the\n  Light of Ethnic Nationalism",
        "comments": "29 pages",
        "journal-ref": "Social Evolution & History (2020)",
        "doi": "10.30884/seh/2020.02.06",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent event of ousting Rohingyas from Rakhine State by the Tatmadaw provoked\nworldwide public-and-academic interest in history and social evolution of the\nRohingyas, and this is to what the article is devoted. As the existing\nliterature presents a debate over Who are the Rohingyas?, and How legitimate is\ntheir claim over Rakhine State?, the paper reinvestigates the issues using a\nqualitative research method. Compiling a detailed history, the paper finds that\nRohingya community developed through historically complicated processes marked\nby invasions and counter-invasions. The paper argues many people entered Bengal\nfrom Arakan before British brought people into Rakhine state. The Rohingyas\nbelieve Rakhine State is their ancestral homeland and they developed a sense of\nEthnic Nationalism. Their right over Rakhine State is as significant as other\ngroups. The paper concludes that the UN must pursue solution to the crisis and\nthe government should accept the Rohingyas as it did the land or territory.\n"
    },
    {
        "paper_id": 2106.03035,
        "authors": "Koya Ishikawa and Kazuhide Nakata",
        "title": "Online Trading Models with Deep Reinforcement Learning in the Forex\n  Market Considering Transaction Costs",
        "comments": "7 pages, 2 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, a wide range of investment models have been created using\nartificial intelligence. Automatic trading by artificial intelligence can\nexpand the range of trading methods, such as by conferring the ability to\noperate 24 hours a day and the ability to trade with high frequency. Automatic\ntrading can also be expected to trade with more information than is available\nto humans if it can sufficiently consider past data. In this paper, we propose\nan investment agent based on a deep reinforcement learning model, which is an\nartificial intelligence model. The model considers the transaction costs\ninvolved in actual trading and creates a framework for trading over a long\nperiod of time so that it can make a large profit on a single trade. In doing\nso, it can maximize the profit while keeping transaction costs low. In\naddition, in consideration of actual operations, we use online learning so that\nthe system can continue to learn by constantly updating the latest online data\ninstead of learning with static data. This makes it possible to trade in\nnon-stationary financial markets by always incorporating current market trend\ninformation.\n"
    },
    {
        "paper_id": 2106.03263,
        "authors": "Maciej Ber\\k{e}sewicz and Herman Cherniaiev and Robert Pater",
        "title": "Estimating the number of entities with vacancies using administrative\n  and online data",
        "comments": "70 pages, 4 figures, 15 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article we describe a study aimed at estimating job vacancy\nstatistics, in particular the number of entities with at least one vacancy. To\nachieve this goal, we propose an alternative approach to the methodology\nexploiting survey data, which is based solely on data from administrative\nregisters and online sources and relies on dual system estimation (DSE).\n  As these sources do not cover the whole reference population and the number\nof units appearing in all datasets is small, we have developed a DSE approach\nfor negatively dependent sources based on a recent work by Chatterjee and\nBhuyan (2020). To achieve the main goal we conducted a thorough data cleaning\nprocedure in order to remove out-of-scope units, identify entities from the\ntarget population, and link them by identifiers to minimize linkage errors. We\nverified the effectiveness and sensitivity of the proposed estimator in\nsimulation studies.\n  From a practical point of view, our results show that the current vacancy\nsurvey in Poland underestimates the number of entities with at least one\nvacancy by about 10-15%. The main reasons for this discrepancy are non-sampling\nerrors due to non-response and under-reporting, which is identified by\ncomparing survey data with administrative data.\n"
    },
    {
        "paper_id": 2106.03272,
        "authors": "Ming Min, Ruimeng Hu",
        "title": "Signatured Deep Fictitious Play for Mean Field Games with Common Noise",
        "comments": "Published at ICML 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existing deep learning methods for solving mean-field games (MFGs) with\ncommon noise fix the sampling common noise paths and then solve the\ncorresponding MFGs. This leads to a nested-loop structure with millions of\nsimulations of common noise paths in order to produce accurate solutions, which\nresults in prohibitive computational cost and limits the applications to a\nlarge extent. In this paper, based on the rough path theory, we propose a novel\nsingle-loop algorithm, named signatured deep fictitious play, by which we can\nwork with the unfixed common noise setup to avoid the nested-loop structure and\nreduce the computational complexity significantly. The proposed algorithm can\naccurately capture the effect of common uncertainty changes on mean-field\nequilibria without further training of neural networks, as previously needed in\nthe existing machine learning algorithms. The efficiency is supported by three\napplications, including linear-quadratic MFGs, mean-field portfolio game, and\nmean-field game of optimal consumption and investment. Overall, we provide a\nnew point of view from the rough path theory to solve MFGs with common noise\nwith significantly improved efficiency and an extensive range of applications.\nIn addition, we report the first deep learning work to deal with extended MFGs\n(a mean-field interaction via both the states and controls) with common noise.\n"
    },
    {
        "paper_id": 2106.03417,
        "authors": "Alvaro Arroyo, Bruno Scalzo, Ljubisa Stankovic, Danilo P. Mandic",
        "title": "Dynamic Portfolio Cuts: A Spectral Approach to Graph-Theoretic\n  Diversification",
        "comments": "5 pages, 3 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market returns are typically analyzed using standard regression, yet\nthey reside on irregular domains which is a natural scenario for graph signal\nprocessing. To this end, we consider a market graph as an intuitive way to\nrepresent the relationships between financial assets. Traditional methods for\nestimating asset-return covariance operate under the assumption of statistical\ntime-invariance, and are thus unable to appropriately infer the underlying true\nstructure of the market graph. This work introduces a class of graph spectral\nestimators which cater for the nonstationarity inherent to asset price\nmovements, and serve as a basis to represent the time-varying interactions\nbetween assets through a dynamic spectral market graph. Such an account of the\ntime-varying nature of the asset-return covariance allows us to introduce the\nnotion of dynamic spectral portfolio cuts, whereby the graph is partitioned\ninto time-evolving clusters, allowing for online and robust asset allocation.\nThe advantages of the proposed framework over traditional methods are\ndemonstrated through numerical case studies using real-world price data.\n"
    },
    {
        "paper_id": 2106.03467,
        "authors": "Luo Ying",
        "title": "The Intellectual Property Protection System of the Foreign Investment\n  Law: Basic Structure, Motivation and Game Logic",
        "comments": "18pages,2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The intellectual property protection system constructed by China's Foreign\nInvestment Law has opened a new phase of rule of law protection of intellectual\nproperty rights for foreign-invested enterprises, which is an important\ninstitutional support indispensable for optimizing the business environment\nunder the rule of law.The development of the regime was influenced by the major\nconcerns of investors' home countries, the \"innovation-driven development\"\nstrategy, and the trend towards a high level of stringent protection of\ninternational intellectual property and investment rules.In addition, there is\na latent game of interests between multiple subjects, which can be analyzed by\nconstructing two standard formal game models according to legal game theory.The\nfirst game model aims to compare and analyze the gains and losses of China and\nIndia's IPR protection system for foreign-invested enterprises to attract\nforeign investment.The second game model is designed to analyze the benefits of\nChina and foreign investors under their respective possible behaviors before\nand after the inclusion of IPR protection provisions in the Foreign Investment\nLaw, with the optimal solution being a \"moderately cautious\" strategy for\nforeign investors and a \"strict enforcement\" strategy for China.\n"
    },
    {
        "paper_id": 2106.0356,
        "authors": "Raviar Karim, Roger J. A. Laeven, Michel Mandjes",
        "title": "Exact and Asymptotic Analysis of General Multivariate Hawkes Processes\n  and Induced Population Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers population processes in which general, not necessarily\nMarkovian, multivariate Hawkes processes dictate the stochastic arrivals. We\nestablish results to determine the corresponding time-dependent joint\nprobability distribution, allowing for general intensity decay functions,\ngeneral intensity jumps, and general sojourn times. We obtain an exact, full\ncharacterization of the time-dependent joint transform of the multivariate\npopulation process and its underlying intensity process in terms of a\nfixed-point representation and corresponding convergence results. We also\nderive the asymptotic tail behavior of the population process and its\nunderlying intensity process in the setting of heavy-tailed intensity jumps. By\nexploiting the results we establish, arbitrary joint spatial-temporal moments\nand other distributional properties can now be readily evaluated using standard\ntransform differentiation and inversion techniques, and we illustrate this in a\nfew examples.\n"
    },
    {
        "paper_id": 2106.03643,
        "authors": "A. G. Adeeth Cariappa, B. S. Chandel, Gopal Sankhala, Veena Mani,\n  Sendhil R, Anil Kumar Dixit, and B. S. Meena",
        "title": "Prevention Is Better Than Cure: Experimental Evidence From Milk Fever\n  Incidence in Dairy Animals of Haryana, India",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3851561",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Calcium deficiency in high yielding bovines during calving causes milk fever\nwhich leads to economic losses of around INR 1000 crores (USD 137 million) per\nannum in Haryana, India. With increasing milk production, the risk of milk\nfever is continuously rising. In the context, we aim to address the most\nfundamental research question: What is the effect of a preventive health\nproduct (anionic mineral mixture (AMM)) on milk fever incidence, milk\nproductivity and farmers income? In an effort to contribute to the scanty\neconomic literature on effect of preventive measures on nutritional deficiency\ndisorders in dairy animals, specifically, on AMM effects in India, this study\nuses a randomized controlled design to estimate internally valid estimates.\nUsing data from 200 dairy farms, results indicate that milk fever incidence\ndecreases from 21 per cent at baseline to 2 per cent in treated animals at\nfollow-up. Further, AMM leads to a 12 per cent and 38 per cent increase in milk\nyield and farmers net income, respectively. Profits earned due to the\nprevention of milk fever [INR 16000 (USD 218.7)] overweighs the losses from\nmilk fever [INR 4000 (USD 54.7)]; thus, prevention using AMM is better than\ncure.\n"
    },
    {
        "paper_id": 2106.03691,
        "authors": "Michele Costola and Matteo Iacopini and Carlo R.M.A. Santagiustina",
        "title": "On the \"mementum\" of Meme Stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The meme stock phenomenon is yet to be explored. In this note, we provide\nevidence that these stocks display common stylized facts on the dynamics of\nprice, trading volume, and social media activity. Using a regime-switching\ncointegration model, we identify the meme stock \"mementum\" which exhibits a\ndifferent characterization with respect to other stocks with high volumes of\nactivity (persistent and not) on social media. Understanding these properties\nhelps the investors and market authorities in their decision.\n"
    },
    {
        "paper_id": 2106.03716,
        "authors": "Marco Di Francesco and Kevin Kamm",
        "title": "How to handle negative interest rates in a CIR framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a new model to address the problem of negative\ninterest rates that preserves the analytical tractability of the original\nCox-Ingersoll-Ross (CIR) model without introducing a shift to the market\ninterest rates, because it is defined as the difference of two independent CIR\nprocesses. The strength of our model lies within the fact that it is very\nsimple and can be calibrated to the market zero yield curve using an analytical\nformula. We run several numerical experiments at two different dates, once with\na partially sub-zero interest rate and once with a fully negative interest\nrate. In both cases, we obtain good results in the sense that the model\nreproduces the market term structures very well. We then simulate the model\nusing the Euler-Maruyama scheme and examine the mean, variance and distribution\nof the model. The latter agrees with the skewness and fat tail seen in the\noriginal CIR model. In addition, we compare the model's zero coupon prices with\nmarket prices at different future points in time. Finally, we test the market\nconsistency of the model by evaluating swaptions with different tenors and\nmaturities.\n"
    },
    {
        "paper_id": 2106.04028,
        "authors": "Jorge Guijarro-Ordonez, Markus Pelger and Greg Zanotti",
        "title": "Deep Learning Statistical Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a unifying conceptual framework for statistical arbitrage\nand a novel data driven solution. First, we construct arbitrage portfolios of\nsimilar assets as residual portfolios from conditional latent asset pricing\nfactors. Second, we extract their time series signals with a powerful\nmachine-learning time-series solution, a convolutional transformer. Lastly, we\nuse these signals to form an optimal trading policy, that maximizes\nrisk-adjusted returns under constraints. Our comprehensive empirical study on\ndaily US equities shows a high compensation for arbitrageurs to enforce the law\nof one price. Our arbitrage strategies obtain consistently high out-of-sample\nmean returns and Sharpe ratios, and substantially outperform all benchmark\napproaches.\n"
    },
    {
        "paper_id": 2106.04114,
        "authors": "Liu Ziyin, Kentaro Minami, Kentaro Imajo",
        "title": "Theoretically Motivated Data Augmentation and Regularization for\n  Portfolio Construction",
        "comments": "The full version of our work published at 3rd ACM International\n  Conference on AI in Finance (ICAIF'22)",
        "journal-ref": null,
        "doi": "10.1145/3533271.3561720",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The task we consider is portfolio construction in a speculative market, a\nfundamental problem in modern finance. While various empirical works now exist\nto explore deep learning in finance, the theory side is almost non-existent. In\nthis work, we focus on developing a theoretical framework for understanding the\nuse of data augmentation for deep-learning-based approaches to quantitative\nfinance. The proposed theory clarifies the role and necessity of data\naugmentation for finance; moreover, our theory implies that a simple algorithm\nof injecting a random noise of strength $\\sqrt{|r_{t-1}|}$ to the observed\nreturn $r_{t}$ is better than not injecting any noise and a few other\nfinancially irrelevant data augmentation techniques.\n"
    },
    {
        "paper_id": 2106.04218,
        "authors": "Piero Quatto, Gianmarco Vacca, Maria Grazia Zoia",
        "title": "Modeling Portfolios with Leptokurtic and Dependent Risk Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Recently, an approach to modeling portfolio distribution with risk factors\ndistributed as Gram-Charlier (GC) expansions of the Gaussian law, has been\nconceived. GC expansions prove effective when dealing with moderately\nleptokurtic data. In order to cover the case of possibly severe leptokurtosis,\nthe so-called GC-like expansions have been devised by reshaping parent\nleptokurtic distributions by means of orthogonal polynomials specific to them.\nIn this paper, we focus on the hyperbolic-secant (HS) law as parent\ndistribution whose GC-like expansions fit with kurtosis levels up to 19.4. A\nportfolio distribution has been obtained with risk factors modeled as GClike\nexpansions of the HS law which duly account for excess kurtosis. Empirical\nevidence of the workings of the approach dealt with in the paper is included.\n"
    },
    {
        "paper_id": 2106.04239,
        "authors": "Victor Grebenik, Yuri Tarasenko, Dmitry Zerkin, Mattia Masolletti",
        "title": "The Russian practice of applying cluster approach in regional\n  development",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article considers the practice of applying the cluster approach in Russia\nas a tool for overcoming economic inequality between Russian regions. The\nauthors of the study analyze the legal framework of cluster policy in Russia,\nnoting that its successful implementation requires a little more time than\noriginally planned. Special attention is paid to the experience of\nbenchmarking.\n"
    },
    {
        "paper_id": 2106.04338,
        "authors": "Jeffrey Ding and Allan Dafoe",
        "title": "Engines of Power: Electricity, AI, and General-Purpose Military\n  Transformations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Major theories of military innovation focus on relatively narrow\ntechnological developments, such as nuclear weapons or aircraft carriers.\nArguably the most profound military implications of technological change,\nhowever, come from more fundamental advances arising from general purpose\ntechnologies, such as the steam engine, electricity, and the computer. With few\nexceptions, political scientists have not theorized about GPTs. Drawing from\nthe economics literature on GPTs, we distill several propositions on how and\nwhen GPTs affect military affairs. We call these effects general-purpose\nmilitary transformations. In particular, we argue that the impacts of GMTs on\nmilitary effectiveness are broad, delayed, and shaped by indirect productivity\nspillovers. Additionally, GMTs differentially advantage those militaries that\ncan draw from a robust industrial base in the GPT. To illustrate the\nexplanatory value of our theory, we conduct a case study of the military\nconsequences of electricity, the prototypical GPT. Finally, we apply our\nfindings to artificial intelligence, which will plausibly cause a profound\ngeneral-purpose military transformation.\n"
    },
    {
        "paper_id": 2106.04421,
        "authors": "Ying-Hui Shao and Yan-Hong Yang and Wei-Xing Zhou",
        "title": "How does economic policy uncertainty comove with stock markets: New\n  evidence from symmetric thermal optimal path method",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.127745",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the dynamic relationship between domestic economic policy\nuncertainty and stock markets using the symmetric thermal optimal path (TOPS)\nmethod. We observe different interaction patterns in emerging and developed\nmarkets. Economic policy uncertainty drives the stock market in China, while\nstock markets play a leading role in the UK and the US. Meanwhile, the lead-lag\nrelationship of the three countries reacts significantly to extreme events. Our\nfindings have important implications for investors and policy makers.\n"
    },
    {
        "paper_id": 2106.04518,
        "authors": "Matthew Lorig, Natchanon Suaysom",
        "title": "Options on Bonds: Implied Volatilities from Affine Short-Rate Dynamics",
        "comments": "28 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an explicit asymptotic approximation for the implied volatilities\nof Call options written on bonds assuming the short-rate is described by an\naffine short-rate model. For specific affine short-rate models, we perform\nnumerical experiments in order to gauge the accuracy of our approximation.\n"
    },
    {
        "paper_id": 2106.04906,
        "authors": "Edward J. Oughton and Erik Boch and Julius Kusuma",
        "title": "Engineering-Economic Evaluation of Diffractive Non-Line-Of-Sight\n  Backhaul (e3nb): A Techno-economic Model for 3D Wireless Backhaul Assessment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Developing ways to affordably deliver broadband connectivity is one of the\nmajor issues of our time. In challenging deployment locations with irregular\nterrain, traditional Clear-Line-Of-Sight (CLOS) wireless links can be\nuneconomical to deploy, as the number of required towers make infrastructure\ninvestment unviable. With new research focusing on developing wireless\ndiffractive backhaul technologies to provide Non-Line-Of-Sight (NLOS) links,\nthis paper evaluates the engineering-economic implications. A Three-Dimensional\n(3D) techno-economic assessment framework is developed, utilizing a combination\nof remote sensing and viewshed geospatial techniques, in order to quantify the\nimpact of different wireless backhaul strategies. This framework is applied to\nassess both Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight strategies\nfor deployment in Peru, as well as the islands of Kalimantan and Papua, in\nIndonesia. The results find that a hybrid strategy combining the use of\nClear-Line-Of-Sight and diffractive Non-Line-Of-Sight links produces a 9-45\npercent cost-efficiency saving, relative to only using traditional\nClear-Line-Of-Sight wireless backhaul links.\n"
    },
    {
        "paper_id": 2106.0524,
        "authors": "David Ardia, Keven Bluteau, Alaa Kassem",
        "title": "A Century of Economic Policy Uncertainty Through the French-Canadian\n  Lens",
        "comments": null,
        "journal-ref": "Economics Letters, 2021, volume 205, 109938",
        "doi": "10.1016/j.econlet.2021.109938",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A novel token-distance-based triple approach is proposed for identifying EPU\nmentions in textual documents. The method is applied to a corpus of\nFrench-language news to construct a century-long historical EPU index for the\nCanadian province of Quebec. The relevance of the index is shown in a\nmacroeconomic nowcasting experiment.\n"
    },
    {
        "paper_id": 2106.05797,
        "authors": "Paul Glasserman, Mike Li",
        "title": "Linear Classifiers Under Infinite Imbalance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the behavior of linear discriminant functions for binary\nclassification in the infinite-imbalance limit, where the sample size of one\nclass grows without bound while the sample size of the other remains fixed. The\ncoefficients of the classifier minimize an empirical loss specified through a\nweight function. We show that for a broad class of weight functions, the\nintercept diverges but the rest of the coefficient vector has a finite almost\nsure limit under infinite imbalance, extending prior work on logistic\nregression. The limit depends on the left-tail growth rate of the weight\nfunction, for which we distinguish two cases: subexponential and exponential.\nThe limiting coefficient vectors reflect robustness or conservatism properties\nin the sense that they optimize against certain worst-case alternatives. In the\nsubexponential case, the limit is equivalent to an implicit choice of\nupsampling distribution for the minority class. We apply these ideas in a\ncredit risk setting, with particular emphasis on performance in the\nhigh-sensitivity and high-specificity regions.\n"
    },
    {
        "paper_id": 2106.05972,
        "authors": "Vincent Zha",
        "title": "The separation of market and price in some free competitions and its\n  related solution to the over-application problem in the job market",
        "comments": "4 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  According to common understanding, in free completion of a private product,\nmarket and price, the two main factors in the competition that leads to\neconomic efficiency, always exist together. This paper, however, points out the\nphenomenon that in some free competitions the two factors are separated hence\ncausing inefficiency. For one type, the market exists whereas the price is\nabsent, i.e. free, for a product. An example of this type is the job\napplication market where the problem of over-application commonly exists,\ncosting recruiters much time in finding desired candidates from massive\napplicants, resulting in inefficiency. To solve the problem, this paper\nproposes a solution that the recruiters charge submission fees to the\napplications to make the competition complete with both factors, hence\nenhancing the efficiency. For the other type, the price exists whereas the\nmarket is absent for a product. An example of this type is the real estate\nagent market, where the price of the agents exists but the market, i.e. the\nfacility allowing the sellers' information to be efficiently discovered, is\nlargely absent, also causing inefficiency. In summary, the contribution of this\npaper consists of two aspects: one is the discovery of the possible separation\nof the two factors in free competitions; the other is, thanks to the discovery,\na solution to the over-application problem in the job market.\n"
    },
    {
        "paper_id": 2106.06028,
        "authors": "Runhuan Feng and Peng Li",
        "title": "Sample Recycling Method -- A New Approach to Efficient Nested Monte\n  Carlo Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Nested stochastic modeling has been on the rise in many fields of the\nfinancial industry. Such modeling arises whenever certain components of a\nstochastic model are stochastically determined by other models. There are at\nleast two main areas of applications, including (1) portfolio risk management\nin the banking sector and (2) principle-based reserving and capital\nrequirements in the insurance sector. As financial instrument values often\nchange with economic fundamentals, the risk management of a portfolio (outer\nloop) often requires the assessment of financial positions subject to changes\nin risk factors in the immediate future. The valuation of financial position\n(inner loop) is based on projections of cashflows and risk factors into the\ndistant future. The nesting of such stochastic modeling can be computationally\nchallenging.\n  Most of existing techniques to speed up nested simulations are based on curve\nfitting. The main idea is to establish a functional relationship between inner\nloop estimator and risk factors by running a limited set of economic scenarios,\nand, instead of running inner loop simulations, inner loop estimations are made\nby feeding other scenarios into the fitted curve. This paper presents a\nnon-conventional approach based on the concept of sample recycling. Its essence\nis to run inner loop estimation for a small set of outer loop scenarios and to\nfind inner loop estimates under other outer loop scenarios by recycling those\nknown inner loop paths. This new approach can be much more efficient when\ntraditional techniques are difficult to implement in practice.\n"
    },
    {
        "paper_id": 2106.0603,
        "authors": "Carolyn E. Phelan and Daniele Marazzina and Guido Germano",
        "title": "Pricing methods for $\\alpha$-quantile and perpetual early exercise\n  options based on Spitzer identities",
        "comments": "29 pages, 14 figures, 5 tables",
        "journal-ref": "Quantitative Finance 20.6 (2020): 899-918",
        "doi": "10.1080/14697688.2020.1718192",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present new numerical schemes for pricing perpetual Bermudan and American\noptions as well as $\\alpha$-quantile options. This includes a new direct\ncalculation of the optimal exercise barrier for early-exercise options. Our\napproach is based on the Spitzer identities for general L\\'evy processes and on\nthe Wiener-Hopf method. Our direct calculation of the price of\n$\\alpha$-quantile options combines for the first time the Dassios-Port-Wendel\nidentity and the Spitzer identities for the extrema of processes. Our results\nshow that the new pricing methods provide excellent error convergence with\nrespect to computational time when implemented with a range of L\\'evy\nprocesses.\n"
    },
    {
        "paper_id": 2106.06164,
        "authors": "Darko Stosic, Dusan Stosic, Irena Vodenska, H. Eugene Stanley,\n  Tatijana Stosic",
        "title": "A new look at calendar anomalies: Multifractality and day of the week\n  effect",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e24040562",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock markets can become inefficient due to calendar anomalies known as\nday-of-the-week effect. Calendar anomalies are well-known in financial\nliterature, but the phenomena remain to be explored in econophysics. In this\npaper we use multifractal analysis to evaluate if the temporal dynamics of\nmarket returns also exhibits calendar anomalies such as day-of-the-week\neffects. We apply the multifractal detrended fluctuation analysis (MF-DFA) to\ndaily returns of market indices around the world for each day of the week. Our\nresults indicate that individual days of the week are characterized by distinct\nmultifractal properties. Monday returns tend to exhibit more persistent\nbehavior and richer multifractal structures than other day-resolved returns.\nShuffling the series reveals that multifractality arises both from a broad\nprobability density function and from long-term correlations. From the\ntime-dependent multifractal analysis we find that multifractal spectra for\nMonday returns are much wider than for other days during periods of financial\ncrises. The presence of day-of-the-week effects in multifractal dynamics of\nmarket returns motivates further research on calendar anomalies from an\neconophysics perspective.\n"
    },
    {
        "paper_id": 2106.06185,
        "authors": "Guanxing Fu and Chao Zhou",
        "title": "Mean Field Portfolio Games",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study mean field portfolio games with random market parameters, where each\nplayer is concerned with not only her own wealth but also relative performance\nto her competitors. We use the martingale optimality principle approach to\ncharacterize the unique Nash equilibrium in terms of a mean field FBSDE with\nquadratic growth, which is solvable under a weak interaction assumption.\nMotivated by the weak interaction assumption, we establish an asymptotic\nexpansion result in powers of the competition parameter. When the market\nparameters do not depend on the Brownian paths, we obtain the Nash equilibrium\nin closed form.\n"
    },
    {
        "paper_id": 2106.06364,
        "authors": "Florian Eckerli, Joerg Osterrieder",
        "title": "Generative Adversarial Networks in finance: an overview",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modelling in finance is a challenging task: the data often has complex\nstatistical properties and its inner workings are largely unknown. Deep\nlearning algorithms are making progress in the field of data-driven modelling,\nbut the lack of sufficient data to train these models is currently holding back\nseveral new applications. Generative Adversarial Networks (GANs) are a neural\nnetwork architecture family that has achieved good results in image generation\nand is being successfully applied to generate time series and other types of\nfinancial data. The purpose of this study is to present an overview of how\nthese GANs work, their capabilities and limitations in the current state of\nresearch with financial data, and present some practical applications in the\nindustry. As a proof of concept, three known GAN architectures were tested on\nfinancial time series, and the generated data was evaluated on its statistical\nproperties, yielding solid results. Finally, it was shown that GANs have made\nconsiderable progress in their finance applications and can be a solid\nadditional tool for data scientists in this field.\n"
    },
    {
        "paper_id": 2106.06373,
        "authors": "Jabir Ali Ouassou, Julian Straus, Marte Fodstad, Gunhild Reigstad, Ove\n  Wolfgang",
        "title": "Applying endogenous learning models in energy system optimization",
        "comments": "review paper: main article (11 pages), appendices (8 pages),\n  references (4 pages)",
        "journal-ref": "Energies 14, 4819 (2021)",
        "doi": "10.3390/en14164819",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conventional energy production based on fossil fuels causes emissions which\ncontribute to global warming. Accurate energy system models are required for a\ncost-optimal transition to a zero-emission energy system, an endeavor that\nrequires an accurate modeling of cost reductions due to technological learning\neffects. In this review, we summarize common methodologies for modeling\ntechnological learning and associated cost reductions. The focus is on learning\neffects in hydrogen production technologies due to their importance in a\nlow-carbon energy system, as well as the application of endogenous learning in\nenergy system models. Finally, we present an overview of the learning rates of\nrelevant low-carbon technologies required to model future energy systems.\n"
    },
    {
        "paper_id": 2106.06377,
        "authors": "Fatima-Zahra Jaouimaa, Daniel Dempsey, Suzanne van Osch, Stephen\n  Kinsella, Kevin Burke, Jason Wyse and James Sweeney",
        "title": "An age-structured SEIR model for COVID--19 incidence in Dublin, Ireland\n  with framework for evaluating health intervention cost",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0260632",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Strategies adopted globally to mitigate the threat of COVID-19 have primarily\ninvolved lockdown measures with substantial economic and social costs with\nvarying degrees of success. Morbidity patterns of COVID-19 variants have a\nstrong association with age, while restrictive lockdown measures have\nassociation with negative mental health outcomes in some age groups. Reduced\neconomic prospects may also afflict some age cohorts more than others.\nMotivated by this, we propose a model to describe COVID-19 community spread\nincorporating the role of age-specific social interactions. Through a flexible\nparameterisation of an age-structured deterministic Susceptible Exposed\nInfectious Removed (SEIR) model, we provide a means for characterising\ndifferent forms of lockdown which may impact specific age groups differently.\nSocial interactions are represented through age group to age group contact\nmatrices, which can be trained using available data and are thus locally\nadapted. This framework is easy to interpret and suitable for describing\ncounterfactual scenarios, which could assist policy makers with regard to\nminimising morbidity balanced with the costs of prospective suppression\nstrategies. Our work originates from an Irish context and we use disease\nmonitoring data from February 29th 2020 to January 31st 2021 gathered by Irish\ngovernmental agencies. We demonstrate how Irish lockdown scenarios can be\nconstructed using the proposed model formulation and show results of\nretrospective fitting to incidence rates and forward planning with relevant\n``what if/instead of'' lockdown counterfactuals with uncertainty\nquantification. Our formulation is agnostic to a specific locale, in that\nlockdown strategies in other regions can be straightforwardly encoded using\nthis model. The methods we describe are made publicly available online through\nan accessible and easy to use web interface.\n"
    },
    {
        "paper_id": 2106.06389,
        "authors": "Kaihua Qin, Liyi Zhou, Pablo Gamito, Philipp Jovanovic and Arthur\n  Gervais",
        "title": "An Empirical Study of DeFi Liquidations: Incentives, Risks, and\n  Instabilities",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3487552.3487811",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial speculators often seek to increase their potential gains with\nleverage. Debt is a popular form of leverage, and with over 39.88B USD of total\nvalue locked (TVL), the Decentralized Finance (DeFi) lending markets are\nthriving. Debts, however, entail the risks of liquidation, the process of\nselling the debt collateral at a discount to liquidators. Nevertheless, few\nquantitative insights are known about the existing liquidation mechanisms.\n  In this paper, to the best of our knowledge, we are the first to study the\nbreadth of the borrowing and lending markets of the Ethereum DeFi ecosystem. We\nfocus on Aave, Compound, MakerDAO, and dYdX, which collectively represent over\n85% of the lending market on Ethereum. Given extensive liquidation data\nmeasurements and insights, we systematize the prevalent liquidation mechanisms\nand are the first to provide a methodology to compare them objectively. We find\nthat the existing liquidation designs well incentivize liquidators but sell\nexcessive amounts of discounted collateral at the borrowers' expenses. We\nmeasure various risks that liquidation participants are exposed to and quantify\nthe instabilities of existing lending protocols. Moreover, we propose an\noptimal strategy that allows liquidators to increase their liquidation profit,\nwhich may aggravate the loss of borrowers.\n"
    },
    {
        "paper_id": 2106.06436,
        "authors": "Habiba Akter, Ilham Sentosa, Sheikh Muhamad Hizam, Waqas Ahmed, Arifa\n  Akter",
        "title": "Finding the Contextual Gap Towards Employee Engagement in Financial\n  Sector: A Review Study",
        "comments": "22 pages, 4 figures, 1 table",
        "journal-ref": "International Journal of Academic Research in Business and Social\n  Sciences, Vol. 11, No. 5, 2021",
        "doi": "10.6007/IJARBSS/v11-i5/9847",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This review paper identifies the core evidence of research on employee\nengagement , considering a stern challenge facing the financial sector\nnowadays. The study highlights the noteworthy knowledge gaps that will support\nhuman resource management practitioners to embed in the research towards\nsectoral context. Pertinent articles were selected through key search points\nand excerpt-related literature. The key search points covered the topic related\nto different terms of engagement for example \"employee engagement\" OR \"work\nengagement\" OR \"job engagement\" OR \"organization engagement\" OR \"staff\nengagement\" OR \"personnel engagement\" which were steered in diverse context\nparticularly financial sector. Through critically reviewing the literature for\nthe last 11 years i.e., 2009-2019, we discovered 91 empirical studies in\nfinancial sector. From these studies, we found the overall concept of\nengagement and its different determinants (e.g., organizational factors,\nindividual factors, job factors) as well as its various outcomes (e.g.,\nemployee outcomes, organizational outcomes). We also formulated a conceptual\nmodel to expand the body of knowledge in the area of employee engagement for a\nbetter understanding of its predictors and outcomes. Besides, limitations of\nthe study and future recommendations are also contemplated.\n"
    },
    {
        "paper_id": 2106.06518,
        "authors": "Luca Merlo, Lea Petrella, Valentina Raponi",
        "title": "Forecasting VaR and ES using a joint quantile regression and\n  implications in portfolio allocation",
        "comments": null,
        "journal-ref": "Journal of Banking & Finance (2021), 106248",
        "doi": "10.1016/j.jbankfin.2021.106248",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we propose a multivariate quantile regression framework to\nforecast Value at Risk (VaR) and Expected Shortfall (ES) of multiple financial\nassets simultaneously, extending Taylor (2019). We generalize the Multivariate\nAsymmetric Laplace (MAL) joint quantile regression of Petrella and Raponi\n(2019) to a time-varying setting, which allows us to specify a dynamic process\nfor the evolution of both VaR and ES of each asset. The proposed methodology\naccounts for the dependence structure among asset returns. By exploiting the\nproperties of the MAL distribution, we then propose a new portfolio\noptimization method that minimizes the portfolio risk and controls for\nwell-known characteristics of financial data. We evaluate the advantages of the\nproposed approach on both simulated and real data, using weekly returns on\nthree major stock market indices. We show that our method outperforms other\nexisting models and provides more accurate risk measure forecasts compared to\nunivariate ones.\n"
    },
    {
        "paper_id": 2106.06599,
        "authors": "Cornelis Dirk van Goeverden",
        "title": "The value of travel speed",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Travel speed is an intrinsic feature of transport, and enlarging the speed is\nconsidered as beneficial. The benefit of a speed increase is generally assessed\nas the value of the saved travel time. However, this approach conflicts with\nthe observation that time spent on travelling is rather constant and might not\nbe affected by speed changes. The paper aims to define the benefits of a speed\nincrease and addresses two research questions. First, how will a speed increase\nin person transport work out, which factors are affected? Second, is the value\nof time a good proxy for the value of speed? Based on studies on time spending\nand research on the association between speed and land use, we argue that human\nwealth could be the main affected factor by speed changes, rather than time or\naccess. Then the value of time is not a good proxy for the value of speed: the\nbenefits of a wealth increase are negatively correlated with prosperity\nfollowing the law of diminishing marginal utility, while the calculated\nbenefits of saved travel time prove to be positively correlated. The inadequacy\nof the value of time is explained by some shortcomings with respect to the\nwillingness to pay that is generally used for assessing the value of time:\npeople do not predict correctly the personal benefits that will be gained from\na decision, and they neglect the social impacts.\n"
    },
    {
        "paper_id": 2106.06665,
        "authors": "Lei Liang, Xuan Zhang, Hongbin Sun",
        "title": "An Integration and Operation Framework of Geothermal Heat Pumps in\n  Distribution Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The application of the energy-efficient thermal and energy management\nmechanism in Geothermal Heat Pumps (GHPs) is indispensable to reduce the\noverall energy consumption and carbon emission across the building sector.\nBesides, in the Virtual Power Plant (VPP) system, the demand response of\nclustered GHP systems can improve the operating flexibility of the power grid.\nThis paper presents an integration and operation framework of GHPs in the\ndistribution network, applying a layered communication and optimization method\nto coordinate multiple clustered GHPs in a community. In the proposed\nhierarchical operation scheme, the operator of regional GHPs collects the\nthermal zone information and the disturbance prediction of buildings in a short\ntime granularity, predicts the energy demand, and transmits the information to\nan aggregator. Using a novel linearized optimal power flow model, the\naggregator coordinates and aggregates load resources of GHP systems in the\ndistribution network. In this way, GHP systems with thermal and energy\nmanagement mechanisms can be applied to achieve the demand response in the VPP\nand offer more energy flexibility to the community.\n"
    },
    {
        "paper_id": 2106.06735,
        "authors": "Samuel Palmer, Serkan Sahin, Rodrigo Hernandez, Samuel Mugel, Roman\n  Orus",
        "title": "Quantum Portfolio Optimization with Investment Bands and Target\n  Volatility",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show how to implement in a simple way some complex real-life\nconstraints on the portfolio optimization problem, so that it becomes amenable\nto quantum optimization algorithms. Specifically, first we explain how to\nobtain the best investment portfolio with a given target risk. This is\nimportant in order to produce portfolios with different risk profiles, as\ntypically offered by financial institutions. Second, we show how to implement\nindividual investment bands, i.e., minimum and maximum possible investments for\neach asset. This is also important in order to impose diversification and avoid\ncorner solutions. Quite remarkably, we show how to build the constrained cost\nfunction as a quadratic binary optimization (QUBO) problem, this being the\nnatural input of quantum annealers. The validity of our implementation is\nproven by finding the optimal portfolios, using D-Wave Hybrid and its Advantage\nquantum processor, on portfolios built with all the assets from S&P100 and\nS&P500. Our results show how practical daily constraints found in quantitative\nfinance can be implemented in a simple way in current NISQ quantum processors,\nwith real data, and under realistic market conditions. In combination with\nclustering algorithms, our methods would allow to replicate the behaviour of\nmore complex indexes, such as Nasdaq Composite or others, in turn being\nparticularly useful to build and replicate Exchange Traded Funds (ETF).\n"
    },
    {
        "paper_id": 2106.06974,
        "authors": "Alexander Barzykin, Philippe Bergault, Olivier Gu\\'eant",
        "title": "Algorithmic market making in dealer markets with hedging and market\n  impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In dealer markets, dealers provide prices at which they agree to buy and sell\nthe assets and securities they have in their scope. With ever increasing\ntrading volume, this quoting task has to be done algorithmically in most\nmarkets such as foreign exchange markets or corporate bond markets. Over the\nlast ten years, many mathematical models have been designed that can be the\nbasis of quoting algorithms in dealer markets. Nevertheless, in most (if not\nall) models, the dealer is a pure internalizer, setting quotes and waiting for\nclients. However, on many dealer markets, dealers also have access to an\ninter-dealer market or even public trading venues where they can hedge part of\ntheir inventory. In this paper, we propose a model taking this possibility into\naccount, therefore allowing dealers to externalize part of their risk. The\nmodel displays an important feature well known to practitioners that within a\ncertain inventory range the dealer internalizes the flow by appropriately\nadjusting the quotes and starts externalizing outside of that range. The larger\nthe franchise, the wider is the inventory range suitable for pure\ninternalization. The model is illustrated numerically with realistic parameters\nfor USDCNH spot market.\n"
    },
    {
        "paper_id": 2106.0704,
        "authors": "Riccardo Marcaccioli, Jean-Philippe Bouchaud and Michael Benzaquen",
        "title": "Exogenous and Endogenous Price Jumps Belong to Different Dynamical\n  Classes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/ac498c",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Synchronising a database of stock specific news with 5 years worth of order\nbook data on 300 stocks, we show that abnormal price movements following news\nreleases (exogenous) exhibit markedly different dynamical features from those\narising spontaneously (endogenous). On average, large volatility fluctuations\ninduced by exogenous events occur abruptly and are followed by a decaying\npower-law relaxation, while endogenous price jumps are characterized by\nprogressively accelerating growth of volatility, also followed by a power-law\nrelaxation, but slower than for exogenous jumps. Remarkably, our results are\nreminiscent of what is observed in different contexts, namely Amazon book sales\nand YouTube views. Finally, we show that fitting power-laws to {\\it individual}\nvolatility profiles allows one to classify large events into endogenous and\nexogenous dynamical classes, without relying on the news feed.\n"
    },
    {
        "paper_id": 2106.07103,
        "authors": "Liao Zhu, Haoxuan Wu, Martin T. Wells",
        "title": "A News-based Machine Learning Model for Adaptive Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a new asset pricing model -- the News Embedding UMAP\nSelection (NEUS) model, to explain and predict the stock returns based on the\nfinancial news. Using a combination of various machine learning algorithms, we\nfirst derive a company embedding vector for each basis asset from the financial\nnews. Then we obtain a collection of the basis assets based on their company\nembedding. After that for each stock, we select the basis assets to explain and\npredict the stock return with high-dimensional statistical methods. The new\nmodel is shown to have a significantly better fitting and prediction power than\nthe Fama-French 5-factor model.\n"
    },
    {
        "paper_id": 2106.07104,
        "authors": "Nektarios Aslanidis, Aurelio F. Bariviera, \\'Oscar G. L\\'opez",
        "title": "The link between Bitcoin and Google Trends attention",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows that Bitcoin is not correlated to a general uncertainty\nindex as measured by the Google Trends data of Castelnuovo and Tran (2017).\nInstead, Bitcoin is linked to a Google Trends attention measure specific for\nthe cryptocurrency market. First, we find a bidirectional relationship between\nGoogle Trends attention and Bitcoin returns up to six days. Second, information\nflows from Bitcoin volatility to Google Trends attention seem to be larger than\ninformation flows in the other direction. These relations hold across different\nsub-periods and different compositions of the proposed Google Trends\nCryptocurrency index.\n"
    },
    {
        "paper_id": 2106.07177,
        "authors": "Wenyong Zhang, Lingfei Li, Gongqiu Zhang",
        "title": "A Two-Step Framework for Arbitrage-Free Prediction of the Implied\n  Volatility Surface",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a two-step framework for predicting the implied volatility surface\nover time without static arbitrage. In the first step, we select features to\nrepresent the surface and predict them over time. In the second step, we use\nthe predicted features to construct the implied volatility surface using a deep\nneural network (DNN) model by incorporating constraints that prevent static\narbitrage. We consider three methods to extract features from the implied\nvolatility data: principal component analysis, variational autoencoder and\nsampling the surface, and we predict these features using LSTM. Using a long\ntime series of implied volatility data for S\\&P500 index options to train our\nmodels, we find two feature construction methods, sampling the surface and\nvariational autoencoders combined with DNN for surface construction, are the\nbest performers in out-of-sample prediction. In particular, they outperform a\nclassical method substantially. Furthermore, the DNN model for surface\nconstruction not only removes static arbitrage, but also significantly reduces\nthe prediction error compared with a standard interpolation method. Our\nframework can also be used to simulate the dynamics of the implied volatility\nsurface without static arbitrage.\n"
    },
    {
        "paper_id": 2106.07191,
        "authors": "Zhengqing Zhou, Jose Blanchet, Peter W. Glynn",
        "title": "Distributionally Robust Martingale Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of bounding path-dependent expectations (within any\nfinite time horizon $d$) over the class of discrete-time martingales whose\nmarginal distributions lie within a prescribed tolerance of a given collection\nof benchmark marginal distributions. This problem is a relaxation of the\nmartingale optimal transport (MOT) problem and is motivated by applications to\nsuper-hedging in financial markets. We show that the empirical version of our\nrelaxed MOT problem can be approximated within $O\\left( n^{-1/2}\\right)$ error\nwhere $n$ is the number of samples of each of the individual marginal\ndistributions (generated independently) and using a suitably constructed\nfinite-dimensional linear programming problem.\n"
    },
    {
        "paper_id": 2106.07354,
        "authors": "Laurence Lacey",
        "title": "The relationship between the US broad money supply and US GDP for the\n  time period 2001 to 2019 with that of the corresponding time series for US\n  national property and stock market indices, using an information entropy\n  methodology",
        "comments": "20 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The primary objective of this paper was to investigate whether the growth in\nthe major US asset indices could be a function of the US broad money supply\nand/or US GDP, over the time period 2001 to 2019, using an information entropy\nmethodology. The four US asset indices investigated were: (1) US National\nProperty index; (2) Russell 2000 index; (3) S&P 500 index; and (4) NASDAQ\nindex. Notwithstanding the financial crisis of 2007-2008, US real GDP increased\nexponentially over the period 2001 to 2019, with an average annual growth rate\nof approximately 2%. However, over this time period, the average annual rate of\ngrowth of US GDP was considerably lower than the average annual rate of growth\nof the US broad money supply (5.7%). The main determinant of the average growth\nrate for all four US asset indices studied would appear to be the growth rate\nin the US broad money supply. In addition, the growth rate in the US Russell\n2000 stock index and the NASDAQ index would appear to be a function of the\ncombined positive effects of both the growth rate in the US Broad Money Supply\nand the growth rate of US GDP.\n"
    },
    {
        "paper_id": 2106.07358,
        "authors": "Mathieu Mercadier, Jean-Pierre Lardy",
        "title": "Credit spread approximation and improvement using random forest\n  regression",
        "comments": null,
        "journal-ref": "European Journal of Operational Research, Elsevier, 2019, 277 (1),\n  pp.351-365",
        "doi": "10.1016/j.ejor.2019.02.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit Default Swap (CDS) levels provide a market appreciation of companies'\ndefault risk. These derivatives are not always available, creating a need for\nCDS approximations. This paper offers a simple, global and transparent CDS\nstructural approximation, which contrasts with more complex and proprietary\napproximations currently in use. This Equity-to-Credit formula (E2C), inspired\nby CreditGrades, obtains better CDS approximations, according to empirical\nanalyses based on a large sample spanning 2016-2018. A random forest regression\nrun with this E2C formula and selected additional financial data results in an\n87.3% out-of-sample accuracy in CDS approximations. The transparency property\nof this algorithm confirms the predominance of the E2C estimate, and the impact\nof companies' debt rating and size, in predicting their CDS.\n"
    },
    {
        "paper_id": 2106.07361,
        "authors": "Jonathan Dumas, Ioannis Boukas, Miguel Manuel de Villena, S\\'ebastien\n  Mathieu, Bertrand Corn\\'elusse",
        "title": "Probabilistic Forecasting of Imbalance Prices in the Belgian Context",
        "comments": null,
        "journal-ref": "2019 16th International Conference on the European Energy Market\n  (EEM). IEEE, 2019",
        "doi": "10.1109/EEM.2019.8916375",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Forecasting imbalance prices is essential for strategic participation in the\nshort-term energy markets. A novel two-step probabilistic approach is proposed,\nwith a particular focus on the Belgian case. The first step consists of\ncomputing the net regulation volume state transition probabilities. It is\nmodeled as a matrix computed using historical data. This matrix is then used to\ninfer the imbalance prices since the net regulation volume can be related to\nthe level of reserves activated and the corresponding marginal prices for each\nactivation level are published by the Belgian Transmission System Operator one\nday before electricity delivery. This approach is compared to a deterministic\nmodel, a multi-layer perceptron, and a widely used probabilistic technique,\nGaussian Processes.\n"
    },
    {
        "paper_id": 2106.07362,
        "authors": "Len Patrick Dominic M. Garces and Gerald H. L. Cheang",
        "title": "A Numerical Approach to Pricing Exchange Options under Stochastic\n  Volatility and Jump-Diffusion Dynamics",
        "comments": "38 pages with 19 figures. To appear in Quantitative Finance. Sections\n  2 to 4 of this paper present a condensed version of similar arguments made in\n  our earlier arXiv preprint (arXiv:2002.10194). arXiv admin note: text overlap\n  with arXiv:2002.10194",
        "journal-ref": null,
        "doi": "10.1080/14697688.2021.1926534",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a method of lines (MOL) approach to determine prices of European\nand American exchange options when underlying asset prices are modelled with\nstochastic volatility and jump-diffusion dynamics. As the MOL, as with any\nother numerical scheme for PDEs, becomes increasingly complex when higher\ndimensions are involved, we first simplify the problem by transforming the\nexchange option into a call option written on the ratio of the yield processes\nof the two assets. This is achieved by taking the second asset yield process as\nthe numeraire. We also characterize the near-maturity behavior of the early\nexercise boundary of the American exchange option and analyze how model\nparameters affect this behavior. Using the MOL scheme, we conduct a numerical\ncomparative static analysis of exchange option prices with respect to the model\nparameters and investigate the impact of stochastic volatility and jumps to\noption prices. We also consider the effect of boundary conditions at\nfar-but-finite limits of the computational domain on the overall efficiency of\nthe MOL scheme. Toward these objectives, a brief exposition of the MOL and how\nit can be implemented on computing software are provided.\n"
    },
    {
        "paper_id": 2106.07377,
        "authors": "Nick James and Max Menzies",
        "title": "A new measure between sets of probability distributions with\n  applications to erratic financial behavior",
        "comments": "Accepted manuscript. Substantial edits since v1. Equal contribution",
        "journal-ref": "J. Stat. Mech. (2021) 123404",
        "doi": "10.1088/1742-5468/ac3d91",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new framework to quantify distance between finite\nsets with uncertainty present, where probability distributions determine the\nlocations of individual elements. Combining this with a Bayesian change point\ndetection algorithm, we produce a new measure of similarity between time series\nwith respect to their structural breaks. First, we demonstrate the algorithm's\neffectiveness on a collection of piecewise autoregressive processes. Next, we\napply this to financial data to study the erratic behavior profiles of 19\ncountries and 11 sectors over the past 20 years. Our measure provides\nquantitative evidence that there is greater collective similarity among\nsectors' erratic behavior profiles than those of countries, which we observe\nupon individual inspection of these time series. Our measure could be used as a\nnew framework or complementary tool for investors seeking to make asset\nallocation decisions for financial portfolios.\n"
    },
    {
        "paper_id": 2106.07679,
        "authors": "Esther Yusuf Enoch, Usman Abubakar Arabo, Abubakar Mahmud Digil",
        "title": "The Effect of Client Appraisal on the Efficiency of Micro Finance Bank",
        "comments": "13 pages journal. arXiv admin note: text overlap with\n  arXiv:2105.10991",
        "journal-ref": "American International Journal of Business Management (AIJBM),\n  ISSN- 2379-106X, www.aijbm.com Volume 4, Issue 06 (June-2021), PP 39-51",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the major problems confronting financial institutions most especially\nmicrofinance institutions is the increasing incidence of loan defaults and\nconsequence loan losses which manifested in their financial performance with\nhuge uncollectible loans and advances. This study assessed the effects of\ncredit management on financial performance on microfinance institutions in\nAdamawa State, Nigeria. Specifically, we examine the effect of client appraisal\non the efficiency of microfinance banks in Adamawa State. The methodology\nemployed in this study is the survey method in which both primary and secondary\nsources were used in the collection of data. A multi-stage sampling method was\nadopted in selecting a sample of 21 respondents from a total population of 52\ncredit officers. Questionnaires were used in the due collection of data from\nthe respondents. Descriptive statistics (simple percentage) and inferential\nstatistics (regression analysis) were used to analyze the data collected and in\ntesting the hypotheses. The study showed that client appraisal has a positive\neffect on efficiency and productivity.\n"
    },
    {
        "paper_id": 2106.08066,
        "authors": "Daniel Woldeab, Robert Yawson and Irina Woldeab",
        "title": "Re-examining the Philosophical Underpinnings of the Melting Pot vs.\n  Multiculturalism in the Current Immigration Debate in the United States",
        "comments": null,
        "journal-ref": "In Harnessing Analytics for Enhancing Healthcare and Business.\n  Proceedings of the 50th Northeast Decision Sciences Institute (NEDSI) Annual\n  Meeting, Pgs. 264 to 285. Virtual Conference, March 26 to 27, 2021",
        "doi": "10.31124/advance.14749101.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Immigration to the United States is certainly not a new phenomenon, and it is\ntherefore natural for immigration, culture and identity to be given due\nattention by the public and policy makers. However, current discussion of\nimmigration, legal and illegal, and the philosophical underpinnings is lost in\ntranslation, not necessarily on ideological lines, but on political\norientation. In this paper we reexamine the philosophical underpinnings of the\nmelting pot versus multiculturalism as antecedents and precedents of current\nimmigration debate and how the core issues are lost in translation. We take a\nbrief look at immigrants and the economy to situate the current immigration\ndebate. We then discuss the two philosophical approaches to immigration and how\nthe understanding of the philosophical foundations can help streamline the\ncurrent immigration debate.\n"
    },
    {
        "paper_id": 2106.08097,
        "authors": "Xavier Warin",
        "title": "Reservoir optimization and Machine Learning methods",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After showing the efficiency of feedforward networks to estimate control in\nhigh dimension in the global optimization of some storages problems, we develop\na modification of an algorithm based on some dynamic programming principle. We\nshow that classical feedforward networks are not effective to estimate Bellman\nvalues for reservoir problems and we propose some neural networks giving far\nbetter results. At last, we develop a new algorithm mixing LP resolution and\nconditional cuts calculated by neural networks to solve some stochastic linear\nproblems.\n"
    },
    {
        "paper_id": 2106.08144,
        "authors": "Orhan Gokmen",
        "title": "The Relationship between Foreign Direct Investment and Economic Growth:\n  A Case of Turkey",
        "comments": null,
        "journal-ref": "International Journal of Economics and Finance; Vol. 13, No. 7;\n  2021",
        "doi": "10.5539/ijef.v13n7p85",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the relationship between net FDI inflows and real GDP for\nTurkey from 1970 to 2019. Although conventional economic growth theories and\nmost empirical research suggest that there is a bi-directional positive effect\nbetween these macro variables, the results indicate that there is a\nuni-directional significant short-run positive effect of real GDP on net FDI\ninflows to Turkey by employing the Vector Error Correction Model, Granger\nCausality, Impulse Response Functions and Variance Decomposition. Also, there\nis no long-run effect has been found. The findings recommend Turkish\nauthorities optimally benefit from the potential positive effect of net\nincoming FDI on the real GDP by allocating it for the productive sectoral\nestablishments while effectively maintaining the country's real economic growth\nto attract further FDI inflows.\n"
    },
    {
        "paper_id": 2106.08157,
        "authors": "Kaihua Qin, Liyi Zhou, Yaroslav Afonin, Ludovico Lazzaretti, Arthur\n  Gervais",
        "title": "CeFi vs. DeFi -- Comparing Centralized to Decentralized Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To non-experts, the traditional Centralized Finance (CeFi) ecosystem may seem\nobscure, because users are typically not aware of the underlying rules or\nagreements of financial assets and products. Decentralized Finance (DeFi),\nhowever, is making its debut as an ecosystem claiming to offer transparency and\ncontrol, which are partially attributable to the underlying integrity-protected\nblockchain, as well as currently higher financial asset yields than CeFi. Yet,\nthe boundaries between CeFi and DeFi may not be always so clear cut.\n  In this work, we systematically analyze the differences between CeFi and\nDeFi, covering legal, economic, security, privacy and market manipulation. We\nprovide a structured methodology to differentiate between a CeFi and a DeFi\nservice. Our findings show that certain DeFi assets (such as USDC or USDT\nstablecoins) do not necessarily classify as DeFi assets, and may endanger the\neconomic security of intertwined DeFi protocols. We conclude this work with the\nexploration of possible synergies between CeFi and DeFi.\n"
    },
    {
        "paper_id": 2106.08296,
        "authors": "Davide Fiaschi, Cristina Tealdi",
        "title": "Young people between education and the labour market during the COVID-19\n  pandemic in Italy",
        "comments": "16 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.1108/IJM-06-2021-0352",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the distribution and the flows between different types of\nemployment (self-employment, temporary, and permanent), unemployment,\neducation, and other types of inactivity, with particular focus on the duration\nof the school-to-work transition (STWT). The aim is to assess the impact of the\nCOVID-19 pandemic in Italy on the careers of individuals aged 15-34. We find\nthat the pandemic worsened an already concerning situation of higher\nunemployment and inactivity rates and significantly longer STWT duration\ncompared to other EU countries, particularly for females and residents in the\nSouth of Italy. In the midst of the pandemic, individuals aged 20-29 were less\nin (permanent and temporary) employment and more in the NLFET (Neither in the\nLabour Force nor in Education or Training) state, particularly females and non\nItalian citizens. We also provide evidence of an increased propensity to return\nto schooling, but most importantly of a substantial prolongation of the STWT\nduration towards permanent employment, mostly for males and non Italian\ncitizens. Our contribution lies in providing a rigorous estimation and analysis\nof the impact of COVID-19 on the carriers of young individuals in Italy, which\nhas not yet been explored in the literature.\n"
    },
    {
        "paper_id": 2106.08361,
        "authors": "Ivan Fursov, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\n  Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\n  Zaytsev, Evgeny Burnaev",
        "title": "Adversarial Attacks on Deep Models for Financial Transaction Records",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Machine learning models using transaction records as inputs are popular among\nfinancial institutions. The most efficient models use deep-learning\narchitectures similar to those in the NLP community, posing a challenge due to\ntheir tremendous number of parameters and limited robustness. In particular,\ndeep-learning models are vulnerable to adversarial attacks: a little change in\nthe input harms the model's output.\n  In this work, we examine adversarial attacks on transaction records data and\ndefences from these attacks. The transaction records data have a different\nstructure than the canonical NLP or time series data, as neighbouring records\nare less connected than words in sentences, and each record consists of both\ndiscrete merchant code and continuous transaction amount. We consider a\nblack-box attack scenario, where the attack doesn't know the true decision\nmodel, and pay special attention to adding transaction tokens to the end of a\nsequence. These limitations provide more realistic scenario, previously\nunexplored in NLP world.\n  The proposed adversarial attacks and the respective defences demonstrate\nremarkable performance using relevant datasets from the financial industry. Our\nresults show that a couple of generated transactions are sufficient to fool a\ndeep-learning model. Further, we improve model robustness via adversarial\ntraining or separate adversarial examples detection. This work shows that\nembedding protection from adversarial attacks improves model robustness,\nallowing a wider adoption of deep models for transaction records in banking and\nfinance.\n"
    },
    {
        "paper_id": 2106.0842,
        "authors": "Bruno P. C. Levy, Hedibert F. Lopes",
        "title": "Trend-Following Strategies via Dynamic Momentum Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time series momentum strategies are widely applied in the quantitative\nfinancial industry and its academic research has grown rapidly since the work\nof Moskowitz, Ooi and Pedersen (2012). However, trading signals are usually\nobtained via simple observation of past return measurements. In this article we\nstudy the benefits of incorporating dynamic econometric models to sequentially\nlearn the time-varying importance of different look-back periods for individual\nassets. By the use of a dynamic binary classifier model, the investor is able\nto switch between time-varying or constant relations between past momentum and\nfuture returns, dynamically combining or selecting different momentum speeds\nduring turning points, improving trading signals accuracy and portfolio\nperformance. Using data from 56 future contracts we show that a mean-variance\ninvestor will be willing to pay a considerable management fee to switch from\nthe traditional naive time series momentum strategy to the dynamic classifier\napproach.\n"
    },
    {
        "paper_id": 2106.08421,
        "authors": "Julien Hok and Sergei Kucherenko",
        "title": "Pricing and Risk Analysis in Hyperbolic Local Volatility Model with\n  Quasi Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Local volatility models usually capture the surface of implied volatilities\nmore accurately than other approaches, such as stochastic volatility models. We\npresent the results of application of Monte Carlo (MC) and Quasi Monte Carlo\n(QMC) methods for derivative pricing and risk analysis based on Hyperbolic\nLocal Volatility Model. In high-dimensional integration QMC shows a superior\nperformance over MC if the effective dimension of an integrand is not too\nlarge. In application to derivative pricing and computation of Greeks effective\ndimensions depend on path discretization algorithms. The results presented for\nthe Asian option show the superior performance of the Quasi Monte Carlo methods\nespecially for the Brownian Bridge discretization scheme.\n"
    },
    {
        "paper_id": 2106.08437,
        "authors": "Ali Hirsa, Joerg Osterrieder, Branka Hadji-Misheva, Jan-Alexander\n  Posth",
        "title": "Deep reinforcement learning on a multi-asset environment for trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Financial trading has been widely analyzed for decades with market\nparticipants and academics always looking for advanced methods to improve\ntrading performance. Deep reinforcement learning (DRL), a recently\nreinvigorated method with significant success in multiple domains, still has to\nshow its benefit in the financial markets. We use a deep Q-network (DQN) to\ndesign long-short trading strategies for futures contracts. The state space\nconsists of volatility-normalized daily returns, with buying or selling being\nthe reinforcement learning action and the total reward defined as the\ncumulative profits from our actions. Our trading strategy is trained and tested\nboth on real and simulated price series and we compare the results with an\nindex benchmark. We analyze how training based on a combination of artificial\ndata and actual price series can be successfully deployed in real markets. The\ntrained reinforcement learning agent is applied to trading the E-mini S&P 500\ncontinuous futures contract. Our results in this study are preliminary and need\nfurther improvement.\n"
    },
    {
        "paper_id": 2106.08563,
        "authors": "Wei He, Xiang Sun, Yeneng Sun, Yishu Zeng",
        "title": "Characterization of equilibrium existence and purification in general\n  Bayesian games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies Bayesian games with general action spaces, correlated\ntypes and interdependent payoffs. We introduce the condition of ``decomposable\ncoarser payoff-relevant information'', and show that this condition is both\nsufficient and necessary for the existence of pure-strategy equilibria and\npurification from behavioral strategies. As a consequence of our purification\nmethod, a new existence result on pure-strategy equilibria is also obtained for\ndiscontinuous Bayesian games. Illustrative applications of our results to\noligopolistic competitions and all-pay auctions are provided.\n"
    },
    {
        "paper_id": 2106.08778,
        "authors": "Isobel Seabrook, Fabio Caccioli, Tomaso Aste",
        "title": "An Information Filtering approach to stress testing: an application to\n  FTSE markets",
        "comments": "17 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel methodology to quantify the \"impact\" of and \"response\" to\nmarket shocks. We apply shocks to a group of stocks in a part of the market,\nand we quantify the effects in terms of average losses on another part of the\nmarket using a sparse probabilistic elliptical model for the multivariate\nreturn distribution of the whole market. Sparsity is introduced with an\n$L_0$-norm regularization, which forces to zero some elements of the inverse\ncovariance according to a dependency structure inferred from an information\nfiltering network. Our study concerns the FTSE 100 and 250 markets and analyzes\nimpact and response to shocks both applied to and received from individual\nstocks and group of stocks. We observe that the shock pattern is related to the\nstructure of the network associated with the sparse structure of the inverse\ncovariance of stock returns. Central sectors appear more likely to be affected\nby shocks, and stocks with a large level of underlying diversification have a\nlarger impact on the rest of the market when experiencing shocks. By analyzing\nthe system during times of crisis and comparative market calmness, we observe\nchanges in the shock patterns with a convergent behavior in times of crisis.\n"
    },
    {
        "paper_id": 2106.08883,
        "authors": "Stefano Carattini, Sam Fankhauser, Jianjian Gao, Caterina Gennaioli,\n  and Pietro Panzarasa",
        "title": "What does Network Analysis teach us about International Environmental\n  Cooperation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past 70 years, the number of international environmental agreements\n(IEAs) has increased substantially, highlighting their prominent role in\nenvironmental governance. This paper applies the toolkit of network analysis to\nidentify the network properties of international environmental cooperation\nbased on 546 IEAs signed between 1948 and 2015. We identify four stylised facts\nthat offer topological corroboration for some key themes in the IEA literature.\nFirst, we find that a statistically significant cooperation network did not\nemerge until early 1970, but since then the network has grown continuously in\nstrength, resulting in higher connectivity and intensity of cooperation between\nsignatory countries. Second, over time the network has become closer, denser\nand more cohesive, allowing more effective policy coordination and knowledge\ndiffusion. Third, the network, while global, has a noticeable European imprint:\ninitially the United Kingdom and more recently France and Germany have been the\nmost strategic players to broker environmental cooperation. Fourth,\ninternational environmental coordination started with the management of\nfisheries and the sea, but is now most intense on waste and hazardous\nsubstances. The network of air and atmosphere treaties is weaker on a number of\nmetrics and lacks the hierarchical structure found in other networks. It is the\nonly network whose topological properties are shaped significantly by\nUN-sponsored treaties.\n"
    },
    {
        "paper_id": 2106.089,
        "authors": "Lukas Gonon",
        "title": "Random feature neural networks learn Black-Scholes type PDEs without\n  curse of dimensionality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates the use of random feature neural networks for\nlearning Kolmogorov partial (integro-)differential equations associated to\nBlack-Scholes and more general exponential L\\'evy models. Random feature neural\nnetworks are single-hidden-layer feedforward neural networks in which only the\noutput weights are trainable. This makes training particularly simple, but (a\npriori) reduces expressivity. Interestingly, this is not the case for\nBlack-Scholes type PDEs, as we show here. We derive bounds for the prediction\nerror of random neural networks for learning sufficiently non-degenerate\nBlack-Scholes type models. A full error analysis is provided and it is shown\nthat the derived bounds do not suffer from the curse of dimensionality. We also\ninvestigate an application of these results to basket options and validate the\nbounds numerically.\n  These results prove that neural networks are able to \\textit{learn} solutions\nto Black-Scholes type PDEs without the curse of dimensionality. In addition,\nthis provides an example of a relevant learning problem in which random feature\nneural networks are provably efficient.\n"
    },
    {
        "paper_id": 2106.08945,
        "authors": "Edward J. Oughton",
        "title": "The Economic Impact of Critical National Infrastructure Failure Due to\n  Space Weather",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Space weather is a collective term for different solar or space phenomena\nthat can detrimentally affect technology. However, current understanding of\nspace weather hazards is still relatively embryonic in comparison to\nterrestrial natural hazards such as hurricanes or earthquakes. Indeed, certain\ntypes of space weather such as large Coronal Mass Ejections (CMEs) are an\narchetypal example of a low probability, high severity hazard. Few major\nevents, short time-series data and a lack of consensus regarding the potential\nimpacts on critical infrastructure have hampered the economic impact assessment\nof space weather. Yet, space weather has the potential to disrupt a wide range\nof Critical National Infrastructure (CNI) systems including electricity\ntransmission, satellite communications and positioning, aviation and rail\ntransportation. Recently there has been growing interest in these potential\neconomic and societal impacts. Estimates range from millions of dollars of\nequipment damage from the Quebec 1989 event, to some analysts reporting\nbillions of lost dollars in the wider economy from potential future disaster\nscenarios. Hence, this provides motivation for this article which tracks the\norigin and development of the socio-economic evaluation of space weather, from\n1989 to 2017, and articulates future research directions for the field.\n"
    },
    {
        "paper_id": 2106.09055,
        "authors": "Jaehyung Choi, Hyangju Kim, Young Shin Kim",
        "title": "Diversified reward-risk parity in portfolio construction",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce diversified risk parity embedded with various reward-risk\nmeasures and more generic allocation rules for portfolio construction. We\nempirically test the proposed reward-risk parity strategies and compare their\nperformance with an equally-weighted risk portfolio in various asset universes.\nThe reward-risk parity strategies we tested exhibit consistent outperformance\nevidenced by higher average returns, Sharpe ratios, and Calmar ratios. The\nalternative allocations also reflect less downside risks in Value-at-Risk,\nconditional Value-at-Risk, and maximum drawdown. In addition to the enhanced\nperformance and reward-risk profile, transaction costs can be reduced by\nlowering turnover rates. The diversified reward-risk parity allocations gain\nsuperior performance in the Carhart four-factor analysis.\n"
    },
    {
        "paper_id": 2106.09128,
        "authors": "Yuan Hu, Abootaleb Shirvani, W. Brent Lindquist, Frank J. Fabozzi, and\n  Svetlozar T. Rachev",
        "title": "Market Complete Option Valuation using a Jarrow-Rudd Pricing Tree with\n  Skewness and Kurtosis",
        "comments": "25 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Applying the Cherny-Shiryaev-Yor invariance principle, we introduce a\ngeneralized Jarrow-Rudd (GJR) option pricing model with uncertainty driven by a\nskew random walk. The GJR pricing tree exhibits skewness and kurtosis in both\nthe natural and risk-neutral world. We construct implied surfaces for the\nparameters determining the GJR tree. Motivated by Merton's pricing tree\nincorporating transaction costs, we extend the GJR pricing model to include a\nhedging cost. We demonstrate ways to fit the GJR pricing model to a market\ndriver that influences the price dynamics of the underlying asset. We\nsupplement our findings with numerical examples.\n"
    },
    {
        "paper_id": 2106.09132,
        "authors": "Chenyanzi Yu, Tianyang Xie",
        "title": "Multivariate Pair Trading by Volatility & Model Adaption Trade-off",
        "comments": "Submitting to Journal of Financial Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Pair trading is one of the most discussed topics among financial researches.\nDespite a growing base of work, portfolio management for multivariate time\nseries is rarely discussed. On the other hand, most researches focus on\nrefining strategy rules instead of finding the optimal portfolio weight. In\nthis paper, we brought up a simple yet profitable strategy called Volatility &\nModel Adaption Trade-off (VMAT) to leverage the issues. Experiment studies show\nits superior profit performance over baselines.\n"
    },
    {
        "paper_id": 2106.09163,
        "authors": "Pablo Henr\\'iquez, Jorge Sabat, Jos\\'e Patr\\`icio Sullivan",
        "title": "Politicians' Willingness to Agree: Evidence from the interactions in\n  Twitter of Chilean Deputies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Measuring the number of \"likes\" in Twitter and the number of bills voted in\nfavor by the members of the Chilean Chambers of Deputies. We empirically study\nhow signals of agreement in Twitter translates into cross-cutting voting during\na high political polarization period of time. Our empirical analysis is guided\nby a spatial voting model that can help us to understand Twitter as a market of\nsignals. Our model, which is standard for the public choice literature,\nintroduces authenticity, an intrinsic factor that distort politicians'\nwilligness to agree (Trilling, 2009). As our main contribution, we document\nempirical evidence that \"likes\" between opponents are positively related to the\nnumber of bills voted by the same pair of politicians in Congress, even when we\ncontrol by politicians' time-invariant characteristics, coalition affiliation\nand following links in Twitter. Our results shed light into several contingent\ntopics, such as polarization and disagreement within the public sphere.\n"
    },
    {
        "paper_id": 2106.0925,
        "authors": "Ahmet Goncu",
        "title": "Effects of Covid-19 Pandemic on Chinese Commodity Futures Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, empirical moments and the cointegration for all the liquid\ncommodity futures traded in the Chinese futures markets are analyzed for the\nperiods before and after Covid-19, which is important for trading strategies\nsuch as pairs trading. The results show that the positive change in the average\nreturns of the products such as soybean, corn, corn starch, and iron ore\nfutures are significantly stronger than other products in the post Covid-19\nera, whereas the volatility increased most for silver, petroleum asphalt and\negg futures after the pandemic started. The number of cointegrated pairs are\nreduced after the pandemic indicating the differentiation in returns due to the\nstructural changes caused in the demand and supply conditions across\ncommodities.\n"
    },
    {
        "paper_id": 2106.09267,
        "authors": "Eyal Neuman and Moritz Vo{\\ss}",
        "title": "Trading with the Crowd",
        "comments": "73 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We formulate and solve a multi-player stochastic differential game between\nfinancial agents who seek to cost-efficiently liquidate their position in a\nrisky asset in the presence of jointly aggregated transient price impact, along\nwith taking into account a common general price predicting signal. The unique\nNash-equilibrium strategies reveal how each agent's liquidation policy adjusts\nthe predictive trading signal to the aggregated transient price impact induced\nby all other agents. This unfolds a quantitative relation between trading\nsignals and the order flow in crowded markets. We also formulate and solve the\ncorresponding mean field game in the limit of infinitely many agents. We prove\nthat the equilibrium trading speed and the value function of an agent in the\nfinite $N$-player game converges to the corresponding trading speed and value\nfunction in the mean field game at rate $O(N^{-2})$. In addition, we prove that\nthe mean field optimal strategy provides an approximate Nash-equilibrium for\nthe finite-player game.\n"
    },
    {
        "paper_id": 2106.09437,
        "authors": "Antonio Di Cintio, Marco Torri, Federico Falcini, Raffaele Corrado,\n  Guglielmo Lacorata, Angela Cuttitta, Bernardo Patti, Rosalia Santoleri",
        "title": "Hydrographic variability and biomass fluctuations of European anchovy\n  (Engraulis encrasicolus) in the Central Mediterranean Sea: Monetary\n  estimations and impacts on fishery from Lagrangian analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  During the last decades, scientific community has been investigating both\nbiological and hydrographic processes that affect fisheries. Such an\ninterdisciplinary and synergic approach is nowadays giving a fundamental\ncontribution, in particular, in connecting the dots between hydrographic\nphenomena and biomass variability and distribution of small pelagic fish. Here\nwe estimate impacts of hydrographic fluctuations on small pelagic fishery,\nfocusing on the inter-annual variability that characterizes connectivity\nbetween spawning and recruiting areas for the European anchovy (Engraulis\nencrasicolus, Linnaues 1758), in the Northern side of the Sicily Channel\n(Mediterranean Sea). Results show that coastal transport dynamics of a specific\nyear largely affect the biomass recorded the following year. Our work,\nmoreover, quantifies the specific monetary impacts on landings of European\nanchovy fishery due to hydrodynamics variability, connecting biomass\nfluctuations with fishery economics in a highly dynamic and exploited marine\nenvironment as the Sicily Channel. In particular, we build a model that\nattributes a monetary value to the hydrographic phenomena (i.e., cross-shore\nvs. alongshore eggs and larvae transport), registered in the FAO Geographical\nSub-Area (GSA) 16 (Southern Sicily). This allows us to provide a monetary\nestimation of catches, derived from different transport dynamics. Our results\nhighlight the paramount importance that hydrographic phenomena can have over\nthe socio-economic performance of a fishery.\n"
    },
    {
        "paper_id": 2106.09498,
        "authors": "Oleg Antonov, Ekaterina Lineva",
        "title": "The Concept, Types and Structure of Corruption",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article analyzes the essence of the phenomenon of corruption, highlights\nits main varieties and characteristics. The authors of the study apply\nhistorical analysis, emphasizing the long-term nature of corruption and its\nhistorical roots. The paper uses legal analysis to characterize the legal\ninterpretation of corruption as an economic crime.\n"
    },
    {
        "paper_id": 2106.09664,
        "authors": "Jaydip Sen and Sidra Mehtab",
        "title": "Design and Analysis of Robust Deep Learning Models for Stock Price\n  Prediction",
        "comments": "This is the pre-print of our chapter that has been accepted for\n  publication in the forthcoming book entitled \"Machine Learning: Algorithms,\n  Models, and Applications\". The book will be published by IntechOpen, London,\n  UK, in an open access in the later part of the year 2021. The chapter is 29\n  pages long, and it has 20 figures and 21 tables. arXiv admin note:\n  substantial text overlap with arXiv:2103.15096",
        "journal-ref": null,
        "doi": "10.5772/intechopen.99982",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Building predictive models for robust and accurate prediction of stock prices\nand stock price movement is a challenging research problem to solve. The\nwell-known efficient market hypothesis believes in the impossibility of\naccurate prediction of future stock prices in an efficient stock market as the\nstock prices are assumed to be purely stochastic. However, numerous works\nproposed by researchers have demonstrated that it is possible to predict future\nstock prices with a high level of precision using sophisticated algorithms,\nmodel architectures, and the selection of appropriate variables in the models.\nThis chapter proposes a collection of predictive regression models built on\ndeep learning architecture for robust and precise prediction of the future\nprices of a stock listed in the diversified sectors in the National Stock\nExchange (NSE) of India. The Metastock tool is used to download the historical\nstock prices over a period of two years (2013- 2014) at 5 minutes intervals.\nWhile the records for the first year are used to train the models, the testing\nis carried out using the remaining records. The design approaches of all the\nmodels and their performance results are presented in detail. The models are\nalso compared based on their execution time and accuracy of prediction.\n"
    },
    {
        "paper_id": 2106.09783,
        "authors": "Agostino Capponi, Sveinn Olafsson, and Humoud Alsabah",
        "title": "Proof-of-Work Cryptocurrencies: Does Mining Technology Undermine\n  Decentralization?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Does the proof-of-work protocol serve its intended purpose of supporting\ndecentralized cryptocurrency mining? To address this question, we develop a\ngame-theoretical model where miners first invest in hardware to improve the\nefficiency of their operations, and then compete for mining rewards in a\nrent-seeking game. We argue that because of capacity constraints faced by\nminers, centralization in mining is lower than indicated by both public\ndiscourse and recent academic work. We show that advancements in hardware\nefficiency do not necessarily lead to larger miners increasing their advantage,\nbut rather allow smaller miners to expand and new miners to enter the\ncompetition. Our calibrated model illustrates that hardware efficiency has a\nsmall impact on the cost of attacking a network, while the mining reward has a\nsignificant impact. This highlights the vulnerability of smaller and emerging\ncryptocurrencies, as well as of established cryptocurrencies transitioning to a\nfee-based mining reward scheme.\n"
    },
    {
        "paper_id": 2106.09978,
        "authors": "Lijun Bo, Tongqing Li, Xiang Yu",
        "title": "Centralized systemic risk control in the interbank system: Weak\n  formulation and Gamma-convergence",
        "comments": "Final version, forthcoming in Stochastic Processes and their\n  Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a systemic risk control problem by the central bank, which\ndynamically plans monetary supply to stabilize the interbank system with\nborrowing and lending activities. Facing both heterogeneity among banks and the\ncommon noise, the central bank aims to find an optimal strategy to minimize the\naverage distance between log-monetary reserves of all banks and the benchmark\nof some target steady levels. A weak formulation is adopted, and an optimal\nrandomized control can be obtained in the system with finite banks by applying\nEkeland's variational principle. As the number of banks grows large, we prove\nthe convergence of optimal strategies using the Gamma-convergence argument,\nwhich yields an optimal weak control in the mean field model. It is shown that\nthis mean field optimal control is associated to the solution of a stochastic\nFokker-Planck-Kolmogorov (FPK) equation, for which the uniqueness of the\nsolution is established under some mild conditions.\n"
    },
    {
        "paper_id": 2106.10012,
        "authors": "Hideaki Aoyama",
        "title": "XRP Network and Proposal of Flow Index",
        "comments": "15 pages, 15 Figures, for \"Blockchain Kyoto 2011\" JPS Conference\n  Proceedings",
        "journal-ref": null,
        "doi": "10.7566/JPSCP.36.011003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  XRP is a modern crypto-asset (crypto-currency) developed by Ripple Labs,\nwhich has been increasing its financial presence. We study its transaction\nhistory available as ledger data. An analysis of its basic statistics,\ncorrelations, and network properties are presented. Motivated by the behavior\nof some nodes with histories of large transactions, we propose a new index: the\n``Flow Index.'' The Flow Index is a pair of indices suitable for characterizing\ntransaction frequencies as a source and destination of a node. Using this Flow\nIndex, we study the global structure of the XRP network and construct\nbow-tie/walnut structure.\n"
    },
    {
        "paper_id": 2106.10024,
        "authors": "Eva L\\\"utkebohmert, Thorsten Schmidt, Julian Sester",
        "title": "Robust deep hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study pricing and hedging under parameter uncertainty for a class of\nMarkov processes which we call generalized affine processes and which includes\nthe Black-Scholes model as well as the constant elasticity of variance (CEV)\nmodel as special cases. Based on a general dynamic programming principle, we\nare able to link the associated nonlinear expectation to a variational form of\nthe Kolmogorov equation which opens the door for fast numerical pricing in the\nrobust framework.\n  The main novelty of the paper is that we propose a deep hedging approach\nwhich efficiently solves the hedging problem under parameter uncertainty. We\nnumerically evaluate this method on simulated and real data and show that the\nrobust deep hedging outperforms existing hedging approaches, in particular in\nhighly volatile periods.\n"
    },
    {
        "paper_id": 2106.1003,
        "authors": "Alex Garivaltis",
        "title": "Universal Risk Budgeting",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I juxtapose Cover's vaunted universal portfolio selection algorithm (Cover\n1991) with the modern representation (Qian 2016; Roncalli 2013) of a portfolio\nas a certain allocation of risk among the available assets, rather than a mere\nallocation of capital. Thus, I define a Universal Risk Budgeting scheme that\nweights each risk budget (instead of each capital budget) by its historical\nperformance record (a la Cover). I prove that my scheme is mathematically\nequivalent to a novel type of Cover and Ordentlich 1996 universal portfolio\nthat uses a new family of prior densities that have hitherto not appeared in\nthe literature on universal portfolio theory. I argue that my universal risk\nbudget, so-defined, is a potentially more perspicuous and flexible type of\nuniversal portfolio; it allows the algorithmic trader to incorporate, with\nadvantage, his prior knowledge (or beliefs) about the particular covariance\nstructure of instantaneous asset returns. Say, if there is some dispersion in\nthe volatilities of the available assets, then the uniform (or Dirichlet)\npriors that are standard in the literature will generate a dangerously lopsided\nprior distribution over the possible risk budgets. In the author's opinion, the\nproposed \"Garivaltis prior\" makes for a nice improvement on Cover's timeless\nexpert system (Cover 1991), that is properly agnostic and open (from the very\nget-go) to different risk budgets. Inspired by Jamshidian 1992, the universal\nrisk budget is formulated as a new kind of exotic option in the continuous time\nBlack and Scholes 1973 market, with all the pleasure, elegance, and convenience\nthat that entails.\n"
    },
    {
        "paper_id": 2106.10033,
        "authors": "Mauricio Elizalde, Carlos Escudero",
        "title": "Chances for the honest in honest versus insider trading",
        "comments": null,
        "journal-ref": "SIAM J. Financial Math., Vol. 13, No. 2, pp. SC39-SC52 (2022)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a Black-Scholes market with a finite time horizon and two investors:\nan honest and an insider trader. We analyze it with anticipating stochastic\ncalculus in two steps. First, we recover the classical result on portfolio\noptimization that shows that the expected logarithmic utility of the insider is\nstrictly greater than that of the honest trader. Then, we prove that, whenever\nthe market is viable, the honest trader can get a higher logarithmic utility,\nand therefore more wealth, than the insider with a strictly positive\nprobability. Our proof relies on the analysis of a sort of forward integral\nvariant of the Dol\\'eans-Dade exponential process. The main financial\nconclusion is that the logarithmic utility is perhaps too conservative for some\ninsiders.\n"
    },
    {
        "paper_id": 2106.10091,
        "authors": "Natsuki Arai and Shian Chang and Biing-Shen Kuo",
        "title": "Introductory Economics: Gender, Majors, and Future Performance",
        "comments": "18 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  By investigating the exam scores of introductory economics in a business\nschool in Taiwan between 2008 and 2019, we find three sets of results: First,\nwe find no significant difference between genders in the exam scores. Second,\nstudents' majors are significantly associated with their exam scores, which\nlikely reflects their academic ability measured at college admission. Third,\nthe exam scores are strong predictors of students' future academic performance.\n"
    },
    {
        "paper_id": 2106.10141,
        "authors": "Daniel Goller, Tamara Harrer, Michael Lechner, Joachim Wolff",
        "title": "Active labour market policies for the long-term unemployed: New evidence\n  from causal machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Active labor market programs are important instruments used by European\nemployment agencies to help the unemployed find work. Investigating large\nadministrative data on German long-term unemployed persons, we analyze the\neffectiveness of three job search assistance and training programs using Causal\nMachine Learning. Participants benefit from quickly realizing and long-lasting\npositive effects across all programs, with placement services being the most\neffective. For women, we find differential effects in various characteristics.\nEspecially, women benefit from better local labor market conditions. We propose\nmore effective data-driven rules for allocating the unemployed to the\nrespective labor market programs that could be employed by decision-makers.\n"
    },
    {
        "paper_id": 2106.10236,
        "authors": "Anand Deo, Karthyek Murthy",
        "title": "Efficient Black-Box Importance Sampling for VaR and CVaR Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper considers Importance Sampling (IS) for the estimation of tail\nrisks of a loss defined in terms of a sophisticated object such as a machine\nlearning feature map or a mixed integer linear optimisation formulation.\nAssuming only black-box access to the loss and the distribution of the\nunderlying random vector, the paper presents an efficient IS algorithm for\nestimating the Value at Risk and Conditional Value at Risk. The key challenge\nin any IS procedure, namely, identifying an appropriate change-of-measure, is\nautomated with a self-structuring IS transformation that learns and replicates\nthe concentration properties of the conditional excess from less rare samples.\nThe resulting estimators enjoy asymptotically optimal variance reduction when\nviewed in the logarithmic scale. Simulation experiments highlight the efficacy\nand practicality of the proposed scheme\n"
    },
    {
        "paper_id": 2106.10474,
        "authors": "P.G.J. Persoon, R.N.A. Bekkers and F. Alkemade",
        "title": "The Knowledge Mobility of Renewable Energy Technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the race to achieve climate goals, many governments and organizations are\nencouraging the local development of Renewable Energy Technology (RET). The\nspatial innovation dynamics of the development of a technology partly depends\non the characteristics of the knowledge base on which this technology builds,\nin particular the analyticity and cumulativeness of knowledge. Theoretically,\ngreater analyticity and lesser cumulativeness are positively associated with\nmore widespread development. In this study, we first empirically evaluate these\nrelations for general technology and then systematically determine the\nknowledge base characteristics for a set of 14 different RETs. We find that,\nwhile several RETs (photovoltaics, fuel-cells, energy storage) have a highly\nanalytic knowledge base and develop more widespread, there are also important\nRETs (wind turbines, solar thermal, geothermal and hydro energy) for which the\nknowledge base is less analytic and which develop less widespread. Likewise,\nthe technological cumulativeness tends to be lower for the former than for the\nlatter group. This calls for regional and country-level policies to be specific\nfor different RETs, taking for a given RET into account both the type of\nknowledge it builds on as well as the local presence of this knowledge.\n"
    },
    {
        "paper_id": 2106.10491,
        "authors": "Andrew Paskaramoorthy, Tim Gebbie, Terence van Zyl",
        "title": "The efficient frontiers of mean-variance portfolio rules under\n  distribution misspecification",
        "comments": "8 pages, 2 figures, 4 tables, submitted to Fusion2021",
        "journal-ref": "2021 IEEE 24th International Conference on Information Fusion\n  (FUSION), 2021, pp. 1-8",
        "doi": "10.23919/FUSION49465.2021.9626916",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Mean-variance portfolio decisions that combine prediction and optimisation\nhave been shown to have poor empirical performance. Here, we consider the\nperformance of various shrinkage methods by their efficient frontiers under\ndifferent distributional assumptions to study the impact of reasonable\ndepartures from Normality. Namely, we investigate the impact of first-order\nauto-correlation, second-order auto-correlation, skewness, and excess kurtosis.\nWe show that the shrinkage methods tend to re-scale the sample efficient\nfrontier, which can change based on the nature of local perturbations from\nNormality. This re-scaling implies that the standard approach of comparing\ndecision rules for a fixed level of risk aversion is problematic, and more so\nin a dynamic market setting. Our results suggest that comparing efficient\nfrontiers has serious implications which oppose the prevailing thinking in the\nliterature. Namely, that sample estimators out-perform Stein type estimators of\nthe mean, and that improving the prediction of the covariance has greater\nimportance than improving that of the means.\n"
    },
    {
        "paper_id": 2106.10498,
        "authors": "Daniel Sevcovic, Cyril Izuchukwu Udeani",
        "title": "Multidimensional linear and nonlinear partial integro-differential\n  equation in Bessel potential spaces with applications in option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to analyze solutions of a non-local nonlinear\npartial integro-differential equation (PIDE) in multidimensional spaces. Such\nclass of PIDE often arises in financial modeling. We employ the theory of\nabstract semilinear parabolic equations in order to prove existence and\nuniqueness of solutions in the scale of Bessel potential spaces. We consider a\nwide class of L\\'evy measures satisfying suitable growth conditions near the\norigin and infinity. The novelty of the paper is the generalization of already\nknown results in the one space dimension to the multidimensional case. We\nconsider Black-Scholes models for option pricing on underlying assets following\na L\\'evy stochastic process with jumps. As an application to option pricing in\nthe one-dimensional space, we consider a general shift function arising from\nnonlinear option pricing models taking into account a large trader\nstock-trading strategy. We prove existence and uniqueness of a solution to the\nnonlinear PIDE in which the shift function may depend on a prescribed large\ninvestor stock-trading strategy function.\n"
    },
    {
        "paper_id": 2106.10841,
        "authors": "Md Shahadath Hossain, Plamen Nikolov",
        "title": "Entitled to Property: How Breaking the Gender Barrier Improves Child\n  Health in India",
        "comments": "56 pages, 2 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Non-unitary household models suggest that enhancing women's bargaining power\ncan influence child health, a crucial determinant of human capital and economic\nstanding throughout adulthood. We examine the effects of a policy shift, the\nHindu Succession Act Amendment (HSAA), which granted inheritance rights to\nunmarried women in India, on child health. Our findings indicate that the HSAA\nimproved children's height and weight. Furthermore, we uncover evidence\nsupporting a mechanism whereby the policy bolstered women's intra-household\nbargaining power, resulting in downstream benefits through enhanced parental\ncare for children and improved child health. These results emphasize that\nchildren fare better when mothers control a larger share of family resources.\nPolicies empowering women can yield additional positive externalities for\nchildren's human capital.\n"
    },
    {
        "paper_id": 2106.10844,
        "authors": "Masud Alam",
        "title": "Output, Employment, and Price Effects of U.S. Narrative Tax Changes: A\n  Factor-Augmented Vector Autoregression Approach",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper examines the short- and long-run effects of U.S. federal personal\nincome and corporate income tax cuts on a wide array of economic policy\nvariables in a data-rich environment. Using a panel of U.S. macroeconomic data\nset, made up of 132 quarterly macroeconomic series for 1959-2018, the study\nestimates factor-augmented vector autoregression (FAVARs) models where an\nextended narrative tax changes dataset combined with unobserved factors. The\nnarrative approach classifies if tax changes are exogenous or endogenous. This\npaper identifies narrative tax shocks in the vector autoregression model using\nthe sign restrictions with Uhlig's (2005) penalty function. Empirical findings\nshow a significant expansionary effect of tax cuts on the macroeconomic\nvariables. Cuts in personal and corporate income taxes cause a rise in output,\ninvestment, employment, and consumption; however, cuts in personal taxes appear\nto be a more effective fiscal policy tool than the cut in corporate income\ntaxes. Real GDP, employment, investment, and industrial production increase\nsignificantly and reach their maximum response values two years after personal\nincome tax cuts. The effects of corporate tax cuts have relatively smaller\neffects on output and consumption but show immediate and higher effects on\nfixed investment and price levels.\n"
    },
    {
        "paper_id": 2106.11012,
        "authors": "Emily Breza, Fatima Cody Stanford, Marcela Alsan, M.D. Ph.D., Burak\n  Alsan, Abhijit Banerjee, Arun G. Chandrasekhar, Sarah Eichmeyer, Traci\n  Glushko, Paul Goldsmith-Pinkham, Kelly Holland, Emily Hoppe, Mohit Karnani,\n  Sarah Liegl, Tristan Loisel, Lucy Ogbu-Nwobodo, Benjamin A. Olken Carlos\n  Torres, Pierre-Luc Vautrey, Erica Warner, Susan Wootton, Esther Duflo",
        "title": "Doctors and Nurses Social Media Ads Reduced Holiday Travel and COVID-19\n  infections: A cluster randomized controlled trial in 13 States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  During the COVID-19 epidemic, many health professionals started using mass\ncommunication on social media to relay critical information and persuade\nindividuals to adopt preventative health behaviors. Our group of clinicians and\nnurses developed and recorded short video messages to encourage viewers to stay\nhome for the Thanksgiving and Christmas Holidays. We then conducted a two-stage\nclustered randomized controlled trial in 820 counties (covering 13 States) in\nthe United States of a large-scale Facebook ad campaign disseminating these\nmessages. In the first level of randomization, we randomly divided the counties\ninto two groups: high intensity and low intensity. In the second level, we\nrandomly assigned zip codes to either treatment or control such that 75% of zip\ncodes in high intensity counties received the treatment, while 25% of zip codes\nin low intensity counties received the treatment. In each treated zip code, we\nsent the ad to as many Facebook subscribers as possible (11,954,109 users\nreceived at least one ad at Thanksgiving and 23,302,290 users received at least\none ad at Christmas). The first primary outcome was aggregate holiday travel,\nmeasured using mobile phone location data, available at the county level: we\nfind that average distance travelled in high-intensity counties decreased by\n-0.993 percentage points (95% CI -1.616, -0.371, p-value 0.002) the three days\nbefore each holiday. The second primary outcome was COVID-19 infection at the\nzip-code level: COVID-19 infections recorded in the two-week period starting\nfive days post-holiday declined by 3.5 percent (adjusted 95% CI [-6.2 percent,\n-0.7 percent], p-value 0.013) in intervention zip codes compared to control zip\ncodes.\n"
    },
    {
        "paper_id": 2106.1112,
        "authors": "Kaiying Lin, Beibei Wang, Pengcheng You",
        "title": "Mechanism Design for Efficient Nash Equilibrium in Oligopolistic Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the efficiency loss in social cost caused by\nstrategic bidding behavior of individual participants in a supply-demand\nbalancing market, and proposes a mechanism to fully recover equilibrium social\noptimum via subsidization and taxation. We characterize the competition among\nsupply-side firms to meet given inelastic demand, with linear supply function\nbidding and the proposed efficiency recovery mechanism. We show that the Nash\nequilibrium of such a game exists under mild conditions, and more importantly,\nit achieves the underlying efficient supply dispatch and the market clearing\nprice that reflects the truthful system marginal production cost. Further, the\nmechanism can be tuned to guarantee self-sufficiency, i.e., taxes collected\ncounterbalance subsidies needed. Extensive numerical case studies are run to\nvalidate the equilibrium analysis, and we employ individual net profit and a\nmodified version of Lerner index as two metrics to evaluate the impact of the\nmechanism on market outcomes by varying its tuning parameter and firm\nheterogeneity.\n"
    },
    {
        "paper_id": 2106.11128,
        "authors": "Maximilian Linek and Christian Traxler",
        "title": "Framing and Social Information Nudges at Wikipedia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a series of trials that randomly assigned Wikipedia users in\nGermany to different web banners soliciting donations. The trials varied\nframing or content of social information about how many other users are\ndonating. Framing a given number of donors in a negative way increased donation\nrates. Variations in the communicated social information had no detectable\neffects. The findings are consistent with the results from a survey experiment.\nIn line with donations being strategic substitutes, the survey documents that\nthe negative framing lowers beliefs about others' donations. Varying the social\ninformation, in contrast, is ineffective in changing average beliefs.\n"
    },
    {
        "paper_id": 2106.11129,
        "authors": "Jano\\'s Gabler, Tobias Raabe, Klara R\\\"ohrl, Hans-Martin von Gaudecker",
        "title": "The Effectiveness of Strategies to Contain SARS-CoV-2: Testing,\n  Vaccinations, and NPIs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to slow the spread of the CoViD-19 pandemic, governments around the\nworld have enacted a wide set of policies limiting the transmission of the\ndisease. Initially, these focused on non-pharmaceutical interventions; more\nrecently, vaccinations and large-scale rapid testing have started to play a\nmajor role. The objective of this study is to explain the quantitative effects\nof these policies on determining the course of the pandemic, allowing for\nfactors like seasonality or virus strains with different transmission profiles.\nTo do so, the study develops an agent-based simulation model, which is\nestimated using data for the second and the third wave of the CoViD-19 pandemic\nin Germany. The paper finds that during a period where vaccination rates rose\nfrom 5% to 40%, large-scale rapid testing had the largest effect on reducing\ninfection numbers. Frequent large-scale rapid testing should remain part of\nstrategies to contain CoViD-19; it can substitute for many non-pharmaceutical\ninterventions that come at a much larger cost to individuals, society, and the\neconomy.\n"
    },
    {
        "paper_id": 2106.11184,
        "authors": "Michael Park, Erin Leahey, Russell Funk",
        "title": "The decline of disruptive science and technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Theories of scientific and technological change view discovery and invention\nas endogenous processes, wherein prior accumulated knowledge enables future\nprogress by allowing researchers to, in Newton's words, \"stand on the shoulders\nof giants\". Recent decades have witnessed exponential growth in the volume of\nnew scientific and technological knowledge, thereby creating conditions that\nshould be ripe for major advances. Yet contrary to this view, studies suggest\nthat progress is slowing in several major fields of science and technology.\nHere, we analyze these claims at scale across 6 decades, using data on 45\nmillion papers and 3.5 million patents from 6 large-scale datasets. We find\nthat papers and patents are increasingly less likely to break with the past in\nways that push science and technology in new directions, a pattern that holds\nuniversally across fields. Subsequently, we link this decline in disruptiveness\nto a narrowing in the use of prior knowledge, allowing us to reconcile the\npatterns we observe with the \"shoulders of giants\" view. We find that the\nobserved declines are unlikely to be driven by changes in the quality of\npublished science, citation practices, or field-specific factors. Overall, our\nresults suggest that slowing rates of disruption may reflect a fundamental\nshift in the nature of science and technology.\n"
    },
    {
        "paper_id": 2106.11433,
        "authors": "Duarte Gon\\c{c}alves, Jonathan Libgober, and Jack Willis",
        "title": "Retractions: Updating from Complex Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We modify a canonical experimental design to identify the effectiveness of\nretractions. Comparing beliefs after retractions to beliefs (a) without the\nretracted information and (b) after equivalent new information, we find that\nretractions result in diminished belief updating in both cases. We propose this\nreflects updating from retractions being more complex, and our analysis\nsupports this: we find longer response times, lower accuracy, and higher\nvariability. The results -- robust across diverse subject groups and design\nvariations -- enhance our understanding of belief updating and offer insights\ninto addressing misinformation.\n"
    },
    {
        "paper_id": 2106.11446,
        "authors": "Yoshi Fujiwara, Rubaiyat Islam",
        "title": "Bitcoin's Crypto Flow Network",
        "comments": "39 pages, 18 Figures; \"Blockchain in Kyoto 2021\" conference;\n  forthcoming in JPS Conference Proceedings",
        "journal-ref": null,
        "doi": "10.7566/JPSCP.36.011002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How crypto flows among Bitcoin users is an important question for\nunderstanding the structure and dynamics of the cryptoasset at a global scale.\nWe compiled all the blockchain data of Bitcoin from its genesis to the year\n2020, identified users from anonymous addresses of wallets, and constructed\nmonthly snapshots of networks by focusing on regular users as big players. We\napply the methods of bow-tie structure and Hodge decomposition in order to\nlocate the users in the upstream, downstream, and core of the entire crypto\nflow. Additionally, we reveal principal components hidden in the flow by using\nnon-negative matrix factorization, which we interpret as a probabilistic model.\nWe show that the model is equivalent to a probabilistic latent semantic\nanalysis in natural language processing, enabling us to estimate the number of\nsuch hidden components. Moreover, we find that the bow-tie structure and the\nprincipal components are quite stable among those big players. This study can\nbe a solid basis on which one can further investigate the temporal change of\ncrypto flow, entry and exit of big players, and so forth.\n"
    },
    {
        "paper_id": 2106.11484,
        "authors": "Vrinda Dhingra (1), Amita Sharma (2), Shiv K. Gupta (1) ((1) Indian\n  Institute of Technology, Roorkee, (2) Netaji Subhas University of Technology,\n  New Delhi)",
        "title": "Sectoral portfolio optimization by judicious selection of financial\n  ratios via PCA",
        "comments": "30 pages, 13 tables, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Embedding value investment in portfolio optimization models has always been a\nchallenge. In this paper, we attempt to incorporate it by employing principal\ncomponent analysis to filter out dominant financial ratios from each sector and\nthereafter, use the portfolio optimization model incorporating second-order\nstochastic dominance criteria to derive an optimal investment. We consider a\ntotal of $11$ financial ratios corresponding to each sector representing four\ncategories of ratios, namely liquidity, solvency, profitability, and valuation.\nPCA is then applied over a period of 10 years to extract dominant ratios from\neach sector in two ways, one from the component solution and the other from\neach category on the basis of their communalities. The two-step Sectoral\nPortfolio Optimization (SPO) model is then utilized to build an optimal\nportfolio. The strategy formed using the formerly extracted ratios is termed\nPCA-SPO(A) and the latter PCA-SPO(B). The results obtained from the proposed\nstrategies are compared with those from mean-variance, minimum variance, SPO,\nand nominal SSD models, with and without financial ratios. The empirical\nperformance of proposed strategies is analyzed in two ways, viz., using a\nrolling window scheme and using market trend scenarios for S\\&P BSE 500 (India)\nand S\\&P 500 (U.S.) markets. We observe that the proposed strategy PCA-SPO(B)\noutperforms all other models in terms of downside deviation, CVaR, VaR,\nSortino, Rachev, and STARR ratios over almost all out-of-sample periods. This\nhighlights the importance of value investment where ratios are carefully\nselected and embedded quantitatively in portfolio selection process.\n"
    },
    {
        "paper_id": 2106.1151,
        "authors": "Jean-Pierre Fouque, Ruimeng Hu, Ronnie Sircar",
        "title": "Sub- and Super-solution Approach to Accuracy Analysis of Portfolio\n  Optimization Asymptotics in Multiscale Stochastic Factor Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The problem of portfolio optimization when stochastic factors drive returns\nand volatilities has been studied in previous works by the authors. In\nparticular, they proposed asymptotic approximations for value functions and\noptimal strategies in the regime where these factors are running on both slow\nand fast timescales. However, the rigorous justification of the accuracy of\nthese approximations has been limited to power utilities and a single factor.\nIn this paper, we provide an accurate analysis for cases with general utility\nfunctions and two timescale factors by constructing sub- and super-solutions to\nthe fully nonlinear problem so that their difference is at the desired level of\naccuracy. This approach will be valuable in various related stochastic control\nproblems.\n"
    },
    {
        "paper_id": 2106.11537,
        "authors": "Eiji Yamamura",
        "title": "Information of income position and its impact on perceived tax burden\n  and preference for redistribution: An Internet Survey Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A customized internet survey experiment is conducted in Japan to examine how\nindividuals' relative income position influences preferences for income\nredistribution and individual perceptions regarding income tax burden. I first\nasked respondents about their perceived income position in their country and\ntheir preferences for redistribution and perceived tax burden. In the follow-up\nsurvey for the treatment group, I provided information on their true income\nposition and asked the same questions as in the first survey. For the control\ngroup, I did not provide their true income position and asked the same\nquestions. I gathered a large sample that comprised observations of the\ntreatment group (4,682) and the control group (2,268). The key findings suggest\nthat after being informed of individuals' real income position, (1) individuals\nwho thought their income position was higher than the true one perceived their\ntax burden to be larger, (2) individuals' preference for redistribution hardly\nchanges, and (3) irreciprocal individuals perceive their tax burden to be\nlarger and are more likely to prefer redistribution. However, the share of\nirreciprocal ones is small. This leads Japan to be a non-welfare state.\n"
    },
    {
        "paper_id": 2106.11691,
        "authors": "Sebastian M. Krause, Edgar Jungblut, Thomas Guhr",
        "title": "Two Price Regimes in Limit Order Books: Liquidity Cushion and Fragmented\n  Distant Field",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/ac4517",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The distribution of liquidity within the limit order book is essential for\nthe impact of market orders on the stock price and the emergence of price\nshocks. Limit orders are characterized by stylized facts: The number of\ninserted limit orders declines with the price distance from the quotes\nfollowing a power law and limit order lifetimes and volumes are power law\ndistributed. Strong dependencies among these quantities add to the complexity\nof limit order books. Here we analyze the limit order book in the dimensions of\nprice, time, limit order lifetime and volume altogether. This allows us to\nidentify regularities which are not visible in marginal distributions.\nParticularly we find that the limit order book is divided into two regimes.\nAround the quotes we find a densely filled regime with mostly short living\nlimit orders closely adapting to the price. Far away from the quotes we find a\nsparse filling with long living limit orders, mostly inserted at particular\ntimes of the day being prone to flash crashes. We determine the characteristics\nof those two regimes and point out the main differences. Based on our research\nwe propose a model for simulating the regime around the quotes.\n"
    },
    {
        "paper_id": 2106.11846,
        "authors": "Austin P Wright, Caleb Ziems, Haekyu Park, Jon Saad-Falcon, Duen Horng\n  Chau, Diyi Yang, Maria Tomprou",
        "title": "Quantifying the Impact of Human Capital, Job History, and Language\n  Factors on Job Seniority with a Large-scale Analysis of Resumes",
        "comments": "9 Pages, 5 Figures, 8 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As job markets worldwide have become more competitive and applicant selection\ncriteria have become more opaque, and different (and sometimes contradictory)\ninformation and advice is available for job seekers wishing to progress in\ntheir careers, it has never been more difficult to determine which factors in a\nr\\'esum\\'e most effectively help career progression. In this work we present a\nnovel, large scale dataset of over half a million r\\'esum\\'es with preliminary\nanalysis to begin to answer empirically which factors help or hurt people\nwishing to transition to more senior roles as they progress in their career. We\nfind that previous experience forms the most important factor, outweighing\nother aspects of human capital, and find which language factors in a r\\'esum\\'e\nhave significant effects. This lays the groundwork for future inquiry in career\ntrajectories using large scale data analysis and natural language processing\ntechniques.\n"
    },
    {
        "paper_id": 2106.11901,
        "authors": "Samuel S.-H. Wang, Jonathan Cervas, Bernard Grofman, Keena Lipsitz",
        "title": "A systems framework for remedying dysfunction in U.S. democracy",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.2102154118",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Democracy often fails to meet its ideals, and these failures may be made\nworse by electoral institutions. Unwanted outcomes include polarized\ninstitutions, unresponsive representatives, and the ability of a faction of\nvoters to gain power at the expense of the majority. Various reforms have been\nproposed to address these problems, but their effectiveness is difficult to\npredict against a backdrop of complex interactions. Here we outline a path for\nsystems-level modeling to help understand and optimize repairs to U.S.\ndemocracy. Following the tradition of engineering and biology, models of\nsystems include mechanisms with dynamical properties that include\nnonlinearities and amplification (voting rules), positive feedback mechanisms\n(single-party control, gerrymandering), negative feedback (checks and\nbalances), integration over time (lifetime judicial appointments), and low\ndimensionality (polarization). To illustrate a systems-level approach we\nanalyze three emergent phenomena: low dimensionality, elite polarization, and\nanti-majoritarianism in legislatures. In each case, long-standing rules now\ncontribute to undesirable outcomes as a consequence of changes in the political\nenvironment. Theoretical understanding at a general level will also help\nevaluate whether a proposed reform's benefits will materialize and be lasting,\nespecially as conditions change again. In this way, rigorous modeling may not\nonly shape new lines of research, but aid in the design of effective and\nlasting reform.\n"
    },
    {
        "paper_id": 2106.12049,
        "authors": "Fabien Le Floc'h",
        "title": "Pricing American options with the Runge-Kutta-Legendre finite difference\n  scheme",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents the Runge-Kutta-Legendre finite difference scheme,\nallowing for an additional shift in its polynomial representation. A short\npresentation of the stability region, comparatively to the\nRunge-Kutta-Chebyshev scheme follows. We then explore the problem of pricing\nAmerican options with the Runge-Kutta-Legendre scheme under the one factor\nBlack-Scholes and the two factor Heston stochastic volatility models, as well\nas the pricing of butterfly spread and digital options under the uncertain\nvolatility model, where a Hamilton-Jacobi-Bellman partial differential equation\nneeds to be solved. We explore the order of convergence in these problems, as\nwell as the option greeks stability, compared to the literature and popular\nschemes such as Crank-Nicolson, with Rannacher time-stepping.\n"
    },
    {
        "paper_id": 2106.12051,
        "authors": "Fabien Le Floc'h",
        "title": "More stochastic expansions for the pricing of vanilla options with cash\n  dividends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is no exact closed form formula for pricing of European options with\ndiscrete cash dividends under the model where the underlying asset price\nfollows a piecewise lognormal process with jumps at dividend ex-dates. This\npaper presents alternative expansions based on the technique of Etore and\nGobet, leading to more robust first, second and third-order expansions across\nthe range of strikes and the range of dividend dates.\n"
    },
    {
        "paper_id": 2106.12292,
        "authors": "Francesco Buono, Camilla Cal\\`i and Maria Longobardi",
        "title": "Dispersion indices based on Kerridge inaccuracy and Kullback-Leibler\n  divergence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The concept of varentropy has been recently introduced as a dispersion index\nof the reliability of measure of information. In this paper, we introduce new\nmeasures of variability for two measures of uncertainty, the Kerridge\ninaccuracy measure and the Kullback-Leibler divergence. These new definitions\nand related properties, bounds and examples are presented. Finally we show an\napplication of Kullback-Leibler divergence and its dispersion index using the\nmean-variance rule.\n"
    },
    {
        "paper_id": 2106.12315,
        "authors": "Beni Egressy, Roger Wattenhofer",
        "title": "Bailouts in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider networks of banks with assets and liabilities. Some banks may be\ninsolvent, and a central bank can decide which insolvent banks, if any, to bail\nout. We view bailouts as an optimization problem where the central bank has\ngiven resources at its disposal and an objective it wants to maximize. We show\nthat under various assumptions and for various natural objectives this\noptimization problem is NP-hard, and in some cases even hard to approximate.\nFurthermore, we also show that given a fixed central bank bailout objective,\nbanks in the network can make new debt contracts to increase their own market\nvalue in the event of a bailout (at the expense of the central bank).\n"
    },
    {
        "paper_id": 2106.12395,
        "authors": "Mathias Beiglb\\\"ock, Gudmund Pammer, Walter Schachermayer",
        "title": "From Bachelier to Dupire via Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Famously mathematical finance was started by Bachelier in his 1900 PhD thesis\nwhere - among many other achievements - he also provides a formal derivation of\nthe Kolmogorov forward equation. This forms also the basis for Dupire's (again\nformal) solution to the problem of finding an arbitrage free model calibrated\nto the volatility surface. The later result has rigorous counterparts in the\ntheorems of Kellerer and Lowther. In this survey article we revisit these\nhallmarks of stochastic finance, highlighting the role played by some optimal\ntransport results in this context.\n"
    },
    {
        "paper_id": 2106.12425,
        "authors": "Anders D. Sleire, B{\\aa}rd St{\\o}ve, H{\\aa}kon Otneim, Geir Drage\n  Berentsen, Dag Tj{\\o}stheim, Sverre Hauso Haugen",
        "title": "Portfolio Allocation under Asymmetric Dependence in Asset Returns using\n  Local Gaussian Correlations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that there are asymmetric dependence structures between\nfinancial returns. In this paper we use a new nonparametric measure of local\ndependence, the local Gaussian correlation, to improve portfolio allocation. We\nextend the classical mean-variance framework, and show that the portfolio\noptimization is straightforward using our new approach, only relying on a\ntuning parameter (the bandwidth). The new method is shown to outperform the\nequally weighted (1/N) portfolio and the classical Markowitz portfolio for\nmonthly asset returns data.\n"
    },
    {
        "paper_id": 2106.12431,
        "authors": "Andrea Maran, Andrea Pallavicini and Stefano Scoleri",
        "title": "Chebyshev Greeks: Smoothing Gamma without Bias",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The computation of Greeks is a fundamental task for risk managing of\nfinancial instruments. The standard approach to their numerical evaluation is\nvia finite differences. Most exotic derivatives are priced via Monte Carlo\nsimulation: in these cases, it is hard to find a fast and accurate\napproximation of Greeks, mainly because of the need of a tradeoff between bias\nand variance. Recent improvements in Greeks computation, such as Adjoint\nAlgorithmic Differentiation, are unfortunately uneffective on second order\nGreeks (such as Gamma), which are plagued by the most significant\ninstabilities, so that a viable alternative to standard finite differences is\nstill lacking. We apply Chebyshev interpolation techniques to the computation\nof spot Greeks, showing how to improve the stability of finite difference\nGreeks of arbitrary order, in a simple and general way. The increased\nperformance of the proposed technique is analyzed for a number of real payoffs\ncommonly traded by financial institutions.\n"
    },
    {
        "paper_id": 2106.12827,
        "authors": "Maciej Ber\\k{e}sewicz, Dagmara Nikulin, Marcin Szymkowiak, Kamil Wilak",
        "title": "The gig economy in Poland: evidence based on mobile big data",
        "comments": "44 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article we address the question of how to measure the size and\ncharacteristics of the platform economy. We propose a~different, to sample\nsurveys, approach based on smartphone data, which are passively collected\nthrough programmatic systems as part of online marketing. In particular, in our\nstudy we focus on two types of services: food delivery (Bolt Courier, Takeaway,\nGlover, Wolt and transport services (Bolt Driver, Free Now, iTaxi and Uber).\nOur results show that the platform economy in Poland is growing. In particular,\nwith respect to food delivery and transportation services performed by means of\napplications, we observed a growing trend between January 2018 and December\n2020. Taking into account the demographic structure of apps users, our results\nconfirm findings from past studies: the majority of platform workers are young\nmen but the age structure of app users is different for each of the two\ncategories of services. Another surprising finding is that foreigners do not\naccount for the majority of gig workers in Poland. When the number of platform\nworkers is compared with corresponding working populations, the estimated share\nof active app users accounts for about 0.5-2% of working populations in 9\nlargest Polish cities.\n"
    },
    {
        "paper_id": 2106.1295,
        "authors": "Hengxu Lin, Dong Zhou, Weiqing Liu, Jiang Bian",
        "title": "Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor\n  and Optimal Transport",
        "comments": "Accepted by KDD 2021 (research track)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Successful quantitative investment usually relies on precise predictions of\nthe future movement of the stock price. Recently, machine learning based\nsolutions have shown their capacity to give more accurate stock prediction and\nbecome indispensable components in modern quantitative investment systems.\nHowever, the i.i.d. assumption behind existing methods is inconsistent with the\nexistence of diverse trading patterns in the stock market, which inevitably\nlimits their ability to achieve better stock prediction performance. In this\npaper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to\nempower existing stock prediction models with the ability to model multiple\nstock trading patterns. Essentially, TRA is a lightweight module that consists\nof a set of independent predictors for learning multiple patterns as well as a\nrouter to dispatch samples to different predictors. Nevertheless, the lack of\nexplicit pattern identifiers makes it quite challenging to train an effective\nTRA-based model. To tackle this challenge, we further design a learning\nalgorithm based on Optimal Transport (OT) to obtain the optimal sample to\npredictor assignment and effectively optimize the router with such assignment\nthrough an auxiliary loss term. Experiments on the real-world stock ranking\ntask show that compared to the state-of-the-art baselines, e.g., Attention LSTM\nand Transformer, the proposed method can improve information coefficient (IC)\nfrom 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used\nin this work are publicly available:\nhttps://github.com/microsoft/qlib/tree/main/examples/benchmarks/TRA.\n"
    },
    {
        "paper_id": 2106.12961,
        "authors": "Liping Yang",
        "title": "Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, Bitcoin price prediction has attracted the interest of\nresearchers and investors. However, the accuracy of previous studies is not\nwell enough. Machine learning and deep learning methods have been proved to\nhave strong prediction ability in this area. This paper proposed a method\ncombined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning\nmethod called long short-term memory (LSTM) to research the problem of next-day\nBitcoin price forecast.\n"
    },
    {
        "paper_id": 2106.12971,
        "authors": "Jherek Healy",
        "title": "The Pricing of Vanilla Options with Cash Dividends as a Classic Vanilla\n  Basket Option Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the standard Black-Scholes-Merton framework, dividends are represented as\na continuous dividend yield and the pricing of Vanilla options on a stock is\nachieved through the well-known Black-Scholes formula. In reality however,\nstocks pay a discrete fixed cash dividend at each dividend ex-date. This leads\nto the so-called piecewise lognormal model, where the asset jumps from a fixed\nknown amount at each dividend date. There is however no exact closed-form\nformula for the pricing of Vanilla options under this model. Approximations\nmust be used. While there exists many approximations taylored to this specific\nproblem in the litterature, this paper explores the use of existing well-known\nbasket option formulas for the pricing of European options on a single asset\nwith cash dividends in the piecewise lognormal model.\n"
    },
    {
        "paper_id": 2106.12985,
        "authors": "Kamaladdin Fataliyev, Aneesh Chivukula, Mukesh Prasad and Wei Liu",
        "title": "Stock Market Analysis with Text Data: A Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market movements are influenced by public and private information\nshared through news articles, company reports, and social media discussions.\nAnalyzing these vast sources of data can give market participants an edge to\nmake profit. However, the majority of the studies in the literature are based\non traditional approaches that come short in analyzing unstructured, vast\ntextual data. In this study, we provide a review on the immense amount of\nexisting literature of text-based stock market analysis. We present input data\ntypes and cover main textual data sources and variations. Feature\nrepresentation techniques are then presented. Then, we cover the analysis\ntechniques and create a taxonomy of the main stock market forecast models.\nImportantly, we discuss representative work in each category of the taxonomy,\nanalyzing their respective contributions. Finally, this paper shows the\nfindings on unaddressed open problems and gives suggestions for future work.\nThe aim of this study is to survey the main stock market analysis models, text\nrepresentation techniques for financial market prediction, shortcomings of\nexisting techniques, and propose promising directions for future research.\n"
    },
    {
        "paper_id": 2106.12987,
        "authors": "Vipul Satone, Dhruv Desai, Dhagash Mehta",
        "title": "Fund2Vec: Mutual Funds Similarity using Graph Learning",
        "comments": "2 column format, 8 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying similar mutual funds with respect to the underlying portfolios\nhas found many applications in financial services ranging from fund recommender\nsystems, competitors analysis, portfolio analytics, marketing and sales, etc.\nThe traditional methods are either qualitative, and hence prone to biases and\noften not reproducible, or, are known not to capture all the nuances\n(non-linearities) among the portfolios from the raw data. We propose a\nradically new approach to identify similar funds based on the weighted\nbipartite network representation of funds and their underlying assets data\nusing a sophisticated machine learning method called Node2Vec which learns an\nembedded low-dimensional representation of the network. We call the embedding\n\\emph{Fund2Vec}. Ours is the first ever study of the weighted bipartite network\nrepresentation of the funds-assets network in its original form that identifies\nstructural similarity among portfolios as opposed to merely portfolio overlaps.\n"
    },
    {
        "paper_id": 2106.13059,
        "authors": "Anne G. Balter and Nikolaus Schweizer",
        "title": "Robust Decisions for Heterogeneous Agents via Certainty Equivalents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of a planner who resolves risk-return trade-offs - like\nfinancial investment decisions - on behalf of a collective of agents with\nheterogeneous risk preferences. The planner's objective is a two-stage utility\nfunctional where an outer utility function is applied to the distribution of\nthe agents' certainty equivalents from a given decision. Assuming lognormal\nrisks and heterogeneous power utility preferences for the agents, we\ncharacterize optimal behavior in a setting where the planner can let each agent\nchoose between different options from a fixed menu of possible decisions,\nleading to a grouping of the agents by risk preferences. These optimal decision\nmenus are derived first for the case where the planner knows the distribution\nof preferences exactly and then for a case where he faces uncertainty about\nthis distribution, only having access to upper and lower bounds on agents'\nrelative risk aversion. Finally, we provide tight bounds on the welfare loss\nfrom offering a finite menu of choices rather than fully personalized\ndecisions.\n"
    },
    {
        "paper_id": 2106.13283,
        "authors": "Jarek K\\k{e}dra, Assaf Libman, Victoria Steblovskaya",
        "title": "Pricing multi-asset contingent claims in a multi-dimensional binomial\n  market",
        "comments": "30 pages; title slightly changed",
        "journal-ref": "Journal of Stochastic Analysis, vol. 4, no. 1 (2023)",
        "doi": "10.31390/josa.4.1.02",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an incomplete multi-asset binomial market model. We prove that\nfor a wide class of contingent claims the extremal multi-step martingale\nmeasure is a power of the corresponding single-step extremal martingale\nmeasure. This allows for closed form formulas for the bounds of a no-arbitrage\ncontingent claim price interval. We construct a feasible algorithm for\ncomputing those boundaries as well as for the corresponding hedging strategies.\nOur results apply, for example, to European basket call and put options and\nAsian arithmetic average options.\n"
    },
    {
        "paper_id": 2106.13321,
        "authors": "Abdelghani Maddi (HCERES)",
        "title": "Game theory and scholarly publishing: premises for an agreement around\n  open access",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stakeholders in research and scientific publishing are gradually joining the\nOpen-Access (OA) movement, which is gaining momentum to become nowadays at the\nheart of scientific policies in high-income countries. The rise of OA generates\nprofound changes in the chain of production and dissemination of knowledge.\nFree access to peer-reviewed research methods and results has contributed to\nthe dynamics of science observed in recent years. The modes of publication and\naccess have also evolved; the classic model, based on journal subscriptions is\ngradually giving way to new economic models that have appeared with the arrival\nof OA. The objective of this article is twofold. First, propose a model for the\npublishing market based on the literature as well as on changes in open science\npolicies. Second, analyze publishing strategies of publishers and institutions.\nTo do so, we relied on game theory in economics. Results show that in the short\nterm, the publisher's equilibrium strategy is to adopt a hybridpublishing\nmodel, while the institutions' equilibrium strategy is to publish in OA. This\nequilibrium is not stable and that in the medium/long term, the two players\nwill converge on an OA publishing strategy. The analysis of the equilibrium in\nmixed-strategies confirms this result.\n"
    },
    {
        "paper_id": 2106.13347,
        "authors": "Nazli Mohammad, Yvonne Stedham",
        "title": "Relationship between Cultural Values, Sense of Community and Trust and\n  the Effect of Trust in Workplace",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper provides a general overview of different perspectives and studies\non trust, offers a definition of trust, and provides factors that play a\nsubstantial role in developing social trust, and shows from which perspectives\nit can be fostered. The results showed that trust is playing an important role\nin success for organizations involved in cross-national strategic partnerships.\nTrust can reduce transaction costs, promotes inter-organizational\nrelationships, and improve subordinate relationships between managers.\n"
    },
    {
        "paper_id": 2106.13598,
        "authors": "Fenny Marietza, Ridwan Nurazi, Fitri Santi, Saiful",
        "title": "Bibliometric Analysis Of Herding Behavior In Times Of Crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The social and psychological concept of herding behavior provides a suitable\nsolution to give an understanding of the behavioral biases that often occur in\nthe capital market. The aim of this paper is to provide an overview of the\nbroader bibliometric literature on the term and concept of herding behavior.\nArticles are collected through the help of software consisting of Publish or\nPerish (PoP), Google Scholar, Mendeley, and VOSViewer through a systematic\napproach, explicit and reproductive methods. In addition, the articles were\nscanned by Scimagojr.com (Q1, Q2, Q3, and Q4), analyzing 83 articles of 261\nrelated articles from reputable and non-reputable journals from 1996 to 2021.\nMendeley software is used to manage and resume references. To review this\ndatabase, classification was performed using the VOSviewer software. Four\nclusters were reviewed; The words that appear most often in each group are the\ntype of stock market, the type of crisis, and the factors that cause herding.\nThus these four clusters became the main research themes on the topic of\nherding in times of crisis. Meanwhile, methodology and strategy are the themes\nfor future research in the future.\n"
    },
    {
        "paper_id": 2106.13612,
        "authors": "Bo Cowgill, Andrea Prat and Tommaso Valletti",
        "title": "Political Power and Market Power",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the link between political influence and industrial concentration.\nWe present a joint model of political influence and market competition: an\noligopoly lobbies the government over regulation, and competes in the product\nmarket shaped by this influence. We show broad conditions for mergers to\nincrease lobbying, both on the intensive margin and the extensive margin. We\ncombine data on mergers with data on lobbying expenditures and campaign\ncontributions in the US from 1999 to 2017. We document a positive association\nbetween mergers and lobbying, both by individual firms and by industry trade\nassociations. Mergers are also associated with extensive margin changes such as\nthe formation of in-house lobbying teams and corporate PACs. We find some\nevidence for a positive association between mergers and higher campaign\ncontributions.\n"
    },
    {
        "paper_id": 2106.13644,
        "authors": "An Chen, Motonobu Kanagawa, Fangyuan Zhang",
        "title": "Intergenerational risk sharing in a Defined Contribution pension system:\n  analysis with Bayesian optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a fully funded, collective defined-contribution (DC) pension system\nwith multiple overlapping generations. We investigate whether the welfare of\nparticipants can be improved by intergenerational risk sharing (IRS)\nimplemented with a realistic investment strategy (e.g., no borrowing) and\nwithout an outside entity (e.g., share holders) that helps finance the pension\nfund. To implement IRS, the pension system uses an automatic adjustment rule\nfor the indexation of individual accounts, which adapts to the notional funding\nratio of the pension system. The pension system has two parameters that\ndetermine the investment strategy and the strength of the adjustment rule,\nwhich are optimized by expected utility maximization using Bayesian\noptimization. The volatility of the retirement benefits and that of the funding\nratio are analyzed, and it is shown that the trade-off between them can be\ncontrolled by the optimal adjustment parameter to attain IRS. Compared with the\noptimal individual DC benchmark using the life-cycle strategy, the studied\npension system with IRS is shown to improve the welfare of risk-averse\nparticipants, when the financial market is volatile.\n"
    },
    {
        "paper_id": 2106.1367,
        "authors": "Oksana Mamina, Alexander Barannikov, Ludmila Gruzdeva",
        "title": "Sovereign wealth funds: main activity trends",
        "comments": "Total pages: 7. Key words: export, investment, gas, oil, revenue,\n  sovereign wealth fund, trend, commodity prices, market, investor, budget. JEL\n  codes: E-00; E-6",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sovereign wealth funds are created in those countries whose budget is highly\ndependent on market factors, usually world commodity prices. At the same time,\nthese funds are large institutional investors. An analysis of the nature of\ninvestments by the State Pension Fund Global of Norway showed that investments\nof the Fund are based on a seven-level model of diversifying its investments.\nThis model can also be applied to the investments of the National Wealth Fund\nof Russia to increase its profitability.\n"
    },
    {
        "paper_id": 2106.13888,
        "authors": "Katia Colaneri, Alessandra Cretarola, Benedetta Salterini",
        "title": "Optimal investment and proportional reinsurance in a regime-switching\n  market model under forward preferences",
        "comments": "32 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the optimal investment and reinsurance problem of an\ninsurance company whose investment preferences are described via a forward\ndynamic exponential utility in a regime-switching market model. Financial and\nactuarial frameworks are dependent since stock prices and insurance claims vary\naccording to a common factor given by a continuous time finite state Markov\nchain. We construct the value function and we prove that it is a forward\ndynamic utility. Then, we characterize the investment strategy and the optimal\nproportional level of reinsurance. We also perform numerical experiments and\nprovide sensitivity analyses with respect to some model parameters.\n"
    },
    {
        "paper_id": 2106.14168,
        "authors": "William A. Barnett, Xue Wang, Hai-Chuan Xu and Wei-Xing Zhou",
        "title": "Hierarchical contagions in the interdependent financial network",
        "comments": "18 pages, 4 figures, and 8 tables",
        "journal-ref": "Journal of Financial Stability, 2022, 61: 101037",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We derive the default cascade model and the fire-sale spillover model in a\nunified interdependent framework. The interactions among banks include not only\ndirect cross-holding, but also indirect dependency by holding mutual assets\noutside the banking system. Using data extracted from the European Banking\nAuthority, we present the interdependency network composed of 48 banks and 21\nasset classes. For the robustness, we employ three methods, called\n$\\textit{Anan}$, $\\textit{Ha\\l{}a}$ and $\\textit{Maxe}$, to reconstruct the\nasset/liability cross-holding network. Then we combine the external portfolio\nholdings of each bank to compute the interdependency matrix. The\ninterdependency network is much denser than the direct cross-holding network,\nshowing the complex latent interaction among banks. Finally, we perform\nmacroprudential stress tests for the European banking system, using the adverse\nscenario in EBA stress test as the initial shock. For different reconstructed\nnetworks, we illustrate the hierarchical cascades and show that the failure\nhierarchies are roughly the same except for a few banks, reflecting the\noverlapping portfolio holding accounts for the majority of defaults. We also\ncalculate systemic vulnerability and individual vulnerability, which provide\nimportant information for supervision and relevant management actions.\n"
    },
    {
        "paper_id": 2106.14204,
        "authors": "Nassim Nicholas Taleb",
        "title": "Bitcoin, Currencies, and Fragility",
        "comments": "Accepted in Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This discussion applies quantitative finance methods and economic arguments\nto cryptocurrencies in general and bitcoin in particular -- as there are about\n$10,000$ cryptocurrencies, we focus (unless otherwise specified) on the most\ndiscussed crypto of those that claim to hew to the original protocol (Nakamoto\n2009) and the one with, by far, the largest market capitalization.\n  In its current version, in spite of the hype, bitcoin failed to satisfy the\nnotion of \"currency without government\" (it proved to not even be a currency at\nall), can be neither a short nor long term store of value (its expected value\nis no higher than $0$), cannot operate as a reliable inflation hedge, and,\nworst of all, does not constitute, not even remotely, a safe haven for one's\ninvestments, a shield against government tyranny, or a tail protection vehicle\nfor catastrophic episodes.\n  Furthermore, bitcoin promoters appear to conflate the success of a payment\nmechanism (as a decentralized mode of exchange), which so far has failed, with\nthe speculative variations in the price of a zero-sum maximally fragile asset\nwith massive negative externalities.\n  Going through monetary history, we show how a true numeraire must be one of\nminimum variance with respect to an arbitrary basket of goods and services, how\ngold and silver lost their inflation hedge status during the Hunt brothers\nsqueeze in the late 1970s and what would be required from a true inflation\nhedged store of value.\n"
    },
    {
        "paper_id": 2106.14351,
        "authors": "Farhad Billimoria, Filiberto Fele, Iacopo Savelli, Thomas Morstyn,\n  Malcolm McCulloch",
        "title": "On the Design of an Insurance Mechanism for Reliability Differentiation\n  in Electricity Markets",
        "comments": "11 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.apenergy.2022.119356",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Securing an adequate supply of dispatchable resources is critical for keeping\na power system reliable under high penetrations of variable generation.\nTraditional resource adequacy mechanisms are poorly suited to exploiting the\ngrowing flexibility and heterogeneity of load enabled by advancements in\ndistributed resource and control technology. To address these challenges this\npaper develops a resource adequacy mechanism for the electricity sector\nutilising insurance risk management frameworks that is adapted to a future with\nvariable generation and flexible demand. The proposed design introduces a\ncentral insurance scheme with prudential requirements that align diverse\nconsumer reliability preferences with the financial objectives of an\ninsurer-of-last-resort. We illustrate the benefits of the scheme in (i)\ndifferentiating load by usage to enable better management of the system during\ntimes of extreme scarcity, (ii) incentivising incremental investment in\ngeneration infrastructure that is aligned with consumer reliability preferences\nand (iii) improving overall reliability outcomes for consumers.\n"
    },
    {
        "paper_id": 2106.14404,
        "authors": "Andreas A. Aigner and Gurvinder Dhaliwal",
        "title": "UNISWAP: Impermanent Loss and Risk Profile of a Liquidity Provider",
        "comments": "16 pages, 8 Figures, 1 Table",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.32419.58400/6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Uniswap is a decentralized exchange (DEX) and was first launched on November\n2, 2018 on the Ethereum mainnet [1] and is part of an Ecosystem of products in\nDecentralized Finance (DeFi). It replaces a traditional order book type of\ntrading common on centralized exchanges (CEX) with a deterministic model that\nswaps currencies (or tokens/assets) along a fixed price function determined by\nthe amount of currencies supplied by the liquidity providers. Liquidity\nproviders can be regarded as investors in the decentralized exchange and earn\nfixed commissions per trade. They lock up funds in liquidity pools for distinct\npairs of currencies allowing market participants to swap them using the fixed\nprice function. Liquidity providers take on market risk as a liquidity provider\nin exchange for earning commissions on each trade. Here we analyze the risk\nprofile of a liquidity provider and the so called impermanent (unrealized) loss\nin particular. We provide an improved version of the commonly denoted\nimpermanent loss function for Uniswap v2 on the semi-infinite domain. The\ndifferences between Uniswap v2 and v3 are also discussed.\n"
    },
    {
        "paper_id": 2106.14758,
        "authors": "Pedro Patr\\'icio, Nuno A. M. Ara\\'ujo",
        "title": "Inheritances, social classes, and wealth distribution",
        "comments": "7 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0259002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a simple theoretical model to investigate the impact of\ninheritances on the wealth distribution. Wealth is described as a finite\nresource, which remains constant over different generations and is divided\nequally among offspring. All other sources of wealth are neglected. We consider\ndifferent societies characterized by a different offspring probability\ndistribution. We find that, if the population remains constant, the society\nreaches a stationary wealth distribution. We show that inequality emerges every\ntime the number of children per family is not always the same. For realistic\noffspring distributions from developed countries, the model predicts a Gini\ncoefficient of $G\\approx 0.3$. If we divide the society into wealth classes and\nset the probability of getting married to depend on the distance between\nclasses, the stationary wealth distribution crosses over from an exponential to\na power-law regime as the number of wealth classes and the level of class\ndistinction increase.\n"
    },
    {
        "paper_id": 2106.1482,
        "authors": "Alex Garivaltis",
        "title": "Rational Pricing of Leveraged ETF Expense Ratios",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the general relationship between the gearing ratio of a\nLeveraged ETF and its corresponding expense ratio, viz., the investment\nmanagement fees that are charged for the provision of this levered financial\nservice. It must not be possible for an investor to combine two or more LETFs\nin such a way that his (continuously-rebalanced) LETF portfolio can match the\ngearing ratio of a given, professionally managed product and, at the same time,\nenjoy lower weighted-average expenses than the existing LETF. Given a finite\nset of LETFs that exist in the marketplace, I give necessary and sufficient\nconditions for these products to be undominated in the price-gearing plane. In\na beautiful application of the duality theorem of linear programming, I prove a\nkind of two-fund theorem for LETFs: given a target gearing ratio for the\ninvestor, the cheapest way to achieve it is to combine (uniquely) the two\nnearest undominated LETF products that bracket it on the leverage axis. This\nalso happens to be the implementation that has the lowest annual turnover. For\nthe writer's enjoyment, we supply a second proof of the Main Theorem on LETFs\nthat is based on Carath\\'eodory's theorem in convex geometry. Thus, say, a\ntriple-leveraged (\"UltraPro\") exchange-traded product should never be mixed\nwith cash, if the investor is able to trade in the underlying index. In terms\nof financial innovation, our two-fund theorem for LETFs implies that the\nintroduction of new, undominated 2.5x products would increase the welfare of\nall investors whose preferred gearing ratios lie between 2x (\"Ultra\") and 3x\n(\"UltraPro\"). Similarly for a 1.5x product.\n"
    },
    {
        "paper_id": 2106.14824,
        "authors": "Akif Ince, Ilaria Peri, Silvana Pesenti",
        "title": "Risk contributions of lambda quantiles",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2022.2092543",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Risk contributions of portfolios form an indispensable part of risk adjusted\nperformance measurement. The risk contribution of a portfolio, e.g., in the\nEuler or Aumann-Shapley framework, is given by the partial derivatives of a\nrisk measure applied to the portfolio profit and loss in direction of the asset\nunits. For risk measures that are not positively homogeneous of degree 1,\nhowever, known capital allocation principles do not apply. We study the class\nof lambda quantile risk measures that includes the well-known Value-at-Risk as\na special case but for which no known allocation rule is applicable. We prove\ndifferentiability and derive explicit formulae of the derivatives of lambda\nquantiles with respect to their portfolio composition, that is their risk\ncontribution. For this purpose, we define lambda quantiles on the space of\nportfolio compositions and consider generic (also non-linear) portfolio\noperators.\n  We further derive the Euler decomposition of lambda quantiles for generic\nportfolios and show that lambda quantiles are homogeneous in the space of\nportfolio compositions, with a homogeneity degree that depends on the portfolio\ncomposition and the lambda function. This result is in stark contrast to the\npositive homogeneity properties of risk measures defined on the space of random\nvariables which admit a constant homogeneity degree. We introduce a generalised\nversion of Euler contributions and Euler allocation rule, which are compatible\nwith risk measures of any homogeneity degree and non-linear but homogeneous\nportfolios. These concepts are illustrated by a non-linear portfolio using\nfinancial market data.\n"
    },
    {
        "paper_id": 2106.1487,
        "authors": "Kaustav Das, Ivan Guo, Gr\\'egoire Loeper",
        "title": "On Stochastic Partial Differential Equations and their applications to\n  Derivative Pricing through a conditional Feynman-Kac formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a multi-dimensional diffusion framework, the price of a financial\nderivative can be expressed as an iterated conditional expectation, where the\ninner conditional expectation conditions on the future of an auxiliary process\nthat enters into the dynamics for the spot. Inspired by results from non-linear\nfiltering theory, we show that this inner conditional expectation solves a\nbackward SPDE (a so-called 'conditional Feynman-Kac formula'), thereby\nestablishing a connection between SPDE and derivative pricing theory. Unlike\nsituations considered previously in the literature, the problem at hand\nrequires conditioning on a backward filtration generated by the noise of the\nauxiliary process and enlarged by its terminal value, leading us to search for\na backward Brownian motion in this filtration. This adds an additional source\nof irregularity to the associated SPDE which must be tackled with new\ntechniques. Moreover, through the conditional Feynman-Kac formula, we establish\nan alternative class of so-called mixed Monte-Carlo PDE numerical methods for\npricing financial derivatives. Finally, we provide a simple demonstration of\nthis method by pricing a European put option.\n"
    },
    {
        "paper_id": 2106.15035,
        "authors": "Gaurab Aryal and Federico Zincenko",
        "title": "Empirical Framework for Cournot Oligopoly with Private Information",
        "comments": "forthcoming, The RAND Journal of Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an empirical framework for asymmetric Cournot oligopoly with\nprivate information about variable costs. First, considering a linear demand\nfor a homogenous product with a random intercept, we characterize the Bayesian\nCournot-Nash equilibrium. Then we establish the identification of the joint\ndistribution of demand and firm-specific cost distributions. Following the\nidentification steps, we propose a likelihood-based estimation method and apply\nit to the global market for crude-oil and quantify the welfare effect of\nprivate information. We also consider extensions of the model to include either\nproduct differentiation, conduct parameters, nonlinear demand, or selective\nentry.\n"
    },
    {
        "paper_id": 2106.15198,
        "authors": "Jann Michael Weinand, Russell McKenna, Heidi Heinrichs, Michael Roth,\n  Detlef Stolten, Wolf Fichtner",
        "title": "Exploring the trilemma of cost-efficient, equitable and publicly\n  acceptable onshore wind expansion planning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Onshore wind development has historically focused on cost-efficiency, which\nmay lead to inequitable turbine distributions and public resistance due to\nlandscape impacts. Using a multi-criteria planning approach, we show how\nonshore wind capacity targets can be achieved by 2050 in a cost-efficient,\nequitable and publicly acceptable way. For the case study of Germany, we build\non the existing turbine stock and use open data on technically feasible turbine\nlocations and scenicness of landscapes to plan the optimal expansion. The\nanalysis shows that while the trade-off between cost-efficiency and public\nacceptance is rather weak with about 15% higher costs or scenicness, an\nequitable distribution has a large impact on these criteria. Although the\nonshore wind capacity per inhabitant could be distributed about 220% more\nequitably through the expansion, equity would severely limit planning\nflexibility by 2050. Our analysis assists stakeholders in resolving the onshore\nwind expansion trilemma.\n"
    },
    {
        "paper_id": 2106.15208,
        "authors": "H\\'el\\`ene Halconruy",
        "title": "The insider problem in the trinomial model: a discrete-time jump process\n  approach",
        "comments": "38 pages. Comments are welecome!",
        "journal-ref": "Decisions in Economics and Finance, 46, 379-413, 2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an incomplete market underpinned by the trinomial model, we consider two\ninvestors : an ordinary agent whose decisions are driven by public information\nand an insider who possesses from the beginning a surplus of information\nencoded through a random variable for which he or she knows the outcome.\nThrough the definition of an auxiliary model based on a marked binomial\nprocess, we handle the trinomial model as a volatility one, and use the\nstochastic analysis and Malliavin calculus toolboxes available in that context.\nIn particular, we connect the information drift, the drift to eliminate in\norder to preserve the martingale property within an initial enlargement of\nfiltration in terms of the Malliavin derivative. We solve explicitly the agent\nand the insider expected logarithmic utility maximisation problems and provide\na hedging formula for replicable claims. We identify the insider expected\nadditional utility with the Shannon entropy of the extra information, and\nexamine then the existence of arbitrage opportunities for the insider.\n"
    },
    {
        "paper_id": 2106.1527,
        "authors": "Andres Fielbaum, Alejandro Tirachini, Javier Alonso-Mora",
        "title": "New sources of economies and diseconomies of scale in on-demand\n  ridepooling systems and comparison with public transport",
        "comments": "30 pages, 12 figures, submitted to ITEA 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  On-demand ridepooling (ODRP) can become a powerful alternative to reduce\ncongestion and emissions, if it attracts private car users. Therefore, it is\ncrucial to identify the strategic phenomena that determine when ODRP systems\ncan run efficiently. In this paper, we analyze the performance of an ODRP\nsystem, in which the fleet of low-capacity vehicles is endogenously adapted to\nthe demand, and operated in a zone covered by a single transit line. The\nrouting of the on-demand fleet follows some of the rules of public transport\nsystems; namely, it is not-for-profit, some users can be required to walk, and\nall requests must be served. Considering both users' and operators' costs we\nidentify two sources of scale economies: when demand grows, the average cost is\nreduced due to a) an equivalent of the Mohring Effect (also present in public\ntransport), and b) due to matching users with more similar routes when they are\nassigned to the vehicles, which we call Better-matching Effect. A\ncounter-balance force, called Flex-route Effect, is observed when the vehicle\nloads increase and users face longer detours. We find a specific demand range\nin which the latter effect dominates the others, imposing diseconomies of scale\nwhen only users' costs are considered. Such a phenomenon emerges because the\nroutes are not fixed; hence, it is not observed in traditional public transport\nsystems. However, when considering both users' and operators' costs, scale\neconomies prevail. Our simulations show that relaxing door-to-door vehicle\nrequirements to allow short walks is crucial for the performance of ODRP. In\nfact, we observe that an ODRP system with human-driven vehicles and walks\nallowed has a total cost at a similar level to that of a door-to-door ODRP\nsystem with driverless vehicles.\n"
    },
    {
        "paper_id": 2106.15376,
        "authors": "Athary Janiso (1), Prakash Kumar Shukla (1), Bheemeshwar Reddy A (1)\n  ((1) BITS-PILANI, Hyderabad Campus)",
        "title": "What Explains Gender Gap in Unpaid Household and Care Work in India?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to the unavailability of nationally representative data on time use, a\nsystematic analysis of the gender gap in unpaid household and care work has not\nbeen undertaken in the context of India. The present paper, using the recent\nTime Use Survey (2019) data, examines the socioeconomic and demographic factors\nassociated with variation in time spent on unpaid household and care work among\nmen and women. It analyses how much of the gender gap in the time allocated to\nunpaid work can be explained by differences in these factors. The findings show\nthat women spend much higher time compared to men in unpaid household and care\nwork. The decomposition results reveal that differences in socioeconomic and\ndemographic factors between men and women do not explain most of the gender gap\nin unpaid household work. Our results indicate that unobserved gender norms and\npractices most crucially govern the allocation of unpaid work within Indian\nhouseholds.\n"
    },
    {
        "paper_id": 2106.15426,
        "authors": "Guodong Ding, Daniele Marazzina",
        "title": "Effect of Labour Income on the Optimal Bankruptcy Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we deal with the optimal bankruptcy problem for an agent who\ncan optimally allocate her consumption rate, the amount of capital invested in\nthe risky asset as well as her leisure time. In our framework, the agent is\nendowed by an initial debt, and she is required to repay her debt continuously.\nDeclaring bankruptcy, the debt repayment is exempted at the cost of a wealth\nshrinkage. We implement the duality method to solve the problem analytically\nand conduct a sensitivity analysis to the cost and benefit parameters of\nbankruptcy. Introducing the flexible leisure/working rate, and therefore the\nlabour income, into the bankruptcy model, we investigate its effect on the\noptimal strategies.\n"
    },
    {
        "paper_id": 2106.15452,
        "authors": "M. Gardini, P. Sabino, and E. Sasso",
        "title": "The Variance Gamma++ Process and Applications to Energy Markets",
        "comments": "39 pages, 5 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of this article is to introduce a new L\\'evy process, termed\nVariance Gamma++ process, to model the dynamic of assets in illiquid markets.\nSuch a process has the mathematical tractability of the Variance Gamma process\nand is obtained applying the self-decomposability of the gamma law. Compared to\nthe Variance Gamma model, it has an additional parameter representing the\nmeasure of the trading activity. We give a full characterization of the\nVariance Gamma++ process in terms of its characteristic triplet, characteristic\nfunction and transition density. In addition, we provide efficient path\nsimulation algorithms, both forward and backward in time. We also obtain an\nefficient \"integral-free\" explicit pricing formula for European options. These\nresults are instrumental to apply Fourier-based option pricing and maximum\nlikelihood techniques for the parameter estimation. Finally, we apply our model\nto illiquid markets, namely to the calibration of European power future market\ndata. We accordingly evaluate exotic derivatives using the Monte Carlo method\nand compare these values to those obtained using the Variance Gamma process and\ngive an economic interpretation of the obtained results. Finally, we illustrate\nan extension to the multivariate framework.\n"
    },
    {
        "paper_id": 2106.15466,
        "authors": "\\\"Ozge Sahin, Karoline Bax, Claudia Czado, Sandra Paterlini",
        "title": "Environmental, Social, Governance scores and the Missing pillar -- Why\n  does missing information matter?",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/csr.2326",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Environmental, Social, and Governance (ESG) scores measure companies'\nperformance concerning sustainability and societal impact and are organized on\nthree pillars: Environmental (E), Social (S), and Governance (G). These\ncomplementary non-financial ESG scores should provide information about the ESG\nperformance and risks of different companies. However, the extent of not yet\npublished ESG information makes the reliability of ESG scores questionable. To\nexplicitly denote the not yet published information on ESG category scores, a\nnew pillar, the so-called Missing (M) pillar, is formulated. Environmental,\nSocial, Governance, and Missing (ESGM) scores are introduced to consider the\npotential release of new information in the future. Furthermore, an\noptimization scheme is proposed to compute ESGM scores, linking them to the\ncompanies' riskiness. By relying on the data provided by Refinitiv, we show\nthat the ESGM scores strengthen the companies' risk relationship. These new\nscores could benefit investors and practitioners as ESG exclusion strategies\nusing only ESG scores might exclude assets with a low score solely because of\ntheir missing information and not necessarily because of a low ESG merit.\n"
    },
    {
        "paper_id": 2106.15479,
        "authors": "Robert M Yawson",
        "title": "The Ecological System of Innovation: A New Architectural Framework for a\n  Functional Evidence-Based Platform for Science and Innovation Policy",
        "comments": null,
        "journal-ref": "The Future of Innovation: Proceedings of the XX ISPIM 2009\n  Conference, Vienna, Austria, June 21-24, 2009",
        "doi": "10.31124/advance.7367138.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Models on innovation, for the most part, do not include a comprehensive and\nend-to-end view. Most innovation policy attention seems to be focused on the\ncapacity to innovate and on input factors such as R&D investment, scientific\ninstitutions, human resources and capital. Such inputs frequently serve as\nproxies for innovativeness and are correlated with intermediate outputs such as\npatent counts and outcomes such as GDP per capita. While this kind of analysis\nis generally indicative of innovative behaviour, it is less useful in terms of\ndiscriminating causality and what drives successful strategy or public policy\ninterventions. This situation has led to the developing of new frameworks for\nthe innovation system led by National Science and Technology Policy Centres\nacross the globe. These new models of innovation are variously referred to as\nthe National Innovation Ecosystem. There is, however, a fundamental question\nthat needs to be answered: what elements should an innovation policy include,\nand how should such policies be implemented? This paper attempts to answer this\nquestion.\n"
    },
    {
        "paper_id": 2106.15496,
        "authors": "Jean-Fran\\c{c}ois Chassagneux, Mohan Yang",
        "title": "Numerical approximation of singular Forward-Backward SDEs",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jcp.2022.111459",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this work, we study the numerical approximation of a class of singular\nfully coupled forward backward stochastic differential equations. These\nequations have a degenerate forward component and non-smooth terminal\ncondition. They are used, for example, in the modeling of carbon market[9] and\nare linked to scalar conservation law perturbed by a diffusion. Classical\nFBSDEs methods fail to capture the correct entropy solution to the associated\nquasi-linear PDE. We introduce a splitting approach that circumvent this\ndifficulty by treating differently the numerical approximation of the diffusion\npart and the non-linear transport part. Under the structural condition\nguaranteeing the well-posedness of the singular FBSDEs [8], we show that the\nsplitting method is convergent with a rate $1/2$. We implement the splitting\nscheme combining non-linear regression based on deep neural networks and\nconservative finite difference schemes. The numerical tests show very good\nresults in possibly high dimensional framework.\n"
    },
    {
        "paper_id": 2106.15518,
        "authors": "Fabian Scheller, Frauke Wiese, Jann Michael Weinand, Dominik Franjo\n  Dominkovi\\'c, Russell McKenna",
        "title": "An expert survey to assess the current status and future challenges of\n  energy system analysis",
        "comments": null,
        "journal-ref": "Smart Energy (2021): 100057",
        "doi": "10.1016/j.segy.2021.100057",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Decision support systems like computer-aided energy system analysis (ESA) are\nconsidered one of the main pillars for developing sustainable and reliable\nenergy transformation strategies. Although today's diverse tools can already\nsupport decision-makers in a variety of research questions, further\ndevelopments are still necessary. Intending to identify opportunities and\nchallenges in the field, we classify modelling capabilities (32), methodologies\n(15) implementation issues (15) and management issues (7) from an extensive\nliterature review. Based on a quantitative expert survey of energy system\nmodellers (N=61) mainly working with simulation and optimisation models, the\nstatus of development and the complexity of realisation of those modelling\ntopics are assessed. While the rated items are considered to be more complex\nthan actually represented, no significant outliers are determinable, showing\nthat there is no consensus about particular aspects of ESA that are lacking\ndevelopment. Nevertheless, a classification of the items in terms of a\nspecially defined modelling strategy matrix identifies capabilities like\nland-use planning patterns, equity and distributional effects and endogenous\ntechnological learning as \"low hanging fruits\" for enhancement, as well as a\nlarge number of complex topics that are already well implemented. The remaining\n\"tough nuts\" regarding modelling capabilities include non-energy sector and\nsocial behaviour interaction effects. In general, the optimisation and\nsimulation models differ in their respective strengths, justifying the\nexistence of both. While methods were generally rated as quite well developed,\ncombinatorial optimisation approaches, as well as machine learning, are\nidentified as important research methods to be developed further for ESA.\n"
    },
    {
        "paper_id": 2106.15698,
        "authors": "Sergio Consoli and Luca Tiozzo Pezzoli and Elisa Tosetti",
        "title": "Emotions in Macroeconomic News and their Impact on the European Bond\n  Market",
        "comments": "Journal of International Money and Finance (to appear); 39 pages; 14\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how emotions extracted from macroeconomic news can be used to explain\nand forecast future behaviour of sovereign bond yield spreads in Italy and\nSpain. We use a big, open-source, database known as Global Database of Events,\nLanguage and Tone to construct emotion indicators of bond market affective\nstates. We find that negative emotions extracted from news improve the\nforecasting power of government yield spread models during distressed periods\neven after controlling for the number of negative words present in the text. In\naddition, stronger negative emotions, such as panic, reveal useful information\nfor predicting changes in spread at the short-term horizon, while milder\nemotions, such as distress, are useful at longer time horizons. Emotions\ngenerated by the Italian political turmoil propagate to the Spanish news\naffecting this neighbourhood market.\n"
    },
    {
        "paper_id": 2106.15844,
        "authors": "Benjamin Patrick Evans, Mikhail Prokopenko",
        "title": "Bounded rationality for relaxing best response and mutual consistency:\n  The Quantal Hierarchy model of decision-making",
        "comments": "31 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While game theory has been transformative for decision-making, the\nassumptions made can be overly restrictive in certain instances. In this work,\nwe investigate some of the underlying assumptions of rationality, such as\nmutual consistency and best response, and consider ways to relax these\nassumptions using concepts from level-$k$ reasoning and quantal response\nequilibrium (QRE) respectively. Specifically, we propose an\ninformation-theoretic two-parameter model called the Quantal Hierarchy model,\nwhich can relax both mutual consistency and best response while still\napproximating level-$k$, QRE, or typical Nash equilibrium behaviour in the\nlimiting cases. The model is based on a recursive form of the variational free\nenergy principle, representing higher-order reasoning as (pseudo) sequential\ndecision-making in extensive-form game tree. This representation enables us to\ntreat simultaneous games in a similar manner to sequential games, where\nreasoning resources deplete throughout the game-tree. Bounds in player\nprocessing abilities are captured as information costs, where future branches\nof reasoning are discounted, implying a hierarchy of players where lower-level\nplayers have fewer processing resources. We demonstrate the effectiveness of\nthe Quantal Hierarchy model in several canonical economic games, {both\nsimultaneous and sequential}, using out-of-sample modelling.\n"
    },
    {
        "paper_id": 2106.15917,
        "authors": "R Vaidehi, A Bheemeshwar Reddy and Sudatta Banerjee",
        "title": "Explaining Caste-based Digital Divide in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the increasing importance of information and communication technologies\nin access to basic services like education and health, the question of the\ndigital divide based on caste assumes importance in India where large\nsocioeconomic disparities persist between different caste groups. Studies on\ncaste-based digital inequality are still scanty in India. Using nationally\nrepresentative survey data, this paper analyzes the first-level digital divide\n(ownership of computer and access to the internet) and the second-level digital\ndivide (individual's skill to use computer and the internet) between the\ndisadvantaged caste group and the others. Further, this paper identifies the\ncaste group-based differences in socioeconomic factors that contribute to the\ndigital divide between these groups using a non-linear decomposition method.\nThe results show that there exists a large first-level and second-level digital\ndivide between the disadvantaged caste groups and others in India. The\nnon-linear decomposition results indicate that the caste-based digital divide\nin India is rooted in historical socioeconomic deprivation of disadvantaged\ncaste groups. More than half of the caste-based digital gap is attributable to\ndifferences in educational attainment and income between the disadvantaged\ncaste groups and others. The findings of this study highlight the urgent need\nfor addressing educational and income inequality between the different caste\ngroups in India in order to bridge the digital divide.\n"
    },
    {
        "paper_id": 2106.16047,
        "authors": "Peter Tankov and Laura Tinsi",
        "title": "Decision making with dynamic probabilistic forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a sequential decision making process, such as renewable energy\ntrading or electrical production scheduling, whose outcome depends on the\nfuture realization of a random factor, such as a meteorological variable. We\nassume that the decision maker disposes of a dynamically updated probabilistic\nforecast (predictive distribution) of the random factor. We propose several\nstochastic models for the evolution of the probabilistic forecast, and show how\nthese models may be calibrated from ensemble forecasts, commonly provided by\nweather centers. We then show how these stochastic models can be used to\ndetermine optimal decision making strategies depending on the forecast updates.\nApplications to wind energy trading are given.\n"
    },
    {
        "paper_id": 2106.16088,
        "authors": "Supriya Bajpai",
        "title": "Application of deep reinforcement learning for Indian stock trading\n  automation",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In stock trading, feature extraction and trading strategy design are the two\nimportant tasks to achieve long-term benefits using machine learning\ntechniques. Several methods have been proposed to design trading strategy by\nacquiring trading signals to maximize the rewards. In the present paper the\ntheory of deep reinforcement learning is applied for stock trading strategy and\ninvestment decisions to Indian markets. The experiments are performed\nsystematically with three classical Deep Reinforcement Learning models Deep\nQ-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten\nIndian stock datasets. The performance of the models are evaluated and\ncomparison is made.\n"
    },
    {
        "paper_id": 2106.16117,
        "authors": "Julia Edigareva, Tatiana Khimich, Oleg Antonov, and Jesus Gonzalez",
        "title": "Energy security: key concepts, components, and change of the paradigm",
        "comments": "7 pages. Key words: energy security, energy concept, energy paradigm.\n  JEL codes: E-00; E-6",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The authors of the study conduct a legal analysis of the concept of energy\nsecurity. Energy is vital for sustainable development, and sustainability is\nnot only at the heart of development, but also economic, environmental, social\nand military policies. To ensure the sustainability of the policy, 'security'\nseems to be a mandatory goal to achieve. The article critically assesses the\nchange in the energy paradigm.\n"
    },
    {
        "paper_id": 2106.16149,
        "authors": "Carsten Chong and Thomas Delerue and Guoying Li",
        "title": "When Frictions are Fractional: Rough Noise in High-Frequency Data",
        "comments": "A previous version of this paper was circulated under the title\n  \"Mixed semimartingales: Volatility estimation in the presence of rough noise\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The analysis of high-frequency financial data is often impeded by the\npresence of noise. This article is motivated by intraday transactions data in\nwhich market microstructure noise appears to be rough, that is, best captured\nby a continuous-time stochastic process that locally behaves as fractional\nBrownian motion. Assuming that the underlying efficient price process follows a\ncontinuous It\\^o semimartingale, we derive consistent estimators and asymptotic\nconfidence intervals for the roughness parameter of the noise and the\nintegrated price and noise volatilities, in all cases where these quantities\nare identifiable. In addition to desirable features such as serial dependence\nof increments, compatibility between different sampling frequencies and diurnal\neffects, the rough noise model can further explain divergence rates in\nvolatility signature plots that vary considerably over time and between assets.\n"
    },
    {
        "paper_id": 2106.16177,
        "authors": "Ge-zhi Wu and Da-ming You",
        "title": "\"Stabilizer\" or \"catalyst\"? How green technology innovation affects the\n  risk of stock price crashes: an analysis based on the quantity and quality of\n  patents",
        "comments": "The article was completed in October 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To explore the relationship between corporate green technological innovation\nand the risk of stock price crashes, we first analyzed the data of listed\ncompanies in China from 2008 to 2018 and constructed indicators for the\nquantity and quality of corporate green technology innovation. The study found\nthat the quantity of green technology innovation is not related to the risk of\nstock price crashes, while the quality of green technology innovation is\nnegatively related to the risk of stock price crashes. Second, we studied the\nimpact of corporate ownership on the relationship between the quality of green\ntechnological innovation and the risk of stock price crashes and found that in\nnonstate-owned enterprises, the quality of green technological innovation is\nnegatively correlated with the risk of a stock price collapse, while in\nstate-owned enterprises, the quality of green technological innovation and the\nrisk of a stock price collapse are positive and not significant. Furthermore,\nwe studied the mediating effect of the number of negative news reports in the\nmedia of listed companies on the relationship between the quality of corporate\ngreen technology innovation and the stock price crash and found that the\nquality of green technology innovation is positively correlated with the number\nof negative news reports in the media of listed companies, while the number of\nnegative news reports in the media of listed companies is positively correlated\nwith the risk of a stock price collapse. Finally, we conducted a DID regression\nby using the impact of exogenous policy shocks on the quality of green\ntechnology innovation, and the main results passed the robustness test.\n"
    },
    {
        "paper_id": 2106.1624,
        "authors": "Kevin Kurt, R\\\"udiger Frey",
        "title": "Markov-Modulated Affine Processes",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": "10.1016/j.spa.2022.08.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study Markov-modulated affine processes (abbreviated MMAPs), a class of\nMarkov processes that are created from affine processes by allowing some of\ntheir coefficients to be a function of an exogenous Markov process. MMAPs allow\nfor richer models in various applications. At the same time MMAPs largely\npreserve the tractability of standard affine processes, as their characteristic\nfunction has a computationally convenient functional form. Our setup is a\nsubstantial generalization of earlier work, since we consider the case where\nthe generator of the exogenous process $X$ is an unbounded operator (as is the\ncase for diffusions or jump processes with infinite activity). We prove\nexistence of MMAPs via a martingale problem approach, we derive the formula for\ntheir characteristic function and we study various mathematical properties of\nMMAPs. The paper closes with a discussion of several applications of MMAPs in\nfinance.\n"
    },
    {
        "paper_id": 2107.00066,
        "authors": "Paul Bilokon and Antoine Jacquier and Conor McIndoe",
        "title": "Market regime classification with signatures",
        "comments": "14 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a data-driven algorithm to classify market regimes for time\nseries. We utilise the path signature, encoding time series into\neasy-to-describe objects, and provide a metric structure which establishes a\nconnection between separation of regimes and clustering of points.\n"
    },
    {
        "paper_id": 2107.00106,
        "authors": "Kaibalyapati Mishra",
        "title": "Choice of a Mentor: A Subjective Evaluation of Expectations, Experiences\n  and Feedbacks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent trends in academics show an increase in enrollment levels in higher\neducation Predominantly in Doctoral programmes where individual scholars\ninstitutes and supervisors play the key roles The human factor at receiving end\nof academic excellence is the scholar having a supervisor at the facilitating\nend In this paper I try to establish the role of different factors and\navailability of information about them in forming the basic choice set in a\nscholars mind After studying three different groups of individuals who were\nsubjected to substitutive choices we found that scholars prefer an\napproachable, moderately intervening and frequently interacting professor as\ntheir guide\n"
    },
    {
        "paper_id": 2107.00261,
        "authors": "Wei Dai, Yuan An, Wen Long",
        "title": "Price change prediction of ultra high frequency financial data based on\n  temporal convolutional network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through in-depth analysis of ultra high frequency (UHF) stock price change\ndata, more reasonable discrete dynamic distribution models are constructed in\nthis paper. Firstly, we classify the price changes into several categories.\nThen, temporal convolutional network (TCN) is utilized to predict the\nconditional probability for each category. Furthermore, attention mechanism is\nadded into the TCN architecture to model the time-varying distribution for\nstock price change data. Empirical research on constituent stocks of Chinese\nShenzhen Stock Exchange 100 Index (SZSE 100) found that the TCN framework model\nand the TCN (attention) framework have a better overall performance than GARCH\nfamily models and the long short-term memory (LSTM) framework model for the\ndescription of the dynamic process of the UHF stock price change sequence. In\naddition, the scale of the dataset reached nearly 10 million, to the best of\nour knowledge, there has been no previous attempt to apply TCN to such a\nlarge-scale UHF transaction price dataset in Chinese stock market.\n"
    },
    {
        "paper_id": 2107.00298,
        "authors": "Carol Alexander and Daniel Heck and Andreas Kaeck",
        "title": "The Role of Binance in Bitcoin Volatility Transmission",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We analyse high-frequency realised volatility dynamics and spillovers in the\nbitcoin market, focusing on two pairs: bitcoin against the US dollar (the main\nfiat-crypto pair) and trading bitcoin against tether (the main crypto-crypto\npair). We find that the tether-margined perpetual contract on Binance is\nclearly the main source of volatility, continuously transmitting strong flows\nto all other instruments and receiving only a little volatility. Moreover, we\nfind that (i) during US trading hours, traders pay more attention and are more\nreactive to prevailing market conditions when updating their expectations and\n(ii) the crypto market exhibits a higher interconnectedness when traditional\nWestern stock markets are open. Our results highlight that regulators should\nnot only consider spot exchanges offering bitcoin-fiat trading but also the\ntether-margined derivatives products available on most unregulated exchanges,\nmost importantly Binance.\n"
    },
    {
        "paper_id": 2107.0039,
        "authors": "Alexey Gubin, Valeri Lipunov, and Mattia Masolletti",
        "title": "Political and legal aspects of the COVID-19 pandemic impact on world\n  transport systems",
        "comments": "11 pages. 1 figure. Key words: pandemic, COVID-19, transportation,\n  logistics. JEL codes: K10",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The authors of the article analyze the impact of the global COVID-19 pandemic\non the transport and logistics sector. The research is interdisciplinary in\nnature. The purpose of the study is to identify and briefly characterize new\ntrends in the field of transport and cargo transportation in post-COVID\nconditions.\n"
    },
    {
        "paper_id": 2107.00427,
        "authors": "Wolfgang Schadner",
        "title": "Feasible Implied Correlation Matrices from Factor Structures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Forward-looking correlations are of interest in different financial\napplications, including factor-based asset pricing, forecasting stock-price\nmovements or pricing index options. With a focus on non-FX markets, this paper\ndefines necessary conditions for option implied correlation matrices to be\nmathematically and economically feasible and argues, that existing models are\ntypically not capable of guaranteeing so. To overcome this difficulty, the\nproblem is addressed from the underlying factor structure and introduces two\napproaches to solve it. Under the quantitative approach, the puzzle is\nreformulated into a nearest correlation matrix problem which can be used either\nas a stand-alone estimate or to re-establish positive-semi-definiteness of any\nother model's estimate. From an economic approach, it is discussed how expected\ncorrelations between stocks and risk factors (like CAPM, Fama-French) can be\ntranslated into a feasible implied correlation matrix. Empirical experiments\nare carried out on monthly option data of the S\\&P 100 and S\\&P 500 index\n(1996-2020).\n"
    },
    {
        "paper_id": 2107.00534,
        "authors": "Zijian Shi and John Cartlidge",
        "title": "The Limit Order Book Recreation Model (LOBRM): An Extended Analysis",
        "comments": "16 pages, preprint accepted for publication in the European\n  Conference on Machine Learning and Principles and Practice of Knowledge\n  Discovery in Databases (ECML-PKDD 2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The limit order book (LOB) depicts the fine-grained demand and supply\nrelationship for financial assets and is widely used in market microstructure\nstudies. Nevertheless, the availability and high cost of LOB data restrict its\nwider application. The LOB recreation model (LOBRM) was recently proposed to\nbridge this gap by synthesizing the LOB from trades and quotes (TAQ) data.\nHowever, in the original LOBRM study, there were two limitations: (1)\nexperiments were conducted on a relatively small dataset containing only one\nday of LOB data; and (2) the training and testing were performed in a\nnon-chronological fashion, which essentially re-frames the task as\ninterpolation and potentially introduces lookahead bias. In this study, we\nextend the research on LOBRM and further validate its use in real-world\napplication scenarios. We first advance the workflow of LOBRM by (1) adding a\ntime-weighted z-score standardization for the LOB and (2) substituting the\nordinary differential equation kernel with an exponential decay kernel to lower\ncomputation complexity. Experiments are conducted on the extended LOBSTER\ndataset in a chronological fashion, as it would be used in a real-world\napplication. We find that (1) LOBRM with decay kernel is superior to\ntraditional non-linear models, and module ensembling is effective; (2)\nprediction accuracy is negatively related to the volatility of order volumes\nresting in the LOB; (3) the proposed sparse encoding method for TAQ exhibits\ngood generalization ability and can facilitate manifold tasks; and (4) the\ninfluence of stochastic drift on prediction accuracy can be alleviated by\nincreasing historical samples.\n"
    },
    {
        "paper_id": 2107.00554,
        "authors": "Peter Carr, Roger Lee, Matthew Lorig",
        "title": "Robust Replication of Volatility and Hybrid Derivatives on Jump\n  Diffusions",
        "comments": "28 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We price and replicate a variety of claims written on the log price $X$ and\nquadratic variation $[X]$ of a risky asset, modeled as a positive\nsemimartingale, subject to stochastic volatility and jumps. The pricing and\nhedging formulas do not depend on the dynamics of volatility process, aside\nfrom integrability and independence assumptions; in particular, the volatility\nprocess may be non-Markovian and exhibit jumps of unknown distribution. The\njump risk may be driven by any finite activity Poisson random measure with\nbounded jump sizes. As hedging instruments, we use the underlying risky asset,\na zero-coupon bond, and European calls and puts with the same maturity as the\nclaim to be hedged. Examples of contracts that we price include variance swaps,\nvolatility swaps, a claim that pays the realized Sharpe ratio, and a call on a\nleveraged exchange traded fund.\n"
    },
    {
        "paper_id": 2107.00711,
        "authors": "Dmitry Levando",
        "title": "Formation of coalition structures as a non-cooperative game",
        "comments": "Submitted to the Dynamic Games and Applications; Special Issue: Group\n  Formation and Farsightedness",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We study coalition structure formation with intra and inter-coalition\nexternalities in the introduced family of nested non-cooperative simultaneous\nfinite games. A non-cooperative game embeds a coalition structure formation\nmechanism, and has two outcomes: an allocation of players over coalitions and a\npayoff for every player. Coalition structures of a game are described by Young\ndiagrams. They serve to enumerate coalition structures and allocations of\nplayers over them. For every coalition structure a player has a set of finite\nstrategies. A player chooses a coalition structure and a strategy.\n  A (social) mechanism eliminates conflicts in individual choices and produces\nfinal coalition structures. Every final coalition structure is a\nnon-cooperative game. Mixed equilibrium always exists and consists of a mixed\nstrategy profile, payoffs and equilibrium coalition structures. We use a\nmaximum coalition size to parametrize the family of the games. The\nnon-cooperative game of Nash is a partial case of the model. The result is\ndifferent from the Shapley value, a strong Nash, coalition-proof equilibria,\ncore solutions, and other equilibrium concepts. We supply few non-cooperative\ncoalition structure stability criteria.\n"
    },
    {
        "paper_id": 2107.00837,
        "authors": "Joseph Levine",
        "title": "Investment AUM Fee Costs: Evaluating a Simple Formula",
        "comments": "Updated acknowledgements",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How much do financial management fees cost investors? This article studies an\napproximate formula for the cumulative costs of annual Assets Under Management\n(AUM) fees. The formula states that an investment paying an annual fee of\n$\\epsilon$% of AUM over $N$ years loses almost $N\\epsilon$% of its value,\ncompared to an investment with the same returns paying no fee. The article\nexplains this formula intuitively, derives it analytically, and studies its\napproximation error. The article concludes with a discussion of the formula's\nuses and limitations.\n"
    },
    {
        "paper_id": 2107.0096,
        "authors": "Martin Bladt and Alexander J. McNeil",
        "title": "Time series models with infinite-order partial copula dependence",
        "comments": "30 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Stationary and ergodic time series can be constructed using an s-vine\ndecomposition based on sets of bivariate copula functions. The extension of\nsuch processes to infinite copula sequences is considered and shown to yield a\nrich class of models that generalizes Gaussian ARMA and ARFIMA processes to\nallow both non-Gaussian marginal behaviour and a non-Gaussian description of\nthe serial partial dependence structure. Extensions of classical causal and\ninvertible representations of linear processes to general s-vine processes are\nproposed and investigated. A practical and parsimonious method for\nparameterizing s-vine processes using the Kendall partial autocorrelation\nfunction is developed. The potential of the resulting models to give improved\nstatistical fits in many applications is indicated with an example using\nmacroeconomic data.\n"
    },
    {
        "paper_id": 2107.01017,
        "authors": "Angelo Garangau Menezes and Saulo Martiello Mastelini",
        "title": "MegazordNet: combining statistical and machine learning standpoints for\n  time series forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Forecasting financial time series is considered to be a difficult task due to\nthe chaotic feature of the series. Statistical approaches have shown solid\nresults in some specific problems such as predicting market direction and\nsingle-price of stocks; however, with the recent advances in deep learning and\nbig data techniques, new promising options have arises to tackle financial time\nseries forecasting. Moreover, recent literature has shown that employing a\ncombination of statistics and machine learning may improve accuracy in the\nforecasts in comparison to single solutions. Taking into consideration the\nmentioned aspects, in this work, we proposed the MegazordNet, a framework that\nexplores statistical features within a financial series combined with a\nstructured deep learning model for time series forecasting. We evaluated our\napproach predicting the closing price of stocks in the S&P 500 using different\nmetrics, and we were able to beat single statistical and machine learning\nmethods.\n"
    },
    {
        "paper_id": 2107.01031,
        "authors": "Sohrab Mokhtari, Kang K. Yen, Jin Liu",
        "title": "Effectiveness of Artificial Intelligence in Stock Market Prediction\n  based on Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5120/ijca2021921347",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper tries to address the problem of stock market prediction leveraging\nartificial intelligence (AI) strategies. The stock market prediction can be\nmodeled based on two principal analyses called technical and fundamental. In\nthe technical analysis approach, the regression machine learning (ML)\nalgorithms are employed to predict the stock price trend at the end of a\nbusiness day based on the historical price data. In contrast, in the\nfundamental analysis, the classification ML algorithms are applied to classify\nthe public sentiment based on news and social media. In the technical analysis,\nthe historical price data is exploited from Yahoo Finance, and in fundamental\nanalysis, public tweets on Twitter associated with the stock market are\ninvestigated to assess the impact of sentiments on the stock market's forecast.\nThe results show a median performance, implying that with the current\ntechnology of AI, it is too soon to claim AI can beat the stock markets.\n"
    },
    {
        "paper_id": 2107.01065,
        "authors": "Silvana M. Pesenti",
        "title": "Reverse Sensitivity Analysis for Risk Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the problem where a modeller conducts sensitivity analysis of a\nmodel consisting of random input factors, a corresponding random output of\ninterest, and a baseline probability measure. The modeller seeks to understand\nhow the model (the distribution of the input factors as well as the output)\nchanges under a stress on the output's distribution. Specifically, for a stress\non the output random variable, we derive the unique stressed distribution of\nthe output that is closest in the Wasserstein distance to the baseline output's\ndistribution and satisfies the stress. We further derive the stressed model,\nincluding the stressed distribution of the inputs, which can be calculated in a\nnumerically efficient way from a set of baseline Monte Carlo samples and which\nis implemented in the R package SWIM on CRAN.\n  The proposed reverse sensitivity analysis framework is model-free and allows\nfor stresses on the output such as (a) the mean and variance, (b) any\ndistortion risk measure including the Value-at-Risk and Expected-Shortfall, and\n(c) expected utility type constraints, thus making the reverse sensitivity\nanalysis framework suitable for risk models.\n"
    },
    {
        "paper_id": 2107.01095,
        "authors": "Eric Jacobson",
        "title": "Who Votes for Library Bonds? A Principal Component Exploration",
        "comments": "26 pages, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous research has shown a relationship between voter characteristics and\nvoter support for tax bonds. These findings, however, are difficult to\ninterpret because of the high degree of collinearity across the measures. From\n13 demographic measures of voters in a library bond election, seven independent\nprincipal components were extracted which accounted for 95 percent of the\nvariance. Whereas the direct demographic measures showed inconsistent\nrelationships with voting, the principal components of low SES, college\nexperience, female and service job were related to affirmative voting, while\nhigh home value was related to negative voting.\n"
    },
    {
        "paper_id": 2107.01098,
        "authors": "Devansh Bajpai and Rishi Ranjan Singh",
        "title": "Temporal Analysis of Worldwide War",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Analysis of wars and conflicts between regions has been an important topic of\ninterest throughout the history of humankind. In the latter part of the 20th\ncentury, in the aftermath of two World Wars and the shadow of nuclear,\nbiological, and chemical holocaust, more was written on the subject than ever\nbefore. Wars have a negative impact on a country's economy, social order,\ninfrastructure, and public health. In this paper, we study the wars fought in\nhistory and draw conclusions from that. We explore the participation of\ncountries in wars and the nature of relationships between various countries\nduring different timelines. A big part of today's wars is fought against\nterrorism. Therefore, this study also attempts to shed light on different\ncountries' exposure to terrorist encounters and analyses the impact of wars on\na country's economy in terms of change in GDP.\n"
    },
    {
        "paper_id": 2107.01273,
        "authors": "Naftali Cohen, Srijan Sood, Zhen Zeng, Tucker Balch, Manuela Veloso",
        "title": "Visual Time Series Forecasting: An Image-driven Approach",
        "comments": "This work was intended as a replacement of arXiv:2011.09052 and any\n  subsequent updates will appear there",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.\n"
    },
    {
        "paper_id": 2107.01352,
        "authors": "Zdzislaw Burda and Andrzej Jarosz",
        "title": "Cleaning large-dimensional covariance matrices for correlated samples",
        "comments": "16 pages, 12 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.105.034136",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We elucidate the problem of estimating large-dimensional covariance matrices\nin the presence of correlations between samples. To this end, we generalize the\nMarcenko-Pastur equation and the Ledoit-Peche shrinkage estimator using methods\nof random matrix theory and free probability. We develop an efficient algorithm\nthat implements the corresponding analytic formulas, based on the Ledoit-Wolf\nkernel estimation technique. We also provide an associated open-source Python\nlibrary, called \"shrinkage\", with a user-friendly API to assist in practical\ntasks of estimation of large covariance matrices. We present an example of its\nusage for synthetic data generated according to exponentially-decaying\nauto-correlations.\n"
    },
    {
        "paper_id": 2107.01532,
        "authors": "Julien Grenet, YingHua He, Dorothea K\\\"ubler",
        "title": "Decentralizing Centralized Matching Markets: Implications from Early\n  Offers in University Admissions",
        "comments": null,
        "journal-ref": "Journal of Political Economy, June 2022, Volume 130, Number 6, pp.\n  1427 - 1476",
        "doi": "10.1086/718983",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The matching literature often recommends market centralization under the\nassumption that agents know their own preferences and that their preferences\nare fixed. We find counterevidence to this assumption in a quasi-experiment. In\nGermany's university admissions, a clearinghouse implements the early stages of\nthe Gale-Shapley algorithm in real time. We show that early offers made in this\ndecentralized phase, although not more desirable, are accepted more often than\nlater ones. These results, together with survey evidence and a theoretical\nmodel, are consistent with students' costly learning about universities. We\npropose a hybrid mechanism to combine the advantages of decentralization and\ncentralization.\n  Published at The Journal of Political Economy under a new title, ``Preference\nDiscovery in University Admissions: The Case for Dynamic Multioffer\nMechanisms,'' available at https://doi.org/10.1086/718983 (Open Access).\n"
    },
    {
        "paper_id": 2107.01568,
        "authors": "Erhan Bayraktar, Christoph Czichowsky, Leonid Dolinskyi and Yan\n  Dolinsky",
        "title": "A Note on Utility Maximization with Proportional Transaction Costs and\n  Stability of Optimal Portfolios",
        "comments": "to appear in SIAM Journal on Financial Mathematics. arXiv admin note:\n  text overlap with arXiv:1912.08863",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this short note is to establish a limit theorem for the optimal\ntrading strategies in the setup of the utility maximization problem with\nproportional transaction costs. This limit theorem resolves the open question\nfrom [4]. The main idea of our proof is to establish a uniqueness result for\nthe optimal strategy. The proof of the uniqueness is heavily based on the dual\napproach which was developed recently in [6,7,8].\n"
    },
    {
        "paper_id": 2107.01611,
        "authors": "Mathieu Rosenbaum and Jianfei Zhang",
        "title": "Deep calibration of the quadratic rough Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The quadratic rough Heston model provides a natural way to encode Zumbach\neffect in the rough volatility paradigm. We apply multi-factor approximation\nand use deep learning methods to build an efficient calibration procedure for\nthis model. We show that the model is able to reproduce very well both SPX and\nVIX implied volatilities. We typically obtain VIX option prices within the\nbid-ask spread and an excellent fit of the SPX at-the-money skew. Moreover, we\nalso explain how to use the trained neural networks for hedging with\ninstantaneous computation of hedging quantities.\n"
    },
    {
        "paper_id": 2107.01629,
        "authors": "Ziwei Cong, Jia Liu, Puneet Manchanda",
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal\n  Random Forest",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A common belief about the growing medium of livestreaming is that its value\nlies in its \"live\" component. We examine this belief by comparing how the price\nelasticity of demand for live events varies before, on the day of, and after\nlivestream. We do this using unique and rich data from a large livestreaming\nplatform that allows consumers to purchase the recorded version of livestream\nafter the stream is over. A challenge in our context is that there exist\nhigh-dimensional confounders whose relationships with treatment policy (i.e.,\nprice) and outcome of interest (i.e., demand) are complex and only partially\nknown. We address this challenge via the use of a generalized Orthogonal Random\nForest framework for heterogeneous treatment effect estimation. We find\nsignificant temporal dynamics in the price elasticity of demand over the entire\nevent life-cycle. Specifically, demand becomes less price sensitive over time\nto the livestreaming day, turning to inelastic on that day. Over the\npost-livestream period, the demand for the recorded version is still sensitive\nto price, but much less than in the pre-livestream period. We further show that\nthis temporal variation in price elasticity is driven by the quality\nuncertainty inherent in such events and the opportunity of real-time\ninteraction with content creators during the livestream.\n"
    },
    {
        "paper_id": 2107.01696,
        "authors": "Matthew Smith and Yasaman Sarabi",
        "title": "Trading patterns within and between regions: an analysis of\n  Gould-Fernandez brokerage roles",
        "comments": "32 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines patterns of regionalisation in the International Trade\nNetwork (ITN). The study makes use of Gould Fernandez brokerage to examine the\nroles countries play in the ITN linking different regional partitions. An\nexamination of three ITNs is provided for three networks with varying levels of\ntechnological content, representing trade in high tech, medium tech, and\nlow-tech goods. Simulated network data, based on multiple approaches including\nan advanced network model controlling for degree centralisation and clustering\npatterns, is compared to the observed data to examine whether the roles\ncountries play within and between regions are a result of centralisation and\nclustering patterns. The findings indicate that the roles countries play\nbetween and within regions are indeed a result of centralisation patterns and\nchiefly clustering patterns; indicating a need to examine the presence of hubs\nwhen investigating regionalisation and globalisation patterns in the modern\nglobal economy.\n"
    },
    {
        "paper_id": 2107.0173,
        "authors": "Thomas Knispel, Roger J. A. Laeven, Gregor Svindland",
        "title": "Asymptotic Analysis of Risk Premia Induced by Law-Invariant Risk\n  Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the limiting behavior of the risk premium associated with the\nPareto optimal risk sharing contract in an infinitely expanding pool of risks\nunder a general class of law-invariant risk measures encompassing\nrank-dependent utility preferences. We show that the corresponding convergence\nrate is typically only $n^{1/2}$ instead of the conventional $n$, with $n$ the\nmultiplicity of risks in the pool, depending upon the precise risk preferences.\n"
    },
    {
        "paper_id": 2107.01731,
        "authors": "Salvatore Greco and Sajid Siraj and Michele Lundy",
        "title": "Supporting decisions by unleashing multiple mindsets using pairwise\n  comparisons method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inconsistency in pairwise comparison judgements is often perceived as an\nunwanted phenomenon and researchers have proposed a number of techniques to\neither reduce it or to correct it. We take a viewpoint that this inconsistency\nunleashes different mindsets of the decision maker(s) that should be taken into\naccount when generating recommendations as decision support. With this aim we\nconsider the spanning trees analysis which is a recently emerging idea for use\nwith the pairwise comparison approach that represents the plurality of mindsets\n(in terms of a plurality of vectors corresponding to different spanning trees).\nUntil now, the multiplicity of the vectors supplied by the spanning trees\napproach have been amalgamated into a single preference vector, losing the\ninformation about the plurality of mindsets. To preserve this information, we\npropose a novel methodology taking an approach similar to Stochastic\nMulti-criteria Acceptability Analysis. Considering all the rankings of\nalternatives corresponding to the different mindsets, our methodology gives the\nprobability that an alternative attains a given ranking position as well as the\nprobability that an alternative is preferred to another one. Since the\nexponential number of spanning trees makes their enumeration prohibitive, we\npropose computing approximate probabilities using statistical sampling of the\nspanning trees. Our approach is also appealing because it can be applied also\nto incomplete sets of pairwise comparisons. We demonstrate its usefulness with\na didactic example as well as with an application to a real-life case of\nselecting a Telecom backbone infrastructure for rural areas.\n"
    },
    {
        "paper_id": 2107.01746,
        "authors": "Giorgio Fabbri, Salvatore Federico, Davide Fiaschi, Fausto Gozzi",
        "title": "Mobility decisions, economic dynamics and epidemic",
        "comments": "33 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1007/s00199-023-01485-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model, which nests a susceptible-infected-recovered-deceased\n(SIRD) epidemic model into a dynamic macroeconomic equilibrium framework with\nagents' mobility. The latter affect both their income and their probability of\ninfecting and being infected. Strategic complementarities among individual\nmobility choices drive the evolution of aggregate economic activity, while\ninfection externalities caused by individual mobility affect disease diffusion.\nThe continuum of rational forward-looking agents coordinates on the Nash\nequilibrium of a discrete time, finite-state, infinite-horizon Mean Field Game.\nWe prove the existence of an equilibrium and provide a recursive construction\nmethod for the search of an equilibrium(a), which also guides our numerical\ninvestigations. We calibrate the model by using Italian experience on COVID-19\nepidemic and we discuss policy implications.\n"
    },
    {
        "paper_id": 2107.01978,
        "authors": "Ivan Guo, Gregoire Loeper, Jan Obloj, Shiyi Wang",
        "title": "Optimal transport for model calibration",
        "comments": "15 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a survey of recent results on model calibration by Optimal\nTransport. We present the general framework and then discuss the calibration of\nlocal, and local-stochastic, volatility models to European options, the joint\nVIX/SPX calibration problem as well as calibration to some path-dependent\noptions. We explain the numerical algorithms and present examples both on\nsynthetic and market data.\n"
    },
    {
        "paper_id": 2107.02242,
        "authors": "Shan Huang",
        "title": "Two Stochastic Control Problems In Capital Structure and Portfolio\n  Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis mainly focuses on two problems in capital structure and\nindividual's life-cycle portfolio choice. In the first problem, we derive a\nstochastic control model to optimize banks' dividend and recapitalization\npolicies and calibrate that to a sample of U.S. banks in the situation where we\nmodel banks' true accounting asset values as partially observed variables due\nto the opaqueness in banks' assets. By the calibrated model, the noise in\nreported accounting asset values hides about one-third of the true asset return\nvolatility and raises the banks' market equity value by 7.8\\% because the noise\nhides the banks' solvency risk from banking regulators. Particularly, those\nbanks with a high level of loan loss provisions, nonperforming assets, and real\nestate loans, and with a low volatility of reported total assets have noisy\naccounting asset values. Because of the substantial shock on the true asset\nvalues, the banks' assets were more opaque during the recent financial crisis.\nIn the second problem, we present an optimal portfolio selection model with\nvoluntary retirement option in an economic situation, where an investor is\nfacing borrowing and short sale constraints, as well as the cointegration\nbetween the stock and labor markets. Our model reinterprets the\nnon-participation puzzle in stock investment and early retirement in market\nbooms. Investor's willingness to retire earlier becomes stronger as risk\naversion increases or as wages decline in the long term. Consistent with the\nempirical evidence, we find that retirement flexibility makes the optimal\nportfolio invest less in the stock market. We also find that our\nmodel-generated portfolio share rises in wealth.\n"
    },
    {
        "paper_id": 2107.02283,
        "authors": "Liao Zhu, Ningning Sun, Martin T. Wells",
        "title": "Clustering Structure of Microstructure Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper builds the clustering model of measures of market microstructure\nfeatures which are popular in predicting stock returns. In a 10-second\ntime-frequency, we study the clustering structure of different measures to find\nout the best ones for predicting. In this way, we can predict more accurately\nwith a limited number of predictors, which removes the noise and makes the\nmodel more interpretable.\n"
    },
    {
        "paper_id": 2107.02394,
        "authors": "Prateek Bansal, Roselinde Kessels, Rico Krueger, Daniel J Graham",
        "title": "Face masks, vaccination rates and low crowding drive the demand for the\n  London Underground during the COVID-19 pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has drastically impacted people's travel behaviour and\nout-of-home activity participation. While countermeasures are being eased with\nincreasing vaccination rates, the demand for public transport remains\nuncertain. To investigate user preferences to travel by London Underground\nduring the pandemic, we conducted a stated choice experiment among its\npre-pandemic users (N=961). We analysed the collected data using multinomial\nand mixed logit models. Our analysis provides insights into the sensitivity of\nthe demand for the London Underground with respect to travel attributes\n(crowding density and travel time), the epidemic situation (confirmed new\nCOVID-19 cases), and interventions (vaccination rates and mandatory face\nmasks). Mandatory face masks and higher vaccination rates are the top two\ndrivers of travel demand for the London Underground during COVID-19. The\npositive impact of vaccination rates on the Underground demand increases with\ncrowding density, and the positive effect of mandatory face masks decreases\nwith travel time. Mixed logit reveals substantial preference heterogeneity. For\ninstance, while the average effect of mandatory face masks is positive,\npreferences of around 20% of the pre-pandemic users to travel by the\nUnderground are negatively affected. The estimated demand sensitivities are\nrelevant for supply-demand management in transit systems and the calibration of\nadvanced epidemiological models.\n"
    },
    {
        "paper_id": 2107.02512,
        "authors": "Francesca Micocci and Armando Rungi",
        "title": "Predicting Exporters with Machine Learning",
        "comments": null,
        "journal-ref": "World Trade Review , Volume 22 , Issue 5 , December 2023 , pp. 584\n  - 607",
        "doi": "10.1017/S1474745623000265",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this contribution, we exploit machine learning techniques to evaluate\nwhether and how close firms are to becoming successful exporters. First, we\ntrain and test various algorithms using financial information on both exporters\nand non-exporters in France in 2010-2018. Thus, we show that we are able to\npredict the distance of non-exporters from export status. In particular, we\nfind that a Bayesian Additive Regression Tree with Missingness In Attributes\n(BART-MIA) performs better than other techniques with an accuracy of up to\n0.90. Predictions are robust to changes in definitions of exporters and in the\npresence of discontinuous exporting activity. Eventually, we discuss how our\nexporting scores can be helpful for trade promotion, trade credit, and\nassessing aggregate trade potential. For example, back-of-the-envelope\nestimates show that a representative firm with just below-average exporting\nscores needs up to 44% more cash resources and up to 2.5 times more capital to\nget to foreign markets.\n"
    },
    {
        "paper_id": 2107.02537,
        "authors": "Yacine Koucha and Alfredo D. Egidio dos Reis",
        "title": "Approximations to ultimate ruin probabilities with a Wienner process\n  perturbation",
        "comments": "Master dissertation work, 18 pages, 4 figures, 8 numerical tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we adapt the classic Cram\\'er-Lundberg collective risk theory\nmodel to a perturbed model by adding a Wiener process to the compound Poisson\nprocess, which can be used to incorporate premium income uncertainty, interest\nrate fluctuations and changes in the number of policyholders. Our study is part\nof a Master dissertation, our aim is to make a short overview and present\nadditionally some new approximation methods for the infinite time ruin\nprobabilities for the perturbed risk model. We present four different\napproximation methods for the perturbed risk model. The first method is based\non iterative upper and lower approximations to the maximal aggregate loss\ndistribution. The second method relies on a four-moment exponential De Vylder\napproximation. The third method is based on the first-order Pad\\'e\napproximation of the Renyi and De Vylder approximations. The last method is the\nsecond order Pad\\'e-Ramsay approximation. These are generated by fitting one,\ntwo, three or four moments of the claim amount distribution, which greatly\ngeneralizes the approximations. We test the precision of approximations using a\ncombination of light and heavy tailed distributions for the individual claim\namount. We assess the ultimate ruin probability and present numerical results\nfor the exponential, gamma, and mixed exponential claim distributions,\ndemonstrating the high accuracy of these four methods. Analytical and numerical\nmethods are used to highlight the practical implications of our findings.\n"
    },
    {
        "paper_id": 2107.02628,
        "authors": "Francesca Biagini, Thomas Reitsam",
        "title": "A dynamic version of the super-replication theorem under proportional\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the super-replication theorems of [27] in a dynamic setting, both\nin the num\\'eraire-based as well as in the num\\'eraire-free setting. For this\npurpose, we generalize the notion of admissible strategies. In particular, we\nobtain a well-defined super-replication price process, which is\nright-continuous under some regularity assumptions.\n"
    },
    {
        "paper_id": 2107.02633,
        "authors": "Luis E C Rocha and Petter Holme and Claudio D G Linhares",
        "title": "The global migration network of sex-workers",
        "comments": "Comments and feedback welcomed. Two tables and 6 figures including SI",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Differences in the social and economic environment across countries encourage\nhumans to migrate in search of better living conditions, including job\nopportunities, higher salaries, security and welfare. Quantifying global\nmigration is, however, challenging because of poor recording, privacy issues\nand residence status. This is particularly critical for some classes of\nmigrants involved in stigmatised, unregulated or illegal activities. Escorting\nservices or high-end prostitution are well-paid activities that attract workers\nall around the world. In this paper, we study international migration patterns\nof sex-workers by using network methods. Using an extensive international\nonline advertisement directory of escorting services and information about\nindividual escorts, we reconstruct a migrant flow network where nodes represent\neither origin or destination countries. The links represent the direct routes\nbetween two countries. The migration network of sex-workers shows different\nstructural patterns than the migration of the general population. The network\ncontains a strong core where mutual migration is often observed between a group\nof high-income European countries, yet Europe is split into different network\ncommunities with specific ties to non-European countries. We find\nnon-reciprocal relations between countries, with some of them mostly offering\nwhile others attract workers. The GDP per capita is a good indicator of country\nattractiveness for incoming workers and service rates but is unrelated to the\nprobability of emigration. The median financial gain of migrating, in\ncomparison to working at the home country, is 15.9%. Only sex-workers coming\nfrom 77% of the countries have financial gains with migration and average gains\ndecrease with the GDPc of the country of origin. Our results shows that\nhigh-end sex-worker migration is regulated by economic, geographic and cultural\naspects.\n"
    },
    {
        "paper_id": 2107.02656,
        "authors": "Xiaoqing Liang, Ruodu Wang, Virginia Young",
        "title": "Optimal Insurance to Maximize RDEU Under a Distortion-Deviation Premium\n  Principle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study an optimal insurance problem for a risk-averse\nindividual who seeks to maximize the rank-dependent expected utility (RDEU) of\nher terminal wealth, and insurance is priced via a general distortion-deviation\npremium principle. We prove necessary and sufficient conditions satisfied by\nthe optimal solution and consider three ambiguity orders to further determine\nthe optimal indemnity. Finally, we analyze examples under three\ndistortion-deviation premium principles to explore the specific conditions\nunder which no insurance or deductible insurance is optimal.\n"
    },
    {
        "paper_id": 2107.02764,
        "authors": "Arthur Charpentier and Lariosse Kouakou and Matthias L\\\"owe and\n  Philipp Ratz and Franck Vermet",
        "title": "Collaborative Insurance Sustainability and Network Structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The peer-to-peer (P2P) economy has been growing with the advent of the\nInternet, with well known brands such as Uber or Airbnb being examples thereof.\nIn the insurance sector the approach is still in its infancy, but some\ncompanies have started to explore P2P-based collaborative insurance products\n(eg. Lemonade in the U.S. or Inspeer in France). The actuarial literature only\nrecently started to consider those risk sharing mechanisms, as in Denuit and\nRobert (2021) or Feng et al. (2021). In this paper, describe and analyse such a\nP2P product, with some reciprocal risk sharing contracts. Here, we consider the\ncase where policyholders still have an insurance contract, but the first\nself-insurance layer, below the deductible, can be shared with friends. We\nstudy the impact of the shape of the network (through the distribution of\ndegrees) on the risk reduction. We consider also some optimal setting of the\nreciprocal commitments, and discuss the introduction of contracts with friends\nof friends to mitigate some possible drawbacks of having people without enough\nconnections to exchange risks.\n"
    },
    {
        "paper_id": 2107.02775,
        "authors": "Ayesha Ali, Ihsan Ayyub Qazi",
        "title": "Countering Misinformation on Social Media Through Educational\n  Interventions: Evidence from a Randomized Experiment in Pakistan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fake news is a growing problem in developing countries with potentially\nfar-reaching consequences. We conduct a randomized experiment in urban Pakistan\nto evaluate the effectiveness of two educational interventions to counter\nmisinformation among low-digital literacy populations. We do not find a\nsignificant effect of video-based general educational messages about\nmisinformation. However, when such messages are augmented with personalized\nfeedback based on individuals' past engagement with fake news, we find an\nimprovement of 0.14 standard deviations in identifying fake news. We also find\nnegative but insignificant effects on identifying true news, driven by female\nrespondents. Our results suggest that educational interventions can enable\ninformation discernment but their effectiveness critically depends on how well\ntheir features and delivery are customized for the population of interest.\n"
    },
    {
        "paper_id": 2107.02888,
        "authors": "Nickolas Gagnon, Riccardo D. Saulle, Henrik W. Zaunbrecher",
        "title": "Decreasing Incomes Increase Selfishness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a controlled laboratory experiment to study the causal impact of\nincome decreases within a time period on redistribution decisions at the end of\nthat period, in an environment where we keep fixed the sum of incomes over the\nperiod. First, we investigate the effect of a negative income trend\n(intra-personal decrease), which means a decreasing income compared to one's\nrecent past. Second, we investigate the effect ofa negative income trend\nrelative to the income trend of another person (inter-personal decrease). If\nintra-personal or inter-personal decreases create dissatisfaction for an\nindividual, that person may become more selfish to obtain compensation. We\nformal-ize both effects in a multi-period model augmenting a standard model of\ninequality aversion. Overall, conditional on exhibiting sufficiently-strong\nsocial preferences, we find that individuals indeed behave more selfishly when\nthey experience decreasing incomes. While many studies examine the effect of\nincome inequality on redistribution decisions, we delve into the history behind\none's income to isolate the effect of income changes.\n"
    },
    {
        "paper_id": 2107.03034,
        "authors": "Eunjung Cho and Youngsang Cho",
        "title": "Estimating the economic value of ultrafine particles information: A\n  contingent valuation method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Global concern regarding ultrafine particles (UFPs), which are particulate\nmatter (PM) with a diameter of less than 100nm, is increasing. These\nparticles-with more serious health effects than PM less than 2.5 micrometers\n(PM2.5)-are difficult to measure using the current methods because their\ncharacteristics are different from those of other air pollutants. Therefore, a\nnew monitoring system is required to obtain accurate UFPs information, which\nwill raise the financial burden of the government and people. In this study, we\nestimated the economic value of UFPs information by evaluating the\nwillingness-to-pay (WTP) for the UFPs monitoring and reporting system. We used\nthe contingent valuation method (CVM) and the one-and-one-half-bounded\ndichotomous choice (OOHBDC) spike model. We analyzed how the respondents'\nsocio-economic variables, as well as their cognition level of PM, affected\ntheir WTP. Therefore, we collected WTP data of 1,040 Korean respondents through\nan online survey. The estimated mean WTP for building a UFPs monitoring and\nreporting system is KRW 6,958.55-7,222.55 (USD 6.22-6.45) per household per\nyear. We found that people satisfied with the current air pollutant\ninformation, and generally possessing relatively greater knowledge of UFPs,\nhave higher WTP for a UFPs monitoring and reporting system. The results can be\nused to establish new policies response to PM including UFPs.\n"
    },
    {
        "paper_id": 2107.03116,
        "authors": "Elena Rudakova, Alla Pavlova, Oleg Antonov, Kira Kuntsevich and Yue\n  Yang",
        "title": "Economic prospects of the Russian-Chinese partnership in the logistics\n  projects of the Eurasian Economic Union and the Silk Road Economic Belt: a\n  scientific literature review",
        "comments": "13 pages. Key words: logistics, partnership, Eurasian Economic Union,\n  Silk Road Economic Belt. JEL codes: F-01; F-02; F-15",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The authors of the article have reviewed the scientific literature on the\ndevelopment of the Russian-Chinese cooperation in the field of combining\neconomic and logistics projects of the Eurasian Economic Union and the Silk\nRoad Economic Belt. The opinions of not only Russian, but also Chinese experts\non these projects are indicated, which provides the expansion of the vision of\nthe concept of the New Silk Road in both countries.\n"
    },
    {
        "paper_id": 2107.03299,
        "authors": "Ali B. Barlas (BBVA Research), Seda Guler Mert (BBVA Research), Berk\n  Orkun Isa (BBVA Research) Alvaro Ortiz (BBVA Research), Tomasa Rodrigo (BBVA\n  Research), Baris Soybilgen (Bilgi University) and Ege Yazgan (Bilgi\n  University)",
        "title": "Big Data Information and Nowcasting: Consumption and Investment from\n  Bank Transactions in Turkey",
        "comments": "31 pages, 7 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use the aggregate information from individual-to-firm and firm-to-firm in\nGaranti BBVA Bank transactions to mimic domestic private demand. Particularly,\nwe replicate the quarterly national accounts aggregate consumption and\ninvestment (gross fixed capital formation) and its bigger components (Machinery\nand Equipment and Construction) in real time for the case of Turkey. In order\nto validate the usefulness of the information derived from these indicators we\ntest the nowcasting ability of both indicators to nowcast the Turkish GDP using\ndifferent nowcasting models. The results are successful and confirm the\nusefulness of Consumption and Investment Banking transactions for nowcasting\npurposes. The value of the Big data information is more relevant at the\nbeginning of the nowcasting process, when the traditional hard data information\nis scarce. This makes this information specially relevant for those countries\nwhere statistical release lags are longer like the Emerging Markets.\n"
    },
    {
        "paper_id": 2107.0334,
        "authors": "Wing Fung Chong and Haoen Cui and Yuxuan Li",
        "title": "Pseudo-Model-Free Hedging for Variable Annuities via Deep Reinforcement\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes a two-phase deep reinforcement learning approach, for\nhedging variable annuity contracts with both GMMB and GMDB riders, which can\naddress model miscalibration in Black-Scholes financial and constant force of\nmortality actuarial market environments. In the training phase, an infant\nreinforcement learning agent interacts with a pre-designed training\nenvironment, collects sequential anchor-hedging reward signals, and gradually\nlearns how to hedge the contracts. As expected, after a sufficient number of\ntraining steps, the trained reinforcement learning agent hedges, in the\ntraining environment, equally well as the correct Delta while outperforms\nmisspecified Deltas. In the online learning phase, the trained reinforcement\nlearning agent interacts with the market environment in real time, collects\nsingle terminal reward signals, and self-revises its hedging strategy. The\nhedging performance of the further trained reinforcement learning agent is\ndemonstrated via an illustrative example on a rolling basis to reveal the\nself-revision capability on the hedging strategy by online learning.\n"
    },
    {
        "paper_id": 2107.03577,
        "authors": "Khalid El-Awady",
        "title": "Adaptive Stress Testing for Adversarial Learning in a Financial\n  Environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We demonstrate the use of Adaptive Stress Testing to detect and address\npotential vulnerabilities in a financial environment. We develop a simplified\nmodel for credit card fraud detection that utilizes a linear regression\nclassifier based on historical payment transaction data coupled with business\nrules. We then apply the reinforcement learning model known as Adaptive Stress\nTesting to train an agent, that can be thought of as a potential fraudster, to\nfind the most likely path to system failure -- successfully defrauding the\nsystem. We show the connection between this most likely failure path and the\nlimits of the classifier and discuss how the fraud detection system's business\nrules can be further augmented to mitigate these failure modes.\n"
    },
    {
        "paper_id": 2107.03674,
        "authors": "Mikkel Bennedsen, Asger Lunde, Neil Shephard, Almut E. D. Veraart",
        "title": "Inference and forecasting for continuous-time integer-valued trawl\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops likelihood-based methods for estimation, inference, model\nselection, and forecasting of continuous-time integer-valued trawl processes.\nThe full likelihood of integer-valued trawl processes is, in general, highly\nintractable, motivating the use of composite likelihood methods, where we\nconsider the pairwise likelihood in lieu of the full likelihood. Maximizing the\npairwise likelihood of the data yields an estimator of the parameter vector of\nthe model, and we prove consistency and, in the short memory case, asymptotic\nnormality of this estimator. When the underlying trawl process has long memory,\nthe asymptotic behaviour of the estimator is more involved; we present some\npartial results for this case. The pairwise approach further allows us to\ndevelop probabilistic forecasting methods, which can be used to construct the\npredictive distribution of integer-valued time series. In a simulation study,\nwe document the good finite sample performance of the likelihood-based\nestimator and the associated model selection procedure. Lastly, the methods are\nillustrated in an application to modelling and forecasting financial bid-ask\nspread data, where we find that it is beneficial to carefully model both the\nmarginal distribution and the autocorrelation structure of the data.\n"
    },
    {
        "paper_id": 2107.03712,
        "authors": "Emmanuel Coffie",
        "title": "Numerical approximation of hybrid Poisson-jump Ait-Sahalia-type interest\n  rate model with delay",
        "comments": "this article supersedes arXiv:2103.07651",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the original Ait-Sahalia interest rate model has been found\nconsiderable use as a model for describing time series evolution of interest\nrates, it may not possess adequate specifications to explain responses of\ninterest rates to empirical phenomena such as volatility 'skews' and 'smiles',\njump behaviour, market regulatory lapses, economic crisis, financial clashes,\npolitical instability, among others collectively. The aim of this paper is to\npropose a modified version of this model by incorporating additional features\nto collectively describe these empirical phenomena adequately. Moreover, due to\nlack of a closed-form solution to the proposed model, we employ several new\ntruncated EM techniques to examine this model and justify the scheme within\nMonte Carlo framework to compute expected payoffs of some financial quantities\nsuch as a bond and a barrier option.\n"
    },
    {
        "paper_id": 2107.03764,
        "authors": "Patrick Reinwald, Stephan Leitner and Friederike Wall",
        "title": "Limited intelligence and performance-based compensation: An agent-based\n  model of the hidden action problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models of economic decision makers often include idealized assumptions, such\nas rationality, perfect foresight, and access to all relevant pieces of\ninformation. These assumptions often assure the models' internal validity, but,\nat the same time, might limit the models' power to explain empirical phenomena.\nThis paper is particularly concerned with the model of the hidden action\nproblem, which proposes an optimal performance-based sharing rule for\nsituations in which a principal assigns a task to an agent, and the action\ntaken to carry out this task is not observable by the principal. We follow the\nagentization approach and introduce an agent-based version of the hidden action\nproblem, in which some of the idealized assumptions about the principal and the\nagent are relaxed so that they only have limited information access, are\nendowed with the ability to gain information, and store it in and retrieve it\nfrom their (limited) memory. We follow an evolutionary approach and analyze how\nthe principal's and the agent's decisions affect the sharing rule, task\nperformance, and their utility over time. The results indicate that the optimal\nsharing rule does not emerge. The principal's utility is relatively robust to\nvariations in intelligence, while the agent's utility is highly sensitive to\nlimitations in intelligence. The principal's behavior appears to be driven by\nopportunism, as she withholds a premium from the agent to assure the optimal\nutility for herself.\n"
    },
    {
        "paper_id": 2107.03857,
        "authors": "Christof Schmidhuber",
        "title": "Financial Markets and the Phase Transition between Water and Steam",
        "comments": "34 pages, 7 figures, significantly revised section 4, added\n  predictions for Hurst exponents",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.126873",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated by empirical observations on the interplay of trends and reversion,\na lattice gas model of financial markets is presented. The shares of an asset\nare modeled by gas molecules that are distributed across a hidden social\nnetwork of investors. The model is equivalent to the Ising model on this\nnetwork, whose magnetization represents the deviation of the asset price from\nits value. Moreover, the system should drive itself to its critical temperature\nin efficient markets. There, it is characterized by universal critical\nexponents, in analogy with the second-order phase transition between water and\nsteam. These critical exponents imply predictions for the auto-correlations of\nfinancial market returns and for Hurst exponents. For a simple network\ntopology, consistency with empirical observations implies a fractal network\ndimension near 3, and a correlation time at least as long as the economic cyle.\nTo also explain the observed market auto-correlations at intermediate scales,\nthe model should be extended beyond the critical domain, to other network\ntopologies, and to other models of critical dynamics.\n"
    },
    {
        "paper_id": 2107.03926,
        "authors": "Rian Dolphin, Barry Smyth, Yang Xu and Ruihai Dong",
        "title": "Measuring Financial Time Series Similarity With a View to Identifying\n  Profitable Stock Market Opportunities",
        "comments": "15 pages. Accepted for presentation at the International Conference\n  on Case-Based Reasoning 2021 (ICCBR)",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-86957-1_5",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Forecasting stock returns is a challenging problem due to the highly\nstochastic nature of the market and the vast array of factors and events that\ncan influence trading volume and prices. Nevertheless it has proven to be an\nattractive target for machine learning research because of the potential for\neven modest levels of prediction accuracy to deliver significant benefits. In\nthis paper, we describe a case-based reasoning approach to predicting stock\nmarket returns using only historical pricing data. We argue that one of the\nimpediments for case-based stock prediction has been the lack of a suitable\nsimilarity metric when it comes to identifying similar pricing histories as the\nbasis for a future prediction -- traditional Euclidean and correlation based\napproaches are not effective for a variety of reasons -- and in this regard, a\nkey contribution of this work is the development of a novel similarity metric\nfor comparing historical pricing data. We demonstrate the benefits of this\nmetric and the case-based approach in a real-world application in comparison to\na variety of conventional benchmarks.\n"
    },
    {
        "paper_id": 2107.03957,
        "authors": "Salma Khedr, Katrin Rehdanz, Roy Brouwer, Hanna Dijkstra, Sem\n  Duijndam, Pieter van Beukering, and Ikechukwu C. Okoli",
        "title": "Public preferences for marine plastic litter reductions across Europe",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Plastic pollution is one of the most challenging problems affecting the\nmarine environment of our time. Based on a unique dataset covering four\nEuropean seas and eight European countries, this paper adds to the limited\nempirical evidence base related to the societal welfare effects of marine\nlitter management. We use a discrete choice experiment to elicit public\nwillingness-to-pay (WTP) for macro and micro plastic removal to achieve Good\nEnvironmental Status across European seas as required by the European Marine\nStrategy Framework Directive. Using a common valuation design and following\nbest-practice guidelines, we draw meaningful comparisons between countries,\nseas and policy contexts. European citizens have strong preferences to improve\nthe environmental status of the marine environment by removing both micro and\nmacro plastic litter favouring a pan-European approach. However, public WTP\nestimates differ significantly across European countries and seas. We explain\nwhy and discuss implications for policymaking.\n"
    },
    {
        "paper_id": 2107.03979,
        "authors": "Daniel Hadley, Harry Joe, Natalia Nolde",
        "title": "On the Selection of Loss Severity Distributions to Model Operational\n  Risk",
        "comments": "Submitted to Journal of Operational Risk on October 19, 2018;\n  Accepted May 2, 2019",
        "journal-ref": "Journal of Operational Risk, 14(3):73-94 (2019)",
        "doi": "10.21314/JOP.2019.229",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate modeling of operational risk is important for a bank and the finance\nindustry as a whole to prepare for potentially catastrophic losses. One\napproach to modeling operational is the loss distribution approach, which\nrequires a bank to group operational losses into risk categories and select a\nloss frequency and severity distribution for each category. This approach\nestimates the annual operational loss distribution, and a bank must set aside\ncapital, called regulatory capital, equal to the 0.999 quantile of this\nestimated distribution. In practice, this approach may produce unstable\nregulatory capital calculations from year-to-year as selected loss severity\ndistribution families change. This paper presents truncation probability\nestimates for loss severity data and a consistent quantile scoring function on\nannual loss data as useful severity distribution selection criteria that may\nlead to more stable regulatory capital. Additionally, the Sinh-arcSinh\ndistribution is another flexible candidate family for modeling loss severities\nthat can be easily estimated using the maximum likelihood approach. Finally, we\nrecommend that loss frequencies below the minimum reporting threshold be\ncollected so that loss severity data can be treated as censored data.\n"
    },
    {
        "paper_id": 2107.04098,
        "authors": "Marcelo Ariel Fernandez, Kirill Rudov, Leeat Yariv",
        "title": "Centralized Matching with Incomplete Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impacts of incomplete information on centralized one-to-one\nmatching markets. We focus on the commonly used Deferred Acceptance mechanism\n(Gale and Shapley, 1962). We show that many complete-information results are\nfragile to a small infusion of uncertainty about others' preferences.\n"
    },
    {
        "paper_id": 2107.0426,
        "authors": "Christian Meier, Lingfei Li, Gongqiu Zhang",
        "title": "Simulation of Multidimensional Diffusions with Sticky Boundaries via\n  Markov Chain Approximation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new simulation method for multidimensional diffusions with\nsticky boundaries. The challenge comes from simulating the sticky boundary\nbehavior, for which standard methods like the Euler scheme fail. We approximate\nthe sticky diffusion process by a multidimensional continuous time Markov chain\n(CTMC), for which we can simulate easily. We develop two ways of constructing\nthe CTMC: approximating the infinitesimal generator of the sticky diffusion by\nfinite difference using standard coordinate directions, and matching the local\nmoments using the drift and the eigenvectors of the covariance matrix as\ntransition directions. The first approach does not always guarantee a valid\nMarkov chain whereas the second one can. We show that both construction methods\nyield a first order simulation scheme, which can capture the sticky behavior\nand it is free from the curse of dimensionality. We apply our method to two\napplications: a multidimensional Brownian motion with all dimensions sticky\nwhich arises as the limit of a queuing system with exceptional service policy,\nand a multi-factor short rate model for low interest rate environment in which\nthe stochastic factors are unbounded but the short rate is sticky at zero.\n"
    },
    {
        "paper_id": 2107.04358,
        "authors": "Patrick Mellacher",
        "title": "Endogenous viral mutations, evolutionary selection, and containment\n  policy design",
        "comments": null,
        "journal-ref": "Journal of Economic Interaction and Coordination 2022",
        "doi": "10.1007/s11403-021-00344-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How will the novel coronavirus evolve? I study a simple epidemiological\nmodel, in which mutations may change the properties of the virus and its\nassociated disease stochastically and antigenic drifts allow new variants to\npartially evade immunity. I show analytically that variants with higher\ninfectiousness, longer disease duration, and shorter latent period prove to be\nfitter. \"Smart\" containment policies targeting symptomatic individuals may\nredirect the evolution of the virus, as they give an edge to variants with a\nlonger incubation period and a higher share of asymptomatic infections. Reduced\nmortality, on the other hand, does not per se prove to be an evolutionary\nadvantage. I then implement this model as an agent-based simulation model in\norder to explore its aggregate dynamics. Monte Carlo simulations show that a)\ncontainment policy design has an impact on both speed and direction of viral\nevolution, b) the virus may circulate in the population indefinitely, provided\nthat containment efforts are too relaxed and the propensity of the virus to\nescape immunity is high enough, and crucially c) that it may not be possible to\ndistinguish between a slowly and a rapidly evolving virus by looking only at\nshort-term epidemiological outcomes. Thus, what looks like a successful\nmitigation strategy in the short run, may prove to have devastating long-run\neffects. These results suggest that optimal containment policy must take the\npropensity of the virus to mutate and escape immunity into account,\nstrengthening the case for genetic and antigenic surveillance even in the early\nstages of an epidemic.\n"
    },
    {
        "paper_id": 2107.04568,
        "authors": "Ren\\'e Carmona and Mathieu Lauri\\`ere",
        "title": "Deep Learning for Mean Field Games and Mean Field Control with\n  Applications to Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets and more generally macro-economic models involve a large\nnumber of individuals interacting through variables such as prices resulting\nfrom the aggregate behavior of all the agents. Mean field games have been\nintroduced to study Nash equilibria for such problems in the limit when the\nnumber of players is infinite. The theory has been extensively developed in the\npast decade, using both analytical and probabilistic tools, and a wide range of\napplications have been discovered, from economics to crowd motion. More\nrecently the interaction with machine learning has attracted a growing\ninterest. This aspect is particularly relevant to solve very large games with\ncomplex structures, in high dimension or with common sources of randomness. In\nthis chapter, we review the literature on the interplay between mean field\ngames and deep learning, with a focus on three families of methods. A special\nemphasis is given to financial applications.\n"
    },
    {
        "paper_id": 2107.04636,
        "authors": "Ayse Sinem Uysal, Xiaoyue Li, and John M. Mulvey",
        "title": "End-to-End Risk Budgeting Portfolio Optimization with Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Portfolio optimization has been a central problem in finance, often\napproached with two steps: calibrating the parameters and then solving an\noptimization problem. Yet, the two-step procedure sometimes encounter the\n\"error maximization\" problem where inaccuracy in parameter estimation\ntranslates to unwise allocation decisions. In this paper, we combine the\nprediction and optimization tasks in a single feed-forward neural network and\nimplement an end-to-end approach, where we learn the portfolio allocation\ndirectly from the input features. Two end-to-end portfolio constructions are\nincluded: a model-free network and a model-based network. The model-free\napproach is seen as a black-box, whereas in the model-based approach, we learn\nthe optimal risk contribution on the assets and solve the allocation with an\nimplicit optimization layer embedded in the neural network. The model-based\nend-to-end framework provides robust performance in the out-of-sample\n(2017-2021) tests when maximizing Sharpe ratio is used as the training\nobjective function, achieving a Sharpe ratio of 1.16 when nominal risk parity\nyields 0.79 and equal-weight fix-mix yields 0.83. Noticing that risk-based\nportfolios can be sensitive to the underlying asset universe, we develop an\nasset selection mechanism embedded in the neural network with stochastic gates,\nin order to prevent the portfolio being hurt by the low-volatility assets with\nlow returns. The gated end-to-end with filter outperforms the nominal\nrisk-parity benchmarks with naive filtering mechanism, boosting the Sharpe\nratio of the out-of-sample period (2017-2021) to 1.24 in the market data.\n"
    },
    {
        "paper_id": 2107.04698,
        "authors": "Alex Garivaltis",
        "title": "Waiting to Borrow From a 457(b) Plan",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper formulates and solves the optimal stopping problem for a loan made\nto one's self from a tax-advantaged retirement account such as a 401(k),\n403(b), or 457(b) plan. If the plan participant has access to an external asset\nwith a higher expected rate of return than the investment funds and indices\nthat are available within the retirement account, then he must decide how long\nto wait before exercising the loan option. On the one hand, taking the loan\nquickly will result in many years of exponential capital growth at the higher\n(external) rate; on the other hand, if we wait to accumulate more funds in the\n457(b), then we can make a larger deposit into the external asset (albeit for a\nshorter period of time). I derive a variety of cutoff rules for optimal loan\ncontrol; in general, the investor must wait until he accumulates a certain\namount of money (measured in contribution-years) that depends on the disparate\nyields, the loan parameters, and the date certain at which he will liquidate\nthe retirement account. Letting the horizon tend to infinity, the optimal\n(horizon-free) policy gains in elegance, simplicity, and practical robustness\nto different life outcomes. When asset prices and returns are stochastic, the\n(continuous time) cutoff rule turns into a \"wait region,\" whereby the mean of\nterminal wealth is rising and the variance of terminal wealth is falling. After\nhis sojourn through the wait region is over, the participant finds himself on\nthe mean-variance frontier, at which point his subsequent behavior is a matter\nof personal risk preference.\n"
    },
    {
        "paper_id": 2107.047,
        "authors": "Alfred Galichon",
        "title": "The unreasonable effectiveness of optimal transport in economics",
        "comments": "Submitted to the proceeding of the 2020 World Congress of the\n  Econometric Society",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Optimal transport has become part of the standard quantitative economics\ntoolbox. It is the framework of choice to describe models of matching with\ntransfers, but beyond that, it allows to: extend quantile regression; identify\ndiscrete choice models; provide new algorithms for computing the random\ncoefficient logit model; and generalize the gravity model in trade. This paper\noffer a brief review of the basics of the theory, its applications to\neconomics, and some extensions.\n"
    },
    {
        "paper_id": 2107.04925,
        "authors": "Daniel Guth, Shiyu Zhang",
        "title": "Geographic Spillover Effects of Prescription Drug Monitoring Programs\n  (PDMPs)",
        "comments": "Updated version only adds the bibliography, which was accidentally\n  omitted in previous Latex compilation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Prescription Drug Monitoring Programs (PDMPs) seek to potentially reduce\nopioid misuse by restricting the sale of opioids in a state. We examine\ndiscontinuities along state borders, where one side may have a PDMP and the\nother side may not. We find that electronic PDMP implementation, whereby\ndoctors and pharmacists can observe a patient's opioid purchase history,\nreduces a state's opioid sales but increases opioid sales in neighboring\ncounties on the other side of the state border. We also find systematic\ndifferences in opioid sales and mortality between border counties and interior\ncounties. These differences decrease when neighboring states both have ePDMPs,\nwhich is consistent with the hypothesis that individuals cross state lines to\npurchase opioids. Our work highlights the importance of understanding the\nopioid market as connected across counties or states, as we show that states\nare affected by the opioid policies of their neighbors.\n"
    },
    {
        "paper_id": 2107.04928,
        "authors": "Aniruddh Mohan, Shayak Sengupta, Parth Vaishnav, Rahul Tongia, Asim\n  Ahmed, Ines L. Azevedo",
        "title": "Sustained cost declines in solar PV and battery storage needed to\n  eliminate coal generation in India",
        "comments": "Forthcoming in Environmental Research Letters",
        "journal-ref": "Environ. Res. Lett. (2022) 17 114043",
        "doi": "10.1088/1748-9326/ac98d8",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Unabated coal power in India must be phased out by mid-century to achieve\nglobal climate targets under the Paris Agreement. Here we estimate the costs of\nhybrid power plants - lithium-ion battery storage with wind and solar PV - to\nreplace coal generation. We design least cost mixes of these technologies to\nsupply stylized baseload and load-following generation profiles in three Indian\nstates - Karnataka, Gujarat, and Tamil Nadu. Our analysis shows that\navailability of low cost capital, solar PV capital costs of at least $250/kW,\nand battery storage capacity costs at least 50% cheaper than current levels\nwill be required to phase out existing coal power plants. Phaseout by 2040\nrequires a 6% annual decline in the cost of hybrid systems over the next two\ndecades. We find that replacing coal generation with hybrid systems 99% of the\nhours over multiple decades is roughly 40% cheaper than 100% replacement,\nindicating a key role for other low cost grid flexibility mechanisms to help\nhasten coal phaseout. Solar PV is more suited to pairing with short duration\nstorage than wind power. Overall, our results describe the challenging\ntechnological and policy advances needed to achieve the temperature goals of\nthe Paris Agreement.\n"
    },
    {
        "paper_id": 2107.05041,
        "authors": "Avni Singh",
        "title": "E-Learning and its Socioeconomics",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While controversial, e-learning has become an essential tool for all kinds of\neducation: especially within the kindergarten-to-twelfth sector. However,\npockets of this sector lack access, mainly economically underserved students.\nThis paper explores the options available to underserved and aptly resourced\nmembers of the kindergarten-to-twelfth educational sector: a 250-million-person\nmarket, with only 9 million students enrolled in online education. The paper\nalso provides a brief overview of the options and challenges of making\ne-learning available to everyone in the kindergarten-to-twelfth educational\nsector. To establish whether e-learning is beneficial, it also discusses the\nresults of a survey conducted on students and educators who have experienced\ne-learning, with the results showing that it is beneficial, with a general\ntrend of teachers showing more comfort with online learning than students. The\npaper utilizes primary and secondary resources for this purpose, with\ninformation both from the internet, and from surveys conducted within people\nfrom the system: parents, students, and teachers.\n"
    },
    {
        "paper_id": 2107.05064,
        "authors": "Neeraja Gupta, Luca Rigotti and Alistair Wilson",
        "title": "The Experimenters' Dilemma: Inferential Preferences over Populations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We compare three populations commonly used in experiments by economists and\nother social scientists: undergraduate students at a physical location (lab),\nAmazon's Mechanical Turk (MTurk), and Prolific. The comparison is made along\nthree dimensions: the noise in the data due to inattention, the cost per\nobservation, and the elasticity of response. We draw samples from each\npopulation, examining decisions in four one-shot games with varying tensions\nbetween the individual and socially efficient choices. When there is no\ntension, where individual and pro-social incentives coincide, noisy behavior\naccounts for 60% of the observations on MTurk, 19% on Prolific, and 14% for the\nlab. Taking costs into account, if noisy data is the only concern Prolific\ndominates from an inferential power point of view, combining relatively low\nnoise with a cost per observation one fifth of the lab's. However, because the\nlab population is more sensitive to treatment, across our main PD game\ncomparison the lab still outperforms both Prolific and MTurk.\n"
    },
    {
        "paper_id": 2107.05163,
        "authors": "Jing Guo and Xue Dong He",
        "title": "Recursive Utility with Investment Gains and Losses: Existence,\n  Uniqueness, and Convergence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a generalization of the recursive utility model by adding a new\ncomponent that represents utility of investment gains and losses. We also study\nthe utility process in this generalized model with constant elasticity of\nintertemporal substitution and relative risk aversion degree, and with infinite\ntime horizon. In a specific, finite-state Markovian setting, we prove that the\nutility process uniquely exists when the agent derives nonnegative gain-loss\nutility, and that it can be non-existent or non-unique otherwise. Moreover, we\nprove that the utility process, when it uniquely exists, can be computed by\nstarting from any initial guess and applying the recursive equation that\ndefines the utility process repeatedly. We then consider a portfolio selection\nproblem with gain-loss utility and solve it by proving that the corresponding\ndynamic programming equation has a unique solution. Finally, we extend certain\nprevious results to the case in which the state space is infinite.\n"
    },
    {
        "paper_id": 2107.05201,
        "authors": "Hengxu Lin, Dong Zhou, Weiqing Liu, Jiang Bian",
        "title": "Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors\n  to Improve Covariance Matrix Estimation",
        "comments": "Published at ICAIF'21: ACM International Conference on AI in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modeling and managing portfolio risk is perhaps the most important step to\nachieve growing and preserving investment performance. Within the modern\nportfolio construction framework that built on Markowitz's theory, the\ncovariance matrix of stock returns is a required input to calculate portfolio\nrisk. Traditional approaches to estimate the covariance matrix are based on\nhuman-designed risk factors, which often require tremendous time and effort to\ndesign better risk factors to improve the covariance estimation. In this work,\nwe formulate the quest of mining risk factors as a learning problem and propose\na deep learning solution to effectively ``design'' risk factors with neural\nnetworks. The learning objective is also carefully set to ensure the learned\nrisk factors are effective in explaining the variance of stock returns as well\nas having desired orthogonality and stability. Our experiments on the stock\nmarket data demonstrate the effectiveness of the proposed solution: our method\ncan obtain $1.9\\%$ higher explained variance measured by $R^2$ and also reduce\nthe risk of a global minimum variance portfolio. The incremental analysis\nfurther supports our design of both the architecture and the learning\nobjective.\n"
    },
    {
        "paper_id": 2107.05251,
        "authors": "Carlos Garcia-Velasquez and Yvonne van der Meer",
        "title": "Can we improve the environmental benefits of biobased PET production\n  through local 1 biomass value chains? A life cycle assessment perspective",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jclepro.2022.135039",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The transition to a low-carbon economy is one of the ambitions of the\nEuropean Union for 2030. Biobased industries play an essential role in this\ntransition. However, there has been an on-going discussion about the actual\nbenefit of using biomass to produce biobased products, specifically the use of\nagricultural materials (e.g., corn and sugarcane). This paper presents the\nenvironmental impact assessment of 30% and 100% biobased PET (polyethylene\nterephthalate) production using EU biomass supply chains (e.g., sugar beet,\nwheat, and Miscanthus). An integral assessment between the life cycle\nassessment methodology and the global sensitivity assessment is presented as an\nearly-stage support tool to propose and select supply chains that improve the\nenvironmental performance of biobased PET production. From the results,\nMiscanthus is the best option for the production of biobased PET: promoting EU\nlocal supply chains, reducing greenhouse gas (GHG) emissions (process and\nland-use change), and generating lower impacts in midpoint categories related\nto resource depletion, ecosystem quality, and human health. This tool can help\nimproving the environmental performance of processes that could boost the shift\nto a low-carbon economy.\n"
    },
    {
        "paper_id": 2107.05263,
        "authors": "Giacomo Bormetti and Fulvio Corsi",
        "title": "A Lucas Critique Compliant SVAR model with Observation-driven\n  Time-varying Parameters",
        "comments": "48 pages, 10 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an observation-driven time-varying SVAR model where, in agreement\nwith the Lucas Critique, structural shocks drive both the evolution of the\nmacro variables and the dynamics of the VAR parameters. Contrary to existing\napproaches where parameters follow a stochastic process with random and\nexogenous shocks, our observation-driven specification allows the evolution of\nthe parameters to be driven by realized past structural shocks, thus opening\nthe possibility to gauge the impact of observed shocks and hypothetical policy\ninterventions on the future evolution of the economic system.\n"
    },
    {
        "paper_id": 2107.05347,
        "authors": "Iraj Daizadeh",
        "title": "Seasonal and Secular Periodicities Identified in the Dynamics of US FDA\n  Medical Devices (1976 2020) Portends Intrinsic Industrial Transformation and\n  Independence of Certain Crises",
        "comments": "5 figures, 4 tables. Ther Innov Regul Sci (2021)",
        "journal-ref": null,
        "doi": "10.1007/s43441-021-00334-4",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Background: The US Food and Drug Administration (FDA) regulates medical\ndevices (MD), which are predicated on a concoction of economic and policy\nforces (e.g., supply/demand, crises, patents). Assuming that the number of FDA\nMD (Premarketing Notifications (PMN), Approvals (PMAs), and their sum)\nApplications behaves similarly to those of other econometrics, this work\nexplores the hypothesis of the existence (and, if so, the length scale(s)) of\neconomic cycles (periodicities). Methods: Beyond summary statistics, the\nmonthly (May, 1976 to December, 2020) number of observed FDA MD Applications\nare investigated via an assortment of time series techniques (including:\nDiscrete Wavelet Transform, Running Moving Average Filter (RMAF), Complete\nEnsemble Empirical Mode with Adaptive Noise decomposition (CEEMDAN), and\nSeasonal Trend Loess (STL) decomposition) to exhaustively search and\ncharacterize such periodicities. Results: The data were found to be non-normal,\nnon-stationary (fractional order of integration < 1), non-linear, and strongly\npersistent (Hurst > 0.5). Importantly, periodicities exist and follow seasonal,\n1 year short-term, 5-6 year (Juglar), and a single 24-year medium-term\n(Kuznets) period (when considering the total number of MD Applications).\nEconomic crises (e.g., COVID-19) do not seem to affect the evolution of the\nperiodicities. Conclusions: This work concludes that (1) PMA and PMN data may\nbe viewed as a proxy measure of the MD industry; (2) periodicities exists in\nthe data with time lengths associated with seasonal/1-year, Juglar and Kuznets\naffects; (4) these metrics do not seem affected by specific crises (such as\nCOVID-19) (similarly with other econometrics used in periodicity assessments);\n(5) PMNs and PMAs evolve inversely and suggest a structural industrial\ntransformation; (6) Total MDs are predicted to continue their decline into the\nmid-2020s prior to recovery.\n"
    },
    {
        "paper_id": 2107.05359,
        "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer",
        "title": "Debt Swapping for Risk Mitigation in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3465456.3467638",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study financial networks where banks are connected by debt contracts. We\nconsider the operation of debt swapping when two creditor banks decide to\nexchange an incoming payment obligation, thus leading to a locally different\nnetwork structure. We say that a swap is positive if it is beneficial for both\nof the banks involved; we can interpret this notion either with respect to the\namount of assets received by the banks, or their exposure to different shocks\nthat might hit the system.\n  We analyze various properties of these swapping operations in financial\nnetworks. We first show that there can be no positive swap for any pair of\nbanks in a static financial system, or when a shock hits each bank in the\nnetwork proportionally. We then study worst-case shock models, when a shock of\ngiven size is distributed in the worst possible way for a specific bank. If the\ngoal of banks is to minimize their losses in such a worst-case setting, then a\npositive swap can indeed exist. We analyze the effects of such a positive swap\non other banks of the system, the computational complexity of finding a swap,\nand special cases where a swap can be found efficiently. Finally, we also\npresent some results for more complex swapping operations when the banks swap\nmultiple contracts, or when more than two banks participate in the swap.\n"
    },
    {
        "paper_id": 2107.05483,
        "authors": "Laurence Francis Lacey (Lacey Solutions Ltd., Skerries, County Dublin,\n  Ireland)",
        "title": "Characterization of the probability and information entropy of a process\n  with an increasing sample space by different functional forms of expansion,\n  with an application to hyperinflation",
        "comments": "19 pages, 1 table, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There is a random variable (X) with a determined outcome (i.e., X = x0),\np(x0) = 1. Consider x0 to have a discrete uniform distribution over the integer\ninterval [1, s], where the size of the sample space (s) = 1, in the initial\nstate, such that p(x0) = 1. What is the probability of x0 and the associated\ninformation entropy (H), as s increases by means of different functional forms\nof expansion? Such a process has been characterised in the case of (1) a\nmono-exponential expansion of the sample space; (2) a power function expansion;\n(3) double exponential expansion. The double exponential expansion of the\nsample space with time (from a natural log relationship between t and n)\ndescribes a \"hyperinflationary\" process. Over the period from the middle of\n1920 to the end of 1923, the purchasing power of the Weimar Republic paper Mark\nto purchase one gold Mark became close to zero (1 paper Mark = 10 to the power\nof -12 gold Mark). From the purchasing power of the paper Mark to purchase one\ngold Mark, the information entropy of this hyperinflationary process was\ndetermined.\n"
    },
    {
        "paper_id": 2107.05527,
        "authors": "Andrea Baronchelli",
        "title": "Collective intelligence and the blockchain: Technology, communities and\n  social experiments",
        "comments": "Brief \"perspective\" commentary piece",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchains are still perceived chiefly as a new technology. But each\nblockchain is also a community and a social experiment, built around social\nconsensus. Here I discuss three examples showing how collective intelligence\ncan help, threat or capitalize on blockchain-based ecosystems. They concern the\nimmutability of smart contracts, code transparency and new forms of property.\nThe examples show that more research, new norms and, eventually, laws are\nneeded to manage the interaction between collective behaviour and the\nblockchain technology. Insights from researchers in collective intelligence can\nhelp society rise up to the challenge.\n"
    },
    {
        "paper_id": 2107.05529,
        "authors": "Scott W. Hegerty",
        "title": "Are Rents Excessive in the Central City?: A Geospatial Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many U.S. central cities, property values are relatively low, while rents\nare closer to those in better-off neighborhoods. This gap can lead to\nrelatively large profits for landlords, and has been referred to as\n\"exploitaton\" for renters. While much of this gap might be explained by risk,\nfactors such as income and race might play important roles as well. This study\ncalculates Census tract-level measures of the rent-to-property-value (RPV)\nratio for 30 large cities and their surrounding metropolitan areas. After\nexamining the spatial distribution of this ratio and relationships with other\nsocioeconomic variables for Milwaukee and three other cities, Z-scores and\nquantiles are used to identify \"extreme\" RPV values nationwide. \"Rust Belt\"\ncities such as Detroit, Cleveland, and Milwaukee are shown to have higher\nmedian and 95% values than do West Coast cities such as Seattle and San\nFrancisco. A spatial lag regression estimation shows that, controlling for\nincome, property values, and vacancy rates, racial characteristics often have\nthe \"opposite\" signs from what might be expected and that there is little\nevidence of purely race-based \"exploitation\" of renters. A significantly\nnegative coefficient for the percentage of Black residents, for example, might\nsuggest that the RPV ratio is lower in a given tract, all else equal. While\nthis study shows where RPV values are highest within as well as between cities,\nfurther investigation might uncover the drivers of these spatial differences\nmore fully.\n"
    },
    {
        "paper_id": 2107.05535,
        "authors": "Nicklas Werge",
        "title": "Predicting Risk-adjusted Returns using an Asset Independent\n  Regime-switching Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets tend to switch between various market regimes over time,\nmaking stationarity-based models unsustainable. We construct a regime-switching\nmodel independent of asset classes for risk-adjusted return predictions based\non hidden Markov models. This framework can distinguish between market regimes\nin a wide range of financial markets such as the commodity, currency, stock,\nand fixed income market. The proposed method employs sticky features that\ndirectly affect the regime stickiness and thereby changing turnover levels. An\ninvestigation of our metric for risk-adjusted return predictions is conducted\nby analyzing daily financial market changes for almost twenty years. Empirical\ndemonstrations of out-of-sample observations obtain an accurate detection of\nbull, bear, and high volatility periods, improving risk-adjusted returns while\nkeeping a preferable turnover level.\n"
    },
    {
        "paper_id": 2107.05592,
        "authors": "Cynthia Pagliaro, Dhagash Mehta, Han-Tai Shiao, Shaofei Wang, Luwei\n  Xiong",
        "title": "Investor Behavior Modeling by Analyzing Financial Advisor Notes: A\n  Machine Learning Perspective",
        "comments": "8 pages, 2 column format, 7 figures+5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling investor behavior is crucial to identifying behavioral coaching\nopportunities for financial advisors. With the help of natural language\nprocessing (NLP) we analyze an unstructured (textual) dataset of financial\nadvisors' summary notes, taken after every investor conversation, to gain first\never insights into advisor-investor interactions. These insights are used to\npredict investor needs during adverse market conditions; thus allowing advisors\nto coach investors and help avoid inappropriate financial decision-making.\nFirst, we perform topic modeling to gain insight into the emerging topics and\ntrends. Based on this insight, we construct a supervised classification model\nto predict the probability that an advised investor will require behavioral\ncoaching during volatile market periods. To the best of our knowledge, ours is\nthe first work on exploring the advisor-investor relationship using\nunstructured data. This work may have far-reaching implications for both\ntraditional and emerging financial advisory service models like robo-advising.\n"
    },
    {
        "paper_id": 2107.05663,
        "authors": "Hirdesh K. Pharasi, Suchetana Sadhukhan, Parisa Majari, Anirban\n  Chakraborti, and Thomas H. Seligman",
        "title": "Dynamics of the market states in the space of correlation matrices with\n  applications to financial markets",
        "comments": "16 pages, 9 figures, to appear in the Proceedings Volume, Eds.\n  Anirban Chakraborti, Emmanuel Haven, Sudip Patra and Naresh Singh, \"Quantum\n  Decision Theory and Complexity Modelling in Economics and Public Policy\"\n  (Springer-Cham, New Economic Windows)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of states of financial markets based on correlations has gained\nincreasing attention during the last 10 years. We propose to retrace some\nimportant steps up to 2018, and then give a more detailed view of recent\ndevelopments that attempt to make the use of this more practical. Finally, we\ntry to give a glimpse to the future proposing the analysis of trajectories in\ncorrelation matrix space directly or in terms of symbolic dynamics as well as\nattempts to analyze the clusters that make up the states in a random matrix\ncontext.\n"
    },
    {
        "paper_id": 2107.05923,
        "authors": "Fabrizio Cipollini, Giampiero M. Gallo",
        "title": "Multiplicative Error Models: 20 years on",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Several phenomena are available representing market activity: volumes, number\nof trades, durations between trades or quotes, volatility - however measured -\nall share the feature to be represented as positive valued time series. When\nmodeled, persistence in their behavior and reaction to new information\nsuggested to adopt an autoregressive-type framework. The Multiplicative Error\nModel (MEM) is borne of an extension of the popular GARCH approach for modeling\nand forecasting conditional volatility of asset returns. It is obtained by\nmultiplicatively combining the conditional expectation of a process\n(deterministically dependent upon an information set at a previous time period)\nwith a random disturbance representing unpredictable news: MEMs have proved to\nparsimoniously achieve their task of producing good performing forecasts. In\nthis paper we discuss various aspects of model specification and inference both\nfor the univariate and the multivariate case. The applications are illustrative\nexamples of how the presence of a slow moving low-frequency component can\nimprove the properties of the estimated models.\n"
    },
    {
        "paper_id": 2107.06002,
        "authors": "Stephan Leitner and Friederike Wall",
        "title": "Micro-level dynamics in hidden action situations with limited\n  information",
        "comments": "36 pages, 6 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The hidden-action model provides an optimal sharing rule for situations in\nwhich a principal assigns a task to an agent who makes an effort to carry out\nthe task assigned to him. However, the principal can only observe the task\noutcome but not the agent's actual action, which is why the sharing rule can\nonly be based on the outcome. The hidden-action model builds on somewhat\nidealized assumptions about the principal's and the agent's capabilities\nrelated to information access. We propose an agent-based model that relaxes\nsome of these assumptions. Our analysis lays particular focus on the\nmicro-level dynamics triggered by limited access to information. For the\nprincipal's sphere, we identify the so-called Sisyphus effect that explains why\nthe sharing rule that provides the agent with incentives to take optimal action\nis difficult to achieve if the information is limited, and we identify factors\nthat moderate this effect. In addition, we analyze the behavioral dynamics in\nthe agent's sphere. We show that the agent might make even more of an effort\nthan optimal under unlimited access to information, which we refer to as excess\neffort. Interestingly, the principal can control the probability of making an\nexcess effort via the incentive mechanism. However, how much excess effort the\nagent finally makes is out of the principal's direct control.\n"
    },
    {
        "paper_id": 2107.06137,
        "authors": "Anton Bondarev and Frank C. Krysiak",
        "title": "Economic development and the structure of cross-technology interactions",
        "comments": null,
        "journal-ref": "European Economic Review (2021) Volume 132",
        "doi": "10.1016/j.euroecorev.2020.103628",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Most explanations of economic growth are based on knowledge spillovers, where\nthe development of some technologies facilitates the enhancement of others.\nEmpirical studies show that these spillovers can have a heterogeneous and\nrather complex structure. But, so far, little attention has been paid to the\nconsequences of different structures of such cross-technology interactions: Is\neconomic development more easily fostered by homogenous or heterogeneous\ninteractions, by uni- or bidirectional spillovers? Using a detailed description\nof an r&d sector with cross-technology interactions embedded in a simple growth\nmodel, we analyse how the structure of spillovers influences growth prospects\nand growth patterns. We show that some type of interactions (e.g., one-way\ninteractions) cannot induce exponential growth, whereas other structures can.\nFurthermore, depending on the structure of interactions, all or only some\ntechnologies will contribute to growth in the long run. Finally, some spillover\nstructures can lead to complex growth patterns, such as technology transitions,\nwhere, over time, different technology clusters are the main engine of growth.\n"
    },
    {
        "paper_id": 2107.06162,
        "authors": "Doris Folini, Felix K\\\"ubler, Aleksandra Malova, Simon Scheidegger",
        "title": "The climate in climate economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  To analyze climate change mitigation strategies, economists rely on\nsimplified climate models - climate emulators. We propose a generic and\ntransparent calibration and evaluation strategy for these climate emulators\nthat is based on Coupled Model Intercomparison Project, Phase 5 (CMIP5). We\ndemonstrate that the appropriate choice of the free model parameters can be of\nkey relevance for the predicted social cost of carbon. We propose to use four\ndifferent test cases: two tests to separately calibrate and evaluate the carbon\ncycle and temperature response, a test to quantify the transient climate\nresponse, and a final test to evaluate the performance for scenarios close to\nthose arising from economic models. We re-calibrate the climate part of the\nwidely used DICE-2016: the multi-model mean as well as extreme, but still\npermissible climate sensitivities and carbon cycle responses. We demonstrate\nthat the functional form of the climate emulator of the DICE-2016 model is fit\nfor purpose, despite its simplicity, but its carbon cycle and temperature\nequations are miscalibrated. We examine the importance of the calibration for\nthe social cost of carbon in the context of a partial equilibrium setting where\ninterest rates are exogenous, as well as the simple general equilibrium setting\nfrom DICE-2016. We find that the model uncertainty from different consistent\ncalibrations of the climate system can change the social cost of carbon by a\nfactor of four if one assumes a quadratic damage function. When calibrated to\nthe multi-model mean, our model predicts similar values for the social cost of\ncarbon as the original DICE-2016, but with a strongly reduced sensitivity to\nthe discount rate and about one degree less long-term warming. The social cost\nof carbon in DICE-2016 is oversensitive to the discount rate, leading to\nextreme comparative statics responses to changes in preferences.\n"
    },
    {
        "paper_id": 2107.06194,
        "authors": "Lara Dalmeyer and Tim Gebbie",
        "title": "Geometric insights into robust portfolio construction",
        "comments": "20 pages, 5 Figures, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate and extend the result that an alpha-weight angle from\nunconstrained quadratic portfolio optimisations has an upper bound dependent on\nthe condition number of the covariance matrix. This is known to imply that\nbetter conditioned covariance matrices produce weights from unconstrained\nmean-variance optimisations that are better aligned with each assets expected\nreturn. Here we relate the inequality between the alpha-weight angle and the\ncondition number to extend the result to include portfolio optimisations with\ngearing constraints to provide an extended family of robust optimisations. We\nuse this to argue that in general the equally weighted portfolio is not\npreferable to the mean-variance portfolio even with poor forecast ability and a\nbadly conditioned covariance matrix. We confirm the distribution free\ntheoretical arguments with a simple Gaussian simulation.\n"
    },
    {
        "paper_id": 2107.0621,
        "authors": "Sonia Oreffice and Dario Sansone",
        "title": "Sissy That Walk: Transportation to Work by Sexual Orientation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0263687",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze differences in mode of transportation to work by sexual\norientation, using the American Community Survey 2008-2019. Individuals in\nsame-sex couples are significantly less likely to drive to work than men and\nwomen in different-sex couples. This gap is particularly stark among men: on\naverage, almost 12 percentage point (or 13%) lower likelihood of driving to\nwork for men in same-sex couples. Individuals in same-sex couples are also more\nlikely to use public transport, walk, or bike to work: on average, men and\nwomen are 7 and 3 percentage points more likely, respectively, to take public\ntransportation to work than those in different-sex couples. These differences\npersist after controlling for demographic characteristics, partner's\ncharacteristics, location, fertility, and marital status. Additional evidence\nfrom the General Social Survey 2008-2018 suggests that these disparities by\nsexual orientation may be due to lesbian, gay, and bisexual individuals caring\nmore for the environment than straight individuals.\n"
    },
    {
        "paper_id": 2107.06349,
        "authors": "Ashish Kumar, Laszlo Markus, Norbert Hari",
        "title": "Arbitrage-free pricing of CVA for cross-currency swap with wrong-way\n  risk under stochastic correlation modeling framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A positive correlation between exposure and counterparty credit risk gives\nrise to the so-called Wrong-Way Risk (WWR). Even after a decade of the\nfinancial crisis, addressing WWR in both sound and tractable ways remains\nchallenging. Academicians have proposed arbitrage-free set-ups through copula\nmethods but those are computationally expensive and hard to use in practice.\nResampling methods are proposed by the industry but they lack mathematical\nfoundations. The purpose of this article is to bridge this gap between the\napproaches used by academicians and industry. To this end, we propose a\nstochastic correlation approach to asses WWR. The methods based on constant\ncorrelation to model the dependency between exposure and counterparty credit\nrisk assume a linear dependency, thus fail to capture the tail dependence.\nUsing a stochastic correlation we move further away from the Gaussian copula\nand can capture the tail risk. This effect is reflected in the results where\nthe impact of stochastic correlation on calculated CVA is substantial when\ncompared to the case when a high constant correlation is assumed between\nexposure and credit. Given the uncertainty inherent to CVA, the proposed method\nis believed to provide a promising way to model WWR.\n"
    },
    {
        "paper_id": 2107.0646,
        "authors": "Zongxia Liang, Yang Liu, Ming Ma, Rahul Pothi Vinoth",
        "title": "A Unified Formula of the Optimal Portfolio for Piecewise Hyperbolic\n  Absolute Risk Aversion Utilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a general family of piecewise hyperbolic absolute risk aversion\n(PHARA) utilities, including many classic and non-standard utilities as\nexamples. A typical application is the composition of a HARA preference and a\npiecewise linear payoff in asset allocation. We derive a unified closed-form\nformula of the optimal portfolio, which is a four-term division. The formula\nhas clear economic meanings, reflecting the behavior of risk aversion, risk\nseeking, loss aversion and first-order risk aversion. We conduct a general\nasymptotic analysis to the optimal portfolio, which directly serves as an\nanalytical tool for financial analysis. We compare this PHARA portfolio with\nthose of other utility families both analytically and numerically. One main\nfinding is that risk-taking behaviors are greatly increased by non-concavity\nand reduced by non-differentiability of the PHARA utility. Finally, we use\nfinancial data to test the performance of the PHARA portfolio in the market.\n"
    },
    {
        "paper_id": 2107.06518,
        "authors": "Suryadeepto Nag, Siddhartha P. Chakrabarty, Sankarshan Basu",
        "title": "Single Event Transition Risk: A Measure for Long Term Carbon Exposure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although there is a growing consensus that a low-carbon transition will be\nnecessary to mitigate the accelerated climate change, the magnitude of\ntransition-risk for investors is difficult to measure exactly. Investors are\ntherefore constrained by the unavailability of suitable measures to quantify\nthe magnitude of the risk and are forced to use the likes of absolute emissions\ndata or ESG scores in order to manage their portfolios. In this article, we\ndefine the Single Event Transition Risk (SETR) and illustrate how it can be\nused to approximate the magnitude of the total exposure of the price of a share\nto low-carbon transition. We also discuss potential applications of the single\nevent framework and the SETR as a risk measure and discuss future direction on\nhow this can be extended to a system with multiple transition events.\n"
    },
    {
        "paper_id": 2107.06544,
        "authors": "Davide Fiaschi, Cristina Tealdi",
        "title": "Winners and losers of immigration",
        "comments": "A (strong) revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of low-skilled immigration in a general equilibrium\nsearch and matching model, with heterogeneous workers producing intermediate\ngoods, which are used in the production of two final goods. In addition to\ncomplementarity/substitution between native and non-native workers, we explore\nhow immigration affects the relative prices of final goods and wages. An\napplication to Italy reveals a positive contribution of immigrants to GDP,\npublic revenues, and the per capita provision of public goods. Employers and\nemployees in the high-skilled-intensive market are winners, while losers are\nemployers in the low-skilled-intensive market. The effects on low-skilled\nemployees are instead inconclusive.\n"
    },
    {
        "paper_id": 2107.06593,
        "authors": "David Hobson, Martin Herdegen, Joseph Jerome",
        "title": "The Infinite Horizon Investment-Consumption Problem for Epstein-Zin\n  Stochastic Differential Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this article we consider the optimal investment-consumption problem for an\nagent with preferences governed by Epstein-Zin stochastic differential utility\nwho invests in a constant-parameter Black-Scholes-Merton market.\n  The paper has three main goals: first, to provide a detailed introduction to\ninfinite-horizon Epstein-Zin stochastic differential utility, including a\ndiscussion of which parameter combinations lead to a well-formulated problem;\nsecond, to prove existence and uniqueness of infinite horizon Epstein-Zin\nstochastic differential utility under a restriction on the parameters governing\nthe agent's risk aversion and temporal variance aversion; and third, to provide\na verification argument for the candidate optimal solution to the\ninvestment-consumption problem among all admissible consumption streams.\n  To achieve these goals, we introduce a slightly different formulation of\nEpstein-Zin stochastic differential utility to that which is traditionally used\nin the literature. This formulation highlights the necessity and\nappropriateness of certain restrictions on the parameters governing the\nstochastic differential utility function.\n"
    },
    {
        "paper_id": 2107.06605,
        "authors": "Gongqiu Zhang, Lingfei Li",
        "title": "A General Approach for Parisian Stopping Times under Markov Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method based on continuous time Markov chain approximation to\ncompute the distribution of Parisian stopping times and price Parisian options\nunder general one-dimensional Markov processes. We prove the convergence of the\nmethod under a general setting and obtain sharp estimate of the convergence\nrate for diffusion models. Our theoretical analysis reveals how to design the\ngrid of the CTMC to achieve faster convergence. Numerical experiments are\nconducted to demonstrate the accuracy and efficiency of our method for both\ndiffusion and jump models. To show the versality of our approach, we develop\nextensions for multi-sided Parisian stopping times, the joint distribution of\nParisian stopping times and first passage times, Parisian bonds and for more\nsophisticated models like regime-switching and stochastic volatility models.\n"
    },
    {
        "paper_id": 2107.06623,
        "authors": "Panagiotis Kanellopoulos, Maria Kyropoulou, Hao Zhou",
        "title": "Financial Network Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study financial systems from a game-theoretic standpoint. A financial\nsystem is represented by a network, where nodes correspond to firms, and\ndirected labeled edges correspond to debt contracts between them. The existence\nof cycles in the network indicates that a payment of a firm to one of its\nlenders might result to some incoming payment. So, if a firm cannot fully repay\nits debt, then the exact (partial) payments it makes to each of its creditors\ncan affect the cash inflow back to itself. We naturally assume that the firms\nare interested in their financial well-being (utility) which is aligned with\nthe amount of incoming payments they receive from the network. This defines a\ngame among the firms, that can be seen as utility-maximizing agents who can\nstrategize over their payments.\n  We are the first to study financial network games that arise under a natural\nset of payment strategies called priority-proportional payments. We investigate\nthe existence and (in)efficiency of equilibrium strategies, under different\nassumptions on how the firms' utility is defined, on the types of debt\ncontracts allowed between the firms, and on the presence of other financial\nfeatures that commonly arise in practice. Surprisingly, even if all firms'\nstrategies are fixed, the existence of a unique payment profile is not\nguaranteed. So, we also investigate the existence and computation of valid\npayment profiles for fixed payment strategies.\n"
    },
    {
        "paper_id": 2107.06659,
        "authors": "Marcin W\\k{a}torek, Jaros{\\l}aw Kwapie\\'n, Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Financial Return Distributions: Past, Present, and COVID-19",
        "comments": null,
        "journal-ref": "Entropy 2021, 23(7), 884",
        "doi": "10.3390/e23070884",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the price return distributions of currency exchange rates,\ncryptocurrencies, and contracts for differences (CFDs) representing stock\nindices, stock shares, and commodities. Based on recent data from the years\n2017--2020, we model tails of the return distributions at different time scales\nby using power-law, stretched exponential, and $q$-Gaussian functions. We focus\non the fitted function parameters and how they change over the years by\ncomparing our results with those from earlier studies and find that, on the\ntime horizons of up to a few minutes, the so-called \"inverse-cubic power-law\"\nstill constitutes an appropriate global reference. However, we no longer\nobserve the hypothesized universal constant acceleration of the market time\nflow that was manifested before in an ever faster convergence of empirical\nreturn distributions towards the normal distribution. Our results do not\nexclude such a scenario but, rather, suggest that some other short-term\nprocesses related to a current market situation alter market dynamics and may\nmask this scenario. Real market dynamics is associated with a continuous\nalternation of different regimes with different statistical properties. An\nexample is the COVID-19 pandemic outburst, which had an enormous yet short-time\nimpact on financial markets. We also point out that two factors -- speed of the\nmarket time flow and the asset cross-correlation magnitude -- while related\n(the larger the speed, the larger the cross-correlations on a given time\nscale), act in opposite directions with regard to the return distribution\ntails, which can affect the expected distribution convergence to the normal\ndistribution.\n"
    },
    {
        "paper_id": 2107.06758,
        "authors": "Gilles Zumbach",
        "title": "On the short term stability of financial ARCH price processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For many financial applications, it is important to have reliable and\ntractable models for the behavior of assets and indexes, for example in risk\nevaluation. A successful approach is based on ARCH processes, which strike the\nright balance between statistical properties and ease of computation. This\nstudy focuses on quadratic ARCH processes and the theoretical conditions to\nhave a stable long-term behavior. In particular, the weights for the variance\nestimators should sum to 1, and the variance of the innovations should be 1.\nUsing historical data, the realized empirical innovations can be computed, and\ntheir statistical properties assessed. Using samples of 3 to 5 decades, the\nvariance of the empirical innovations are always significantly above 1, for a\nsample of stock indexes, commodity indexes and FX rates. This departure points\nto a short term instability, or to a fast adaptability due to changing\nconditions. Another theoretical condition on the innovations is to have a zero\nmean. This condition is also investigated empirically, with some time series\nshowing significant departure from zero.\n"
    },
    {
        "paper_id": 2107.06782,
        "authors": "Mimansa Rana, Nanxiang Mao, Ming Ao, Xiaohui Wu, Poning Liang and\n  Matloob Khushi",
        "title": "Clustering and attention model based for intelligent trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The foreign exchange market has taken an important role in the global\nfinancial market. While foreign exchange trading brings high-yield\nopportunities to investors, it also brings certain risks. Since the\nestablishment of the foreign exchange market in the 20th century, foreign\nexchange rate forecasting has become a hot issue studied by scholars from all\nover the world. Due to the complexity and number of factors affecting the\nforeign exchange market, technical analysis cannot respond to administrative\nintervention or unexpected events. Our team chose several pairs of foreign\ncurrency historical data and derived technical indicators from 2005 to 2021 as\nthe dataset and established different machine learning models for event-driven\nprice prediction for oversold scenario.\n"
    },
    {
        "paper_id": 2107.0681,
        "authors": "Emilia Luoma, Mirka Laurila-Pant, Elias Altarriba, Inari Helle, Lena\n  Granhag, Maiju Lehtiniemi, Greta Sr\\.ebalien\\.e, Sergej Olenin, Annukka\n  Lehikoinen",
        "title": "A decision support tool for ship biofouling management in the Baltic Sea",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Biofouling of ships causes major environmental and economic consequences all\nover the world. In addition, biofouling management of ship hulls causes both\nsocial, environmental and economic risks that should all be considered reaching\nwell-balanced decisions. In addition, each case is unique and thus optimal\nmanagement strategy must be considered case-specifically. We produced a novel\ndecision support tool using Bayesian networks to promote the comprehensive\nunderstanding about the complex biofouling management issue in the Baltic Sea\nand to identify potential management options and their consequences. The tool\ncompares the biofouling management strategies in relation to NIS\n(non-indigenous species) introduction risk, eco-toxicological risk due to\nbiocidal coating, carbon dioxide emissions resulting from fuel consumption and\ncosts related to fuel consumption, in-water cleaning and coating. According to\nthe results, the optimal biofouling management strategy would consist of a\nbiocidal-free coating with regular in-water cleaning and with devices\ncollecting the material. However, the best biocidal-free coating type and the\noptimal in-water cleaning interval varies and depends e.g. on the operational\nprofile of the ship. The decision support tool can increase the\nmulti-perspective understanding about the issue and support the implementation\nof the optimal biofouling management strategies in the Baltic Sea.\n"
    },
    {
        "paper_id": 2107.06839,
        "authors": "N. Packham, F. Woebbeking",
        "title": "Correlation scenarios and correlation stress testing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop a general approach for stress testing correlations of financial\nasset portfolios. The correlation matrix of asset returns is specified in a\nparametric form, where correlations are represented as a function of risk\nfactors, such as country and industry factors. A sparse factor structure\nlinking assets and risk factors is built using Bayesian variable selection\nmethods. Regular calibration yields a joint distribution of economically\nmeaningful stress scenarios of the factors. As such, the method also lends\nitself as a reverse stress testing framework: using the Mahalanobis distance or\nhighest density regions (HDR) on the joint risk factor distribution allows to\ninfer worst-case correlation scenarios. We give examples of stress tests on a\nlarge portfolio of European and North American stocks.\n"
    },
    {
        "paper_id": 2107.06841,
        "authors": "Chongrui Zhu",
        "title": "Optimality of threshold strategies for spectrally negative Levy\n  processes and a positive terminal value at creeping ruin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a dividend optimization problem with a positive\ncreeping-associated terminal value at ruin for spectrally negative Levy\nprocesses. We consider an insurance company whose surplus process evolves\naccording to a spectrally negative Levy process with a Gaussian part and a\nfinite Levy measure. Its objective function relates to dividend payments until\nruin and a creeping-associated terminal value at ruin. The positive\ncreeping-associated terminal value represents the salvage value or the creeping\nreward when creeping happens. Owing to formulas from fluctuation theory, the\nobjective considered is represented explicitly. Under certain restrictions on\nthe terminal value and the surplus process, we show that the threshold strategy\nshould be the optimal one over an admissible class with bounded dividend rates.\n"
    },
    {
        "paper_id": 2107.06855,
        "authors": "S Sidhartha Narayan, Malavika Ranjan and Madhumitha Raghuraman",
        "title": "Comparing Intellectual property policy in the Global North and South --\n  A one-size-fits-all policy for economic prosperity?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper attempts to analyse policymaking in the field of Intellectual\nProperty (IP) as an instrument of economic growth across the Global North and\nSouth. It begins by studying the links between economic growth and IP, followed\nby an understanding of Intellectual Property Rights (IPR) development in the\nUS, a leading proponent of robust IPR protection internationally. The next\nsection compares the IPR in the Global North and South and undertakes an\nanalysis of the diverse factors that result in these differences. The paper\nuses the case study of the Indian Pharmaceutical Industry to understand how IPR\nmay differentially affect economies and conclude that there may not yet be a\none size fits all policy for the adoption of Intellectual Property Rights.\n"
    },
    {
        "paper_id": 2107.07026,
        "authors": "Budhi Surya",
        "title": "A new class of conditional Markov jump processes with regime switching\n  and path dependence: properties and maximum likelihood estimation",
        "comments": "28 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a new class of conditional Markov jump processes with\nregime switching and paths dependence. The key novel feature of the developed\nprocess lies on its ability to switch the transition rate as it moves from one\nstate to another with switching probability depending on the current state and\ntime of the process as well as its past trajectories. As such, the transition\nfrom current state to another depends on the holding time of the process in the\nstate. Distributional properties of the process are given explicitly in terms\nof the speed regimes represented by a finite number of different transition\nmatrices, the probabilities of selecting regime membership within each state,\nand past realization of the process. In particular, it has distributional\nequivalent stochastic representation with a general mixture of Markov jump\nprocesses introduced in Frydman and Surya (2020). Maximum likelihood estimates\n(MLE) of the distribution parameters of the process are derived in closed form.\nThe estimation is done iteratively using the EM algorithm. Akaike information\ncriterion is used to assess the goodness-of-fit of the selected model. An\nexplicit observed Fisher information matrix of the MLE is derived for the\ncalculation of standard errors of the MLE. The information matrix takes on a\nsimplified form of the general matrix formula of Louis (1982). Large sample\nproperties of the MLE are presented. In particular, the covariance matrix for\nthe MLE of transition rates is equal to the Cram\\'er-Rao lower bound, and is\nless for the MLE of regime membership. The simulation study confirms these\nfindings and shows that the parameter estimates are accurate, consistent, and\nhave asymptotic normality as the sample size increases.\n"
    },
    {
        "paper_id": 2107.07142,
        "authors": "{\\L}ukasz Bielak, Aleksandra Grzesiek, Joanna Janczura, Agnieszka\n  Wy{\\l}oma\\'nska",
        "title": "Market risk factors analysis for an international mining company.\n  Multi-dimensional, heavy-tailed-based modelling",
        "comments": "12 pages, 10 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mining companies to properly manage their operations and be ready to make\nbusiness decisions, are required to analyze potential scenarios for main market\nrisk factors. The most important risk factors for KGHM, one of the biggest\ncompanies active in the metals and mining industry, are the price of copper\n(Cu), traded in US dollars, and the Polish zloty (PLN) exchange rate (USDPLN).\nThe main scope of the paper is to understand the mid- and long-term dynamics of\nthese two risk factors. For a mining company it might help to properly evaluate\npotential downside market risk and optimise hedging instruments. From the\nmarket risk management perspective, it is also important to analyze the\ndynamics of these two factors combined with the price of copper in Polish zloty\n(Cu in PLN), which jointly drive the revenues, cash flows, and financial\nresults of the company. Based on the relation between analyzed risk factors and\ndistribution analysis, we propose to use two-dimensional vector autoregressive\n(VAR) model with the $\\alpha-$stable distribution. The non-homogeneity of the\ndata is reflected in two identified regimes: first - corresponding to the 2008\ncrisis and second - to the stable market situation. As a natural implication of\nthe model fitted to market assets, we derive the dynamics of the copper price\nin PLN, which is not a traded asset but is crucial for the KGHM company risk\nexposure. A comparative study is performed to demonstrate the effect of\nincluding dependencies of the assets and the implications of the regime change.\nSince for various international companies, risk factors are given rather in the\nnational than the market currency, the approach is universal and can be used in\ndifferent market contexts, like mining or oil companies, but also other\ncommodities involved in the global trading system.\n"
    },
    {
        "paper_id": 2107.07206,
        "authors": "Matthieu Garcin, Samuel St\\'ephan",
        "title": "Credit scoring using neural networks and SURE posterior probability\n  calibration",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article we compare the performances of a logistic regression and a\nfeed forward neural network for credit scoring purposes. Our results show that\nthe logistic regression gives quite good results on the dataset and the neural\nnetwork can improve a little the performance. We also consider different sets\nof features in order to assess their importance in terms of prediction\naccuracy. We found that temporal features (i.e. repeated measures over time)\ncan be an important source of information resulting in an increase in the\noverall model accuracy. Finally, we introduce a new technique for the\ncalibration of predicted probabilities based on Stein's unbiased risk estimate\n(SURE). This calibration technique can be applied to very general calibration\nfunctions. In particular, we detail this method for the sigmoid function as\nwell as for the Kumaraswamy function, which includes the identity as a\nparticular case. We show that stacking the SURE calibration technique with the\nclassical Platt method can improve the calibration of predicted probabilities.\n"
    },
    {
        "paper_id": 2107.07339,
        "authors": "Onur Babat and Juan C. Vera and Luis F. Zuluaga",
        "title": "Computing near-optimal Value-at-Risk portfolios using Integer\n  Programming techniques",
        "comments": "21 pages, 7 figures",
        "journal-ref": "BEuropean Journal of Operational Research, 266(1), 304-315 (2018)",
        "doi": "10.1016/j.ejor.2017.09.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value-at-Risk (VaR) is one of the main regulatory tools used for risk\nmanagement purposes. However, it is difficult to compute optimal VaR\nportfolios; that is, an optimal risk-reward portfolio allocation using VaR as\nthe risk measure. This is due to VaR being non-convex and of combinatorial\nnature. In particular, it is well known that the VaR portfolio problem can be\nformulated as a mixed integer linear program (MILP) that is difficult to solve\nwith current MILP solvers for medium to large-scale instances of the problem.\nHere, we present an algorithm to compute near-optimal VaR portfolios that takes\nadvantage of this MILP formulation and provides a guarantee of the solution's\nnear-optimality. As a byproduct, we obtain an algorithm to compute tight lower\nbounds on the VaR portfolio problem that outperform related algorithms proposed\nin the literature for this purpose. The near-optimality guarantee provided by\nthe proposed algorithm is obtained thanks to the relation between minimum risk\nportfolios satisfying a reward benchmark and the corresponding maximum reward\nportfolios satisfying a risk benchmark. These alternate formulations of the\nportfolio allocation problem have been frequently studied in the case of convex\nrisk measures and concave reward functions. Here, this relationship is\nconsidered for general risk measures and reward functions. To illustrate the\nefficiency of the presented algorithm, numerical results are presented using\nhistorical asset returns from the US financial market.\n"
    },
    {
        "paper_id": 2107.07348,
        "authors": "Abdelghani Maddi (HCERES), David Sapinho",
        "title": "Article Processing Charges based publications: to which extent the price\n  explains scientific impact?",
        "comments": "ISSI 2021 - 18th International Conference on Scientometrics &\n  Informetrics, Jul 2021, Leuven, Belgium",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present study aims to analyze relationship between Citations Normalized\nScore (NCS) of scientific publications and Article Processing Charges (APCs)\namounts of Gold Open access publications. To do so, we use APCs information\nprovided by OpenAPC database and citations scores of publications in the Web of\nScience database (WoS). Database covers the period from 2006 to 2019 with\n83,752 articles published in 4751 journals belonging to 267 distinct\npublishers. Results show that contrary to this belief, paying dearly does not\nnecessarily increase the impact of publications. First, large publishers with\nhigh impact are not the most expensive. Second, publishers with the highest\nAPCs are not necessarily the best in terms of impact. Correlation between APCs\nand impact is moderate. Otherwise, in the econometric analysis we have shown\nthat publication quality is strongly determined by journal quality in which it\nis published. International collaboration also plays an important role in\ncitations score.\n"
    },
    {
        "paper_id": 2107.07552,
        "authors": "Maria Carnovale, Khahlil Louisy",
        "title": "Public Health, Technology, and Human Rights: Lessons from Digital\n  Contact Tracing",
        "comments": "23 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To mitigate inefficiencies in manual contact tracing processes, Digital\nContact Tracing and Exposure Notifications Systems were developed for use as\npublic-interest technologies during the SARS-CoV-2 global pandemic. Effective\nimplementation of these tools requires alignment across several factors,\nincluding local regulations and policies and trust in government and public\nhealth officials. Careful consideration should also be made to minimize any\npotential conflicts with existing processes in public health which has\ndemonstrated effectiveness. Four unique cases-of Ireland, Guayaquil, Haiti, and\nthe Philippines-detailed in this paper will highlight the importance of\nupholding the principles of Scientific Validity, Necessity, Time Boundedness,\nand Proportionality.\n"
    },
    {
        "paper_id": 2107.07553,
        "authors": "Sally Giuseppe Arcidiacono, Salvatore Corrente and Salvatore Greco",
        "title": "Supporting the robust ordinal regression approach to multiple criteria\n  decision aiding with a set of representative value functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this paper we propose a new methodology to represent the results of the\nrobust ordinal regression approach by means of a family of representative value\nfunctions for which, taken two alternatives $a$ and $b$, the following two\nconditions are satisfied: 1) if for all compatible value functions $a$ is\nevaluated not worse than $b$ and for at least one value function $a$ has a\nbetter evaluation, then the evaluation of $a$ is greater than the evaluation of\n$b$ for all representative value functions; 2) if there exists one compatible\nvalue function giving $a$ an evaluation greater than $b$ and another compatible\nvalue function giving $a$ an evaluation smaller than $b$, then there are also\nat least one representative function giving a better evaluation to $a$ and\nanother representative value function giving $a$ an evaluation smaller than\n$b$. This family of representative value functions intends to provide the\nDecision Maker (DM) a more clear idea of the preferences obtained by the\ncompatible value functions, with the aim to support the discussion in\nconstructive approach of Multiple Criteria Decision Aiding.\n"
    },
    {
        "paper_id": 2107.07556,
        "authors": "R. G. Alcoforado, W. Bernardino, A. D. Eg\\'idio dos Reis and J. A. C.\n  Santos",
        "title": "Modelling risk for commodities in Brazil: An application to live cattle\n  spot and futures prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study analysed a series of live cattle spot and futures prices from the\nBoi Gordo Index (BGI) in Brazil. The objective was to develop a model that best\nportrays this commodity's behaviour to estimate futures prices more accurately.\nThe database created contained 2,010 daily entries in which trade in futures\ncontracts occurred, as well as BGI spot sales in the market, from 1 December\n2006 to 30 April 2015. One of the most important reasons why this type of risk\nneeds to be measured is to set loss limits. To identify patterns in price\nbehaviour in order to improve future transactions' results, investors must\nanalyse fluctuations in assets' value for longer periods. Bibliographic\nresearch revealed that no other study has conducted a comprehensive analysis of\nthis commodity using this approach. Cattle ranching is big business in Brazil\ngiven that in 2017, this sector moved 523.25 billion Brazilian reals (about\n130.5 billion United States dollars). In that year, agribusiness contributed\n22% of Brazil's total gross domestic product. Using the proposed risk modelling\ntechnique, economic agents can make the best decision about which options\nwithin these investors' reach produce more effective risk management. The\nmethodology was based on Holt-Winters exponential smoothing algorithm,\nautoregressive integrated moving average (ARIMA), ARIMA with exogenous inputs,\ngeneralised autoregressive conditionally heteroskedastic and generalised\nautoregressive moving average (GARMA) models. More specifically, 5 different\nmethods were applied that allowed a comparison of 12 different models as ways\nto portray and predict the BGI commodity's behaviour. The results show that\nGARMA with order c(2,1) and without intercept is the best model.\n"
    },
    {
        "paper_id": 2107.07668,
        "authors": "Arthur Charpentier and Molly James and Hani Ali",
        "title": "Predicting Drought and Subsidence Risks in France",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5194/nhess-22-2401-2022",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The economic consequences of drought episodes are increasingly important,\nalthough they are often difficult to apprehend in part because of the\ncomplexity of the underlying mechanisms. In this article, we will study one of\nthe consequences of drought, namely the risk of subsidence (or more\nspecifically clay shrinkage induced subsidence), for which insurance has been\nmandatory in France for several decades. Using data obtained from several\ninsurers, representing about a quarter of the household insurance market, over\nthe past twenty years, we propose some statistical models to predict the\nfrequency but also the intensity of these droughts, for insurers, showing that\nclimate change will have probably major economic consequences on this risk. But\neven if we use more advanced models than standard regression-type models (here\nrandom forests to capture non linearity and cross effects), it is still\ndifficult to predict the economic cost of subsidence claims, even if all\ngeophysical and climatic information is available.\n"
    },
    {
        "paper_id": 2107.07678,
        "authors": "Shaojun Ma, Pengcheng Li",
        "title": "Predicting Daily Trading Volume via Various Hidden States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting intraday trading volume plays an important role in trading alpha\nresearch. Existing methods such as rolling means(RM) and a two-states based\nKalman Filtering method have been presented in this topic. We extend two states\ninto various states in Kalman Filter framework to improve the accuracy of\nprediction. Specifically, for different stocks we utilize cross validation and\ndetermine best states number by minimizing mean squared error of the trading\nvolume. We demonstrate the effectivity of our method through a series of\ncomparison experiments and numerical analysis.\n"
    },
    {
        "paper_id": 2107.07816,
        "authors": "Vladimir Zhavoronkov, Valeri Lipunov, Mattia Masolletti",
        "title": "Key features of administrative responsibility",
        "comments": "Volume: 6 pages. Key words: administrative responsibility. JEL codes:\n  K-1; K-4; K-10",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article examines both the legal responsibility itself and its types, and\nin various aspects. The authors apply legal analysis, as well as the principles\nof consistency and integrity. The contradictions of administrative\nresponsibility, as well as legal gaps in its interpretation, are highlighted.\n"
    },
    {
        "paper_id": 2107.08109,
        "authors": "Shengzhong Chen, Niushan Gao, Denny Leung, Lei Li",
        "title": "Automatic Fatou Property of Law-invariant Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper we investigate automatic Fatou property of law-invariant risk\nmeasures on a rearrangement-invariant function space $\\mathcal{X}$ other than\n$L^\\infty$. The main result is the following characterization: Every\nreal-valued, law-invariant, coherent risk measure on $\\mathcal{X}$ has the\nFatou property at every random variable $X\\in \\mathcal{X}$ whose negative tails\nhave vanishing norm (i.e., $\\lim_n\\|X\\mathbf{1}_{\\{X\\leq -n\\}}\\|=0$) if and\nonly if $\\mathcal{X}$ satisfies the Almost Order Continuous Equidistributional\nAverage (AOCEA) property, namely, $\\mathrm{d}(\\mathcal{CL}(X),\\mathcal{X}_a)\n=0$ for any $X\\in \\mathcal{X}_+$, where $ \\mathcal{CL}(X)$ is the convex hull\nof all random variables having the same distribution as $X$ and\n$\\mathcal{X}_a=\\{X\\in\\mathcal{X}:\\lim_n \\|X\\mathbf{1}_{ \\{|X|\\geq n\\} }\\|\n=0\\}$. As a consequence, we show that under the AOCEA property, every\nreal-valued, law-invariant, coherent risk measure on $\\mathcal{X}$ admits a\ntractable dual representation at every $X\\in \\mathcal{X}$ whose negative tails\nhave vanishing norm. Furthermore, we show that the AOCEA property is satisfied\nby most classical model spaces, including Orlicz spaces, and therefore the\nforegoing results have wide applications.\n"
    },
    {
        "paper_id": 2107.08342,
        "authors": "Mohammad Masud Alam, AFM Zakaria",
        "title": "A Probit Estimation of Urban Bases of Environmental Awareness: Evidence\n  from Sylhet City, Bangladesh",
        "comments": "14 pages",
        "journal-ref": "Development Compilation, Volume 09, Issue 01 (2013)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper evaluates the significant factors contributing to environmental\nawareness among individuals living in the urban area of Sylhet, Bangladesh.\nOrdered Probit(OPM) estimation is applied on the value of ten measures of\nindividual environmental concern. The estimated results of OPM reveal the\ndominance of higher education, higher income, and full-employment status on\nenvironmental concern and environmentally responsible behavior. Younger and\nmore educated respondents tended to be more knowledgeable and concerned than\nolder and less educated respondents. The marginal effect of household size,\nmiddle-income level income, and part-time employment status of the survey\nrespondents played a less significant role in the degree of environmental\nawareness. Findings also validate the \"age hypothesis\" proposed by Van Liere\nand Dunlap (1980), and the gender effect reveals an insignificant role in\ndetermining the degree of environmental concern. Environmental awareness among\nurban individuals with higher income increased linearly with environmental\nawareness programs which may have significant policy importance, such as\nenvironmental awareness programs for old-aged and less-educated individuals,\nand may lead to increased taxation on higher income groups to mitigate city\nareas' pollution problems.\n"
    },
    {
        "paper_id": 2107.08393,
        "authors": "Ana Garcia-Bernabeu and Adolfo Hilario-Caballero",
        "title": "Monitoring multidimensional phenomena with a multicriteria composite\n  performance interval approach",
        "comments": "13 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the last two decades, composite indicators' construction to measure and\ncompare multidimensional phenomena in a broad spectrum of domains has increased\nconsiderably. Different methodological approaches are used to summarize huge\ndata sets of information in a single figure. This paper proposes a new approach\nthat consists of computing a multicriteria composite performance interval based\non different aggregation rules. The suggested approach provides an additional\nlayer of information as the performance interval displays a lower bound from a\nnon-compensability perspective, and an upper bound allowing for\nfull-compensability. The outstanding features of this proposal are: (i) a\ndistance-based multicriteria technique is taken as the baseline to construct\nthe multicriteria performance interval (ii) the aggregation of\ndistances/separation measures is made using particular cases of Minkowski's\n$L_p$ metrics; (iii) the span of the multicriteria performance interval can be\nconsidered as a sign of the dimensions or indicators balance.\n"
    },
    {
        "paper_id": 2107.08586,
        "authors": "Xinyu Dou, Yilong Wang, Philippe Ciais, Fr\\'ed\\'eric Chevallier,\n  Steven J. Davis, Monica Crippa, Greet Janssens-Maenhout, Diego Guizzardi,\n  Efisio Solazzo, Feifan Yan, Da Huo, Zheng Bo, Zhu Deng, Biqing Zhu, Hengqi\n  Wang, Qiang Zhang, Pierre Gentine and Zhu Liu",
        "title": "Global Gridded Daily CO$_2$ Emissions",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.xinn.2021.100182",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Precise and high-resolution carbon dioxide (CO$_2$) emission data is of great\nimportance of achieving the carbon neutrality around the world. Here we present\nfor the first time the near-real-time Global Gridded Daily CO$_2$ Emission\nDatasets (called GRACED) from fossil fuel and cement production with a global\nspatial-resolution of 0.1$^\\circ$ by 0.1$^\\circ$ and a temporal-resolution of\n1-day. Gridded fossil emissions are computed for different sectors based on the\ndaily national CO$_2$ emissions from near real time dataset (Carbon Monitor),\nthe spatial patterns of point source emission dataset Global Carbon Grid (GID),\nEmission Database for Global Atmospheric Research (EDGAR) and spatiotemporal\npatters of satellite nitrogen dioxide (NO$_2$) retrievals. Our study on the\nglobal CO$_2$ emissions responds to the growing and urgent need for\nhigh-quality, fine-grained near-real-time CO2 emissions estimates to support\nglobal emissions monitoring across various spatial scales. We show the spatial\npatterns of emission changes for power, industry, residential consumption,\nground transportation, domestic and international aviation, and international\nshipping sectors between 2019 and 2020. This help us to give insights on the\nrelative contributions of various sectors and provides a fast and fine-grained\noverview of where and when fossil CO$_2$ emissions have decreased and rebounded\nin response to emergencies (e.g. COVID-19) and other disturbances of human\nactivities than any previously published dataset. As the world recovers from\nthe pandemic and decarbonizes its energy systems, regular updates of this\ndataset will allow policymakers to more closely monitor the effectiveness of\nclimate and energy policies and quickly adapt.\n"
    },
    {
        "paper_id": 2107.08684,
        "authors": "Mathieu Rosenbaum and Mehdi Tomas",
        "title": "A characterisation of cross-impact kernels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Trading a financial asset pushes its price as well as the prices of other\nassets, a phenomenon known as cross-impact. We consider a general class of\nkernel-based cross-impact models and investigate suitable parameterisations for\ntrading purposes. We focus on kernels that guarantee that prices are\nmartingales and anticipate future order flow (martingale-admissible kernels)\nand those that ensure there is no possible price manipulation\n(no-statistical-arbitrage-admissible kernels). We determine the overlap between\nthese two classes and provide formulas for calibration of cross-impact kernels\non data. We illustrate our results using SP500 futures data.\n"
    },
    {
        "paper_id": 2107.08713,
        "authors": "Guido Ascari, Qazi Haque, Leandro M. Magnusson, Sophocles Mavroeidis",
        "title": "Empirical evidence on the Euler equation for investment in the US",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is the typical specification of the Euler equation for investment employed in\nDSGE models consistent with aggregate macro data? Using state-of-the-art\neconometric methods that are robust to weak instruments and exploit information\nin possible structural changes, the answer is yes. Unfortunately, however,\nthere is very little information about the values of these parameters in\naggregate data because investment is unresponsive to changes in capital\nutilization and the real interest rate. In DSGE models, the investment\nadjustment cost and the persistence of the investment-specific technology shock\nparameters are mainly identified by, respectively, the cross-equation\nrestrictions and the dynamics implied by the structure of the model.\n"
    },
    {
        "paper_id": 2107.08721,
        "authors": "Qinkai Chen",
        "title": "Stock Movement Prediction with Financial News using Contextualized\n  Embedding from BERT",
        "comments": "22 pages, 6 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  News events can greatly influence equity markets. In this paper, we are\ninterested in predicting the short-term movement of stock prices after\nfinancial news events using only the headlines of the news. To achieve this\ngoal, we introduce a new text mining method called Fine-Tuned\nContextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with\nprevious approaches which use static vector representations of the news (static\nembedding), our model uses contextualized vector representations of the\nheadlines (contextualized embeddings) generated from Bidirectional Encoder\nRepresentations from Transformers (BERT). Our model obtains the\nstate-of-the-art result on this stock movement prediction task. It shows\nsignificant improvement compared with other baseline models, in both accuracy\nand trading simulations. Through various trading simulations based on millions\nof headlines from Bloomberg News, we demonstrate the ability of this model in\nreal scenarios.\n"
    },
    {
        "paper_id": 2107.08808,
        "authors": "Wei Li, Florentina Paraschiv, Georgios Sermpinis",
        "title": "A Data-driven Explainable Case-based Reasoning Approach for Financial\n  Risk Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The rapid development of artificial intelligence methods contributes to their\nwide applications for forecasting various financial risks in recent years. This\nstudy introduces a novel explainable case-based reasoning (CBR) approach\nwithout a requirement of rich expertise in financial risk. Compared with other\nblack-box algorithms, the explainable CBR system allows a natural economic\ninterpretation of results. Indeed, the empirical results emphasize the\ninterpretability of the CBR system in predicting financial risk, which is\nessential for both financial companies and their customers. In addition, our\nresults show that the proposed automatic design CBR system has a good\nprediction performance compared to other artificial intelligence methods,\novercoming the main drawback of a standard CBR system of highly depending on\nprior domain knowledge about the corresponding field.\n"
    },
    {
        "paper_id": 2107.08813,
        "authors": "Marie-Charlotte Brandenburg, Christian Haase and Ngoc Mai Tran",
        "title": "Competitive equilibrium always exists for combinatorial auctions with\n  graphical pricing schemes",
        "comments": "24 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that a competitive equilibrium always exists in combinatorial\nauctions with anonymous graphical valuations and pricing, using discrete\ngeometry. This is an intuitive and easy-to-construct class of valuations that\ncan model both complementarity and substitutes, and to our knowledge, it is the\nfirst class besides gross substitutes that have guaranteed competitive\nequilibrium. We prove through counter-examples that our result is tight, and we\ngive explicit algorithms for constructive competitive pricing vectors. We also\ngive extensions to multi-unit combinatorial auctions (also known as product-mix\nauctions). Combined with theorems on graphical valuations and pricing\nequilibrium of Candogan, Ozdagar and Parrilo, our results indicate that\nquadratic pricing is a highly practical method to run combinatorial auctions.\n"
    },
    {
        "paper_id": 2107.08827,
        "authors": "Matej Uhr\\'in, Gustav \\v{S}ourek, Ond\\v{r}ej Hub\\'a\\v{c}ek, Filip\n  \\v{Z}elezn\\'y",
        "title": "Optimal sports betting strategies in practice: an experimental review",
        "comments": "Accepted to IMA Journal of Management Mathematics where it, however,\n  appeared with swapped names and surnames - putting the correct version here\n  for reference",
        "journal-ref": "IMA Journal of Management Mathematics (2021) 00",
        "doi": "10.1093/imaman/dpaa029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the most popular approaches to the problem of sports betting\ninvestment based on modern portfolio theory and the Kelly criterion. We define\nthe problem setting, the formal investment strategies, and review their common\nmodifications used in practice. The underlying purpose of the reviewed\nmodifications is to mitigate the additional risk stemming from the unrealistic\nmathematical assumptions of the formal strategies. We test the resulting\nmethods using a unified evaluation protocol for three sports: horse racing,\nbasketball and soccer. The results show the practical necessity of the\nadditional risk-control methods and demonstrate their individual benefits.\nParticularly, we show that an adaptive variant of the popular ``fractional\nKelly'' method is a very suitable choice across a wide range of settings.\n"
    },
    {
        "paper_id": 2107.09048,
        "authors": "Anton J. Heckens and Thomas Guhr",
        "title": "A New Attempt to Identify Long-term Precursors for Endogenous Financial\n  Crises in the Market Correlation Structures",
        "comments": "23 pages, 10 figures, 4 tables",
        "journal-ref": "J. Stat. Mech. (2022) 043401",
        "doi": "10.1088/1742-5468/ac59ab",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of events in financial markets is every investor's dream and,\nusually, wishful thinking. From a more general, economic and societal\nviewpoint, the identification of indicators for large events is highly\ndesirable to assess systemic risks. Unfortunately, the very nature of financial\nmarkets, particularly the predominantly non-Markovian character as well as\nnon-stationarity, make this challenge a formidable one, leaving little hope for\nfully fledged answers. Nevertheless, it is called for to collect pieces of\nevidence in a variety of observables to be assembled like the pieces of a\npuzzle that eventually might help to catch a glimpse of long-term indicators or\nprecursors for large events - if at all in a statistical sense. Here, we\npresent a new piece for this puzzle. We use the quasi-stationary market states\nwhich exist in the time evolution of the correlation structure in financial\nmarkets. Recently, we identified such market states relative to the collective\nmotion of the market as a whole. We study their precursor properties in the US\nstock markets over 16 years, including two endogenous crises, the dot-com\nbubble burst and the pre-phase of the Lehman Brothers crash. We identify\ncertain interesting features and critically discuss their suitability as\nindicators.\n"
    },
    {
        "paper_id": 2107.09051,
        "authors": "Longbing Cao",
        "title": "AI in Finance: Challenges, Techniques and Opportunities",
        "comments": "The paper is in the revision for ACM Computing Surveys, 40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  AI in finance broadly refers to the applications of AI techniques in\nfinancial businesses. This area has been lasting for decades with both classic\nand modern AI techniques applied to increasingly broader areas of finance,\neconomy and society. In contrast to either discussing the problems, aspects and\nopportunities of finance that have benefited from specific AI techniques and in\nparticular some new-generation AI and data science (AIDS) areas or reviewing\nthe progress of applying specific techniques to resolving certain financial\nproblems, this review offers a comprehensive and dense roadmap of the\noverwhelming challenges, techniques and opportunities of AI research in finance\nover the past decades. The landscapes and challenges of financial businesses\nand data are firstly outlined, followed by a comprehensive categorization and a\ndense overview of the decades of AI research in finance. We then structure and\nillustrate the data-driven analytics and learning of financial businesses and\ndata. The comparison, criticism and discussion of classic vs. modern AI\ntechniques for finance are followed. Lastly, open issues and opportunities\naddress future AI-empowered finance and finance-motivated AI research.\n"
    },
    {
        "paper_id": 2107.09055,
        "authors": "Priyank Sonkiya, Vikas Bajpai and Anukriti Bansal",
        "title": "Stock price prediction using BERT and GAN",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.\n"
    },
    {
        "paper_id": 2107.09094,
        "authors": "Bertram D\\\"uring and Christof Heuer",
        "title": "Time-adaptive high-order compact finite difference schemes for option\n  pricing in a family of stochastic volatility models",
        "comments": "7 pages, 1 figure",
        "journal-ref": "In: Progress in Industrial Mathematics at ECMI 2021, M. Ehrhardt\n  and M. G\\\"unther (eds.), pp. 373-380, Mathematics in Industry 39, Springer,\n  Berlin, Heidelberg, 2022",
        "doi": "10.1007/978-3-031-11818-0_49",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a time-adaptive, high-order compact finite difference scheme for\noption pricing in a family of stochastic volatility models. We employ a\nsemi-discrete high-order compact finite difference method for the spatial\ndiscretisation, and combine this with an adaptive time discretisation,\nextending ideas from [LSRHF02] to fourth-order multistep methods in time.\n"
    },
    {
        "paper_id": 2107.09148,
        "authors": "Abdul-Lateef Haji-Ali, Jonathan Spence and Aretha Teckentrup",
        "title": "Adaptive Multilevel Monte Carlo for Probabilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the numerical approximation of $\\mathbb{P}[G\\in \\Omega]$ where\nthe $d$-dimensional random variable $G$ cannot be sampled directly, but there\nis a hierarchy of increasingly accurate approximations\n$\\{G_\\ell\\}_{\\ell\\in\\mathbb{N}}$ which can be sampled. The cost of standard\nMonte Carlo estimation scales poorly with accuracy in this setup since it\ncompounds the approximation and sampling cost. A direct application of\nMultilevel Monte Carlo improves this cost scaling slightly, but returns\nsub-optimal computational complexities since estimation of the probability\ninvolves a discontinuous functional of $G_\\ell$. We propose a general adaptive\nframework which is able to return the MLMC complexities seen for smooth or\nLipschitz functionals of $G_\\ell$. Our assumptions and numerical analysis are\nkept general allowing the methods to be used for a wide class of problems. We\npresent numerical experiments on nested simulation for risk estimation, where\n$G = \\mathbb{E}[X|Y]$ is approximated by an inner Monte Carlo estimate. Further\nexperiments are given for digital option pricing, involving an approximation of\na $d$-dimensional SDE.\n"
    },
    {
        "paper_id": 2107.09273,
        "authors": "Wen Su",
        "title": "Volatility of S&P500: Estimation and Evaluation",
        "comments": "in Chinese language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In an era when derivatives is getting popular, risk management has gradually\nbecome the core content of modern finance. In order to study how to accurately\nestimate the volatility of the S&P 500 index, after introducing the theoretical\nbackground of several methods, this paper uses the historical volatility\nmethod, GARCH model method and implied volatility method to estimate the real\nvolatility respectively. At the same time, two ways of adjusting the estimation\nwindow, rolling and increasing, are also considered. The unbiased test and\ngoodness of fit test are used to evaluate these methods. The empirical result\nshows that the implied volatility is the best estimator of the real volatility.\nThe rolling estimation window is recommended when using the historical\nvolatility. On the contrary, the estimation window is supposed to be increased\nwhen using the GARCH model.\n"
    },
    {
        "paper_id": 2107.09396,
        "authors": "Rozane Bezerra de Siqueira, Jos\\'e Ricardo Bezerra Nogueira, Carlos\n  Feitosa Luna",
        "title": "A Incid\\^encia Final dos Tributos Indiretos no Brasil: Estimativa Usando\n  a Matriz de Insumo-Produto 2015",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Taxes on goods and services account for about 45% of total tax revenue in\nBrazil. This tax collection results in a highly complex system, with several\ntaxes, different tax bases, and a multiplicity of rates. Moreover, about 43% of\ntaxes on goods fall on inputs. In this context, the effective tax rates can\nsubstantially differ from the legal rates. In this study we estimate the final\nincidence of indirect taxes in Brazil using the 2015 Brazilian input-output\nmatrix and a method that incorporates the multisector effects of the taxation\nof inputs.\n"
    },
    {
        "paper_id": 2107.09629,
        "authors": "Philip Protter, Qianfan Wu, Shihao Yang",
        "title": "Order Book Queue Hawkes-Markovian Modeling",
        "comments": "71 pages, 80 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article presents a Hawkes process model with Markovian baseline\nintensities for high-frequency order book data modeling. We classify intraday\norder book trading events into a range of categories based on their order types\nand the price changes after their arrivals. To capture the stimulating effects\nbetween multiple types of order book events, we use the multivariate Hawkes\nprocess to model the self- and mutually-exciting event arrivals. We also\nintegrate a Markovian baseline intensity into the event arrival dynamic, by\nincluding the impacts of order book liquidity state and time factor to the\nbaseline intensity. A regression-based non-parametric estimation procedure is\nadopted to estimate the model parameters in our Hawkes+Markovian model. To\neliminate redundant model parameters, LASSO regularization is incorporated in\nthe estimation procedure. Besides, model selection method based on Akaike\nInformation Criteria is applied to evaluate the effect of each part of the\nproposed model. An implementation example based on real LOB data is provided.\nThrough the example, we study the empirical shapes of Hawkes excitement\nfunctions, the effects of liquidity state as well as time factors, the LASSO\nvariable selection, and the explanatory power of Hawkes and Markovian elements\nto the dynamics of the order book.\n"
    },
    {
        "paper_id": 2107.09636,
        "authors": "Yves Smeers, Sebastian Martin, Jose A. Aguado",
        "title": "Co-optimization of Energy and Reserve with Incentives to Wind\n  Generation: Case Study",
        "comments": "8 pages, 4 tables, 5 figures. Supplementary material for article\n  entitled: Co-optimization of Energy and Reserve with Incentives to Wind\n  Generation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This case study presents an analysis and quantification of the impact of the\nlack of co-optimization of energy and reserve in the presence of high\npenetration of wind energy. The methodology is developed in a companion paper,\nPart I. Two models, with and without co-optimization are confronted. The\nmodeling of reserve and the incentive to renewable as well as the calibration\nof the model are inspired by the Spanish market. A sensitivity analysis is\nperformed on configurations that differ by generation capacity, ramping\ncapability, and market parameters (available wind, Feed in Premium to wind,\ngenerators risk aversion, and reserve requirement). The models and the case\nstudy are purely illustrative but the methodology is general.\n"
    },
    {
        "paper_id": 2107.09637,
        "authors": "Daniel Berleant, Venkat Kodali, Richard Segall, Hyacinthe Aboudja and\n  Michael Howell",
        "title": "Moore's law, Wright's law and the Countdown to Exponential Space",
        "comments": "This article may also be obtained at\n  https://www.thespacereview.com/article/3632/1",
        "journal-ref": "The Space Review (2019), Jan. 7,\n  www.thespacereview.com/article/3632/1",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Technologies have often been observed to improve exponentially over time. In\npractice this often means identifying a constant known as the doubling time,\ndescribing the time period over which the technology roughly doubles in some\nmeasure of performance or of performance per dollar. Moore's law is,\nclassically, the empirical observation that the number of electronic components\nthat can be put on a chip doubles every 18 months to 2 years. Today it is\nfrequently stated as the number of computations available per unit of cost.\nGeneralized to the appropriate doubling time, it describes the rate of\nadvancement in many technologies. A frequently noted competitor to Moore's law\nis known as Wright's law, which has aeronautical roots. Wright's law (also\ncalled power law, experience curve and Henderson's law) relates some quality of\na manufactured unit (for Wright, airplanes) to the volume of units\nmanufactured. The Wright's law equation expresses the idea that performance -\nprice or a quality metric - improves according to a power of the number\nproduced, or alternatively stated, improves by a constant percentage for every\ndoubling of the total number produced.\n  Does exploration of outer space conform to Moore's law or Wright's law-like\nbehavior? Our results below are broadly consistent with these laws. This is\ntrue for many technologies. Although the two laws can make somewhat different\npredictions, Sahal found that they converge to the same predictions when\nmanufacturing volume increases exponentially over time. When space exploration\ntransitions into an independent commercial sector, as many people hope and\nexpect, spacecraft technology will then likely enter an era of unambiguously\nexponential advancement.\n"
    },
    {
        "paper_id": 2107.10001,
        "authors": "Evan Hurwitz, George Cevora",
        "title": "Forecasting performance of workforce reskilling programmes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Estimating success rates for programmes aiming to reintegrate theunemployed\ninto the workforce is essential for good stewardship of publicfinances. At the\ncurrent moment, the methods used for this task arebased on the historical\nperformance of comparable programmes. In lightof Brexit and Covid-19\nsimultaneously causing a shock to the labourmarket in the UK we developed an\nestimation method that is basedon fundamental factors involved - workforce\ndemand and supply - asopposed to the historical values which are quickly\nbecoming irrelevant.With an average error of 3.9% of the re-integration success\nrate, ourmodel outperforms the best benchmark known to us by 53%\n"
    },
    {
        "paper_id": 2107.10225,
        "authors": "Wen Su",
        "title": "Pricing Exchange Option Based on Copulas by MCMC Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper focus on pricing exchange option based on copulas by MCMC\nalgorithm. Initially, we introduce the methodologies concerned about\nrisk-netural pricing, copulas and MCMC algorithm. After the basic knowledge, we\ncompare the option prices given by different models, the results show except\nGumbel copula, the other model provide similar estimation.\n"
    },
    {
        "paper_id": 2107.10226,
        "authors": "Wen Su",
        "title": "Default Distances Based on the CEV-KMV Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper presents a new method to assess default risk based on applying the\nCEV process to the KMV model. We find that the volatility of the firm asset\nvalue may not be a constant, so we assume the firm's asset value dynamics are\ngiven by the CEV process $\\frac{dV_A}{V_A} = \\mu_A dt + \\delta V_A^{\\beta-1}dB$\nand use the equivalent volatility method to estimate parameters. Focus on the\ndistances to default, our CEV-KMV model fits the market better when forecasting\nthe credit risk compared to the classical KMV model. Besides, The estimation\nresults show the $\\beta>1$ for non ST companies while $\\beta<1$ for ST\ncompanies, which means their difference in the local volatility structure: ST\nvolatility is decreasing with respect to the firm's asset while non ST\nvolatility is increasing.\n"
    },
    {
        "paper_id": 2107.10306,
        "authors": "Dan Wang, Zhi Chen, Ionut Florescu",
        "title": "A Sparsity Algorithm with Applications to Corporate Credit Rating",
        "comments": "16 pages, 11 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Artificial Intelligence, interpreting the results of a Machine Learning\ntechnique often termed as a black box is a difficult task. A counterfactual\nexplanation of a particular \"black box\" attempts to find the smallest change to\nthe input values that modifies the prediction to a particular output, other\nthan the original one. In this work we formulate the problem of finding a\ncounterfactual explanation as an optimization problem. We propose a new\n\"sparsity algorithm\" which solves the optimization problem, while also\nmaximizing the sparsity of the counterfactual explanation. We apply the\nsparsity algorithm to provide a simple suggestion to publicly traded companies\nin order to improve their credit ratings. We validate the sparsity algorithm\nwith a synthetically generated dataset and we further apply it to quarterly\nfinancial statements from companies in financial, healthcare and IT sectors of\nthe US market. We provide evidence that the counterfactual explanation can\ncapture the nature of the real statement features that changed between the\ncurrent quarter and the following quarter when ratings improved. The empirical\nresults show that the higher the rating of a company the greater the \"effort\"\nrequired to further improve credit rating.\n"
    },
    {
        "paper_id": 2107.10377,
        "authors": "Lorenzo Silotto, Marco Scaringi, Marco Bianchetti",
        "title": "Everything You Always Wanted to Know About XVA Model Risk but Were\n  Afraid to Ask",
        "comments": "59 pages, 15 figures, 16 tables, 43 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Valuation adjustments, collectively named XVA, play an important role in\nmodern derivatives pricing. XVA are an exotic pricing component since they\nrequire the forward simulation of multiple risk factors in order to compute the\nportfolio exposure including collateral, leading to a significant model risk\nand computational effort, even in case of plain vanilla trades. This work\nanalyses the most critical model risk factors, meant as those to which XVA are\nmost sensitive, finding an acceptable compromise between accuracy and\nperformance. This task has been conducted in a complete context including a\nmarket standard multi-curve G2++ model calibrated on real market data, both\nVariation Margin and ISDA-SIMM dynamic Initial Margin, different\ncollateralization schemes, and the most common linear and non-linear interest\nrates derivatives. Moreover, we considered an alternative analytical approach\nfor XVA in case of uncollateralized Swaps. We show that a crucial element is\nthe construction of a parsimonious time grid capable of capturing all\nperiodical spikes arising in collateralized exposure during the Margin Period\nof Risk. To this end, we propose a workaround to efficiently capture all\nspikes. Moreover, we show that there exists a parameterization which allows to\nobtain accurate results in a reasonable time, which is a very important feature\nfor practical applications. In order to address the valuation uncertainty\nlinked to the existence of a range of different parameterizations, we calculate\nthe Model Risk AVA (Additional Valuation Adjustment) for XVA according to the\nprovisions of the EU Prudent Valuation regulation. Finally, this work can serve\nas an handbook containing step-by-step instructions for the implementation of a\ncomplete, realistic and robust modelling framework of collateralized exposure\nand XVA.\n"
    },
    {
        "paper_id": 2107.10455,
        "authors": "Masud Alam",
        "title": "Time Varying Risk in U.S. Housing Sector and Real Estate Investment\n  Trusts Equity Return",
        "comments": "53 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study examines how housing sector volatilities affect real estate\ninvestment trust (REIT) equity return in the United States. I argue that\nunexpected changes in housing variables can be a source of aggregate housing\nrisk, and the first principal component extracted from the volatilities of U.S.\nhousing variables can predict the expected REIT equity returns. I propose and\nconstruct a factor-based housing risk index as an additional factor in asset\nprice models that uses the time-varying conditional volatility of housing\nvariables within the U.S. housing sector. The findings show that the proposed\nhousing risk index is economically and theoretically consistent with the\nrisk-return relationship of the conditional Intertemporal Capital Asset Pricing\nModel (ICAPM) of Merton (1973), which predicts an average maximum of 5.6\npercent of risk premium in REIT equity return. In subsample analyses, the\npositive relationship is not affected by sample periods' choice but shows\nhigher housing risk beta values for the 2009-18 sample period. The relationship\nremains significant after controlling for VIX, Fama-French three factors, and a\nbroad set of macroeconomic and financial variables. Moreover, the proposed\nhousing beta also accurately forecasts U.S. macroeconomic and financial\nconditions.\n"
    },
    {
        "paper_id": 2107.10491,
        "authors": "Matteo Brachetta and Claudia Ceci",
        "title": "A Stochastic Control Approach to Public Debt Management",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We discuss a class of debt management problems in a stochastic environment\nmodel. We propose a model for the debt-to-GDP (Gross Domestic Product) ratio\nwhere the government interventions via fiscal policies affect the public debt\nand the GDP growth rate at the same time. We allow for stochastic interest rate\nand possible correlation with the GDP growth rate through the dependence of\nboth the processes (interest rate and GDP growth rate) on a stochastic factor\nwhich may represent any relevant macroeconomic variable, such as the state of\neconomy. We tackle the problem of a government whose goal is to determine the\nfiscal policy in order to minimize a general functional cost. We prove that the\nvalue function is a viscosity solution to the Hamilton-Jacobi-Bellman equation\nand provide a Verification Theorem based on classical solutions. We investigate\nthe form of the candidate optimal fiscal policy in many cases of interest,\nproviding interesting policy insights. Finally, we discuss two applications to\nthe debt reduction problem and debt smoothing, providing explicit expressions\nof the value function and the optimal policy in some special cases.\n"
    },
    {
        "paper_id": 2107.1051,
        "authors": "Tongseok Lim",
        "title": "Hodge theoretic reward allocation for generalized cooperative games on\n  graphs",
        "comments": "revised overall with section 3 added",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper generalizes L.S. Shapley's celebrated value allocation theory on\ncoalition games by discovering and applying a fundamental connection between\nstochastic path integration driven by canonical time-reversible Markov chains\nand Hodge-theoretic discrete Poisson's equations on general weighted graphs.\n  More precisely, we begin by defining cooperative games on general graphs and\ngeneralize Shapley's value allocation formula for those games in terms of\nstochastic path integral driven by the associated canonical Markov chain. We\nthen show the value allocation operator, one for each player defined by the\npath integral, turns out to be the solution to the Poisson's equation defined\nvia the combinatorial Hodge decomposition on general weighted graphs.\n  Several motivational examples and applications are presented, in particular,\na section is devoted to reinterpret and extend Nash's and Kohlberg and Neyman's\nsolution concept for cooperative games. This and other examples, e.g. on\nrevenue management, suggest that our general framework does not have to be\nrestricted to cooperative games setup, but may apply to broader range of\nproblems arising in economics, finance and other social and physical sciences.\n"
    },
    {
        "paper_id": 2107.10606,
        "authors": "Gautier Marti, Victor Goubet, Frank Nielsen",
        "title": "cCorrGAN: Conditional Correlation GAN for Learning Empirical Conditional\n  Distributions in the Elliptope",
        "comments": "International Conference on Geometric Science of Information",
        "journal-ref": "GSI 2021: Geometric Science of Information pp 613-620",
        "doi": "10.1007/978-3-030-80209-7_66",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a methodology to approximate conditional distributions in the\nelliptope of correlation matrices based on conditional generative adversarial\nnetworks. We illustrate the methodology with an application from quantitative\nfinance: Monte Carlo simulations of correlated returns to compare risk-based\nportfolio construction methods. Finally, we discuss about current limitations\nand advocate for further exploration of the elliptope geometry to improve\nresults.\n"
    },
    {
        "paper_id": 2107.10634,
        "authors": "Jesus M. Gonzalez-Barahona",
        "title": "Factors determining maximum energy consumption of Bitcoin miners",
        "comments": "24 pages, request for comments",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Background: During the last years, there has been a lot of discussion and\nestimations on the energy consumption of Bitcoin miners. However, most of the\nstudies are focused on estimating energy consumption, not in exploring the\nfactors that determine it.\n  Goal: To explore the factors that determine maximum energy consumption of\nBitcoin miners. In particular, analyze the limits of energy consumption, and to\nwhich extent variations of the factors could produce its reduction.\n  Method: Estimate the overall profit of all Bitcoin miners during a certain\nperiod of time, and the costs (including energy) that they face during that\ntime, because of the mining activity. The underlying assumptions is that miners\nwill only consume energy to mine Bitcoin if they have the expectation of\nprofit, and at the same time they are competitive with respect of each other.\nTherefore, they will operate as a group in the point where profits balance\nexpenditures.\n  Results: We show a basic equation that determines energy consumption based on\nsome specific factors: minting, transaction fees, exchange rate, energy price,\nand amortization cost. We also define the Amortization Factor, which can be\ncomputed for mining devices based on their cost and energy consumption, helps\nto understand how the cost of equipment influences total energy consumption.\n  Conclusions: The factors driving energy consumption are identified, and from\nthem, some ways in which Bitcoin energy consumption could be reduced are\ndiscussed. Some of these ways do not reduce the most important properties of\nBitcoin, such as the chances of control of the aggregated hashpower, or the\nfundamentals of the proof of work mechanism. In general, the methods presented\ncan help to predict energy consumption in different scenarios, based on factors\nthat can be calculated from available data, or assumed in scenarios.\n"
    },
    {
        "paper_id": 2107.10635,
        "authors": "Cosimo Munari, Lutz Wilhelmy, Stefan Weber",
        "title": "Capital Requirements and Claims Recovery: A New Perspective on Solvency\n  Regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Protection of creditors is a key objective of financial regulation. Where the\nprotection needs are high, i.e., in banking and insurance, regulatory solvency\nrequirements are an instrument to prevent that creditors incur losses on their\nclaims. The current regulatory requirements based on Value at Risk and Average\nValue at Risk limit the probability of default of financial institutions, but\nthey fail to control the size of recovery on creditors' claims in the case of\ndefault. We resolve this failure by developing a novel risk measure, Recovery\nValue at Risk. Our conceptual approach can flexibly be extended and allows the\nconstruction of general recovery risk measures for various risk management\npurposes. By design, these risk measures control recovery on creditors' claims\nand integrate the protection needs of creditors into the incentive structure of\nthe management. We provide detailed case studies and applications: We analyze\nhow recovery risk measures react to the joint distributions of assets and\nliabilities on firms' balance sheets and compare the corresponding capital\nrequirements with the current regulatory benchmarks based on Value at Risk and\nAverage Value at Risk. We discuss how to calibrate recovery risk measures to\nhistoric regulatory standards. Finally, we show that recovery risk measures can\nbe applied to performance-based management of business divisions of firms and\nthat they allow for a tractable characterization of optimal tradeoffs between\nrisk and return in the context of investment management.\n"
    },
    {
        "paper_id": 2107.10723,
        "authors": "Bheemeshwar Reddy A, Sunny Jose, Vaidehi R",
        "title": "Of Access and Inclusivity Digital Divide in Online Education",
        "comments": null,
        "journal-ref": "Economic and Political Weekly,Vol 36, pp 23-26 (2020)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Can online education enable all students to participate in and benefit from\nit equally? Massive online education without addressing the huge access gap and\ndisparities in digital infrastructure would not only exclude a vast majority of\nstudents from learning opportunities but also exacerbate the existing\nsocio-economic disparities in educational opportunities.\n"
    },
    {
        "paper_id": 2107.10891,
        "authors": "Gian Paolo Clemente, Francesco Della Corte, Nino Savelli",
        "title": "A bridge between Local GAAP and Solvency II frameworks to quantify\n  Capital Requirement for demographic risk",
        "comments": "24 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.3390/risks9100175",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper provides a stochastic model useful for assessing the capital\nrequirement for demographic risk. The model extends to the market consistent\ncontext classical methodologies developed in a local accounting framework. In\nparticular we provide a unique formulation for different non-participating life\ninsurance contracts and we prove analytically that the valuation of demographic\nprofit can be significantly affected by the financial conditions in the market.\nA case study has been also developed considering a portfolio of life insurance\ncontracts. Results prove the effectiveness of the model in highlighting main\ndrivers of capital requirement evaluation, also compared to local GAAP\nframework.\n"
    },
    {
        "paper_id": 2107.10941,
        "authors": "Qinkai Chen and Christian-Yann Robert",
        "title": "Graph-Based Learning for Stock Movement Prediction with Textual and\n  Relational Data",
        "comments": "10 pages, 3 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.1145/3533271.3561663",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting stock prices from textual information is a challenging task due to\nthe uncertainty of the market and the difficulty understanding the natural\nlanguage from a machine's perspective. Previous researches focus mostly on\nsentiment extraction based on single news. However, the stocks on the financial\nmarket can be highly correlated, one news regarding one stock can quickly\nimpact the prices of other stocks. To take this effect into account, we propose\na new stock movement prediction framework: Multi-Graph Recurrent Network for\nStock Forecasting (MGRN). This architecture allows to combine the textual\nsentiment from financial news and multiple relational information extracted\nfrom other financial data. Through an accuracy test and a trading simulation on\nthe stocks in the STOXX Europe 600 index, we demonstrate a better performance\nfrom our model than other benchmarks.\n"
    },
    {
        "paper_id": 2107.1098,
        "authors": "Zihao Wang, Kun Li, Steve Q. Xia, Hongfu Liu",
        "title": "Economic Recession Prediction Using Deep Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate the effectiveness of different machine learning methodologies\nin predicting economic cycles. We identify the deep learning methodology of\nBi-LSTM with Autoencoder as the most accurate model to forecast the beginning\nand end of economic recessions in the U.S. We adopt commonly-available macro\nand market-condition features to compare the ability of different machine\nlearning models to generate good predictions both in-sample and out-of-sample.\nThe proposed model is flexible and dynamic when both predictive variables and\nmodel coefficients vary over time. It provided good out-of-sample predictions\nfor the past two recessions and early warning about the COVID-19 recession.\n"
    },
    {
        "paper_id": 2107.11048,
        "authors": "Antonis Papapantoleon and Dylan Possama\\\"i and Alexandros Saplaouras",
        "title": "Stability of backward stochastic differential equations: the general\n  case",
        "comments": "45 pages, final version, forthcoming in the Electronic Journal of\n  Probability",
        "journal-ref": null,
        "doi": "10.1214/23-EJP939",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we obtain stability results for backward stochastic\ndifferential equations with jumps (BSDEs) in a very general framework. More\nspecifically, we consider a convergent sequence of standard data, each\nassociated to their own filtration, and we prove that the associated sequence\nof (unique) solutions is also convergent. The current result extends earlier\ncontributions in the literature of stability of BSDEs and unifies several\nframeworks for numerical approximations of BSDEs and their implementations.\n"
    },
    {
        "paper_id": 2107.11059,
        "authors": "Ronald Richman and Mario V. W\\\"uthrich",
        "title": "LocalGLMnet: interpretable deep learning for tabular data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Deep learning models have gained great popularity in statistical modeling\nbecause they lead to very competitive regression models, often outperforming\nclassical statistical models such as generalized linear models. The\ndisadvantage of deep learning models is that their solutions are difficult to\ninterpret and explain, and variable selection is not easily possible because\ndeep learning models solve feature engineering and variable selection\ninternally in a nontransparent way. Inspired by the appealing structure of\ngeneralized linear models, we propose a new network architecture that shares\nsimilar features as generalized linear models, but provides superior predictive\npower benefiting from the art of representation learning. This new architecture\nallows for variable selection of tabular data and for interpretation of the\ncalibrated deep learning model, in fact, our approach provides an additive\ndecomposition in the spirit of Shapley values and integrated gradients.\n"
    },
    {
        "paper_id": 2107.11124,
        "authors": "Maciej Ber\\k{e}sewicz, Dagmara Nikulin",
        "title": "COVID-19 and the gig economy in Poland",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We use a dataset covering nearly the entire target population based on\npassively collected data from smartphones to measure the impact of the first\nCOVID-19 wave on the gig economy in Poland. In particular, we focus on\ntransportation (Uber, Bolt) and delivery (Wolt, Takeaway, Glover, DeliGoo)\napps, which make it possible to distinguish between the demand and supply part\nof this market. Based on Bayesian structural time-series models, we estimate\nthe causal impact of the first COVID-19 wave on the number of active drivers\nand couriers. We show a significant relative increase for Wolt and Glover (15%\nand 24%) and a slight relative decrease for Uber and Bolt (-3% and -7%) in\ncomparison to a counterfactual control. The change for Uber and Bolt can be\npartially explained by the prospect of a new law (the so-called Uber Lex),\nwhich was already announced in 2019 and is intended to regulate the work of\nplatform drivers.\n"
    },
    {
        "paper_id": 2107.11133,
        "authors": "Etienne Theising, Dominik Wied, Daniel Ziggel",
        "title": "Reference Class Selection in Similarity-Based Forecasting of Sales\n  Growth",
        "comments": "37 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1002/for.2927",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a general method to handle forecasts exposed to\nbehavioural bias by finding appropriate outside views, in our case corporate\nsales forecasts of analysts. The idea is to find reference classes, i.e. peer\ngroups, for each analyzed company separately that share similarities to the\nfirm of interest with respect to a specific predictor. The classes are regarded\nto be optimal if the forecasted sales distributions match the actual\ndistributions as closely as possible. The forecast quality is measured by\napplying goodness-of-fit tests on the estimated probability integral\ntransformations and by comparing the predicted quantiles. The method is\nout-of-sample backtested on a data set consisting of 21,808 US firms over the\ntime period 1950 - 2019, which is also descriptively analyzed. It appears that\nin particular the past operating margins are good predictors for the\ndistribution of future sales. A case study compares the outside view of our\ndistributional forecasts with actual analysts' forecasts and emphasizes the\nrelevance of our approach in practice.\n"
    },
    {
        "paper_id": 2107.11185,
        "authors": "Eiji Yamamura",
        "title": "Where do I rank? Am I happy?: learning income position and\n  subjective-wellbeing in an internet experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A tailor-made internet survey experiment provides individuals with\ninformation on their income positions to examine their effects on subjective\nwell-being. In the first survey, respondents were asked about their household\nincome and subjective well-being. Based on the data collected, three different\nrespondents' income positions within the residential locality, within a group\nof the same educational background, and cohort were obtained. In the follow-up\nsurvey for the treatment group, respondents are informed of their income\npositions and then asked for subjective well-being. Key findings are that,\nafter obtaining information, a higher individual's income position improves\ntheir subjective well-being. The effects varied according to individual\ncharacteristics and proxies.\n"
    },
    {
        "paper_id": 2107.11255,
        "authors": "Ge-zhi Wu and Da-ming You",
        "title": "Margin trading, short selling and corporate green innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper uses the panel data of Chinese listed companies from 2007 to 2019,\nuses the relaxation of China's margin trading and short selling restrictions as\nthe basis of quasi experimental research, and then constructs a double\ndifference model to analyze whether the margin trading and short selling will\nencourage enterprises to engage in green technology innovation activities.\nFirstly, our research results show that after the implementation of the margin\ntrading and short selling, the green technology innovation behavior of pilot\ncompanies will increase significantly. We believe that the short selling threat\nand pressure brought by short selling to enterprises are the main reasons for\npilot enterprises to engage in green technology innovation. Secondly, the\nempirical results show that the implementation of margin trading and short\nselling will significantly promote the quantity of green technology innovation\nof pilot enterprises, but will not significantly promote the quality of green\ntechnology innovation of pilot enterprises. Furthermore, we analyze the\ndifference of the impact of margin trading and short selling on the quantity of\ngreen technology innovation of pilot enterprises in different periods. Finally,\nwe find that the performance decline, yield gap between financial assets and\noperating assets, the risk of stock price crash, management shareholding,\ninstitutional shareholding ratio, product market competition, short selling\nintensity, margin trading intensity and formal environmental regulation\nintensity will affect the role of policy in promoting green technology\ninnovation of pilot enterprises.\n"
    },
    {
        "paper_id": 2107.11314,
        "authors": "Nicolas Eschenbaum (University of St. Gallen) and Helge Liebert\n  (University of Zurich)",
        "title": "Dealing with Uncertainty: The Value of Reputation in the Absence of\n  Legal Institutions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies reputation in the online market for illegal drugs in which\nno legal institutions exist to alleviate uncertainty. Trade takes place on\nplatforms that offer rating systems for sellers, thereby providing an\nobservable measure of reputation. The analysis exploits the fact that one of\nthe two dominant platforms unexpectedly disappeared. Re-entering sellers reset\ntheir rating. The results show that on average prices decreased by up to 9% and\nthat a 1% increase in rating causes a price increase of 1%. Ratings and prices\nrecover after about three months. We calculate that identified good types earn\n1,650 USD more per week.\n"
    },
    {
        "paper_id": 2107.1134,
        "authors": "Alexandre Carbonneau and Fr\\'ed\\'eric Godin",
        "title": "Deep equal risk pricing of financial derivatives with non-translation\n  invariant risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of non-translation invariant risk measures within the equal risk\npricing (ERP) methodology for the valuation of financial derivatives is\ninvestigated. The ability to move beyond the class of convex risk measures\nconsidered in several prior studies provides more flexibility within the\npricing scheme. In particular, suitable choices for the risk measure embedded\nin the ERP framework such as the semi-mean-square-error (SMSE) are shown herein\nto alleviate the price inflation phenomenon observed under Tail Value-at-Risk\nbased ERP as documented for instance in Carbonneau and Godin (2021b). The\nnumerical implementation of non-translation invariant ERP is performed through\ndeep reinforcement learning, where a slight modification is applied to the\nconventional deep hedging training algorithm (see Buehler et al., 2019) so as\nto enable obtaining a price through a single training run for the two neural\nnetworks associated with the respective long and short hedging strategies. The\naccuracy of the neural network training procedure is shown in simulation\nexperiments not to be materially impacted by such modification of the training\nalgorithm.\n"
    },
    {
        "paper_id": 2107.11371,
        "authors": "Jaydip Sen and Sidra Mehtab",
        "title": "Optimum Risk Portfolio and Eigen Portfolio: A Comparative Analysis Using\n  Selected Stocks from the Indian Stock Market",
        "comments": "The is the preprint of our accepted paper in the journal\n  International Journal of Business Forecasting and Marketing Intelligence\n  published by Inderscience Publishers, Switzerland. It consists of 35 pages,\n  and includes 29 figures and 36 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing an optimum portfolio that allocates weights to its constituent\nstocks in a way that achieves the best trade-off between the return and the\nrisk is a challenging research problem. The classical mean-variance theory of\nportfolio proposed by Markowitz is found to perform sub-optimally on the\nreal-world stock market data since the error in estimation for the expected\nreturns adversely affects the performance of the portfolio. This paper presents\nthree approaches to portfolio design, viz, the minimum risk portfolio, the\noptimum risk portfolio, and the Eigen portfolio, for seven important sectors of\nthe Indian stock market. The daily historical prices of the stocks are scraped\nfrom Yahoo Finance website from January 1, 2016, to December 31, 2020. Three\nportfolios are built for each of the seven sectors chosen for this study, and\nthe portfolios are analyzed on the training data based on several metrics such\nas annualized return and risk, weights assigned to the constituent stocks, the\ncorrelation heatmaps, and the principal components of the Eigen portfolios.\nFinally, the optimum risk portfolios and the Eigen portfolios for all sectors\nare tested on their return over a period of a six-month period. The\nperformances of the portfolios are compared and the portfolio yielding the\nhigher return for each sector is identified.\n"
    },
    {
        "paper_id": 2107.11575,
        "authors": "Jingfeng Lu, Zongwei Lu, Christian Riis",
        "title": "Peace through bribing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a model in which before a conflict between two parties escalates\ninto a war (in the form of an all-pay auction), a party can offer a\ntake-it-or-leave-it bribe to the other for a peaceful settlement. In contrast\nto the received literature, we find that peace security is impossible in our\nmodel. We characterize the necessary and sufficient conditions for peace\nimplementability. Furthermore, we find that separating equilibria do not exist\nand the number of (on-path) bribes in any non-peaceful equilibria is at most\ntwo. We also consider a requesting model and characterize the necessary and\nsufficient conditions for the existence of robust peaceful equilibria, all of\nwhich are sustained by the identical (on-path) request. Contrary to the bribing\nmodel, peace security is possible in the requesting model.\n"
    },
    {
        "paper_id": 2107.11589,
        "authors": "Shahram Heydari, Garyfallos Konstantinoudis, Abdul Wahid Behsoodi",
        "title": "Effect of the COVID-19 pandemic on bike-sharing demand and hire time:\n  Evidence from Santander Cycles in London",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0260969",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has been influencing travel behaviour in many urban\nareas around the world since the beginning of 2020. As a consequence,\nbike-sharing schemes have been affected partly due to the change in travel\ndemand and behaviour as well as a shift from public transit. This study\nestimates the varying effect of the COVID-19 pandemic on the London\nbike-sharing system (Santander Cycles) over the period March-December 2020. We\nemployed a Bayesian second-order random walk time-series model to account for\ntemporal correlation in the data. We compared the observed number of cycle\nhires and hire time with their respective counterfactuals (what would have been\nif the pandemic had not happened) to estimate the magnitude of the change\ncaused by the pandemic. The results indicated that following a reduction in\ncycle hires in March and April 2020, the demand rebounded from May 2020,\nremaining in the expected range of what would have been if the pandemic had not\noccurred. This could indicate the resiliency of Santander Cycles. With respect\nto hire time, an important increase occurred in April, May, and June 2020,\nindicating that bikes were hired for longer trips, perhaps partly due to a\nshift from public transit.\n"
    },
    {
        "paper_id": 2107.11593,
        "authors": "Haoqi Qian, Zhengyu Shi, Libo Wu",
        "title": "Inferring Economic Condition Uncertainty from Electricity Big Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inferring the uncertainty in economic conditions is significant for both\ndecision makers as well as market players. In this paper, we propose a novel\napproach to measure the economic uncertainties by using the Hidden Markov Model\n(HMM). We construct a dimensionless index, Economic Condition Uncertainty (ECU)\nindex, which ranges from zero to one and is comparable among sectors, regions\nand periods. We used the daily electricity consumption data of more than 18,000\nfirms in Shanghai from 2018 to 2020 to construct the ECU indexes. Results show\nthat all ECU indexes, whether at sectoral or regional level, successfully\ncaptured the negative impacts of COVID-19 on Shanghai's economic conditions.\nBesides, the ECU indexes also presented the heterogeneities in different\ndistricts as well as in different sectors. This reflects the facts that changes\nin the uncertainty of economic conditions are mainly related to regional\neconomic structures and targeted regulatory policies faced by sectors. The ECU\nindex can also be readily extended to measure the uncertainty of economic\nconditions in various realms, which has great potentials in the future.\n"
    },
    {
        "paper_id": 2107.11896,
        "authors": "Safa Alsheyab and Tahir Choulli",
        "title": "Reflected backward stochastic differential equations under stopping with\n  an arbitrary random time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses reflected backward stochastic differential equations\n(RBSDE hereafter) that take the form of \\begin{eqnarray*} \\begin{cases}\ndY_t=f(t,Y_t, Z_t)d(t\\wedge\\tau)+Z_tdW_t^{\\tau}+dM_t-dK_t,\\quad Y_{\\tau}=\\xi,\n  Y\\geq S\\quad\\mbox{on}\\quad \\Lbrack0,\\tau\\Lbrack,\\quad\n\\displaystyle\\int_0^{\\tau}(Y_{s-}-S_{s-})dK_s=0\\quad P\\mbox{-a.s..}\\end{cases}\n\\end{eqnarray*} Here $\\tau$ is an arbitrary random time that might not be a\nstopping time for the filtration $\\mathbb F$ generated by the Brownian motion\n$W$. We consider the filtration $\\mathbb G$ resulting from the progressive\nenlargement of $\\mathbb F$ with $\\tau$ where this becomes a stopping time, and\nstudy the RBSDE under $\\mathbb G$. Precisely, we focus on answering the\nfollowing problems: a) What are the sufficient minimal conditions on the data\n$(f, \\xi, S, \\tau)$ that guarantee the existence of the solution of the\n$\\mathbb G$-RBSDE in $L^p$ ($p>1$)? b) How can we estimate the solution in norm\nusing the triplet-data $(f, \\xi, S)$? c) Is there an RBSDE under $\\mathbb F$\nthat is intimately related to the current one and how their solutions are\nrelated to each other? We prove that for any random time, having a positive\nAz\\'ema supermartingale, there exists a positive discount factor\n${\\widetilde{\\cal E}}$ that is vital in answering our questions without\nassuming any further assumption on $\\tau$, and determining the space for the\ntriplet-data $(f,\\xi, S)$ and the space for the solution of the RBSDE as well.\n"
    },
    {
        "paper_id": 2107.11972,
        "authors": "Liang Zeng, Lei Wang, Hui Niu, Ruchen Zhang, Ling Wang, Jian Li",
        "title": "Trade When Opportunity Comes: Price Movement Forecasting via\n  Locality-Aware Attention and Iterative Refinement Labeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price movement forecasting, aimed at predicting financial asset trends based\non current market information, has achieved promising advancements through\nmachine learning (ML) methods. Most existing ML methods, however, struggle with\nthe extremely low signal-to-noise ratio and stochastic nature of financial\ndata, often mistaking noises for real trading signals without careful selection\nof potentially profitable samples. To address this issue, we propose LARA, a\nnovel price movement forecasting framework with two main components:\nLocality-Aware Attention (LA-Attention) and Iterative Refinement Labeling\n(RA-Labeling). (1) LA-Attention, enhanced by metric learning techniques,\nautomatically extracts the potentially profitable samples through masked\nattention scheme and task-specific distance metrics. (2) RA-Labeling further\niteratively refines the noisy labels of potentially profitable samples, and\ncombines the learned predictors robust to the unseen and noisy samples. In a\nset of experiments on three real-world financial markets: stocks,\ncryptocurrencies, and ETFs, LARA significantly outperforms several machine\nlearning based methods on the Qlib quantitative investment platform. Extensive\nablation studies confirm LARA's superior ability in capturing more reliable\ntrading opportunities.\n"
    },
    {
        "paper_id": 2107.12041,
        "authors": "Carol Alexander and Ding Chen and Arben Imeraj",
        "title": "Inverse and Quanto Inverse Options in a Black-Scholes World",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Over 90% of exchange trading on crypto options has always been on the Deribit\nplatform. This centralised crypto exchange only lists inverse products because\nthey do not accept fiat currency. Currently, fiat-based traders can only make\ndeposits in bitcoin, although they can withdraw both bitcoin and ether to their\non-chain wallets. Likewise, other major crypto options platforms only list\ncrypto--stablecoin trading pairs in so-called direct options, which are similar\nto the standard crypto options listed by the CME except the U.S. dollar is\nreplaced by a stablecoin version. Until now a clear mathematical exposition of\nthese products has been lacking. We discuss the sources of market\nincompleteness in direct and inverse options and compare their pricing and\nhedging characteristics. Then we discuss the useful applications of currency\nprotected \"quanto\" direct and inverse options for fiat-based traders and\ndescribe their pricing and hedging characteristics, all in the Black-Scholes\nsetting.\n"
    },
    {
        "paper_id": 2107.1208,
        "authors": "Hossain Ahmed Taufiq",
        "title": "Rohingya Refugee Crisis and the State of Insecurity in Bangladesh",
        "comments": "Book chapter in: Ahmed, Imtiaz, ed. Genocide and Mass Violence:\n  Politics of Singularity. Centre for Genocide Studies, University of Dhaka,\n  2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The ongoing Rohingya refugee crisis is considered as one of the largest\nhuman-made humanitarian disasters of the 21st century. So far, Bangladesh is\nthe largest recipient of these refugees. According to the United Nations Office\nfor the Coordination of Humanitarian Affairs (UN OCHA), approximately 650,000\nnew entrants have been recorded since the new violence erupted on 25 August\n2017 in the Rakhine state of Myanmar.1 However, such crisis is nothing new in\nBangladesh, nor are the security-related challenges new that such an exodus\nbrings with it. Ever since the military came to power in Myanmar (in 1962),\nRohingya exodus to neighboring countries became a recurring incident. The\nlatest mass exodus of Rohingyas from Rakhine state of Myanmar to Bangladesh is\nthe largest of such influxes. Unlike, the previous refugee crisis, the ongoing\ncrisis has wide-ranging security implications on Bangladesh. They are also\nvaried and multifaceted. Thus, responsibilities for ensuring effective\nprotection have become operationally multilateral. The problem of security\nregarding the Rohingya refugee issue is complicated by the Islamist insurgency,\nillicit methamphetamine/yaba drug trafficking, and HIV/AIDS/STI prevalence\nfactors. The chapter examines the different dimensions of security challenges\nthat the recent spell of Rohingya exodus brings to Bangladesh and the refugees\nthemselves. In order to understand the challenges, firstly the chapter attempts\nto conceptualize the prominent security frameworks. Secondly, it examines the\ncontext and political economy behind the persecution of Rohingyas in the\nRakhine state. Thirdly, it explores the political and military aspects of\nsecurity. Fourthly, it explores the social and economic dimensions. Finally, it\nexamines the environmental impacts of Rohingya crisis in Bangladesh.\n"
    },
    {
        "paper_id": 2107.12094,
        "authors": "Martin Herdegen, Johannes Muhle-Karbe, Florian Stebegg",
        "title": "Liquidity Provision with Adverse Selection and Inventory Costs",
        "comments": "35 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study one-shot Nash competition between an arbitrary number of identical\ndealers that compete for the order flow of a client. The client trades either\nbecause of proprietary information, exposure to idiosyncratic risk, or a mix of\nboth trading motives. When quoting their price schedules, the dealers do not\nknow the client's type but only its distribution, and in turn choose their\nprice quotes to mitigate between adverse selection and inventory costs. Under\nessentially minimal conditions, we show that a unique symmetric Nash\nequilibrium exists and can be characterized by the solution of a nonlinear ODE.\n"
    },
    {
        "paper_id": 2107.12439,
        "authors": "Alan L. Lewis and Dan Pirjol",
        "title": "Proof of non-convergence of the short-maturity expansion for the SABR\n  model",
        "comments": "18 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the convergence properties of the short maturity expansion of option\nprices in the uncorrelated log-normal ($\\beta=1$) SABR model. In this model the\noption time-value can be represented as an integral of the form $V(T) =\n\\int_{0}^\\infty e^{-\\frac{u^2}{2T}} g(u) du$ with $g(u)$ a \"payoff function\"\nwhich is given by an integral over the McKean kernel $G(s,t)$. We study the\nanalyticity properties of the function $g(u)$ in the complex $u$-plane and show\nthat it is holomorphic in the strip $|\\Im(u) |< \\pi$. Using this result we show\nthat the $T$-series expansion of $V(T)$ and implied volatility are asymptotic\n(non-convergent for any $T>0$). In a certain limit which can be defined either\nas the large volatility limit $\\sigma_0\\to \\infty$ at fixed $\\omega=1$, or the\nsmall vol-of-vol limit $\\omega\\to 0$ limit at fixed $\\omega\\sigma_0$, the short\nmaturity $T$-expansion for the implied volatility has a finite convergence\nradius $T_c = \\frac{1.32}{\\omega\\sigma_0}$.\n"
    },
    {
        "paper_id": 2107.12447,
        "authors": "Alvaro Guinea Julia and Alet Roux",
        "title": "Bitcoin option pricing: A market attention approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A model is proposed for Bitcoin prices that takes into account market\nattention. Market attention, modeled by a mean-reverting Cox-Ingersoll-Ross\nprocesses, affects the volatility of Bitcoin returns, with some delay. The\nmodel is affine and tractable, with closed formulae for the conditional\ncharacteristic functions with respect to both the conventional and a delayed\nfiltration. This leads to semi-closed formulae for European call and put\nprices. A maximum likelihood estimation procedure is provided, as well as a\nmethod for changing to a risk-neutral measure. The model compares very well\nagainst classical and attention-based models when tested on real data.\n"
    },
    {
        "paper_id": 2107.12462,
        "authors": "Jan Matas and Jan Posp\\'i\\v{s}il",
        "title": "Robustness and sensitivity analyses for rough Volterra stochastic\n  volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze the robustness and sensitivity of various\ncontinuous-time rough Volterra stochastic volatility models in relation to the\nprocess of market calibration. Model robustness is examined from two\nperspectives: the sensitivity of option price estimates and the sensitivity of\nparameter estimates to changes in the option data structure. The following\nsensitivity analysis consists of statistical tests to determine whether a given\nstudied model is sensitive to changes in the option data structure based on the\ndistribution of parameter estimates. Empirical study is performed on a data set\nconsisting of Apple Inc. equity options traded on four different days in April\nand May 2015. In particular, the results for RFSV, rBergomi and $\\alpha$RFSV\nmodels are provided and compared to the results for Heston, Bates, and AFSVJD\nmodels.\n"
    },
    {
        "paper_id": 2107.12484,
        "authors": "Guillermo Angeris, Akshay Agrawal, Alex Evans, Tarun Chitra, Stephen\n  Boyd",
        "title": "Constant Function Market Makers: Multi-Asset Trades via Convex\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rise of Ethereum and other blockchains that support smart contracts has\nled to the creation of decentralized exchanges (DEXs), such as Uniswap,\nBalancer, Curve, mStable, and SushiSwap, which enable agents to trade\ncryptocurrencies without trusting a centralized authority. While traditional\nexchanges use order books to match and execute trades, DEXs are typically\norganized as constant function market makers (CFMMs). CFMMs accept and reject\nproposed trades based on the evaluation of a function that depends on the\nproposed trade and the current reserves of the DEX. For trades that involve\nonly two assets, CFMMs are easy to understand, via two functions that give the\nquantity of one asset that must be tendered to receive a given quantity of the\nother, and vice versa. When more than two assets are being exchanged, it is\nharder to understand the landscape of possible trades. We observe that various\nproblems of choosing a multi-asset trade can be formulated as convex\noptimization problems, and can therefore be reliably and efficiently solved.\n"
    },
    {
        "paper_id": 2107.12516,
        "authors": "Bruce Knuteson",
        "title": "They Chose to Not Tell You",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The world's stock markets display a strikingly suspicious, decades long\npattern of overnight and intraday returns that nobody (other than us) has\nplausibly explained and that nobody (other than us) has clearly and\npersistently alerted you to. We use correspondence on this topic over the past\nfive years to show that the silence of others on this issue does not arise from\ntheir having a good reason to believe this pattern is fine. Separately, and\nregardless of whether this pattern turns out to be fine, we have documented\nthat people in a position to alert you to the presence of strikingly suspicious\nreturn patterns in the world's stock markets that nobody can innocuously\nexplain are aware of this issue, have no good reason to believe it is not a\nproblem, and chose to not tell you.\n"
    },
    {
        "paper_id": 2107.12625,
        "authors": "Hossain Ahmed Taufiq",
        "title": "Dhaka Water-logging: Causes, Effects and Remedial Policy Options",
        "comments": "Proceedings: 'The New Megacity: For Whom?', Global Studies and\n  Governance Department, Independent University, Bangladesh, 18 November 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Water-logging is a major challenge for Dhaka city, the capital of Bangladesh.\nThe rapid, unregulated, and unplanned urbanization, as well as detrimental\nsocial, economic, infrastructural, and environmental consequences, not to\nmention diseases like dengue, challenge the several crash programs combating\nwater-logging in the city. This study provides a brief contextual analysis of\nthe Dhakas topography and natural, as well as storm water drainage systems,\nbefore concentrating on the man-made causes and effects of water-logging,\nultimately exploring a few remedial measures.\n"
    },
    {
        "paper_id": 2107.12702,
        "authors": "Anuradha Singh",
        "title": "Income Inequality and Intergenerational Mobility in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using three rounds of NSS datasets, the present paper attempts to understand\nthe relationship between income inequality and intergenerational income\nmobility (IGIM) by segregating generations into social and income classes. The\noriginality of the paper lies in assessing the IGIM using different approaches,\nwhich we expect to contribute to the existing literature. We conclude that the\ncountry has low-income mobility and high inequality which is no longer\nassociated with a particular social class in India. Also, both may have a\nnegative or positive relationship, hence needs to be studied at a regional\nlevel.\n"
    },
    {
        "paper_id": 2107.12848,
        "authors": "Charlie Pilgrim, Weisi Guo, Thomas T. Hills",
        "title": "The Rising Entropy of English in the Attention Economy",
        "comments": "24 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present evidence that the word entropy of American English has been rising\nsteadily since around 1900, contrary to predictions from existing\nsociolinguistic theories. We also find differences in word entropy between\nmedia categories, with short-form media such as news and magazines having\nhigher entropy than long-form media, and social media feeds having higher\nentropy still. To explain these results we develop an ecological model of the\nattention economy that combines ideas from Zipf's law and information foraging.\nIn this model, media consumers maximize information utility rate taking into\naccount the costs of information search, while media producers adapt to\ntechnologies that reduce search costs, driving them to generate higher entropy\ncontent in increasingly shorter formats.\n"
    },
    {
        "paper_id": 2107.12862,
        "authors": "Laurence Carassus",
        "title": "Quasi-sure essential supremum and applications to finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When uncertainty is modelled by a set of non-dominated and non-compact\nprobability measures, a notion of essential supremum for a family of\nreal-valued functions is developed in terms of upper semi-analytic functions.\nWe show how the properties postulated on the initial functions carry over to\ntheir quasi-sure essential supremum. We propose various applications to\nfinancial problems with frictions. We analyse super-replication and prove a\nbi-dual characterization of the super-hedging cost. We also study a weak\nno-arbitrage condition called Absence of Instantaneous Profit (AIP) under which\nprices are finite. This requires new results on the aggregation of quasi-sure\nstatements.\n"
    },
    {
        "paper_id": 2107.12872,
        "authors": "Emmanouil Sfendourakis, Ioane Muni Toke",
        "title": "LOB modeling using Hawkes processes with a state-dependent factor",
        "comments": "33 pages, 12 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A point process model for order flows in limit order books is proposed, in\nwhich the conditional intensity is the product of a Hawkes component and a\nstate-dependent factor. In the LOB context, state observations may include the\nobserved imbalance or the observed spread. Full technical details for the\ncomputationally-efficient estimation of such a process are provided, using\neither direct likelihood maximization or EM-type estimation. Applications\ninclude models for bid and ask market orders, or for upwards and downwards\nprice movements. Empirical results on multiple stocks traded in Euronext Paris\nunderline the benefits of state-dependent formulations for LOB modeling, e.g.\nin terms of goodness-of-fit to financial data.\n"
    },
    {
        "paper_id": 2107.12885,
        "authors": "Laurence Carassus",
        "title": "No free lunch for markets with multiple num\\'eraires",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jmateco.2022.102805",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a global market constituted by several submarkets, each with its\nown assets and num\\'eraire. We provide theoretical foundations for the\nexistence of equivalent martingale measures and results on superreplication\nprices which allows to take into account difference of features between\nsubmarkets.\n"
    },
    {
        "paper_id": 2107.12899,
        "authors": "Alois Pichler and Dana Uhlig",
        "title": "Mortality in Germany during the Covid-19 pandemic",
        "comments": "We would like to verify and update some important points and thus\n  withdraw this version, which we are not yet satisfied with",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Covid-19 pandemic still causes severe impacts on society and the economy.\nThis paper studies excess mortality during the pandemic years 2020 and 2021 in\nGermany empirically with a special focus on the life insurer's perspective. Our\nconclusions are based on official counts of German governmental offices on the\nliving and deaths of the entire population. Conclusions, relevant for actuaries\nand specific insurance business lines, including portfolios of pension, life,\nand health insurance contracts, are provided.\n"
    },
    {
        "paper_id": 2107.13128,
        "authors": "Hossain Ahmed Taufiq",
        "title": "Towards an enabling environment for social accountability in Bangladesh",
        "comments": "Proceedings: Conference on 'Accountability in Bangladesh: Issues &\n  Debates', Global Studies and Governance Program, Independent University\n  Bangladesh (IUB), 29 March 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social accountability refers to promoting good governance by making ruling\nelites more responsive. In Bangladesh, where bureaucracy and legislature\noperate with little effective accountability or checks and balances,\ntraditional horizontal or vertical accountability proved to be very blunt and\nweak. In the presence of such faulty mechanisms, ordinary citizens access to\ninformation is frequently denied, and their voices are kept mute. It impasses\nthe formation of an enabling environment, where activists and civil society\ninstitutions representing the ordinary peoples interest are actively\ndiscouraged. They become vulnerable to retribution. Social accountability, on\nthe other hand, provides an enabling environment for activists and civil\nsociety institutions to operate freely. Thus, leaders and administration become\nmore accountable to people. An enabling environment means providing legal\nprotection, enhancing the availability of information and increasing citizen\nvoice, strengthening institutional and public service capacities and directing\nincentives that foster accountability. Donors allocate significant shares of\nresources to encouraging civil society to partner with elites rather than\nholding them accountable. This paper advocate for a stronger legal environment\nto protect critical civil society and whistle-blowers, and for independent\ngrant-makers tasked with building strong, self-regulating social accountability\ninstitutions.\n  Key Words: Accountability, Legal Protection, Efficiency, Civil Society,\nResponsiveness\n"
    },
    {
        "paper_id": 2107.13148,
        "authors": "A. K. M. Amanat Ullah, Fahim Imtiaz, Miftah Uddin Md Ihsan, Md. Golam\n  Rabiul Alam, Mahbub Majumdar",
        "title": "Combining Machine Learning Classifiers for Stock Trading with Effective\n  Feature Extraction",
        "comments": null,
        "journal-ref": "Int. J. Computational Science and Engineering, Vol. 26 No.1,\n  (2023)",
        "doi": "10.1504/IJCSE.2023.129152",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The unpredictability and volatility of the stock market render it challenging\nto make a substantial profit using any generalised scheme. Many previous\nstudies tried different techniques to build a machine learning model, which can\nmake a significant profit in the US stock market by performing live trading.\nHowever, very few studies have focused on the importance of finding the best\nfeatures for a particular trading period. Our top approach used the performance\nto narrow down the features from a total of 148 to about 30. Furthermore, the\ntop 25 features were dynamically selected before each time training our machine\nlearning model. It uses ensemble learning with four classifiers: Gaussian Naive\nBayes, Decision Tree, Logistic Regression with L1 regularization, and\nStochastic Gradient Descent, to decide whether to go long or short on a\nparticular stock. Our best model performed daily trade between July 2011 and\nJanuary 2019, generating 54.35% profit. Finally, our work showcased that\nmixtures of weighted classifiers perform better than any individual predictor\nof making trading decisions in the stock market.\n"
    },
    {
        "paper_id": 2107.1338,
        "authors": "Martin Kittel, Wolf-Peter Schill",
        "title": "Renewable Energy Targets and Unintended Storage Cycling: Implications\n  for Energy Modeling",
        "comments": "Update of Table 1 and some minor edits. This is the version that is\n  also available at SSRN under http://dx.doi.org/10.2139/ssrn.3920668",
        "journal-ref": "iScience Volume 25, Issue 4, 15 April 2022, 104002",
        "doi": "10.1016/j.isci.2022.104002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To decarbonize the economy, many governments have set targets for the use of\nrenewable energy sources. These are often formulated as relative shares of\nelectricity demand or supply. Implementing respective constraints in energy\nmodels is a surprisingly delicate issue. They may cause a modeling artifact of\nexcessive electricity storage use. We introduce this phenomenon as 'unintended\nstorage cycling', which can be detected in case of simultaneous storage\ncharging and discharging. In this paper, we provide an analytical\nrepresentation of different approaches for implementing minimum renewable share\nconstraints in models, and show how these may lead to unintended storage\ncycling. Using a parsimonious optimization model, we quantify related\ndistortions of optimal dispatch and investment decisions as well as market\nprices, and identify important drivers of the phenomenon. Finally, we provide\nrecommendations on how to avoid the distorting effects of unintended storage\ncycling in energy modeling.\n"
    },
    {
        "paper_id": 2107.13441,
        "authors": "Klaus Bogenberger, Philipp Blum, Florian Dandl, Lisa-Sophie Hamm,\n  Allister Loder, Patrick Malcolm, Martin Margreiter and Natalie Sautter",
        "title": "MobilityCoins -- A new currency for the multimodal urban transportation\n  system",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The MobilityCoin is a new, all-encompassing currency for the management of\nthe multimodal urban transportation system. MobilityCoins includes and replaces\nvarious existing transport policy instruments while also incentivizing a shift\nto more sustainable modes as well as empowering the public to vote for\ninfrastructure measures.\n"
    },
    {
        "paper_id": 2107.13673,
        "authors": "Jaime D. Acevedo-Viloria, Luisa Roa, Soji Adeshina, Cesar Charalla\n  Olazo, Andr\\'es Rodr\\'iguez-Rey, Jose Alberto Ramos, Alejandro Correa-Bahnsen",
        "title": "Relational Graph Neural Networks for Fraud Detection in a Super-App\n  environment",
        "comments": "Accepted to be appeared in 2021 KDD Workshop on ML in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Large digital platforms create environments where different types of user\ninteractions are captured, these relationships offer a novel source of\ninformation for fraud detection problems. In this paper we propose a framework\nof relational graph convolutional networks methods for fraudulent behaviour\nprevention in the financial services of a Super-App. To this end, we apply the\nframework on different heterogeneous graphs of users, devices, and credit\ncards; and finally use an interpretability algorithm for graph neural networks\nto determine the most important relations to the classification task of the\nusers. Our results show that there is an added value when considering models\nthat take advantage of the alternative data of the Super-App and the\ninteractions found in their high connectivity, further proofing how they can\nleverage that into better decisions and fraud detection strategies.\n"
    },
    {
        "paper_id": 2107.13678,
        "authors": "Masud Alam",
        "title": "Heterogeneous Responses to the U.S. Narrative Tax Changes: Evidence from\n  the U.S. States",
        "comments": "69 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the assumption of homogeneous effects of federal tax\nchanges across the U.S. states and identifies where and why that assumption may\nnot be valid. More specifically, what determines the transmission mechanism of\ntax shocks at the state level? How vital are states' fiscal structures,\nfinancial conditions, labor market rigidities, and industry mix? Do these\neconomic and structural characteristics drive the transmission mechanism of the\ntax changes at the state level at different horizons? This study employs a\npanel factor-augmented vector autoregression (FAVAR) technique to answer these\nissues. The findings show that state economies respond homogeneously in terms\nof employment and price levels; however, they react heterogeneously in real GDP\nand personal income growth. In most states, these reactions are statistically\nsignificant, and the heterogeneity in the effects of tax cuts is significantly\nrelated to the state's fiscal structure, manufacturing and financial\ncomposition, and the labor market's rigidity. A cross-state regression analysis\nshows that states with higher tax elasticity, higher personal income tax,\nstrict labor market regulation, and economic policy uncertainties are\nrelatively less responsive to federal tax changes. In contrast, the magnitude\nof the response in real GDP, personal income, and employment to tax cuts is\nrelatively higher in states with a larger share of finance, manufacturing,\nlower tax burdens, and flexible credit markets.\n"
    },
    {
        "paper_id": 2107.13716,
        "authors": "Hossain Ahmed Taufiq",
        "title": "Role of NGOs in fostering equity and social inclusion in cities of\n  Bangladesh: The Case of Dhaka",
        "comments": "Proceedings: Fifth Annual International Conference on Sustainable\n  Development (ICSD), The Earth Institute, Columbia University, New York, USA",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Non-governmental organisations have made a significant contribution in the\ndevelopment of Bangladesh. Today, Bangladesh has more than 2000 NGOs, and few\nof them are among the largest in the world. NGOs are claimed to have impacts on\nthe sustainable development in Bangladesh. However, to what extent they have\nfostered equity and social inclusion in the urban cities of Bangladesh remains\na subject of thorough examination. The 11th goal of the Sustainable Development\nGoals (SDG) advocates for making cities and human settlements inclusive, safe,\nresilient and sustainable. Bangladesh which is the most densely populated\ncountry in the world faces multifaceted urbanization challenges. The capital\ncity Dhaka itself has experienced staggering population growth in last few\ndecades. Today, Dhaka has become one of the fastest growing megacities in the\nworld. Dhaka started its journey with a manageable population of 2.2 million in\n1975 which now reached 14.54 million. The growth rate averaged 6 per cent each\nyear. As this rapid growth of Dhaka City is not commensurate with its\nindustrial development, a significant portion of its population is living in\ninformal settlements or slums where they experience the highest level of\npoverty and vulnerability. Many NGOs have taken either concerted or individual\nefforts to address socio-economic challenges in the city. Earlier results\nsuggest that programs undertaken by NGOs have shown potential to positively\ncontribute to fostering equity and reducing social exclusion. This paper,\nattempts to explore what types of relevant NGO programs are currently in place\ntaking the case of Dhaka city.\n"
    },
    {
        "paper_id": 2107.13727,
        "authors": "Hossain Ahmed Taufiq",
        "title": "China, India, Myanmar: Playing Rohingya Roulette",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-981-13-7240-7_4",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The 2017 crackdown on Rakhine Rohingyas by the Myanmar army (Tatmadaw) pushed\nmore than 600,000 refugees into Bangladesh. Both Western and Islamic countries\ndenounced Aung Sang Suu Kyis government, but both Asian giants, China and\nIndia, supported Myanmars actions. Both also have high stakes in Myanmar given\ntheir long-term geopolitics and geoeconomic South and Southeast Asian plans. In\nspite of Myanmar-based commonalities, Chinas and Indias approaches differ\nsignificantly, predicting equally dissimilar outcomes. This chapter examines\ntheir foreign policy and stakes in Myanmar in order to draw a sketch of the\nfuture of Rakhine Rohingyas stuck in Bangladesh.\n"
    },
    {
        "paper_id": 2107.13737,
        "authors": "Dmitry Arkhangelsky, Guido W. Imbens, Lihua Lei, Xiaoman Luo",
        "title": "Design-Robust Two-Way-Fixed-Effects Regression For Panel Data",
        "comments": "131 pages; R package available at https://github.com/lihualei71/ripw;\n  replication files available at https://github.com/xiaomanluo/ripwPaper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new estimator for average causal effects of a binary treatment\nwith panel data in settings with general treatment patterns. Our approach\naugments the popular two-way-fixed-effects specification with unit-specific\nweights that arise from a model for the assignment mechanism. We show how to\nconstruct these weights in various settings, including the staggered adoption\nsetting, where units opt into the treatment sequentially but permanently. The\nresulting estimator converges to an average (over units and time) treatment\neffect under the correct specification of the assignment model, even if the\nfixed effect model is misspecified. We show that our estimator is more robust\nthan the conventional two-way estimator: it remains consistent if either the\nassignment mechanism or the two-way regression model is correctly specified. In\naddition, the proposed estimator performs better than the two-way-fixed-effect\nestimator if the outcome model and assignment mechanism are locally\nmisspecified. This strong double robustness property underlines and quantifies\nthe benefits of modeling the assignment process and motivates using our\nestimator in practice. We also discuss an extension of our estimator to handle\ndynamic treatment effects.\n"
    },
    {
        "paper_id": 2107.13866,
        "authors": "Thomas Conlon, John Cotter, Iason Kynigakis",
        "title": "Machine Learning and Factor-Based Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine machine learning and factor-based portfolio optimization. We find\nthat factors based on autoencoder neural networks exhibit a weaker relationship\nwith commonly used characteristic-sorted portfolios than popular dimensionality\nreduction techniques. Machine learning methods also lead to covariance and\nportfolio weight structures that diverge from simpler estimators.\nMinimum-variance portfolios using latent factors derived from autoencoders and\nsparse methods outperform simpler benchmarks in terms of risk minimization.\nThese effects are amplified for investors with an increased sensitivity to\nrisk-adjusted returns, during high volatility periods or when accounting for\ntail risk.\n"
    },
    {
        "paper_id": 2107.13926,
        "authors": "Nick James and Max Menzies",
        "title": "Collective correlations, dynamics, and behavioural inconsistencies of\n  the cryptocurrency market over time",
        "comments": "Final version. Moderate edits since v1. Equal contribution",
        "journal-ref": "Nonlinear Dynamics 107, 4001-4017 (2022)",
        "doi": "10.1007/s11071-021-07166-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces new methods to study behaviours among the 52 largest\ncryptocurrencies between 01-01-2019 and 30-06-2021. First, we explore\nevolutionary correlation behaviours and apply a recently proposed turning point\nalgorithm to identify regimes in market correlation. Next, we inspect the\nrelationship between collective dynamics and the cryptocurrency market size -\nrevealing an inverse relationship between the size of the market and the\nstrength of collective dynamics. We then explore the time-varying consistency\nof the relationships between cryptocurrencies' size and their returns and\nvolatility. There, we demonstrate that there is greater consistency between\nsize and volatility than size and returns. Finally, we study the spread of\nvolatility behaviours across the market changing with time by examining the\nstructure of Wasserstein distances between probability density functions of\nrolling volatility. We demonstrate a new phenomenon of increased uniformity in\nvolatility during market crashes, which we term \\emph{volatility dispersion}.\n"
    },
    {
        "paper_id": 2107.14026,
        "authors": "Han Lin Shang and Fearghal Kearney",
        "title": "Dynamic functional time-series forecasts of foreign exchange implied\n  volatility surfaces",
        "comments": "52 pages, 5 figures, to appear at the International Journal of\n  Forecasting",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents static and dynamic versions of univariate, multivariate,\nand multilevel functional time-series methods to forecast implied volatility\nsurfaces in foreign exchange markets. We find that dynamic functional principal\ncomponent analysis generally improves out-of-sample forecast accuracy. More\nspecifically, the dynamic univariate functional time-series method shows the\ngreatest improvement. Our models lead to multiple instances of statistically\nsignificant improvements in forecast accuracy for daily EUR-USD, EUR-GBP, and\nEUR-JPY implied volatility surfaces across various maturities, when benchmarked\nagainst established methods. A stylised trading strategy is also employed to\ndemonstrate the potential economic benefits of our proposed approach.\n"
    },
    {
        "paper_id": 2107.14033,
        "authors": "Chaoran Cui, Xiaojie Li, Juan Du, Chunyun Zhang, Xiushan Nie, Meng\n  Wang, Yilong Yin",
        "title": "Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend\n  Prediction",
        "comments": "Some revisons are performing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the future price trends of stocks is a challenging yet intriguing\nproblem given its critical role to help investors make profitable decisions. In\nthis paper, we present a collaborative temporal-relational modeling framework\nfor end-to-end stock trend prediction. The temporal dynamics of stocks is\nfirstly captured with an attention-based recurrent neural network. Then,\ndifferent from existing studies relying on the pairwise correlations between\nstocks, we argue that stocks are naturally connected as a collective group, and\nintroduce the hypergraph structures to jointly characterize the stock\ngroup-wise relationships of industry-belonging and fund-holding. A novel\nhypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph\nconvolutional networks with a hierarchical organization of intra-hyperedge,\ninter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN\nadaptively determines the importance of nodes, hyperedges, and hypergraphs\nduring the information propagation among stocks, so that the potential\nsynergies between stock movements can be fully exploited. Extensive experiments\non real-world data demonstrate the effectiveness of our approach. Also, the\nresults of investment simulation show that our approach can achieve a more\ndesirable risk-adjusted return. The data and codes of our work have been\nreleased at https://github.com/lixiaojieff/HGTAN.\n"
    },
    {
        "paper_id": 2107.14049,
        "authors": "Taiwo Adetiloye",
        "title": "Collaboration Planning of Stakeholders for Sustainable City Logistics\n  Operations",
        "comments": "https://spectrum.library.concordia.ca/973901/",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  City logistics involves movements of goods in urban areas respecting the\nmunicipal and administrative guidelines. The importance of city logistics is\ngrowing over the years especially with its role in minimizing traffic\ncongestion and freeing up of public space for city residents. Collaboration is\nkey to managing city logistics operations efficiently. Collaboration can take\nplace in the form of goods consolidation, sharing of resources, information\nsharing, etc. We investigate the problems of collaboration planning of\nstakeholders to achieve sustainable city logistics operations. Two categories\nof models are proposed to evaluate the collaboration strategies. At the macro\nlevel, we have the simplified collaboration square model and advance\ncollaboration square model and at the micro level we have the operational level\nmodel. These collaboration decision making models, with their mathematical\nelaborations on business-to-business, business-to-customer,\ncustomer-to-business, and customer-to-customer provide roadmaps for evaluating\nthe collaboration strategies of stakeholders for achieving sustainable city\nlogistics operations attainable under non-chaotic situation and presumptions of\nhuman levity tendency. City logistics stakeholders can strive to achieve\neffective collaboration strategies for sustainable city logistics operations by\nmitigating the uncertainty effect and understanding the theories behind the\nmoving nature of the individual complexities of a city. To investigate system\ncomplexity, we propose axioms of uncertainty and use spider networks and system\ndynamics modeling to investigate system elements and their behavior over time.\n"
    },
    {
        "paper_id": 2107.14052,
        "authors": "Susan von Struensee",
        "title": "The Role of Social Movements, Coalitions, and Workers in Resisting\n  Harmful Artificial Intelligence and Contributing to the Development of\n  Responsible AI",
        "comments": "184 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There is mounting public concern over the influence that AI based systems has\nin our society. Coalitions in all sectors are acting worldwide to resist hamful\napplications of AI. From indigenous people addressing the lack of reliable\ndata, to smart city stakeholders, to students protesting the academic\nrelationships with sex trafficker and MIT donor Jeffery Epstein, the\nquestionable ethics and values of those heavily investing in and profiting from\nAI are under global scrutiny. There are biased, wrongful, and disturbing\nassumptions embedded in AI algorithms that could get locked in without\nintervention. Our best human judgment is needed to contain AI's harmful impact.\nPerhaps one of the greatest contributions of AI will be to make us ultimately\nunderstand how important human wisdom truly is in life on earth.\n"
    },
    {
        "paper_id": 2107.14055,
        "authors": "Golnaz Shahtahmassebi and Lascelles Wright",
        "title": "Profit and loss manipulations by online trading brokers",
        "comments": "20 pages, 7 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Online trading has attracted millions of people around the world. In March\n2021, it was reported there were 18 million accounts from just one broker.\nHistorically, manipulation in financial markets is considered to be\nfraudulently influencing share, currency pairs or any other indices prices.\nThis article introduces the idea that online trading platform technical issues\ncan be considered as brokers manipulation to control traders profit and loss.\nMore importantly it shows these technical issues are the contributing factors\nof the 82% risk of retail traders losing money. We identify trading platform\ntechnical issues of one of the world's leading online trading providers and\ncalculate retail traders losses caused by these issues. To do this, we\nindependently record each trade details using the REST API response provided by\nthe broker. We show traders log activity files is the only way to assess any\nsuspected profit or loss manipulation by the broker. Therefore, it is essential\nfor any retail trader to have access to their log files. We compare our\nfindings with broker's Trustpilot customer reviews. We illustrate how traders'\nprofit and loss can be negatively affected by broker's platform technical\nissues such as not being able to close profitable trades, closing trades with\ndelays, disappearance of trades, disappearance of profit from clients\nstatements, profit and loss discrepancies, stop loss not being triggered, stop\nloss or limit order triggered too early. Although regulatory bodies try to\nensure that consumers get a fair deal, these attempts are hugely insufficient\nin protecting retail traders. Therefore, regulatory bodies such as the FCA\nshould take these technical issues seriously and not rely on brokers' internal\ninvestigations, because under any other circumstances, these platform\nmanipulations would be considered as crimes and connivingly misappropriating\nfunds.\n"
    },
    {
        "paper_id": 2107.14092,
        "authors": "Yunze Li, Yanan Xie, Chen Yu, Fangxing Yu, Bo Jiang and Matloob Khushi",
        "title": "Feature importance recap and stacking models for forex price prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Forex trading is the largest market in terms of qutantitative trading.\nTraditionally, traders refer to technical analysis based on the historical data\nto make decisions and trade. With the development of artificial intelligent,\ndeep learning plays a more and more important role in forex forecasting. How to\nuse deep learning models to predict future price is the primary purpose of most\nresearchers. Such prediction not only helps investors and traders make\ndecisions, but also can be used for auto-trading system. In this article, we\nhave proposed a novel approach of feature selection called 'feature importance\nrecap' which combines the feature importance score from tree-based model with\nthe performance of deep learning model. A stacking model is also developed to\nfurther improve the performance. Our results shows that proper feature\nselection approach could significantly improve the model performance, and for\nfinancial data, some features have high importance score in many models. The\nresults of stacking model indicate that combining the predictions of some\nmodels and feed into a neural network can further improve the performance.\n"
    },
    {
        "paper_id": 2107.14109,
        "authors": "Suleyman Yukcu, Omer Aydin",
        "title": "Digital Twin As A Cost Reduction Method",
        "comments": "Digital Twin, Cost Accounting, Cyber Physical Systems, Artificial\n  Intelligence, Internet of Things, Sensor",
        "journal-ref": "Muhasebe Bilim Dunyas{\\i} Dergisi , 22 (3) , 563-579",
        "doi": "10.31460/mbdd.694571",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Many fields have been affected by the introduction of concepts such as\nsensors, industry 4.0, internet of things, machine learning and artificial\nintelligence in recent years. As a result of the interaction of cyber physical\nsystems with these concepts, digital twin model has emerged. The concept of\ndigital twin has been used in many areas with its emergence. The use of this\nmodel has made significant gains, especially in decision making processes. The\ngains in decision making processes contribute to every field and cause changes\nin terms of cost. In this study, the historical development of the concept of\ndigital twin has been mentioned and general information about the usage areas\nof digital twin has been given. In the light of this information, the cost\neffect of the digital twin model, therefore its appearance from the cost\naccounting window and its use as a cost reduction method were evaluated. This\nstudy was carried out in order to shed light on the studies with the\ninsufficient resources in the Turkish literature and the cost accounting\nperspective.\n"
    },
    {
        "paper_id": 2107.14113,
        "authors": "Francesca Biagini, Lukas Gonon, Thomas Reitsam",
        "title": "Neural network approximation for superhedging prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article examines neural network-based approximations for the\nsuperhedging price process of a contingent claim in a discrete time market\nmodel. First we prove that the $\\alpha$-quantile hedging price converges to the\nsuperhedging price at time $0$ for $\\alpha$ tending to $1$, and show that the\n$\\alpha$-quantile hedging price can be approximated by a neural network-based\nprice. This provides a neural network-based approximation for the superhedging\nprice at time $0$ and also the superhedging strategy up to maturity. To obtain\nthe superhedging price process for $t>0$, by using the Doob decomposition it is\nsufficient to determine the process of consumption. We show that it can be\napproximated by the essential supremum over a set of neural networks. Finally,\nwe present numerical results.\n"
    },
    {
        "paper_id": 2107.14343,
        "authors": "Edoh Y. Amiran and Joni S. James Charles",
        "title": "Reconciling revealed and stated measures for willingness to pay in\n  recreation by building a probability model",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The consumers' willingness to pay plays an important role in economic theory\nand in setting policy. For a market, this function can often be estimated from\nobserved behavior -- preferences are revealed. However, economists would like\nto measure consumers' willingness to pay for some goods where this can only be\nmeasured through stated valuation. Confirmed convergence of valuations based on\nstated preferences as compared to valuations based on revealed preferences is\nrare, and it is important to establish circumstances under which one can expect\nsuch convergence. By building a simple probabilistic model for the consumers'\nlikelihood of travel, we provide an approach that should make comparing stated\nand revealed preferences easier in cases where the preference is tied to travel\nor some other behavior whose cost can be measured. We implemented this approach\nin a pilot study and found an estimate of willingness to pay for visiting an\nenvironmentally enhanced recreational site based on actual travel in good\nagreement with an estimate based on a survey using stated preferences. To use\nthe probabilistic model we used population statistics to adjust for the\nrelevant duration and thus compare stated and revealed responses.\n"
    },
    {
        "paper_id": 2107.1435,
        "authors": "Egor Malkov",
        "title": "Spousal Occupational Sorting and COVID-19 Incidence: Evidence from the\n  United States",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How do matching of spouses and the nature of work jointly shape the\ndistribution of COVID-19 health risks? To address this question, I study the\nassociation between the incidence of COVID-19 and the degree of spousal sorting\ninto occupations that differ by contact intensity at the workplace. The\nmechanism, that I explore, implies that the higher degree of positive spousal\nsorting mitigates intra-household contagion and this translates into a smaller\nnumber of individuals exposed to COVID-19 risk. Using the U.S. data at the\nstate level, I argue that spousal sorting is an important factor for\nunderstanding the disparities in the prevalence of COVID-19 during the early\nstages of the pandemic. First, I document that it creates about two-thirds of\nthe U.S. dual-earner couples that are exposed to higher COVID-19 health risk\ndue to within-household transmission. Moreover, I uncover substantial\nheterogeneity in the degree of spousal sorting by state. Next, for the first\nweek of April 2020, I estimate that a one standard deviation increase in the\nmeasure of spousal sorting is associated with a 30% reduction in the total\nnumber of cases per 100000 inhabitants and a 39.3% decline in the total number\nof deaths per 100000 inhabitants. Furthermore, I find substantial temporal\nheterogeneity as the coefficients decline in magnitude over time. My results\nspeak to the importance of policies that allow mitigating intra-household\ncontagion.\n"
    },
    {
        "paper_id": 2107.14365,
        "authors": "Dominik Hartmann, Diogo Ferraz, Mayra Bezerra, Andreas Pyka, Flavio L.\n  Pinheiro",
        "title": "Comparing cars with apples? Identifying the appropriate benchmark\n  countries for relative ecological pollution rankings and international\n  learning",
        "comments": "35 pages, 5 Figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Research in Data Envelopment Analysis has created rankings of the ecological\nefficiency of countries' economies. At the same time, research in economic\ncomplexity has provided new methods to depict productive structures and has\nanalyzed how economic diversification and sophistication affect environmental\npollution indicators. However, no research so far has compared the ecological\nefficiency of countries with similar productive structures and levels of\neconomic complexity, combining the strengths of both approaches. In this\narticle, we use data on 774 different types of exports, CO2 emissions, and the\necological footprint of 99 countries to create a relative ecological pollution\nranking (REPR). Moreover, we use methods from network science to reveal a\nbenchmark network of the best learning partners based on country pairs with a\nlarge extent of export similarity, yet significant differences in pollution\nvalues. This is important because it helps to reveal adequate benchmark\ncountries for efficiency improvements and cleaner production, considering that\ncountries may specialize in substantially different types of economic\nactivities. Finally, the article (i) illustrates large efficiency improvements\nwithin current global output levels, (ii) helps to identify countries that can\nbest learn from each other, and (iii) improves the information base in\ninternational negotiations for the sake of a clean global production system.\n"
    },
    {
        "paper_id": 2107.1441,
        "authors": "Liao Zhu",
        "title": "The Adaptive Multi-Factor Model and the Financial Market",
        "comments": "PhD thesis",
        "journal-ref": "eCommons 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern evolvements of the technologies have been leading to a profound\ninfluence on the financial market. The introduction of constituents like\nExchange-Traded Funds, and the wide-use of advanced technologies such as\nalgorithmic trading, results in a boom of the data which provides more\nopportunities to reveal deeper insights. However, traditional statistical\nmethods always suffer from the high-dimensional, high-correlation, and\ntime-varying instinct of the financial data. In this dissertation, we focus on\ndeveloping techniques to stress these difficulties. With the proposed\nmethodologies, we can have more interpretable models, clearer explanations, and\nbetter predictions.\n"
    },
    {
        "paper_id": 2107.14554,
        "authors": "Roberto Antonietti, Paolo Falbo, Fulvio Fontini, Rosanna Grassi,\n  Giorgio Rizzini",
        "title": "International Trade Network: Country centrality and COVID-19 pandemic",
        "comments": null,
        "journal-ref": "Applied Network Science 7 (2022)",
        "doi": "10.1007/s41109-022-00452-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  International trade is based on a set of complex relationships between\ndifferent countries that can be modelled as an extremely dense network of\ninterconnected agents. On the one hand, this network might favour the economic\ngrowth of countries, but on the other, it can also favour the diffusion of\ndiseases, like the COVID-19. In this paper, we study whether, and to what\nextent, the topology of the trade network can explain the rate of COVID-19\ndiffusion and mortality across countries. We compute the countries' centrality\nmeasures and we apply the community detection methodology based on\ncommunicability distance. Then, we use these measures as focal regressors in a\nnegative binomial regression framework. In doing so, we also compare the effect\nof different measures of centrality. Our results show that the number of\ninfections and fatalities are larger in countries with a higher centrality in\nthe global trade network.\n"
    },
    {
        "paper_id": 2107.14678,
        "authors": "Carlos Castro-Iragorri, Julian Ramirez and Sebastian Velez",
        "title": "Financial intermediation and risk in decentralized lending protocols",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide an overview of decentralized protocols like Compound and Aave that\noffer collateralized loans for cryptoasset investors. Compound and Aave are two\nof the most important application in the decentralized finance (DeFi)\necosystem. Using publicly available information on rates, supply and borrow\nactivity, and accounts we analyze different elements of the protocols. In\nparticular, we estimate ex-post margins that give a comprehensive account of\nthe cost of financial intermediation. We find that ex-post margins considering\nall markets are 1% and lower for stablecoin markets. In addition, we estimate\nquarterly indicators regarding solvency, asset quality, earnings and market\nrisk similar to the ones used in traditional banking. This provides a first\nlook at the use of these metrics and a comparison between the similarities and\nchallenges to our understanding of financial intermediation in these protocols\nbased on tools used for traditional banking.\n"
    },
    {
        "paper_id": 2107.14695,
        "authors": "Shubham Ekapure, Nuruddin Jiruwala, Sohan Patnaik, Indranil SenGupta",
        "title": "A data-science-driven short-term analysis of Amazon, Apple, Google, and\n  Microsoft stocks",
        "comments": "19 pages, 10 figures, 7 tables. [This is a REU paper]",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we implement a combination of technical analysis and\nmachine/deep learning-based analysis to build a trend classification model. The\ngoal of the paper is to apprehend short-term market movement, and incorporate\nit to improve the underlying stochastic model. Also, the analysis presented in\nthis paper can be implemented in a \\emph{model-independent} fashion. We execute\na data-science-driven technique that makes short-term forecasts dependent on\nthe price trends of current stock market data. Based on the analysis, three\ndifferent labels are generated for a data set: $+1$ (buy signal), $0$ (hold\nsignal), or $-1$ (sell signal). We propose a detailed analysis of four major\nstocks- Amazon, Apple, Google, and Microsoft. We implement various technical\nindicators to label the data set according to the trend and train various\nmodels for trend estimation. Statistical analysis of the outputs and\nclassification results are obtained.\n"
    },
    {
        "paper_id": 2108.00234,
        "authors": "Julia Eisenberg, Stefan Kremsner, Alexander Steinicke",
        "title": "Two Approaches for a Dividend Maximization Problem under an\n  Ornstein-Uhlenbeck Interest Rate",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a dividend maximization problem under stochastic interest\nrates with Ornstein-Uhlenbeck dynamics. This setup also takes negative rates\ninto account. First a deterministic time is considered, where an explicit\nseparating curve $\\alpha(t)$ can be found to determine the optimal strategy at\ntime $t$. In a second setting we introduce a strategy-independent stopping\ntime. The properties and behavior of these optimal control problems in both\nsettings are analyzed in an analytical HJB-driven approach as well as using\nbackward stochastic differential equations.\n"
    },
    {
        "paper_id": 2108.00242,
        "authors": "Jean-Philippe Bouchaud",
        "title": "The Inelastic Market Hypothesis: A Microstructural Interpretation",
        "comments": "Version to appear in Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We attempt to reconcile Gabaix and Koijen's (GK) recent Inelastic Market\nHypothesis (IMH) with the order-driven view of markets that emerged within the\nmicrostructure literature in the past 20 years. We review the most salient\nempirical facts and arguments that give credence to the idea that market price\nfluctuations are mostly due to order flow, whether informed or non-informed. We\nshow that the Latent Liquidity Theory of price impact makes a precise\nprediction for GK's multiplier $M$, which measures by how many dollars, on\naverage, the market value of a company goes up if one buys one dollar worth of\nits stocks. Our central result is that $M$ is of order unity, as found by GK,\nand increases with the volatility of the stock and decreases with the fraction\nof the market cap. traded daily. We discuss several empirical results\nsuggesting that the lion's share of volatility is due to trading activity. We\nargue that the IMH holds for all asset classes, beyond the case of stock\nmarkets considered by GK.\n"
    },
    {
        "paper_id": 2108.0048,
        "authors": "Eghbal Rahimikia, Stefan Zohren, Ser-Huang Poon",
        "title": "Realised Volatility Forecasting: Machine Learning via Financial Word\n  Embedding",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3895272",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study develops FinText, a financial word embedding compiled from 15\nyears of business news archives. The results show that FinText produces\nsubstantially more accurate results than general word embeddings based on the\ngold-standard financial benchmark we introduced. In contrast to well-known\neconometric models, and over the sample period from 27 July 2007 to 27 January\n2022 for 23 NASDAQ stocks, using stock-related news, our simple natural\nlanguage processing model supported by different word embeddings improves\nrealised volatility forecasts on high volatility days. This improvement in\nrealised volatility forecasting performance switches to normal volatility days\nwhen general hot news is used. By utilising SHAP, an Explainable AI method, we\nalso identify and classify key phrases in stock-related and general hot news\nthat moved volatility.\n"
    },
    {
        "paper_id": 2108.00519,
        "authors": "Mark S. Manger and J. Scott Matthews",
        "title": "Knowing When to Splurge: Precautionary Saving and Chinese-Canadians",
        "comments": "Paper presented at the Asian Political Methodology Conference, Kyoto,\n  November 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Why do household saving rates differ so much across countries? This\nmicro-level question has global implications: countries that systematically\n\"oversave\" export capital by running current account surpluses. In the\nrecipient countries, interest rates are thus too low and financial stability is\nput at risk. Existing theories argue that saving is precautionary, but tests\nare limited to cross-country comparisons and are not always supportive. We\nreport the findings of an original survey experiment. Using a simulated\nfinancial saving task implemented online, we compare the saving preferences of\na large and diverse sample of Chinese-Canadians with other Canadians. This\ncomparison is instructive given that Chinese-Canadians migrated from, or\ndescend from those who migrated from, a high-saving environment to a\nlow-savings, high-debt environment. We also compare behavior in the presence\nand absence of a simulated \"welfare state,\" which we represent in the form of\nmandatory insurance. Our respondents exhibit behavior in the saving task that\ncorresponds to standard economic assumptions about lifecycle savings and risk\naversion. We find strong evidence that precautionary saving is reduced when a\nmandatory insurance is present, but no sign that Chinese cultural influences -\nrepresented in linguistic or ethnic terms - have any effect on saving behavior.\n"
    },
    {
        "paper_id": 2108.00664,
        "authors": "Victor Storchan, Svitlana Vyetrenko, Tucker Balch",
        "title": "Learning who is in the market from time series: market participant\n  discovery through adversarial calibration of multi-agent simulators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In electronic trading markets often only the price or volume time series,\nthat result from interaction of multiple market participants, are directly\nobservable. In order to test trading strategies before deploying them to\nreal-time trading, multi-agent market environments calibrated so that the time\nseries that result from interaction of simulated agents resemble historical are\noften used. To ensure adequate testing, one must test trading strategies in a\nvariety of market scenarios -- which includes both scenarios that represent\nordinary market days as well as stressed markets (most recently observed due to\nthe beginning of COVID pandemic). In this paper, we address the problem of\nmulti-agent simulator parameter calibration to allow simulator capture\ncharacteristics of different market regimes. We propose a novel two-step method\nto train a discriminator that is able to distinguish between \"real\" and \"fake\"\nprice and volume time series as a part of GAN with self-attention, and then\nutilize it within an optimization framework to tune parameters of a simulator\nmodel with known agent archetypes to represent a market scenario. We conclude\nwith experimental results that demonstrate effectiveness of our method.\n"
    },
    {
        "paper_id": 2108.00799,
        "authors": "Lijun Bo, Shihua Wang, Xiang Yu",
        "title": "Mean Field Game of Optimal Relative Investment with Jump Risk",
        "comments": "Final version, forthcoming in Science China Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the n-player game and the mean field game under the CRRA\nrelative performance on terminal wealth, in which the interaction occurs by\npeer competition. In the model with n agents, the price dynamics of underlying\nrisky assets depend on a common noise and contagious jump risk modelled by a\nmulti-dimensional nonlinear Hawkes process. With a continuum of agents, we\nformulate the MFG problem and characterize a deterministic mean field\nequilibrium in an analytical form under some conditions, allowing us to\ninvestigate some impacts of model parameters in the limiting model and discuss\nsome financial implications. Moreover, based on the mean field equilibrium, we\nconstruct an approximate Nash equilibrium for the n-player game when n is\nsufficiently large. The explicit order of the approximation error is also\nderived.\n"
    },
    {
        "paper_id": 2108.00814,
        "authors": "Dimitrios Exadaktylos, Mahdi Ghodsi and Armando Rungi",
        "title": "What do Firms Gain from Patenting? The Case of the Global ICT Industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the causal relationship between patent grants and\nfirms' dynamics in the Information and Communication Technology (ICT) industry,\nas the latter is a peculiar sector of modern economies, often under the lens of\nantitrust authorities. For our purpose, we exploit matched information about\nfinancial accounts and patenting activity in 2009-2017 by 179,660 companies\noperating in 39 countries. Preliminarily, we show how bigger companies are less\nthan 2% of the sample, although they concentrate about 89% of the grants\nobtained in the period of analyses. Thus, we test that patent grants in the ICT\nindustry have a significant and large impact on market shares and firm size of\nsmaller companies (31.5% and 30.7%, respectively) in the first year after the\ngrants, while we have no evidence of an impact for bigger companies. After a\nnovel instrumental variable strategy that exploits information at the level of\npatent offices, we confirm that most of the effects on smaller companies are\ndue to the protection of property rights and not to the innovative content of\ninventions. Finally, we never observe a significant impact on either\nprofitability or productivity for any firm size category. Eventually, we\ndiscuss how our findings support the idea that the ICT industry is a case of\nendogenous R&D sunk costs, which prevent profit margins from rising in the\npresence of a relatively high market concentration.\n"
    },
    {
        "paper_id": 2108.00848,
        "authors": "Fatih Ozhamaratli (1), Oleg Kitov (2), Paolo Barucca (1) ((1)\n  University College London, (2) University of Cambridge)",
        "title": "A generative model for age and income distribution",
        "comments": null,
        "journal-ref": "EPJ Data Science (2022) 11:4",
        "doi": "10.1140/epjds/s13688-022-00317-x",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Each individual in society experiences an evolution of their income during\ntheir lifetime. Macroscopically, this dynamics creates a statistical\nrelationship between age and income for each society. In this study, we\ninvestigate income distribution and its relationship with age and identify a\nstable joint distribution function for age and income within the United Kingdom\nand the United States. We demonstrate a flexible calibration methodology using\npanel and population surveys and capture the characteristic differences between\nthe UK and the US populations. The model here presented can be utilised for\nforecasting income and planning pensions.\n"
    },
    {
        "paper_id": 2108.0085,
        "authors": "Luca J. Santos, Alessandro V. M. Oliveira, Dante Mendes Aldrighi",
        "title": "Testing the differentiated impact of the COVID-19 pandemic on air travel\n  demand considering social inclusion",
        "comments": "30 pages",
        "journal-ref": "Journal of Air Transport Management 94 (2021)",
        "doi": "10.1016/j.jairtraman.2021.102082",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The economic downturn and the air travel crisis triggered by the recent\ncoronavirus pandemic pose a substantial threat to the new consumer class of\nmany emerging economies. In Brazil, considerable improvements in social\ninclusion have fostered the emergence of hundreds of thousands of first-time\nfliers over the past decades. We apply a two-step regression methodology in\nwhich the first step consists of identifying air transport markets\ncharacterized by greater social inclusion, using indicators of the local\neconomies' income distribution, credit availability, and access to the\nInternet. In the second step, we inspect the drivers of the plunge in air\ntravel demand since the pandemic began, differentiating markets by their\npredicted social inclusion intensity. After controlling for potential\nendogeneity stemming from the spread of COVID-19 through air travel, our\nresults suggest that short and low-density routes are among the most impacted\nairline markets and that business-oriented routes are more impacted than\nleisure ones. Finally, we estimate that a market with 1 per cent higher social\ninclusion is associated with a 0.153 per cent to 0.166 per cent more pronounced\ndecline in demand during the pandemic. Therefore, markets that have benefited\nfrom greater social inclusion in the country may be the most vulnerable to the\ncurrent crisis.\n"
    },
    {
        "paper_id": 2108.00867,
        "authors": "Cl\\'oves Gon\\c{c}alves Rodrigues",
        "title": "Overview of the global semiconductor industry market",
        "comments": "in Portuguese",
        "journal-ref": "Brazilian Journal of Development, v. 7, n. 7, pp. 74936-74944,\n  2021",
        "doi": "10.34117/bjdv7n7-597",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mastering semiconductor technology is essential to insert any country into\nthe trends of the future, such as smart cities, internet of things, space\nexploration, etc. In this paper we present the growing annual revenue of the\nsemiconductor industry in the last 20 years and comment on the importance of\nmastering semiconductor production technology and its implications for the\ndevelopment of a nation.\n"
    },
    {
        "paper_id": 2108.00926,
        "authors": "Mohammad Rafiqul Islam, Masud Alam, Munshi Naser \\.Ibne Afzal, Sakila\n  Alam",
        "title": "Nighttime Light Intensity and Child Health Outcomes in Bangladesh",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study examines the impact of nighttime light intensity on child health\noutcomes in Bangladesh. We use nighttime light intensity as a proxy measure of\nurbanization and argue that the higher intensity of nighttime light, the higher\nis the degree of urbanization, which positively affects child health outcomes.\nIn econometric estimation, we employ a methodology that combines parametric and\nnon-parametric approaches using the Gradient Boosting Machine (GBM), K-Nearest\nNeighbors (KNN), and Bootstrap Aggregating that originate from machine learning\nalgorithms. Based on our benchmark estimates, findings show that one standard\ndeviation increase of nighttime light intensity is associated with a 1.515 rise\nof Z-score of weight for age after controlling for several control variables.\nThe maximum increase of weight for height and height for age score range from\n5.35 to 7.18 units. To further understand our benchmark estimates, generalized\nadditive models also provide a robust positive relationship between nighttime\nlight intensity and children's health outcomes. Finally, we develop an economic\nmodel that supports the empirical findings of this study that the marginal\neffect of urbanization on children's nutritional outcomes is strictly positive.\n"
    },
    {
        "paper_id": 2108.00973,
        "authors": "Jin Hyuk Choi and Kim Weston",
        "title": "Endogenous noise trackers in a Radner equilibrium",
        "comments": "To appear in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the existence of an incomplete Radner equilibrium in a model with\nexponential investors and an endogenous noise tracker. We analyze a coupled\nsystem of ODEs and reduce it to a system of two coupled ODEs in order to\nestablish equilibrium existence. As an application, we study the impact of the\nendogenous noise tracker on welfare by comparing to a model with an exogenous\nnoise trader. We show that the aggregate welfare in the endogenous noise\ntracker model is bigger for a sufficiently large stock supply, but the welfare\ncomparison depends in a non-trivial manner on the other model parameters.\n"
    },
    {
        "paper_id": 2108.01026,
        "authors": "Santiago Camara",
        "title": "US Spillovers of US Monetary Policy: Information effects & Financial\n  Flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper quantifies the international spillovers of US interest rates by\nexplicitly controlling for the \"Fed Information Effect\". I use multiple\nidentification strategies that identify two components of monetary policy\nsurprises around FOMC meetings: a pure US monetary policy shock component and a\n\"Fed Information Effect\" component. On the one hand, a US tightening caused by\na pure US monetary policy component leads to an economic recession, an exchange\nrate depreciation and tighter financial conditions. On the other hand, a\ntightening of US monetary policy caused by the \"Fed Information Effect\" leads\nto an economic expansion, an exchange rate appreciation and looser financial\nconditions. Ignoring the \"Fed Information Effect\" biases the impact of US\ninterest rates and may explain recent atypical findings which suggest an\nexpansionary impact of US monetary policy shocks on the rest of the world.\n"
    },
    {
        "paper_id": 2108.01243,
        "authors": "Budhi Arta Surya",
        "title": "Some results on maximum likelihood from incomplete data: finite sample\n  properties and improved M-estimator for resampling",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents some results on the maximum likelihood (ML) estimation\nfrom incomplete data. Finite sample properties of conditional observed\ninformation matrices are established. They possess positive definiteness and\nthe same Loewner partial ordering as the expected information matrices do. An\nexplicit form of the observed Fisher information (OFI) is derived for the\ncalculation of standard errors of the ML estimates. It simplifies Louis (1982)\ngeneral formula for the OFI matrix. To prevent from getting an incorrect\ninverse of the OFI matrix, which may be attributed by the lack of sparsity and\nlarge size of the matrix, a monotone convergent recursive equation for the\ninverse matrix is developed which in turn generalizes the algorithm of Hero and\nFessler (1994) for the Cram\\'er-Rao lower bound. To improve the estimation, in\nparticular when applying repeated sampling to incomplete data, a robust\nM-estimator is introduced. A closed form sandwich estimator of covariance\nmatrix is proposed to provide the standard errors of the M-estimator. By the\nresulting loss of information presented in finite-sample incomplete data, the\nsandwich estimator produces smaller standard errors for the M-estimator than\nthe ML estimates. In the case of complete information or absence of\nre-sampling, the M-estimator coincides with the ML estimates. Application to\nparameter estimation of a regime switching conditional Markov jump process is\ndiscussed to verify the results. The simulation study confirms the accuracy and\nasymptotic properties of the M-estimator.\n"
    },
    {
        "paper_id": 2108.01615,
        "authors": "Maher Said, Emma R. Zajdela and Amanda Stathopoulos",
        "title": "Accelerating the Adoption of Disruptive Technologies: The Impact of\n  COVID-19 on Intentions to Use Autonomous Vehicles",
        "comments": "Accepted at Transportation Research Board 2022 for presentation (2nd\n  revision)",
        "journal-ref": null,
        "doi": "10.1177/03611981221099276",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the most notable global transportation trends is the accelerated pace\nof development in vehicle automation technologies. Uncertainty surrounds the\nfuture of automated mobility as there is no clear consensus on potential\nadoption patterns, ownership versus shared use status and travel impacts.\nAdding to this uncertainty is the impact of the COVID-19 pandemic that has\ntriggered profound changes in mobility behaviors as well as accelerated\nadoption of new technologies at an unprecedented rate. Accordingly, this study\nexamines the impact of the COVID-19 pandemic on willingness to adopt the\nemerging technology of autonomous vehicles (AVs). Using data from a survey\ndisseminated in June 2020 to 700 respondents in the United States, we perform a\ndifference-in-difference regression to analyze the shift in willingness to use\nautonomous vehicles as part of a shared fleet before and during the pandemic.\nThe results reveal that the COVID-19 pandemic has a positive and highly\nsignificant impact on the consideration of using autonomous vehicles. This\nshift is present regardless of tech-savviness, gender or urban/rural household\nlocation. Individuals who are younger, left-leaning and frequent users of\nshared modes of travel are expected to become more likely to use autonomous\nvehicles once offered. Understanding the effects of these attributes on the\nincrease in consideration of AVs is important for policy making, as these\neffects provide a guide to predicting adoption of autonomous vehicles - once\navailable - and to identify segments of the population likely to be more\nresistant to adopting AVs.\n"
    },
    {
        "paper_id": 2108.01617,
        "authors": "Piergiorgio Alessandri, Haroon Mumtaz",
        "title": "The macroeconomic cost of climate volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the impact of climate volatility on economic growth exploiting data\non 133 countries between 1960 and 2019. We show that the conditional (ex ante)\nvolatility of annual temperatures increased steadily over time, rendering\nclimate conditions less predictable across countries, with important\nimplications for growth. Controlling for concomitant changes in temperatures, a\n+1 degree C increase in temperature volatility causes on average a 0.3 percent\ndecline in GDP growth and a 0.7 percent increase in the volatility of GDP.\nUnlike changes in average temperatures, changes in temperature volatility\naffect both rich and poor countries.\n"
    },
    {
        "paper_id": 2108.0172,
        "authors": "Elliott Ash, Germain Gauthier, Philine Widmer",
        "title": "RELATIO: Text Semantics Capture Political and Economic Narratives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social scientists have become increasingly interested in how narratives --\nthe stories in fiction, politics, and life -- shape beliefs, behavior, and\ngovernment policies. This paper provides an unsupervised method to quantify\nlatent narrative structures in text documents. Our new software package RELATIO\nidentifies coherent entity groups and maps explicit relations between them in\nthe text. We provide an application to the United States Congressional Record\nto analyze political and economic narratives in recent decades. Our analysis\nhighlights the dynamics, sentiment, polarization, and interconnectedness of\nnarratives in political discourse.\n"
    },
    {
        "paper_id": 2108.01758,
        "authors": "Zhaolu Dong, Shan Huang, Simiao Ma, Yining Qian",
        "title": "Factor Representation and Decision Making in Stock Markets Using Deep\n  Reinforcement Learning",
        "comments": "finance conference workshop paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep Reinforcement learning is a branch of unsupervised learning in which an\nagent learns to act based on environment state in order to maximize its total\nreward. Deep reinforcement learning provides good opportunity to model the\ncomplexity of portfolio choice in high-dimensional and data-driven environment\nby leveraging the powerful representation of deep neural networks. In this\npaper, we build a portfolio management system using direct deep reinforcement\nlearning to make optimal portfolio choice periodically among S\\&P500 underlying\nstocks by learning a good factor representation (as input). The result shows\nthat an effective learning of market conditions and optimal portfolio\nallocations can significantly outperform the average market.\n"
    },
    {
        "paper_id": 2108.0176,
        "authors": "Asif Lakhany, Andrej Pintar, Amber Zhang",
        "title": "Calibrating the Nelson-Siegel-Svensson Model by Genetic Algorithm",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurately fitting the term structure of interest rates is critical to\ncentral banks and other market participants. The Nelson-Siegel and\nNelson-Siegel-Svensson models are probably the best-known models for this\npurpose due to their intuitive appeal and simple representation. However, this\nsimplicity comes at a price. The difficulty in calibrating these models is\ntwofold. Firstly, the objective function being minimized during the calibration\nprocedure is nonlinear and has multiple local optima. Secondly, there is strong\nco-dependence among the model parameters. As a result, their estimated values\nbehave erratically over time. To avoid these problems, we apply a heuristic\noptimization method, specifically the Genetic Algorithm approach, and show that\nit is able to construct reliable interest rate curves and stable model\nparameters over time, regardless of the shape of the curves.\n"
    },
    {
        "paper_id": 2108.018,
        "authors": "Wenyuan Wang, Xiang Yu, Xiaowen Zhou",
        "title": "On optimality of barrier dividend control under endogenous regime\n  switching with application to Chapter 11 bankruptcy",
        "comments": "Final version, forthcoming in Applied Mathematics & Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by recent developments in risk management based on the U.S.\nbankruptcy code, we revisit the De Finetti's optimal dividend problem by\nincorporating the reorganization process and regulator's intervention\ndocumented in Chapter 11 bankruptcy. The resulting surplus process, bearing\nfinancial stress towards the more subtle concept of bankruptcy, corresponds to\na non-standard spectrally negative Levy process with endogenous regime\nswitching. Some explicit expressions of the expected present values under a\nbarrier strategy, new to the literature, are established in terms of scale\nfunctions. With the help of these expressions, when the tail of the Levy\nmeasure is log-convex, the optimal dividend control is shown to be of the\nbarrier type and the associated optimal barrier can be identified using scale\nfunctions of spectrally negative Levy processes. Some financial implications\nare also discussed in an illustrative example.\n"
    },
    {
        "paper_id": 2108.01881,
        "authors": "Karol Binkowski, Peilun He, Nino Kordzakhia, Pavel Shevchenko",
        "title": "On the Parameter Estimation in the Schwartz-Smiths Two-Factor Model",
        "comments": null,
        "journal-ref": "Binkowski K., He P., Kordzakhia N., Shevchenko P. (2019) On the\n  Parameter Estimation in the Schwartz-Smiths Two-Factor Model. Communications\n  in Computer and Information Science, vol 1150",
        "doi": "10.1007/978-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The two unobservable state variables representing the short and long term\nfactors introduced by Schwartz and Smith in [16] for risk-neutral pricing of\nfutures contracts are modelled as two correlated Ornstein-Uhlenbeck processes.\nThe Kalman Filter (KF) method has been implemented to estimate the short and\nlong term factors jointly with un- known model parameters. The parameter\nidentification problem arising within the likelihood function in the KF has\nbeen addressed by introduc- ing an additional constraint. The obtained model\nparameter estimates are the conditional Maximum Likelihood Estimators (MLEs)\nevaluated within the KF. Consistency of the conditional MLEs is studied. The\nmethodology has been tested on simulated data.\n"
    },
    {
        "paper_id": 2108.01886,
        "authors": "Peilun He, Karol Binkowski, Nino Kordzakhia, Pavel Shevchenko",
        "title": "On Modelling of Crude Oil Futures in a Bivariate State-Space Framework",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-030-78965-7_40",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a bivariate latent factor model for the pricing of commodity fu-\ntures. The two unobservable state variables representing the short and long\nterm fac- tors are modelled as Ornstein-Uhlenbeck (OU) processes. The Kalman\nFilter (KF) algorithm has been implemented to estimate the unobservable factors\nas well as unknown model parameters. The estimates of model parameters were\nobtained by maximising a Gaussian likelihood function. The algorithm has been\napplied to WTI Crude Oil NYMEX futures data.\n"
    },
    {
        "paper_id": 2108.01999,
        "authors": "Jan Matas and Jan Posp\\'i\\v{s}il",
        "title": "On simulation of rough Volterra stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough Volterra volatility models are a progressive and promising field of\nresearch in derivative pricing. Although rough fractional stochastic volatility\nmodels already proved to be superior in real market data fitting, techniques\nused in simulation of these models are still inefficient in terms of speed and\naccuracy. This paper aims to present accurate and efficient tools and\ntechniques for Monte-Carlo simulations for a wide range of rough volatility\nmodels. In particular, we compare three commonly used simulation methods: the\nCholesky method, the Hybrid scheme, and the rDonsker scheme. We also comment on\nthe implementation of variance reduction techniques. In particular, we show the\nobstacles of the so-called turbocharging technique whose performance is\nsometimes counter-productive. To overcome these obstacles, we suggest several\nmodifications.\n"
    },
    {
        "paper_id": 2108.02272,
        "authors": "Shuowen Chen and Yang Ming",
        "title": "R&D Heterogeneity and Countercyclical Productivity Dispersion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Why is the U.S. industry-level productivity dispersion countercyclical?\nTheoretically, we build a duopoly model in which heterogeneous R&D costs\ndetermine firms' optimal behaviors and the equilibrium technology gap after a\nnegative profit shock. Quantitatively, we calibrate a parameterized model,\nsimulate firms' post--shock responses and predict that productivity dispersion\nis due to the low-cost firm increasing R&D efforts and the high-cost firm doing\nthe opposite. Empirically, we construct an index of negative profit shocks and\nprovide two reduced-form tests for this mechanism.\n"
    },
    {
        "paper_id": 2108.02283,
        "authors": "Yang Bai and Kuntara Pukthuanthong",
        "title": "Machine Learning Classification Methods and Portfolio Allocation: An\n  Examination of Market Efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We design a novel framework to examine market efficiency through\nout-of-sample (OOS) predictability. We frame the asset pricing problem as a\nmachine learning classification problem and construct classification models to\npredict return states. The prediction-based portfolios beat the market with\nsignificant OOS economic gains. We measure prediction accuracies directly. For\neach model, we introduce a novel application of binomial test to test the\naccuracy of 3.34 million return state predictions. The tests show that our\nmodels can extract useful contents from historical information to predict\nfuture return states. We provide unique economic insights about OOS\npredictability and machine learning models.\n"
    },
    {
        "paper_id": 2108.02419,
        "authors": "Dave Cliff and James Hawkins and James Keen and Roberto Lau-Soto",
        "title": "Implementing the BBE Agent-Based Model of a Sports-Betting Exchange",
        "comments": "11 pages, 4 figures, 37 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe three independent implementations of a new agent-based model\n(ABM) that simulates a contemporary sports-betting exchange, such as those\noffered commercially by companies including Betfair, Smarkets, and Betdaq. The\nmotivation for constructing this ABM, which is known as the Bristol Betting\nExchange (BBE), is so that it can serve as a synthetic data generator,\nproducing large volumes of data that can be used to develop and test new\nbetting strategies via advanced data analytics and machine learning techniques.\nBetting exchanges act as online platforms on which bettors can find willing\ncounterparties to a bet, and they do this in a way that is directly comparable\nto the manner in which electronic financial exchanges, such as major stock\nmarkets, act as platforms that allow traders to find willing counterparties to\nbuy from or sell to: the platform aggregates and anonymises orders from\nmultiple participants, showing a summary of the market that is updated in\nreal-time. In the first instance, BBE is aimed primarily at producing synthetic\ndata for in-play betting (also known as in-race or in-game betting) where\nbettors can place bets on the outcome of a track-race event, such as a horse\nrace, after the race has started and for as long as the race is underway, with\nbetting only ceasing when the race ends. The rationale for, and design of, BBE\nhas been described in detail in a previous paper that we summarise here, before\ndiscussing our comparative results which contrast a single-threaded\nimplementation in Python, a multi-threaded implementation in Python, and an\nimplementation where Python header-code calls simulations of the track-racing\nevents written in OpenCL that execute on a 640-core GPU -- this runs\napproximately 1000 times faster than the single-threaded Python. Our\nsource-code for BBE is freely available on GitHub.\n"
    },
    {
        "paper_id": 2108.02442,
        "authors": "Mahdieh Yazdani",
        "title": "House Price Determinants and Market Segmentation in Boulder, Colorado: A\n  Hedonic Price Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this research we perform hedonic regression model to examine the\nresidential property price determinants in the city of Boulder in the state of\nColorado, USA. The urban housing markets are too compounded to be considered as\nhomogeneous markets. The heterogeneity of an urban property market requires\ncreation of market segmentation. To test whether residential properties in the\nreal estate market in the city of Boulder are analyzed and predicted in the\ndisaggregate level or at an aggregate level we stratify the housing market\nbased on both property types and location and estimate separate hedonic price\nmodels for each submarket. The results indicate that the implicit values of the\nproperty characteristics are not identical across property types and locations\nin the city of Boulder and market segmentation exists.\n"
    },
    {
        "paper_id": 2108.02447,
        "authors": "Michele Azzone and Roberto Baviera",
        "title": "Short-time implied volatility of additive normal tempered stable\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Empirical studies have emphasized that the equity implied volatility is\ncharacterized by a negative skew inversely proportional to the square root of\nthe time-to-maturity. We examine the short-time-to-maturity behavior of the\nimplied volatility smile for pure jump exponential additive processes. An\nexcellent calibration of the equity volatility surfaces has been achieved by a\nclass of these additive processes with power-law scaling. The two power-law\nscaling parameters are $\\beta$, related to the variance of jumps, and $\\delta$,\nrelated to the smile asymmetry. It has been observed, in option market data,\nthat $\\beta=1$ and $\\delta=-1/2$. In this paper, we prove that the implied\nvolatility of these additive processes is consistent, in the short-time, with\nthe equity market empirical characteristics if and only if $\\beta=1$ and\n$\\delta=-1/2$.\n"
    },
    {
        "paper_id": 2108.02506,
        "authors": "Rytis Kazakevicius, Aleksejus Kononovicius, Bronislovas Kaulakys,\n  Vygintas Gontis",
        "title": "Understanding the nature of the long-range memory phenomenon in\n  socioeconomic systems",
        "comments": "29 pages, 9 figures, 190 references",
        "journal-ref": "Entropy 23: 1125 (2021)",
        "doi": "10.3390/e23091125",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the face of the upcoming 30th anniversary of econophysics, we review our\ncontributions and other related works on the modeling of the long-range memory\nphenomenon in physical, economic, and other social complex systems. Our group\nhas shown that the long-range memory phenomenon can be reproduced using various\nMarkov processes, such as point processes, stochastic differential equations\nand agent-based models. Reproduced well enough to match other statistical\nproperties of the financial markets, such as return and trading activity\ndistributions and first-passage time distributions. Research has lead us to\nquestion whether the observed long-range memory is a result of actual\nlong-range memory process or just a consequence of non-linearity of Markov\nprocesses. As our most recent result we discuss the long-range memory of the\norder flow data in the financial markets and other social systems from the\nperspective of the fractional L\\`{e}vy stable motion. We test widely used\nlong-range memory estimators on discrete fractional L\\`{e}vy stable motion\nrepresented by the ARFIMA sample series. Our newly obtained results seem\nindicate that new estimators of self-similarity and long-range memory for\nanalyzing systems with non-Gaussian distributions have to be developed.\n"
    },
    {
        "paper_id": 2108.02633,
        "authors": "Spiridon Penev, Pavel V. Shevchenko, Wei Wu",
        "title": "The impact of model risk on dynamic portfolio selection under\n  multi-period mean-standard-deviation criterion",
        "comments": null,
        "journal-ref": "European Journal of Operational Research 273 (2019), pp. 772-784",
        "doi": "10.1016/j.ejor.2018.08.026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify model risk of a financial portfolio whereby a multi-period\nmean-standard-deviation criterion is used as a selection criterion. In this\nwork, model risk is defined as the loss due to uncertainty of the underlying\ndistribution of the returns of the assets in the portfolio. The uncertainty is\nmeasured by the Kullback-Leibler divergence, i.e., the relative entropy. In the\nworst case scenario, the optimal robust strategy can be obtained in a\nsemi-analytical form as a solution of a system of nonlinear equations. Several\nnumerical results are presented which allow us to compare the performance of\nthis robust strategy with the optimal non-robust strategy. For illustration, we\nalso quantify the model risk associated with an empirical dataset.\n"
    },
    {
        "paper_id": 2108.02648,
        "authors": "Xun Li, Xiang Yu, Qinyi Zhang",
        "title": "Optimal consumption with loss aversion and reference to past spending\n  maximum",
        "comments": "Final version, forthcoming in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal consumption problem for a loss-averse agent\nwith reference to past consumption maximum. To account for loss aversion on\nrelative consumption, an S-shaped utility is adopted that measures the\ndifference between the non-negative consumption rate and a fraction of the\nhistorical spending peak. We consider the concave envelope of the utility with\nrespect to consumption, allowing us to focus on an auxiliary HJB variational\ninequality on the strength of concavification principle and dynamic programming\narguments. By applying the dual transform and smooth-fit conditions, the\nauxiliary HJB variational inequality is solved in piecewise closed-form and\nsome thresholds of the wealth variable are obtained. The optimal consumption\nand investment control can be derived in the piecewise feedback form. The\nrigorous verification proofs on optimality and concavification principle are\nprovided. Some numerical sensitivity analysis and financial implications are\nalso presented.\n"
    },
    {
        "paper_id": 2108.02755,
        "authors": "Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes,\n  Richard Socher",
        "title": "The AI Economist: Optimal Economic Policy Design via Two-level Deep\n  Reinforcement Learning",
        "comments": "Substantial Extension of arXiv:2004.13332. SZ and AT contributed\n  equally",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  AI and reinforcement learning (RL) have improved many areas, but are not yet\nwidely adopted in economic policy design, mechanism design, or economics at\nlarge. At the same time, current economic methodology is limited by a lack of\ncounterfactual data, simplistic behavioral models, and limited opportunities to\nexperiment with policies and evaluate behavioral responses. Here we show that\nmachine-learning-based economic simulation is a powerful policy and mechanism\ndesign framework to overcome these limitations. The AI Economist is a\ntwo-level, deep RL framework that trains both agents and a social planner who\nco-adapt, providing a tractable solution to the highly unstable and novel\ntwo-level RL challenge. From a simple specification of an economy, we learn\nrational agent behaviors that adapt to learned planner policies and vice versa.\nWe demonstrate the efficacy of the AI Economist on the problem of optimal\ntaxation. In simple one-step economies, the AI Economist recovers the optimal\ntax policy of economic theory. In complex, dynamic economies, the AI Economist\nsubstantially improves both utilitarian social welfare and the trade-off\nbetween equality and productivity over baselines. It does so despite emergent\ntax-gaming strategies, while accounting for agent interactions and behavioral\nchange more accurately than economic theory. These results demonstrate for the\nfirst time that two-level, deep RL can be used for understanding and as a\ncomplement to theory for economic design, unlocking a new computational\nlearning-based approach to understanding economic policy.\n"
    },
    {
        "paper_id": 2108.02838,
        "authors": "Tugce Karatas, Ali Hirsa",
        "title": "Two-Stage Sector Rotation Methodology Using Machine Learning and Deep\n  Learning Techniques",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Market indicators such as CPI and GDP have been widely used over decades to\nidentify the stage of business cycles and also investment attractiveness of\nsectors given market conditions. In this paper, we propose a two-stage\nmethodology that consists of predicting ETF prices for each sector using market\nindicators and ranking sectors based on their predicted rate of returns. We\ninitially start with choosing sector specific macroeconomic indicators and\nimplement Recursive Feature Elimination algorithm to select the most important\nfeatures for each sector. Using our prediction tool, we implement different\nRecurrent Neural Networks models to predict the future ETF prices for each\nsector. We then rank the sectors based on their predicted rate of returns. We\nselect the best performing model by evaluating the annualized return,\nannualized Sharpe ratio, and Calmar ratio of the portfolios that includes the\ntop four ranked sectors chosen by the model. We also test the robustness of the\nmodel performance with respect to lookback windows and look ahead windows. Our\nempirical results show that our methodology beats the equally weighted\nportfolio performance even in the long run. We also find that Echo State\nNetworks exhibits an outstanding performance compared to other models yet it is\nfaster to implement compared to other RNN models.\n"
    },
    {
        "paper_id": 2108.02853,
        "authors": "Tugce Karatas, Federico Klinkert, Ali Hirsa",
        "title": "Supervised Neural Networks for Illiquid Alternative Asset Cash Flow\n  Forecasting",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Institutional investors have been increasing the allocation of the illiquid\nalternative assets such as private equity funds in their portfolios, yet there\nexists a very limited literature on cash flow forecasting of illiquid\nalternative assets. The net cash flow of private equity funds typically follow\na J-curve pattern, however the timing and the size of the contributions and\ndistributions depend on the investment opportunities. In this paper, we develop\na benchmark model and present two novel approaches (direct vs. indirect) to\npredict the cash flows of private equity funds. We introduce a sliding window\napproach to apply on our cash flow data because different vintage year funds\ncontain different lengths of cash flow information. We then pass the data to an\nLSTM/ GRU model to predict the future cash flows either directly or indirectly\n(based on the benchmark model). We further integrate macroeconomic indicators\ninto our data, which allows us to consider the impact of market environment on\ncash flows and to apply stress testing. Our results indicate that the direct\nmodel is easier to implement compared to the benchmark model and the indirect\nmodel, but still the predicted cash flows align better with the actual cash\nflows. We also show that macroeconomic variables improve the performance of the\ndirect model whereas the impact is not obvious on the indirect model.\n"
    },
    {
        "paper_id": 2108.02904,
        "authors": "Alexander Trott, Sunil Srinivasa, Douwe van der Wal, Sebastien\n  Haneuse, Stephan Zheng",
        "title": "Building a Foundation for Data-Driven, Interpretable, and Robust Policy\n  Design using the AI Economist",
        "comments": "41 pages, 14 figures. AT, SS, and SZ contributed equally",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimizing economic and public policy is critical to address socioeconomic\nissues and trade-offs, e.g., improving equality, productivity, or wellness, and\nposes a complex mechanism design problem. A policy designer needs to consider\nmultiple objectives, policy levers, and behavioral responses from strategic\nactors who optimize for their individual objectives. Moreover, real-world\npolicies should be explainable and robust to simulation-to-reality gaps, e.g.,\ndue to calibration issues. Existing approaches are often limited to a narrow\nset of policy levers or objectives that are hard to measure, do not yield\nexplicit optimal policies, or do not consider strategic behavior, for example.\nHence, it remains challenging to optimize policy in real-world scenarios. Here\nwe show that the AI Economist framework enables effective, flexible, and\ninterpretable policy design using two-level reinforcement learning (RL) and\ndata-driven simulations. We validate our framework on optimizing the stringency\nof US state policies and Federal subsidies during a pandemic, e.g., COVID-19,\nusing a simulation fitted to real data. We find that log-linear policies\ntrained using RL significantly improve social welfare, based on both public\nhealth and economic outcomes, compared to past outcomes. Their behavior can be\nexplained, e.g., well-performing policies respond strongly to changes in\nrecovery and vaccination rates. They are also robust to calibration errors,\ne.g., infection rates that are over or underestimated. As of yet, real-world\npolicymaking has not seen adoption of machine learning methods at large,\nincluding RL and AI-driven simulations. Our results show the potential of AI to\nguide policy design and improve social welfare amidst the complexity of the\nreal world.\n"
    },
    {
        "paper_id": 2108.02925,
        "authors": "Okechukwu Christopher Onuegbu",
        "title": "Effectiveness of Anambra Broadcasting Service (ABS) Radio News on\n  Teaching and Learning (a case study of Awka based Students)",
        "comments": "22 pages, 5 tables, articile",
        "journal-ref": "International Journal of Communication and Media Science, 2021",
        "doi": "10.14445/2349641X/IJCMS-V8I2P103",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work sought to find out the effectiveness of Anambra Broadcasting\nService (ABS) Radio news on teaching and learning. The study focused mainly on\nlisteners of ABS radio news broadcast in Awka, the capital of Anambra State,\nNigeria. Its objectives were to find out; if Awka based students are exposed to\nABS radio; to discover the ABS radio program students favorite; the need\ngratification that drives students to listen to ABS radio news; the\ncontributions of radio news to students teaching and learning; and\neffectiveness of ABS radio news on teaching and learning in Awka. The\npopulation of Awka students is 198,868. This is also the population of the\nstudy. But a sample size of 400 was chosen and administered with\nquestionnaires. The study was hinged on the uses and gratification theory. It\nadopted a survey research design. The data gathered was analyzed using simple\npercentages and frequency of tables. The study revealed that news is very\neffective in teaching and learning. It was concluded that news is the best\ninstructional media to be employed in teaching and learning. Among other\nthings, it was recommended that teachers and students should listen to and make\njudicious use of news for academic purposes.\n"
    },
    {
        "paper_id": 2108.02956,
        "authors": "George Abuselidze and Mariam Msakhuradze",
        "title": "Features of international taxation and its impact on business entities\n  of Georgia",
        "comments": "in Georgian language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The work \"International Taxation and its impact on Georgian Business\nSubjects\" discusses the essence, types of international taxation and ways to\nprevent it. Object of international taxation, taxable base and rates, features\nbased on the taxpayer. The approaches of states and its impact on the\nactivities of business entities. The aim of the work was to study the\ntheoretical and methodological bases of international taxation in the tax\nsystem of Georgia and to present the existing problems. To get acquainted with\nthe activities of the free industrial zones in our country and to evaluate\nthem. Sharing opinions and expressing one's attitude towards it. The work\npresents the opinion on the impact of the approaches and recommendations of our\ncountry's legislation on international taxation on the business sector of\nGeorgia to correct the current situation.\n"
    },
    {
        "paper_id": 2108.03027,
        "authors": "George Abuselidze and Rusudan Zoidze",
        "title": "Specifics of formation tax revenues and ways to improve it in Georgia",
        "comments": "in Georgian language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the research there is reviewed the peculiarities of the formation of tax\nrevenues of the state budget, analysis of the recent past and present periods\nof tax system in Georgia, there is reviewed the influence of existing factors\non the revenues, as well as the role and the place of direct and indirect taxes\nin the state budget revenues. In addition, the measures of stimulating action\non formation of tax revenues and their impact on the state budget revenues are\nestablished. At the final stage, there are examples of foreign developed\ncountries, where the tax system is perfectly developed, where various\nstimulating measures are successfully stimulating and consequently it promotes\nmobilization of the amount of money required in the state budget. The exchange\nof foreign experience is very important for Georgia, the existing tax model\nthat is based on foreign experience is greatly successful. For the formation of\ntax policy, it is necessary to take into consideration all the factors\naffecting on it, a complex analysis of the tax system and the steps that will\nbe really useful and perspective for our country.\n"
    },
    {
        "paper_id": 2108.03092,
        "authors": "Thomas Bouquet, Mehdi Hmyene, Fran\\c{c}ois Porcher, Lorenzo Pugliese,\n  Jad Zeroual",
        "title": "Approximating Optimal Asset Allocations using Simulated Bifurcation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper investigates the application of Simulated Bifurcation algorithms\nto approximate optimal asset allocations. It will provide the reader with an\nexplanation of the physical principles underlying the method and a Python\nimplementation of the latter applied to 441 assets belonging to the S&P500\nindex. In addition, the paper tackles the problem of the selection of an\noptimal sub-allocation; in this particular case, we find an adequate solution\nin an unrivaled timescale.\n"
    },
    {
        "paper_id": 2108.0311,
        "authors": "Shuhei Aoki",
        "title": "A Pomeranzian Growth Theory of the Great Divergence",
        "comments": "23 pages, 3 figures, 2 tables; the introduction part revised",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study constructs a growth model of the Great Divergence that formalizes\nPomeranz's (2000) hypothesis that the relief of land constraints in Europe has\ncaused divergence in economic growth between Europe and China since the 19th\ncentury. The model consists of the agricultural and manufacturing sectors. The\nagricultural sector produces subsistence goods from land, intermediate goods\nfrom the manufacturing sector, and labor. The manufacturing sector produces\ngoods from labor, and its productivity grows through the learning-by-doing of\nfull-time manufacturing workers. Households make fertility decisions. In the\nmodel, a large exogenous positive shock in land supply causes the transition of\nthe economy from the Malthusian state, in which all workers are engaged in\nagricultural production and per capita income is constant, to the\nnon-Malthusian state, in which the share of workers engaged in agricultural\nproduction gradually decreases and per capita income grows at a roughly\nconstant growth rate. The quantitative predictions of the model provide several\ninsights into the causes of the Great Divergence.\n"
    },
    {
        "paper_id": 2108.03389,
        "authors": "Dominic Joseph",
        "title": "Predicting Credit Default Probabilities Using Bayesian Statistics and\n  Monte Carlo Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Banks and financial institutions all over the world manage portfolios\ncontaining tens of thousands of customers. Not all customers are high\ncredit-worthy, and many possess varying degrees of risk to the Bank or\nfinancial institutions that lend money to these customers. Hence assessment of\ncredit risk is paramount in the field of credit risk management. This paper\ndiscusses the use of Bayesian principles and simulation-techniques to estimate\nand calibrate the default probability of credit ratings. The methodology is a\ntwo-phase approach where, in the first phase, a posterior density of default\nrate parameter is estimated based the default history data. In the second phase\nof the approach, an estimate of true default rate parameter is obtained through\nsimulations\n"
    },
    {
        "paper_id": 2108.03709,
        "authors": "Alex Garivaltis",
        "title": "Grade Inflation and Stunted Effort in a Curved Economics Course",
        "comments": "64 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To protect his teaching evaluations, an economics professor uses the\nfollowing exam curve: if the class average falls below a known target, $m$,\nthen all students will receive an equal number of free points so as to bring\nthe mean up to $m$. If the average is above $m$ then there is no curve; curved\ngrades above $100\\%$ will never be truncated to $100\\%$ in the gradebook. The\n$n$ students in the course all have Cobb-Douglas preferences over the\ngrade-leisure plane; effort corresponds exactly to earned (uncurved) grades in\na $1:1$ fashion. The elasticity of each student's utility with respect to his\ngrade is his ability parameter, or relative preference for a high score. I\nfind, classify, and give complete formulas for all the pure Nash equilibria of\nmy own game, which my students have been playing for some eight semesters. The\ngame is supermodular, featuring strategic complementarities, negative\nspillovers, and nonsmooth payoffs that generate non-convexities in the reaction\ncorrespondence. The $n+2$ types of equilibria are totally ordered with respect\nto effort and Pareto preference, and the lowest $n+1$ of these types are\ntotally ordered in grade-leisure space. In addition to the no-curve\n(\"try-hard\") and curved interior equilibria, we have the \"$k$-don't care\"\nequilibria, whereby the $k$ lowest-ability students are no-shows. As the class\nsize becomes infinite in the curved interior equilibrium, all students increase\ntheir leisure time by a fixed percentage, i.e., $14\\%$, in response to the\ndisincentive, which amplifies any pre-existing ability differences. All\nstudents' grades inflate by this same (endogenous) factor, say, $1.14$ times\nwhat they would have been under the correct standard.\n"
    },
    {
        "paper_id": 2108.03722,
        "authors": "Kerstin H\\\"otte, Su Jung Jee",
        "title": "Knowledge for a warmer world: a patent analysis of climate change\n  adaptation technologies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Technologies can help strengthen the resilience of our economy against\nexistential climate-risks. We investigate climate change adaptation\ntechnologies (CCATs) in US patents to understand (1) historical patterns and\ndrivers of innovation; (2) scientific and technological requirements to develop\nand use CCATs; and (3) CCATs' potential technological synergies with\nmitigation. First, in contrast to mitigation, innovation in CCATs only slowly\ntakes off, indicating a relatively low awareness of investors for solutions to\ncope with climate risks. Historical trends in environmental regulation, energy\nprices, and public support can be associated with patenting in CCATs. Second,\nCCATs form two main clusters: science-intensive ones in agriculture, health,\nand monitoring technologies; and engineering-intensive ones in coastal, water,\nand infrastructure technologies. Analyses of technology-specific scientific and\ntechnological knowledge bases inform directions for how to facilitate\nadvancement, transfer and use of CCATs. Lastly, CCATs show strong technological\ncomplementarities with mitigation as more than 25% of CCATs bear mitigation\nbenefits. While not judging about the complementarity of mitigation and\nadaptation in general, our results suggest how policymakers can harness these\ntechnological synergies to achieve both goals simultaneously.\n"
    },
    {
        "paper_id": 2108.03733,
        "authors": "Sang Truong and Humberto Barreto",
        "title": "Visualizing Income Distribution in the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The distribution of household income is a central concern of modern economic\npolicy due to its strong influence on life quality. Yet, non-expert audiences\nare unaware of the relationship between these two factors. To effectively\ncommunicate the effect of income inequality on the quality of life and among\nthe strata, we have designed a novel technique for visualizing income\ndistribution and inequality over time by using the U.S. household income\nmicrodata from the Current Population Survey. The result is a striking dynamic\nanimation of income distribution over time, drawing public attention and\nfurther investigating economic inequality. Detailed implementation of this\nproject is available at https://github.com/sangttruong/incomevis.\n"
    },
    {
        "paper_id": 2108.03912,
        "authors": "Elumalai Kannan and Sanjib Pohit",
        "title": "Agricultural Growth Diagnostics: Identifying the Binding Constraints and\n  Policy Remedies for Bihar, India",
        "comments": "23 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Agriculture plays a significant role in economic development of the\nunderdeveloped region. Multiple factors influence the performance of\nagricultural sector but a few of these have a strong bearing on its growth. We\ndevelop a growth diagnostics framework for agricultural sector in Bihar located\nin eastern India to identify the most binding constraints. Our results show\nthat poor functioning of agricultural markets and low level of crop\ndiversification are the important reasons for lower agricultural growth in\nBihar. Rise in the level of instability in the prices of agricultural produces\nindicates a weak price transmission across the markets even after repealing the\nagricultural produce market committee act. Poor market linkages and\nnon-functioning producer collectives at village level affect the farmers\nmotivation for undertaking crop diversification. Our policy suggestions include\nstate provision of basic market infrastructure to attract private investment in\nagricultural marketing, strengthening the farmer producer organisations, and a\ncomprehensive policy on crop diversification.\n"
    },
    {
        "paper_id": 2108.04019,
        "authors": "Sakae Oya and Teruo Nakatsuma",
        "title": "Identification in Bayesian Estimation of the Skewness Matrix in a\n  Multivariate Skew-Elliptical Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Harvey et al. (2010) extended the Bayesian estimation method by Sahu et al.\n(2003) to a multivariate skew-elliptical distribution with a general skewness\nmatrix, and applied it to Bayesian portfolio optimization with higher moments.\nAlthough their method is epochal in the sense that it can handle the skewness\ndependency among asset returns and incorporate higher moments into portfolio\noptimization, it cannot identify all elements in the skewness matrix due to\nlabel switching in the Gibbs sampler. To deal with this identification issue,\nwe propose to modify their sampling algorithm by imposing a positive\nlower-triangular constraint on the skewness matrix of the multivariate skew-\nelliptical distribution and improved interpretability. Furthermore, we propose\na Bayesian sparse estimation of the skewness matrix with the horseshoe prior to\nfurther improve the accuracy. In the simulation study, we demonstrate that the\nproposed method with the identification constraint can successfully estimate\nthe true structure of the skewness dependency while the existing method suffers\nfrom the identification issue.\n"
    },
    {
        "paper_id": 2108.04047,
        "authors": "Francesca Biagini, Andrea Mazzon, Katharina Oberpriller",
        "title": "Reduced-form framework for multiple ordered default times under model\n  uncertainty",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a sublinear conditional operator with respect to a\nfamily of possibly nondominated probability measures in presence of multiple\nordered default times. In this way we generalize the results of [5], where a\nreduced-form framework under model uncertainty for a single default time is\ndeveloped. Moreover, we use this operator for the valuation of credit portfolio\nderivatives under model uncertainty.\n"
    },
    {
        "paper_id": 2108.04198,
        "authors": "Jules Linden, Cathal O'Donoghue, Denisa M. Sologon",
        "title": "The Structure and Incentives of a COVID related Emergency Wage Subsidy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  During recent crisis, wage subsidies played a major role in sheltering firms\nand households from economic shocks. During COVID-19, most workers were\naffected and many liberal welfare states introduced new temporary wage\nsubsidies to protected workers' earnings and employment (OECD, 2021). New wage\nsubsidies marked a departure from the structure of traditional income support\npayments and required reform. This paper uses simulated datasets to assess the\nstructure and incentives of the Irish COVID-19 wage subsidy scheme (CWS) under\nfive designs. We use a nowcasting approach to update 2017 microdata, producing\na near real time picture of the labour market at the peak of the crisis. Using\nmicrosimulation modelling, we assess the impact of different designs on income\nreplacement, work incentives and income inequality. Our findings suggest that\npro rata designs support middle earners more and flat rate designs support low\nearners more. We find evidence for strong work disincentives under all designs,\nthough flat rate designs perform better. Disincentives are primarily driven by\ngenerous unemployment payments and work related costs. The impact of design on\nincome inequality depends on the generosity of payments. Earnings related pro\nrata designs were associated to higher market earnings inequality. The\ndifference in inequality levels falls once benefits, taxes and work related\ncosts are considered. In our discussion, we turn to transaction costs, the\nrationale for reform and reintegration of CWS. We find some support for the\nclaim that design changes were motivated by political considerations. We\nsuggest that establishing permanent wage subsidies based on sectorial turnover\nrules could offer enhanced protection to middle-and high-earners and reduce\nuncertainty, the need for reform, and the risk of politically motivated\ndesigns.\n"
    },
    {
        "paper_id": 2108.04291,
        "authors": "Peter Bank and Yan Dolinsky and Mikl\\'os R\\'asonyi",
        "title": "What if we knew what the future brings? Optimal investment for a\n  frontrunner with price impact",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study optimal investment when the investor can peek some\ntime units into the future, but cannot fully take advantage of this knowledge\nbecause of quadratic transaction costs. In the Bachelier setting with\nexponential utility, we give an explicit solution to this control problem with\nintrinsically infinite-dimensional memory. This is made possible by solving the\ndual problem where we make use of the theory of Gaussian Volterra integral\nequations.\n"
    },
    {
        "paper_id": 2108.04464,
        "authors": "Yichun Chi, Zuo Quan Xu, Sheng Chao Zhuang",
        "title": "Distributionally robust goal-reaching optimization in the presence of\n  background risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/10920277.2021.1966805",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we examine the effect of background risk on portfolio\nselection and optimal reinsurance design under the criterion of maximizing the\nprobability of reaching a goal. Following the literature, we adopt dependence\nuncertainty to model the dependence ambiguity between financial risk (or\ninsurable risk) and background risk. Because the goal-reaching objective\nfunction is non-concave, these two problems bring highly unconventional and\nchallenging issues for which classical optimization techniques often fail.\nUsing quantile formulation method, we derive the optimal solutions explicitly.\nThe results show that the presence of background risk does not alter the shape\nof the solution but instead changes the parameter value of the solution.\nFinally, numerical examples are given to illustrate the results and verify the\nrobustness of our solutions.\n"
    },
    {
        "paper_id": 2108.04739,
        "authors": "George Abuselidze",
        "title": "About inevitability of budgetary code receiving for fiscal politics",
        "comments": "in Georgian language",
        "journal-ref": "Economics and Business, 1, pp. 42-46 (2009)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Since the end of 90s till today when all the elements confirming Georgian\nState System have practically been established, budget system and policy\nremains as the most difficult Georgian macroeconomics challenge and even still\nhalf-and-half unsolved problem. One side of the fiscal policy is quite\ncrucially formulated and administrative Tax Code, and the other side is the\nweak, unmanaged and incomplete law on Budget System. According to the\nabove-mentioned the elaboration and adoption of the Budget Code having equal\nforce as Tax Code is necessary by which the following are to be determined:\nexcellence of government responsibility when it will not perform the budget\nobligations specified by the law permanently; the rights and responsibilities\nof the state, the optimal distribution of the funds mobilized by the tax\ntowards each member of the society. For optimization of the budget system\neffective correlation between the state, regional and local budgets revenues\nand expenditures is particularly important as the social-economic development\nof the regions and territorial units of the country is impossible without the\nfinancial relations. For it the just differentiation of tax base in the section\nof state, regional and local budgets and transfers system for support of the\nbudgets of the territorial units from the central budget are necessary. Solving\nthe most part of these problems is possible by the adoption of the budget code\nwhich, in our opinion, is to be considered as the closest decisive task for the\ncurrent legislative and executive authority.\n"
    },
    {
        "paper_id": 2108.04885,
        "authors": "Davi B. Costa",
        "title": "Benefits of marriage as a search strategy",
        "comments": "34 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose and investigate a model for mate searching and marriage in large\nsocieties based on a stochastic matching process and simple decision rules.\nAgents have preferences among themselves given by some probability\ndistribution. They randomly search for better mates, forming new couples and\nbreaking apart in the process. Marriage is implemented in the model by adding\nthe decision of stopping searching for a better mate when the affinity between\na couple is higher than a certain fixed amount. We show that the average\nutility in the system with marriage can be higher than in the system without\nit. Part of our results can be summarized in what sounds like a piece of\nadvice: don't marry the first person you like and don't search for the love of\nyour life, but get married if you like your partner more than a sigma above\naverage. We also find that the average utility attained in our stochastic model\nis smaller than the one associated with a stable matching achieved using the\nGale-Shapley algorithm. This can be taken as a formal argument in favor of a\ncentral planner (perhaps an app) with the information to coordinate the\nmarriage market in order to set a stable matching. To roughly test the adequacy\nof our model to describe existent societies, we compare the evolution of the\nfraction of married couples in our model with real-world data and obtain good\nagreement. In the last section, we formulate the model in the limit of an\ninfinite number of agents and find an analytical expression for the evolution\nof the system.\n"
    },
    {
        "paper_id": 2108.04941,
        "authors": "Brian Ning, Sebastian Jaimungal, Xiaorong Zhang, Maxime Bergeron",
        "title": "Arbitrage-Free Implied Volatility Surface Generation with Variational\n  Autoencoders",
        "comments": "20 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We propose a hybrid method for generating arbitrage-free implied volatility\n(IV) surfaces consistent with historical data by combining model-free\nVariational Autoencoders (VAEs) with continuous time stochastic differential\nequation (SDE) driven models. We focus on two classes of SDE models: regime\nswitching models and L\\'evy additive processes. By projecting historical\nsurfaces onto the space of SDE model parameters, we obtain a distribution on\nthe parameter subspace faithful to the data on which we then train a VAE.\nArbitrage-free IV surfaces are then generated by sampling from the posterior\ndistribution on the latent space, decoding to obtain SDE model parameters, and\nfinally mapping those parameters to IV surfaces. We further refine the VAE\nmodel by including conditional features and demonstrate its superior generative\nout-of-sample performance.\n"
    },
    {
        "paper_id": 2108.05048,
        "authors": "Christian Bayer, Simon Breneis",
        "title": "Markovian approximations of stochastic Volterra equations with the\n  fractional kernel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider rough stochastic volatility models where the variance process\nsatisfies a stochastic Volterra equation with the fractional kernel, as in the\nrough Bergomi and the rough Heston model. In particular, the variance process\nis therefore not a Markov process or semimartingale, and has quite low\nH\\\"older-regularity. In practice, simulating such rough processes thus often\nresults in high computational cost. To remedy this, we study approximations of\nstochastic Volterra equations using an $N$-dimensional diffusion process\ndefined as solution to a system of ordinary stochastic differential equation.\nIf the coefficients of the stochastic Volterra equation are Lipschitz\ncontinuous, we show that these approximations converge strongly with\nsuperpolynomial rate in $N$. Finally, we apply this approximation to compute\nthe implied volatility smile of a European call option under the rough Bergomi\nand the rough Heston model.\n"
    },
    {
        "paper_id": 2108.05066,
        "authors": "Xia Han, Bin Wang, Ruodu Wang and Qinyu Wu",
        "title": "Risk Concentration and the Mean-Expected Shortfall Criterion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expected Shortfall (ES, also known as CVaR) is the most important coherent\nrisk measure in finance, insurance, risk management, and engineering. Recently,\nWang and Zitikis (2021) put forward four economic axioms for portfolio risk\nassessment and provide the first economic axiomatic foundation for the family\nof ES. In particular, the axiom of no reward for concentration (NRC) is\narguably quite strong, which imposes an additive form of the risk measure on\nportfolios with a certain dependence structure. We move away from the axiom of\nNRC by introducing the notion of concentration aversion, which does not impose\nany specific form of the risk measure. It turns out that risk measures with\nconcentration aversion are functions of ES and the expectation. Together with\nthe other three standard axioms of monotonicity, translation invariance and\nlower semicontinuity, concentration aversion uniquely characterizes the family\nof ES. In addition, we establish an axiomatic foundation for the problem of\nmean-ES portfolio selection and new explicit formulas for convex and consistent\nrisk measures. Finally, we provide an economic justification for concentration\naversion via a few axioms on the attitude of a regulator towards dependence\nstructures.\n"
    },
    {
        "paper_id": 2108.05458,
        "authors": "Mohamad Ebrahim Sadeghi, Morteza Khodabakhsh, Mahmood Reza Ganjipoor,\n  Hamed Kazemipoor, Hamed Nozari",
        "title": "A New Multi Objective Mathematical Model for Relief Distribution\n  Location at Natural Disaster Response Phase",
        "comments": null,
        "journal-ref": null,
        "doi": "10.52547/ijie.1.1.22",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Every year, natural disasters such as earthquake, flood, hurricane and etc.\nimpose immense financial and humane losses on governments owing to their\nunpredictable character and arise of emergency situations and consequently the\nreduction of the abilities due to serious damages to infrastructures, increases\ndemand for logistic services and supplies. First, in this study the necessity\nof paying attention to locating procedures in emergency situations is pointed\nout and an outline for the studied case of disaster relief supply chain was\ndiscussed and the problem was validated at small scale. On the other hand, to\nsolve this kind of problems involving three objective functions and complicated\ntime calculation, meta-heuristic methods which yield almost optimum solutions\nin less time are applied. The EC method and NSGA II algorithm are among the\nevolutionary multi-objective optimization algorithms applied in this case. In\nthis study the aforementioned algorithm is used for solving problems at large\nscale.\n"
    },
    {
        "paper_id": 2108.05488,
        "authors": "Vanessa Echeverri, Juan C. Duque and Daniel E. Restrepo",
        "title": "Identifying poverty traps based on the network structure of economic\n  output",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we explore the relationship between monetary poverty and\nproduction combining relatedness theory, graph theory, and regression analysis.\nWe develop two measures at product level that capture short-run and long-run\npatterns of poverty, respectively. We use the network of related products (or\nproduct space) and both metrics to estimate the influence of the productive\nstructure of a country in its current and future levels of poverty. We found\nthat poverty is highly associated with poorly connected nodes in the PS,\nespecially products based on natural resources. We perform a series of\nregressions with several controls (including human capital, institutions,\nincome, and population) to show the robustness of our measures as predictors of\npoverty. Finally, by means of some illustrative examples, we show how our\nmeasures distinguishes between nuanced cases of countries with similar poverty\nand production and identify possibilities of improving their current poverty\nlevels.\n"
    },
    {
        "paper_id": 2108.05721,
        "authors": "Junjie Hu, Wolfgang Karl H\\\"ardle",
        "title": "Networks of News and Cross-Sectional Returns",
        "comments": "Revision before another submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We uncover networks from news articles to study cross-sectional stock\nreturns. By analyzing a huge dataset of more than 1 million news articles\ncollected from the internet, we construct time-varying directed networks of the\nS&P500 stocks. The well-defined directed news networks are formed based on a\nmodest assumption about firm-specific news structure, and we propose an\nalgorithm to tackle type-I errors in identifying the stock tickers. We find\nstrong evidence for the comovement effect between the news-linked stocks\nreturns and reversal effect from the lead stock return on the 1-day ahead\nfollower stock return, after controlling for many known effects. Furthermore, a\nseries of portfolio tests reveal that the news network attention proxy, network\ndegree, provides a robust and significant cross-sectional predictability of the\nmonthly stock returns. Among different types of news linkages, the linkages of\nwithin-sector stocks, large size lead firms, and lead firms with lower stock\nliquidity are crucial for cross-sectional predictability.\n"
    },
    {
        "paper_id": 2108.05747,
        "authors": "Francisco M. Fern\\'andez",
        "title": "Comment on \"An appropriate approach to pricing european-style options\n  with the Adomian decomposition method\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the Adomian decomposition method proposed by Ke et al [ANZIAM J.\n\\textbf{59} (2018) 349] is just the Taylor series approach in disguise. The\nlatter approach is simpler, more straightforward and yields a recurrence\nrelation free from integrals.\n"
    },
    {
        "paper_id": 2108.05791,
        "authors": "Felix-Benedikt Liebrich",
        "title": "Risk sharing under heterogeneous beliefs without convexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of finding Pareto-optimal allocations of risk among\nfinitely many agents. The associated individual risk measures are law\ninvariant, but with respect to agent-dependent and potentially heterogeneous\nreference probability measures. Moreover, we assume that the individual risk\nassessments are consistent with the respective second-order stochastic\ndominance relations. We do not assume their convexity though. A simple\nsufficient condition for the existence of Pareto optima is provided. The proof\ncombines local comonotone improvement with a Dieudonn\\'e-type argument, which\nalso establishes a link of the optimal allocation problem to the realm of\n\"collapse to the mean\" results. Finally, we extend the results to capital\nrequirements with multidimensional security markets.\n"
    },
    {
        "paper_id": 2108.05801,
        "authors": "Peter Akioyamen (1), Yi Zhou Tang (1), Hussien Hussien (1) ((1)\n  Western University)",
        "title": "A Hybrid Learning Approach to Detecting Regime Switches in Financial\n  Markets",
        "comments": "7 pages, 5 figures, 5 tables, ICAIF 2020: ACM International\n  Conference on AI in Finance",
        "journal-ref": "ICAIF 2020: ACM International Conference on AI in Finance",
        "doi": "10.1145/3383455.3422521",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are of much interest to researchers due to their dynamic\nand stochastic nature. With their relations to world populations, global\neconomies and asset valuations, understanding, identifying and forecasting\ntrends and regimes are highly important. Attempts have been made to forecast\nmarket trends by employing machine learning methodologies, while statistical\ntechniques have been the primary methods used in developing market regime\nswitching models used for trading and hedging. In this paper we present a novel\nframework for the detection of regime switches within the US financial markets.\nPrincipal component analysis is applied for dimensionality reduction and the\nk-means algorithm is used as a clustering technique. Using a combination of\ncluster analysis and classification, we identify regimes in financial markets\nbased on publicly available economic data. We display the efficacy of the\nframework by constructing and assessing the performance of two trading\nstrategies based on detected regimes.\n"
    },
    {
        "paper_id": 2108.05943,
        "authors": "Fernanda Herrera",
        "title": "Partisan affect and political outsiders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the effects of introducing a political outsider to the nomination\nprocess leading to an election. To this end, we develop a sequential game where\npoliticians -- insiders and outsiders -- make a platform offer to a party, and\nparties in turn decide which offer to accept; this process conforms the voting\nballot. Embedded in the evaluation of a party-candidate match is partisan\naffect, a variable comprising the attitudes of voters towards the party.\nPartisan affect may bias the electorate's appraisal of a match in a positive or\nnegative way. We characterize the conditions that lead to the nomination of an\noutsider and determine whether her introduction as a potential candidate has\nany effect on the winning policy and on the welfare of voters. We find that the\nvictory of an outsider generally leads to policy polarization, and that\npartisan affect has a more significant effect on welfare than ideology\nextremism.\n"
    },
    {
        "paper_id": 2108.05954,
        "authors": "Soheil Ghili and Vineet Kumar",
        "title": "Spatial Distribution of Supply and the Role of Market Thickness: Theory\n  and Evidence from Ride Sharing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the effects of economies of density in transportation\nmarkets, focusing on ridesharing. Our theoretical model predicts that (i)\neconomies of density skew the supply of drivers away from less dense regions,\n(ii) the skew will be more pronounced for smaller platforms, and (iii)\nrideshare platforms do not find this skew efficient and thus use prices and\nwages to mitigate (but not eliminate) it. We then develop a general empirical\nstrategy with simple implementation and limited data requirements to test for\nspatial skew of supply from demand. Applying our method to ride-level,\nmulti-platform data from New York City (NYC), we indeed find evidence for a\nskew of supply toward busier areas, especially for smaller platforms. We\ndiscuss the implications of our analysis for business strategy (e.g., spatial\npricing) and public policy (e.g., consequences of breaking up or downsizing a\nrideshare platform)\n"
    },
    {
        "paper_id": 2108.06055,
        "authors": "Damian Clarke and Manuel Llorca Ja\\~na and Daniel Paila\\~nir",
        "title": "The Use of Quantile Methods in Economic History",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantile regression and quantile treatment effect methods are powerful\neconometric tools for considering economic impacts of events or variables of\ninterest beyond the mean. The use of quantile methods allows for an examination\nof impacts of some independent variable over the entire distribution of\ncontinuous dependent variables. Measurement in many quantative settings in\neconomic history have as a key input continuous outcome variables of interest.\nAmong many other cases, human height and demographics, economic growth,\nearnings and wages, and crop production are generally recorded as continuous\nmeasures, and are collected and studied by economic historians. In this paper\nwe describe and discuss the broad utility of quantile regression for use in\nresearch in economic history, review recent quantitive literature in the field,\nand provide an illustrative example of the use of these methods based on 20,000\nrecords of human height measured across 50-plus years in the 19th and 20th\ncenturies. We suggest that there is considerably more room in the literature on\neconomic history to convincingly and productively apply quantile regression\nmethods.\n"
    },
    {
        "paper_id": 2108.06066,
        "authors": "Anandlogesh R R, Breasha Gupta, Divika Agarwal, Rasika Joshi",
        "title": "Analysis of the Indian Chemical Industry in the Post-Covid Era",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The story of the Chemical Industry in India is one of outperformance and\npromise. A consistent value creator, the chemical industry remains an\nattractive hub of opportunities, even in an environment of global uncertainty.\nThis paper aims to analyze the various driving factors, the performance of the\nkey players over fundamental analysis, and the various trends that would shape\nthe performance of the industry due to the various geopolitical and\nmacroeconomic trends in the post-pandemic world.\n"
    },
    {
        "paper_id": 2108.06441,
        "authors": "Eriamiatoe Efosa Festus",
        "title": "Logistics and trade flows in selected ECOWAS Countries: An empirical\n  verification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the role of logistics and its six components on trade\nflows in selected Economic Community of West Africa States (ECOWAS) countries.\nThe impact of other macro-economic variables on trade flows was also\ninvestigated. Ten countries were selected in eight years period. We decomposed\ntrade flows into import and export trade. The World Bank Logistics performance\nindex was used as a measure of logistics performance. The LPI has six\ncomponents, and the impact of these components on trade flows were also\nexamined. The fixed-effect model was used to explain the cross-country result\nthat was obtained. The results showed that logistics has no significant impact\non both Import and export, thus logistics play no role on trade flows among the\nselected ECOWAS countries. The components of logistics except Timeliness of\nshipments in reaching the final destination ( CRC ),have no impact on trade\nflows. Income was found to be positively related to imports. Exchange rate,\nconsumption and money supply, reserve and tariff have no significant impact on\nimports. Relative import price has an inverse and significant relationship with\nimports. GDP has a positive and significant impact on export trade. The study\nalso found FDI, savings, exchange rate and labour to have insignificant impact\non exports. Finally, we found that logistics is not a driver of trade among the\nselected ECOWAS countries. The study recommended the introduction of the single\nwindow system and improvement in border management in order to reduce the cost\nassociated with Logistics and thereby enhance trade.\n"
    },
    {
        "paper_id": 2108.06578,
        "authors": "Matteo Michielon, Asma Khedher, Peter Spreij",
        "title": "From bid-ask credit default swap quotes to risk-neutral default\n  probabilities using distorted expectations",
        "comments": null,
        "journal-ref": "journal={International Journal of Theoretical and Applied\n  Finance}, volume={24}, number={3} journal={International Journal of\n  Theoretical and Applied Finance}, volume={24}, number={3}, year={2021}",
        "doi": "10.1142/S0219024921500175",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk-neutral default probabilities can be implied from credit default swap\n(CDS) market quotes. In practice, mid CDS quotes are used as inputs, as their\nrisk-neutral counterparts are not observable. We show how to imply risk-neutral\ndefault probabilities from bid and ask quotes directly by means of formulating\nthe CDS calibration problem to bid and ask market quotes within the conic\nfinance framework. Assuming the risk-neutral distribution of the default time\nto be driven by a Poisson process we prove, under mild liquidity-related\nassumptions, that the calibration problem admits a unique solution that also\nallows to jointly calculate the implied liquidity of the market.\n"
    },
    {
        "paper_id": 2108.06593,
        "authors": "Nassib Boueri",
        "title": "G3M Impermanent Loss Dynamics",
        "comments": "8 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric Mean Market Makers (G3M) such as Uniswap, Sushiswap or Balancer are\nkey building blocks of the nascent Decentralised Finance system. We establish\nnon-arbitrage bounds for the wealth process of such Automated Market Makers in\nthe presence of transaction fees and highlight the dynamic of their so-called\nImpermanent Losses, which are incurred due to negative convexity and\nessentially void the benefits of portfolio diversification within G3Ms.\n  We then turn to empirical data to establish if transaction fee income has\nhistorically been high enough to offset Impermanent Losses and allow G3M\ninvestments to outperform their continually rebalanced constant-mix portfolio\ncounterparts. It appears that the median liquidity pool had a net nil ROI when\ntaking Impermanent Losses into account. The cross-sectional dispersion of ROI\nhas however been high and the pool net ROI ranking has been significantly\nautocorrelated for several weeks. This suggests that G3M pools are not yet\nefficiently arbitraged as agents may access ex-ante knowledge of which G3M\npools are likely to be far better investment proposals than others.\n  We finally focus on the UniswapV3 protocol, which introduced the notion of\nconcentrated liquidity ranges and show that such a position can be replicated\nby leveraging a classic UniswapV2 pool while simultaneously hedging part of the\nunderlying token price exposition. As such, the herein described Impermanent\nLoss dynamics also apply to UniswapV3 pools.\n"
    },
    {
        "paper_id": 2108.06655,
        "authors": "Yanwei Jia and Xun Yu Zhou",
        "title": "Policy Evaluation and Temporal-Difference Learning in Continuous Time\n  and Space: A Martingale Approach",
        "comments": "58 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a unified framework to study policy evaluation (PE) and the\nassociated temporal difference (TD) methods for reinforcement learning in\ncontinuous time and space. We show that PE is equivalent to maintaining the\nmartingale condition of a process. From this perspective, we find that the\nmean--square TD error approximates the quadratic variation of the martingale\nand thus is not a suitable objective for PE. We present two methods to use the\nmartingale characterization for designing PE algorithms. The first one\nminimizes a \"martingale loss function\", whose solution is proved to be the best\napproximation of the true value function in the mean--square sense. This method\ninterprets the classical gradient Monte-Carlo algorithm. The second method is\nbased on a system of equations called the \"martingale orthogonality conditions\"\nwith test functions. Solving these equations in different ways recovers various\nclassical TD algorithms, such as TD($\\lambda$), LSTD, and GTD. Different\nchoices of test functions determine in what sense the resulting solutions\napproximate the true value function. Moreover, we prove that any convergent\ntime-discretized algorithm converges to its continuous-time counterpart as the\nmesh size goes to zero, and we provide the convergence rate. We demonstrate the\ntheoretical results and corresponding algorithms with numerical experiments and\napplications.\n"
    },
    {
        "paper_id": 2108.0694,
        "authors": "Zuo Quan Xu",
        "title": "Moral-hazard-free insurance: mean-variance premium principle and\n  rank-dependent utility theory",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/03461238.2022.2092892",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a Pareto optimal insurance problem, where the insured\nmaximizes her rank-dependent utility preference and the insurer is risk neutral\nand employs the mean-variance premium principle. To eliminate potential moral\nhazard issues, we only consider the so-called moral-hazard-free insurance\ncontracts that obey the incentive compatibility constraint. The insurance\nproblem is first formulated as a non-concave maximization problem involving\nChoquet expectation, then turned into a concave quantile optimization problem\nand finally solved by the calculus of variations method. The optimal contract\nis expressed by a second-order ordinary integro-differential equation with\nnonlocal operator. An effective numerical method is proposed to compute the\noptimal contract assuming the probability weighting function has a density.\nAlso, we provide an example which is analytically solved.\n"
    },
    {
        "paper_id": 2108.06965,
        "authors": "Zaineb Mezdoud, Carsten Hartmann, Mohamed Riad Remita, Omar Kebiri",
        "title": "$\\alpha$-Hypergeometric Uncertain Volatility Models and their Connection\n  to 2BSDEs",
        "comments": "15 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we propose a $\\alpha$-hypergeometric model with uncertain\nvolatility (UV) where we derive a worst-case scenario for option pricing. The\napproach is based on the connexion between a certain class of nonlinear partial\ndifferential equations of HJB-type (G-HJB equations), that govern the nonlinear\nexpectation of the UV model and that provide an alternative to the difficult\nmodel calibration problem of UV models, and second-order backward stochastic\ndifferential equations (2BSDEs). Using asymptotic analysis for the G-HJB\nequation and the equivalent 2BSDE representation, we derive a limit model that\nprovides an accurate description of the worst-case price scenario in cases when\nthe bounds of the UV model are slowly varying. The analytical results are\ntested by numerical simulations using a deep learning based approximation of\nthe underlying 2BSDE.\n"
    },
    {
        "paper_id": 2108.07035,
        "authors": "Yixiao Lu, Yihong Wang, Tinggan Yang",
        "title": "Adaptive Gradient Descent Methods for Computing Implied Volatility",
        "comments": "Our implement of Newton-Raphson iteration has defects. After\n  correcting the code implement, we find Newton-Raphson won't be\n  non-convergent. See\n  https://github.com/cloudy-sfu/Newton-Raphson-Implied-Volatility for details",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, a new numerical method based on adaptive gradient descent\noptimizers is provided for computing the implied volatility from the\nBlack-Scholes (B-S) option pricing model. It is shown that the new method is\nmore accurate than the close form approximation. Compared with the\nNewton-Raphson method, the new method obtains a reliable rate of convergence\nand tends to be less sensitive to the beginning point.\n"
    },
    {
        "paper_id": 2108.07116,
        "authors": "Nitish Gupta, Ruchir Kaul, Satwik Gupta, Jay Shah",
        "title": "Study Of German Manufacturing Firms: Causal Impact Of European Union\n  Emission Trading Scheme On Firm Behaviour And Economic Performance",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The results based on the nonparametric nearest neighbor matching suggest a\nstatistically significant positive effect of the EU ETS on the economic\nperformance of the regulated firms during Phase I of the EU ETS. A year-by-year\nanalysis shows that the effect was only significant during the first year of\nPhase I. The EU ETS, therefore, had a particularly strong effect when it was\nintroduced. It is important to note that the EU ETS does not homogeneously\naffect firms in the manufacturing sector. We found a significant positive\nimpact of EU ETS on the economic performance of regulated firms in the paper\nindustry.\n"
    },
    {
        "paper_id": 2108.07146,
        "authors": "Stefan Buehler and Nicolas Eschenbaum",
        "title": "Dynamic Monopoly Pricing With Multiple Varieties: Trading Up",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies dynamic monopoly pricing for a class of settings that\nincludes multiple durable, multiple rental, or a mix of varieties. We show that\nthe driving force behind pricing dynamics is the seller's incentive to switch\nconsumers - buyers and non-buyers - to higher-valued consumption options by\nlowering prices (\"trading up\"). If consumers cannot be traded up from the\nstatic optimal allocation, pricing dynamics do not emerge in equilibrium. If\nconsumers can be traded up, pricing dynamics arise until all trading-up\nopportunities are exhausted. We study the conditions under which pricing\ndynamics end in finite time and characterize the final prices at which dynamics\nend.\n"
    },
    {
        "paper_id": 2108.07163,
        "authors": "Nitish Gupta, Jay Shah, Satwik Gupta, Ruchir Kaul",
        "title": "Causal Impact Of European Union Emission Trading Scheme On Firm\n  Behaviour And Economic Performance: A Study Of German Manufacturing Firms",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we estimate the causal impact (i.e. Average Treatment Effect,\nATT) of the EU ETS on GHG emissions and firm competitiveness (primarily\nmeasured by employment, turnover, and exports levels) by combining a\ndifference-in-differences approach with semi-parametric matching techniques and\nestimators an to investigate the effect of the EU ETS on the economic\nperformance of these German manufacturing firms using a Stochastic Production\nFrontier model.\n"
    },
    {
        "paper_id": 2108.07348,
        "authors": "Angela Acocella, Chris Caplice, Yossi Sheffi",
        "title": "Elephants or Goldfish? An Empirical Analysis of Carrier Reciprocity in\n  Dynamic Freight Markets",
        "comments": null,
        "journal-ref": "Transportation Research Part E: Logistics and Transportation\n  Review, Vol 142, 2020",
        "doi": "10.1016/j.tre.2020.102073",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Dynamic macroeconomic conditions and non-binding truckload freight contracts\nenable both shippers and carriers to behave opportunistically. We present an\nempirical analysis of carrier reciprocity in the US truckload transportation\nsector to demonstrate whether consistent performance and fair pricing by\nshippers when markets are in their favor result in maintained primary carrier\ntender acceptance when markets turn. The results suggest carriers have short\nmemories; they do not remember shippers' previous period pricing, tendering\nbehavior, or performance when making freight acceptance decisions. However,\ncarriers appear to be myopic and respond to shippers' current market period\nbehaviors, ostensibly without regard to shippers' previous behaviors.\n"
    },
    {
        "paper_id": 2108.07615,
        "authors": "Hamza Saad, Nagendra Nagarur and Abdulrahman Shamsan",
        "title": "Analysis of Data Mining Process for Improvement of Production Quality in\n  Industrial Sector",
        "comments": "11 pages, 2 figure, 11 tables, peer reviewed paper",
        "journal-ref": null,
        "doi": "10.3923/jas.2021.10.20",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Background and Objective: Different industries go through high-precision and\ncomplex processes that need to analyze their data and discover defects before\ngrowing up. Big data may contain large variables with missed data that play a\nvital role to understand what affect the quality. So, specialists of the\nprocess might be struggling to defined what are the variables that have direct\neffect in the process. Aim of this study was to build integrated data analysis\nusing data mining and quality tools to improve the quality of production and\nprocess. Materials and Methods: Data collected in different steps to reduce\nmissed data. The specialists in the production process recommended to select\nthe most important variables from big data and then predictor screening was\nused to confirm 16 of 71 variables. Seven important variables built the output\nvariable that called textile quality score. After testing ten algorithms,\nboosted tree and random forest were evaluated to extract knowledge. In the\nvoting process, three variables were confirmed to use as input factors in the\ndesign of experiments. The response of design was estimated by data mining and\nthe results were confirmed by the quality specialists. Central composite\n(surface response) has been run 17 times to extract the main effects and\ninteractions on the textile quality score. Results: Current study found that a\nmachine productivity has negative effect on the quality, so this validated by\nthe management. After applying changes, the efficiency of production has\nimproved 21%. Conclusion: Results confirmed a big improvement in quality\nprocesses in industrial sector. The efficiency of production improved to 21%,\nweaving process improved to 23% and the overall process improved to 17.06%.\n"
    },
    {
        "paper_id": 2108.07806,
        "authors": "Ivan Jericevich and Patrick Chang and Tim Gebbie",
        "title": "Simulation and estimation of an agent-based market-model with a matching\n  engine",
        "comments": "29 Pages, 30 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An agent-based model with interacting low frequency liquidity takers\ninter-mediated by high-frequency liquidity providers acting collectively as\nmarket makers can be used to provide realistic simulated price impact curves.\nThis is possible when agent-based model interactions occur asynchronously via\norder matching using a matching engine in event time to replace sequential\ncalendar time market clearing. Here the matching engine infrastructure has been\nmodified to provide a continuous feed of order confirmations and updates as\nmessage streams in order to conform more closely to live trading environments.\nThe resulting trade and quote message data from the simulations are then\naggregated, calibrated and visualised. Various stylised facts are presented\nalong with event visualisations and price impact curves. We argue that\nadditional realism in modelling can be achieved with a small set of agent\nparameters and simple interaction rules once interactions are reactive,\nasynchronous and in event time. We argue that the reactive nature of market\nagents may be a fundamental property of financial markets and when accounted\nfor can allow for parsimonious modelling without recourse to additional sources\nof noise.\n"
    },
    {
        "paper_id": 2108.07847,
        "authors": "Stephen Keen, Timothy M. Lenton, Antoine Godin, Devrim Yilmaz, Matheus\n  Grasselli, Timothy J. Garrett",
        "title": "Economists' erroneous estimates of damages from climate change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economists have predicted that damages from global warming will be as low as\n2.1% of global economic production for a 3$^\\circ$C rise in global average\nsurface temperature, and 7.9% for a 6$^\\circ$C rise. Such relatively trivial\nestimates of economic damages -- when these economists otherwise assume that\nhuman economic productivity will be an order of magnitude higher than today --\ncontrast strongly with predictions made by scientists of significantly reduced\nhuman habitability from climate change. Nonetheless, the coupled economic and\nclimate models used to make such predictions have been influential in the\ninternational climate change debate and policy prescriptions. Here we review\nthe empirical work done by economists and show that it severely underestimates\ndamages from climate change by committing several methodological errors,\nincluding neglecting tipping points, and assuming that economic sectors not\nexposed to the weather are insulated from climate change. Most fundamentally,\nthe influential Integrated Assessment Model DICE is shown to be incapable of\ngenerating an economic collapse, regardless of the level of damages. Given\nthese flaws, economists' empirical estimates of economic damages from global\nwarming should be rejected as unscientific, and models that have been\ncalibrated to them, such as DICE, should not be used to evaluate economic risks\nfrom climate change, or in the development of policy to attenuate damages.\n"
    },
    {
        "paper_id": 2108.07888,
        "authors": "Takeshi Kato, Yoshinori Hiroi",
        "title": "Wealth disparities and economic flow: Assessment using an asset exchange\n  model with the surplus stock of the wealthy",
        "comments": "16 pages, 7 figures. Published online at\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0259323",
        "journal-ref": "PLoS ONE 2021; 16(11): e0259323",
        "doi": "10.1371/journal.pone.0259323",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  How can we limit wealth disparities while stimulating economic flows in\nsustainable societies? To examine the link between these concepts, we propose\nan econophysics asset exchange model with the surplus stock of the wealthy. The\nwealthy are one of the two exchange agents and have more assets than the poor.\nOur simulation model converts the surplus contribution rate of the wealthy to a\nnew variable parameter alongside the saving rate and introduces the total\nexchange (flow) and rank correlation coefficient (metabolism) as new evaluation\nindexes, adding to the Gini index (disparities), thereby assessing both wealth\ndistribution and the relationships among the disparities, flow, and metabolism.\nWe show that these result in a gamma-like wealth distribution, and our model\nreveals a trade-off between limiting disparities and vitalizing the market. To\nlimit disparities and increase flow and metabolism, we also find the need to\nrestrain savings and use the wealthy surplus stock. This relationship is\nexplicitly expressed in the new equation introduced herein. The insights gained\nby uncovering the root of disparities may present a persuasive case for\ninvestments in social security measures or social businesses involving stock\nredistribution or sharing.\n"
    },
    {
        "paper_id": 2108.07924,
        "authors": "Muhammed Taher Al-Mudafer, Benjamin Avanzi, Greg Taylor, Bernard Wong",
        "title": "Stochastic loss reserving with mixture density neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2022.03.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neural networks offer a versatile, flexible and accurate approach to loss\nreserving. However, such applications have focused primarily on the (important)\nproblem of fitting accurate central estimates of the outstanding claims. In\npractice, properties regarding the variability of outstanding claims are\nequally important (e.g., quantiles for regulatory purposes).\n  In this paper we fill this gap by applying a Mixture Density Network (\"MDN\")\nto loss reserving. The approach combines a neural network architecture with a\nmixture Gaussian distribution to achieve simultaneously an accurate central\nestimate along with flexible distributional choice. Model fitting is done using\na rolling-origin approach. Our approach consistently outperforms the classical\nover-dispersed model both for central estimates and quantiles of interest, when\napplied to a wide range of simulated environments of various complexity and\nspecifications.\n  We further extend the MDN approach by proposing two extensions. Firstly, we\npresent a hybrid GLM-MDN approach called \"ResMDN\". This hybrid approach\nbalances the tractability and ease of understanding of a traditional GLM model\non one hand, with the additional accuracy and distributional flexibility\nprovided by the MDN on the other. We show that it can successfully improve the\nerrors of the baseline ccODP, although there is generally a loss of performance\nwhen compared to the MDN in the examples we considered. Secondly, we allow for\nexplicit projection constraints, so that actuarial judgement can be directly\nincorporated in the modelling process.\n  Throughout, we focus on aggregate loss triangles, and show that our\nmethodologies are tractable, and that they out-perform traditional approaches\neven with relatively limited amounts of data. We use both simulated data -- to\nvalidate properties, and real data -- to illustrate and ascertain practicality\nof the approaches.\n"
    },
    {
        "paper_id": 2108.07937,
        "authors": "Ben Boukai",
        "title": "The Generalized Gamma distribution as a useful RND under Heston's\n  stochastic volatility model",
        "comments": "24 pages with 6 figures and 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Following Boukai (2021) we present the Generalized Gamma (GG) distribution as\na possible RND for modeling European options prices under Heston's (1993)\nstochastic volatility (SV) model. This distribution is seen as especially\nuseful in situations in which the spot's price follows a negatively skewed\ndistribution and hence, Black-Scholes based (i.e. the log-normal distribution)\nmodeling is largely inapt. We apply the GG distribution as RND to modeling\ncurrent market option data on three large market-index ETFs, namely the SPY,\nIWM and QQQ as well as on the TLT (an ETF that tracks an index of long term US\nTreasury bonds). The current option chain of each of the three market-index\nETFs shows of a pronounced skew of their volatility `smile' which indicates a\nlikely distortion in the Black-Scholes modeling of such option data. Reflective\nof entirely different market expectations, this distortion appears not to exist\nin the TLT option data. We provide a thorough modeling of the available option\ndata we have on each ETF (with the October 15, 2021 expiration) based on the GG\ndistribution and compared it to the option pricing and RND modeling obtained\ndirectly from a well-calibrated Heston's (1993) SV model (both theoretically\nand empirically, using Monte-Carlo simulations of the spot's price). All three\nmarket-index ETFs exhibit negatively skewed distributions which are\nwell-matched with those derived under the GG distribution as RND. The\ninadequacy of the Black-Scholes modeling in such instances which involve\nnegatively skewed distribution is further illustrated by its impact on the\nhedging factor, delta, and the immediate implications to the retail trader. In\ncontrast, for the TLT ETF, which exhibits no such distortion to the volatility\n`smile', the three pricing models (i.e. Heston's, Black-Scholes and Generalized\nGamma) appear to yield similar results.\n"
    },
    {
        "paper_id": 2108.08097,
        "authors": "Syngjoo Choi, Byung-Yeon Kim, Jungmin Lee, Sokbae Lee",
        "title": "Why North Korean Refugees are Reluctant to Compete: The Roles of\n  Cognitive Ability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study compares the competitiveness of three Korean groups raised in\ndifferent institutional environments: South Korea, North Korea, and China.\nLaboratory experiments reveal that North Korean refugees are less likely to\nparticipate in competitive tournaments than South Koreans and Korean-Chinese\nimmigrants. Analysis using a choice model with probability weighting suggests\nthat lower cognitive ability may lead to lower expected performance, more\npessimistic beliefs, and greater aversion to competition.\n"
    },
    {
        "paper_id": 2108.08229,
        "authors": "Geoff Boeing, Max Besbris, David Wachsmuth, Jake Wegmann",
        "title": "Tilted Platforms: Rental Housing Technology and the Rise of Urban Big\n  Data Oligopolies",
        "comments": null,
        "journal-ref": "Urban Transformations, 3, 6, 2021",
        "doi": "10.1186/s42854-021-00024-2",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article interprets emerging scholarship on rental housing platforms --\nparticularly the most well-known and used short- and long-term rental housing\nplatforms - and considers how the technological processes connecting both\nshort-term and long-term rentals to the platform economy are transforming\ncities. It discusses potential policy approaches to more equitably distribute\nbenefits and mitigate harms. We argue that information technology is not\nvalue-neutral. While rental housing platforms may empower data analysts and\ncertain market participants, the same cannot be said for all users or society\nat large. First, user-generated online data frequently reproduce the systematic\nbiases found in traditional sources of housing information. Evidence is growing\nthat the information broadcasting potential of rental housing platforms may\nincrease rather than mitigate sociospatial inequality. Second, technology\nplatforms curate and shape information according to their creators' own\nfinancial and political interests. The question of which data -- and people --\nare hidden or marginalized on these platforms is just as important as the\nquestion of which data are available. Finally, important differences in\nbenefits and drawbacks exist between short-term and long-term rental housing\nplatforms, but are underexplored in the literature: this article unpacks these\ndifferences and proposes policy recommendations.\n"
    },
    {
        "paper_id": 2108.08366,
        "authors": "Yonatan Berman and Mark Kirstein",
        "title": "Risk Preferences in Time Lotteries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important but understudied question in economics is how people choose when\nfacing uncertainty in the timing of events. Here we study preferences over time\nlotteries, in which the payment amount is certain but the payment time is\nuncertain. Expected discounted utility theory (EDUT) predicts decision makers\nto be risk-seeking over time lotteries. We explore a normative model of\ngrowth-optimality, in which decision makers maximise the long-term growth rate\nof their wealth. Revisiting experimental evidence on time lotteries, we find\nthat growth-optimality accords better with the evidence than EDUT. We outline\nfuture experiments to scrutinise further the plausibility of growth-optimality.\n"
    },
    {
        "paper_id": 2108.08816,
        "authors": "Anuradha Singh",
        "title": "Regional disparities in Social Mobility of India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rapid rise in income inequality in India is a serious concern. While the\nemphasis is on inclusive growth, it seems difficult to tackle the problem\nwithout looking at the intricacies of the problem. The Social Mobility Index is\nan important tool that focuses on bringing long-term equality by identifying\npriority policy areas in the country. The PCA technique is employed in\ncomputation of the index. Overall, the Union Territory of Delhi ranks first,\nwith the highest social mobility and the least social mobility is in\nChhattisgarh. In addition, health and education access, quality and equity are\nkey priority areas that can help improve social mobility in India. Thus, we\nconclude that human capital is of great importance in promoting social mobility\nand development in the present times.\n"
    },
    {
        "paper_id": 2108.08818,
        "authors": "Rui Wang",
        "title": "Discriminating modelling approaches for Point in Time Economic Scenario\n  Generation",
        "comments": "49 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the notion of Point in Time Economic Scenario Generation (PiT\nESG) with a clear mathematical problem formulation to unify and compare\neconomic scenario generation approaches conditional on forward looking market\ndata. Such PiT ESGs should provide quicker and more flexible reactions to\nsudden economic changes than traditional ESGs calibrated solely to long periods\nof historical data. We specifically take as economic variable the S&P500 Index\nwith the VIX Index as forward looking market data to compare the nonparametric\nfiltered historical simulation, GARCH model with joint likelihood estimation\n(parametric), Restricted Boltzmann Machine and the conditional Variational\nAutoencoder (Generative Networks) for their suitability as PiT ESG. Our\nevaluation consists of statistical tests for model fit and benchmarking the out\nof sample forecasting quality with a strategy backtest using model output as\nstop loss criterion. We find that both Generative Networks outperform the\nnonparametric and classic parametric model in our tests, but that the CVAE\nseems to be particularly well suited for our purposes: yielding more robust\nperformance and being computationally lighter.\n"
    },
    {
        "paper_id": 2108.08999,
        "authors": "Lin William Cong, Ke Tang, Jingyuan Wang, Yang Zhang",
        "title": "Deep Sequence Modeling: Development and Applications in Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3905/jfds.2020.1.053",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We predict asset returns and measure risk premia using a prominent technique\nfrom artificial intelligence -- deep sequence modeling. Because asset returns\noften exhibit sequential dependence that may not be effectively captured by\nconventional time series models, sequence modeling offers a promising path with\nits data-driven approach and superior performance. In this paper, we first\noverview the development of deep sequence models, introduce their applications\nin asset pricing, and discuss their advantages and limitations. We then perform\na comparative analysis of these methods using data on U.S. equities. We\ndemonstrate how sequence modeling benefits investors in general through\nincorporating complex historical path dependence, and that Long- and Short-term\nMemory (LSTM) based models tend to have the best out-of-sample performance.\n"
    },
    {
        "paper_id": 2108.09014,
        "authors": "Koichi Miyamoto",
        "title": "Bermudan option pricing by quantum amplitude estimation and Chebyshev\n  interpolation",
        "comments": "14 pages, no figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing of financial derivatives, in particular early exercisable options\nsuch as Bermudan options, is an important but heavy numerical task in financial\ninstitutions, and its speed-up will provide a large business impact. Recently,\napplications of quantum computing to financial problems have been started to be\ninvestigated. In this paper, we first propose a quantum algorithm for Bermudan\noption pricing. This method performs the approximation of the continuation\nvalue, which is a crucial part of Bermudan option pricing, by Chebyshev\ninterpolation, using the values at interpolation nodes estimated by quantum\namplitude estimation. In this method, the number of calls to the oracle to\ngenerate underlying asset price paths scales as $\\widetilde{O}(\\epsilon^{-1})$,\nwhere $\\epsilon$ is the error tolerance of the option price. This means the\nquadratic speed-up compared with classical Monte Carlo-based methods such as\nleast-squares Monte Carlo, in which the oracle call number is\n$\\widetilde{O}(\\epsilon^{-2})$.\n"
    },
    {
        "paper_id": 2108.09029,
        "authors": "Younghun Choi, Takuro Kobashi, Yoshiki Yamagata, and Akito Murayama",
        "title": "Assessment of waterfront office redevelopment plan on optimal building\n  energy demand and rooftop photovoltaics for urban decarbonization",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": "10.3390/en15030883",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Designing waterfront redevelopment generally focuses on attractiveness,\nleisure, and beauty, resulting in various types of building and block shapes\nwith limited considerations on environmental aspects. However, increasing\nclimate change impacts necessitate these buildings to be sustainable,\nresilient, and zero CO2 emissions. By producing five scenarios (plus existing\nbuildings) with constant floor areas, we investigated how building and district\nform with building integrated photovoltaics (BIPV) affect energy consumption\nand production, self-sufficiency, CO2 emission, and energy costs in the context\nof waterfront redevelopment in Tokyo. From estimated hourly electricity demands\nof the buildings, techno-economic analyses are conducted for rooftop PV systems\nfor 2018 and 2030 with declining costs of rooftop PV systems. We found that\nenvironmental building designs with rooftop PV system are increasingly\neconomical in Tokyo with CO2 emission reduction of 2-9% that depends on rooftop\nsizes. Payback periods drop from 14 years in 2018 to 6 years in 2030. Toward\nnet-zero CO2 emissions by 2050, immediate actions are necessary to install\nrooftop PVs on existing and new buildings with energy efficiency improvements\nby construction industry and building owners. To facilitate such actions,\nnational and local governments need to adopt appropriate policies.\n"
    },
    {
        "paper_id": 2108.09035,
        "authors": "Guodong Ding, Daniele Marazzina",
        "title": "Sensitivity of Optimal Retirement Problem to Liquidity Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we analytically solve an optimal retirement problem, in which\nthe agent optimally allocates the risky investment, consumption and leisure\nrate to maximise a gain function characterised by a power utility function of\nconsumption and leisure, through the duality method. We impose different\nliquidity constraints over different time spans and conduct a sensitivity\nanalysis to discover the effect of this kind of constraint.\n"
    },
    {
        "paper_id": 2108.0969,
        "authors": "Muhammad Anwar Hossain and Iryna Zablotska-Manos",
        "title": "The changing dynamics of HIV/AIDS during the Covid-19 pandemic in the\n  Rohingya refugee camps in Bangladesh a call for action",
        "comments": "10 pages, no figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  COVID-19 pandemic has affected each and every country's health service and\nplunged refugees into the most desperate conditions. The plight of Rohingya\nrefugees is among the harshest. It has severely affected their existing HIV/STI\nprevention and management services and further increased the risk of violence\nand onward HIV transmission within the camps. In this commentary, we discuss\nthe context and the changing dynamics of HIV/AIDS during COVID-19 among the\nRohingya refugee community in Bangladesh. What we currently observe is the\nworst crisis in the Rohingya refugee camps thus far. Firstly, because of being\ndisplaced, Rohingya refugees have increased vulnerability to HIV, as well as to\nSTIs and other poor health outcomes. Secondly, for the same reason, they have\ninadequate access to HIV testing treatment and care. Not only because of their\nrefugee status but also because of the poor capacity of the host country to\nprovide services. Thirdly, a host of complex economic, socio-cultural and\nbehavioural factors exacerbate their dire situation with access to HIV testing,\ntreatment and care. And finally, the advent of the COVID-19 pandemic has\nchanged priorities in all societies, including the refugee camps. In the\ncontext of the unfolding COVID-19 crisis, more emphasis is placed on COVID-19\nrather than other health issues, which exacerbates the dire situation with HIV\ndetection, management, and prevention among Rohingya refugees. Despite the\ncommon crisis experienced by most countries around the world, the international\ncommunity has an obligation to work together to improve the life, livelihood,\nand health of those who are most vulnerable. Rohingya refugees are among them.\n"
    },
    {
        "paper_id": 2108.0975,
        "authors": "Jakob Albers, Mihai Cucuringu, Sam Howison, Alexander Y. Shestopaloff",
        "title": "Fragmentation, Price Formation, and Cross-Impact in Bitcoin Markets",
        "comments": "62 pages, 34 figures, 24 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In light of micro-scale inefficiencies induced by the high degree of\nfragmentation of the Bitcoin trading landscape, we utilize a granular data set\ncomprised of orderbook and trades data from the most liquid Bitcoin markets, in\norder to understand the price formation process at sub-1 second time scales. To\nachieve this goal, we construct a set of features that encapsulate relevant\nmicrostructural information over short lookback windows. These features are\nsubsequently leveraged first to generate a leader-lagger network that\nquantifies how markets impact one another, and then to train linear models\ncapable of explaining between 10% and 37% of total variation in $500$ms future\nreturns (depending on which market is the prediction target). The results are\nthen compared with those of various PnL calculations that take trading\nrealities, such as transaction costs, into account. The PnL calculations are\nbased on natural $\\textit{taker}$ strategies (meaning they employ market\norders) that we associate to each model. Our findings emphasize the role of a\nmarket's fee regime in determining its propensity to being a leader or a\nlagger, as well as the profitability of our taker strategy. Taking our analysis\nfurther, we also derive a natural $\\textit{maker}$ strategy (i.e., one that\nuses only passive limit orders), which, due to the difficulties associated with\nbacktesting maker strategies, we test in a real-world live trading experiment,\nin which we turned over 1.5 million USD in notional volume. Lending additional\nconfidence to our models, and by extension to the features they are based on,\nthe results indicate a significant improvement over a naive benchmark strategy,\nwhich we also deploy in a live trading environment with real capital, for the\nsake of comparison.\n"
    },
    {
        "paper_id": 2108.09763,
        "authors": "J. Gavin, M. Crane",
        "title": "Community Detection in Cryptocurrencies with Potential Applications to\n  Portfolio Diversification",
        "comments": "14 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the cross-correlations of cryptocurrency returns are analysed.\nThe paper examines one years worth of data for 146 cryptocurrencies from the\nperiod January 1 2019 to December 31 2019. The cross-correlations of these\nreturns are firstly analysed by comparing eigenvalues and eigenvector\ncomponents of the cross-correlation matrix C with Random Matrix Theory (RMT)\nassumptions. Results show that C deviates from these assumptions indicating\nthat C contains genuine information about the correlations between the\ndifferent cryptocurrencies. From here, Louvain community detection method is\napplied as a clustering mechanism and 15 community groupings are detected.\nFinally, PCA is completed on the standardised returns of each of these clusters\nto create a portfolio of cryptocurrencies for investment. This method selects a\nportfolio which contains a number of high value coins when compared back\nagainst their market ranking in the same year. In the interest of assessing\ncontinuity of the initial results, the method is also applied to a smaller\ndataset of the top 50 cryptocurrencies across three time periods of T = 125\ndays, which produces similar results. The results obtained in this paper show\nthat these methods could be useful for constructing a portfolio of optimally\nperforming cryptocurrencies.\n"
    },
    {
        "paper_id": 2108.09981,
        "authors": "Egor Malkov",
        "title": "Welfare Effects of Labor Income Tax Changes on Married Couples: A\n  Sufficient Statistics Approach",
        "comments": "60 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a framework for assessing the welfare effects of labor\nincome tax changes on married couples. I build a static model of couples' labor\nsupply that features both intensive and extensive margins and derive a\ntractable expression that delivers a transparent understanding of how labor\nsupply responses, policy parameters, and income distribution affect the\nreform-induced welfare gains. Using this formula, I conduct a comparative\nwelfare analysis of four tax reforms implemented in the United States over the\nlast four decades, namely the Tax Reform Act of 1986, the Omnibus Budget\nReconciliation Act of 1993, the Economic Growth and Tax Relief Reconciliation\nAct of 2001, and the Tax Cuts and Jobs Act of 2017. I find that these reforms\ncreated welfare gains ranging from -0.16 to 0.62 percent of aggregate labor\nincome. A sizable part of the gains is generated by the labor force\nparticipation responses of women. Despite three reforms resulted in aggregate\nwelfare gains, I show that each reform created both winners and losers.\nFurthermore, I uncover two patterns in the relationship between welfare gains\nand couples' labor income. In particular, the reforms of 1986 and 2017 display\na monotonically increasing relationship, while the other two reforms\ndemonstrate a U-shaped pattern. Finally, I characterize the bias in welfare\ngains resulting from the assumption about a linear tax function. I consider a\nreform that changes tax progressivity and show that the linearization bias is\ngiven by the ratio between the tax progressivity parameter and the inverse\nelasticity of taxable income. Quantitatively, it means that linearization\noverestimates the welfare effects of the U.S. tax reforms by 3.6-18.1%.\n"
    },
    {
        "paper_id": 2108.09985,
        "authors": "Masashi Ieda",
        "title": "Continuous-time Portfolio Optimization for Absolute Return Funds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a continuous-time portfolio optimization problem with\nthe following features: (i) a no-short selling constraint; (ii) a leverage\nconstraint, that is, an upper limit for the sum of portfolio weights; and (iii)\na performance criterion based on the lower mean square error between the\ninvestor's wealth and a predetermined target wealth level. Since the target\nlevel is defined by a deterministic function independent of market indices, it\ncorresponds to the criterion of absolute return funds. The model is formulated\nusing the stochastic control framework with explicit boundary conditions. The\ncorresponding Hamilton-Jacobi-Bellman equation is solved numerically using the\nkernel-based collocation method. However, a straightforward implementation does\nnot offer a stable and acceptable investment strategy; thus, some techniques to\naddress this shortcoming are proposed. By applying the proposed methodology,\ntwo numerical results are obtained: one uses artificial data, and the other\nuses empirical data from Japanese organizations. There are two implications\nfrom the first result: how to stabilize the numerical solution, and a technique\nto circumvent the plummeting achievement rate close to the terminal time. The\nsecond result implies that leverage is inevitable to achieve the target level\nin the setting discussed in this paper.\n"
    },
    {
        "paper_id": 2108.10065,
        "authors": "Gabriel de Oliveira Guedes Nogueira and Marcel Otoboni de Lima",
        "title": "Previs\\~ao dos pre\\c{c}os de abertura, m\\'inima e m\\'axima de \\'indices\n  de mercados financeiros usando a associa\\c{c}\\~ao de redes neurais LSTM",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In order to make good investment decisions, it is vitally important for an\ninvestor to know how to make good analysis of financial time series. Within\nthis context, studies on the forecast of the values and trends of stock prices\nhave become more relevant. Currently, there are different approaches to dealing\nwith the task. The two main ones are the historical analysis of stock prices\nand technical indicators and the analysis of sentiments in news, blogs and\ntweets about the market. Some of the most used statistical and artificial\nintelligence techniques are genetic algorithms, Support Vector Machines (SVM)\nand different architectures of artificial neural networks. This work proposes\nthe improvement of a model based on the association of three distinct LSTM\nneural networks, each acting in parallel to predict the opening, minimum and\nmaximum prices of stock exchange indices on the day following the analysis. The\ndataset is composed of historical data from more than 10 indices from the\nworld's largest stock exchanges. The results demonstrate that the model is able\nto predict trends and stock prices with reasonable accuracy.\n"
    },
    {
        "paper_id": 2108.10075,
        "authors": "Ragnar Levy Gudmundarson and Manuel Guerra and Alexandra Bugalho de\n  Moura",
        "title": "Minimizing ruin probability under dependencies for insurance pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work the ruin probability of the Lundberg risk process is used as a\ncriterion for determining the optimal security loading of premia in the\npresence of price-sensitive demand for insurance. Both single and aggregated\nclaim processes are considered and the independent and the dependent cases are\nanalyzed. For the single-risk case, we show that the optimal loading does not\ndepend on the initial reserve. In the multiple risk case we account for\narbitrary dependency structures between different risks and for dependencies\nbetween the probabilities of a client acquiring policies for different risks.\nIn this case, the optimal loadings depend on the initial reserve. In all cases\nthe loadings minimizing the ruin probability do not coincide with the loadings\nmaximizing the expected profit.\n"
    },
    {
        "paper_id": 2108.10109,
        "authors": "Yuki Takahashi",
        "title": "Gender Differences in the Cost of Corrections in Group Work",
        "comments": "This draft is still preliminary; future versions can differ\n  substantially from this version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corrections among colleagues are an integral part of group work, but people\nmay take corrections as personal criticism, especially corrections by women. I\nstudy whether people dislike collaborating with someone who corrects them and\nmore so when that person is a woman. People, including those with high\nproductivity, are less willing to collaborate with a person who has corrected\nthem even if the correction improves group performance. Yet, people respond to\ncorrections by women as negatively as by men. These findings suggest that\nalthough women do not face a higher hurdle, correcting colleagues is costly and\nreduces group efficiency.\n"
    },
    {
        "paper_id": 2108.10176,
        "authors": "Heidar Eyjolfsson and Dag Tj{\\o}stheim",
        "title": "Multivariate self-exciting jump processes with applications to financial\n  data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper discusses multivariate self- and cross-exciting processes. We\ndefine a class of multivariate point processes via their corresponding\nstochastic intensity processes that are driven by stochastic jumps.\nEssentially, there is a jump in an intensity process whenever the corresponding\npoint process records an event. An attribute of our modelling class is that not\nonly a jump is recorded at each instance, but also its magnitude. This allows\nlarge jumps to influence the intensity to a larger degree than smaller jumps.\nWe give conditions which guarantee that the process is stable, in the sense\nthat it does not explode, and provide a detailed discussion on when the\nsubclass of linear models is stable. Finally, we fit our model to financial\ntime series data from the S\\&P 500 and Nikkei 225 indices respectively. We\nconclude that a nonlinear variant from our modelling class fits the data best.\nThis supports the observation that in times of crises (high intensity) jumps\ntend to arrive in clusters, whereas there are typically longer times between\njumps when the markets are calmer. We moreover observe more variability in jump\nsizes when the intensity is high, than when it is low.\n"
    },
    {
        "paper_id": 2108.10244,
        "authors": "David Haritone Shikumo",
        "title": "Effect of Share Capital on Financial Growth of Non-Financial Firms\n  Listed at the Nairobi Securities Exchange",
        "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:2011.03339, arXiv:2010.12596",
        "journal-ref": "ISSN 2522-3186 2021",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Purpose: A significant number of the non-financial firms listed at the\nNairobi Securities Exchange (NSE) have been experiencing declining financial\nperformance which deters investors from investing in such firms. The lenders\nare also not willing to lend to such firms. As such, the firms struggle to\nraise funds for their operations. Prudent financing decisions can lead to\nfinancial growth of the firm. The purpose of this study is to assess the effect\nof Share capital on financial growth of Non-financial firms listed at the\nNairobi Securities Exchange. Financial firms were excluded because of their\nspecific sector characteristics and stringent regulatory framework. The study\nis guided by Market Timing Theory and Theory of Growth of the Firm.\nMethodology: Explanatory research design was adopted. The target population of\nthe study comprised of 45 non-financial firms listed at NSE for a period of ten\nyears from 2008 to 2017. The study conducted both descriptive statistics\nanalysis and panel data analysis. Findings: The result indicates that, share\ncapital explains 32.73% and 11.62% of variations in financial growth as measure\nby growth in earnings per share and growth in market capitalization\nrespectively. Share capital positively and significantly influences financial\ngrowth as measured by both growth in earnings per share and growth in market\ncapitalization. Implications: The study recommends for the Non-financial firms\nto utilize equity financing as a way of raising capital for major expansions,\nasset growth or acquisitions which may require heavy funding. In this way,\nfirms will be assured of improved performance as well as high financial growth.\nThe study also recommends for substantial firm financing through equity. Value:\nEquity financing is important to any firm, if the proceeds are used to invest\nin projects which eventually bring growth to the firm.\n"
    },
    {
        "paper_id": 2108.10403,
        "authors": "Sebastian Jaimungal, Silvana Pesenti, Ye Sheng Wang, and Hariom Tatsat",
        "title": "Robust Risk-Aware Reinforcement Learning",
        "comments": "12 pages, 5 figures",
        "journal-ref": "SIAM J. Financial Mathematics, Forthcoming 2021",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We present a reinforcement learning (RL) approach for robust optimisation of\nrisk-aware performance criteria. To allow agents to express a wide variety of\nrisk-reward profiles, we assess the value of a policy using rank dependent\nexpected utility (RDEU). RDEU allows the agent to seek gains, while\nsimultaneously protecting themselves against downside risk. To robustify\noptimal policies against model uncertainty, we assess a policy not by its\ndistribution, but rather, by the worst possible distribution that lies within a\nWasserstein ball around it. Thus, our problem formulation may be viewed as an\nactor/agent choosing a policy (the outer problem), and the adversary then\nacting to worsen the performance of that strategy (the inner problem). We\ndevelop explicit policy gradient formulae for the inner and outer problems, and\nshow its efficacy on three prototypical financial problems: robust portfolio\nallocation, optimising a benchmark, and statistical arbitrage.\n"
    },
    {
        "paper_id": 2108.10418,
        "authors": "Chinonso Nwankwo, Weizhong Dai",
        "title": "On the Efficiency of 5(4) RK-Embedded Pairs with High Order Compact\n  Scheme and Robin Boundary Condition for Options Valuation",
        "comments": "This is a preprint of an article published in Japan Journal of\n  Industrial and Applied Mathematics (JJIAM). The final authenticated version\n  is available online at: https://doi.org/10.1007/s13160-022-00507-0",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When solving the American options with or without dividends, numerical\nmethods often obtain lower convergence rates if further treatment is not\nimplemented even using high-order schemes. In this article, we present a fast\nand explicit fourth-order compact scheme for solving the free boundary options.\nIn particular, the early exercise features with the asset option and option\nsensitivity are computed based on a coupled of nonlinear PDEs with fixed\nboundaries for which a high order analytical approximation is obtained.\nFurthermore, we implement a new treatment at the left boundary by introducing a\nthird-order Robin boundary condition. Rather than computing the optimal\nexercise boundary from the analytical approximation, we simply obtain it from\nthe asset option based on the linear relationship at the left boundary. As\nsuch, a high order convergence rate can be achieved. We validate by examples\nthat the improvement at the left boundary yields a fourth-order convergence\nrate without further implementation of mesh refinement, Rannacher\ntime-stepping, and/or smoothing of the initial condition. Furthermore, we\nextensively compare, the performance of our present method with several 5(4)\nRunge-Kutta pairs and observe that Dormand and Prince and Bogacki and Shampine\n5(4) pairs are faster and provide more accurate numerical solutions. Based on\nnumerical results and comparison with other existing methods, we can validate\nthat the present method is very fast and provides more accurate solutions with\nvery coarse grids.\n"
    },
    {
        "paper_id": 2108.10504,
        "authors": "Qi Feng, Man Luo, Zhaoyu Zhang",
        "title": "Deep Signature FBSDE Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deep signature/log-signature FBSDE algorithm to solve\nforward-backward stochastic differential equations (FBSDEs) with state and path\ndependent features. By incorporating the deep signature/log-signature\ntransformation into the recurrent neural network (RNN) model, our algorithm\nshortens the training time, improves the accuracy, and extends the time horizon\ncomparing to methods in the existing literature. Moreover, our algorithms can\nbe applied to a wide range of applications such as state and path dependent\noption pricing involving high-frequency data, model ambiguity, and stochastic\ngames, which are linked to parabolic partial differential equations (PDEs), and\npath-dependent PDEs (PPDEs). Lastly, we also derive the convergence analysis of\nthe deep signature/log-signature FBSDE algorithm.\n"
    },
    {
        "paper_id": 2108.10982,
        "authors": "Lin William Cong, Charles M. C. Lee, Yuanyu Qu, Tao Shen",
        "title": "Financing Entrepreneurship and Innovation in China",
        "comments": "Stanford University Graduate School of Business Research Paper No.\n  18-46",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study reports on the current state-of-affairs in the funding of\nentrepreneurship and innovations in China and provides a broad survey of\nacademic findings on the subject. We also discuss the implications of these\nfindings for public policies governing the Chinese financial system,\nparticularly regulations governing the initial public offering (IPO) process.\nWe also identify and discuss promising areas for future research.\n"
    },
    {
        "paper_id": 2108.10984,
        "authors": "Lin William Cong, Xi Li, Ke Tang, Yang Yang",
        "title": "Crypto Wash Trading",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3530220",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce systematic tests exploiting robust statistical and behavioral\npatterns in trading to detect fake transactions on 29 cryptocurrency exchanges.\nRegulated exchanges feature patterns consistently observed in financial markets\nand nature; abnormal first-significant-digit distributions, size rounding, and\ntransaction tail distributions on unregulated exchanges reveal rampant\nmanipulations unlikely driven by strategy or exchange heterogeneity. We\nquantify the wash trading on each unregulated exchange, which averaged over 70%\nof the reported volume. We further document how these fabricated volumes\n(trillions of dollars annually) improve exchange ranking, temporarily distort\nprices, and relate to exchange characteristics (e.g., age and userbase), market\nconditions, and regulation.\n"
    },
    {
        "paper_id": 2108.11141,
        "authors": "Ludovic Gouden\\`ege and Andrea Molent and Antonino Zanette",
        "title": "Moving average options: Machine Learning and Gauss-Hermite quadrature\n  for a double non-Markovian problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Evaluating moving average options is a tough computational challenge for the\nenergy and commodity market as the payoff of the option depends on the prices\nof a certain underlying observed on a moving window so, when a long window is\nconsidered, the pricing problem becomes high dimensional. We present an\nefficient method for pricing Bermudan style moving average options, based on\nGaussian Process Regression and Gauss-Hermite quadrature, thus named GPR-GHQ.\nSpecifically, the proposed algorithm proceeds backward in time and, at each\ntime-step, the continuation value is computed only in a few points by using\nGauss-Hermite quadrature, and then it is learned through Gaussian Process\nRegression. We test the proposed approach in the Black-Scholes model, where the\nGPR-GHQ method is made even more efficient by exploiting the positive\nhomogeneity of the continuation value, which allows one to reduce the problem\nsize. Positive homogeneity is also exploited to develop a binomial Markov\nchain, which is able to deal efficiently with medium-long windows. Secondly, we\ntest GPR-GHQ in the Clewlow-Strickland model, the reference framework for\nmodeling prices of energy commodities. Finally, we consider a challenging\nproblem which involves double non-Markovian feature, that is the rough-Bergomi\nmodel. In this case, the pricing problem is even harder since the whole history\nof the volatility process impacts the future distribution of the process. The\nmanuscript includes a numerical investigation, which displays that GPR-GHQ is\nvery accurate and it is able to handle options with a very long window, thus\novercoming the problem of high dimensionality.\n"
    },
    {
        "paper_id": 2108.11177,
        "authors": "Federico Vaccari",
        "title": "Influential News and Policy-making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is believed that interventions that change the media's costs of\nmisreporting can increase the information provided by media outlets. This paper\nanalyzes the validity of this claim and the welfare implications of those types\nof interventions that affect misreporting costs. I study a model of\ncommunication between an uninformed voter and a media outlet that knows the\nquality of two competing candidates. The alternatives available to the voter\nare endogenously championed by the two candidates. I show that higher costs may\nlead to more misreporting and persuasion, whereas low costs result in full\nrevelation; interventions that increase misreporting costs never harm the\nvoter, but those that do so slightly may be wasteful of public resources. I\nconclude that intuitions derived from the interaction between the media and\nvoters, without incorporating the candidates' strategic responses to the media\nenvironment, do not capture properly the effects of these types of\ninterventions.\n"
    },
    {
        "paper_id": 2108.11287,
        "authors": "Ivan Kucherkov, Mattia Masolletti",
        "title": "Prevention of Terrorist Crimes in the North Caucasus Region",
        "comments": "10 pages. Key words: terrorism, prevention, North Caucasus,\n  counter-terrorism. JEL codes: K-14",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The relevance of the topic is dictated by the fact that in recent decades,\nthe threat to international security emanating from terrorism has increased\nmany times. Terrorist organizations have become full-fledged subjects of\npolitics on a par with political parties. In addition, enormous power and\nresources are concentrated in the hands of terrorist groups. Terrorist activity\nhas become the usual way of leading a political struggle, expressing social\nprotest. In addition, terrorism has become a tool in economic competition. Each\nterrorist action entails more and more human casualties. It breeds instability,\nfear, hatred, and distrust in society. The authors pay special attention to\ncounter-terrorism activities in the North Caucasus Region.\n"
    },
    {
        "paper_id": 2108.11318,
        "authors": "Liang Zhao, Wei Li, Ruihan Bao, Keiko Harimoto, YunfangWu and Xu Sun",
        "title": "Long-term, Short-term and Sudden Event: Trading Volume Movement\n  Prediction with Graph-based Multi-view Modeling",
        "comments": "Accepted as a main track paper by IJCAI 21",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading volume movement prediction is the key in a variety of financial\napplications. Despite its importance, there is few research on this topic\nbecause of its requirement for comprehensive understanding of information from\ndifferent sources. For instance, the relation between multiple stocks, recent\ntransaction data and suddenly released events are all essential for\nunderstanding trading market. However, most of the previous methods only take\nthe fluctuation information of the past few weeks into consideration, thus\nyielding poor performance. To handle this issue, we propose a graphbased\napproach that can incorporate multi-view information, i.e., long-term stock\ntrend, short-term fluctuation and sudden events information jointly into a\ntemporal heterogeneous graph. Besides, our method is equipped with deep\ncanonical analysis to highlight the correlations between different perspectives\nof fluctuation for better prediction. Experiment results show that our method\noutperforms strong baselines by a large margin.\n"
    },
    {
        "paper_id": 2108.11344,
        "authors": "Sarwar J. Minar",
        "title": "Refugees and Host State Security: An Empirical Investigation of Rohingya\n  Refuge in Bangladesh",
        "comments": "28 pages, 78th Annual Conference of the Midwest Political Science\n  Association",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Conventional wisdom suggests that large-scale refugees pose security threats\nto the host community or state. With massive influx of Rohingyas in Bangladesh\nin 2017 resulting a staggering total of 1.6 million Rohingyas, a popular\ndiscourse emerged that Bangladesh would face severe security threats. This\narticle investigates the security experience of Bangladesh in case of Rohingya\ninflux over a three-year period, August 2017 to August 2020. The research\nquestion I intend to address is, 'has Bangladesh experienced security threat\ndue to massive Rohingya influx?' If so in what ways? I test four security\nthreat areas: societal security, economic security, internal security, and\npublic security. I have used newspaper content analysis over past three years\nalong with interview data collected from interviewing local people in coxs\nbazar area where the Rohingya camps are located. To assess if the threats are\nlow level, medium level, or high level, I investigated both the frequency of\nreports and the way they are interpreted. I find that Bangladesh did not\nexperience any serious security threats over the last three years. There are\nsome criminal activities and offenses, but these are only low-level security\nthreat at best. My research presents empirical evidence that challenges\nconventional assertions that refugees are security threats or challenges to the\nhost states.\n"
    },
    {
        "paper_id": 2108.11545,
        "authors": "Marisa Miraldo, Carol Propper and Christiern Rose",
        "title": "Identification of Peer Effects using Panel Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide new identification results for panel data models with peer effects\noperating through unobserved individual heterogeneity. The results apply for\ngeneral network structures governing peer interactions and allow for correlated\neffects. Identification hinges on a conditional mean restriction requiring\nexogenous mobility of individuals between groups over time. We apply our method\nto surgeon-hospital-year data to study take-up of keyhole surgery for cancer,\nfinding a positive effect of the average individual heterogeneity of other\nsurgeons practicing in the same hospital\n"
    },
    {
        "paper_id": 2108.11631,
        "authors": "Marko Corn, Nejc Ro\\v{z}man",
        "title": "Unihedge -- A decentralized market prediction platform",
        "comments": "12 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Unihedge is a decentralized platform for prediction markets with a novel\napproach. Using Harberger Tax (HTAX) economic policies a new type of prediction\nmarket, named HTAX prediction market, was build. HTAX prediction market\nderivates from Dynamic PariMutuel (DPM) type of prediction markets thus\noffering its users an unlimited liquidity for any preferred time horizon. It\ntries to solve some problems of DPM by introducing a new incentive mechanism to\nsupport early information incorporation and a protection against share\nreadjustment for hedgers. In the paper also implementation of platform on\nEthereum Virtual Machine (EVM) is presented with the usage of Decentralized\nExchange (DEX) as an price discovery mechanism for prediction market\nresolutions.\n"
    },
    {
        "paper_id": 2108.11732,
        "authors": "Vitaly Khrustalev, Mattia Masolletti",
        "title": "Ensuring Transport Security; Features of Legal Regulation",
        "comments": "10 pages. Key words: transport, transport security. JEL codes: K-1;\n  K-14; K-42",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article analyzes the legal framework regulating the legal provision of\ntransport security in Russia. Special attention is paid to the role of\nprosecutor's supervision in the field of prevention of crimes in transport.\n"
    },
    {
        "paper_id": 2108.11755,
        "authors": "HyeonJun Kim",
        "title": "Market Crash Prediction Model for Markets in A Rational Bubble",
        "comments": "5 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Renowned method of log-periodic power law(LPPL) is one of the few ways that a\nfinancial market crash could be predicted. Alongside with LPPL, this paper\npropose a novel method of stock market crash using white box model derived from\nsimple assumptions about the state of rational bubble. By applying this model\nto Dow Jones Index and Bitcoin market price data, it is shown that the model\nsuccessfully predicts some major crashes of both markets, implying the high\nsensitivity and generalization abilities of the model.\n"
    },
    {
        "paper_id": 2108.11892,
        "authors": "John C. Stevenson",
        "title": "Dynamics of Wealth Inequality in Simple Artificial Societies",
        "comments": "12 pages, 2 tables, 5 figures. Presented at the Social Simulation\n  Conference 2021. arXiv admin note: substantial text overlap with\n  arXiv:2101.09817",
        "journal-ref": "Advances in Social Simulation, (2021) Chapter 13, Ed. Czupryna M,\n  Kamiriski B. Springer Nature, Switzerland AG",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A simple generative model of a foraging society generates significant wealth\ninequalities from identical agents on an equal opportunity landscape. These\ninequalities arise in both equilibrium and non-equilibrium regimes with some\nsocieties essentially never reaching equilibrium. Reproduction costs mitigate\ninequality beyond their affect on intrinsic growth rate. The highest levels of\ninequality are found during non-equilibrium regimes. Inequality in dynamic\nregimes is driven by factors different than those driving steady state\ninequality. Evolutionary pressures drive the intrinsic growth rate as high as\npossible, leading to a tragedy of the commons.\n"
    },
    {
        "paper_id": 2108.11915,
        "authors": "Wolfgang Karl H\\\"ardle, Rainer Schulz, Taojun Sie",
        "title": "Cooling Measures and Housing Wealth: Evidence from Singapore",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Excessive house price growth was at the heart of the financial crisis in\n2007/08. Since then, many countries have added cooling measures to their\nregulatory frameworks. It has been found that these measures can indeed control\nprice growth, but no one has examined whether this has adverse consequences for\nthe housing wealth distribution. We examine this for Singapore, which started\nin 2009 to target price growth over ten rounds in total. We find that welfare\nfrom housing wealth in the last round might not be higher than before 2009.\nThis depends on the deflator used to convert nominal into real prices.\nIrrespective of the deflator, we can reject that welfare increased\nmonotonically over the different rounds.\n"
    },
    {
        "paper_id": 2108.11921,
        "authors": "Li Guo, Wolfgang Karl H\\\"ardle, Yubo Tao",
        "title": "A Time-Varying Network for Cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies return cross-predictability and technological similarity\nyield information on risk propagation and market segmentation. To investigate\nthese effects, we build a time-varying network for cryptocurrencies, based on\nthe evolution of return cross-predictability and technological similarities. We\ndevelop a dynamic covariate-assisted spectral clustering method to consistently\nestimate the latent community structure of cryptocurrencies network that\naccounts for both sets of information. We demonstrate that investors can\nachieve better risk diversification by investing in cryptocurrencies from\ndifferent communities. A cross-sectional portfolio that implements an\ninter-crypto momentum trading strategy earns a 1.08% daily return. By\ndissecting the portfolio returns on behavioral factors, we confirm that our\nresults are not driven by behavioral mechanisms.\n"
    },
    {
        "paper_id": 2108.11998,
        "authors": "Mikhail Zhitlukhin",
        "title": "Asymptotically optimal strategies in a diffusion approximation of a\n  repeated betting game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a diffusion approximation of a repeated game in which agents\nmake bets on outcomes of i.i.d. random vectors and their strategies are close\nto an asymptotically optimal strategy. This model can be interpreted as trading\nin an asset market with short-lived assets. We obtain sufficient conditions for\na strategy to maintain a strictly positive share of total wealth over the\ninfinite time horizon. For the game with two players, we find necessary and\nsufficient conditions for the wealth share process to be transient or recurrent\nin this model, and also in its generalization with Markovian regime switching.\n"
    },
    {
        "paper_id": 2108.12042,
        "authors": "Axel A. Araneda",
        "title": "Price modelling under generalized fractional Brownian motion",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Generalized fractional Brownian motion (gfBm) is a stochastic process\nthat acts as a generalization for both fractional, sub-fractional, and standard\nBrownian motion. Here we study its use as the main driver for price\nfluctuations, replacing the standard Brownian Brownian motion in the well-known\nBlack-Scholes model. By the derivation of the generalized fractional Ito's\nlemma and the related effective Fokker-Planck equation, we discuss its\napplication to both the option pricing problem valuing European options, and\nthe computation of Value-at-Risk and Expected Shortfall. Moreover, the option\nprices are computed for a CEV-type model driven by gfBm.\n"
    },
    {
        "paper_id": 2108.12542,
        "authors": "Mani Bayani",
        "title": "Robust PCA Synthetic Control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, I propose a five-step algorithm for synthetic control method\nfor comparative studies. My algorithm builds on the synthetic control model of\nAbadie et al., 2015 and the later model of Amjad et al., 2018. I apply all\nthree methods (robust PCA synthetic control, synthetic control, and robust\nsynthetic control) to answer the hypothetical question, what would have been\nthe per capita GDP of West Germany if it had not reunified with East Germany in\n1990? I then apply all three algorithms in two placebo studies. Finally, I\ncheck for robustness. This paper demonstrates that my method can outperform the\nrobust synthetic control model of Amjad et al., 2018 in placebo studies and is\nless sensitive to the weights of synthetic members than the model of Abadie et\nal., 2015.\n"
    },
    {
        "paper_id": 2108.12598,
        "authors": "Pedro Polvora and Daniel Sevcovic",
        "title": "Utility indifference Option Pricing Model with a Non-Constant\n  Risk-Aversion under Transaction Costs and Its Numerical Approximation",
        "comments": null,
        "journal-ref": "J. Risk Financial Manag. 2021, 14(9), 399",
        "doi": "10.3390/jrfm14090399",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our goal is to analyze the system of Hamilton-Jacobi-Bellman equations\narising in derivative securities pricing models. The European style of an\noption price is constructed as a difference of the certainty equivalents to the\nvalue functions solving the system of HJB equations. We introduce the\ntransformation method for solving the penalized nonlinear partial differential\nequation. The transformed equation involves possibly non-constant the risk\naversion function containing the negative ratio between the second and first\nderivatives of the utility function. Using comparison principles we derive\nuseful bounds on the option price. We also propose a finite difference\nnumerical discretization scheme with some computational examples.\n"
    },
    {
        "paper_id": 2108.12633,
        "authors": "Giuseppe Lopomo, Luca Rigotti, and Chris Shannon",
        "title": "Uncertainty in Mechanism Design",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies the design of mechanisms that are robust to\nmisspecification. We introduce a novel notion of robustness that connects a\nvariety of disparate approaches and study its implications in a wide class of\nmechanism design problems. This notion is quantifiable, allowing us to\nformalize and answer comparative statics questions relating the nature and\ndegree of misspecification to sharp predictions regarding features of feasible\nmechanisms. This notion also has a behavioral foundation which reflects the\nperception of ambiguity, thus allowing the degree of misspecification to emerge\nendogenously. In a number of standard settings, robustness to arbitrarily small\namounts of misspecification generates a discontinuity in the set of feasible\nmechanisms and uniquely selects simple, ex post incentive compatible mechanisms\nsuch as second-price auctions. Robustness also sheds light on the value of\nprivate information and the prevalence of full or virtual surplus extraction.\n"
    },
    {
        "paper_id": 2108.12744,
        "authors": "Suguru Otani",
        "title": "Estimating Endogenous Coalitional Mergers: Merger Costs and\n  Assortativeness of Size and Specialization",
        "comments": "36 pages and its appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I present a structural empirical model of a one-sided one-to-many matching\nwith complementarities to quantify the effect of subsidy design on endogenous\nmerger matching. I investigate shipping mergers and consolidations in Japan in\n1964. At the time, 95 firms formed six large groups. I find that the existence\nof unmatched firms enables us to recover merger costs, and the importance of\ntechnological diversification varies across carrier and firm types. The\ncounterfactual simulations show that 20 \\% of government subsidy expenditures\ncould have been cut. The government could have possibly changed the equilibrium\nnumber of groups to between one and six.\n"
    },
    {
        "paper_id": 2108.1291,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, M\\\"ucahit Ayg\\\"un",
        "title": "Dual representations of quasiconvex compositions with applications to\n  systemic risk",
        "comments": "65 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the problem of finding dual representations for quasiconvex\nsystemic risk measures in financial mathematics, we study quasiconvex\ncompositions in an abstract infinite-dimensional setting. We calculate an\nexplicit formula for the penalty function of the composition in terms of the\npenalty functions of the ingredient functions. The proof makes use of a\nnonstandard minimax inequality (rather than equality as in the standard case)\nthat is available in the literature. In the second part of the paper, we apply\nour results in concrete probabilistic settings for systemic risk measures, in\nparticular, in the context of Eisenberg-Noe clearing model. We also provide\nnovel economic interpretations of the dual representations in these settings.\n"
    },
    {
        "paper_id": 2108.13072,
        "authors": "Paul Bilokon and David Finkelstein",
        "title": "Iterated and exponentially weighted moving principal component analysis",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The principal component analysis (PCA) is a staple statistical and\nunsupervised machine learning technique in finance. The application of PCA in a\nfinancial setting is associated with several technical difficulties, such as\nnumerical instability and nonstationarity. We attempt to resolve them by\nproposing two new variants of PCA: an iterated principal component analysis\n(IPCA) and an exponentially weighted moving principal component analysis\n(EWMPCA). Both variants rely on the Ogita-Aishima iteration as a crucial step.\n"
    },
    {
        "paper_id": 2108.13315,
        "authors": "Zhaojun Wang, Amanda M. Countryman, James J. Corbett, Mandana Saebi",
        "title": "Economic and environmental impacts of ballast water management on Small\n  Island Developing States and Least Developed Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Ballast Water Management Convention can decrease the introduction risk of\nharmful aquatic organisms and pathogens, yet the Convention increases shipping\ncosts and causes subsequent economic impacts. This paper examines whether the\nConvention generates disproportionate invasion risk reduction results and\neconomic impacts on Small Island Developing States (SIDS) and Least Developed\nCountries (LDCs). Risk reduction is estimated with an invasion risk assessment\nmodel based on a higher-order network, and the effects of the regulation on\nnational economies and trade are estimated with an integrated shipping cost and\ncomputable general equilibrium modeling framework. Then we use the Lorenz curve\nto examine if the regulation generates risk or economic inequality among\nregions. Risk reduction ratios of all regions (except Singapore) are above 99%,\nwhich proves the effectiveness of the Convention. The Gini coefficient of 0.66\nshows the inequality in risk changes relative to income levels among regions,\nbut risk reductions across all nations vary without particularly high risks for\nSIDS and LDCs than for large economies. Similarly, we reveal inequality in\neconomic impacts relative to income levels (the Gini coefficient is 0.58), but\nthere is no evidence that SIDS and LDCs are disproportionately impacted\ncompared to more developed regions. Most changes in GDP, real exports, and real\nimports of studied regions are minor (smaller than 0.1%). However, there are\nmore noteworthy changes for select sectors and trade partners including Togo,\nBangladesh, and Dominican Republic, whose exports may decrease for textiles and\nmetal and chemicals. We conclude the Convention decreases biological invasion\nrisk and does not generate disproportionate negative impacts on SIDS and LDCs.\n"
    },
    {
        "paper_id": 2108.13356,
        "authors": "Fabian Braesemann, Fabian Stephany, Ole Teutloff, Otto K\\\"assi, Mark\n  Graham, Vili Lehdonvirta",
        "title": "The global polarisation of remote work",
        "comments": "This is an Accepted Manuscript of an article published in the journal\n  PLOS ONE",
        "journal-ref": "PloS one, 17(10), e0274630 (2022)",
        "doi": "10.1371/journal.pone.0274630",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Covid-19 pandemic has led to the rise of remote work with consequences\nfor the global division of work. Remote work could connect labour markets, but\nit could also increase spatial polarisation. However, our understanding of the\ngeographies of remote work is limited. Specifically, does remote work bring\njobs to rural areas or is it concentrating in large cities, and how do skill\nrequirements affect competition for jobs and wages? We use data from a fully\nremote labour market - an online labour platform - to show that remote work is\npolarised along three dimensions. First, countries are globally divided: North\nAmerican, European, and South Asian remote workers attract most jobs, while\nmany Global South countries participate only marginally. Secondly, remote jobs\nare pulled to urban regions; rural areas fall behind. Thirdly, remote work is\npolarised along the skill axis: workers with in-demand skills attract\nprofitable jobs, while others face intense competition and obtain low wages.\nThe findings suggest that remote work is shaped by agglomerative forces, which\nare deepening the gap between urban and rural areas. To make remote work an\neffective tool for rural development, it needs to be embedded in local\nskill-building and labour market programmes.\n"
    },
    {
        "paper_id": 2108.135,
        "authors": "Marlon Moresco and Marcelo Brutti Righi",
        "title": "On the link between monetary and star-shaped risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, Castagnoli et al. (2021) introduce the class of star-shaped risk\nmeasures as a generalization of convex and coherent ones, proving that there is\na representation as the pointwise minimum of some family composed by convex\nrisk measures. Concomitantly, Jia et al. (2020) prove a similar representation\nresult for monetary risk measures, which are more general than star-shaped\nones. Then, there is a question on how both classes are connected. In this\nletter, we provide an answer by casting light on the importance of the\nacceptability of 0, which is linked to the property of normalization. We then\nshow that under mild conditions, a monetary risk measure is only a translation\naway from star-shapedness.\n"
    },
    {
        "paper_id": 2108.13506,
        "authors": "Mark Whitmeyer",
        "title": "Submission Fees in Risk-Taking Contests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates stochastic continuous time contests with a twist: the\ndesigner requires that contest participants incur some cost to submit their\nentries. When the designer wishes to maximize the (expected) performance of the\ntop performer, a strictly positive submission fee is optimal. When the designer\nwishes to maximize total (expected) performance, either the highest submission\nfee or the lowest submission fee is optimal.\n"
    },
    {
        "paper_id": 2108.13552,
        "authors": "Fernando Alarid-Escudero, Eline M. Krijkamp, Eva A. Enns, Alan Yang,\n  M.G. Myriam Hunink, Petros Pechlivanoglou, Hawre Jalal",
        "title": "A Tutorial on Time-Dependent Cohort State-Transition Models in R using a\n  Cost-Effectiveness Analysis Example",
        "comments": "34 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2001.07824",
        "journal-ref": "Medical Decision Making, 2022",
        "doi": "10.1177/0272989X221121747",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an introductory tutorial, we illustrated building cohort state-transition\nmodels (cSTMs) in R, where the state transitions probabilities were constant\nover time. However, in practice, many cSTMs require transitions, rewards, or\nboth to vary over time (time-dependent). This tutorial illustrates adding two\ntypes of time-dependency using a previously published cost-effectiveness\nanalysis of multiple strategies as an example. The first is simulation-time\ndependence, which allows for the transition probabilities to vary as a function\nof time as measured since the start of the simulation (e.g., varying\nprobability of death as the cohort ages). The second is state-residence time\ndependence, allowing for history by tracking the time spent in any particular\nhealth state using tunnel states. We use these time-dependent cSTMs to conduct\ncost-effectiveness and probabilistic sensitivity analyses. We also obtain\nvarious epidemiological outcomes of interest from the outputs generated from\nthe cSTM, such as survival probability and disease prevalence, often used for\nmodel calibration and validation. We present the mathematical notation first,\nfollowed by the R code to execute the calculations. The full R code is provided\nin a public code repository for broader implementation.\n"
    },
    {
        "paper_id": 2108.13671,
        "authors": "David Bartram",
        "title": "Is happiness u-shaped in age everywhere? A methodological\n  reconsideration for Europe",
        "comments": "17 pages, 4 tables, 2 figures; submitted to National Institute\n  Economic Review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A recent contribution to research on age and well-being (Blanchflower 2021)\nfound that the impact of age on happiness is \"u-shaped\" virtually everywhere:\nhappiness declines towards middle age and subsequently rises, in almost all\ncountries. This paper evaluates that finding for European countries,\nconsidering whether it is robust to alternative methodological approaches. The\nanalysis here excludes control variables that are affected by age (noting that\nthose variable are not themselves antecedents of age) and uses data from the\nentire adult age range (rather than using data only from respondents younger\nthan 70). I also explore the relationship via models that do not impose a\nquadratic functional form. The paper shows that these alternate approaches do\nnot lead us to perceive a u-shape \"everywhere\": u-shapes are evident for some\ncountries, but for others the pattern is quite different.\n"
    },
    {
        "paper_id": 2109.00064,
        "authors": "Alexander M.G. Cox, Sigrid K\\\"allblad, Martin Larsson and Sara\n  Svaluto-Ferro",
        "title": "Controlled Measure-Valued Martingales: a Viscosity Solution Approach",
        "comments": "56 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of stochastic control problems where the state process is\na probability measure-valued process satisfying an additional martingale\ncondition on its dynamics, called measure-valued martingales (MVMs). We\nestablish the `classical' results of stochastic control for these problems:\nspecifically, we prove that the value function for the problem can be\ncharacterised as the unique solution to the Hamilton-Jacobi-Bellman equation in\nthe sense of viscosity solutions. In order to prove this result, we exploit\nstructural properties of the MVM processes. Our results also include an\nappropriate version of It\\^o's lemma for controlled MVMs. We also show how\nproblems of this type arise in a number of applications, including\nmodel-independent derivatives pricing, the optimal Skorokhod embedding problem,\nand two player games with asymmetric information.\n"
    },
    {
        "paper_id": 2109.001,
        "authors": "Snehalkumar (Neil) S. Gaikwad, Shankar Iyer, Dalton Lunga, Elizabeth\n  Bondi",
        "title": "Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping:\n  Harnessing Human-Machine Intelligence for High-Stake Public Policy and\n  Resilience Planning",
        "comments": "The proceedings of the 2nd Data-driven Humanitarian Mapping workshop\n  at the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.\n  August 15th, 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 2021. Despite\nthese growing perils, there remains a notable paucity of data science research\nto scientifically inform equitable public policy decisions for improving the\nlivelihood of at-risk populations. Scattered data science efforts exist to\naddress these challenges, but they remain isolated from practice and prone to\nalgorithmic harms concerning lack of privacy, fairness, interpretability,\naccountability, transparency, and ethics. Biases in data-driven methods carry\nthe risk of amplifying inequalities in high-stakes policy decisions that impact\nthe livelihood of millions of people. Consequently, proclaimed benefits of\ndata-driven innovations remain inaccessible to policymakers, practitioners, and\nmarginalized communities at the core of humanitarian actions and global\ndevelopment. To help fill this gap, we propose the Data-driven Humanitarian\nMapping Research Program, which focuses on developing novel data science\nmethodologies that harness human-machine intelligence for high-stakes public\npolicy and resilience planning.\n  The proceedings of the 2nd Data-driven Humanitarian Mapping workshop at the\n27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. August 15th,\n2021\n"
    },
    {
        "paper_id": 2109.0013,
        "authors": "Natalia A. Van Heerden, Juan B. Cabral, Nadia Luczywo",
        "title": "Evaluation of the importance of criteria for the selection of\n  cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, cryptocurrencies have gone from an obscure niche to a\nprominent place, with investment in these assets becoming increasingly popular.\nHowever, cryptocurrencies carry a high risk due to their high volatility. In\nthis paper, criteria based on historical cryptocurrency data are defined in\norder to characterize returns and risks in different ways, in short time\nwindows (7 and 15 days); then, the importance of criteria is analyzed by\nvarious methods and their impact is evaluated. Finally, the future plan is\nprojected to use the knowledge obtained for the selection of investment\nportfolios by applying multi-criteria methods.\n"
    },
    {
        "paper_id": 2109.00148,
        "authors": "Hubeyb Gurdogan, Alec Kercheval",
        "title": "Multi Anchor Point Shrinkage for the Sample Covariance Matrix (Extended\n  Version)",
        "comments": "60 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Portfolio managers faced with limited sample sizes must use factor models to\nestimate the covariance matrix of a high-dimensional returns vector. For the\nsimplest one-factor market model, success rests on the quality of the estimated\nleading eigenvector \"beta\".\n  When only the returns themselves are observed, the practitioner has available\nthe \"PCA\" estimate equal to the leading eigenvector of the sample covariance\nmatrix. This estimator performs poorly in various ways. To address this problem\nin the high-dimension, limited sample size asymptotic regime and in the context\nof estimating the minimum variance portfolio, Goldberg, Papanicolau, and\nShkolnik developed a shrinkage method (the \"GPS estimator\") that improves the\nPCA estimator of beta by shrinking it toward a constant target unit vector.\n  In this paper we continue their work to develop a more general framework of\nshrinkage targets that allows the practitioner to make use of further\ninformation to improve the estimator. Examples include sector separation of\nstock betas, and recent information from prior estimates. We prove some precise\nstatements and illustrate the resulting improvements over the GPS estimator\nwith some numerical experiments.\n"
    },
    {
        "paper_id": 2109.00297,
        "authors": "Jose Ricardo Bezerra Nogueira",
        "title": "Nota Sobre Algumas Interpretacoes da Teoria de Tributacao Otima",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This note discusses some aspects of interpretations of the theory of optimal\ntaxation presented in recent works on the Brazilian tax system.\n"
    },
    {
        "paper_id": 2109.00306,
        "authors": "Hampus Engsner, Filip Lindskog, Julie Thoegersen",
        "title": "Multiple-prior valuation of cash flows subject to capital requirements",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study market-consistent valuation of liability cash flows motivated by\ncurrent regulatory frameworks for the insurance industry. Building on the\ntheory on multiple-prior optimal stopping we propose a valuation functional\nwith sound economic properties that applies to any liability cash flow. Whereas\na replicable cash flow is assigned the market value of the replicating\nportfolio, a cash flow that is not fully replicable is assigned a value which\nis the sum of the market value of a replicating portfolio and a positive\nmargin. The margin is a direct consequence of considering a hypothetical\ntransfer of the liability cash flow from an insurance company to an empty\ncorporate entity set up with the sole purpose to manage the liability run-off,\nsubject to repeated capital requirements, and considering the valuation of this\nentity from the owner's perspective taking model uncertainty into account.\nAiming for applicability, we consider a detailed insurance application and\nexplain how the optimisation problems over sets of probability measures can be\ncast as simpler optimisation problems over parameter sets corresponding to\nparameterised density processes appearing in applications.\n"
    },
    {
        "paper_id": 2109.00433,
        "authors": "Marcos Escobar-Anel, Maximilian Gollart, Rudi Zagst",
        "title": "Closed-form portfolio optimization under GARCH models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops the first closed-form optimal portfolio allocation\nformula for a spot asset whose variance follows a GARCH(1,1) process. We\nconsider an investor with constant relative risk aversion (CRRA) utility who\nwants to maximize the expected utility from terminal wealth under a Heston and\nNandi (2000) GARCH (HN-GARCH) model. We obtain closed formulas for the optimal\ninvestment strategy, the value function and the optimal terminal wealth. We\nfind the optimal strategy is independent of the development of the risky asset,\nand the solution converges to that of a continuous-time Heston stochastic\nvolatility model, albeit under additional conditions. For a daily trading\nscenario, the optimal solutions are quite robust to variations in the\nparameters, while the numerical wealth equivalent loss (WEL) analysis shows\ngood performance of the Heston solution, with a quite inferior performance of\nthe Merton solution.\n"
    },
    {
        "paper_id": 2109.00435,
        "authors": "Snehalkumar (Neil) S. Gaikwad, Shankar Iyer, Dalton Lunga, Yu-Ru Lin",
        "title": "Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping:\n  Harnessing Human-Machine Intelligence for High-Stake Public Policy and\n  Resilience Planning",
        "comments": "The proceedings of the 1st Data-driven Humanitarian Mapping workshop\n  at the 26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 2021 . Despite\nthese growing perils, there remains a notable paucity of data science research\nto scientifically inform equitable public policy decisions for improving the\nlivelihood of at-risk populations. Scattered data science efforts exist to\naddress these challenges, but they remain isolated from practice and prone to\nalgorithmic harms concerning lack of privacy, fairness, interpretability,\naccountability, transparency, and ethics. Biases in data-driven methods carry\nthe risk of amplifying inequalities in high-stakes policy decisions that impact\nthe livelihood of millions of people. Consequently, proclaimed benefits of\ndata-driven innovations remain inaccessible to policymakers, practitioners, and\nmarginalized communities at the core of humanitarian actions and global\ndevelopment. To help fill this gap, we propose the Data-driven Humanitarian\nMapping Research Program, which focuses on developing novel data science\nmethodologies that harness human-machine intelligence for high-stakes public\npolicy and resilience planning.\n  The proceedings of the 1st Data-driven Humanitarian Mapping workshop at the\n26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, August 24th,\n2020.\n"
    },
    {
        "paper_id": 2109.00446,
        "authors": "Hamed Amini, Maxim Bichuch and Zachary Feinstein",
        "title": "Decentralized Payment Clearing using Blockchain and Optimal Bidding",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we construct a decentralized clearing mechanism which\nendogenously and automatically provides a claims resolution procedure. This\nmechanism can be used to clear a network of obligations through blockchain. In\nparticular, we investigate default contagion in a network of smart contracts\ncleared through blockchain. In so doing, we provide an algorithm which\nconstructs the blockchain so as to guarantee the payments can be verified and\nthe miners earn a fee. We, additionally, consider the special case in which the\nblocks have unbounded capacity to provide a simple equilibrium clearing\ncondition for the terminal net worths; existence and uniqueness are proven for\nthis system. Finally, we consider the optimal bidding strategies for each firm\nin the network so that all firms are utility maximizers with respect to their\nterminal wealths. We first look for a mixed Nash equilibrium bidding\nstrategies, and then also consider Pareto optimal bidding strategies. The\nimplications of these strategies, and more broadly blockchain, on systemic risk\nare considered.\n"
    },
    {
        "paper_id": 2109.00453,
        "authors": "Elmar Zozmann, Mirjam Helena Eerma, Dylan Manning, Gro Lill {\\O}kland,\n  Citlali Rodriguez del Angel, Paul E. Seifert, Johanna Winkler, Alfredo Zamora\n  Blaumann, Seyedsaeed Hosseinioun, Leonard G\\\"oke, Mario Kendziorski and\n  Christian von Hirschhausen",
        "title": "The Potential of Sufficiency Measures to Achieve a Fully Renewable\n  Energy System -- A case study for Germany",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper provides energy system-wide estimates of the effects sufficiency\nmeasures in different sectors can have on energy supply and system costs. In\ndistinction to energy efficiency, we define sufficiency as behavioral changes\nto reduce useful energy without significantly reducing utility, for example by\nadjusting thermostats. By reducing demand, sufficiency measures are a\npotentially decisive but seldomly considered factor to support the\ntransformation towards a decarbonized energy system. Therefore, this paper\naddresses the following question: What is the potential of sufficiency measures\nand what is their impacts on the supply side of a 100% renewable energy system?\nFor this purpose, an extensive literature review is conducted to obtain\nestimates for the effects of different sufficiency measures on final energy\ndemand in Germany. Afterwards, the impact of these measures on the supply side\nand system costs is quantified using a bottom-up planning model of a renewable\nenergy system. Results indicate that final energy could be reduced by up to\n20.5% and as a result cost reduction between 11.3% to 25.6% are conceivable.\nThe greatest potential for sufficiency measures was identified in the heating\nsector.\n"
    },
    {
        "paper_id": 2109.00643,
        "authors": "Michael J. Roberts, Sisi Zhang, Eleanor Yuan, James Jones, Matthias\n  Fripp",
        "title": "Using Temperature Sensitivity to Estimate Shiftable Electricity Demand:\n  Implications for power system investments and climate change",
        "comments": "23 pages, plus 16 pages supplement, 4 figures, 1 table, plus\n  supplementary tables and figures",
        "journal-ref": null,
        "doi": "10.1016/j.isci.2022.104940",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Growth of intermittent renewable energy and climate change make it\nincreasingly difficult to manage electricity demand variability. Centralized\nstorage can help but is costly. An alternative is to shift demand. Cooling and\nheating demands are substantial and can be economically shifted using thermal\nstorage. To estimate what thermal storage, employed at scale, might do to\nreshape electricity loads, we pair fine-scale weather data with hourly\nelectricity use to estimate the share of temperature-sensitive demand across 31\nregions that span the continental United States. We then show how much\nvariability can be reduced by shifting temperature-sensitive loads, with and\nwithout improved transmission between regions. We find that approximately three\nquarters of within-day, within-region demand variability can be eliminated by\nshifting just half of temperature-sensitive demand. The variability-reducing\nbenefits of shifting temperature-sensitive demand complement those gained from\nimproved interregional transmission, and greatly mitigate the challenge of\nserving higher peaks under climate change.\n"
    },
    {
        "paper_id": 2109.00923,
        "authors": "Siddarth Srinivasan, Jamie Morgenstern",
        "title": "Auctions and Peer Prediction for Academic Peer Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Peer reviewed publications are considered the gold standard in certifying and\ndisseminating ideas that a research community considers valuable. However, we\nidentify two major drawbacks of the current system: (1) the overwhelming demand\nfor reviewers due to a large volume of submissions, and (2) the lack of\nincentives for reviewers to participate and expend the necessary effort to\nprovide high-quality reviews. In this work, we adopt a mechanism-design\napproach to propose improvements to the peer review process, tying together the\npaper submission and review processes and simultaneously incentivizing\nhigh-quality submissions and reviews. In the submission stage, authors\nparticipate in a VCG auction for review slots by submitting their papers along\nwith a bid that represents their expected value for having their paper\nreviewed. For the reviewing stage, we propose a novel peer prediction mechanism\n(H-DIPP) building on recent work in the information elicitation literature,\nwhich incentivizes participating reviewers to provide honest and effortful\nreviews. The revenue raised in the submission stage auction is used to pay\nreviewers based on the quality of their reviews in the reviewing stage.\n"
    },
    {
        "paper_id": 2109.00983,
        "authors": "Dat Thanh Tran, Juho Kanniainen, Moncef Gabbouj, Alexandros Iosifidis",
        "title": "Bilinear Input Normalization for Neural Networks in Financial\n  Forecasting",
        "comments": "1 figure, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data normalization is one of the most important preprocessing steps when\nbuilding a machine learning model, especially when the model of interest is a\ndeep neural network. This is because deep neural network optimized with\nstochastic gradient descent is sensitive to the input variable range and prone\nto numerical issues. Different than other types of signals, financial\ntime-series often exhibit unique characteristics such as high volatility,\nnon-stationarity and multi-modality that make them challenging to work with,\noften requiring expert domain knowledge for devising a suitable processing\npipeline. In this paper, we propose a novel data-driven normalization method\nfor deep neural networks that handle high-frequency financial time-series. The\nproposed normalization scheme, which takes into account the bimodal\ncharacteristic of financial multivariate time-series, requires no expert\nknowledge to preprocess a financial time-series since this step is formulated\nas part of the end-to-end optimization process. Our experiments, conducted with\nstate-of-the-arts neural networks and high-frequency data from two large-scale\nlimit order books coming from the Nordic and US markets, show significant\nimprovements over other normalization techniques in forecasting future stock\nprice dynamics.\n"
    },
    {
        "paper_id": 2109.0103,
        "authors": "Gero Junike and Konstantin Pankrashkin",
        "title": "Precise option pricing by the COS method--How to choose the truncation\n  range",
        "comments": "Corrected typos in particular in Eq. (14)",
        "journal-ref": "Applied Mathematics and Computation, 421, 126935 (2022)",
        "doi": "10.1016/j.amc.2022.126935",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Fourier cosine expansion (COS) method is used for pricing European\noptions numerically very fast. To apply the COS method, a truncation range for\nthe density of the log-returns need to be provided. Using Markov's inequality,\nwe derive a new formula to obtain the truncation range and prove that the range\nis large enough to ensure convergence of the COS method within a predefined\nerror tolerance. We also show by several examples that the classical approach\nto determine the truncation range by cumulants may lead to serious mispricing.\nUsually, the computational time of the COS method is of similar magnitude in\nboth cases.\n"
    },
    {
        "paper_id": 2109.01031,
        "authors": "Diego Ferraro, Daniela Blanco, Sebasti\\'an Pessah and Rodrigo Castro",
        "title": "Land use change in agricultural systems: an integrated ecological-social\n  simulation model of farmer decisions and cropping system performance based on\n  a cellular automata approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.18564/jasss.4772",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Agricultural systems experience land-use changes that are driven by\npopulation growth and intensification of technological inputs. This results in\nland-use and cover change (LUCC) dynamics representing a complex landscape\ntransformation process. In order to study the LUCC process we developed a\nspatially explicit agent-based model in the form of a Cellular Automata\nimplemented with the Cell-DEVS formalism. The resulting model called AgroDEVS\nis used for predicting LUCC dynamics along with their associated economic and\nenvironmental changes. AgroDEVS is structured using behavioral rules and\nfunctions representing a) crop yields, b) weather conditions, c) economic\nprofit, d) farmer preferences, e) technology level adoption and f) natural\nresources consumption based on embodied energy accounting. Using data from a\ntypical location of the Pampa region (Argentina) for the 1988-2015 period,\nsimulation exercises showed that the economic goals were achieved, on average,\neach 6 out of 10 years, but the environmental thresholds were only achieved in\n1.9 out of 10 years. In a set of 50-years simulations, LUCC patterns quickly\nconverge towards the most profitable crop sequences, with no noticeable\ntradeoff between the economic and environmental conditions.\n"
    },
    {
        "paper_id": 2109.01044,
        "authors": "Lucien Boulet",
        "title": "Forecasting High-Dimensional Covariance Matrices of Asset Returns with\n  Hybrid GARCH-LSTMs",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several academics have studied the ability of hybrid models mixing univariate\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH) models and\nneural networks to deliver better volatility predictions than purely\neconometric models. Despite presenting very promising results, the\ngeneralization of such models to the multivariate case has yet to be studied.\nMoreover, very few papers have examined the ability of neural networks to\npredict the covariance matrix of asset returns, and all use a rather small\nnumber of assets, thus not addressing what is known as the curse of\ndimensionality. The goal of this paper is to investigate the ability of hybrid\nmodels, mixing GARCH processes and neural networks, to forecast covariance\nmatrices of asset returns. To do so, we propose a new model, based on\nmultivariate GARCHs that decompose volatility and correlation predictions. The\nvolatilities are here forecast using hybrid neural networks while correlations\nfollow a traditional econometric process. After implementing the models in a\nminimum variance portfolio framework, our results are as follows. First, the\naddition of GARCH parameters as inputs is beneficial to the model proposed.\nSecond, the use of one-hot-encoding to help the neural network differentiate\nbetween each stock improves the performance. Third, the new model proposed is\nvery promising as it not only outperforms the equally weighted portfolio, but\nalso by a significant margin its econometric counterpart that uses univariate\nGARCHs to predict the volatilities.\n"
    },
    {
        "paper_id": 2109.01045,
        "authors": "Janak Parmar, Gulnazbanu Saiyed, Sanjaykumar Dave",
        "title": "Analysis of taste heterogeneity in commuters travel decisions using\n  joint parking and mode choice model: A case from urban India",
        "comments": "36 pages, 2 figures",
        "journal-ref": "Transportation Research Part A: Policy and Practice, 2023",
        "doi": "10.1016/j.tra.2023.103610",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The concept of transportation demand management (TDM) upholds the development\nof sustainable mobility through the triumph of optimally balanced transport\nmodal share in cities. The modal split management directly reflects on TDM of\neach transport subsystem, including parking. In developing countries, the\npolicy-makers have largely focused on supply-side measures, yet demand-side\nmeasures have remained unaddressed in policy implications. Ample literature is\navailable presenting responses of TDM strategies, but most studies account mode\nchoice and parking choice behaviour separately rather than considering\ntrade-offs between them. Failing to do so may lead to biased model estimates\nand impropriety in policy implications. This paper seeks to fill this gap by\nadmitting parking choice as an endogenous decision within the model of mode\nchoice behaviour. This study integrates attitudinal factors and\nbuilt-environment variables in addition to parking and travel attributes for\ndeveloping comprehensive estimation results. A mixed logit model with random\ncoefficients is estimated using hierarchical Bayes approach based on the Markov\nChain Monte Carlo simulation method. The results reveal significant influence\nof mode/parking specific attitudes on commuters choice behaviour in addition to\nthe built-environment factors and mode/parking related attributes. It is\nidentified that considerable shift is occurring between parking-types in\npreference to switching travel mode with hypothetical changes in parking\nattributes. Besides, study investigates the heterogeneity in the\nwillingness-to-pay through a follow-up regression model, which provides\nimportant insights for identifying possible sources of this heterogeneity among\nrespondents. The study provides remarkable results which may be beneficial to\nplanning authorities for improving TDM strategies especially in developing\ncountries.\n"
    },
    {
        "paper_id": 2109.01046,
        "authors": "Mohammadreza Mahmoudi, Hana Ghaneei",
        "title": "Detection of Structural Regimes and Analyzing the Impact of Crude Oil\n  Market on Canadian Stock Market: Markov Regime-Switching Approach",
        "comments": "14 pages,10 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study aims to analyze the impact of the crude oil market on the Toronto\nStock Exchange Index (TSX)c based on monthly data from 1970 to 2021 using\nMarkov-switching vector autoregressive (MSI-VAR) model. The results indicate\nthat TSX return contains two regimes, including: positive return (regime 1),\nwhen growth rate of stock index is positive; and negative return (regime 2),\nwhen growth rate of stock index is negative. Moreover, regime 1 is more\nvolatile than regime 2. The findings also show the crude oil market has\nnegative effect on the stock market in regime 1, while it has positive effect\non the stock market in regime 2. In addition, we can see this effect in regime\n1 more significantly in comparison to regime 2. Furthermore, two period lag of\noil price decreases stock return in regime 1, while it increases stock return\nin regime 2.\n"
    },
    {
        "paper_id": 2109.01214,
        "authors": "Andr\\'es Garc\\'ia-Medina, and Toan Luu Duc Huynh3",
        "title": "What drives bitcoin? An approach from continuous local transfer entropy\n  and deep learning classification models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e23121582",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin has attracted attention from different market participants due to\nunpredictable price patterns. Sometimes, the price has exhibited big jumps.\nBitcoin prices have also had extreme, unexpected crashes. We test the\npredictive power of a wide range of determinants on bitcoins' price direction\nunder the continuous transfer entropy approach as a feature selection\ncriterion. Accordingly, the statistically significant assets in the sense of\npermutation test on the nearest neighbour estimation of local transfer entropy\nare used as features or explanatory variables in a deep learning classification\nmodel to predict the price direction of bitcoin. The proposed variable\nselection methodology excludes the NASDAQ index and Tesla as drivers. Under\ndifferent scenarios and metrics, the best results are obtained using the\nsignificant drivers during the pandemic as validation. In the test, the\naccuracy increased in the post-pandemic scenario of July 2020 to January 2021\nwithout drivers. In other words, our results indicate that in times of high\nvolatility, Bitcoin seems to autoregulate and does not need additional drivers\nto improve the accuracy of the price direction.\n"
    },
    {
        "paper_id": 2109.01578,
        "authors": "Denissa Sari Darmawi Purba, Eleftheria Kontou, Chrysafis Vogiatzis",
        "title": "Evacuation Route Planning for Alternative Fuel Vehicles",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.trc.2022.103837",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As the number of adopted alternative fuel vehicles increases, it is crucial\nfor communities (especially those that are susceptible to hazards) to make\nevacuation plans that account for such vehicles refueling needs. During\nemergencies that require preemptive evacuation planning, travelers using\nalternative fuel vehicles are vulnerable when evacuation routes do not provide\naccess to refueling stations on their way to shelters. In this paper, we\nformulate and solve a novel seamless evacuation route plan problem, by\ndesigning $k$-minimum spanning trees with hop constraints that capture the\nrefueling needs of each $k \\in K$ vehicle fuel type on their way to reach a\nshelter. We develop a branch-and-price algorithm based on a matheuristic column\ngeneration approach to solve the evacuation problem. We apply the proposed\nframework to the Sioux Falls transportation network with existing\ninfrastructure deployment and present numerical experiments. Specifically, we\ndiscuss the evacuation travel and refueling times under scenarios of various\nalternative fuel vehicles driving ranges. Our findings show that the\ncharacteristics of each vehicle fuel type, like driving range and the refueling\ninfrastructure topology, play a pivotal role in determining evacuation route\nplans. This means that an evacuation route could prove unique to a single\nvehicle fuel type, while being infeasible to the others. Finally, we observe\nthat the driving range constraints could force evacuee vehicles to detour to\nmeet their refueling needs before reaching safety and increase the total\nevacuation time by 7.32 % in one of the evaluated scenarios.\n"
    },
    {
        "paper_id": 2109.01822,
        "authors": "Viktor Stojkoski, Petar Jolakoski, Arnab Pal, Trifce Sandev, Ljupco\n  Kocarev and Ralf Metzler",
        "title": "Income inequality and mobility in geometric Brownian motion with\n  stochastic resetting: theoretical results and empirical evidence of\n  non-ergodicity",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1098/rsta.2021.0157",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the role of non-ergodicity in the relationship between income\ninequality, the extent of concentration in the income distribution, and\nmobility, the feasibility of an individual to change their position in the\nincome distribution. For this purpose, we explore the properties of an\nestablished model for income growth that includes \"resetting\" as a stabilising\nforce which ensures stationary dynamics. We find that the dynamics of\ninequality is regime-dependent and may range from a strictly non-ergodic state\nwhere this phenomenon has an increasing trend, up to a stable regime where\ninequality is steady and the system efficiently mimics ergodic behaviour.\nMobility measures, conversely, are always stable over time, but the stationary\nvalue is dependent on the regime, suggesting that economies become less mobile\nin non-ergodic regimes. By fitting the model to empirical data for the dynamics\nof income share of the top earners in the United States, we provide evidence\nthat the income dynamics in this country is consistently in a regime in which\nnon-ergodicity characterises inequality and immobility dynamics. Our results\ncan serve as a simple rationale for the observed real world income dynamics and\nas such aid in addressing non-ergodicity in various empirical settings across\nthe globe.\n"
    },
    {
        "paper_id": 2109.01843,
        "authors": "Andrew L. Allan, Christa Cuchiero, Chong Liu and David J. Pr\\\"omel",
        "title": "Model-free Portfolio Theory: A Rough Path Approach",
        "comments": "48 pages, 1 figure",
        "journal-ref": "Mathematical Finance, vol. 33, no. 3, p. 709--765, 2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on a rough path foundation, we develop a model-free approach to\nstochastic portfolio theory (SPT). Our approach allows to handle significantly\nmore general portfolios compared to previous model-free approaches based on\nF{\\\"o}llmer integration. Without the assumption of any underlying probabilistic\nmodel, we prove a pathwise formula for the relative wealth process which\nreduces in the special case of functionally generated portfolios to a pathwise\nversion of the so-called master formula of classical SPT. We show that the\nappropriately scaled asymptotic growth rate of a far reaching generalization of\nCover's universal portfolio based on controlled paths coincides with that of\nthe best retrospectively chosen portfolio within this class. We provide several\nnovel results concerning rough integration, and highlight the advantages of the\nrough path approach by showing that (non-functionally generated) log-optimal\nportfolios in an ergodic It{\\^o} diffusion setting have the same asymptotic\ngrowth rate as Cover's universal portfolio and the best retrospectively chosen\none.\n"
    },
    {
        "paper_id": 2109.02082,
        "authors": "Kaan Gokcesu, Hakan Gokcesu",
        "title": "Nonparametric Extrema Analysis in Time Series for Envelope Extraction,\n  Peak Detection and Clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a nonparametric approach that can be used in\nenvelope extraction, peak-burst detection and clustering in time series. Our\nproblem formalization results in a naturally defined splitting/forking of the\ntime series. With a possibly hierarchical implementation, it can be used for\nvarious applications in machine learning, signal processing and mathematical\nfinance. From an incoming input signal, our iterative procedure sequentially\ncreates two signals (one upper bounding and one lower bounding signal) by\nminimizing the cumulative $L_1$ drift. We show that a solution can be\nefficiently calculated by use of a Viterbi-like path tracking algorithm\ntogether with an optimal elimination rule. We consider many interesting\nsettings, where our algorithm has near-linear time complexities.\n"
    },
    {
        "paper_id": 2109.02129,
        "authors": "Mostafa Abdelrashied, Dikshita Bhattacharya",
        "title": "Future Photovoltaic Electricity Production Targets and The Link to\n  Consumption per Capita on The Policy Level in MENA Region",
        "comments": "9 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper provides an overview of the status of the electricity market in\nthe region, indicating the nexus between electricity consumption with\npopulation growth and GDP. It also analyzes the policy portfolio in different\ncountries, indicating some of the in-action policies' effectiveness and\nrecommended alternatives. World Bank datasets were used for the analysis\nbetween 2000 and 2014. We found that the MENA region is at an early stage for\nrenewable energy with a high potential for solar energy, making it attractive\nfor investors. However, the high dependency on oil for consumption and\nexporting might not provide a prosperous environment for renewable technologies\nto grow. Therefore, a greater focus on decoupling economic growth from energy\nconsumption will have a long-lasting impact on fiscal revenues for net-oil\nexporting countries. Moreover, the consequences of the decoupling will allow\nmore renewables penetration in the current energy mix enabling many countries\nto reach their Paris Agreement goals. For short-term energy policy actions,\nstarting a subsidy reform towards the final repeal of subsidies is a must as\nthese measures relate to all end-use sectors and impact fiscal stability in\nmany countries. With its 1.65GW Benban Solar Park in Aswan, Egypt has shown an\nexample of shifting from subsidizing fossil fuel products to commissioning\nrenewable projects to get closer to its Paris Agreement targets.\n"
    },
    {
        "paper_id": 2109.02134,
        "authors": "Andrey Itkin and Dmitry Muravey",
        "title": "Semi-analytical pricing of barrier options in the time-dependent\n  $\\lambda$-SABR model",
        "comments": "26 pages, 8 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extend the approach of Carr, Itkin and Muravey, 2021 for getting\nsemi-analytical prices of barrier options for the time-dependent Heston model\nwith time-dependent barriers by applying it to the so-called $\\lambda$-SABR\nstochastic volatility model. In doing so we modify the general integral\ntransform method (see Itkin, Lipton, Muravey, Generalized integral transforms\nin mathematical finance, World Scientific, 2021) and deliver solution of this\nproblem in the form of Fourier-Bessel series. The weights of this series solve\na linear mixed Volterra-Fredholm equation (LMVF) of the second kind also\nderived in the paper. Numerical examples illustrate speed and accuracy of our\nmethod which are comparable with those of the finite-difference approach at\nsmall maturities and outperform them at high maturities even by using a\nsimplistic implementation of the RBF method for solving the LMVF.\n"
    },
    {
        "paper_id": 2109.02177,
        "authors": "Plamen Nikolov, Steve Yeh",
        "title": "Reaping the Rewards Later: How Education Improves Old-Age Cognition in\n  South Africa",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cognition, a component of human capital, is fundamental for decision-making,\nand understanding the causes of human capital depreciation in old age is\nespecially important in aging societies. Using various proxy measures of\ncognitive performance from a longitudinal survey in South Africa, we study how\neducation affects cognition in late adulthood. We show that an extra year of\nschooling improves memory performance and general cognition. We find evidence\nof heterogeneous effects by gender: the effects are stronger among women. We\nexplore potential mechanisms, and we show that a more supportive social\nenvironment, improved health habits, and reduced stress levels likely play a\ncritical role in mediating the beneficial effects of educational attainment on\ncognition among the elderly.\n"
    },
    {
        "paper_id": 2109.02331,
        "authors": "Michael Roos and Matthias Reccius",
        "title": "Narratives in economics",
        "comments": "40 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.4419/96973068",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  There is growing awareness within the economics profession of the important\nrole narratives play in the economy. Even though empirical approaches that try\nto quantify economic narratives are getting increasingly popular, there is no\ntheory or even a universally accepted definition of economic narratives\nunderlying this research. First, we review and categorize the economic\nliterature concerned with narratives and work out the different paradigms that\nare at play. Only a subset of the literature considers narratives to be active\ndrivers of economic activity. In order to solidify the foundation of narrative\neconomics, we propose a definition of collective economic narratives, isolating\nfive important characteristics. We argue that, for a narrative to be\neconomically relevant, it must be a sense-making story that emerges in a social\ncontext and suggests action to a social group. We also systematize how a\ncollective economic narrative differs from a topic and from other kinds of\nnarratives that are likely to have less impact on the economy. With regard to\nthe popular use of topic modeling as an empirical strategy, we suggest that the\ncomplementary use of other canonical methods from the natural language\nprocessing toolkit and the development of new methods is inevitable to go\nbeyond identifying topics and be able to move towards true empirical narrative\neconomics.\n"
    },
    {
        "paper_id": 2109.0236,
        "authors": "Fabio Bellini and Ilaria Peri",
        "title": "An axiomatization of $\\Lambda$-quantiles",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an axiomatic foundation to $\\Lambda$-quantiles, a family of\ngeneralized quantiles introduced by Frittelli et al. (2014) under the name of\nLambda Value at Risk. Under mild assumptions, we show that these functionals\nare characterized by a property that we call \"locality\", that means that any\nchange in the distribution of the probability mass that arises entirely above\nor below the value of the $\\Lambda$-quantile does not modify its value. We\ncompare with a related axiomatization of the usual quantiles given by Chambers\n(2009), based on the stronger property of \"ordinal covariance\", that means that\nquantiles are covariant with respect to increasing transformations. Further, we\npresent a systematic treatment of the properties of $\\Lambda$-quantiles,\nrefining some of the results of Frittelli et al. (2014) and Burzoni et al.\n(2017) and showing that in the case of a nonincreasing $\\Lambda$ the properties\nof $\\Lambda$-quantiles closely resemble those of the usual quantiles.\n"
    },
    {
        "paper_id": 2109.02405,
        "authors": "Fabien Le Floc'h and Cornelis W. Oosterlee",
        "title": "Positive Stochastic Collocation for the Collocated Local Volatility\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents how to apply the stochastic collocation technique to\nassets that can not move below a boundary. It shows that the polynomial\ncollocation towards a lognormal distribution does not work well. Then, the\npotentials issues of the related collocated local volatility model (CLV) are\nexplored. Finally, a simple analytical expression for the Dupire local\nvolatility derived from the option prices modelled by stochastic collocation is\ngiven.\n"
    },
    {
        "paper_id": 2109.02452,
        "authors": "Elena Georgarakis, Thomas Bauwens, Anne-Marie Pronk, Tarek AlSkaif",
        "title": "Keep it green, simple and socially fair: a choice experiment on\n  prosumers' preferences for peer to peer electricity trading in the\n  Netherlands",
        "comments": "44 pages, 7 figures, submitted to Energy Policy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the potential for peer-to-peer electricity trading, where households\ntrade surplus electricity with peers in a local energy market, is rapidly\ngrowing, the drivers of participation in this trading scheme have been\nunderstudied so far. In particular, there is a dearth of research on the role\nof non-monetary incentives for trading surplus electricity, despite their\npotentially important role. This paper presents the first discrete choice\nexperiment conducted with prosumers (i.e. proactive households actively\nmanaging their electricity production and consumption) in the Netherlands.\nElectricity trading preferences are analyzed regarding economic, environmental,\nsocial and technological parameters, based on survey data (N = 74). The\ndimensions most valued by prosumers are the environmental and, to a lesser\nextent, economic dimensions, highlighting the key motivating roles of\nenvironmental factors. Furthermore, a majority of prosumers stated they would\nprovide surplus electricity for free or for non-monetary compensations,\nespecially to energy-poor households. These observed trends were more\npronounced among members of energy cooperatives. This suggests that\npeer-to-peer energy trading can advance a socially just energy transition.\nRegarding policy recommendations, these findings point to the need for\ncommunicating environmental and economic benefits when marketing P2P\nelectricity trading platforms and for technical designs enabling effortless and\ncustomizable transactions\n"
    },
    {
        "paper_id": 2109.02512,
        "authors": "Gopal K. Basak, Chandramauli Chakraborty and Pranab Kumar Das",
        "title": "Optimal Lockdown Strategy in a Pandemic: An Exploratory Analysis for\n  Covid-19",
        "comments": "17 pages plus 22 figures and 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper addresses the question of lives versus livelihood in an SIRD model\naugmented with a macroeconomic structure. The constraints on the availability\nof health facilities - both infrastructure and health workers determine the\nprobability of receiving treatment which is found to be higher for the patients\nwith severe infection than the patients with mild infection for the specific\nparametric configuration of the paper. Distinguishing between two types of\ndirect intervention policy - hard lockdown and soft lockdown, the study derives\nalternative policy options available to the government. The study further\nindicates that the soft lockdown policy is optimal from a public policy\nperspective under the specific parametric configuration considered in this\npaper.\n"
    },
    {
        "paper_id": 2109.0273,
        "authors": "Job Boerma, Aleh Tsyvinski, Alexander P. Zimin",
        "title": "Sorting with Teams",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We fully solve a sorting problem with heterogeneous firms and multiple\nheterogeneous workers whose skills are imperfect substitutes. We show that\noptimal sorting, which we call mixed and countermonotonic, is comprised of two\nregions. In the first region, mediocre firms sort with mediocre workers and\ncoworkers such that the output losses are equal across all these teams\n(mixing). In the second region, a high skill worker sorts with low skill\ncoworkers and a high productivity firm (countermonotonicity). We characterize\nthe equilibrium wages and firm values. Quantitatively, our model can generate\nthe dispersion of earnings within and across US firms.\n"
    },
    {
        "paper_id": 2109.02776,
        "authors": "Carol Alexander, Jun Deng, Jianfen Feng, and Huning Wan",
        "title": "Net Buying Pressure and the Information in Bitcoin Option Trades",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  How do supply and demand from informed traders drive market prices of bitcoin\noptions? Deribit options tick-level data supports the limits-to-arbitrage\nhypothesis about the market maker's supply. The main demand-side effects are\nthat at-the-money option prices are largely driven by volatility traders and\nout-of-the-money options are simultaneously driven by volatility traders and\nthose with proprietary information about the direction of future bitcoin price\nmovements. The demand-side trading results contrast with prior studies on\nestablished options markets in the US and Asia, but we also show that Deribit\nis rapidly evolving into a more efficient channel for aggregating information\nfrom informed traders.\n"
    },
    {
        "paper_id": 2109.02787,
        "authors": "Mohammadreza Mahmoudi",
        "title": "Identifying the Main Factors of Iran's Economic Growth using Growth\n  Accounting Framework",
        "comments": "22 pages, 4 Tables, 7 figures",
        "journal-ref": "European Journal of Business and Management Research, Vol 6 No 5\n  (2021), 239-245",
        "doi": "10.24018/EJBMR",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to present empirical analysis of Iranian economic growth from\n1950 to 2018 using data from the World Bank, Madison Data Bank, Statistical\nCenter of Iran, and Central Bank of Iran. The results show that Gross Domestic\nProduct (GDP) per capital increased by 2 percent annually during this time,\nhowever this indicator has had a huge fluctuation over time. In addition, the\neconomic growth of Iran and oil revenue have close relationship with each\nother. In fact, whenever oil crises happen, great fluctuation in growth rate\nand other indicators happened subsequently. Even though the shares of other\nsectors like industry and services in GDP have increased over time, the oil\nsector still plays a key role in the economic growth of Iran. Moreover, growth\naccounting analysis shows contribution of capital plays a significant role in\neconomic growth of Iran. Furthermore, based on growth accounting framework the\nsteady state of effective capital is 4.27 for Iran's economy.\n"
    },
    {
        "paper_id": 2109.02872,
        "authors": "Dongdong Hu, Hasanjan Sayit, Svetlozar T. Rachev",
        "title": "Moment Matching Method for Pricing Spread Options with Mean-Variance\n  Mixture L\\'evy Motions",
        "comments": "The paper is not publishable",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper Borovkova et al. [4] uses moment matching method to obtain closed\nform formulas for spread and basket call option prices under log normal models.\nIn this note, we also use moment matching method to obtain semi-closed form\nformulas for the price of spread options under exponential L\\'evy models with\nmean-variance mixture. Unlike the semi-closed form formulas in Caldana and\nFusai [5], where spread prices were expressed by using Fourier inversion\nformula for general price dynamics, our formula expresses spread prices in\nterms of the mixing distribution. Numerical tests show that our formulas give\naccurate spread prices also\n"
    },
    {
        "paper_id": 2109.0289,
        "authors": "Nathan Ratledge, Gabe Cadamuro, Brandon de la Cuesta, Matthieu\n  Stigler, Marshall Burke",
        "title": "Using Satellite Imagery and Machine Learning to Estimate the Livelihood\n  Impact of Electricity Access",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41586-022-05322-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many regions of the world, sparse data on key economic outcomes inhibits\nthe development, targeting, and evaluation of public policy. We demonstrate how\nadvancements in satellite imagery and machine learning can help ameliorate\nthese data and inference challenges. In the context of an expansion of the\nelectrical grid across Uganda, we show how a combination of satellite imagery\nand computer vision can be used to develop local-level livelihood measurements\nappropriate for inferring the causal impact of electricity access on\nlivelihoods. We then show how ML-based inference techniques deliver more\nreliable estimates of the causal impact of electrification than traditional\nalternatives when applied to these data. We estimate that grid access improves\nvillage-level asset wealth in rural Uganda by 0.17 standard deviations, more\nthan doubling the growth rate over our study period relative to untreated\nareas. Our results provide country-scale evidence on the impact of a key\ninfrastructure investment, and provide a low-cost, generalizable approach to\nfuture policy evaluation in data sparse environments.\n"
    },
    {
        "paper_id": 2109.02933,
        "authors": "Akihiko Noda",
        "title": "Examining the Dynamic Asset Market Linkages under the COVID-19 Global\n  Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the dynamic asset market linkages under the COVID-19\nglobal pandemic based on market efficiency, in the sense of Fama (1970).\nParticularly, we estimate the joint degree of market efficiency by applying Ito\net al.'s (2014; 2017) Generalized Least Squares-based time-varying vector\nautoregression model. The empirical results show that (1) the joint degree of\nmarket efficiency changes widely over time, as shown in Lo's (2004) adaptive\nmarket hypothesis, (2) the COVID-19 pandemic may eliminate arbitrage and\nimprove market efficiency through enhanced linkages between the asset markets;\nand (3) the market efficiency has continued to decline due to the Bitcoin\nbubble that emerged at the end of 2020.\n"
    },
    {
        "paper_id": 2109.03,
        "authors": "G\\\"uray Kara, Asgeir Tomasgard and Hossein Farahmand",
        "title": "Characterization of flexible electricity in power and energy markets",
        "comments": "This paper was sent to the Renewable & Sustainable Energy Reviews at\n  August 2021. It is currently under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The authors provide a comprehensive overview of flexibility characterization\nalong the dimensions of time, spatiality, resource, and risk in power systems.\nThese dimensions are discussed in relation to flexibility assets, products, and\nservices, as well as new and existing flexibility market designs. The authors\nargue that flexibility should be evaluated based on the dimensions under\ndiscussion. Flexibility products and services can increase the efficiency of\npower systems and markets if flexibility assets and related services are taken\ninto consideration and used along the time, geography, technology, and risk\ndimensions. Although it is possible to evaluate flexibility in existing market\ndesigns, a local flexibility market may be needed to exploit the value of the\nflexibility, depending on the dimensions of the flexibility products and\nservices. To locate flexibility in power grids and prevent incorrect\nvaluations, the authors also discuss TSO-DSO coordination along the four\ndimensions, and they present interrelations between flexibility dimensions,\nproducts, services, and related market designs for productive usage of flexible\nelectricity.\n"
    },
    {
        "paper_id": 2109.03061,
        "authors": "Laura Doval and Alex Smolin",
        "title": "Persuasion and Welfare",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information policies such as scores, ratings, and recommendations are\nincreasingly shaping society's choices in high-stakes domains. We provide a\nframework to study the welfare implications of information policies on a\npopulation of heterogeneous individuals. We define and characterize the Bayes\nwelfare set, consisting of the population's utility profiles that are feasible\nunder some information policy. The Pareto frontier of this set can be recovered\nby a series of standard Bayesian persuasion problems, in which a utilitarian\nplanner takes the role of the information designer. We provide necessary and\nsufficient conditions under which an information policy exists that Pareto\ndominates the no-information policy. We illustrate our results with\napplications to data leakage, price discrimination, and credit ratings.\n"
    },
    {
        "paper_id": 2109.03541,
        "authors": "Jiamin Yu",
        "title": "Three fundamental problems in risk modeling on big data: an information\n  theory view",
        "comments": "6 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Since Claude Shannon founded Information Theory, information theory has\nwidely fostered other scientific fields, such as statistics, artificial\nintelligence, biology, behavioral science, neuroscience, economics, and\nfinance. Unfortunately, actuarial science has hardly benefited from information\ntheory. So far, only one actuarial paper on information theory can be searched\nby academic search engines. Undoubtedly, information and risk, both as\nUncertainty, are constrained by entropy law. Today's insurance big data era\nmeans more data and more information. It is unacceptable for risk management\nand actuarial science to ignore information theory. Therefore, this paper aims\nto exploit information theory to discover the performance limits of insurance\nbig data systems and seek guidance for risk modeling and the development of\nactuarial pricing systems.\n"
    },
    {
        "paper_id": 2109.03644,
        "authors": "Larisa Kargina, Mattia Masolletti",
        "title": "Eurasian Economic Union: Current Concept and Prospects",
        "comments": "8 pages. Key words: integration, Eurasian Economic Union, economic\n  integration. JEL codes: F-02; F-15. Affiliations: Russian University of\n  Transport (Russia), NUST University (Italy)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The authors of the article analyze the content of the Eurasian integration,\nfrom the initial initiative to the modern Eurasian Economic Union, paying\nattention to the factors that led to the transition from the Customs Union and\nthe Single Economic Space to a stronger integration association. The main\nmethod of research is historical and legal analysis.\n"
    },
    {
        "paper_id": 2109.0374,
        "authors": "Ravi Kashyap",
        "title": "Behavioral Bias Benefits: Beating Benchmarks By Bundling Bouncy Baskets",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1910.02144",
        "journal-ref": "Accounting & Finance, Aug 2021, 12826, XX(X), XX-XX",
        "doi": "10.1111/acfi.12826",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider in detail an investment strategy, titled \"The Bounce Basket\",\ndesigned for someone to express a bullish view on the market by allowing them\nto take long positions on securities that would benefit the most from a rally\nin the markets. We demonstrate the use of quantitative metrics and large\namounts of historical data towards decision making goals. This investment\nconcept combines macroeconomic views with characteristics of individual\nsecurities to beat the market returns. The central idea of this theme is to\nidentity securities from a regional perspective that are heavily shorted and\nyet are fundamentally sound with at least a minimum buy rating from a consensus\nof stock analysts covering the securities. We discuss the components of\ncreating such a strategy including the mechanics of constructing the portfolio.\nUsing simulations, in which securities lending data is modeled as geometric\nbrownian motions, we provide a few flavors of creating a ranking of securities\nto identity the ones that are heavily shorted.\n  An investment strategy of this kind will be ideal in market scenarios when a\ndownturn happens due to unexpected extreme events and the markets are\nanticipated to bounce back thereafter. This situation is especially applicable\nto incidents being observed, and relevant proceedings, during the Coronavirus\npandemic in 2020-2021. This strategy is one particular way to overcome a\npotential behavioral bias related to investing, which we term the \"rebound\neffect\".\n"
    },
    {
        "paper_id": 2109.0393,
        "authors": "Gnecco Giorgio, Nutarelli Federico, Riccaboni Massimo",
        "title": "Matrix Completion of World Trade",
        "comments": "The main paper contains 11 pages with the Appendix. Supplemental\n  material is also reported",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work applies Matrix Completion (MC) -- a class of machine-learning\nmethods commonly used in the context of recommendation systems -- to analyse\neconomic complexity. MC is applied to reconstruct the Revealed Comparative\nAdvantage (RCA) matrix, whose elements express the relative advantage of\ncountries in given classes of products, as evidenced by yearly trade flows. A\nhigh-accuracy binary classifier is derived from the application of MC, with the\naim of discriminating between elements of the RCA matrix that are,\nrespectively, higher or lower than one. We introduce a novel Matrix cOmpletion\niNdex of Economic complexitY (MONEY) based on MC, which is related to the\npredictability of countries' RCA (the lower the predictability, the higher the\ncomplexity). Differently from previously-developed indices of economic\ncomplexity, the MONEY index takes into account the various singular vectors of\nthe matrix reconstructed by MC, whereas other indices are based only on one/two\neigenvectors of a suitable symmetric matrix, derived from the RCA matrix.\nFinally, MC is compared with a state-of-the-art economic complexity index\n(GENEPY). We show that the false positive rate per country of a binary\nclassifier constructed starting from the average entry-wise output of MC can be\nused as a proxy of GENEPY.\n"
    },
    {
        "paper_id": 2109.0394,
        "authors": "Yiduo Huang, Zuojun Max Shen",
        "title": "Optimizing timetable and network reopen plans for public transportation\n  networks during a COVID19-like pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recovery of the public transportation system is critical for both social\nre-engagement and economic rebooting after the shutdown during pandemic like\nCOVID-19. In this study, we focus on the integrated optimization of service\nline reopening plan and timetable design. We model the transit system as a\nspace-time network. In this network, the number of passengers on each vehicle\nat the same time can be represented by arc flow. We then apply a simplified\nspatial compartmental model of epidemic (SCME) to each vehicle and platform to\nmodel the spread of pandemic in the system as our objective, and calculate the\noptimal open plan and timetable. We demonstrate that this optimization problem\ncan be decomposed into a simple integer programming and a linear\nmulti-commodity network flow problem using Lagrangian relaxation techniques.\nFinally, we test the proposed model using real-world data from the Bay Area\nRapid Transit (BART) and give some useful suggestions to system managers.\n"
    },
    {
        "paper_id": 2109.03977,
        "authors": "Julius O. Campeci\\~no",
        "title": "Portfolio Theory and Security Investment Risk Analysis Using Coefficient\n  of Variation: An Alternative to Mean-Variance Analysis",
        "comments": "10 pages of main text, 76 pages of appendix, 7 figures, and 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provided proof here that coefficient of variation (CV) is a direct measure\nof risk using an equation that has been derived here for the first time. We\nalso presented a method to generate a stock CV based on return that strongly\ncorrelates with stock price performance. Consequently, we found that the price\ngrowths of stocks with low but positive CV are approximately exponential which\nexplains our finding here that the total return of US domestic stocks within $0\n\\le CV \\le 1$ between Dec 2008 to Dec 2018 averaged at around 475% and\noutperformed the average total return of stocks within $CV > 1$ and $CV > 4$ by\n144% and 2000%, respectively. From these observations, we posit that minimizing\nportfolio CV does not only minimize risk but also maximizes return. Minimizing\nrisk by minimizing the standard deviation of return (volatility) as espoused by\nthe Modern Portfolio Theory only resulted in a meager average total return of\n15%, and the low-risk (low volatility) portfolio outperformed the high-risk\nportfolio by only 25%. These observations suggest that CV is a more reliable\nmeasure of risk than volatility.\n"
    },
    {
        "paper_id": 2109.04001,
        "authors": "Saeed Marzban, Erick Delage, Jonathan Yumeng Li",
        "title": "Deep Reinforcement Learning for Equal Risk Pricing and Hedging under\n  Dynamic Expectile Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently equal risk pricing, a framework for fair derivative pricing, was\nextended to consider dynamic risk measures. However, all current\nimplementations either employ a static risk measure that violates time\nconsistency, or are based on traditional dynamic programming solution schemes\nthat are impracticable in problems with a large number of underlying assets\n(due to the curse of dimensionality) or with incomplete asset dynamics\ninformation. In this paper, we extend for the first time a famous off-policy\ndeterministic actor-critic deep reinforcement learning (ACRL) algorithm to the\nproblem of solving a risk averse Markov decision process that models risk using\na time consistent recursive expectile risk measure. This new ACRL algorithm\nallows us to identify high quality time consistent hedging policies (and equal\nrisk prices) for options, such as basket options, that cannot be handled using\ntraditional methods, or in context where only historical trajectories of the\nunderlying assets are available. Our numerical experiments, which involve both\na simple vanilla option and a more exotic basket option, confirm that the new\nACRL algorithm can produce 1) in simple environments, nearly optimal hedging\npolicies, and highly accurate prices, simultaneously for a range of maturities\n2) in complex environments, good quality policies and prices using reasonable\namount of computing resources; and 3) overall, hedging strategies that actually\noutperform the strategies produced using static risk measures when the risk is\nevaluated at later points of time.\n"
    },
    {
        "paper_id": 2109.04058,
        "authors": "Benjamin Avanzi, Gregory Clive Taylor, and Melantha Wang",
        "title": "SPLICE: A Synthetic Paid Loss and Incurred Cost Experience Simulator",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1017/S1748499522000057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we first introduce a simulator of cases estimates of incurred\nlosses, called `SPLICE` (Synthetic Paid Loss and Incurred Cost Experience). In\nthree modules, case estimates are simulated in continuous time, and a record is\noutput for each individual claim. Revisions for the case estimates are also\nsimulated as a sequence over the lifetime of the claim, in a number of\ndifferent situations. Furthermore, some dependencies in relation to case\nestimates of incurred losses are incorporated, particularly recognizing certain\nproperties of case estimates that are found in practice. For example, the\nmagnitude of revisions depends on ultimate claim size, as does the distribution\nof the revisions over time. Some of these revisions occur in response to\noccurrence of claim payments, and so `SPLICE` requires input of simulated\nper-claim payment histories. The claim data can be summarized by accident and\npayment \"periods\" whose duration is an arbitrary choice (e.g. month, quarter,\netc.) available to the user.\n  `SPLICE` is a fully documented R package that is publicly available and open\nsource (on CRAN). It is built on an existing simulator of individual claim\nexperience called `SynthETIC` (Avanzi et al., 2021a,b), which offers flexible\nmodelling of occurrence, notification, as well as the timing and magnitude of\nindividual partial payments. This is in contrast with the incurred losses,\nwhich constitute the additional contribution of `SPLICE`. The inclusion of\nincurred loss estimates provides a facility that almost no other simulators do.\n"
    },
    {
        "paper_id": 2109.04214,
        "authors": "Roy Cerqueti, Giulia Rotundo, and Marcel Ausloos",
        "title": "Tsallis entropy for cross-shareholding network configurations",
        "comments": "25 pages, 4 figures, 85 references",
        "journal-ref": "Entropy 27 (2020) 676",
        "doi": "10.3390/e22060676",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we develop the Tsallis entropy approach for examining the\ncross-shareholding network of companies traded on the Italian stock market. In\nsuch a network, the nodes represent the companies, and the links represent the\nownership. Within this context, we introduce the out-degree of the nodes --\nwhich represents the diversification -- and the in-degree of them -- capturing\nthe integration. Diversification and integration allow a clear description of\nthe industrial structure formed by the considered companies. The stochastic\ndependence of diversification and integration is modelled through copulas. We\nargue that copulas are well suited for modelling the joint distribution. The\nanalysis of the stochastic dependence between integration and diversification\nby means of the Tsallis entropy gives a crucial information on the reaction of\nthe market structure to the external shocks, - on the basis of some relevant\ncases of dependence between the considered variables. In this respect, the\nconsidered entropy framework provides insights on the relationship between\nin-degree and out-degree dependence structure and market polarisation or\nfairness. Moreover, the interpretation of the results in the light of the\nTsallis entropy parameter gives relevant suggestions for policymakers who aim\nat shaping the industrial context for having high polarisation or fair joint\ndistribution of diversification and integration. Furthermore, a discussion of\npossible parametrisations of the in-degree and out-degree marginal\ndistribution, -- by means of power laws or exponential functions, -- is also\ncarried out. An empirical experiment on a large dataset of Italian companies\nvalidates the theoretical framework.\n"
    },
    {
        "paper_id": 2109.04225,
        "authors": "Andrew L. Allan, Chong Liu and David J. Pr\\\"omel",
        "title": "A C\\`adl\\`ag Rough Path Foundation for Robust Finance",
        "comments": "35 pages",
        "journal-ref": "Finance Stoch. 28 (2024), no.1, 215--257",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using rough path theory, we provide a pathwise foundation for stochastic\nIt\\^o integration, which covers most commonly applied trading strategies and\nmathematical models of financial markets, including those under Knightian\nuncertainty. To this end, we introduce the so-called Property (RIE) for\nc\\`adl\\`ag paths, which is shown to imply the existence of a c\\`adl\\`ag rough\npath and of quadratic variation in the sense of F\\\"ollmer. We prove that the\ncorresponding rough integrals exist as limits of left-point Riemann sums along\na suitable sequence of partitions. This allows one to treat integrands of\nnon-gradient type, and gives access to the powerful stability estimates of\nrough path theory. Additionally, we verify that (path-dependent) functionally\ngenerated trading strategies and Cover's universal portfolio are admissible\nintegrands, and that Property (RIE) is satisfied by both (Young)\nsemimartingales and typical price paths.\n"
    },
    {
        "paper_id": 2109.04324,
        "authors": "B. De Bruyne, J. Randon-Furling, S. Redner",
        "title": "A Tale of Two (and More) Altruists",
        "comments": "16 pages, 6 figures, IOP format",
        "journal-ref": "J. Stat. Mech. 103405 (2021)",
        "doi": "10.1088/1742-5468/ac2906",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a minimalist dynamical model of wealth evolution and wealth\nsharing among $N$ agents as a platform to compare the relative merits of\naltruism and individualism. In our model, the wealth of each agent\nindependently evolves by diffusion. For a population of altruists, whenever any\nagent reaches zero wealth (that is, the agent goes bankrupt), the remaining\nwealth of the other $N-1$ agents is equally shared among all. The population is\ncollectively defined to be bankrupt when its total wealth falls below a\nspecified small threshold value. For individualists, each time an agent goes\nbankrupt (s)he is considered to be \"dead\" and no wealth redistribution occurs.\nWe determine the evolution of wealth in these two societies. Altruism leads to\nmore global median wealth at early times; eventually, however, the\nlongest-lived individualists accumulate most of the wealth and are richer and\nmore long lived than the altruists.\n"
    },
    {
        "paper_id": 2109.0437,
        "authors": "Valery Dolgov, Mattia Masolletti",
        "title": "Protection of the Rights of Large Families as One of the Key Tasks of\n  the State's Social Policy",
        "comments": "9 pages, 1 figure. Key words: rights, family policy, social policy,\n  large families, family support. JEL codes: H-53, I-31",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The authors of the article analyze the policy of the Russian government in\nthe field of family support, paying attention to legal programs at the federal\nand regional levels. The maternity capital program is considered separately, as\nwell as measures aimed at supporting large families.\n"
    },
    {
        "paper_id": 2109.04793,
        "authors": "Carol Alexander, Xi Chen, Charles Ward",
        "title": "Risk-Adjusted Valuation for Real Option Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We model investor heterogeneity using different required returns on an\ninvestment and evaluate the impact on the valuation of an investment. By\nassuming no disagreement on the cash flows, we emphasize how risk preferences\nin particular, but also the costs of capital, influence a subjective evaluation\nof the decision to invest now or retain the option to invest in future. We\npropose a risk-adjusted valuation model to facilitate investors' subjective\ndecision making, in response to the market valuation of an investment\nopportunity. The investor's subjective assessment arises from their perceived\nmisvaluation of the investment by the market, so projected cash flows are\ndiscounted using two different rates representing the investor's and the\nmarket's view. This liberates our model from perfect or imperfect hedging\nassumptions and instead, we are able to illustrate the hedging effect on the\nreal option value when perceptions of risk premia diverge. During crises\nperiods, delaying an investment becomes more valuable as the idiosyncratic risk\nof future cash flows increases, but the decision-maker may rush to invest too\nquickly when the risk level is exceptionally high. Our model verifies features\nestablished by classical real-option valuation models and provides many new\ninsights about the importance of modelling divergences in decision-makers risk\npremia, especially during crisis periods. It also has many practical advantages\nbecause it requires no more parameter inputs than basic discounted cash flow\napproaches, such as the marketed asset disclaimer method, but the outputs are\nmuch richer. They allow for complex interactions between cost and revenue\nuncertainties as well as an easy exploration of the effects of hedgeable and\nun-hedgeable risks on the real option value. Furthermore, we provide\nfully-adjustable Python code in which all parameter values can be chosen by the\nuser.\n"
    },
    {
        "paper_id": 2109.04812,
        "authors": "Marjolein E. Verhulst, Philippe Debie, Stephan Hageboeck, Joost M.E.\n  Pennings, Cornelis Gardebroek, Axel Naumann, Paul van Leeuwen, Andres A.\n  Trujillo-Barrera, Lorenzo Moneta",
        "title": "When Two Worlds Collide: Using Particle Physics Tools to Visualize the\n  Limit Order Book",
        "comments": "51 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1002/fut.22251",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce a methodology to visualize the limit order book (LOB) using a\nparticle physics lens. Open-source data-analysis tool ROOT, developed by CERN,\nis used to reconstruct and visualize futures markets. Message-based data is\nused, rather than snapshots, as it offers numerous visualization advantages.\nThe visualization method can include multiple variables and markets\nsimultaneously and is not necessarily time dependent. Stakeholders can use it\nto visualize high-velocity data to gain a better understanding of markets or\neffectively monitor markets. In addition, the method is easily adjustable to\nuser specifications to examine various LOB research topics, thereby\ncomplementing existing methods.\n"
    },
    {
        "paper_id": 2109.04913,
        "authors": "Andrei Goloubentsev, Dmitri Goloubentsev, Evgeny Lakshtanov",
        "title": "Adjoint Differentiation for generic matrix functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a formula for the adjoint $\\overline{A}$ of a square-matrix\noperation of the form $C=f(A)$, where $f$ is holomorphic in the neighborhood of\neach eigenvalue. We then apply the formula to derive closed-form expressions in\nparticular cases of interest such as the case when we have a spectral\ndecomposition $A=UDU^{-1}$, the spectrum cut-off $C=A_+$ and the Nearest\nCorrelation Matrix routine. Finally, we explain how to simplify the computation\nof adjoints for regularized linear regression coefficients.\n"
    },
    {
        "paper_id": 2109.0492,
        "authors": "Kushantha Fernando, Vajira Manathunga",
        "title": "An Alternative Approach to Evaluate American Options Price Using HJM\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Developments in finance industry and academic research has led to innovative\nfinancial products. This paper presents an alternative approach to price\nAmerican options. Our approach utilizes famous \\cite{heath1992bond} (\"HJM\")\ntechnique to calculate American option written on an asset. Originally, HJM\nforward modeling approach was introduced as an alternative approach to bond\npricing in fixed income market. Since then, \\cite{schweizer2008term} and\n\\cite{carmona2008infinite} extended HJM forward modeling approach to equity\nmarket by capturing dynamic nature of volatility. They modeled the term\nstructure of volatility, which is commonly observed in the market place as\nopposed to constant volatility assumption under Black - Scholes framework.\nUsing this approach, we propose an alternative value function, a stopping\ncriteria and a stopping time. We give an example of how to price American put\noption using proposed methodology.\n"
    },
    {
        "paper_id": 2109.05262,
        "authors": "Abhin Kakkad and Arnab K. Ray",
        "title": "Global dynamics of GDP and trade",
        "comments": "8 pages, 12 figures, ReVTeX double column format",
        "journal-ref": "IJMPC, 2022 (Published online in August 2022)",
        "doi": "10.1142/S0129183123500201",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the logistic equation to model the dynamics of the GDP and the trade\nof the six countries with the highest GDP in the world, namely, USA, China,\nJapan, Germany, UK and India. From the modelling of the economic data, which\nare made available by the World Bank, we predict the maximum values of the\ngrowth of GDP and trade, as well as the duration over which exponential growth\ncan be sustained. We set up the correlated growth of GDP and trade as the phase\nsolutions of an autonomous second-order dynamical system. GDP and trade are\nrelated to each other by a power law, whose exponent seems to differentiate the\nsix national economies into two types. Under conducive conditions for economic\ngrowth, our conclusions have general validity.\n"
    },
    {
        "paper_id": 2109.05283,
        "authors": "Lin Li",
        "title": "Financial Trading with Feature Preprocessing and Recurrent Reinforcement\n  Learning",
        "comments": "accepted for publication by IEEE ISKE21",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial trading aims to build profitable strategies to make wise investment\ndecisions in the financial market. It has attracted interests in the machine\nlearning community for a long time. This paper proposes to trade financial\nassets automatically using feature preprocessing skills and Recurrent\nReinforcement Learning (RRL) algorithm. The strategy starts from technical\nindicators extracted from assets' market information. Then these technical\nindicators are preprocessed by Principal Component Analysis (PCA) and Discrete\nWavelet Transform (DWT) and eventually inputted to the RRL algorithm to do the\ntrading. The extensive empirical evidence shows that the proposed strategy is\nnot only effective and robust in its performance, but also can mitigate the\ndrawbacks underlying the initial trading using RRL.\n"
    },
    {
        "paper_id": 2109.05416,
        "authors": "Mohammad Nur Nobi and Dr. A N M Moinul Islam",
        "title": "Estimating the Environmental Cost of Shrimp Farming in Coastal Areas of\n  Chittagong and Coxs bazaar in Bangladesh",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the last three decades, shrimp has remained one of the major export\nitems in Bangladesh. It contributes to the development of this country by\nenhancing export earnings and promoting employment. However, coastal wetlands\nand agricultural lands are used for shrimp culture, which reduces agricultural\nopportunity and peasants income, and destroys the mangroves and coastal\neco-system. These are the external environmental costs that are not reflected\nin farmers price and output decisions. This study has aimed to estimate those\nexternal environmental costs through the contingent valuation method. The\ncalculated environmental cost of shrimp farming is USD 13.66 per acre per year.\nFindings suggest that current shrimp production and shrimp price will no longer\nbe optimal once the external costs are internalized. Thus alternative policy\nrecommendations have been proposed so that shrimp farming becomes a sustainable\nand equitable means of aquaculture.\n"
    },
    {
        "paper_id": 2109.05419,
        "authors": "Mohammad Nur Nobi",
        "title": "Cost-Benefit Analysis of Kaptai Dam in Rangamati District, Chittagong,\n  Bangladesh",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to assess the net benefit of the kaptai dam on the Karnafuli\nriver in Kaptai, Chittagong, Bangladesh. Kaptai Dam, the only hydroelectricity\npower source in Bangladesh, provides only 5% electricity demand of Bangladesh.\nThe Dam is located on the Karnafuli River at Kaptai in Rangamati District, 65\nkm upstream from Chittagong. It is an earth-fill or embankment dam with a\nreservoir with a water storage capacity of 11,000 skm. Though the Dam's primary\npurpose is to generate electricity, it became a reservoir of water used for\nfishing and tourism. To find the net benefit value and estimate the\nenvironmental costs and benefits, we considered the environmental net benefit\nfrom 1962 to 1997. We identify the costs of Kaptai Dam, including its\nestablishment cost, operational costs, the costs of lives that have been lost\ndue to conflicts, and environmental costs, including loss of biodiversity, loss\nof land uses, and loss of human displacements. Also, we assess the benefits of\nelectricity production, earnings from fisheries production, and gain from\ntourism to Kaptai Lake. The findings show that the Dam contributes tremendous\nvalue to Bangladesh. As a source of hydroelectricity, the Kaptai Dam is a\nsource of clean energy, and its value might have been worthy of this Dam\nproduced a significant portion of the electricity. However, providing less than\n5% of the national demand for electricity followed by various external and\nsensitive costs, the Dam hardly contributes to the Bangladesh economy. This\nstudy thus recommends that Bangladesh should look for other sources of clean\nenergy that have no chances of eco-political conflicts.\n"
    },
    {
        "paper_id": 2109.05421,
        "authors": "Mohammad Nur Nobi",
        "title": "Willingness to Pay to Prevent Water and Sanitation-Related Diseases\n  Suffered by Slum Dwellers and Beneficiary Households: Evidence from\n  Chittagong, Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A significant proportion of slum residents offer vital services that are\nrelied upon by wealthier urban residents. However, the lack of access to clean\ndrinking water and adequate sanitation facilities causes considerable health\nrisks for slum residents, leading to interruption in services and potential\ntransmission of diseases to the beneficiaries. This study explores the\nwillingness of the households benefitting from these services to contribute\nfinancially towards the measures that can mitigate the negative externalities\nof the diseases resulting from poor water and sanitation in slums. This study\nadopts the Contingent Valuation Method using face-to-face interviews with 260\nservice-receiving households in Chittagong City Corporation of Bangladesh.\nEstimating the logistic regression model, the findings indicate that 74 percent\nof respondents express their willingness to contribute financially towards an\nimprovement of water and sanitation facilities in the slums. Within this group,\n16 percent are willing to pay 1.88 USD/month, 18 percent prefer 3.86 USD/year,\nand 40 percent are willing to contribute a lump sum of 3.92 USD. The empirical\nfindings suggest a significant influence of gender, college, and housemaids\nworking hours in the households on respondents willingness to pay. For example,\nfemale respondents with a college degree and households with longer working\nhours of housemaids are more likely to contribute towards the improvement of\nthe water and sanitation facilities in slums. Though the findings are\nstatistically significant at a 5% level across different estimated models, the\nregression model exhibits a low goodness of fit.\n"
    },
    {
        "paper_id": 2109.05431,
        "authors": "Nuerxiati Abudurexiti and Kai He and Dongdong Hu and Hasanjan Sayit",
        "title": "A note on closed-form spread option valuation under log-normal models",
        "comments": "37 Pages, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the papers Carmona and Durrleman [7] and Bjerksund and Stensland [1],\nclosed form approximations for spread call option prices were studied under the\nlog normal models. In this paper, we give an alternative closed form formula\nfor the price of spread call options under the log-normal models also. Our\nformula can be seen as a generalization of the closed-form formula presented in\nBjerksund and Stensland [1] as their formula can be obtained by selecting\nspecial parameter values to our formula. Numerical tests show that our formula\nperforms better for certain range of model parameters than the closed-form\nformula presented in Bjerksund and Stensland [1].\n"
    },
    {
        "paper_id": 2109.05564,
        "authors": "Carlo Marinelli",
        "title": "On certain representations of pricing functionals",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit two classical problems: the determination of the law of the\nunderlying with respect to a risk-neutral measure on the basis of option\nprices, and the pricing of options with convex payoffs in terms of prices of\ncall options with the same maturity (all options are European). The formulation\nof both problems is expressed in a language loosely inspired by the theory of\ninverse problems, and several proofs of the corresponding solutions are\nprovided that do not rely on any special assumptions on the law of the\nunderlying and that may, in some cases, extend results currently available in\nthe literature.\n"
    },
    {
        "paper_id": 2109.05917,
        "authors": "Leysan Anvarovna Davletshina, Natalia Alekseevna Sadovnikova,\n  Alexander Valeryevich Bezrukov, Olga Guryevna Lebedinskaya",
        "title": "The state of health of the Russian population during the pandemic\n  (according to sample surveys)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article analyzes the population's assessment of their own health and\nattitude to a healthy lifestyle in the context of distribution by age groups.\nOf particular interest is the presence of transformations taking into account\nthe complex epidemiological situation, the increase in the incidence of\ncoronavirus infection in the population (the peak of the incidence came during\nthe period of selective observation in 2020). The article assesses the\ncloseness of the relationship between the respondents ' belonging to a\nparticular socio-demographic group and their social well-being during the\nperiod of self-isolation, quarantine or other restrictions imposed during the\ncoronavirus pandemic in 2020. To solve this problem, the demographic and\nsocio-economic characteristics of respondents are presented, the distribution\nof responses according to the survey results is estimated and the most\nsignificant factor characteristics are selected. The distributions of\nrespondents ' responses are presented for the selected questions. To determine\nthe closeness of the relationship between the respondents ' answers to the\nquestion and their gender or age distribution, the coefficients of mutual\nconjugacy and rank correlation coefficients were calculated and analyzed. The\nultimate goal of the analytical component of this study is to determine the\nsocial well-being of the Russian population during the pandemic on the basis of\nsample survey data. As a result of the analysis of changes for the period\n2019-2020, the assessment of the closeness of communication revealed the\nparameters that form differences (gender, wealth, territory of residence).\n"
    },
    {
        "paper_id": 2109.05998,
        "authors": "Battulga Gankhuu",
        "title": "Options Pricing under Bayesian MS-VAR Process",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we have studied option pricing methods that are based on a\nBayesian Markov-Switching Vector Autoregressive (MS-BVAR) process using a\nrisk-neutral valuation approach. A BVAR process, which is a special case of the\nBayesian MS-VAR process is widely used to model interdependencies of economic\nvariables and forecast economic variables. Here we assumed that a\nregime-switching process is generated by a homogeneous Markov process and a\nresidual process follows a conditional heteroscedastic model. With a direct\ncalculation and change of probability measure, for some frequently used\noptions, we derived pricing formulas. An advantage of our model is it depends\non economic variables and is easy to use compared to previous option pricing\npapers, which depend on regime-switching.\n"
    },
    {
        "paper_id": 2109.06169,
        "authors": "Subodh Dubey, Ishant Sharma, Sabyasachee Mishra, Oded Cats, and\n  Prateek Bansal",
        "title": "A General Framework to Forecast the Adoption of Novel Products: A Case\n  of Autonomous Vehicles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to the unavailability of prototypes, the early adopters of novel products\nactively seek information from multiple sources (e.g., media and social\nnetworks) to minimize the potential risk. The existing behavior models not only\nfail to capture the information propagation within the individual's social\nnetwork, but also they do not incorporate the impact of such word-of-mouth\n(WOM) dissemination on the consumer's risk preferences. Moreover, even\ncutting-edge forecasting models rely on crude/synthetic consumer behavior\nmodels. We propose a general framework to forecast the adoption of novel\nproducts by developing a new consumer behavior model and integrating it into a\npopulation-level agent-based model. Specifically, we extend the hybrid choice\nmodel to estimate consumer behavior, which incorporates social network effects\nand interplay between WOM and risk aversion. The calibrated consumer behavior\nmodel and synthetic population are passed through the agent-based model for\nforecasting the product market share. We apply the proposed framework to\nforecast the adoption of autonomous vehicles (AVs) in Nashville, USA. The\nconsumer behavior model is calibrated with a stated preference survey data of\n1,495 Nashville residents. The output of the agent-based model provides the\neffect of the purchase price, post-purchase satisfaction, and safety\nmeasures/regulations on the forecasted AV market share. With an annual AV price\nreduction of 5% at the initial purchase price of $40,000 and 90% of satisfied\nadopters, AVs are forecasted to attain around 85% market share in thirty years.\nThese findings are crucial for policymakers to develop infrastructure plans and\nmanufacturers to conduct an after-sales cost-benefit analysis.\n"
    },
    {
        "paper_id": 2109.06322,
        "authors": "Mathias Beiglb\\\"ock, Benjamin Jourdain, William Margheriti, Gudmund\n  Pammer",
        "title": "Stability of the Weak Martingale Optimal Transport Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While many questions in (robust) finance can be posed in the martingale\noptimal transport (MOT) framework, others require to consider also non-linear\ncost functionals. Following the terminology of Gozlan, Roberto, Samson and\nTetali this corresponds to weak martingale optimal transport (WMOT).\n  In this article we establish stability of WMOT which is important since\nfinancial data can give only imprecise information on the underlying marginals.\nAs application, we deduce the stability of the superreplication bound for VIX\nfutures as well as the stability of stretched Brownian motion and we derive a\nmonotonicity principle for WMOT.\n"
    },
    {
        "paper_id": 2109.06378,
        "authors": "Chonghu Guan, Zuo Quan Xu, Fahuai Yi",
        "title": "A consumption-investment model with state-dependent lower bound\n  constraint on consumption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a life-time consumption-investment problem under the\nBlack-Scholes framework, where the consumption rate is subject to a lower bound\nconstraint that linearly depends on her wealth. It is a stochastic control\nproblem with state-dependent control constraint to which the standard\nstochastic control theory cannot be directly applied. We overcome this by\ntransforming it into an equivalent stochastic control problem in which the\ncontrol constraint is state-independent so that the standard theory can be\napplied. We give an explicit optimal consumption-investment strategy when the\nconstraint is homogeneous. When the constraint is non-homogeneous, it is shown\nthat the value function is third-order continuously differentiable by\ndifferential equation approach, and a feedback form optimal\nconsumption-investment strategy is provided. According to our findings, if one\nis concerned with long-term more than short-term consumption, then she should\nalways consume as few as possible; otherwise, she should consume optimally when\nher wealth is above a threshold, and consume as few as possible when her wealth\nis below the threshold.\n"
    },
    {
        "paper_id": 2109.06453,
        "authors": "Dongwoo Kim and Young Jun Lee",
        "title": "Vaccination strategies and transmission of COVID-19: evidence across\n  advanced countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given limited supply of approved vaccines and constrained medical resources,\ndesign of a vaccination strategy to control a pandemic is an economic problem.\nWe use time-series and panel methods with real-world country-level data to\nestimate effects on COVID-19 cases and deaths of two key elements of mass\nvaccination - time between doses and vaccine type. We find that new infections\nand deaths are both significantly negatively associated with the fraction of\nthe population vaccinated with at least one dose. Conditional on first-dose\ncoverage, an increased fraction with two doses appears to offer no further\nreductions in new cases and deaths. For vaccines from China, however, we find\nsignificant effects on both health outcomes only after two doses. Our results\nsupport a policy of extending the interval between first and second doses of\nvaccines developed in Europe and the US. As vaccination progresses, population\nmobility increases, which partially offsets the direct effects of vaccination.\nThis suggests that non-pharmaceutical interventions remain important to contain\ntransmission as vaccination is rolled out.\n"
    },
    {
        "paper_id": 2109.06567,
        "authors": "Zhe Wang and Ryan Martin",
        "title": "Gibbs posterior inference on a Levy density under discrete sampling",
        "comments": "35 pages, 3 figures. Comments welcome at\n  https://researchers.one/articles/21.09.00013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In mathematical finance, Levy processes are widely used for their ability to\nmodel both continuous variation and abrupt, discontinuous jumps. These jumps\nare practically relevant, so reliable inference on the feature that controls\njump frequencies and magnitudes, namely, the Levy density, is of critical\nimportance. A specific obstacle to carrying out model-based (e.g., Bayesian)\ninference in such problems is that, for general Levy processes, the likelihood\nis intractable. To overcome this obstacle, here we adopt a Gibbs posterior\nframework that updates a prior distribution using a suitable loss function\ninstead of a likelihood. We establish asymptotic posterior concentration rates\nfor the proposed Gibbs posterior. In particular, in the most interesting and\npractically relevant case, we give conditions under which the Gibbs posterior\nconcentrates at (nearly) the minimax optimal rate, adaptive to the unknown\nsmoothness of the true Levy density.\n"
    },
    {
        "paper_id": 2109.06591,
        "authors": "Andrew R. Casey, Ilya Mandel, Prasun K. Ray",
        "title": "The impact of the COVID-19 pandemic on academic productivity",
        "comments": "Submitted to RSOS",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  'Publish or perish' is an expression describing the pressure on academics to\nconsistently publish research to ensure a successful career in academia. With a\nglobal pandemic that has changed the world, how has it changed academic\nproductivity? Here we show that academics are posting just as many publications\non the arXiv pre-print server as if there were no pandemic: 168,630 were posted\nin 2020, a +12.6% change from 2019 and $+1.4\\sigma$ deviation above the\npredicted 162,577 $\\pm$ 4,393. However, some immediate impacts are visible in\nindividual research fields. Conference cancellations have led to sharp drops in\npre-prints, but laboratory closures have had mixed effects. Only some\nexperimental fields show mild declines in outputs, with most being consistent\non previous years or even increasing above model expectations. The most\nsignificant change is a 50% increase ($+8\\sigma$) in quantitative biology\nresearch, all related to the COVID-19 pandemic. Some of these publications are\nby biologists using arXiv for the first time, and some are written by\nresearchers from other fields (e.g., physicists, mathematicians). While\nquantitative biology pre-prints have returned to pre-pandemic levels, 20% of\nthe research in this field is now focussed on the COVID-19 pandemic,\ndemonstrating a strong shift in research focus.\n"
    },
    {
        "paper_id": 2109.06847,
        "authors": "Giulio Caldarelli",
        "title": "Wrapping trust for interoperability. A study of wrapped tokens",
        "comments": "14 pages, 4 figures and 1 table. Oriented to a conference",
        "journal-ref": "Information 2022",
        "doi": "10.3390/info13010006",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As known, blockchains are traditionally blind to the real world. This implies\nthe reliance on third parties called oracles when extrinsic data is needed for\nsmart contracts. However, reintroducing trust and single point of failure,\noracles implementation is still controversial and debated. The blindness to the\nreal world makes blockchains also unable to communicate with each other\npreventing any form of interoperability. An early approach to the\ninteroperability issue is constituted by wrapped tokens, representing\nblockchain native tokens issued on a non-native blockchain. Similar to how\noracles reintroduce trust, and single point of failure, the issuance of wrapped\ntokens involves third parties whose characteristics need to be considered when\nevaluating the advantages of crossing-chains. This paper provides an overview\nof the wrapped tokens and the main technologies implemented in their issuance.\nAdvantages, as well as limitations, are also listed and discussed.\n"
    },
    {
        "paper_id": 2109.06995,
        "authors": "Xiaowei Hu, Jaejin Jang, Nabeel Hamoud, Amirsaman Bajgiran",
        "title": "Strategic Inventories in a Supply Chain with Downstream Cournot Duopoly",
        "comments": null,
        "journal-ref": "International Journal of Operational Research, 2021 Vol.42 No.4,\n  pp.524 - 542",
        "doi": "10.1504/IJOR.2021.119934",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The inventories carried in a supply chain as a strategic tool to influence\nthe competing firms are considered to be strategic inventories (SI). We present\na two-period game-theoretic supply chain model, in which a singular\nmanufacturer supplies products to a pair of identical Cournot duopolistic\nretailers. We show that the SI carried by the retailers under dynamic contract\nis Pareto-dominating for the manufacturer, retailers, consumers, the channel,\nand the society as well. We also find that the retailer's SI, however, can be\neliminated when the manufacturer commits wholesale contract or inventory\nholding cost is too high. In comparing the cases with and without downstream\ncompetition, we also show that the downstream Cournot duopoly undermines the\nretailers in profits, but benefits all others.\n"
    },
    {
        "paper_id": 2109.07005,
        "authors": "Saeed Marzban, Erick Delage, Jonathan Yumeng Li, Jeremie\n  Desgagne-Bouchard, Carl Dussault",
        "title": "WaveCorr: Correlation-savvy Deep Reinforcement Learning for Portfolio\n  Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The problem of portfolio management represents an important and challenging\nclass of dynamic decision making problems, where rebalancing decisions need to\nbe made over time with the consideration of many factors such as investors\npreferences, trading environments, and market conditions. In this paper, we\npresent a new portfolio policy network architecture for deep reinforcement\nlearning (DRL)that can exploit more effectively cross-asset dependency\ninformation and achieve better performance than state-of-the-art architectures.\nIn particular, we introduce a new property, referred to as \\textit{asset\npermutation invariance}, for portfolio policy networks that exploit multi-asset\ntime series data, and design the first portfolio policy network, named\nWaveCorr, that preserves this invariance property when treating asset\ncorrelation information. At the core of our design is an innovative permutation\ninvariant correlation processing layer. An extensive set of experiments are\nconducted using data from both Canadian (TSX) and American stock markets (S&P\n500), and WaveCorr consistently outperforms other architectures with an\nimpressive 3%-25% absolute improvement in terms of average annual return, and\nup to more than 200% relative improvement in average Sharpe ratio. We also\nmeasured an improvement of a factor of up to 5 in the stability of performance\nunder random choices of initial asset ordering and weights. The stability of\nthe network has been found as particularly valuable by our industrial partner.\n"
    },
    {
        "paper_id": 2109.07077,
        "authors": "Shreya Biswas",
        "title": "Effect of mobile financial services on financial behavior in developing\n  economies-Evidence from India",
        "comments": "22 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study examines the relationship between mobile financial services and\nindividual financial behavior in India wherein a sizeable population is yet to\nbe financially included. Addressing the endogeneity associated with the use of\nmobile financial services using an instrumental variable method, the study\nfinds that the use of mobile financial services increases the likelihood of\ninvestment, having insurance and borrowing from formal financial institutions.\nFurther, the analysis highlights that access to mobile financial services have\nthe potential to bridge the gender divide in financial inclusion. Fastening the\npace of access to mobile financial services may partially alter pandemic\ninduced poverty.\n"
    },
    {
        "paper_id": 2109.07211,
        "authors": "Jiamin Yu",
        "title": "Risk Measurement, Risk Entropy, and Autonomous Driving Risk Modeling",
        "comments": "11 pages, 5 figures, IME 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  It has been for a long time to use big data of autonomous vehicles for\nperception, prediction, planning, and control of driving. Naturally, it is\nincreasingly questioned why not using this big data for risk management and\nactuarial modeling. This article examines the emerging technical difficulties,\nnew ideas, and methods of risk modeling under autonomous driving scenarios.\nCompared with the traditional risk model, the novel model is more consistent\nwith the real road traffic and driving safety performance. More importantly, it\nprovides technical feasibility for realizing risk assessment and car insurance\npricing under a computer simulation environment.\n"
    },
    {
        "paper_id": 2109.07212,
        "authors": "Patricia Bickert, Cristian Grozea, Ronny Hans, Matthias Koch,\n  Christina Riehn, Armin Wolf",
        "title": "Optimising Rolling Stock Planning including Maintenance with Constraint\n  Programming and Quantum Annealing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose and compare Constraint Programming (CP) and Quantum Annealing (QA)\napproaches for rolling stock assignment optimisation considering necessary\nmaintenance tasks. In the CP approach, we model the problem with an\nAlldifferent constraint, extensions of the Element constraint, and logical\nimplications, among others. For the QA approach, we develop a quadratic\nunconstrained binary optimisation (QUBO) model. For evaluation, we use data\nsets based on real data from Deutsche Bahn and run the QA approach on real\nquantum computers from D-Wave. Classical computers are used to evaluate the CP\napproach as well as tabu search for the QUBO model. At the current development\nstage of the physical quantum annealers, we find that both approaches tend to\nproduce comparable results.\n"
    },
    {
        "paper_id": 2109.07267,
        "authors": "David Cerezo S\\'anchez",
        "title": "JUBILEE: Secure Debt Relief and Forgiveness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  JUBILEE is a securely computed mechanism for debt relief and forgiveness in a\nfrictionless manner without involving trusted third parties, leading to more\nharmonious debt settlements by incentivising the parties to truthfully reveal\ntheir private information. JUBILEE improves over all previous methods:\n  - individually rational, incentive-compatible, truthful/strategy-proof,\nex-post efficient, optimal mechanism for debt relief and forgiveness with\nprivate information\n  - by the novel introduction of secure computation techniques to debt relief,\nthe \"blessing of the debtor\" is hereby granted for the first time: debt\nsettlements with higher expected profits and a higher probability of success\nthan without using secure computation\n  A simple and practical implementation is included for \"The Secure\nSpreadsheet\". Another implementation is realised using Raziel smart contracts\non a blockchain with Pravuil consensus.\n"
    },
    {
        "paper_id": 2109.07928,
        "authors": "Rafa{\\l} M. {\\L}ochowski",
        "title": "BDG inequalities and their applications for model-free continuous price\n  paths with instant enforcement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Shafer and Vovk introduce in their book \\cite{ShaferVovk:2018} the notion of\n\\emph{instant enforcement} and \\emph{instantly blockable} properties. However,\nthey do not associate these notions with any outer measure, unlike what Vovk\ndid in the case of sets of ''typical'' price paths. In this paper we introduce\nan outer measure on the space $[0, +\\ns) \\times \\Omega$ which assigns zero\nvalue exactly to those sets (properties) of pairs of time $t$ and an elementary\nevent $\\omega$ which are instantly blockable. Next, for a slightly modified\nmeasure, we prove It\\^o's isometry and BDG inequalities, and then use them to\ndefine an It\\^o-type integral. Additionally, we prove few properties for the\nquadratic variation of model-free, continuous martingales, which hold with\ninstant enforcement.\n"
    },
    {
        "paper_id": 2109.07988,
        "authors": "Mohammadjavad Javadinasr, Tassio B. Magassy, Ehsan Rahimi, Motahare\n  (Yalda) Mohammadi, Amir Davatgari, Abolfazl (Kouros) Mohammadian, Deborah\n  Salon, Matthew Wigginton Bhagat-Conway, Rishabh Singh Chauhan, Ram M.\n  Pendyala, Sybil Derrible, Sara Khoeini",
        "title": "The Enduring Effects of COVID-19 on Travel Behavior in the United\n  States: A Panel Study on Observed and Expected Changes in Telecommuting, Mode\n  Choice, Online Shopping and Air Travel",
        "comments": "26 pages, 13 images",
        "journal-ref": null,
        "doi": "10.1016/j.trf.2022.09.019",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The explosive nature of Covid-19 transmission drastically altered the rhythm\nof daily life by forcing billions of people to stay at their homes. A critical\nchallenge facing transportation planners is to identify the type and the extent\nof changes in people's activity-travel behavior in the post-pandemic world. In\nthis study, we investigated the travel behavior evolution by analyzing a\nlongitudinal two-wave panel survey data conducted in the United States from\nApril 2020 to October 2020 (wave 1) and from November 2020 to May 2021(wave 2).\nEncompassing nearly 3,000 respondents across different states, we explored\npandemic-induced changes and underlying reasons in four major categories of\ntelecommute/telemedicine, commute mode choice, online shopping, and air travel.\nUpon concrete evidence, our findings substantiate significantly observed and\nexpected changes in habits and preferences. According to results, nearly half\nof employees anticipate having the alternative to telecommute and among which\n71% expect to work from home at least twice a week after the pandemic. In the\npost-pandemic period, auto and transit commuters are expected to be 9% and 31%\nless than pre-pandemic, respectively. A considerable rise in hybrid work and\ngrocery/non-grocery online shopping is expected. Moreover, 41% of pre-covid\nbusiness travelers expect to have fewer flights (after the pandemic) while only\n8% anticipate more, compared to the pre-pandemic. Upon our analyses, we discuss\na spectrum of policy implications in all mentioned areas.\n"
    },
    {
        "paper_id": 2109.08099,
        "authors": "Tianyong Zhou",
        "title": "An Economic Analysis on the Potential and Steady Growth of China: a\n  Practice Based on the Dualistic System Economics in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existing theorization of development economics and transition economics\nis probably inadequate and perhaps even flawed to accurately explain and\nanalyze a dual economic system such as that in China. China is a country in the\ntransition of dual structure and system. The reform of its economic system has\nbrought off a long period of transformation. The allocation of factors is\nsubjected to the dualistic regulation of planning or administration and market\ndue to the dualistic system, and thus the signal distortion will be a commonly\nseen existence. From the perspective of balanced and safe growth, the\ninstitutional distortions of population birth, population flow, land\ntransaction and housing supply, with the changing of export, may cause great\ninfluences on the production demand, which includes the iterative contraction\nof consumption, the increase of export competitive cost, the widening of\nurban-rural income gap, the transferring of residents' income and the crowding\nout of consumption. In view of the worldwide shift from a conservative model\nwith more income than expenditure to the debt-based model with more expenditure\nthan income and the need for loose monetary policy, we must explore a basic\nmodel that includes variables of debt and land assets that affecting money\nsupply and price changes, especially in China, where the current debt ratio is\nhigh and is likely to rise continuously. Based on such a logical framework of\ndualistic system economics and its analysis method, a preliminary calculation\nsystem is formed through the establishment of models.\n"
    },
    {
        "paper_id": 2109.08124,
        "authors": "Dr Mohammad Rafiqul Islam and Dr Nicholas Sim",
        "title": "Education and Food Consumption Patterns: Quasi-Experimental Evidence\n  from Indonesia",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How does food consumption improve educational outcomes is an important policy\nissue for developing countries. Applying the Indonesian Family Life Survey\n(IFLS) 2014, we estimate the returns of food consumption to education and\ninvestigate if more educated individuals tend to consume healthier bundles than\nless-educated individuals do. We implement the Expected Outcome Methodology,\nwhich is similar to Average Treatment on The Treated (ATT) conceptualized by\nAngrist and Pischke (2009). We find that education tends to tilt consumption\ntowards healthier foods. Specifically, individuals with upper secondary or\nhigher levels of education, on average, consume 31.5% more healthy foods than\nthose with lower secondary education or lower levels of education. With respect\nto unhealthy food consumption, more highly-educated individuals, on average,\nconsume 22.8% less unhealthy food than less-educated individuals. This suggests\nthat education can increase the inequality in the consumption of healthy food\nbundles. Our study suggests that it is important to design policies to expand\neducation for all for at least up to higher secondary level in the context of\nIndonesia. Our finding also speaks to the link between food-health gradient and\nhuman capital formation for a developing country such as Indonesia.\n"
    },
    {
        "paper_id": 2109.08242,
        "authors": "Lawrence Middleton, James Dodd, Simone Rijavec",
        "title": "Trading styles and long-run variance of asset prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading styles can be classified into either trend-following or\nmean-reverting. If the net trading style is trend-following the traded asset is\nmore likely to move in the same direction it moved previously (the opposite is\ntrue if the net style is mean-reverting). The result of this is to introduce\npositive (or negative) correlations into the time series. We here explore the\neffect of these correlations on the long-run variance of the series through\nprobabilistic models designed to explicitly capture the direction of trading.\nOur theoretical insights suggests that relative to random walk models of asset\nprices the long-run variance is increased under trend-following strategies and\ncan actually be reduced under mean-reversal conditions. We apply these models\nto some of the largest US stocks by market capitalisation as well as\nhigh-frequency EUR/USD data and show that in both these settings, the ability\nto predict the asset price is generally increased relative to a random walk.\n"
    },
    {
        "paper_id": 2109.08738,
        "authors": "Svetlana Boyarchenko, Sergei Levendorski\\u{i}, J. Lars Kirkby and\n  Zhenyu Cui",
        "title": "SINH-acceleration for B-spline projection with Option Pricing\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We clarify the relations among different Fourier-based approaches to option\npricing, and improve the B-spline probability density projection method using\nthe sinh-acceleration technique. This allows us to efficiently separate the\ncontrol of different sources of errors better than the FFT-based realization\nallows; in many cases, the CPU time decreases as well. We demonstrate the\nimprovement of the B-spline projection method through several numerical\nexperiments in option pricing, including European and barrier options, where\nthe SINH acceleration technique proves to be robust and accurate.\n"
    },
    {
        "paper_id": 2109.08939,
        "authors": "Lucy Huo, Ariah Klages-Mundt, Andreea Minca, Frederik Christian\n  M\\\"unter, Mads Rude Wind",
        "title": "Decentralized Governance of Stablecoins with Closed Form Valuation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-031-18679-0_4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model incentive security in non-custodial stablecoins and derive\nconditions for participation in a stablecoin system across risk absorbers\n(vaults/CDPs) and holders of governance tokens. We apply option pricing theory\nto derive closed form solutions to the stakeholders' problems, and to value\ntheir positions within the capital structure of the stablecoin. We derive the\noptimal interest rate that is incentive compatible, as well as conditions for\nthe existence of equilibria without governance attacks, and discuss\nimplications for designing secure protocols.\n"
    },
    {
        "paper_id": 2109.09238,
        "authors": "Ehsan Samani, Mahdi Kohansal, Hamed Mohsenian-Rad",
        "title": "A Data-Driven Convergence Bidding Strategy Based on Reverse Engineering\n  of Market Participants' Performance: A Case of California ISO",
        "comments": "IEEE Transactions on Power Systems",
        "journal-ref": null,
        "doi": "10.1109/TPWRS.2021.3114362",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Convergence bidding, a.k.a., virtual bidding, has been widely adopted in\nwholesale electricity markets in recent years. It provides opportunities for\nmarket participants to arbitrage on the difference between the day-ahead market\nlocational marginal prices and the real-time market locational marginal prices.\nGiven the fact that convergence bids (CBs) have a significant impact on the\noperation of electricity markets, it is important to understand how market\nparticipants strategically select their CBs in real-world. We address this open\nproblem with focus on the electricity market that is operated by the California\nISO. In this regard, we use the publicly available electricity market data to\nlearn, characterize, and evaluate different types of convergence bidding\nstrategies that are currently used by market participants. Our analysis\nincludes developing a data-driven reverse engineering method that we apply to\nthree years of real-world data. Our analysis involves feature selection and\ndensity-based data clustering. It results in identifying three main clusters of\nCB strategies in the California ISO market. Different characteristics and the\nperformance of each cluster of strategies are analyzed. Interestingly, we\nunmask a common real-world strategy that does not match any of the existing\nstrategic convergence bidding methods in the literature. Next, we build upon\nthe lessons learned from the existing real-world strategies to propose a new CB\nstrategy that can significantly outperform them. Our analysis includes\ndeveloping a new strategy for convergence bidding. The new strategy has three\nsteps: net profit maximization by capturing price spikes, dynamic node\nlabeling, and strategy selection algorithm. We show through case studies that\nthe annual net profit for the most lucrative market participants can increase\nby over 40% if the proposed convergence bidding strategy is used.\n"
    },
    {
        "paper_id": 2109.09302,
        "authors": "Nazym Azimbayev and Yerkin Kitapbayev",
        "title": "On the valuation of multiple reset options: integral equation approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study a pricing problem of the multiple reset put option,\nwhich allows the holder to reset several times a current strike price to obtain\nan at-the-money European put option. We formulate the pricing problem as a\nmultiple optimal stopping problem, then reduce it to a sequence of single\noptimal stopping problems and study the associated free-boundary problems. We\nsolve this sequence of problems by induction in the number of remaining reset\nrights and exploit probabilistic arguments such as local time-space calculus on\ncurves. As a result, we characterize each optimal reset boundary as the unique\nsolution to a nonlinear integral equation and derive the reset premium\nrepresentations for the option prices. We propose that the multiple reset\noptions can be used as cryptocurrency derivatives and an attractive alternative\nto standard European options due to the extreme volatility of underlying\ncryptocurrencies.\n"
    },
    {
        "paper_id": 2109.09386,
        "authors": "Federico Guglielmo Morelli, Karl Naumann-Woleske, Michael Benzaquen,\n  Marco Tarzia, Jean-Philippe Bouchaud",
        "title": "Economic Crises in a Model with Capital Scarcity and Self-Reflexive\n  Confidence",
        "comments": "17 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the General Theory, Keynes remarked that the economy's state depends on\nexpectations, and that these expectations can be subject to sudden swings. In\nthis work, we develop a multiple equilibria behavioural business cycle model\nthat can account for demand or supply collapses due to abrupt drops in consumer\nconfidence, which affect both consumption propensity and investment. We show\nthat, depending on the model parameters, four qualitatively different outcomes\ncan emerge, characterised by the frequency of capital scarcity and/or demand\ncrises. In the absence of policy measures, the duration of such crises can\nincrease by orders of magnitude when parameters are varied, as a result of the\n\"paradox of thrift\". Our model suggests policy recommendations that prevent the\neconomy from getting trapped in extended stretches of low output, low\ninvestment and high unemployment.\n"
    },
    {
        "paper_id": 2109.09515,
        "authors": "Shreya Biswas",
        "title": "She Innovates- Female owner and firm innovation in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using data from World Bank Enterprises Survey 2014, we find that having a\nfemale owner in India increases firm innovation probability using both input\nand output indicators of innovation. We account for possible endogeneity of\nfemale owner variable using a two stage instrumental variable probit model. We\nfind that the positive effect of female owner variable is observed in the\nsub-samples of firms with more access to internal funding, young firms and\nfirms located in regions with no or less crime This study highlights the need\nto promote female entrepreneurship as a potential channel for promoting firm\ninnovation in India.\n"
    },
    {
        "paper_id": 2109.09871,
        "authors": "Ned Augenblick, Eben Lazarus, Michael Thaler",
        "title": "Overinference from Weak Signals and Underinference from Strong Signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When people receive new information, sometimes they revise their beliefs too\nmuch, and sometimes too little. In this paper, we show that a key driver of\nwhether people overinfer or underinfer is the strength of the information.\nBased on a model in which people know which direction to update in, but not\nexactly how much to update, we hypothesize that people will overinfer from weak\nsignals and underinfer from strong signals. We then test this hypothesis across\nfour different environments: abstract experiments, a naturalistic experiment,\nsports betting markets, and financial markets. In each environment, our\nconsistent and robust finding is overinference from weak signals and\nunderinference from strong signals. Our framework and findings can help\nharmonize apparently contradictory results from the experimental and empirical\nliteratures.\n"
    },
    {
        "paper_id": 2109.10009,
        "authors": "Lin William Cong, Ke Tang, Bing Wang, Jingyuan Wang",
        "title": "An AI-assisted Economic Model of Endogenous Mobility and Infectious\n  Diseases: The Case of COVID-19 in the United States",
        "comments": "Preprint, not peer reviewed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build a deep-learning-based SEIR-AIM model integrating the classical\nSusceptible-Exposed-Infectious-Removed epidemiology model with forecast modules\nof infection, community mobility, and unemployment. Through linking Google's\nmulti-dimensional mobility index to economic activities, public health status,\nand mitigation policies, our AI-assisted model captures the populace's\nendogenous response to economic incentives and health risks. In addition to\nbeing an effective predictive tool, our analyses reveal that the long-term\neffective reproduction number of COVID-19 equilibrates around one before mass\nvaccination using data from the United States. We identify a \"policy frontier\"\nand identify reopening schools and workplaces to be the most effective. We also\nquantify protestors' employment-value-equivalence of the Black Lives Matter\nmovement and find that its public health impact to be negligible.\n"
    },
    {
        "paper_id": 2109.10058,
        "authors": "Magnus Hansson",
        "title": "Evolution of topics in central bank speech communication",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the content of central bank speech communication from 1997\nthrough 2020 and asks the following questions: (i) What global topics do\ncentral banks talk about? (ii) How do these topics evolve over time? I turn to\nnatural language processing, and more specifically Dynamic Topic Models, to\nanswer these questions. The analysis consists of an aggregate study of nine\nmajor central banks and a case study of the Federal Reserve, which allows for\nregion specific control variables. I show that: (i) Central banks address a\nbroad range of topics. (ii) The topics are well captured by Dynamic Topic\nModels. (iii) The global topics exhibit strong and significant autoregressive\nproperties not easily explained by financial control variables.\n"
    },
    {
        "paper_id": 2109.10072,
        "authors": "Solveig Flaig and Gero Junike",
        "title": "Scenario generation for market risk models using generative neural\n  networks",
        "comments": null,
        "journal-ref": "Risks 10.11 (2022): 199",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this research, we show how to expand existing approaches of using\ngenerative adversarial networks (GANs) as economic scenario generators (ESG) to\na whole internal market risk model - with enough risk factors to model the full\nband-width of investments for an insurance company and for a one year time\nhorizon as required in Solvency 2. We demonstrate that the results of a\nGAN-based internal model are similar to regulatory approved internal models in\nEurope. Therefore, GAN-based models can be seen as a data-driven alternative\nway of market risk modeling.\n"
    },
    {
        "paper_id": 2109.10177,
        "authors": "Matheus R. Grasselli and Alexander Lipton",
        "title": "Cryptocurrencies and the Future of Money",
        "comments": "Submitted to \"Central banking, monetary policy and the future of\n  money\", Louis-Philippe Rochon, Sylvio Kappes and Guillaume Vallet (eds),\n  Edward Elgar, 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We review different classes of cryptocurrencies with emphasis on their\neconomic properties. Pure-asset coins such as Bitcoin, Ethereum and Ripple are\ncharacterized by not being a liability of any economic agent and most resemble\ncommodities such as gold. Central bank digital currencies, at the other end of\nthe economic spectrum, are liabilities of a Central Bank and most resemble\ncash. In between, there exist a range of so-called stable coins, with varying\ndegrees of economic complexity. We use balance sheet operations to highlight\nthe properties of each class of cryptocurrency and their potential uses. In\naddition, we propose the basic structure for a macroeconomic model\nincorporating all the different types of cryptocurrencies under consideration.\n"
    },
    {
        "paper_id": 2109.10429,
        "authors": "Nik Alexandrov and Dave Cliff and Charlie Figuero",
        "title": "Exploring Coevolutionary Dynamics of Competitive Arms-Races Between\n  Infinitely Diverse Heterogenous Adaptive Automated Trader-Agents",
        "comments": "17 pages; 4 figures; 54 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report on a series of experiments in which we study the coevolutionary\n\"arms-race\" dynamics among groups of agents that engage in adaptive automated\ntrading in an accurate model of contemporary financial markets. At any one\ntime, every trader in the market is trying to make as much profit as possible\ngiven the current distribution of different other trading strategies that it\nfinds itself pitched against in the market; but the distribution of trading\nstrategies and their observable behaviors is constantly changing, and changes\nin any one trader are driven to some extent by the changes in all the others.\nPrior studies of coevolutionary dynamics in markets have concentrated on\nsystems where traders can choose one of a small number of fixed pure\nstrategies, and can change their choice occasionally, thereby giving a market\nwith a discrete phase-space, made up of a finite set of possible system states.\nHere we present first results from two independent sets of experiments, where\nwe use minimal-intelligence trading-agents but in which the space of possible\nstrategies is continuous and hence infinite. Our work reveals that by taking\nonly a small step in the direction of increased realism we move immediately\ninto high-dimensional phase-spaces, which then present difficulties in\nvisualising and understanding the coevolutionary dynamics unfolding within the\nsystem. We conclude that further research is required to establish better\nanalytic tools for monitoring activity and progress in co-adapting markets. We\nhave released relevant Python code as open-source on GitHub, to enable others\nto continue this work.\n"
    },
    {
        "paper_id": 2109.10517,
        "authors": "Tomoo Kikuchi and Satoshi Tobe",
        "title": "Does Foreign Debt Contribute to Economic Growth?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the relationship between foreign debt and GDP growth using a panel\ndataset of 50 countries from 1997 to 2015. We find that economic growth\ncorrelates positively with foreign debt and that the relationship is causal in\nnature by using the sovereign credit default swap spread as an instrumental\nvariable. Furthermore, we find that foreign debt increases investment and then\nGDP growth in subsequent years. Our findings suggest that lower sovereign\ndefault risks lead to higher foreign debt contributing to GDP growth more in\nOECD than non-OECD countries.\n"
    },
    {
        "paper_id": 2109.10567,
        "authors": "Areski Cousin (IRMA), J\\'er\\^ome Lelong (DAO), Tom Picard (DAO)",
        "title": "Rating transitions forecasting: a filtering approach",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance, In press",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Analyzing the effect of business cycle on rating transitions has been a\nsubject of great interest these last fifteen years, particularly due to the\nincreasing pressure coming from regulators for stress testing. In this paper,\nwe consider that the dynamics of rating migrations is governed by an unobserved\nlatent factor. Under a point process filtering framework, we explain how the\ncurrent state of the hidden factor can be efficiently inferred from\nobservations of rating histories. We then adapt the classical Baum-Welsh\nalgorithm to our setting and show how to estimate the latent factor parameters.\nOnce calibrated, we may reveal and detect economic changes affecting the\ndynamics of rating migration, in real-time. To this end we adapt a filtering\nformula which can then be used for predicting future transition probabilities\naccording to economic regimes without using any external covariates. We propose\ntwo filtering frameworks: a discrete and a continuous version. We demonstrate\nand compare the efficiency of both approaches on fictive data and on a\ncorporate credit rating database. The methods could also be applied to retail\ncredit loans.\n"
    },
    {
        "paper_id": 2109.10662,
        "authors": "Masood Tadi, Irina Kortchmeski",
        "title": "Evaluation of Dynamic Cointegration-Based Pairs Trading Strategy in the\n  Cryptocurrency Market",
        "comments": "28 pages with 7 figures and 6 tables. Studies in Economics and\n  Finance, Vol. ahead-of-print No. ahead-of-print (2021)",
        "journal-ref": null,
        "doi": "10.1108/SEF-12-2020-0497",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This research aims to demonstrate a dynamic cointegration-based pairs trading\nstrategy, including an optimal look-back window framework in the cryptocurrency\nmarket, and evaluate its return and risk by applying three different scenarios.\nWe employ the Engle-Granger methodology, the Kapetanios-Snell-Shin (KSS) test,\nand the Johansen test as cointegration tests in different scenarios. We\ncalibrate the mean-reversion speed of the Ornstein-Uhlenbeck process to obtain\nthe half-life used for the asset selection phase and look-back window\nestimation. By considering the main limitations in the market microstructure,\nour strategy exceeds the naive buy-and-hold approach in the Bitmex exchange.\nAnother significant finding is that we implement a numerous collection of\ncryptocurrency coins to formulate the model's spread, which improves the\nrisk-adjusted profitability of the pairs trading strategy. Besides, the\nstrategy's maximum drawdown level is reasonably low, which makes it useful to\nbe deployed. The results also indicate that a class of coins has better\npotential arbitrage opportunities than others. This research has some\nnoticeable advantages, making it stand out from similar studies in the\ncryptocurrency market. First is the accuracy of data in which minute-binned\ndata create the signals in the formation period. Besides, to backtest the\nstrategy during the trading period, we simulate the trading signals using best\nbid/ask quotes and market trades. We exclusively take the order execution into\naccount when the asset size is already available at its quoted price (with one\nor more period gaps after signal generation). This action makes the backtesting\nmuch more realistic.\n"
    },
    {
        "paper_id": 2109.10779,
        "authors": "Sascha Desmettre, Simon Hochgerner, Sanela Omerovic, Stefan Thonhauser",
        "title": "A mean-field extension of the LIBOR market model",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance Vol. 25,\n  No. 01, 2250005 (2022)",
        "doi": "10.1142/S0219024922500054",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a mean-field extension of the LIBOR market model (LMM) which\npreserves the basic features of the original model. Among others, these\nfeatures are the martingale property, a directly implementable calibration and\nan economically reasonable parametrization of the classical LMM. At the same\ntime, the mean-field LIBOR market model (MF-LMM) is designed to reduce the\nprobability of exploding scenarios, arising in particular in the\nmarket-consistent valuation of long-term guarantees. To this end, we prove\nexistence and uniqueness of the corresponding MF-LMM and investigate its\npractical aspects, including a Black '76-type formula. Moreover, we present an\nextensive numerical analysis of the MF-LMM. The corresponding Monte Carlo\nmethod is based on a suitable interacting particle system which approximates\nthe underlying mean-field equation.\n"
    },
    {
        "paper_id": 2109.10814,
        "authors": "Anthony E. Brockwell",
        "title": "Fractional Growth Portfolio Investment",
        "comments": "19 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We review some fundamental concepts of investment from a mathematical\nperspective, concentrating specifically on fractional-Kelly portfolios, which\nallocate a fraction of wealth to a growth-optimal portfolio while the remainder\ncollects (or pays) interest at a risk-free rate. We elucidate a coherent\ncontinuous-parameter time-series framework for analysis of these portfolios,\nexplaining relationships between Sharpe ratios, growth rates, and leverage. We\nsee how Kelly's criterion prescribes the same leverage as Markowitz\nmean-variance optimization. Furthermore, for fractional Kelly portfolios, we\nstate a simple distributional relationship between portfolio Sharpe ratio, the\nfractional coefficient, and portfolio log-returns. These results provide\ncritical insight into realistic expectations of growth for different classes of\ninvestors, from individuals to quantitative trading operations.\n  We then illustrate application of the results by analyzing performance of\nvarious bond and equity mixes for an investor. We also demonstrate how the\nrelationships can be exploited by a simple method-of-moments calculation to\nestimate portfolio Sharpe ratios and levels of risk deployment, given a fund's\nreported returns.\n"
    },
    {
        "paper_id": 2109.10818,
        "authors": "Hyong-Chol O, Tae-Song Kim, Tae-Song Choe",
        "title": "Solution Representations of Solving Problems for the Black-Scholes\n  equations and Application to the Pricing Options on Bond with Credit Risk",
        "comments": "12 pages, 5 figures, in version 2 some errors corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper is investigated the pricing problem of options on bonds with\ncredit risk based on analysis on two kinds of solving problems for the\nBlack-Scholes equations. First, a solution representation of the Black-Scholes\nequation with the maturity payoff function which is the product of the power\nfunction, normal distribution function and characteristic function is provided.\nThen a solution representation of a special terminal boundary value problem of\nthe Black-Sholes equation is provided and its monotonicity is proved. The\nsimplest case of the structural model of the credit bond is studied and its\npricing formula is provided, and based on the results, the pricing model of\noption on corporate bond with credit risk is transformed into a terminal\nboundary value problem of the Black-Scholes equation with some special maturity\npayoff functions and the solution formula is obtained. Using it, we provide the\npricing formulae of the puttable and callable bonds with credit risk.\n"
    },
    {
        "paper_id": 2109.10832,
        "authors": "Alessio Muscillo, Simona Re, Sergio Gambacorta, Giuseppe Ferrara,\n  Nicola Tagliafierro, Emiliano Borello, Alessandro Rubino, Angelo Facchini",
        "title": "Circular City Index: An Open Data analysis to assess the urban\n  circularity preparedness of cities to address the green transition -- A study\n  on the Italian municipalities",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a circularity transition index based on open data principles and\ncircularity of energy, material, and information. The aim of the Circular City\nIndex is to provide data and a succinct measurement of the attributes related\nto municipalities performances that can support the definition of green\npolicies at national and local level. We have identified a set of key\nperformance indicators, defined at municipality level, measuring factors that,\ndirectly and indirectly, could influence circularity and green transition, with\na focus on the green new deal vision embraced by the European Union. The CCI is\ntested on a open dataset that collects data covering 100% of the Italian\nmunicipalities (7,904). Our results show that the computation of the CCI on a\nlarge sample leads to a normal distribution of the index, suggesting\ndisparities both under the territorial point of view and under the point of\nview of city size. Results provide useful information to practitioner, policy\nmaker and experts from academia alike, to define effective tools able to\nunderpin a careful planning of investments supported by the national recovery\nand resilience plan recently issued by the Italian government. This may be\nparticularly useful to enhance enabling factors of the green transition that\nmay differ across territories, helping policymakers to promote a smooth and\nfair transition by fostering the preparedness of municipalities in addressing\nthe challenge.\n"
    },
    {
        "paper_id": 2109.10946,
        "authors": "Simon Fritzsch, Maike Timphus, Gregor Weiss",
        "title": "Marginals Versus Copulas: Which Account For More Model Risk In\n  Multivariate Risk Forecasting?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Copulas. We study the model risk of multivariate risk models in a\ncomprehensive empirical study on Copula-GARCH models used for forecasting\nValue-at-Risk and Expected Shortfall. To determine whether model risk inherent\nin the forecasting of portfolio risk is caused by the candidate marginal or\ncopula models, we analyze different groups of models in which we fix either the\nmarginals, the copula, or neither. Model risk is economically significant, is\nespecially high during periods of crisis, and is almost completely due to the\nchoice of the copula. We then propose the use of the model confidence set\nprocedure to narrow down the set of available models and reduce model risk for\nCopula-GARCH risk models. Our proposed approach leads to a significant\nimprovement in the mean absolute deviation of one day ahead forecasts by our\nvarious candidate risk models.\n"
    },
    {
        "paper_id": 2109.10958,
        "authors": "Pietro Saggese, Alessandro Belmonte, Nicola Dimitri, Angelo Facchini,\n  Rainer B\\\"ohme",
        "title": "Who are the arbitrageurs? Empirical evidence from Bitcoin traders in the\n  Mt. Gox exchange platform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We mine the leaked history of trades on Mt. Gox, the dominant Bitcoin\nexchange from 2011 to early 2014, to detect the triangular arbitrage activity\nconducted within the platform. The availability of user identifiers per trade\nallows us to focus on the historical record of 440 investors, detected as\narbitrageurs, and consequently to describe their trading behavior. We begin by\nshowing that a considerable difference appears between arbitrageurs when\nindicators of their expertise are taken into account. In particular, we\ndistinguish between those who conducted arbitrage in a single or in multiple\nmarkets: using this element as a proxy for trade ability, we find that\narbitrage actions performed by expert users are on average non-profitable when\ntransaction costs are accounted for, while skilled investors conduct arbitrage\nat a positive and statistically significant premium. Next, we show that\nspecific trading strategies, such as splitting orders or conducting arbitrage\nnon aggressively, are further indicators of expertise that increase the\nprofitability of arbitrage. Most importantly, we exploit within-user (across\nhours and markets) variation and document that expert users make profits on\narbitrage by reacting quickly to plausible exogenous variations on the official\nexchange rates. We present further evidence that such differences are chiefly\ndue to a better ability of the latter in incorporating information, both on the\ntransactions costs and on the exchange rates volatility, eventually resulting\nin a better timing choice at small time scale intervals. Our results support\nthe hypothesis that arbitrageurs are few and sophisticated users.\n"
    },
    {
        "paper_id": 2109.10968,
        "authors": "Claudia Cerrone and Francesco Feri and Philip R. Neary",
        "title": "Ignorance is Bliss: A Game of Regret",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An individual can only experience regret if she learns about an unchosen\nalternative. In many situations, learning about an unchosen alternative is\npossible only if someone else chose it. We develop a model where the ex-post\ninformation available to each regret averse individual depends both on their\nown choice and on the choices of others, as others can reveal ex-post\ninformation about what might have been. This implies that what appears to be a\nseries of isolated single-person decision problems is in fact a rich\nmulti-player behavioural game, the regret game, where the psychological payoffs\nthat depend on ex-post information are interconnected. For an open set of\nparameters, the regret game is a coordination game with multiple equilibria,\ndespite the fact that all individuals possess a uniquely optimal choice in\nisolation. We experimentally test this prediction and find support for it.\n"
    },
    {
        "paper_id": 2109.11403,
        "authors": "R\\\"udiger Frey and Verena K\\\"ock",
        "title": "Deep Neural Network Algorithms for Parabolic PIDEs and Applications in\n  Insurance Mathematics",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years a large literature on deep learning based methods for the\nnumerical solution partial differential equations has emerged; results for\nintegro-differential equations on the other hand are scarce. In this paper we\nstudy deep neural network algorithms for solving linear and semilinear\nparabolic partial integro-differential equations with boundary conditions in\nhigh dimension. To show the viability of our approach we discuss several case\nstudies from insurance and finance.\n"
    },
    {
        "paper_id": 2109.1155,
        "authors": "Muhammad Salar Khan",
        "title": "Absorptive capacities and economic growth in low and middle income\n  economies",
        "comments": "79 pages including figures, tables, and appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I extend the concept of absorptive capacity, used in the analysis of firms,\nto a framework applicable to the national level. First, employing confirmatory\nfactor analyses on 47 variables, I build 13 composite factors crucial to\nmeasuring six national level capacities: technological capacity, financial\ncapacity, human capacity, infrastructural capacity, public policy capacity, and\nsocial capacity. My data cover most low- and middle-income- economies (LMICs),\neligible for the World Bank's International Development Association (IDA)\nsupport between 2005 and 2019. Second, I analyze the relationship between the\nestimated capacity factors and economic growth while controlling for some of\nthe incoming flows from abroad and other confounders that might influence the\nrelationship. Lastly, I conduct K-means cluster analysis and then analyze the\nresults alongside regression estimates to glean patterns and classifications\nwithin the LMICs. Results indicate that enhancing infrastructure (ICT, energy,\ntrade, and transport), financial (apparatus and environment), and public policy\ncapacities is a prerequisite for attaining economic growth. Similarly, I find\nimproving human capital with specialized skills positively impacts economic\ngrowth. Finally, by providing a ranking of which capacity is empirically more\nimportant for economic growth, I offer suggestions to governments with limited\nbudgets to make wise investments. Likewise, my findings inform international\npolicy and monetary bodies on how they could better channel their funding in\nLMICs to achieve sustainable development goals and boost shared prosperity.\n"
    },
    {
        "paper_id": 2109.11917,
        "authors": "Ji-Won Park, Jaeup U. Kim, Cheol-Min Ghim and Chae Un Kim",
        "title": "The Boltzmann fair division for distributive justice",
        "comments": "36 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Fair division is a significant, long-standing problem and is closely related\nto social and economic justice. The conventional division methods such as\ncut-and-choose are hardly applicable to realworld problems because of their\ncomplexity and unrealistic assumptions about human behaviors. Here we propose a\nfair division method from a completely different perspective, using the\nBoltzmann distribution. The Boltzmann distribution adopted from the physical\nsciences gives the most probable and unbiased distribution derived from a\ngoods-centric, rather than a player-centric, division process. The mathematical\nmodel of the Boltzmann fair division was developed for both homogeneous and\nheterogeneous division problems, and the players' key factors (contributions,\nneeds, and preferences) could be successfully integrated. We show that the\nBoltzmann fair division is a well-balanced division method maximizing the\nplayers' total utility, and it could be easily finetuned and applicable to\ncomplex real-world problems such as income/wealth redistribution or\ninternational negotiations on fighting climate change.\n"
    },
    {
        "paper_id": 2109.12142,
        "authors": "Peter Reinhard Hansen and Chan Kim and Wade Kimbrough",
        "title": "Periodicity in Cryptocurrency Volatility and Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study recurrent patterns in volatility and volume for major\ncryptocurrencies, Bitcoin and Ether, using data from two centralized exchanges\n(Coinbase Pro and Binance) and a decentralized exchange (Uniswap V2). We find\nsystematic patterns in both volatility and volume across day-of-the-week,\nhour-of-the-day, and within the hour. These patterns have grown stronger over\nthe years and can be related to algorithmic trading and funding times in\nfutures markets. We also document that price formation mainly takes place on\nthe centralized exchanges while price adjustments on the decentralized\nexchanges can be sluggish.\n"
    },
    {
        "paper_id": 2109.12166,
        "authors": "Misha Perepelitsa",
        "title": "Psychological dimension of adaptive trading in cryptocurrency markets",
        "comments": "20 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we extend the analysis of an agent-based model for adaptive\ntrading, called asynchronous stochastic price pump (ASPP) introduced by\nPerepelitsa and Timofeyev (2019), to the model with heterogeneous distribution\nof psychological parameters of speculative optimism and pessimism across the\npopulation of traders. We show that the new model has a range of qualitatively\ndifferent dynamics when the correlation between those factors ranges from low\nnegative to large positive values. A statistical parameter estimation suggests\na heterogeneous ASPP with negative correlation as a model of price variations\nof Bitcoin.\n"
    },
    {
        "paper_id": 2109.12196,
        "authors": "Alex Lipton, and Artur Sepp",
        "title": "Automated Market-Making for Fiat Currencies",
        "comments": "16 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present an automated market-making (AMM) cross-settlement mechanism for\ndigital assets on interoperable blockchains, focusing on central bank digital\ncurrencies (CBDCs) and stable coins. We develop an innovative approach for\ngenerating fair exchange rates for on-chain assets consistent with traditional\noff-chain markets. We illustrate the efficacy of our approach on realized FX\nrates for G-10 currencies.\n"
    },
    {
        "paper_id": 2109.12337,
        "authors": "G. Mazzei, F.G. Bellora, J.A. Serur",
        "title": "Delta Hedging with Transaction Costs: Dynamic Multiscale Strategy using\n  Neural Nets",
        "comments": null,
        "journal-ref": "MACI 8 2021 p.459-462",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In most real scenarios the construction of a risk-neutral portfolio must be\nperformed in discrete time and with transaction costs. Two human imposed\nconstraints are the risk-aversion and the profit maximization, which together\ndefine a nonlinear optimization problem with a model-dependent solution. In\nthis context, an optimal fixed frequency hedging strategy can be determined a\nposteriori by maximizing a sharpe ratio simil path dependent reward function.\nSampling from Heston processes, a convolutional neural network was trained to\ninfer which period is optimal using partial information, thus leading to a\ndynamic hedging strategy in which the portfolio is hedged at various\nfrequencies, each weighted by the probability estimate of that frequency being\noptimal.\n"
    },
    {
        "paper_id": 2109.12477,
        "authors": "Zehao Chen, Yanchen Zhu, Tianyang Shen and Yufan Ye",
        "title": "Reputation dependent pricing strategy: analysis based on a Chinese C2C\n  marketplace",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most online markets establish reputation systems to assist building trust\nbetween sellers and buyers. Sellers' reputations not only provide guidelines\nfor buyers but may also inform sellers their optimal pricing strategy. In this\nresearch, we assumed two types of buyer: informed buyers and uninformed buyers.\nInformed buyers know more about the reputation about the seller but may incur a\nsearch cost. Then we developed a benchmark model and a competition model. We\nfound that high reputation sellers and low reputation sellers adapt different\npricing strategy depending on the informativeness of buyers and the competition\namong sellers. With a large proportion of informed buyers, high reputation\nsellers may charge lower price than low reputation sellers, which exists a\nnegative price premium effect, in contrast to conclusions of some previous\nstudies. Empirical findings were in consistence with our theoretical models. We\ncollected data of five categories of products, televisions, laptops, cosmetics,\nshoes, and beverages, from Taobao, a leading C2C Chinese online market.\nNegative price premium effect was observed for TVs, laptops, and cosmetics;\nprice premium effect was observed for beverages; no significant trend was\nobserved for shoes. We infer product value and market complexity are the main\nfactors of buyer informativeness.\n"
    },
    {
        "paper_id": 2109.12491,
        "authors": "M. Keith Chen, Katherine L. Christensen, Elicia John, Emily Owens, and\n  Yilin Zhuo",
        "title": "Smartphone Data Reveal Neighborhood-Level Racial Disparities in Police\n  Presence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  While extensive, research on policing in America has focused on documented\nactions such as stops and arrests -- less is known about patrolling and\npresence. We map the movements of over ten thousand police officers across\ntwenty-one of America's largest cities by combining anonymized smartphone data\nwith station and precinct boundaries. Police spend considerably more time in\nBlack neighborhoods, a disparity which persists after controlling for density,\nsocioeconomics, and crime-driven demand for policing. Our results suggest that\nroughly half of observed racial disparities in arrests are associated with this\nexposure disparity, which is lower in cities with more supervisor (but not\nofficer) diversity.\n"
    },
    {
        "paper_id": 2109.12568,
        "authors": "Abreu I., Mesias F.J., Ramajo, J",
        "title": "Design and validation of an index to measure development in rural areas\n  through stakeholder participation",
        "comments": "27 pages, 4 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes the development of an index to assess rural development\nbased on a set of 25 demographic, economic, environmental, and social welfare\nindicators previously selected through a Delphi approach. Three widely accepted\naggregation methods were then tested: a mixed arithmetic/geometric mean without\nweightings for each indicator; a weighted arithmetic mean using the weights\npreviously generated by the Delphi panel and an aggregation through Principal\nComponent Analysis. These three methodologies were later applied to 9\nPortuguese NUTS III regions, and the results were presented to a group of\nexperts in rural development who indicated which of the three forms of\naggregation best measured the levels of rural development of the different\nterritories. Finally, it was concluded that the unweighted arithmetic/geometric\nmean was the most accurate methodology for aggregating indicators to create a\nRural Development Index.\n"
    },
    {
        "paper_id": 2109.12621,
        "authors": "Eduardo Ramos-P\\'erez, Pablo J. Alonso-Gonz\\'alez, Jos\\'e Javier\n  N\\'u\\~nez-Vel\\'azquez",
        "title": "Multi-Transformer: A New Neural Network-Based Architecture for\n  Forecasting S&P Volatility",
        "comments": null,
        "journal-ref": "Mathematics 2021, 9, 1794",
        "doi": "10.3390/math9151794",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Events such as the Financial Crisis of 2007-2008 or the COVID-19 pandemic\ncaused significant losses to banks and insurance entities. They also\ndemonstrated the importance of using accurate equity risk models and having a\nrisk management function able to implement effective hedging strategies. Stock\nvolatility forecasts play a key role in the estimation of equity risk and,\nthus, in the management actions carried out by financial institutions.\nTherefore, this paper has the aim of proposing more accurate stock volatility\nmodels based on novel machine and deep learning techniques. This paper\nintroduces a neural network-based architecture, called Multi-Transformer.\nMulti-Transformer is a variant of Transformer models, which have already been\nsuccessfully applied in the field of natural language processing. Indeed, this\npaper also adapts traditional Transformer layers in order to be used in\nvolatility forecasting models. The empirical results obtained in this paper\nsuggest that the hybrid models based on Multi-Transformer and Transformer\nlayers are more accurate and, hence, they lead to more appropriate risk\nmeasures than other autoregressive algorithms or hybrid models based on feed\nforward layers or long short term memory cells.\n"
    },
    {
        "paper_id": 2109.12896,
        "authors": "Koichi Miyamoto, Kenji Kubo",
        "title": "Pricing multi-asset derivatives by finite difference method on a quantum\n  computer",
        "comments": "35 pages, no figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the recent great advance of quantum computing technology, there are\ngrowing interests in its applications to industries, including finance. In this\npaper, we focus on derivative pricing based on solving the Black-Scholes\npartial differential equation by finite difference method (FDM), which is a\nsuitable approach for some types of derivatives but suffers from the {\\it curse\nof dimensionality}, that is, exponential growth of complexity in the case of\nmultiple underlying assets. We propose a quantum algorithm for FDM-based\npricing of multi-asset derivative with exponential speedup with respect to\ndimensionality compared with classical algorithms. The proposed algorithm\nutilizes the quantum algorithm for solving differential equations, which is\nbased on quantum linear system algorithms. Addressing the specific issue in\nderivative pricing, that is, extracting the derivative price for the present\nunderlying asset prices from the output state of the quantum algorithm, we\npresent the whole of the calculation process and estimate its complexity. We\nbelieve that the proposed method opens the new possibility of accurate and\nhigh-speed derivative pricing by quantum computers.\n"
    },
    {
        "paper_id": 2109.12927,
        "authors": "Mathias Beiglb\\\"ock, George Lowther, Gudmund Pammer, Walter\n  Schachermayer",
        "title": "Faking Brownian motion with continuous Markov martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hamza-Klebaner posed the problem of constructing martingales with Brownian\nmarginals that differ from Brownian motion, so called fake Brownian motions.\nBesides its theoretical appeal, the problem represents the quintessential\nversion of the ubiquitous fitting problem in mathematical finance where the\ntask is to construct martingales that satisfy marginal constraints imposed by\nmarket data.\n  Non-continuous solutions to this challenge were given by Madan-Yor,\nHamza-Klebaner, Hobson, and Fan-Hamza-Klebaner whereas continuous (but\nnon-Markovian) fake Brownian motions were constructed by Oleszkiewicz, Albin,\nBaker-Donati-Yor, Hobson, Jourdain-Zhou.\n  In contrast it is known from Gy\\\"ongy, Dupire, and ultimately Lowther that\nBrownian motion is the unique continuous strong Markov martingale with Brownian\nmarginals.\n  We took this as a challenge to construct examples of a \"very fake'' Brownian\nmotion, that is, continuous Markov martingales with Brownian marginals that\nmiss out only on the strong Markov property.\n"
    },
    {
        "paper_id": 2109.1298,
        "authors": "Laurence Francis Lacey",
        "title": "On the nature of monetary and price inflation and hyperinflation",
        "comments": "24 pages, 6 figures, 2 tables, 1 appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Monetary inflation is a sustained increase in the money supply than can\nresult in price inflation, which is a rise in the general level of prices of\ngoods and services. The objectives of this paper were to develop economic\nmodels to (1) predict the annual rate of growth in the US consumer price index\n(CPI), based on the annual growth in the US broad money supply (BMS), the\nannual growth in US real GDP, and the annual growth in US savings, over the\ntime period 2001 to 2019; (2) investigate the means by which monetary and price\ninflation can develop into monetary and price hyperinflation. The hypothesis\nthat the annual rate of growth in the US CPI is a function of the annual growth\nin the US BMS minus the annual growth in US real GDP minus the annual growth in\nUS savings, over the time period investigated, has been shown to be the case.\nHowever, an exact relationship required the use of a non-zero residual term. A\nmathematical statistical formulation of a hyperinflationary process has been\nprovided and used to quantify the period of hyperinflation in the Weimar\nRepublic, from July 1922 until the end of November 1923.\n"
    },
    {
        "paper_id": 2109.13633,
        "authors": "Anik Burman and Sayantan Banerjee",
        "title": "High-dimensional Portfolio Optimization using Joint Shrinkage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the problem of optimizing a portfolio of financial assets, where\nthe number of assets can be much larger than the number of observations. The\noptimal portfolio weights require estimating the inverse covariance matrix of\nexcess asset returns, classical solutions of which behave badly in\nhigh-dimensional scenarios. We propose to use a regression-based joint\nshrinkage method for estimating the partial correlation among the assets.\nExtensive simulation studies illustrate the superior performance of the\nproposed method with respect to variance, weight, and risk estimation errors\ncompared with competing methods for both the global minimum variance portfolios\nand Markowitz mean-variance portfolios. We also demonstrate the excellent\nempirical performances of our method on daily and monthly returns of the\ncomponents of the S&P 500 index.\n"
    },
    {
        "paper_id": 2109.13796,
        "authors": "Karim Barigou (ISFA), Dani\\\"el Linders (UvA), Fan Yang",
        "title": "Actuarial-consistency and two-step actuarial valuations: a new paradigm\n  to insurance valuation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces new valuation schemes called actuarial-consistent\nvaluations for insurance liabilities which depend on both financial and\nactuarial risks, which imposes that all actuarial risks are priced via standard\nactuarial principles. We propose to extend standard actuarial principles by a\nnew actuarial-consistent procedure, which we call \"two-step actuarial\nvaluations\". \\bluebis{In the case valuations are coherent}, we show that\nactuarial-consistent valuations are equivalent to two-step actuarial\nvaluations. We also discuss the connection with \"two-step market-consistent\nvaluations\" from Pelsser and Stadje (2014). In particular, we discuss how the\ndependence structure between actuarial and financial risks impacts both\nactuarial-consistent and market-consistent valuations.\n"
    },
    {
        "paper_id": 2109.13851,
        "authors": "Shuo Sun, Rundong Wang, Bo An",
        "title": "Reinforcement Learning for Quantitative Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantitative trading (QT), which refers to the usage of mathematical models\nand data-driven techniques in analyzing the financial market, has been a\npopular topic in both academia and financial industry since 1970s. In the last\ndecade, reinforcement learning (RL) has garnered significant interest in many\ndomains such as robotics and video games, owing to its outstanding ability on\nsolving complex sequential decision making problems. RL's impact is pervasive,\nrecently demonstrating its ability to conquer many challenging QT tasks. It is\na flourishing research direction to explore RL techniques' potential on QT\ntasks. This paper aims at providing a comprehensive survey of research efforts\non RL-based methods for QT tasks. More concretely, we devise a taxonomy of\nRL-based QT models, along with a comprehensive summary of the state of the art.\nFinally, we discuss current challenges and propose future research directions\nin this exciting field.\n"
    },
    {
        "paper_id": 2109.13905,
        "authors": "Ye-Sheen Lim, Denise Gorse",
        "title": "Intra-Day Price Simulation with Generative Adversarial Modelling of the\n  Order Flow",
        "comments": "6 pages, ICMLA 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Intra-day price variations in financial markets are driven by the sequence of\norders, called the order flow, that is submitted at high frequency by traders.\nThis paper introduces a novel application of the Sequence Generative\nAdversarial Networks framework to model the order flow, such that random\nsequences of the order flow can then be generated to simulate the intra-day\nvariation of prices. As a benchmark, a well-known parametric model from the\nquantitative finance literature is selected. The models are fitted, and then\nmultiple random paths of the order flow sequences are sampled from each model.\nModel performances are then evaluated by using the generated sequences to\nsimulate price variations, and we compare the empirical regularities between\nthe price variations produced by the generated and real sequences. The\nempirical regularities considered include the distribution of the price\nlog-returns, the price volatility, and the heavy-tail of the log-returns\ndistributions. The results show that the order sequences from the generative\nmodel are better able to reproduce the statistical behaviour of real price\nvariations than the sequences from the benchmark.\n"
    },
    {
        "paper_id": 2109.13928,
        "authors": "Fintan Oeri, Adrian Rinscheid and Aya Kachi",
        "title": "Lobbying Influence -- The Role of Money, Strategies and Measurements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Comparing the results for preference attainment, self-perceived influence and\nreputational influence, this paper analyzes the relationship between financial\nresources and lobbying influence. The empirical analysis builds on data from an\noriginal survey with 312 Swiss energy policy stakeholders combined with\ndocument data from multiple policy consultation submission processes. The\nresults show that the distribution of influence varies substantially depending\non the measure. While financial resources for political purposes predict\ninfluence across all measures, the relationship is positive only for some. An\nanalysis of indirect effects sheds light on the potential mechanisms that\ntranslate financial resources into influence.\n"
    },
    {
        "paper_id": 2109.14204,
        "authors": "Philip Solimine and Luke Boosey",
        "title": "Strategic formation of collaborative networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We examine behavior in an experimental collaboration game that incorporates\nendogenous network formation. The environment is modeled as a generalization of\nthe voluntary contributions mechanism. By varying the information structure in\na controlled laboratory experiment, we examine the underlying mechanisms of\nreciprocity that generate emergent patterns in linking and contribution\ndecisions. Providing players more detailed information about the sharing\nbehavior of others drastically increases efficiency, and positively affects a\nnumber of other key outcomes. To understand the driving causes of these changes\nin behavior we develop and estimate a structural model for actions and small\nnetwork panels and identify how social preferences affect behavior. We find\nthat the treatment reduces altruism but stimulates reciprocity, helping players\ncoordinate to reach mutually beneficial outcomes. In a set of counterfactual\nsimulations, we show that increasing trust in the community would encourage\nhigher average contributions at the cost of mildly increased free-riding.\nIncreasing overall reciprocity greatly increases collaborative behavior when\nthere is limited information but can backfire in the treatment, suggesting that\nnegative reciprocity and punishment can reduce efficiency. The largest returns\nwould come from an intervention that drives players away from negative and\ntoward positive reciprocity.\n"
    },
    {
        "paper_id": 2109.14209,
        "authors": "Barry W. Brook, Jessie C. Buettel, Sanghyun Hong",
        "title": "Constrained scenarios for twenty-first century human population size\n  based on the empirical coupling to economic growth",
        "comments": "11 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Growth in the global human population this century will have momentous\nconsequences for societies and the environment. Population growth has come with\nhigher aggregate human welfare, but also climate change and biodiversity loss.\nBased on the well-established empirical association and plausible causal\nrelationship between economic and population growth, we devised a novel method\nfor forecasting population based on Gross Domestic Product (GDP) per capita.\nAlthough not mechanistically causal, our model is intuitive, transparent,\nreplicable, and grounded on historical data. Our central finding is that a\nricher world is likely to be associated with a lower population, an effect\nespecially pronounced in rapidly developing countries. In our baseline\nscenario, where GDP per capita follows a business-as-usual trajectory, global\npopulation is projected to reach 9.2 billion in 2050 and peak in 2062. With 50%\nhigher annual economic growth, population peaks even earlier, in 2056, and\ndeclines to below 8 billion by the end of the century. Without any economic\ngrowth after 2020, however, the global population will grow to 9.9 billion in\n2050 continue rising thereafter. Economic growth has the largest effect on\nlow-income countries. The gap between the highest and lowest GDP scenarios\nreaches almost 4 billion by 2100. Education and family planning are important\ndeterminants of population growth, but economic growth is also likely to be a\ndriver of slowing population growth by changing incentives for childbearing.\nSince economic growth could slow population growth, it will offset\nenvironmental impacts stemming from higher per-capita consumption of food,\nwater, and energy, and work in tandem with technological innovation.\n"
    },
    {
        "paper_id": 2109.1436,
        "authors": "Alessandro Ferracci and Giulio Cimini",
        "title": "Systemic risk in interbank networks: disentangling balance sheets and\n  network effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the difference between the level of systemic risk that is\nempirically measured on an interbank network and the risk that can be deduced\nfrom the balance sheets composition of the participating banks. Using\ngeneralised DebtRank dynamics, we measure observed systemic risk on e-MID\nnetwork data (augmented by BankFocus information) and compare it with the\nexpected systemic risk of a null model network, obtained through an appropriate\nmaximum-entropy approach constraining relevant balance sheet variables. We show\nthat the aggregate levels of observed and expected systemic risks are usually\ncompatible but differ significantly during turbulent times (in our case, after\nthe default of Lehman Brothers and the VLTRO implementation by the ECB). At the\nindividual level instead, banks are typically more or less risky than what\ntheir balance sheet prescribes due to their position in the network. Our\nresults confirm on one hand that balance sheet information used within a proper\nmaximum-entropy network model provides good aggregate estimates of systemic\nrisk, and on the other hand the importance of knowing the empirical details of\nthe network for conducting precise stress tests on individual banks, especially\nafter systemic events.\n"
    },
    {
        "paper_id": 2109.14438,
        "authors": "Ali Al-Ameer and Khaled Alshehri",
        "title": "Conditional Value-at-Risk for Quantitative Trading: A Direct\n  Reinforcement Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose a convex formulation for a trading system with the Conditional\nValue-at-Risk as a risk-adjusted performance measure under the notion of Direct\nReinforcement Learning. Due to convexity, the proposed approach can uncover a\nlucrative trading policy in a \"pure\" online manner where it can interactively\nlearn and update the policy without multi-epoch training and validation. We\nassess our proposed algorithm on a real financial market where it trades one of\nthe largest US trust funds, SPDR, for three years. Numerical experiments\ndemonstrate the algorithm's robustness in detecting central market-regime\nswitching. Moreover, the results show the algorithm's effectiveness in\nextracting profitable policy while meeting an investor's risk preference under\na conservative frictional market with a transaction cost of 0.15% per trade.\n"
    },
    {
        "paper_id": 2109.14554,
        "authors": "Mikrajuddin Abdullah",
        "title": "Coulomb-like Model for International Trade Flow and Derivation of\n  Distribution Function for Trade Flow Strength",
        "comments": "12 pages, 3 figures, 16 pages of Supplementary",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To describe international trade flows, we propose the coulomb force\nformulation, in which the magnitude of the charge represents gross domestic\nproduct (GDP) and the distance between countries is the bilateral distance, the\nproduct of spatial distance and \"dielectric constant,\" rather than the spatial\ndistance as used in the gravitation model, allowing it to be time dependent.\nThe \"dielectric constant\" is influenced by factors such as warfare,\ntransportation disruptions, trade agreements, social, geography, politics,\nculture, and others. The GDP and distance power parameters were estimated using\ndata from high-GDP countries' export-import transactions. We also developed a\ntrade strength distribution equation that fits World Bank data reasonably well\nover a decade.\n"
    },
    {
        "paper_id": 2109.14567,
        "authors": "Tim Janke, Mohamed Ghanmi, Florian Steinke",
        "title": "Implicit Generative Copulas",
        "comments": "Accepted at NeurIPS 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copulas are a powerful tool for modeling multivariate distributions as they\nallow to separately estimate the univariate marginal distributions and the\njoint dependency structure. However, known parametric copulas offer limited\nflexibility especially in high dimensions, while commonly used non-parametric\nmethods suffer from the curse of dimensionality. A popular remedy is to\nconstruct a tree-based hierarchy of conditional bivariate copulas. In this\npaper, we propose a flexible, yet conceptually simple alternative based on\nimplicit generative neural networks. The key challenge is to ensure marginal\nuniformity of the estimated copula distribution. We achieve this by learning a\nmultivariate latent distribution with unspecified marginals but the desired\ndependency structure. By applying the probability integral transform, we can\nthen obtain samples from the high-dimensional copula distribution without\nrelying on parametric assumptions or the need to find a suitable tree\nstructure. Experiments on synthetic and real data from finance, physics, and\nimage generation demonstrate the performance of this approach.\n"
    },
    {
        "paper_id": 2109.14596,
        "authors": "Rajni Kant Bansal, Pengcheng You, Dennice F. Gayme, and Enrique\n  Mallada",
        "title": "A Market Mechanism for Truthful Bidding with Energy Storage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a market mechanism for multi-interval electricity markets\nwith generator and storage participants. Drawing ideas from supply function\nbidding, we introduce a novel bid structure for storage participation that\nallows storage units to communicate their cost to the market using\nenergy-cycling functions that map prices to cycle depths. The resulting\nmarket-clearing process--implemented via convex programming--yields\ncorresponding schedules and payments based on traditional energy prices for\npower supply and per-cycle prices for storage utilization. We illustrate the\nbenefits of our solution by comparing the competitive equilibrium of the\nresulting mechanism to that of an alternative solution that uses prosumer-based\nbids. Our solution shows several advantages over the prosumer-based approach.\nIt does not require a priori price estimation. It also incentivizes\nparticipants to reveal their truthful cost, thus leading to an efficient,\ncompetitive equilibrium. Numerical experiments using New York Independent\nSystem Operator (NYISO) data validate our findings.\n"
    },
    {
        "paper_id": 2109.14932,
        "authors": "Zachary Feinstein, Birgit Rudloff",
        "title": "Characterizing and Computing the Set of Nash Equilibria via Vector\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nash equilibria and Pareto optimality are two distinct concepts when dealing\nwith multiple criteria. It is well known that the two concepts do not coincide.\nHowever, in this work we show that it is possible to characterize the set of\nall Nash equilibria for any non-cooperative game as the Pareto optimal\nsolutions of a certain vector optimization problem. To accomplish this task, we\nincrease the dimensionality of the objective function and formulate a\nnon-convex ordering cone under which Nash equilibria are Pareto efficient. We\ndemonstrate these results, first, for shared constraint games in which a joint\nconstraint is applied to all players in a non-cooperative game. In doing so, we\ndirectly relate our proposed Pareto optimal solutions to the best response\nfunctions of each player. These results are then extended to generalized Nash\ngames, where, in addition to providing an extension of the above\ncharacterization, we deduce two vector optimization problems providing\nnecessary and sufficient conditions, respectively, for generalized Nash\nequilibria. Finally, we show that all prior results hold for vector-valued\ngames as well. Multiple numerical examples are given and demonstrate that our\nproposed vector optimization formulation readily finds the set of all Nash\nequilibria.\n"
    },
    {
        "paper_id": 2109.14977,
        "authors": "Emanuele Casamassima, Lech A. Grzelak, Frank A. Mulder, Cornelis W.\n  Oosterlee",
        "title": "Pricing and Hedging Prepayment Risk in a Mortgage Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Understanding mortgage prepayment is crucial for any financial institution\nproviding mortgages, and it is important for hedging the risk resulting from\nsuch unexpected cash flows. Here, in the setting of a Dutch mortgage provider,\nwe propose to include non-linear financial instruments in the hedge portfolio\nwhen dealing with mortgages with the option to prepay part of the notional\nearly. Based on the assumption that there is a correlation between prepayment\nand the interest rates in the market, a model is proposed which is based on a\nspecific refinancing incentive. The linear and non-linear risks are addressed\nby a set of tradeable instruments in a static hedge strategy. We will show that\na stochastic model for the notional of a mortgage unveils non-linear risk\nembedded in a prepayment option. Based on a calibration of the refinancing\nincentive on a data set of more than thirty million observations, a functional\nform of the prepayments is defined, which accurately reflects the borrowers'\nbehaviour. We compare this functional form with a fully rational model, where\nthe option to prepay is assumed to be exercised rationally.\n"
    },
    {
        "paper_id": 2109.15045,
        "authors": "Jaeyoung Cheong, Heejoon Lee, Minjung Kang",
        "title": "Stock Index Prediction using Cointegration test and Quantile Loss",
        "comments": "8pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent researches on stock prediction using deep learning methods has been\nactively studied. This is the task to predict the movement of stock prices in\nthe future based on historical trends. The approach to predicting the movement\nbased solely on the pattern of the historical movement of it on charts, not on\nfundamental values, is called the Technical Analysis, which can be divided into\nunivariate and multivariate methods in the regression task. According to the\nlatter approach, it is important to select different factors well as inputs to\nenhance the performance of the model. Moreover, its performance can depend on\nwhich loss is used to train the model. However, most studies tend to focus on\nbuilding the structures of models, not on how to select informative factors as\ninputs to train them. In this paper, we propose a method that can get better\nperformance in terms of returns when selecting informative factors using the\ncointegration test and learning the model using quantile loss. We compare the\ntwo RNN variants with quantile loss with only five factors obtained through the\ncointegration test among the entire 15 stock index factors collected in the\nexperiment. The Cumulative return and Sharpe ratio were used to evaluate the\nperformance of trained models. Our experimental results show that our proposed\nmethod outperforms the other conventional approaches.\n"
    },
    {
        "paper_id": 2109.15051,
        "authors": "Abootaleb Shirvani, Stefan Mittnik, W. Brent Lindquist and Svetlozar\n  T. Rachev",
        "title": "Bitcoin Volatility and Intrinsic Time Using Double Subordinated Levy\n  Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a doubly subordinated Levy process, NDIG, to model the time series\nproperties of the cryptocurrency bitcoin. NDIG captures the skew and fat-tailed\nproperties of bitcoin prices and gives rise to an arbitrage free, option\npricing model. In this framework we derive two bitcoin volatility measures. The\nfirst combines NDIG option pricing with the Cboe VIX model to compute an\nimplied volatility; the second uses the volatility of the unit time increment\nof the NDIG model. Both are compared to a volatility based upon historical\nstandard deviation. With appropriate linear scaling, the NDIG process perfectly\ncaptures observed, in-sample, volatility.\n"
    },
    {
        "paper_id": 2109.15052,
        "authors": "Fiammetta Menchetti, Fabrizio Cipollini, Fabrizia Mealli",
        "title": "Causal effect of regulated Bitcoin futures on volatility and volume",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In December 2017, two leading derivative exchanges, CBOE and CME, introduced\nthe first regulated Bitcoin futures. Our aim is estimating their causal impact\non Bitcoin volatility and trading volume. Employing a new causal approach,\nC-ARIMA, we find that the CME future triggered an increase in both outcomes.\nThere is also evidence of a positive volume-volatility relationship and that\nthe effect on volatility was partially due to the higher trading volumes\ninduced by the launch of the contract. After controlling for the effect on\nvolumes, we find that the CME instrument caused Bitcoin volatility to increase\nby more than double.\n"
    },
    {
        "paper_id": 2109.15059,
        "authors": "Jinlong Ruan and Wei Wu and Jiebo Luo",
        "title": "Stock Price Prediction Under Anomalous Circumstances",
        "comments": "8 pages, 5 figures, 3 tables. 2020 IEEE International Conference on\n  Big Data (Big Data)",
        "journal-ref": "2020 IEEE International Conference on Big Data (Big Data), 2020,\n  pp. 4787-4794",
        "doi": "10.1109/BigData50022.2020.9378030",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The stock market is volatile and complicated, especially in 2020. Because of\na series of global and regional \"black swans,\" such as the COVID-19 pandemic,\nthe U.S. stock market triggered the circuit breaker three times within one week\nof March 9 to 16, which is unprecedented throughout history. Affected by the\nwhole circumstance, the stock prices of individual corporations also plummeted\nby rates that were never predicted by any pre-developed forecasting models. It\nreveals that there was a lack of satisfactory models that could predict the\nchanges in stocks prices when catastrophic, highly unlikely events occur. To\nfill the void of such models and to help prevent investors from heavy losses\nduring uncertain times, this paper aims to capture the movement pattern of\nstock prices under anomalous circumstances. First, we detect outliers in\nsequential stock prices by fitting a standard ARIMA model and identifying the\npoints where predictions deviate significantly from actual values. With the\nselected data points, we train ARIMA and LSTM models at the single-stock level,\nindustry level, and general market level, respectively. Since the public moods\naffect the stock market tremendously, a sentiment analysis is also incorporated\ninto the models in the form of sentiment scores, which are converted from\ncomments about specific stocks on Reddit. Based on 100 companies' stock prices\nin the period of 2016 to 2020, the models achieve an average prediction\naccuracy of 98% which can be used to optimize existing prediction\nmethodologies.\n"
    },
    {
        "paper_id": 2109.1506,
        "authors": "Marcel Ausloos, Yining Zhang, and Gurjeet Dhesi",
        "title": "Stock index futures trading impact on spot price volatility. The CSI 300\n  studied with a TGARCH model",
        "comments": "31 pages, 10 tables, 2 figures, 109 references",
        "journal-ref": "Expert Systems with Applications 160 (2020) 113688",
        "doi": "10.1016/j/eswa.2020.113688",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A TGARCH modeling is argued to be the optimal basis for investigating the\nimpact of index futures trading on spot price variability. We discuss the\nCSI-300 index (China-Shanghai-Shenzhen-300-Stock Index) as a test case. The\nresults prove that the introduction of CSI-300 index futures (CSI-300-IF)\ntrading significantly reduces the volatility in the corresponding spot market.\nIt is also found that there is a stationary equilibrium relationship between\nthe CSI-300 spot and CCSI-300-IF markets. A bidirectional Granger causality is\nalso detected. ''Finally'', it is deduced that spot prices are predicted with\ngreater accuracy over a 3 or 4 lag day time span.\n"
    },
    {
        "paper_id": 2109.1511,
        "authors": "Pankaj Kumar",
        "title": "Deep Hawkes Process for High-Frequency Market Making",
        "comments": "49 Pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  High-frequency market making is a liquidity-providing trading strategy that\nsimultaneously generates many bids and asks for a security at ultra-low latency\nwhile maintaining a relatively neutral position. The strategy makes a profit\nfrom the bid-ask spread for every buy and sell transaction, against the risk of\nadverse selection, uncertain execution and inventory risk. We design realistic\nsimulations of limit order markets and develop a high-frequency market making\nstrategy in which agents process order book information to post the optimal\nprice, order type and execution time. By introducing the Deep Hawkes process to\nthe high-frequency market making strategy, we allow a feedback loop to be\ncreated between order arrival and the state of the limit order book, together\nwith self- and cross-excitation effects. Our high-frequency market making\nstrategy accounts for the cancellation of orders that influence order queue\nposition, profitability, bid-ask spread and the value of the order. The\nexperimental results show that our trading agent outperforms the baseline\nstrategy, which uses a probability density estimate of the fundamental price.\nWe investigate the effect of cancellations on market quality and the agent's\nprofitability. We validate how closely the simulation framework approximates\nreality by reproducing stylised facts from the empirical analysis of the\nsimulated order book data.\n"
    },
    {
        "paper_id": 2109.15157,
        "authors": "Jherek Healy",
        "title": "Pricing American options under negative rates",
        "comments": null,
        "journal-ref": null,
        "doi": "10.21314/JCF.2021.004",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper starts by defining the criteria where the early-exercise of an\nAmerican option is never optimal, under positive, or negative rates. It follows\nwith a short analysis of the various shapes of the exercise region under\nnegative interest rates. It then presents a new integral equation, which\nestablishes the option price, and the two early exercise boundaries, under\nnegative rates. It shows how to solve this new equation, through modifications\nof the modern and efficient algorithm of Andersen and Lake, from the initial\nguess of the two boundaries to more subtle changes required in their fixed\npoint method for stability. Finally, the performance and accuracy of the\nresulting algorithm is assessed against a cutting edge finite difference method\nimplementation.\n"
    },
    {
        "paper_id": 2110.00098,
        "authors": "Simon Rudkin, Wanling Qiu and Pawel Dlotko",
        "title": "Uncertainty, volatility and the persistence norms of financial time\n  series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Norms of Persistent Homology introduced in topological data analysis are seen\nas indicators of system instability, analogous to the changing predictability\nthat is captured in financial market uncertainty indexes. This paper\ndemonstrates norms from the financial markets are significant in explaining\nfinancial uncertainty, whilst macroeconomic uncertainty is only explainable by\nmarket volatility. Meanwhile, volatility is insignificant in the determination\nof norms when uncertainty enters the regression. Persistence norms therefore\nhave potential as a further tool in asset pricing, and also as a means of\ncapturing signals from financial time series beyond volatility.\n"
    },
    {
        "paper_id": 2110.00182,
        "authors": "Mohammad Nur Nobi, A. H. M. Raihan Sarker, Biswajit Nath, Eivin\n  R{\\o}skaft, Ma Suza and Paul Kvinta",
        "title": "Economic valuation of tourism of the Sundarban Mangroves, Bangladesh",
        "comments": "10 pages",
        "journal-ref": "Journal of Ecology and The Natural Environment, October- December\n  2021",
        "doi": "10.5897/JENE2021.0910",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Sundarban Reserve Forest (SRF) of Bangladesh provides tourism services to\nlocal and international visitors. Indeed, tourism is one of the major ecosystem\nservices that this biodiversity-rich mangrove forest provides. Through a\nconvenient sampling technique, 421 tourist respondents were interviewed to\nassess their willingness to pay for the tourism services of the Sundarban,\nusing the Zonal Travel Cost Method (ZTCM). The estimated annual economic\ncontribution of tourism in the Sundarban mangroves to the Bangladesh economy is\nUSD 53 million. The findings of this study showed that facilities for watching\nwildlife and walking inside the forest can increase the number of tourists in\nthe SRF. The findings also show that the availability of information like\nforest maps, wildlife precautionary signs, and danger zones would increase the\nnumber of tourists as well. Thus, the government of Bangladesh should consider\nincreasing visitor entry fees to fund improvements and to enhance the\necotourism potential of the Sundarban mangroves.\n"
    },
    {
        "paper_id": 2110.00302,
        "authors": "Aurelio Patelli, Andrea Zaccaria, Luciano Pietronero",
        "title": "Universal Database for Economic Complexity",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41597-022-01732-5",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present an integrated database suitable for the investigations of the\nEconomic development of countries by using the Economic Fitness and Complexity\nframework. Firstly, we implement machine learning techniques to reconstruct the\ndatabase of Trade of Services and we integrate it with the database of the\nTrade of the physical Goods, generating a complete view of the International\nTrade and denoted the Universal database. Using this data, we derive a\nstatistically significant network of interaction of the Economic activities,\nwhere preferred paths of development and clusters of High-Tech industries\nnaturally emerge. Finally, we compute the Economic Fitness, an algorithmic\nassessment of the competitiveness of countries, removing the unexpected\nmisbehaviour of Economies under-represented by the sole consideration of the\nTrade of the physical Goods.\n"
    },
    {
        "paper_id": 2110.0036,
        "authors": "Karl Naumann-Woleske, Michael Benzaquen, Maxim Gusev, Dimitri\n  Kroujiline",
        "title": "Capital Demand Driven Business Cycles: Mechanism and Effects",
        "comments": "51 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a tractable macroeconomic model that captures dynamic behaviors\nacross multiple timescales, including business cycles. The model is anchored in\na dynamic capital demand framework reflecting an interactions-based process\nwhereby firms determine capital needs and make investment decisions at the\nmicro level. We derive equations for aggregate demand from this micro setting\nand embed them in the Solow growth economy. As a result, we obtain a\nclosed-form dynamical system with which we study economic fluctuations and\ntheir impact on long-term growth. For realistic parameters, the model has two\nattracting equilibria: one at which the economy contracts and one at which it\nexpands. This bi-stable configuration gives rise to quasiperiodic fluctuations,\ncharacterized by the economy's prolonged entrapment in either a contraction or\nexpansion mode punctuated by rapid alternations between them. We identify the\nunderlying endogenous mechanism as a coherence resonance phenomenon. In\naddition, the model admits a stochastic limit cycle likewise capable of\ngenerating quasiperiodic fluctuations; however, we show that these fluctuations\ncannot be realized as they induce unrealistic growth dynamics. We further find\nthat while the fluctuations powered by coherence resonance can cause\nsubstantial excursions from the equilibrium growth path, such deviations vanish\nin the long run as supply and demand converge.\n"
    },
    {
        "paper_id": 2110.00582,
        "authors": "Guy Tchuente, Johnson Kakeu, John Nana Francois",
        "title": "The Forest Behind the Tree: Heterogeneity in How US Governor's Party\n  Affects Black Workers",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Income inequality is a distributional phenomenon. This paper examines the\nimpact of U.S governor's party allegiance (Republican vs Democrat) on ethnic\nwage gap. A descriptive analysis of the distribution of yearly earnings of\nWhites and Blacks reveals a divergence in their respective shapes over time\nsuggesting that aggregate analysis may mask important heterogeneous effects.\nThis motivates a granular estimation of the comparative causal effect of\ngovernors' party affiliation on labor market outcomes. We use a regression\ndiscontinuity design (RDD) based on marginal electoral victories and samples of\nquantiles groups by wage and hours worked. Overall, the distributional causal\nestimations show that the vast majority of subgroups of black workers earnings\nare not affected by democrat governors' policies, suggesting the possible\nexistence of structural factors in the labor markets that contribute to create\nand keep a wage trap and/or hour worked trap for most of the subgroups of black\nworkers. Democrat governors increase the number of hours worked of black\nworkers at the highest quartiles of earnings. A bivariate quantiles groups\nanalysis shows that democrats decrease the total hours worked for black workers\nwho have the largest number of hours worked and earn the least. Black workers\nearning more and working fewer hours than half of the sample see their number\nof hours worked increase under a democrat governor.\n"
    },
    {
        "paper_id": 2110.00597,
        "authors": "Leonardo Martins and Marcelo C. Medeiros",
        "title": "The Impacts of Mobility on Covid-19 Dynamics: Using Soft and Hard Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper has the goal of evaluating how changes in mobility has affected\nthe infection spread of Covid-19 throughout the 2020-2021 years. However,\nidentifying a \"clean\" causal relation is not an easy task due to a high number\nof non-observable (behavioral) effects. We suggest the usage of Google Trends\nand News-based indexes as controls for some of these behavioral effects and we\nfind that a 1\\% increase in residential mobility (i.e. a reduction in overall\nmobility) have significant impacts for reducing both Covid-19 cases (at least\n3.02\\% on a one-month horizon) and deaths (at least 2.43\\% at the two-weeks\nhorizon) over the 2020-2021 sample. We also evaluate the effects of mobility on\nCovid-19 spread on the restricted sample (only 2020) where vaccines were not\navailable. The results of diminishing mobility over cases and deaths on the\nrestricted sample are still observable (with similar magnitudes in terms of\nresidential mobility) and cumulative higher, as the effects of restricting\nworkplace mobility turns to be also significant: a 1\\% decrease in workplace\nmobility diminishes cases around 1\\% and deaths around 2\\%.\n"
    },
    {
        "paper_id": 2110.00771,
        "authors": "Claudio Bellani, Damiano Brigo, Mikko Pakkanen, Leandro\n  Sanchez-Betancourt",
        "title": "Non-average price impact in order-driven markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a measurement of price impact in order-driven markets that does\nnot require averages across executions or scenarios. Given the order book data\nassociated with one single execution of a sell metaorder, we measure its\ncontribution to price decrease during the trade. We do so by modelling the\nlimit order book using state-dependent Hawkes processes, and by defining the\nprice impact profile of the execution as a function of the compensator of a\nstochastic process in our model. We apply our measurement to a data set from\nNASDAQ, and we conclude that the clustering of sell child orders has a bigger\nimpact on price than their sizes.\n"
    },
    {
        "paper_id": 2110.00774,
        "authors": "Andreas Binder (1), Onkar Jadhav (1 and 2) and Volker Mehrmann (2)\n  ((1) MathConsult GmbH, Linz, Austria, (2) Institute of Mathematics, TU\n  Berlin, Berlin, Germany)",
        "title": "Error Analysis of a Model Order Reduction Framework for Financial Risk\n  Analysis",
        "comments": "44 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A parametric model order reduction (MOR) approach for simulating the high\ndimensional models arising in financial risk analysis is proposed on the basis\nof the proper orthogonal decomposition (POD) approach to generate small model\napproximations for the high dimensional parametric convection-diffusion\nreaction partial differential equations (PDE). The proposed technique uses an\nadaptive greedy sampling approach based on surrogate modeling to efficiently\nlocate the most relevant training parameters, thus generating the optimal\nreduced basis. The best suitable reduced model is procured such that the total\nerror is less than a user-defined tolerance. The three major errors considered\nare the discretization error associated with the full model obtained by\ndiscretizing the PDE, the model order reduction error, and the parameter\nsampling error. The developed technique is analyzed, implemented, and tested on\nindustrial data of a puttable steepener under the two-factor Hull-White model.\nThe results illustrate that the reduced model provides a significant speedup\nwith excellent accuracy over a full model approach, demonstrating its potential\napplications in the historical or Monte Carlo value at risk calculations.\n"
    },
    {
        "paper_id": 2110.00879,
        "authors": "Steven DiSilvio, Yu (Anna) Luo, Anthony Ozerov",
        "title": "Traders in a Strange Land: Agent-based discrete-event market simulation\n  of the Figgie card game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Figgie is a card game that approximates open-outcry commodities trading. We\ndesign strategies for Figgie and study their performance and the resulting\nmarket behavior. To do this, we develop a flexible agent-based discrete-event\nmarket simulation in which agents operating under our strategies can play\nFiggie. Our simulation builds upon previous work by simulating latencies\nbetween agents and the market in a novel and efficient way. The fundamentalist\nstrategy we develop takes advantage of Figgie's unique notion of asset value,\nand is, on average, the profit-maximizing strategy in all combinations of agent\nstrategies tested. We develop a strategy, the \"bottom-feeder\", which estimates\nvalue by observing orders sent by other agents, and find that it limits the\nsuccess of fundamentalists. We also find that chartist strategies implemented,\nincluding one from the literature, fail by going into feedback loops in the\nsmall Figgie market. We further develop a bootstrap method for statistically\ncomparing strategies in a zero-sum game. Our results demonstrate the\nwide-ranging applicability of agent-based discrete-event simulations in\nstudying markets.\n"
    },
    {
        "paper_id": 2110.01127,
        "authors": "Steven Campbell, Yichao Chen, Arvind Shrivats, Sebastian Jaimungal",
        "title": "Deep Learning for Principal-Agent Mean Field Games",
        "comments": "22 pages, 8 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Here, we develop a deep learning algorithm for solving Principal-Agent (PA)\nmean field games with market-clearing conditions -- a class of problems that\nhave thus far not been studied and one that poses difficulties for standard\nnumerical methods. We use an actor-critic approach to optimization, where the\nagents form a Nash equilibria according to the principal's penalty function,\nand the principal evaluates the resulting equilibria. The inner problem's Nash\nequilibria is obtained using a variant of the deep backward stochastic\ndifferential equation (BSDE) method modified for McKean-Vlasov forward-backward\nSDEs that includes dependence on the distribution over both the forward and\nbackward processes. The outer problem's loss is further approximated by a\nneural net by sampling over the space of penalty functions. We apply our\napproach to a stylized PA problem arising in Renewable Energy Certificate (REC)\nmarkets, where agents may rent clean energy production capacity, trade RECs,\nand expand their long-term capacity to navigate the market at maximum profit.\nOur numerical results illustrate the efficacy of the algorithm and lead to\ninteresting insights into the nature of optimal PA interactions in the\nmean-field limit of these markets.\n"
    },
    {
        "paper_id": 2110.01152,
        "authors": "Hoon Oh, Yanhan Tang, Zong Zhang, Alexandre Jacquillat, Fei Fang",
        "title": "Efficiency, Fairness, and Stability in Non-Commercial Peer-to-Peer\n  Ridesharing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Unlike commercial ridesharing, non-commercial peer-to-peer (P2P) ridesharing\nhas been subject to limited research -- although it can promote viable\nsolutions in non-urban communities. This paper focuses on the core problem in\nP2P ridesharing: the matching of riders and drivers. We elevate users'\npreferences as a first-order concern and introduce novel notions of fairness\nand stability in P2P ridesharing. We propose algorithms for efficient matching\nwhile considering user-centric factors, including users' preferred departure\ntime, fairness, and stability. Results suggest that fair and stable solutions\ncan be obtained in reasonable computational times and can improve baseline\noutcomes based on system-wide efficiency exclusively.\n"
    },
    {
        "paper_id": 2110.01302,
        "authors": "Thierry Roncalli",
        "title": "Liquidity Stress Testing in Asset Management -- Part 3. Managing the\n  Asset-Liability Liquidity Risk",
        "comments": "90 pages, 40 figures, 17 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article is part of a comprehensive research project on liquidity risk in\nasset management, which can be divided into three dimensions. The first\ndimension covers the modeling of the liability liquidity risk (or funding\nliquidity), the second dimension is dedicated to the modeling of the asset\nliquidity risk (or market liquidity), whereas the third dimension considers the\nmanagement of the asset-liability liquidity risk (or asset-liability matching).\nThe purpose of this research is to propose a methodological and practical\nframework in order to perform liquidity stress testing programs, which comply\nwith regulatory guidelines (ESMA, 2019, 2020) and are useful for fund managers.\nIn this third and last research paper focused on managing the asset-liability\nliquidity risk, we explore the ALM tools that can be put in place to control\nthe liquidity gap. These ALM tools can be split into three categories:\nmeasurement tools, management tools and monitoring tools. In terms of\nmeasurement tools, we focus on the computation of the redemption coverage ratio\n(RCR), which is the central instrument of liquidity stress testing programs. We\nalso study the redemption liquidation policy and the different implementation\nmethodologies, and we show how reverse stress testing can be developed. In\nterms of liquidity management tools, we study the calibration of liquidity\nbuffers, the pros and cons of special arrangements (redemption suspensions,\ngates, side pockets and in-kind redemptions) and the effectiveness of swing\npricing. In terms of liquidity monitoring tools, we compare the macro- and\nmicro-approaches of liquidity monitoring in order to identify the transmission\nchannels of liquidity risk.\n"
    },
    {
        "paper_id": 2110.01325,
        "authors": "Mahmoud Mahfouz, Tucker Balch, Manuela Veloso, Danilo Mandic",
        "title": "Learning to Classify and Imitate Trading Agents in Continuous Double\n  Auction Markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3490354.3494386",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Continuous double auctions such as the limit order book employed by exchanges\nare widely used in practice to match buyers and sellers of a variety of\nfinancial instruments. In this work, we develop an agent-based model for\ntrading in a limit order book and show (1) how opponent modelling techniques\ncan be applied to classify trading agent archetypes and (2) how behavioural\ncloning can be used to imitate these agents in a simulated setting. We\nexperimentally compare a number of techniques for both tasks and evaluate their\napplicability and use in real-world scenarios.\n"
    },
    {
        "paper_id": 2110.01368,
        "authors": "Robin Fritsch",
        "title": "Concentrated Liquidity in Automated Market Makers",
        "comments": null,
        "journal-ref": "Proceedings of the 2021 ACM CCS Workshop on Decentralized Finance\n  and Security (2021) 15-20",
        "doi": "10.1145/3464967.3488590",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine how the introduction of concentrated liquidity has changed the\nliquidity provision market in automated market makers such as Uniswap. To this\nend, we compare average liquidity provider returns from trading fees before and\nafter its introduction. Furthermore, we quantify the performance of a number of\nfundamental concentrated liquidity strategies using historical trade data. We\nestimate their possible returns and evaluate which perform best for certain\ntrading pairs and market conditions.\n"
    },
    {
        "paper_id": 2110.01496,
        "authors": "S. Kabaivanov, V. Zhelinski, B. Zlatanov",
        "title": "Coupled Fixed Points for Hardy-Rogers Type of Maps and Their\n  Applications in the Investigations of Market Equilibrium in Duopoly Markets\n  for Non-Differentiable, Nonlinear Response Functions",
        "comments": "12 pages, 2 figure and 3 tables. arXiv admin note: text overlap with\n  arXiv:2008.03337",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we generalize Hardy-Rogers maps in the context of coupled fixed\npoints. We generalizes with the help of the obtained main theorem some known\nresults about existence and uniqueness of market equilibrium in duopoly\nmarkets. We investigate some recent results about market equilibrium in duopoly\nmarkets with the help of the main theorem and we enrich them. We define a\ngeneralized response function including production and surpluses. Finally we\nillustrate a possible application of the main result in the investigation of\nmarket equilibrium, when the pay off functions are non differentiable.\n"
    },
    {
        "paper_id": 2110.01523,
        "authors": "Kiyoshi Kanazawa and Didier Sornette",
        "title": "Exact asymptotic solutions to nonlinear Hawkes processes: a systematic\n  classification of the steady-state solutions",
        "comments": "63 pages, 12 figures, 1 table",
        "journal-ref": "Phys. Rev. Research 5, 013067 (2023)",
        "doi": "10.1103/PhysRevResearch.5.013067",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hawkes point processes are first-order non-Markovian stochastic models of\nintermittent bursty dynamics with applications to physical, seismic, epidemic,\nbiological, financial, and social systems. While accounting for positive\nfeedback loops that may lead to critical phenomena in complex systems, the\nstandard linear Hawkes process only describes excitative phenomena. To describe\nthe co-existence of excitatory and inhibitory effects (or negative feedbacks),\nextensions involving nonlinear dependences of the intensity as a function of\npast activity are needed. However, such nonlinear Hawkes processes have been\nfound hitherto to be analytically intractable due to the interplay between\ntheir non-Markovian and nonlinear characteristics, with no analytical solutions\navailable. Here, we present various exact and robust asymptotic solutions to\nnonlinear Hawkes processes using the field master equation approach. We report\nexplicit power law formulas for the steady state intensity distributions\n$P_{\\mathrm{ss}}(\\lambda)\\propto \\lambda^{-1-a}$, where the tail exponent $a$\nis expressed analytically as a function of parameters of the nonlinear Hawkes\nmodels. We present three robust interesting characteristics of the nonlinear\nHawkes process: (i) for one-sided positive marks (i.e., in the absence of\ninhibitory effects), the nonlinear Hawkes process can exhibit any power law\nrelation either as intermediate asymptotics ($a\\leq 0$) or as true asymptotics\n($a>0$) by appropriate model selection; (ii) for distribution of marks with\nzero mean (i.e., for balanced excitatory and inhibitory effects), the Zipf law\n($a\\approx 1$) is universally observed for a wide class of nonlinear Hawkes\nprocesses with fast-accelerating intensity map; (iii) for marks with a negative\nmean, the asymptotic power law tail becomes lighter with an exponent increasing\nabove 1 ($a>1$) as the mean mark becomes more negative.\n"
    },
    {
        "paper_id": 2110.01615,
        "authors": "Aurelio Patelli, Lorenzo Napolitano, Giulio Cimini, Andrea Gabrielli",
        "title": "Geography of Science: Competitiveness and Inequality",
        "comments": null,
        "journal-ref": "Journal of Informetrics 17(1), 101357 (2023)",
        "doi": "10.1016/j.joi.2022.101357",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using ideas and tools of complexity science we design a holistic measure of\n\\textit{Scientific Fitness}, encompassing the scientific knowledge,\ncapabilities and competitiveness of a research system. We characterize the\ntemporal dynamics of Scientific Fitness and R\\&D expenditures at the\ngeographical scale of nations, highlighting patterns of similar research\nsystems, and showing how developing nations (China in particular) are quickly\ncatching up the developed ones. Down-scaling the aggregation level of the\nanalysis, we find that even developed nations show a considerable level of\ninequality in the Scientific Fitness of their internal regions. Further, we\nassess comparatively how the competitiveness of each geographic region is\ndistributed over the spectrum of research sectors. Overall, the Scientific\nFitness represents the first high quality estimation of the scientific strength\nof nations and regions, opening new policy-making applications for better\nallocating resources, filling inequality gaps and ultimately promoting\ninnovation.\n"
    },
    {
        "paper_id": 2110.02016,
        "authors": "Paolo Falbo and Carlos Ruiz",
        "title": "Joint optimization of sales-mix and generation plan for a large\n  electricity producer",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eneco.2023.106535",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper develops a typical management problem of a large power producer\n(i.e., he can partly influence the market price). In particular, he routinely\nneeds to decide how much of his generation it is preferable to commit to fixed\nprice bilateral contracts (e.g., futures) or to the spot market. However, he\nalso needs to plan how to distribute the production across the different plants\nunder his control. The two decisions, namely the sales-mix and the generation\nplan, naturally interact, since the opportunity to influence the spot price\ndepends, among other things, by the amount of the energy that the producer\ndirects on the spot market. We develop a risk management problem, since we\nconsider an optimization problem combining a trade-off between expectation and\nconditional value at risk of the profit function of the producer. The sources\nof uncertainty are relatively large and encompass demand, renewables generation\nand the fuel costs of conventional plants. We also model endogenously the price\nof futures in a way reflecting an information advantage of a large power\nproducer. In particular, it is assumed that the market forecast the price of\nfutures in a naive way, namely not anticipating the impact of the large\nproducer on the spot market. The paper provides a MILP formulation of the\nproblem, and it analyzes the solution through a simulation based on Spanish\npower market data.\n"
    },
    {
        "paper_id": 2110.02206,
        "authors": "K.S. Naik",
        "title": "Predicting Credit Risk for Unsecured Lending: A Machine Learning\n  Approach",
        "comments": "9 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Since the 1990s, there have been significant advances in the technology space\nand the e-Commerce area, leading to an exponential increase in demand for\ncashless payment solutions. This has led to increased demand for credit cards,\nbringing along with it the possibility of higher credit defaults and hence\nhigher delinquency rates, over a period of time. The purpose of this research\npaper is to build a contemporary credit scoring model to forecast credit\ndefaults for unsecured lending (credit cards), by employing machine learning\ntechniques. As much of the customer payments data available to lenders, for\nforecasting Credit defaults, is imbalanced (skewed), on account of a limited\nsubset of default instances, this poses a challenge for predictive modelling.\nIn this research, this challenge is addressed by deploying Synthetic Minority\nOversampling Technique (SMOTE), a proven technique to iron out such imbalances,\nfrom a given dataset. On running the research dataset through seven different\nmachine learning models, the results indicate that the Light Gradient Boosting\nMachine (LGBM) Classifier model outperforms the other six classification\ntechniques. Thus, our research indicates that the LGBM classifier model is\nbetter equipped to deliver higher learning speeds, better efficiencies and\nmanage larger data volumes. We expect that deployment of this model will enable\nbetter and timely prediction of credit defaults for decision-makers in\ncommercial lending institutions and banks.\n"
    },
    {
        "paper_id": 2110.02337,
        "authors": "Adam Potter and Rabab Haider and Giulio Ferro and Michela Robba and\n  Anuradha M. Annaswamy",
        "title": "A Reactive Power Market for the Future Grid",
        "comments": "26 pages, 9 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As pressures to decarbonize the electricity grid increase, the grid edge is\nwitnessing a rapid adoption of distributed and renewable generation. As a\nresult, traditional methods for reactive power management and compensation may\nbecome ineffective. Current state of art for reactive power compensation, which\nrely primarily on capacity payments, exclude distributed generation (DG). We\npropose an alternative: a reactive power market at the distribution level\ndesigned to meet the needs of decentralized and decarbonized grids. The\nproposed market uses variable payments to compensate DGs equipped with smart\ninverters, at an increased spatial and temporal granularity, through a\ndistribution-level Locational Marginal Price (d-LMP). We validate our proposed\nmarket with a case study of the US New England grid on a modified IEEE-123 bus,\nwhile varying DG penetration from 5% to 160%. Results show that our market can\naccommodate such a large penetration, with stable reactive power revenue\nstreams. The market can leverage the considerable flexibility afforded by\ninverter-based resources to meet over 40% of reactive power load when operating\nin a power factor range of 0.6 to 1.0. DGs participating in the market can earn\nup to 11% of their total revenue from reactive power payments. Finally, the\ncorresponding daily d-LMPs determined from the proposed market were observed to\nexhibit limited volatility.\n"
    },
    {
        "paper_id": 2110.02358,
        "authors": "Vineet Jagadeesan Nair, Venkatesh Venkataramanan, Rabab Haider, and\n  Anuradha Annaswamy",
        "title": "A Hierarchical Local Electricity Market for a DER-rich Grid Edge",
        "comments": "14 pages, 13 figures, 1 table. This work has been submitted to the\n  IEEE Transactions on Smart Grid for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With increasing penetration of DERs in the distribution system, it is\ncritical to design market structures that enable smooth integration of DERs. A\nhierarchical local electricity market (LEM) structure is proposed in this paper\nwith a secondary market (SM) at the lower level representing secondary feeders\nand a primary market (PM) at the upper level, representing primary feeders, in\norder to effectively use DERs to increase grid efficiency and resilience. The\nlower level SM enforces budget, power balance and flexibility constraints and\naccounts for costs related to consumers, such as their disutility, flexibility\nlimits, and commitment reliability, while the upper level PM enforces power\nphysics constraints such as power balance and capacity limits, and also\nminimizes line losses. The hierarchical LEM is extensively evaluated using a\nmodified IEEE-123 bus with high DER penetration, with each primary feeder\nconsisting of at least three secondary feeders. Data from a GridLAB-D model is\nused to emulate realistic power injections and load profiles over the course of\n24 hours. The performance of the LEM is illustrated by delineating the family\nof power-injection profiles across the primary and secondary feeders as well as\ncorresponding local electricity tariffs that vary across the distribution grid.\nTogether, it represents an overall framework for a Distribution System Operator\n(DSO) who can provide the oversight for the entire LEM.\n"
    },
    {
        "paper_id": 2110.02492,
        "authors": "Shijia Song and Handong Li",
        "title": "Value-at-Risk forecasting model based on normal inverse Gaussian\n  distribution driven by dynamic conditional score",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under the framework of dynamic conditional score, we propose a parametric\nforecasting model for Value-at-Risk based on the normal inverse Gaussian\ndistribution (Hereinafter NIG-DCS-VaR), which creatively incorporates intraday\ninformation into daily VaR forecast. NIG specifies an appropriate distribution\nto return and the semi-additivity of the NIG parameters makes it feasible to\nimprove the estimation of daily return in light of intraday return, and thus\nthe VaR can be explicitly obtained by calculating the quantile of the\nre-estimated distribution of daily return. We conducted an empirical analysis\nusing two main indexes of the Chinese stock market, and a variety of\nbacktesting approaches as well as the model confidence set approach prove that\nthe VaR forecasts of NIG-DCS model generally gain an advantage over those of\nrealized GARCH (RGARCH) models. Especially when the risk level is relatively\nhigh, NIG-DCS-VaR beats RGARCH-VaR in terms of coverage ability and\nindependence.\n"
    },
    {
        "paper_id": 2110.02735,
        "authors": "Rom\\'an P\\'erez-Santalla, Miguel Carri\\'on and Carlos Ruiz",
        "title": "Optimal pricing for electricity retailers based on data-driven\n  consumers' price-response",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11750-022-00622-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work we tackle the problem of finding the optimal price tariff\nto be set by a risk-averse electric retailer participating in the pool and\nwhose customers are price-sensitive. We assume that the retailer has access to\na sufficiently large smart-meter dataset from which it can statistically\ncharacterize the relationship between the tariff price and the demand load of\nits clients. Three different models are analyzed to predict the aggregated load\nas a function of the electricity prices and other parameters, as humidity or\ntemperature. More specifically, we train linear regression (predictive) models\nto forecast the resulting demand load as a function of the retail price. Then\nwe will insert this model in a quadratic optimization problem which evaluates\nthe optimal price to be offered. This optimization problem accounts for\ndifferent sources of uncertainty including consumer's response, pool prices and\nrenewable source availability, and relies on a stochastic and risk-averse\nformulation. In particular, one important contribution of this work is to base\nthe scenario generation and reduction procedure on the statistical properties\nof the resulting predictive model. This allows us to properly quantify\n(data-driven) not only the expected value but the level of uncertainty\nassociated with the main problem parameters. Moreover, we consider both\nstandard forward based contracts and the recently introduced power purchase\nagreement contracts as risk-hedging tools for the retailer. The results are\npromising as profits are found for the retailer with highly competitive prices\nand some possible improvements are shown if richer datasets could be available\nin the future. A realistic case study and multiple sensitivity analyses have\nbeen performed to characterize the risk-aversion behavior of the retailer\nconsidering price-sensitive consumers.\n"
    },
    {
        "paper_id": 2110.02742,
        "authors": "Amine Assouel, Antoine Jacquier, Alexei Kondratyev",
        "title": "A Quantum Generative Adversarial Network for distributions",
        "comments": "19 pages, 17 Figures, 2 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generative Adversarial Networks are becoming a fundamental tool in Machine\nLearning, in particular in the context of improving the stability of deep\nneural networks. At the same time, recent advances in Quantum Computing have\nshown that, despite the absence of a fault-tolerant quantum computer so far,\nquantum techniques are providing exponential advantage over their classical\ncounterparts. We develop a fully connected Quantum Generative Adversarial\nnetwork and show how it can be applied in Mathematical Finance, with a\nparticular focus on volatility modelling.\n"
    },
    {
        "paper_id": 2110.02953,
        "authors": "Shijia Song and Handong Li",
        "title": "A Method for Predicting VaR by Aggregating Generalized Distributions\n  Driven by the Dynamic Conditional Score",
        "comments": "20 pages. arXiv admin note: text overlap with arXiv:2110.02492",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Constructing a more effective value at risk (VaR) prediction model has long\nbeen a goal in financial risk management. In this paper, we propose a novel\nparametric approach and provide a standard paradigm to demonstrate the\nmodeling. We establish a dynamic conditional score (DCS) model based on\nhigh-frequency data and a generalized distribution (GD), namely, the GD-DCS\nmodel, to improve the forecasts of daily VaR. The model assumes that intraday\nreturns at different moments are independent of each other and obey the same\nkind of GD, whose dynamic parameters are driven by DCS. By predicting the\nmotion law of the time-varying parameters, the conditional distribution of\nintraday returns is determined; then, the bootstrap method is used to simulate\ndaily returns. An empirical analysis using data from the Chinese stock market\nshows that Weibull-Pareto -DCS model incorporating high-frequency data is\nsuperior to traditional benchmark models, such as RGARCH, in the prediction of\nVaR at high risk levels, which proves that this approach contributes to the\nimprovement of risk measurement tools.\n"
    },
    {
        "paper_id": 2110.0314,
        "authors": "Danial Ludwig and Victor M. Yakovenko",
        "title": "Physics-inspired analysis of the two-class income distribution in the\n  USA in 1983-2018",
        "comments": "18 pages, 8 figures, submitted to Philosophical Transactions of the\n  Royal Society A for the special issue \"Kinetic exchange models of societies\n  and economies\"",
        "journal-ref": "Philosophical Transactions of the Royal Society A 380, 20210162\n  (2022)",
        "doi": "10.1098/rsta.2021.0162",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The first part of this paper is a brief survey of the approaches to economic\ninequality based on ideas from statistical physics and kinetic theory. These\ninclude the Boltzmann kinetic equation, the time-reversal symmetry, the\nergodicity hypothesis, entropy maximization, and the Fokker-Planck equation.\nThe origins of the exponential Boltzmann-Gibbs distribution and the Pareto\npower law are discussed in relation to additive and multiplicative stochastic\nprocesses. The second part of the paper analyzes income distribution data in\nthe USA for the time period 1983-2018 using a two-class decomposition. We\npresent overwhelming evidence that the lower class (more than 90% of the\npopulation) is described by the exponential distribution, whereas the upper\nclass (about 4% of the population in 2018) by the power law. We show that the\nsignificant growth of inequality during this time period is due to the sharp\nincrease in the upper-class income share, whereas relative inequality within\nthe lower class remains constant. We speculate that the expansion of the\nupper-class population and income shares may be due to increasing digitization\nand non-locality of the economy in the last 40 years.\n"
    },
    {
        "paper_id": 2110.03432,
        "authors": "Dorje C. Brody",
        "title": "Noise, fake news, and tenacious Bayesians",
        "comments": "23 pages, 6 figures, version to appear in Frontiers in Psychology",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A modelling framework, based on the theory of signal processing, for\ncharacterising the dynamics of systems driven by the unravelling of information\nis outlined, and is applied to describe the process of decision making. The\nmodel input of this approach is the specification of the flow of information.\nThis enables the representation of (i) reliable information, (ii) noise, and\n(iii) disinformation, in a unified framework. Because the approach is designed\nto characterise the dynamics of the behaviour of people, it is possible to\nquantify the impact of information control, including those resulting from the\ndissemination of disinformation. It is shown that if a decision maker assigns\nan exceptionally high weight on one of the alternative realities, then under\nthe Bayesian logic their perception hardly changes in time even if evidences\npresented indicate that this alternative corresponds to a false reality. Thus\nconfirmation bias need not be incompatible with Bayesian updating. By observing\nthe role played by noise in other areas of natural sciences, where noise is\nused to excite the system away from false attractors, a new approach to tackle\nthe dark forces of fake news is proposed.\n"
    },
    {
        "paper_id": 2110.03443,
        "authors": "Laura Blattner, Scott Nelson, Jann Spiess",
        "title": "Unpacking the Black Box: Regulating Algorithmic Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What should regulators of complex algorithms regulate? We propose a model of\noversight over 'black-box' algorithms used in high-stakes applications such as\nlending, medical testing, or hiring. In our model, a regulator is limited in\nhow much she can learn about a black-box model deployed by an agent with\nmisaligned preferences. The regulator faces two choices: first, whether to\nallow for the use of complex algorithms; and second, which key properties of\nalgorithms to regulate. We show that limiting agents to algorithms that are\nsimple enough to be fully transparent is inefficient as long as the\nmisalignment is limited and complex algorithms have sufficiently better\nperformance than simple ones. Allowing for complex algorithms can improve\nwelfare, but the gains depend on how the regulator regulates them. Regulation\nthat focuses on the overall average behavior of algorithms, for example based\non standard explainer tools, will generally be inefficient. Targeted regulation\nthat focuses on the source of incentive misalignment, e.g., excess false\npositives or racial disparities, can provide second-best solutions. We provide\nempirical support for our theoretical findings using an application in consumer\nlending, where we document that complex models regulated based on\ncontext-specific explanation tools outperform simple, fully transparent models.\nThis gain from complex models represents a Pareto improvement across our\nempirical applications that is preferred both by the lender and from the\nperspective of the financial regulator.\n"
    },
    {
        "paper_id": 2110.03512,
        "authors": "Safa El Kefi",
        "title": "Application of DEA in International Market Selection for the export of\n  products from Spain",
        "comments": "6 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article presents a Benchmarking methodology to support decision-making\nfor international market selection (IMS). In order to do so, we will be using\nan output-oriented Data Envelopment Analysis (DEA) model. This methodology\nconsiders multiple variables validated with a correlation analysis. The\nmethodology is applied to all of the products directly exported from Spain, it\ntakes into consideration different Inputs variables and returns us the\nefficient and regions generating higher benefits to access international\nmarkets with the lowest costs possible.\n"
    },
    {
        "paper_id": 2110.03517,
        "authors": "Felix Polyakov",
        "title": "Representation of probability distributions with implied volatility and\n  biological rationale",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Economic and financial theories and practice essentially deal with uncertain\nfuture. Humans encounter uncertainty in different kinds of activity, from\nsensory-motor control to dynamics in financial markets, what has been subject\nof extensive studies. Representation of uncertainty with normal or lognormal\ndistribution is a common feature of many of those studies. For example,\nproposed Bayessian integration of Gaussian multisensory input in the brain or\nlog-normal distribution of future asset price in renowned Black-Scholes-Merton\n(BSM) model for pricing contingent claims.\n  Standard deviation of log(future asset price) scaled by square root of time\nin the BSM model is called implied volatility. Actually, log(future asset\nprice) is not normally distributed and traders account for that to avoid\nlosses. Nevertheless the BSM formula derived under the assumption of constant\nvolatility remains a major uniform framework for pricing options in financial\nmarkets. I propose that one of the reasons for such a high popularity of the\nBSM formula could be its ability to translate uncertainty measured with implied\nvolatility into price in a way that is compatible with human intuition for\nmeasuring uncertainty.\n  The present study deals with mathematical relationship between uncertainty\nand the BSM implied volatility. Examples for a number of common probability\ndistributions are presented. Overall, this work proposes that representation of\nvarious probability distributions in terms of the BSM implied volatility\nprofile may be meaningful in both biological and financial worlds. Necessary\nbackground from financial mathematics is provided in the text.\n"
    },
    {
        "paper_id": 2110.03687,
        "authors": "Jean-No\\\"el Tuccella and Philip Nadler and Ovidiu \\c{S}erban",
        "title": "Protecting Retail Investors from Order Book Spoofing using a GRU-based\n  Detection Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Market manipulation is tackled through regulation in traditional markets\nbecause of its detrimental effect on market efficiency and many participating\nfinancial actors. The recent increase of private retail investors due to new\nlow-fee platforms and new asset classes such as decentralised digital\ncurrencies has increased the number of vulnerable actors due to lack of\ninstitutional sophistication and strong regulation. This paper proposes a\nmethod to detect illicit activity and inform investors on spoofing attempts, a\nwell-known market manipulation technique. Our framework is based on a highly\nextendable Gated Recurrent Unit (GRU) model and allows the inclusion of market\nvariables that can explain spoofing and potentially other illicit activities.\nThe model is tested on granular order book data, in one of the most unregulated\nmarkets prone to spoofing with a large number of non-institutional traders. The\nresults show that the model is performing well in an early detection context,\nallowing the identification of spoofing attempts soon enough to allow investors\nto react. This is the first step to a fully comprehensive model that will\nprotect investors in various unregulated trading environments and regulators to\nidentify illicit activity.\n"
    },
    {
        "paper_id": 2110.0381,
        "authors": "Bastien Baldacci, Jerome Benveniste, Gordon Ritter",
        "title": "Optimal Turnover, Liquidity, and Autocorrelation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The steady-state turnover of a trading strategy is of clear interest to\npractitioners and portfolio managers, as is the steady-state Sharpe ratio. In\nthis article, we show that in a convenient Gaussian process model, the\nsteady-state turnover can be computed explicitly, and obeys a clear relation to\nthe liquidity of the asset and to the autocorrelation of the alpha forecast\nsignals. Indeed, we find that steady-state optimal turnover is given by $\\gamma\n\\sqrt{n+1}$ where $\\gamma$ is a liquidity-adjusted notion of risk-aversion, and\n$n$ is the ratio of mean-reversion speed to $\\gamma$.\n"
    },
    {
        "paper_id": 2110.03986,
        "authors": "Anish Rai, Ajit Mahata, Md. Nurujjaman, Sushovan Majhi and Kanish\n  debnath",
        "title": "A sentiment-based modeling and analysis of stock price during the\n  COVID-19: U- and Swoosh-shaped recovery",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126810",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, a stock price model is proposed by A. Mahata et al. [Physica A,\n574, 126008 (2021)] to understand the effect of COVID-19 on stock market. It\ndescribes V- and L-shaped recovery of the stocks and indices, but fails to\nsimulate the U- and Swoosh-shaped recovery that arises due to sharp crisis and\nprolong drop followed by quick recovery (U-shaped) or slow recovery for longer\nperiod (Swoosh-shaped recovery). We propose a modified model by introducing a\nnew variable $\\theta$ that quantifies the sentiment of the investors.\n$\\theta=+1,~0,~-1$ for positive, neutral and negative sentiment, respectively.\nThe model explains the movement of sectoral indices with positive $\\phi$\nshowing U- and Swoosh-shaped recovery. The simulation using synthetic fund-flow\n($\\Psi_{st}$) with different shock lengths ($T_S$), $\\phi$, negative sentiment\nperiod ($T_N$) and portion of fund-flow ($\\lambda$) during recovery period show\nU- and Swoosh-shaped recovery. The results show that the recovery of the\nindices with positive $\\phi$ becomes very weak with the extended $T_S$ and\n$T_N$. The stocks with higher $\\phi$ and $\\lambda$ recover quickly. The\nsimulation of the Nifty Bank, Nifty Financial and Nifty Realty show U-shaped\nrecovery and Nifty IT shows Swoosh-shaped recovery. The simulation result is\nconsistent with the real stock price movement. The time-scale ($\\tau$) of the\nshock and recovery of these indices during the COVID-19 are consistent with the\ntime duration of the change of negative sentiment from the onset of the\nCOVID-19. This study may help the investors to plan their investment during\ndifferent crises.\n"
    },
    {
        "paper_id": 2110.04088,
        "authors": "Thomas M\\\"obius, Iegor Riepin, Felix M\\\"usgens, Adriaan H. van der\n  Weijde",
        "title": "Risk aversion in flexible electricity markets",
        "comments": "28 pages, 3 Figures, 11 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Flexibility options, such as demand response, energy storage and\ninterconnection, have the potential to reduce variation in electricity prices\nbetween different future scenarios, therefore reducing investment risk.\nMoreover, investment in flexibility options can lower the need for generation\ncapacity. However, there are complex interactions between different flexibility\noptions. In this paper, we investigate the interactions between flexibility and\ninvestment risk in electricity markets. We employ a large-scale stochastic\ntransmission and generation expansion model of the European electricity system.\nUsing this model, we first investigate the effect of risk aversion on the\ninvestment decisions. We find that the interplay of parameters leads to (i)\nmore investment in a less emission-intensive energy system if planners are risk\naverse (hedging against CO2 price uncertainty) and (ii) constant total\ninstalled capacity, regardless of the level of risk aversion (planners do not\nhedge against demand and RES deployment uncertainties). Second, we investigate\nthe individual effects of three flexibility elements on optimal investment\nlevels under different levels of risk aversion: demand response, investment in\nadditional interconnection capacity and investment in additional energy\nstorage. We find that that flexible technologies have a higher value for\nrisk-averse decision-makers, although the effects are nonlinear. Finally, we\ninvestigate the interactions between the flexibility elements. We find that\nrisk-averse decision-makers show a strong preference for transmission grid\nexpansion once flexibility is available at low cost levels.\n"
    },
    {
        "paper_id": 2110.04745,
        "authors": "Gabriel Borrageiro and Nick Firoozye and Paolo Barucca",
        "title": "Reinforcement Learning for Systematic FX Trading",
        "comments": null,
        "journal-ref": "IEEE Access, vol. 10, pp. 5024-5036, 2022",
        "doi": "10.1109/ACCESS.2021.3139510",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explore online inductive transfer learning, with a feature representation\ntransfer from a radial basis function network formed of Gaussian mixture model\nhidden processing units to a direct, recurrent reinforcement learning agent.\nThis agent is put to work in an experiment, trading the major spot market\ncurrency pairs, where we accurately account for transaction and funding costs.\nThese sources of profit and loss, including the price trends that occur in the\ncurrency markets, are made available to the agent via a quadratic utility, who\nlearns to target a position directly. We improve upon earlier work by targeting\na risk position in an online transfer learning context. Our agent achieves an\nannualised portfolio information ratio of 0.52 with a compound return of 9.3\\%,\nnet of execution and funding cost, over a 7-year test set; this is despite\nforcing the model to trade at the close of the trading day at 5 pm EST when\ntrading costs are statistically the most expensive.\n"
    },
    {
        "paper_id": 2110.04752,
        "authors": "Yufei Wu, Mahmoud Mahfouz, Daniele Magazzeni, Manuela Veloso",
        "title": "How Robust are Limit Order Book Representations under Data Perturbation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The success of machine learning models in the financial domain is highly\nreliant on the quality of the data representation. In this paper, we focus on\nthe representation of limit order book data and discuss the opportunities and\nchallenges for learning representations of such data. We also experimentally\nanalyse the issues associated with existing representations and present a\nguideline for future research in this area.\n"
    },
    {
        "paper_id": 2110.05299,
        "authors": "Lin Li",
        "title": "An Automated Portfolio Trading System with Feature Preprocessing and\n  Recurrent Reinforcement Learning",
        "comments": "accepted for publication by ICAIF 2021. arXiv admin note: text\n  overlap with arXiv:2109.05283",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a novel portfolio trading system, which contains a feature\npreprocessing module and a trading module. The feature preprocessing module\nconsists of various data processing operations, while in the trading part, we\nintegrate the portfolio weight rebalance function with the trading algorithm\nand make the trading system fully automated and suitable for individual\ninvestors, holding a handful of stocks. The data preprocessing procedures are\napplied to remove the white noise in the raw data set and uncover the general\npattern underlying the data set before the processed feature set is inputted\ninto the trading algorithm. Our empirical results reveal that the proposed\nportfolio trading system can efficiently earn high profit and maintain a\nrelatively low drawdown, which clearly outperforms other portfolio trading\nstrategies.\n"
    },
    {
        "paper_id": 2110.05479,
        "authors": "Yufei Wu, Mahmoud Mahfouz, Daniele Magazzeni, Manuela Veloso",
        "title": "Towards Robust Representation of Limit Orders Books for Deep Learning\n  Models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2110.04752",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The success of deep learning-based limit order book forecasting models is\nhighly dependent on the quality and the robustness of the input data\nrepresentation. A significant body of the quantitative finance literature\nfocuses on utilising different deep learning architectures without taking into\nconsideration the key assumptions these models make with respect to the input\ndata representation. In this paper, we highlight the issues associated with the\ncommonly-used representations of limit order book data from both a theoretical\nand practical perspectives. We also show the fragility of the representations\nunder adversarial perturbations and propose two simple modifications to the\nexisting representations that match the theoretical assumptions of deep\nlearning models. Finally, we show experimentally how our proposed\nrepresentations lead to state-of-the-art performance in both accuracy and\nrobustness utilising very simple neural network architectures.\n"
    },
    {
        "paper_id": 2110.05482,
        "authors": "Jyotirmoy Bhattacharya",
        "title": "Indian urban workers' labour market transitions",
        "comments": "24 pages, 3 figures. Major revision. Section on post-COVID period\n  removed. Qualitative results unchanged",
        "journal-ref": null,
        "doi": "10.1007/s41027-023-00434-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies gross labour market flows and determinants of labour\nmarket transitions for urban Indian workers using a panel dataset constructed\nfrom Indian Periodic Labour Force Survey (PLFS) data for the period 2017--18 to\n2019--20. Longitudinal studies based on the PLFS have been hampered by data\nproblems that prevent a straightforward merging of the 2017--18 and 2018--19\ndata releases. In this paper, we propose and validate a matching procedure\nbased on individual and household characteristics that can successfully link\nalmost all records across these two years. We use the constructed data set to\ndocument a number of stylised facts about gross worker flows and to estimate\nthe effects of different individual characteristics and work histories on\nprobabilities of job gain and loss.\n"
    },
    {
        "paper_id": 2110.05608,
        "authors": "John Lynham and Philip R. Neary",
        "title": "Tiebout sorting in online communities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper proposes a stylized, dynamic model to address the issue of sorting\nonline. There are two large homogeneous groups of individuals. Everyone must\nchoose between two online platforms, one of which has superior amenities (akin\nto having superior local public goods). Each individual enjoys interacting\nonline with those from their own group but dislikes being on the same platform\nas those in the other group. Unlike a Tiebout model of residential sorting,\nboth platforms have unlimited capacity so there are no constraints on\ncross-platform migration. It is clear how each group would like to sort\nthemselves but, in the presence of the other type, only the two segregated\noutcomes are guaranteed to be equilibria. Integration on a platform can be\nsupported in equilibrium as long as the platform is sufficiently desirable. If\nonline integration of the two communities is a desired social outcome then the\noptimal policy is clear: make the preferred platform even more desirable.\nRevitalizing the inferior platform will never lead to integration and even\nincreases the likelihood the segregation. Finally, integration is more elastic\nin response to an increase in platform amenities than to reductions in\nintolerance.\n"
    },
    {
        "paper_id": 2110.05611,
        "authors": "Michelle Escobar Carias, David Johnston, Rachel Knott, Rohan Sweeney",
        "title": "Heat and Economic Preferences",
        "comments": "48 pages, 5 figures, 4 Appendix Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The empirical evidence suggests that key accumulation decisions and risky\nchoices associated with economic development depend, at least in part, on\neconomic preferences such as willingness to take risk and patience. This paper\nstudies whether temperature could be one of the potential channels that\ninfluences such economic preferences. Using data from the Indonesia Family Life\nSurvey and NASAs Modern Era Retrospective Analysis for Research and\nApplications data we exploit quasi exogenous variations in outdoor temperatures\ncaused by the random allocation of survey dates. This approach allows us to\nestimate the effects of temperature on elicited measures of risk aversion,\nrational choice violations, and impatience. We then explore three possible\nmechanisms behind this relationship, cognition, sleep, and mood. Our findings\nshow that higher temperatures lead to significantly increased rational choice\nviolations and impatience, but do not significantly increase risk aversion.\nThese effects are mainly driven by night time temperatures on the day prior to\nthe survey and less so by temperatures on the day of the survey. This impact is\nquasi linear and increasing when midnight outdoor temperatures are above 22C.\nThe evidence shows that night time temperatures significantly deplete cognitive\nfunctioning, mathematical skills in particular. Based on these findings we\nposit that heat induced night time disturbances cause stress on critical parts\nof the brain, which then manifest in significantly lower cognitive functions\nthat are critical for individuals to perform economically rational decision\nmaking.\n"
    },
    {
        "paper_id": 2110.05625,
        "authors": "Tobias Reisch, Georg Heiler, Christian Diem, Stefan Thurner",
        "title": "Inferring supply networks from mobile phone data to estimate the\n  resilience of a national economy",
        "comments": "19 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  National economies rest on networks of millions of customer-supplier\nrelations. Some companies -- in the case of their default -- can trigger\nsignificant cascades of shock in the supply-chain network and are thus\nsystemically risky. Up to now, systemic risk of individual companies was\npractically not quantifiable, due to the unavailability of firm-level\ntransaction data. So far, economic shocks are typically studied in the\nframework of input-output analysis on the industry-level that can't relate risk\nto individual firms. Exact firm-level supply networks based on tax or payment\ndata exist only for very few countries. Here we explore to what extent\ntelecommunication data can be used as an inexpensive, easily available, and\nreal-time alternative to reconstruct national supply networks on the\nfirm-level. We find that the conditional probability of correctly identifying a\ntrue customer-supplier link -- given a communication link exists -- is about\n90%. This quality level allows us to reliably estimate a systemic risk profile\nof an entire country that serves as a proxy for the resilience of its economy.\nIn particular, we are able to identify the high systemic risk companies. We\nfind that 65 firms have the potential to trigger large cascades of disruption\nin production chains that could cause severe damages in the economy. We verify\nthat the topological features of the inter-firm communication network are\nhighly similar to national production networks with exact firm-level\ninteractions.\n"
    },
    {
        "paper_id": 2110.06133,
        "authors": "Muhammad Apriandito Arya Saputra, Andry Alamsyah, Fajar Ibnu Fatihan",
        "title": "Hotel Preference Rank based on Online Customer Review",
        "comments": "5 pages, 6 figures, 5 tables",
        "journal-ref": "Test Engineering and Management, Vol. 83: March/April 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Topline hotels are now shifting into the digital way in how they understand\ntheir customers to maintain and ensuring satisfaction. Rather than the\nconventional way which uses written reviews or interviews, the hotel is now\nheavily investing in Artificial Intelligence particularly Machine Learning\nsolutions. Analysis of online customer reviews changes the way companies make\ndecisions in a more effective way than using conventional analysis. The purpose\nof this research is to measure hotel service quality. The proposed approach\nemphasizes service quality dimensions reviews of the top-5 luxury hotel in\nIndonesia that appear on the online travel site TripAdvisor based on section\nBest of 2018. In this research, we use a model based on a simple Bayesian\nclassifier to classify each customer review into one of the service quality\ndimensions. Our model was able to separate each classification properly by\naccuracy, kappa, recall, precision, and F-measure measurements. To uncover\nlatent topics in the customer's opinion we use Topic Modeling. We found that\nthe common issue that occurs is about responsiveness as it got the lowest\npercentage compared to others. Our research provides a faster outlook of hotel\nrank based on service quality to end customers based on a summary of the\nprevious online review.\n"
    },
    {
        "paper_id": 2110.06136,
        "authors": "Victor Chernozhukov, Hiroyuki Kasahara, Paul Schrimpf",
        "title": "A Response to Philippe Lemoine's Critique on our Paper \"Causal Impact of\n  Masks, Policies, Behavior on Early Covid-19 Pandemic in the U.S.\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Recently, Phillippe Lemoine posted a critique of our paper \"Causal Impact of\nMasks, Policies, Behavior on Early Covid-19 Pandemic in the U.S.\"\n[arXiv:2005.14168] at his post titled \"Lockdowns, econometrics and the art of\nputting lipstick on a pig.\" Although Lemoine's critique appears ideologically\ndriven and overly emotional, some of his points are worth addressing. In\nparticular, the sensitivity of our estimation results for (i) including \"masks\nin public spaces\" and (ii) updating the data seems important critiques and,\ntherefore, we decided to analyze the updated data ourselves. This note\nsummarizes our findings from re-examining the updated data and responds to\nPhillippe Lemoine's critique on these two important points. We also briefly\ndiscuss other points Lemoine raised in his post. After analyzing the updated\ndata, we find evidence that reinforces the conclusions reached in the original\nstudy.\n"
    },
    {
        "paper_id": 2110.0619,
        "authors": "Hideyuki Takagi",
        "title": "Exploring the Endogenous Nature of Meme Stocks Using the Log-Periodic\n  Power Law Model and Confidence Indicator",
        "comments": "9 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examined the endogenous nature of negative bubbles forming in meme\nstocks with the Log-Periodic Power Law (LPPL) Confidence Indicator (CI). A meme\nstock is a stock that has gained a significant amount of attention on a large\nsocial media platform such as Yahoo! or Reddit. This study examined four meme\nstocks including Tesla, Inc. (TSLA), GameStop Corp. (GME), Koss Corporation\n(KOSS), and AMC Entertainment Holdings Inc (AMC). The CI was able to detect\nnumerous bubbles forming in meme stocks, but had difficulty in significantly\npredicting social media-induced exogenous rallies. This may have been due to\nprice movements affected by external causes such as short squeezes. However,\nthe model did provide proof for the formation of previous bubbles that could\nhave been a catalyst for the meme stocks rallies. This study outlines the real\nunpredictability of many black-swan events, and further studies could be done\nexamining exogenous bubbles.\n"
    },
    {
        "paper_id": 2110.06464,
        "authors": "Giorgio Costa and Roy H. Kwon",
        "title": "Data-driven distributionally robust risk parity portfolio optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a distributionally robust formulation of the traditional risk\nparity portfolio optimization problem. Distributional robustness is introduced\nby targeting the discrete probabilities attached to each observation used\nduring parameter estimation. Instead of assuming that all observations are\nequally likely, we consider an ambiguity set that provides us with the\nflexibility to find the most adversarial probability distribution based on the\ninvestor's desired degree of robustness. This allows us to derive robust\nestimates to parametrize the distribution of asset returns without having to\nimpose any particular structure on the data. The resulting distributionally\nrobust optimization problem is a constrained convex-concave minimax problem.\nOur approach is financially meaningful and attempts to attain full risk\ndiversification with respect to the worst-case instance of the portfolio risk\nmeasure. We propose a novel algorithmic approach to solve this minimax problem,\nwhich blends projected gradient ascent with sequential convex programming. By\ndesign, this algorithm is highly flexible and allows the user to choose among\nalternative statistical distance measures to define the ambiguity set.\nMoreover, the algorithm is highly tractable and scalable. Our numerical\nexperiments suggest that a distributionally robust risk parity portfolio can\nyield a higher risk-adjusted rate of return when compared against the nominal\nportfolio.\n"
    },
    {
        "paper_id": 2110.06617,
        "authors": "Rapha\\\"el Semet, Thierry Roncalli, Lauren Stagnol",
        "title": "ESG and Sovereign Risk: What is Priced in by the Bond Market and Credit\n  Rating Agencies?",
        "comments": "102 pages, 22 figures, 69 tables. This version has been removed by\n  arXiv administrators due to a copyright claim by a 3rd party",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we examine the materiality of ESG on country creditworthiness\nfrom a credit risk and fundamental analysis viewpoint. We first determine the\nESG indicators that are most relevant when it comes to explaining the sovereign\nbond yield, after controlling the effects of traditional fundamental variables\nsuch as economic strength and credit rating. We also emphasize the major themes\nthat are directly useful for investors when assessing the country risk premium.\nAt the global level, we notice that these themes mainly belong to the E and G\npillars. Those results confirm that extra-financial criteria are integrated\ninto bond pricing. However, we also identify a clear difference between\nhigh-and middle-income countries. Indeed, whereas the S pillar is lagging for\nthe highest income countries, it is nearly as important as the G pillar for the\nmiddle-income ones. Second, we determine which ESG metrics are indirectly\nvaluable for assessing a country's solvency. More precisely, we attempt to\ninfer credit rating solely from extra-financial criteria, that is the ESG\nindicators that are priced in by credit rating agencies. We find that there is\nno overlap between the set of indicators that predict credit ratings and those\nthat directly explain sovereign bond yields. The results also highlight the\nimportance of the G and S pillars when predicting credit ratings. The E pillar\nis lagging, suggesting that credit rating agencies are undermining the impact\nof climate change and environmental topics on country creditworthiness. This is\nconsistent with the traditional view that social and governance issues are the\nmain drivers of the sovereign risk, because they are more specific and less\nglobal than environmental issues. Finally, taking these different results\ntogether, this research shows that opposing extra-financial and fundamental\nanalysis does not make a lot of sense.\n"
    },
    {
        "paper_id": 2110.06822,
        "authors": "Pallavi Gupta (BSE Institute Ltd., Mumbai, INDIA) and Satyanarayan\n  Kothe (Mumbai School of Economics and Public Policy, University of Mumbai,\n  INDIA)",
        "title": "Interpreting the Caste-based Earning Gaps in the Indian Labour Market:\n  Theil and Oaxaca Decomposition Analysis",
        "comments": "Keywords: inequality, wage discrimination, Theil index, Theil\n  decomposition, Oaxaca Three-Fold decomposition, NSSO-EUS 68th round JEL\n  Classification: J01, J08, J15, J30, J31, J71",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The UN states that inequalities are determined along with income by other\nfactors - gender, age, origin, ethnicity, disability, sexual orientation,\nclass, and religion. India, since the ancient period, has socio-political\nstratification that induced socio-economic inequality and continued till now.\nThere have been attempts to reduce socio-economic inequality through policy\ninterventions since the first plan, still there are evidences of social and\neconomic discrimination. This paper examines earning gaps between the forward\ncastes and the traditionally disadvantaged caste workers in the Indian labour\nmarket using two distinct estimation methods. First, we interpret the\ninequality indicator of the Theil index and decompose Theil to show within and\nbetween-group inequalities. Second, a Threefold Oaxaca Decomposition is\nemployed to break the earnings differentials into components of endowment,\ncoefficient and interaction. Earnings gaps are examined separately in urban and\nrural divisions. Within-group, inequalities are found larger than between\ngroups across variables; with a higher overall inequality for forward castes. A\nhigh endowment is observed which implies pre-market discrimination in human\ncapital investment such as nutrition and education. Policymakers should first\ninvest in basic quality education and simultaneously expand post-graduate\ndiploma opportunities, subsequently increasing the participation in the labour\nforce for the traditionally disadvantaged in disciplines and occupations where\nthe forward castes have long dominated.\n"
    },
    {
        "paper_id": 2110.06829,
        "authors": "Leo Ardon, Nelson Vadori, Thomas Spooner, Mengda Xu, Jared Vann,\n  Sumitra Ganesh",
        "title": "Towards a fully RL-based Market Simulator",
        "comments": null,
        "journal-ref": "ACM International Conference on AI in Finance, 2021",
        "doi": "10.1145/3490354.3494372",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new financial framework where two families of RL-based agents\nrepresenting the Liquidity Providers and Liquidity Takers learn simultaneously\nto satisfy their objective. Thanks to a parametrized reward formulation and the\nuse of Deep RL, each group learns a shared policy able to generalize and\ninterpolate over a wide range of behaviors. This is a step towards a fully\nRL-based market simulator replicating complex market conditions particularly\nsuited to study the dynamics of the financial market under various scenarios.\n"
    },
    {
        "paper_id": 2110.06876,
        "authors": "Laura Derksen, Jason Kerwin, Natalia Ordaz Reynoso, and Olivier Sterck",
        "title": "Appointments: A More Effective Commitment Device for Health Behaviors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Health behaviors are plagued by self-control problems, and commitment devices\nare frequently proposed as a solution. We show that a simple alternative works\neven better: appointments. We randomly offer HIV testing appointments and\nfinancial commitment devices to high-risk men in Malawi. Appointments are much\nmore effective than financial commitment devices, more than doubling testing\nrates. In contrast, most men who take up financial commitment devices lose\ntheir investments. Appointments address procrastination without the potential\ndrawback of commitment failure, and also address limited memory problems.\nAppointments have the potential to increase demand for healthcare in the\ndeveloping world.\n"
    },
    {
        "paper_id": 2110.07047,
        "authors": "Mario L\\'opez P\\'erez, Ricardo Mansilla",
        "title": "Ordinal Synchronization and Typical States in High-Frequency Digital\n  Markets",
        "comments": "Two brief appendices have been added at the end of the paper to deal\n  with correlation coefficient-based dynamical networks and multi-scale\n  analysis. The paper was accepted for publication in \"Physica A: Statistical\n  Mechanics and its Applications\"",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.127331",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study Algorithmic High-Frequency Financial Markets as\ndynamical networks. After an individual analysis of 24 stocks of the US market\nduring a trading year of fully automated transactions by means of ordinal\npattern series, we define an information-theoretic measure of pairwise\nsynchronization for time series which allows us to study this subset of the US\nmarket as a dynamical network. We apply to the resulting network a couple of\nclustering algorithms in order to detect collective market states,\ncharacterized by their degree of centralized or descentralized synchronicity.\nThis collective analysis has shown to reproduce, classify and explain the\nanomalous behavior previously observed at the individual level. We also find\ntwo whole coherent seasons of highly centralized and descentralized\nsynchronicity, respectively. Finally, we model these states dynamics through a\nsimple Markov model.\n"
    },
    {
        "paper_id": 2110.07075,
        "authors": "Myles Sjogren (1) and Timothy DeLise (2) ((1) University of Calgary,\n  (2) Universit\\'e de Montr\\'eal)",
        "title": "General Compound Hawkes Processes for Mid-Price Prediction",
        "comments": "20 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  High frequency financial data is burdened by a level of randomness that is\nunavoidable and obfuscates the task of modelling. This idea is reflected in the\nintraday evolution of limit orders book data for many financial assets and\nsuggests several justifications for the use of stochastic models. For instance,\nthe arbitrary distribution of inter arrival times and the subsequent dependence\nstructure between consecutive book events. This has lead to the development of\nmany stochastic models for the dynamics of limit order books. In this paper we\nlook to examine the adaptability of one family of such models, the General\nCompound Hawkes Process (GCHP) models, to new data and new tasks. We further\nfocus on the prediction problem for the mid-price within a limit order book and\nthe practical applications of these stochastic models, which is the main\ncontribution of this paper. To this end we examine the use of the GCHP for\npredicting the direction and volatility of futures and stock data and discuss\npossible extensions of the model to help improve its predictive capabilities.\n"
    },
    {
        "paper_id": 2110.07138,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "ETF Risk Models",
        "comments": "20 pages; to appear in Bulletin of Applied Economics (in press)",
        "journal-ref": "Bulletin of Applied Economics 9(1) (2022) 1-17",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We discuss how to build ETF risk models. Our approach anchors on i) first\nbuilding a multilevel (non-)binary classification/taxonomy for ETFs, which is\nutilized in order to define the risk factors, and ii) then building the risk\nmodels based on these risk factors by utilizing the heterotic risk model\nconstruction of https://ssrn.com/abstract=2600798 (for binary classifications)\nor general risk model construction of https://ssrn.com/abstract=2722093 (for\nnon-binary classifications). We discuss how to build an ETF taxonomy using ETF\nconstituent data. A multilevel ETF taxonomy can also be constructed by\nappropriately augmenting and expanding well-built and granular third-party\nsingle-level ETF groupings.\n"
    },
    {
        "paper_id": 2110.07489,
        "authors": "Muhammad Hassan Bin Afzal",
        "title": "An Empirical Analysis of how Internet Access Influences Public Opinion\n  towards Undocumented Immigrants and Unaccompanied Children",
        "comments": "20 pages, 2 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research adds to the expanding field of data-driven analysis, scientific\nmodeling, and forecasting on the impact of having access to the Internet and\nIoT on the general US population regarding immigrants and immigration policies.\nMore specifically, this research focuses on the public opinion of undocumented\nimmigrants in the United States and having access to the Internet in their\nlocal settings. The term Undocumented Immigrants refers to those who live in\nthe United States without legal papers, documents, or visas. Undocumented\nimmigrants may have come into the country unlawfully or with valid\ndocumentation, but their legal status has expired. Using the 2020 American\nNational Election Studies (ANES) time series dataset, I investigated the\nrelationship between internet access (A2I) and public perception of\nundocumented immigrants. According to my research and analysis, increasing\ninternet access among non-Hispanic whites with at least a bachelors degree with\nan annual household income of less than 99K is more likely to oppose the\ndeportation of undocumented immigrants and separating unaccompanied children\nfrom their families in borderland areas. The individuals with substantial\nRepublican political ideology exhibit significantly lower opposing effects in\ndeporting undocumented immigrants or separating unaccompanied children from\ntheir families. The evidence from multiple statistical models is resilient to a\nvariety of factors. The findings show that increased internet access may\nimprove undocumented immigrants social integration and acceptability. During\nhealth emergencies, it may be especially beneficial to make them feel safe,\nincluded, and supported in their local settings.\n"
    },
    {
        "paper_id": 2110.07611,
        "authors": "Reka Sundaram-Stukel and Steven C Deller",
        "title": "Locational Factors in the Competition between Credit Unions and Banks\n  after the Great Recession",
        "comments": "59pages, 4 Tables, 8 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the aftermath of the Great Recession, the regulatory framework for credit\nunion operations has become a subject of controversy. Competing financial\nenterprises such as thrifts and other banks have argued that credit unions have\nreceived preferential treatment under existing tax and regulatory codes,\nwhereas credit unions complained of undue restrictions on their ability to\nscale up and increase their scope of operations. Building on previous location\nmodels, this analysis focuses on credit union headquarter locations immediately\nfollowing the 2008 financial crisis. We offer a new perspective for\nunderstanding credit union behavior based on central place theory, new\nindustrial organization literature, and credit union location analysis. Our\nfindings indicate that credit unions avoid locating near other lending\ninstitutions, instead operating in areas with a low concentration of other\nbanks. This finding provides evidence that credit unions serve niche markets,\nproduct differential, and are not a significant source of direct competition\nfor thrifts and other banks.\n"
    },
    {
        "paper_id": 2110.0832,
        "authors": "Jingtang Ma, Wensheng Yang, Zhenyu Cui",
        "title": "Semimartingale and continuous-time Markov chain approximation for rough\n  stochastic local volatility models",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rough volatility models have recently been empirically shown to provide a\ngood fit to historical volatility time series and implied volatility smiles of\nSPX options. They are continuous-time stochastic volatility models, whose\nvolatility process is driven by a fractional Brownian motion with Hurst\nparameter less than half. Due to the challenge that it is neither a\nsemimartingale nor a Markov process, there is no unified method that not only\napplies to all rough volatility models, but also is computationally efficient.\nThis paper proposes a semimartingale and continuous-time Markov chain (CTMC)\napproximation approach for the general class of rough stochastic local\nvolatility (RSLV) models. In particular, we introduce the perturbed stochastic\nlocal volatility (PSLV) model as the semimartingale approximation for the RSLV\nmodel and establish its existence , uniqueness and Markovian representation. We\npropose a fast CTMC algorithm and prove its weak convergence. Numerical\nexperiments demonstrate the accuracy and high efficiency of the method in\npricing European, barrier and American options. Comparing with existing\nliterature, a significant reduction in the CPU time to arrive at the same level\nof accuracy is observed.\n"
    },
    {
        "paper_id": 2110.08367,
        "authors": "Ananthan Nambiar, Tobias Rubel, James McCaull, Jon deVries and Mark\n  Bedau",
        "title": "Dropping diversity of products of large US firms: Models and measures",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0264330",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It is widely assumed that in our lifetimes the products available in the\nglobal economy have become more diverse. This assumption is difficult to\ninvestigate directly, however, because it is difficult to collect the necessary\ndata about every product in an economy each year. We solve this problem by\nmining publicly available textual descriptions of the products of every large\nUS firms each year from 1997 to 2017. Although many aspects of economic\nproductivity have been steadily rising during this period, our text-based\nmeasurements show that the diversity of the products of at least large US firms\nhas steadily declined. This downward trend is visible using a variety of\nproduct diversity metrics, including some that depend on a measurement of the\nsimilarity of the products of every single pair of firms. The current state of\nthe art in comprehensive and detailed firm-similarity measurements is a Boolean\nword vector model due to Hoberg and Phillips. We measure diversity using\nfirm-similarities from this Boolean model and two more sophisticated variants,\nand we consistently observe a significant dropping trend in product diversity.\nThese results make it possible to frame and start to test specific hypotheses\nfor explaining the dropping product diversity trend.\n"
    },
    {
        "paper_id": 2110.08612,
        "authors": "Satoshi Nakano and Kazuhiko Nishimura",
        "title": "The elastic origins of tail asymmetry",
        "comments": null,
        "journal-ref": "Macroeconomic Dynamics 2023",
        "doi": "10.1017/S1365100523000172",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on a multisector general equilibrium framework, we show that the\nsectoral elasticity of substitution plays the key role in the evolution of\nasymmetric tails of macroeconomic fluctuations and the establishment of\nrobustness against productivity shocks. Non-unitary elasticity of substitution\nrenders a nonlinear Domar aggregation, where normal sectoral productivity\nshocks translate into non-normal aggregated shocks with variable expected\noutput growth. We empirically estimate 100 sectoral elasticities of\nsubstitution, using the time-series linked input-output tables for Japan, and\nfind that the production economy is elastic overall, relative to Cobb-Douglas\nwith unitary elasticity. Along with the previous assessment of an inelastic\nproduction economy for the US, the contrasting tail asymmetry of the\ndistribution of aggregated shocks between the US and Japan is explained.\nMoreover, robustness of an economy is assessed by the expected output growth,\nthe level of which is led by the sectoral elasticities of substitution, under\nzero mean productivity shocks.\n"
    },
    {
        "paper_id": 2110.0863,
        "authors": "Marcelo Brutti Righi",
        "title": "Star-shaped acceptability indexes",
        "comments": null,
        "journal-ref": "Insurance Mathematics and Economics, 117, 170-181 (2024)",
        "doi": "10.1016/j.insmatheco.2024.05.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the star-shaped acceptability indexes as generalizations of both\nthe approaches of Cherny and Madan (2009) and Rosazza Gianin and Sgarra (2013)\nin the same vein as star-shaped risk measures generalize both the classes of\ncoherent and convex risk measures in Castagnoli et al. (2022). We characterize\nacceptability indexes through star-shaped risk measures, star-shaped acceptance\nsets, and as the minimum of a family of quasi-concave acceptability indexes.\nFurther, we introduce concrete examples under our approach linked to Value at\nRisk, risk-adjusted reward on capital, reward-based gain-loss ratio, monotone\nreward-deviation ratio, and robust acceptability indexes.\n"
    },
    {
        "paper_id": 2110.08673,
        "authors": "Alon Benhaim, Brett Hemenway Falk, Gerry Tsoukalas",
        "title": "Scaling Blockchains: Can Committee-Based Consensus Help?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In the high-stakes race to develop more scalable blockchains, some platforms\n(Binance, Cosmos, EOS, TRON, etc.) have adopted committee-based consensus (CBC)\nprotocols, whereby the blockchain's record-keeping rights are entrusted to a\ncommittee of elected block producers. In theory, the smaller the committee, the\nfaster the blockchain can reach consensus and the more it can scale. What's\nless clear, is whether such protocols ensure that honest committees can be\nconsistently elected, given blockchain users typically have limited information\non who to vote for. We show that the approval voting mechanism underlying most\nCBC protocols is complex and can lead to intractable optimal voting strategies.\nWe empirically characterize some simpler intuitive voting strategies that users\ntend to resort to in practice and prove that these nonetheless converge to\noptimality exponentially quickly in the number of voters. Exponential\nconvergence ensures that despite its complexity, CBC exhibits robustness and\nhas some efficiency advantages over more popular staked-weighted lottery\nprotocols currently underlying many prominent blockchains such as Ethereum.\n"
    },
    {
        "paper_id": 2110.08723,
        "authors": "Han Dongcheng, Kong Fanbo, Wang Zixun",
        "title": "Gender identity and relative income within household: Evidence from\n  China",
        "comments": "This is a paper written by three high school students living in\n  Singapore. We look forward to valuable comments and suggestions to improve\n  the paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How does women's obedience to traditional gender roles affect their labour\noutcomes? To investigate on this question, we employ discontinuity tests and\nfixed effect regressions with time lag to measure how married women in China\ndiminish their labour outcomes so as to maintain the bread-winning status of\ntheir husbands. In the first half of this research, our discontinuity test\nexhibits a missing mass of married women who just out-earn their husbands,\nwhich is interpreted as an evidence showing that these females diminish their\nearnings under the influence of gender norms. In the second half, we use fixed\neffect regressions with time lag to assess the change of a female's future\nlabour outcomes if she currently earns more than her husband. Our results\nsuggest that women's future labour participation decisions (whether they still\njoin the workforce) are unaffected, but their yearly incomes and weekly working\nhours will be reduced in the future. Lastly, heterogeneous studies are\nconducted, showing that low-income and less educated married women are more\nsusceptible to the influence of gender norms.\n"
    },
    {
        "paper_id": 2110.08807,
        "authors": "Aur\\'elien Sallin",
        "title": "Estimating returns to special education: combining machine learning and\n  text analysis to address confounding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Leveraging unique insights into the special education placement process\nthrough written individual psychological records, I present results from the\nfirst ever study to examine short- and long-term returns to special education\nprograms with causal machine learning and computational text analysis methods.\nI find that special education programs in inclusive settings have positive\nreturns in terms of academic performance as well as labor-market integration.\nMoreover, I uncover a positive effect of inclusive special education programs\nin comparison to segregated programs. This effect is heterogenous: segregation\nhas least negative effects for students with emotional or behavioral problems,\nand for nonnative students with special needs. Finally, I deliver optimal\nprogram placement rules that would maximize aggregated school performance and\nlabor market integration for students with special needs at lower program\ncosts. These placement rules would reallocate most students with special needs\nfrom segregation to inclusion.\n"
    },
    {
        "paper_id": 2110.08884,
        "authors": "Semyon Malamud and Andreas Schrimpf",
        "title": "Persuasion by Dimension Reduction",
        "comments": "This paper has been replaced and subsumed by arXiv:2210.00637. arXiv\n  admin note: text overlap with arXiv:2102.10909",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How should an agent (the sender) observing multi-dimensional data (the state\nvector) persuade another agent to take the desired action? We show that it is\nalways optimal for the sender to perform a (non-linear) dimension reduction by\nprojecting the state vector onto a lower-dimensional object that we call the\n\"optimal information manifold.\" We characterize geometric properties of this\nmanifold and link them to the sender's preferences. Optimal policy splits\ninformation into \"good\" and \"bad\" components. When the sender's marginal\nutility is linear, revealing the full magnitude of good information is always\noptimal. In contrast, with concave marginal utility, optimal information design\nconceals the extreme realizations of good information and only reveals its\ndirection (sign). We illustrate these effects by explicitly solving several\nmulti-dimensional Bayesian persuasion problems.\n"
    },
    {
        "paper_id": 2110.089,
        "authors": "Gechun Liang, Moris S. Strub, Yuwei Wang",
        "title": "Predictable Forward Performance Processes: Infrequent Evaluation and\n  Applications to Human-Machine Interactions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study discrete-time predictable forward processes when trading times do\nnot coincide with performance evaluation times in a binomial tree model for the\nfinancial market. The key step in the construction of these processes is to\nsolve a linear functional equation of higher order associated with the inverse\nproblem driving the evolution of the predictable forward process. We provide\nsufficient conditions for the existence and uniqueness and an explicit\nconstruction of the predictable forward process under these conditions.\nFurthermore, we find that these processes are inherently myopic in the sense\nthat optimal strategies do not make use of future model parameters even if\nthese are known. Finally, we argue that predictable forward preferences are a\nviable framework to model human-machine interactions occuring in automated\ntrading or robo-advising. For both applications, we determine an optimal\ninteraction schedule of a human agent interacting infrequently with a machine\nthat is in charge of trading.\n"
    },
    {
        "paper_id": 2110.09098,
        "authors": "Jonathan Labbe (CEREFIGE)",
        "title": "Study of The Relationship Between Public and Private Venture Capitalists\n  in France: A Qualitative Approach",
        "comments": "3rd International Conference on Digital, Innovation, Entrepreneurship\n  & Financing., Oct 2021, Lyon, France",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research focuses on the study of relationships between public and\nprivate equity investors in France. In this regard, we need to apprehend the\nformal or informal nature of interactions that can sometimes take place within\ntraditional innovation networks (Djellal \\& Gallouj, 2018). For this, our\narticle mobilizes a public-private partnerships approach (PPPs) and the\nresource-based view theory. These perspectives emphasize the complementary role\nof disciplinary and incentive mechanisms as well as the exchange of specific\nresources as levers for value creation. Moreover, these orientations crossed\nwith the perspective of a hybrid form of co-investment allow us to build a\ncoherent and explanatory framework of the mixed syndication phenomenon. Our\nmethodology is based on a qualitative approach with an interpretative aim,\nwhich includes twenty-seven semi-structured interviews. These data were\nsubjected to a thematic content analysis using Nvivo software. The results\nsuggest that the relationships between public and private Venture capitalists\n(VCs) of a formal or informal nature, more specifically in a syndication\ncontext, at a national or regional level, are representative of an\n''economico-cognitive'' (Farrugia, 2014, page 6) approach to networking and\ninnovation. Moreover, the phenomenon of mixed syndication reveals a context of\nhybridization of public and private actors that would allow the private VCs to\nbenefit from the distribution of wealth when the company develops its\ninnovation. We can also identify a process related to a quest for legitimacy on\nthe part of the public actor characterized by its controlling role within the\npublic-private partnership (Beuve and Saussier, 2019). Finally, our study has\nsome limitations. One example is the measurement of the effects of\nrelationships on ''visible'' or ''invisible'' innovation (Djellal \\& Gallouj,\n2018, page 90).\n"
    },
    {
        "paper_id": 2110.09169,
        "authors": "Chika O. Okafor",
        "title": "Prosecutor Politics: The Impact of Election Cycles on Criminal\n  Sentencing in the Era of Rising Incarceration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I investigate how political incentives affect the behavior of district\nattorneys (DAs). I develop a theoretical model that predicts DAs will increase\nsentencing intensity in an election period compared to the period prior. To\nempirically test this prediction, I compile one of the most comprehensive\ndatasets to date on the political careers of all district attorneys in office\nduring the steepest rise in incarceration in U.S. history (roughly 1986-2006).\nUsing quasi-experimental methods, I find causal evidence that being in a DA\nelection year increases total admissions per capita and total months sentenced\nper capita. I estimate that the election year effects on admissions are akin to\nmoving 0.85 standard deviations along the distribution of DA behavior within\nstate (e.g., going from the 50th to 80th percentile in sentencing intensity). I\nfind evidence that election effects are larger (1) when DA elections are\ncontested, (2) in Republican counties, and (3) in the southern United\nStates--all these factors are consistent with the perspective that election\neffects arise from political incentives influencing DAs. Further, I find that\ndistrict attorney election effects decline over the period 1986-2006, in tandem\nwith U.S. public opinion softening regarding criminal punishment. These\nfindings suggest DA behavior may respond to voter preferences--in particular to\npublic sentiment regarding the harshness of the court system.\n"
    },
    {
        "paper_id": 2110.09315,
        "authors": "Tugce Karatas, Ali Hirsa",
        "title": "Predicting Status of Pre and Post M&A Deals Using Machine Learning and\n  Deep Learning Techniques",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Risk arbitrage or merger arbitrage is a well-known investment strategy that\nspeculates on the success of M&A deals. Prediction of the deal status in\nadvance is of great importance for risk arbitrageurs. If a deal is mistakenly\nclassified as a completed deal, then enormous cost can be incurred as a result\nof investing in target company shares. On the contrary, risk arbitrageurs may\nlose the opportunity of making profit. In this paper, we present an ML and DL\nbased methodology for takeover success prediction problem. We initially apply\nvarious ML techniques for data preprocessing such as kNN for data imputation,\nPCA for lower dimensional representation of numerical variables, MCA for\ncategorical variables, and LSTM autoencoder for sentiment scores. We experiment\nwith different cost functions, different evaluation metrics, and oversampling\ntechniques to address class imbalance in our dataset. We then implement\nfeedforward neural networks to predict the success of the deal status. Our\npreliminary results indicate that our methodology outperforms the benchmark\nmodels such as logit and weighted logit models. We also integrate sentiment\nscores into our methodology using different model architectures, but our\npreliminary results show that the performance is not changing much compared to\nthe simple FFNN framework. We will explore different architectures and employ a\nthorough hyperparameter tuning for sentiment scores as a future work.\n"
    },
    {
        "paper_id": 2110.094,
        "authors": "Dario Laudati and M. Hashem Pesaran",
        "title": "Identifying the Effects of Sanctions on the Iranian Economy using\n  Newspaper Coverage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers how sanctions affected the Iranian economy using a novel\nmeasure of sanctions intensity based on daily newspaper coverage. It finds\nsanctions to have significant effects on exchange rates, inflation, and output\ngrowth, with the Iranian rial over-reacting to sanctions, followed up with a\nrise in inflation and a fall in output. In absence of sanctions, Iran's average\nannual growth could have been around 4-5 per cent, as compared to the 3 per\ncent realized. Sanctions are also found to have adverse effects on employment,\nlabor force participation, secondary and high-school education, with such\neffects amplified for females.\n"
    },
    {
        "paper_id": 2110.09416,
        "authors": "Ale\\v{s} \\v{C}ern\\'y, Christoph Czichowsky, and Jan Kallsen",
        "title": "Numeraire-invariant quadratic hedging and mean--variance portfolio\n  allocation",
        "comments": "37 pages",
        "journal-ref": "Mathematics of Operations Research 49(2), 752-781, 2024",
        "doi": "10.1287/moor.2023.1374",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper investigates quadratic hedging in a semimartingale market that does\nnot necessarily contain a risk-free asset. An equivalence result for hedging\nwith and without numeraire change is established (Proposition 3.16). This\npermits direct computation of the optimal strategy without choosing a reference\nasset and/or performing a numeraire change (Theorem 4.1). New explicit\nexpressions for optimal strategies are obtained, featuring the use of oblique\nprojections that provide unified treatment of the case with and without a\nrisk-free asset (Theorem 4.3). The analysis yields a streamlined computation of\nthe efficient frontier for the pure investment problem in terms of three easily\ninterpreted processes (Equation~1.1). The main result advances our\nunderstanding of the efficient frontier formation in the most general case\nwhere a risk-free asset may not be present. Several illustrations of the\nnumeraire-invariant approach are given.\n"
    },
    {
        "paper_id": 2110.09417,
        "authors": "Yang Shen and Bin Zou",
        "title": "Mean-Variance Portfolio Selection in Contagious Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a mean-variance portfolio selection problem in a financial market\nwith contagion risk. The risky assets follow a jump-diffusion model, in which\njumps are driven by a multivariate Hawkes process with mutual-excitation\neffect. The mutual-excitation feature of the Hawkes process captures the\ncontagion risk in the sense that each price jump of an asset increases the\nlikelihood of future jumps not only in the same asset but also in other assets.\nWe apply the stochastic maximum principle, backward stochastic differential\nequation theory, and linear-quadratic control technique to solve the problem\nand obtain the efficient strategy and efficient frontier in semi-closed form,\nsubject to a non-local partial differential equation. Numerical examples are\nprovided to illustrate our results.\n"
    },
    {
        "paper_id": 2110.09429,
        "authors": "Danial Saef, Odett Nagy, Sergej Sizov, Wolfgang Karl H\\\"ardle",
        "title": "Understanding jumps in high frequency digital asset markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While attention is a predictor for digital asset prices, and jumps in Bitcoin\nprices are well-known, we know little about its alternatives. Studying high\nfrequency crypto data gives us the unique possibility to confirm that cross\nmarket digital asset returns are driven by high frequency jumps clustered\naround black swan events, resembling volatility and trading volume\nseasonalities. Regressions show that intra-day jumps significantly influence\nend of day returns in size and direction. This provides fundamental research\nfor crypto option pricing models. However, we need better econometric methods\nfor capturing the specific market microstructure of cryptos. All calculations\nare reproducible via the quantlet.com technology.\n"
    },
    {
        "paper_id": 2110.09489,
        "authors": "Curtis Nybo",
        "title": "Sector Volatility Prediction Performance Using GARCH Models and\n  Artificial Neural Networks",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently artificial neural networks (ANNs) have seen success in volatility\nprediction, but the literature is divided on where an ANN should be used rather\nthan the common GARCH model. The purpose of this study is to compare the\nvolatility prediction performance of ANN and GARCH models when applied to\nstocks with low, medium, and high volatility profiles. This approach intends to\nidentify which model should be used for each case. The volatility profiles\ncomprise of five sectors that cover all stocks in the U.S stock market from\n2005 to 2020. Three GARCH specifications and three ANN architectures are\nexamined for each sector, where the most adequate model is chosen to move on to\nforecasting. The results indicate that the ANN model should be used for\npredicting volatility of assets with low volatility profiles, and GARCH models\nshould be used when predicting volatility of medium and high volatility assets.\n"
    },
    {
        "paper_id": 2110.09516,
        "authors": "Linda Chamakh and Zolt\\'an Szab\\'o",
        "title": "Kernel Minimum Divergence Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio optimization is a key challenge in finance with the aim of creating\nportfolios matching the investors' preference. The target distribution approach\nrelying on the Kullback-Leibler or the $f$-divergence represents one of the\nmost effective forms of achieving this goal. In this paper, we propose to use\nkernel and optimal transport (KOT) based divergences to tackle the task, which\nrelax the assumptions and the optimization constraints of the previous\napproaches. In case of the kernel-based maximum mean discrepancy (MMD) we (i)\nprove the analytic computability of the underlying mean embedding for various\ntarget distribution-kernel pairs, (ii) show that such analytic knowledge can\nlead to faster convergence of MMD estimators, and (iii) extend the results to\nthe unbounded exponential kernel with minimax lower bounds. Numerical\nexperiments demonstrate the improved performance of our KOT estimators both on\nsynthetic and real-world examples.\n"
    },
    {
        "paper_id": 2110.09673,
        "authors": "James McNerney, Yang Li, Andres Gomez-Lievano, Frank Neffke",
        "title": "Bridging the short-term and long-term dynamics of economic structural\n  change",
        "comments": "18 pages + 11 pages supplementary text, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic transformation -- change in what an economy produces -- is\nfoundational to development and rising standards of living. Our understanding\nof this process has been propelled recently by two branches of work in the\nfield of economic complexity, one studying how economies diversify, the other\nhow the complexity of an economy is expressed in the makeup of its output.\nHowever, the connection between these branches is not well understood, nor how\nthey relate to a classic understanding of structural transformation. Here, we\npresent a simple dynamical modeling framework that unifies these areas of work,\nbased on the widespread observation that economies diversify preferentially\ninto activities that are related to ones they do already. We show how stylized\nfacts of long-run structural change, as well as complexity metrics, can both\nemerge naturally from this one observation. However, complexity metrics take on\nnew meanings, as descriptions of the long-term changes an economy experiences\nrather than measures of complexity per se. This suggests relatedness and\ncomplexity metrics are connected, in a hitherto overlooked way: Both describe\nstructural change, on different time scales. Whereas relatedness probes\ntransformation on short time scales, complexity metrics capture long-term\nchange.\n"
    },
    {
        "paper_id": 2110.10781,
        "authors": "Mikhail Freer and Khushboo Surana",
        "title": "Marital Stability With Committed Couples: A Revealed Preference Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a revealed preference characterization of marital stability where\nsome couples are committed. A couple is committed if they can only divorce upon\nmutual consent. We provide theoretical insights into the potential of the\ncharacterization for identifying intrahousehold consumption patterns. We show\nthat when there is no price variation for private goods between potential\ncouples, it is only possible to identify intrahousehold resource allocations\nfor non-committed couples. Simulation exercises using household data drawn from\nthe Longitudinal Internet Studies for the Social Sciences (LISS) panel support\nour theoretical findings. Our results show that in the presence of price\nvariation, the empirical implications of marital stability can be used for\nidentifying household consumption allocations for both committed and\nnon-committed couples.\n"
    },
    {
        "paper_id": 2110.10792,
        "authors": "Tolulope Fadina, Yang Liu, Ruodu Wang",
        "title": "A Framework for Measures of Risk under Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A risk analyst assesses potential financial losses based on multiple sources\nof information. Often, the assessment does not only depend on the specification\nof the loss random variable but also various economic scenarios. Motivated by\nthis observation, we design a unified axiomatic framework for risk evaluation\nprinciples which quantifies jointly a loss random variable and a set of\nplausible probabilities. We call such an evaluation principle a generalized\nrisk measure. We present a series of relevant theoretical results. The\nworst-case, coherent, and robust generalized risk measures are characterized\nvia different sets of intuitive axioms. We establish the equivalence between a\nfew natural forms of law invariance in our framework, and the technical\nsubtlety therein reveals a sharp contrast between our framework and the\ntraditional one. Moreover, coherence and strong law invariance are derived from\na combination of other conditions, which provides additional support for\ncoherent risk measures such as Expected Shortfall over Value-at-Risk, a\nrelevant issue for risk management practice.\n"
    },
    {
        "paper_id": 2110.108,
        "authors": "David Ardia, Keven Bluteau, Kris Boudt",
        "title": "Media abnormal tone, earnings announcements, and the stock market",
        "comments": "Forthcoming in Journal of Financial Markets",
        "journal-ref": "Journal of Financial Markets, Volume 61, November 2022, 100683",
        "doi": "10.1016/j.finmar.2021.100683",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We conduct a tone-based event study to examine the aggregate abnormal tone\ndynamics in media articles around earnings announcements. We test whether they\nconvey incremental information that is useful for price discovery for\nnonfinancial S&P 500 firms. The relation we find between the abnormal tone and\nabnormal returns suggests that media articles provide incremental information\nrelative to the information contained in earnings press releases and earnings\ncalls.\n"
    },
    {
        "paper_id": 2110.10936,
        "authors": "Robert Jarrow and Philip Protter and Alejandra Quintos",
        "title": "Computing the Probability of a Financial Market Failure: A New Measure\n  of Systemic Risk",
        "comments": "Ann Oper Res (2022)",
        "journal-ref": null,
        "doi": "10.1007/s10479-022-05146-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper characterizes the probability of a market failure defined as the\ndefault of two or more globally systemically important banks (G-SIBs) in a\nsmall interval of time. The default probabilities of the G-SIBs are correlated\nthrough the possible existence of a market-wide stress event. The\ncharacterization employs a multivariate Cox process across the G-SIBs, which\nallows us to relate our work to the existing literature on intensity-based\nmodels. Various theorems related to market failure probabilities are derived,\nincluding the probability of a market failure due to two banks defaulting over\nthe next infinitesimal interval, the probability of a catastrophic market\nfailure, the impact of increasing the number of G-SIBs in an economy, and the\nimpact of changing the initial conditions of the economy's state variables. We\nalso show that if there are too many G-SIBs, a market failure is inevitable,\ni.e., the probability of a market failure tends to 1.\n"
    },
    {
        "paper_id": 2110.11008,
        "authors": "Simon Clinet, Jean-Fran\\c{c}ois Perreton and Serge Reydellet",
        "title": "Optimal trading: a model predictive control approach",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a dynamic trading strategy in the Linear Quadratic Regulator (LQR)\nframework. By including a price mean-reversion signal into the optimization\nprogram, in a trading environment where market impact is linear and stage costs\nare quadratic, we obtain an optimal trading curve that reacts opportunistically\nto price changes while retaining its ability to satisfy smooth or hard\ncompletion constraints. The optimal allocation is affine in the spot price and\nin the number of outstanding shares at any time, and it can be fully derived\niteratively. It is also aggressive in the money, meaning that it accelerates\nwhenever the price is favorable, with an intensity that can be calibrated by\nthe practitioner. Since the LQR may yield locally negative participation rates\n(i.e round trip trades) which are often undesirable, we show that the\naforementioned optimization problem can be improved and solved under positivity\nconstraints following a Model Predictive Control (MPC) approach. In particular,\nit is smoother and more consistent with the completion constraint than putting\na hard floor on the participation rate. We finally examine how the LQR can be\nsimplified in the continuous trading context, which allows us to derive a\nclosed formula for the trading curve under further assumptions, and we document\na two-step strategy for the case where trades can also occur in an additional\ndark pool.\n"
    },
    {
        "paper_id": 2110.11156,
        "authors": "Parley Ruogu Yang, Ryan Lucas",
        "title": "DMS, AE, DAA: methods and applications of adaptive time series model\n  selection, ensemble, and financial evaluation",
        "comments": "Key words: Time series, model selection, model evaluation,\n  cross-asset strategy, market crash, VIX",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce three adaptive time series learning methods, called Dynamic\nModel Selection (DMS), Adaptive Ensemble (AE), and Dynamic Asset Allocation\n(DAA). The methods respectively handle model selection, ensembling, and\ncontextual evaluation in financial time series. Empirically, we use the methods\nto forecast the returns of four key indices in the US market, incorporating\ninformation from the VIX and Yield curves. We present financial applications of\nthe learning results, including fully-automated portfolios and dynamic hedging\nstrategies. The strategies strongly outperform long-only benchmarks over our\ntesting period, spanning from Q4 2015 to the end of 2021. The key outputs of\nthe learning methods are interpreted during the 2020 market crash.\n"
    },
    {
        "paper_id": 2110.11581,
        "authors": "Yanrong Li, Lai Wei and Wei Jiang",
        "title": "A Two-stage Pricing Strategy Considering Learning Effects and\n  Word-of-Mouth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a two-stage pricing strategy for nondurable (such as\ntypical electronics) products, where retail price is cut down at certain time\npoints of the product lifecycle. We consider learning effect of electronic\nproducts that, with the accumulation of production, average production cost\ndecreases over time as manufacturers get familiar with the production process.\nMoreover, word-of-mouth (WOM) of existing customers is used to analyze future\ndemand, which is sensitive to the difference between the actual reliability and\nthe perceived reliability of products. We theoretically prove the existence and\nuniqueness of the optimal switch time between the two stages and the optimal\nprice in each stage. In addition, warranty as another important factor of\nelectronic products is also considered, whose interaction with word-of-mouth as\nwell as the corresponding influences on total profit are analyzed.\nInterestingly, our findings indicate that (1) the main reason for manufacturers\nto cut down prices for electronic products pertains to the learning effects;\n(2) even through both internal factors (e.g., the learning effects of\nmanufacturers) and external factors (e.g., the price elasticity of customers)\nhave impacts on product price, their influence on manufacturer's profit is\nwidely divergent; (3) generally warranty weakens the influence of external\nadvertising on the reliability estimate, because warranty price only partially\nreflects the actual reliability information of products; (4) and the optimal\nwarranty price can increase the profits for the manufacturer by approximately\n10%.\n"
    },
    {
        "paper_id": 2110.11582,
        "authors": "Artem Kuriksha",
        "title": "An Economy of Neural Networks: Learning from Heterogeneous Experiences",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new way to model behavioral agents in dynamic\nmacro-financial environments. Agents are described as neural networks and learn\npolicies from idiosyncratic past experiences. I investigate the feedback\nbetween irrationality and past outcomes in an economy with heterogeneous shocks\nsimilar to Aiyagari (1994). In the model, the rational expectations assumption\nis seriously violated because learning of a decision rule for savings is\nunstable. Agents who fall into learning traps save either excessively or save\nnothing, which provides a candidate explanation for several empirical puzzles\nabout wealth distribution. Neural network agents have a higher average MPC and\nexhibit excess sensitivity of consumption. Learning can negatively affect\nintergenerational mobility.\n"
    },
    {
        "paper_id": 2110.11594,
        "authors": "Marui Du, Yue Ma, Zuoquan Zhang",
        "title": "A Meta Path Based Evaluation Method for Enterprise Credit Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays small and medium-sized enterprises have become an essential part of\nthe national economy. With the increasing number of such enterprises, how to\nevaluate their credit risk becomes a hot issue. Unlike big enterprises with\nmassive data to analyze, it is hard to find enough information of small\nenterprises to assess their financial status. Limited by the lack of primary\ndata, how to inference small enterprises' credit risk from secondary data, like\ninformation of their upstream, downstream, parent, and subsidiary enterprises\nattracts big attention from industry and academy. Targeting on accurately\nevaluating the credit risk of the small and medium-sized enterprise (SME), in\nthis paper, we exploit the representative power of Information Network on\nvarious kinds of SME entities and SME relationships to solve the problem. A\nnovel feature named meta path feature proposed to measure the credit risk,\nwhich makes us able to evaluate the financial status of SMEs from various\nperspectives. Experiments show that our method is effective to identify SMEs\nwith credit risks.\n"
    },
    {
        "paper_id": 2110.11694,
        "authors": "Aasheesh Dixit, Patanjal Kumar and Suresh Jakhar",
        "title": "Airport-Airline Coordination with Economic, Environmental and Social\n  Considerations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we examine the effect of various contracts between a socially\nconcerned airport and an environmentally conscious airline regarding their\nprofitability and channel coordination under two distinct settings. First, we\nconsider no government interventions, while in the second, we explore\ngovernment-imposed taxations to curb emissions. Furthermore, we investigate the\nimpact of passenger greening sensitivity, greening cost, and consumer surplus\ncoefficient on conveyance fees, ticket fare, greening level and the channel\nwelfare. Our analysis shows that the revenue sharing and linear two part tariff\ncontracts coordinate the decentralised airport-airline channel. Our findings\nalso reveal that players greening and social efforts can improve both the\nwelfare and efficiency of the channel simultaneously. Importantly, under\ngovernment interventions, taxation does help improve the greening level of the\nchannel in both coordinating and non coordinating contracts. However, the\ngreening level in the non-coordinating contracts with taxation is still less\nthan the coordinating contracts even without tax. Finally, we also extended the\nmodel to include a duopoly airline market with pricing and greening\ncompetition. We analyze the effect of competetiton between airlines on airport\nutility, airline profit, ticket fare and greening level.\n"
    },
    {
        "paper_id": 2110.11718,
        "authors": "Matteo Michielon and Asma Khedher and Peter Spreij",
        "title": "Liquidity-free implied volatilities: an approach using conic finance",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S2424786321500419",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of calculating risk-neutral implied volatilities of\nEuropean options without relying on option mid prices but solely on bid and ask\nprices. We provide an approach, based on the conic finance paradigm, that\nallows to uniquely strip risk-neutral implied volatilities from bid and ask\nquotes, and that does not require restrictive assumptions. Our methodology also\nallows to jointly calculate the implied liquidity of the market. The idea\noutlined in this paper can be applied to calculate other implied parameters\nfrom bid and ask security prices as soon as their theoretical risk-neutral\ncounterparts are strictly increasing with respect to the former.\n"
    },
    {
        "paper_id": 2110.11751,
        "authors": "Douglas Castilho, Tharsis T. P. Souza, Soong Moon Kang, Jo\\~ao Gama\n  and Andr\\'e C. P. L. F. de Carvalho",
        "title": "Forecasting Financial Market Structure from Network Features using\n  Machine Learning",
        "comments": "22 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a model that forecasts market correlation structure from link- and\nnode-based financial network features using machine learning. For such, market\nstructure is modeled as a dynamic asset network by quantifying time-dependent\nco-movement of asset price returns across company constituents of major global\nmarket indices. We provide empirical evidence using three different network\nfiltering methods to estimate market structure, namely Dynamic Asset Graph\n(DAG), Dynamic Minimal Spanning Tree (DMST) and Dynamic Threshold Networks\n(DTN). Experimental results show that the proposed model can forecast market\nstructure with high predictive performance with up to $40\\%$ improvement over a\ntime-invariant correlation-based benchmark. Non-pair-wise correlation features\nshowed to be important compared to traditionally used pair-wise correlation\nmeasures for all markets studied, particularly in the long-term forecasting of\nstock market structure. Evidence is provided for stock constituents of the\nDAX30, EUROSTOXX50, FTSE100, HANGSENG50, NASDAQ100 and NIFTY50 market indices.\nFindings can be useful to improve portfolio selection and risk management\nmethods, which commonly rely on a backward-looking covariance matrix to\nestimate portfolio risk.\n"
    },
    {
        "paper_id": 2110.11848,
        "authors": "Blanka Horvath, Zacharia Issa, Aitor Muguruza",
        "title": "Clustering Market Regimes using the Wasserstein Distance",
        "comments": "37 pages, 40 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The problem of rapid and automated detection of distinct market regimes is a\ntopic of great interest to financial mathematicians and practitioners alike. In\nthis paper, we outline an unsupervised learning algorithm for clustering\nfinancial time-series into a suitable number of temporal segments (market\nregimes). As a special case of the above, we develop a robust algorithm that\nautomates the process of classifying market regimes. The method is robust in\nthe sense that it does not depend on modelling assumptions of the underlying\ntime series as our experiments with real datasets show. This method -- dubbed\nthe Wasserstein $k$-means algorithm -- frames such a problem as one on the\nspace of probability measures with finite $p^\\text{th}$ moment, in terms of the\n$p$-Wasserstein distance between (empirical) distributions. We compare our\nWK-means approach with a more traditional clustering algorithms by studying the\nso-called maximum mean discrepancy scores between, and within clusters. In both\ncases it is shown that the WK-means algorithm vastly outperforms all considered\ncompetitor approaches. We demonstrate the performance of all approaches both in\na controlled environment on synthetic data, and on real data.\n"
    },
    {
        "paper_id": 2110.11999,
        "authors": "Jaydip Sen, Rajdeep Sen, Abhishek Dutta",
        "title": "Machine Learning in Finance-Emerging Trends and Challenges",
        "comments": "The chapter is 12 pages long and will appear as the introductory\n  chapter in the book titled \"Machine Learning: Algorithms, Models, and\n  Applications\" edited by Jaydip Sen, published by IntechOpen Publishers,\n  London, UK in November 2021. It will be published in open-access mode",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paradigm of machine learning and artificial intelligence has pervaded our\neveryday life in such a way that it is no longer an area for esoteric academics\nand scientists putting their effort to solve a challenging research problem.\nThe evolution is quite natural rather than accidental. With the exponential\ngrowth in processing speed and with the emergence of smarter algorithms for\nsolving complex and challenging problems, organizations have found it possible\nto harness a humongous volume of data in realizing solutions that have\nfar-reaching business values. This introductory chapter highlights some of the\nchallenges and barriers that organizations in the financial services sector at\nthe present encounter in adopting machine learning and artificial\nintelligence-based models and applications in their day-to-day operations.\n"
    },
    {
        "paper_id": 2110.12,
        "authors": "Maria Begicheva, Alexey Zaytsev",
        "title": "Bank transactions embeddings help to uncover current macroeconomics",
        "comments": null,
        "journal-ref": "ICMLA 2021",
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Macroeconomic indexes are of high importance for banks: many risk-control\ndecisions utilize these indexes. A typical workflow of these indexes evaluation\nis costly and protracted, with a lag between the actual date and available\nindex being a couple of months. Banks predict such indexes now using\nautoregressive models to make decisions in a rapidly changing environment.\nHowever, autoregressive models fail in complex scenarios related to appearances\nof crises.\n  We propose to use clients' financial transactions data from a large Russian\nbank to get such indexes. Financial transactions are long, and a number of\nclients is huge, so we develop an efficient approach that allows fast and\naccurate estimation of macroeconomic indexes based on a stream of transactions\nconsisting of millions of transactions. The approach uses a neural networks\nparadigm and a smart sampling scheme.\n  The results show that our neural network approach outperforms the baseline\nmethod on hand-crafted features based on transactions. Calculated embeddings\nshow the correlation between the client's transaction activity and bank\nmacroeconomic indexes over time.\n"
    },
    {
        "paper_id": 2110.12001,
        "authors": "Yorgos Protonotarios and Pantelis Tassopoulos",
        "title": "Brownian Motion & The Stochastic Behaviour of Stocks",
        "comments": "13 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We begin by exploring the intuition of Brownian motion by explaining its\nbirth through the observations of Robert Brown and later through Bachelier's\nwork on its applications to the financial market and finally its rigorous and\nconcretized form proposed by Norbert Wiener. The aforementioned motivates a\nstochastic differential equation to model the future price fluctuations of a\nstock traded wherein It\\^o integration is prominent and consequently expanded\nupon. The final part of this paper focuses on the accuracy of the model by\nbacktesting it with Apple stock and deriving a correlation coefficient.\n"
    },
    {
        "paper_id": 2110.12003,
        "authors": "Shareefuddin Mohammed, Rusty Bealer, Jason Cohen",
        "title": "Embracing advanced AI/ML to help investors achieve success: Vanguard\n  Reinforcement Learning for Financial Goal Planning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the world of advice and financial planning, there is seldom one right\nanswer. While traditional algorithms have been successful in solving linear\nproblems, its success often depends on choosing the right features from a\ndataset, which can be a challenge for nuanced financial planning scenarios.\nReinforcement learning is a machine learning approach that can be employed with\ncomplex data sets where picking the right features can be nearly impossible. In\nthis paper, we will explore the use of machine learning for financial\nforecasting, predicting economic indicators, and creating a savings strategy.\nVanguard ML algorithm for goals-based financial planning is based on deep\nreinforcement learning that identifies optimal savings rates across multiple\ngoals and sources of income to help clients achieve financial success. Vanguard\nlearning algorithms are trained to identify market indicators and behaviors too\ncomplex to capture with formulas and rules, instead, it works to model the\nfinancial success trajectory of investors and their investment outcomes as a\nMarkov decision process. We believe that reinforcement learning can be used to\ncreate value for advisors and end-investors, creating efficiency, more\npersonalized plans, and data to enable customized solutions.\n"
    },
    {
        "paper_id": 2110.12013,
        "authors": "George Georgiadis, Youngsoo Kim, H. Dharma Kwon",
        "title": "The Absence of Attrition in a War of Attrition under Complete\n  Information",
        "comments": "36 pages",
        "journal-ref": "Games and Economic Behavior, Volume 131, January 2022, Pages\n  171-185",
        "doi": "10.1016/j.geb.2021.11.004",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider a two-player game of war of attrition under complete information.\nIt is well-known that this class of games admits equilibria in pure, as well as\nmixed strategies, and much of the literature has focused on the latter. We show\nthat if the players' payoffs whilst in \"war\" vary stochastically and their exit\npayoffs are heterogeneous, then the game admits Markov Perfect equilibria in\npure strategies only. This is true irrespective of the degree of randomness and\nheterogeneity, thus highlighting the fragility of mixed-strategy equilibria to\na natural perturbation of the canonical model. In contrast, when the players'\nflow payoffs are deterministic or their exit payoffs are homogeneous, the game\nadmits equilibria in pure and mixed strategies.\n"
    },
    {
        "paper_id": 2110.1205,
        "authors": "Jeffrey P. Cohen, Felix L. Friedt, Jackson P. Lautier",
        "title": "The Impact of the Coronavirus Pandemic on New York City Real Estate:\n  First Evidence",
        "comments": "38 pages, 5 tables, 3 figures, Revised 1/27/2022",
        "journal-ref": null,
        "doi": "10.1111/jors.12591",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate whether pandemic-induced contagion disamenities and income\neffects arising due to COVID-related unemployment adversely affected real\nestate prices of one- or two-family owner-occupied properties across New York\nCity (NYC). First, OLS hedonic results indicate that greater COVID case numbers\nare concentrated in neighborhoods with lower-valued properties. Second, we use\na repeat-sales approach for the period 2003 to 2020, and we find that both the\npossibility of contagion and pandemic-induced income effects adversely impacted\nhome sale prices. Estimates suggest sale prices fell by roughly $60,000 or\naround 8% in response to both of the following: 1,000 additional infections per\n100,000 residents; and a 10-percentage point increase in unemployment in a\ngiven Modified Zip Code Tabulation Area (MODZCTA). These price effects were\nmore pronounced during the second wave of infections. Based on cumulative\nMODZCTA infection rates through 2020, the estimated COVID-19 price discount\nranged from approximately 1% to 50% in the most affected neighborhoods, and\naveraged 14%. The contagion effect intensified in the more affluent, but less\ndensely populated NYC neighborhoods, while the income effect was more\npronounced in the most densely populated neighborhoods with more rental\nproperties and greater population shares of foreign-born residents. This\ndisparity implies the pandemic may have been correlated with a wider gap in\nhousing wealth in NYC between homeowners in lower-priced and higher-priced\nneighborhoods.\n"
    },
    {
        "paper_id": 2110.12085,
        "authors": "Anna Gunnthorsdottir and Palmar Thorsteinsson",
        "title": "Reciprocity or community: Different cultural pathways to cooperation and\n  welfare",
        "comments": "36 pages, Center for the Philosophy of Freedom WP",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a laboratory experiment we compare voluntary cooperation in Iceland and\nthe US. We furthermore compare the associated thought processes across\ncultures. The two countries have similar economic performance, but survey\nmeasures show that they differ culturally. Our hypotheses are based on two such\nmeasures, The Inglehart cultural world map and the Knack and Keefers scale of\ncivic attitudes toward large-scale societal functioning. We prime the\nparticipants with different social foci, emphasizing in one a narrow grouping\nand in the other a larger social unit. In each country we implement this using\ntwo different feedback treatments. Under group feedback, participants only know\nthe contributions by the four members of their directly cooperating group.\nUnder session feedback they are informed of the contributions within their\ngroup as well as by everyone else in the session. Under group feedback,\ncooperation levels do not differ between the two cultures. However, under\nsession feedback cooperation levels increase in Iceland and decline in the US.\nEven when contribution levels are the same members of the two cultures differ\nin their motives to cooperate: Icelanders tend to cooperate unconditionally and\nUS subjects conditionally. Our findings indicate that different cultures can\nachieve similar economic and societal performance through different cultural\nnorms and suggest that cooperation should be encouraged through culturally\ntailored suasion tactics. We also find that some decision factors such as\nInequity Aversion do not differ across the two countries, which raises the\nquestion whether they are human universals.\n"
    },
    {
        "paper_id": 2110.12198,
        "authors": "Xia Han, Qiuqi Wang, Ruodu Wang and Jianming Xia",
        "title": "Cash-subadditive risk measures without quasi-convexity",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In the literature of risk measures, cash subadditivity was proposed to\nreplace cash additivity, motivated by the presence of stochastic or ambiguous\ninterest rates and defaultable contingent claims. Cash subadditivity has been\ntraditionally studied together with quasi-convexity, in a way similar to cash\nadditivity with convexity. In this paper, we study cash-subadditive risk\nmeasures without quasi-convexity. One of our major results is that a general\ncash-subadditive risk measure can be represented as the lower envelope of a\nfamily of quasi-convex and cash-subadditive risk measures. Representation\nresults of cash-subadditive risk measures with some additional properties are\nalso examined. The notion of quasi-star-shapedness, which is a natural analogue\nof star-shapedness, is introduced and we obtain a corresponding representation\nresult.\n"
    },
    {
        "paper_id": 2110.12282,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Francesco Cesarone, Mustafa \\c{C}elebi\n  P{\\i}nar, Jacopo Maria Ricci",
        "title": "MAD Risk Parity Portfolios",
        "comments": "39 pages, 8 tables, 2 figures",
        "journal-ref": null,
        "doi": "10.1007/s10479-023-05797-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the features and the performance of the Risk\nParity (RP) portfolios using the Mean Absolute Deviation (MAD) as a risk\nmeasure. The RP model is a recent strategy for asset allocation that aims at\nequally sharing the global portfolio risk among all the assets of an investment\nuniverse. We discuss here some existing and new results about the properties of\nMAD that are useful for the RP approach. We propose several formulations for\nfinding MAD-RP portfolios computationally, and compare them in terms of\naccuracy and efficiency. Furthermore, we provide extensive empirical analysis\nbased on three real-world datasets, showing that the performances of the RP\napproaches generally tend to place both in terms of risk and profitability\nbetween those obtained from the minimum risk and the Equally Weighted\nstrategies.\n"
    },
    {
        "paper_id": 2110.12394,
        "authors": "Jingwen Tan (1), Shixi Kang (1) ((1) School of Economics, Henan\n  University)",
        "title": "Housing property rights and social integration of migrant population:\n  based on the 2017 china migrants' dynamic survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Push-pull theory, one of the most important macro theories in demography,\nargues that population migration is driven by a combination of push (repulsive)\nforces at the place of emigration and pull (attractive) forces at the place of\nemigration. Based on the push-pull theory, this paper shows another practical\nperspective of the theory by measuring the reverse push and pull forces from\nthe perspective of housing property rights. We use OLS and sequential Probit\nmodels to analyze the impact of urban and rural property rights factors on the\nsocial integration of the migrant population-based, on \"China Migrants' Dynamic\nSurvey\". We found that after controlling for personal and urban\ncharacteristics, there is a significant negative effect of rural property\nrights (homestead) ownership of the mobile population on their socio-economic\nintegration, and cultural and psychological integration in the inflow area. The\neffect of urban house price on social integration of the migrant population is\nconsistent with the \"inverted U-shaped\" nonlinear assumption: when the house\nprice to income ratio of the migrant population in the inflow area increases\nbeyond the inflection point, its social integration level decreases. That is,\nthere is an inverse push force and pull force mechanism of housing property\nrights on population mobility.\n"
    },
    {
        "paper_id": 2110.12568,
        "authors": "Michael Macgregor Perry",
        "title": "Analyzing a Complex Game for the South China Sea Fishing Dispute using\n  Response Surface Methodologies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The South China Sea (SCS) is one of the most economically valuable resources\non the planet, and as such has become a source of territorial disputes between\nits bordering nations. Among other things, states compete to harvest the\nmultitude of fish species in the SCS. In an effort to gain a competitive\nadvantage states have turned to increased maritime patrols, as well as the use\nof \"maritime militias,\" which are fishermen armed with martial assets to resist\nthe influence of patrols. This conflict suggests a game of strategic resource\nallocation where states allocate patrols intelligently to earn the greatest\npossible utility. The game, however, is quite computationally challenging when\nconsidering its size (there are several distinct fisheries in the SCS), the\nnonlinear nature of biomass growth, and the influence of patrol allocations on\ncosts imposed on fishermen. Further, uncertainty in player behavior attributed\nto modeling error requires a robust analysis to fully capture the dispute's\ndynamics. To model such a complex scenario, this paper employs a response\nsurface methodology to assess optimal patrolling strategies and their impact on\nrealized utilities. The methodology developed successfully finds strategies\nwhich are more robust to behavioral uncertainty than a more straight-forward\nmethod.\n"
    },
    {
        "paper_id": 2110.12572,
        "authors": "Michael Macgregor Perry, Hadi El-Amine",
        "title": "Computational Efficiency in Multivariate Adversarial Risk Analysis\n  Models",
        "comments": null,
        "journal-ref": "Decision Analysis 16.4 (2019): 314-332",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we address the computational feasibility of the class of\ndecision theoretic models referred to as adversarial risk analyses (ARA). These\nare models where a decision must be made with consideration for how an\nintelligent adversary may behave and where the decision-making process of the\nadversary is unknown, and is elicited by analyzing the adversary's decision\nproblem using priors on his utility function and beliefs. The motivation of\nthis research was to develop a computational algorithm that can be applied\nacross a broad range of ARA models; to the best of our knowledge, no such\nalgorithm currently exists. Using a two-person sequential model, we\nincrementally increase the size of the model and develop a simulation-based\napproximation of the true optimum where an exact solution is computationally\nimpractical. In particular, we begin with a relatively large decision space by\nconsidering a theoretically continuous space that must be discretized. Then, we\nincrementally increase the number of strategic objectives which causes the\ndecision space to grow exponentially. The problem is exacerbated by the\npresence of an intelligent adversary who also must solve an exponentially large\ndecision problem according to some unknown decision-making process.\nNevertheless, using a stylized example that can be solved analytically we show\nthat our algorithm not only solves large ARA models quickly but also accurately\nselects to the true optimal solution. Furthermore, the algorithm is\nsufficiently general that it can be applied to any ARA model with a large, yet\nfinite, decision space.\n"
    },
    {
        "paper_id": 2110.12853,
        "authors": "Qi Feng and Jianfeng Zhang",
        "title": "Cubature Method for Stochastic Volterra Integral Equations",
        "comments": "Published version",
        "journal-ref": "SIAM J. Financial Mathematics.2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce the cubature formula for Stochastic Volterra\nIntegral Equations. We first derive the stochastic Taylor expansion in this\nsetting, by utilizing a functional It\\^{o} formula, and provide its tail\nestimates. We then introduce the cubature measure for such equations, and\nconstruct it explicitly in some special cases, including a long memory\nstochastic volatility model. We shall provide the error estimate rigorously.\nOur numerical examples show that the cubature method is much more efficient\nthan the Euler scheme, provided certain conditions are satisfied.\n"
    },
    {
        "paper_id": 2110.13021,
        "authors": "Andrea Maran, Andrea Pallavicini",
        "title": "Interpolating commodity futures prices with Kriging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The shape of the futures term structure is essential to commodity hedgers and\nspeculators as futures prices serve as a forecast of future spot prices.\nCommodity markets quotes futures prices on a selection of maturities and\ndelivery periods. In this note, we investigate a Bayesian technique known as\nKriging to build a term structure of futures prices by embedding trends and\nseasonalities and by taking into account bid-ask spreads of market quotations\non different delivery periods.\n"
    },
    {
        "paper_id": 2110.13287,
        "authors": "Andrea Coletta, Matteo Prata, Michele Conti, Emanuele Mercanti,\n  Novella Bartolini, Aymeric Moulin, Svitlana Vyetrenko, Tucker Balch",
        "title": "Towards Realistic Market Simulations: a Generative Adversarial Networks\n  Approach",
        "comments": "8 pages, 9 figures, ICAIF'21 - 2nd ACM International Conference on AI\n  in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Simulated environments are increasingly used by trading firms and investment\nbanks to evaluate trading strategies before approaching real markets.\nBacktesting, a widely used approach, consists of simulating experimental\nstrategies while replaying historical market scenarios. Unfortunately, this\napproach does not capture the market response to the experimental agents'\nactions. In contrast, multi-agent simulation presents a natural bottom-up\napproach to emulating agent interaction in financial markets. It allows to set\nup pools of traders with diverse strategies to mimic the financial market\ntrader population, and test the performance of new experimental strategies.\nSince individual agent-level historical data is typically proprietary and not\navailable for public use, it is difficult to calibrate multiple market agents\nto obtain the realism required for testing trading strategies. To addresses\nthis challenge we propose a synthetic market generator based on Conditional\nGenerative Adversarial Networks (CGANs) trained on real aggregate-level\nhistorical data. A CGAN-based \"world\" agent can generate meaningful orders in\nresponse to an experimental agent. We integrate our synthetic market generator\ninto ABIDES, an open source simulator of financial markets. By means of\nextensive simulations we show that our proposal outperforms previous work in\nterms of stylized facts reflecting market responsiveness and realism.\n"
    },
    {
        "paper_id": 2110.13296,
        "authors": "Asif Lakhany and Amber Zhang",
        "title": "Efficient ISDA Initial Margin Calculations Using Least Squares\n  Monte-Carlo",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Non-cleared bilateral OTC derivatives between two financial firms or\nsystemically important non-financial entities are subject to regulations that\nrequire the posting of initial and variation margin. The ISDA standard approach\n(SIMM) provides a way for computing the initial margin. It involves computing\nsensitivities of the contracts with respect to several market factors. In this\npaper, the authors extend the well known LSMC technique to efficiently estimate\nthe sensitivities required in the ISDA SIMM methodology.\n"
    },
    {
        "paper_id": 2110.13317,
        "authors": "Benjamin Meindl, Morgan R. Frank, Joana Mendon\\c{c}a",
        "title": "Exposure of occupations to technologies of the fourth industrial\n  revolution",
        "comments": "65 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fourth industrial revolution (4IR) is likely to have a substantial impact\non the economy. Companies need to build up capabilities to implement new\ntechnologies, and automation may make some occupations obsolete. However,\nwhere, when, and how the change will happen remain to be determined. Robust\nempirical indicators of technological progress linked to occupations can help\nto illuminate this change. With this aim, we provide such an indicator based on\npatent data. Using natural language processing, we calculate patent exposure\nscores for more than 900 occupations, which represent the technological\nprogress related to them. To provide a lens on the impact of the 4IR, we\ndifferentiate between traditional and 4IR patent exposure. Our method differs\nfrom previous approaches in that it both accounts for the diversity of\ntask-level patent exposures within an occupation and reflects work activities\nmore accurately. We find that exposure to 4IR patents differs from traditional\npatent exposure. Manual tasks, and accordingly occupations such as construction\nand production, are exposed mainly to traditional (non-4IR) patents but have\nlow exposure to 4IR patents. The analysis suggests that 4IR technologies may\nhave a negative impact on job growth; this impact appears 10 to 20 years after\npatent filing. Further, we compared the 4IR exposure to other automation and AI\nexposure scores. Whereas many measures refer to theoretical automation\npotential, our patent-based indicator reflects actual technology diffusion. Our\nwork not only allows analyses of the impact of 4IR technologies as a whole, but\nalso provides exposure scores for more than 300 technology fields, such as AI\nand smart office technologies. Finally, the work provides a general mapping of\npatents to tasks and occupations, which enables future researchers to construct\nindividual exposure measures.\n"
    },
    {
        "paper_id": 2110.13467,
        "authors": "Thomas Bernhardt and Ge Qu",
        "title": "Wealth heterogeneity in a closed pooled annuity fund",
        "comments": "31 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stability of income payments in a pooled annuity fund is studied. In\nthose funds, members receive a fluctuating income depending on their\nexperienced mortality in exchange for their pension savings. The focus is on\ndescribing the influence of different initial savings on the ability of the\nfund to provide a stable income in retirement. Because of this, members\ncoincide in their characteristics except for their initial savings. We identify\na term, which we dub ``implied number of homogeneous members'', that directly\nlinks the initial savings to the size of the income fluctuations. Our main\ncontribution is the analysis of this term and the development of a criterion to\nanswer the question of whether or not a given group of same-aged people should\npool their funds together.\n"
    },
    {
        "paper_id": 2110.13533,
        "authors": "Aetienne Sardon",
        "title": "Zero-Liquidation Loans: A Structured Product Approach to DeFi Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Zero-Liquidation loans allow DeFi users to borrow USDC against their ETH\nholdings, but without the risk of being liquidated in case of LTV shortfalls.\nThis is achieved by giving users the option to repay their loans, either in\nUSDC or through their previously pledged ETH (the concept can be generalized to\nother currency pairs as well). Liquidity providers, on the other hand side, are\ncompensated with a higher yield for bearing the ETH downside risk. A positive\nside effect of zero-liquidation loans is that they are more robust against\nflash crashes and have a lower financial contagion effect than current lending\nand borrowing protocols.\n"
    },
    {
        "paper_id": 2110.13678,
        "authors": "Yannick Limmer and Thilo Meyer-Brandis",
        "title": "Large Platonic Markets with Delays",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The objective is to develop a general stochastic approach to delays on\nfinancial markets. We suggest such a concept in the context of large platonic\nmarkets, which allow infinitely many assets and incorporate a restricted\ninformation setting. The discussion is divided into information delays and\norder execution delays. The former enables modelling of markets where the\nobserved information is delayed, while the latter provides the opportunity to\ndefer the indexed time of a received asset price. Both delays may be designed\nrandomly and inhomogeneously over time. We show that delayed markets are\nequipped with a fundamental theorem of asset pricing and our main result is\ninheritance of the no asymptotic Lp-free lunch condition under both delay\ntypes. Eventually, we suggest an approach to verify absence of Lp-free lunch on\nmarkets with multiple brokers endowed with deviating trading speeds.\n"
    },
    {
        "paper_id": 2110.13701,
        "authors": "Jeremy Turiel and Tomaso Aste",
        "title": "Heterogenous criticality in high frequency finance: a phase transition\n  in flash crashes",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.3390/e24020257",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Flash crashes in financial markets have become increasingly important\nattracting attention from financial regulators, market makers as well as from\nthe media and the broader audience. Systemic risk and propagation of shocks in\nfinancial markets is also a topic of great relevance that attracted increasing\nattention in recent years. In the present work we bridge the gap between these\ntwo topics with an in-depth investigation of the systemic risk structure of\nco-crashes in high frequency trading. We find that large co-crashes are\nsystemic in their nature and differ from small crashes. We demonstrate that\nthere is a phase transition between co-crashes of small and large sizes, where\nthe former involves mostly illiquid stocks while large and liquid stocks are\nthe most represented and central in the latter. This suggests that systemic\neffects and shock propagation might be triggered by simultaneous withdrawals or\nmovement of liquidity by HFTs, arbitrageurs and market makers with cross-asset\nexposures.\n"
    },
    {
        "paper_id": 2110.13716,
        "authors": "Wentao Xu, Weiqing Liu, Lewen Wang, Yingce Xia, Jiang Bian, Jian Yin,\n  Tie-Yan Liu",
        "title": "HIST: A Graph-based Framework for Stock Trend Forecasting via Mining\n  Concept-Oriented Shared Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock trend forecasting, which forecasts stock prices' future trends, plays\nan essential role in investment. The stocks in a market can share information\nso that their stock prices are highly correlated. Several methods were recently\nproposed to mine the shared information through stock concepts (e.g.,\ntechnology, Internet Retail) extracted from the Web to improve the forecasting\nresults. However, previous work assumes the connections between stocks and\nconcepts are stationary, and neglects the dynamic relevance between stocks and\nconcepts, limiting the forecasting results. Moreover, existing methods overlook\nthe invaluable shared information carried by hidden concepts, which measure\nstocks' commonness beyond the manually defined stock concepts. To overcome the\nshortcomings of previous work, we proposed a novel stock trend forecasting\nframework that can adequately mine the concept-oriented shared information from\npredefined concepts and hidden concepts. The proposed framework simultaneously\nutilize the stock's shared information and individual information to improve\nthe stock trend forecasting performance. Experimental results on the real-world\ntasks demonstrate the efficiency of our framework on stock trend forecasting.\nThe investment simulation shows that our framework can achieve a higher\ninvestment return than the baselines.\n"
    },
    {
        "paper_id": 2110.13718,
        "authors": "Jeremy D. Turiel and Tomaso Aste",
        "title": "Self-organised criticality in high frequency finance: the case of flash\n  crashes",
        "comments": "4 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the rise of computing and artificial intelligence, advanced modeling and\nforecasting has been applied to High Frequency markets. A crucial element of\nsolid production modeling though relies on the investigation of data\ndistributions and how they relate to modeling assumptions. In this work we\ninvestigate volume distributions during anomalous price events and show how\ntheir tail exponents < 2 indicate a diverging second moment of the\ndistribution, i.e. variance. We then tie the dynamics of flash crashes to\nself-organised criticality. The findings are of great relevance for regulators\nand market makers as they advocate for rigorous heavy-tailed modeling of risk\nand changes in regulation to avoid simultaneous liquidity withdrawals and hard\nrisk constraints which lead to synchronisation and critical events.\n"
    },
    {
        "paper_id": 2110.13814,
        "authors": "Shumpei Goke, Gabriel Y. Weintraub, Ralph Mastromonaco and Sam Seljan",
        "title": "Bidders' Responses to Auction Format Change in Internet Display\n  Advertising Auctions",
        "comments": "35 pages, 37 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study actual bidding behavior when a new auction format gets introduced\ninto the marketplace. More specifically, we investigate this question using a\nnovel dataset on internet display advertising auctions that exploits a\nstaggered adoption by different publishers (sellers) of first-price auctions\n(FPAs), instead of the traditional second-price auctions (SPAs). Event study\nregression estimates indicate that, immediately after the auction format\nchange, the revenue per sold impression (price) jumped considerably for the\ntreated publishers relative to the control publishers, ranging from 35% to 75%\nof the pre-treatment price level of the treatment group. Further, we observe\nthat in later auction format changes the increase in the price levels under\nFPAs relative to price levels under SPAs dissipates over time, reminiscent of\nthe celebrated revenue equivalence theorem. We take this as evidence of\ninitially insufficient bid shading after the format change rather than an\nimmediate shift to a new Bayesian Nash equilibrium. Prices then went down as\nbidders learned to shade their bids. We also show that bidders' sophistication\nimpacted their response to the auction format change. Our work constitutes one\nof the first field studies on bidders' responses to auction format changes,\nproviding an important complement to theoretical model predictions. As such, it\nprovides valuable information to auction designers when considering the\nimplementation of different formats.\n"
    },
    {
        "paper_id": 2110.13815,
        "authors": "P. Rovelli, C. Benedetti, A. Fronzetti Colladon, A. De Massis",
        "title": "As long as you talk about me: The importance of family firm brands and\n  the contingent role of family-firm identity",
        "comments": null,
        "journal-ref": "Journal of Business Research 139, 692-700 (2022)",
        "doi": "10.1016/j.jbusres.2021.09.075",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study explores the role of external audiences in determining the\nimportance of family firm brands and the relationship with firm performance.\nDrawing on text mining and social network analysis techniques, and considering\nthe brand prevalence, diversity, and connectivity dimensions, we use the\nsemantic brand score to measure the importance the media give to family firm\nbrands. The analysis of a sample of 52,555 news articles published in 2017\nabout 63 Italian entrepreneurial families reveals that brand importance is\npositively associated with family firm revenues, and this relationship is\nstronger when there is identity match between the family and the firm. This\nstudy advances current literature by offering a rich and multifaceted\nperspective on how external audiences perceptions of the brand shape family\nfirm performance.\n"
    },
    {
        "paper_id": 2110.13966,
        "authors": "Michael Macgregor Perry",
        "title": "Fisheries Management in Congested Waters: A Game-Theoretic Assessment of\n  the East China Sea",
        "comments": null,
        "journal-ref": "Environmental and Resource Economics (2022)",
        "doi": "10.1007/s10640-022-00688-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fisheries in the East China Sea (ECS) face multiple concerning trends. Aside\nfrom depleted stocks caused by overfishing, illegal encroachments by fishermen\nfrom one nation into another's legal waters are a common occurrence. This\nbehavior presumably could be stopped via strong monitoring, controls, and\nsurveillance (MCS), but MCS is routinely rated below standards for nations\nbordering the ECS. This paper generalizes the ECS to a model of a congested\nmaritime environment, defined as an environment where multiple nations can fish\nin the same waters with equivalent operating costs, and uses game-theoretic\nanalysis to explain why the observed behavior persists in the ECS. The paper\nfinds that nations in congested environments are incentivized to issue\nexcessive quotas, which in turn tacitly encourages illegal fishing and extracts\nillegal rent from another's legal waters. This behavior couldn't persist in the\nface of strong MCS measures, and states are thus likewise incentivized to use\npoor MCS. A bargaining problem is analyzed to complement the noncooperative\ngame, and a key finding is the nation with lower nonoperating costs has great\nleverage during the bargain.\n"
    },
    {
        "paper_id": 2110.14046,
        "authors": "David Itkin, Martin Larsson",
        "title": "Open Markets and Hybrid Jacobi Processes",
        "comments": "51 pages. To appear in the Annals of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a unified approach to several problems in Stochastic Portfolio\nTheory (SPT), which is a framework for equity markets with a large number $d$\nof stocks. Our approach combines open markets, where trading is confined to the\ntop $N$ capitalized stocks as well as the market portfolio consisting of all\n$d$ assets, with a parametric family of models which we call hybrid Jacobi\nprocesses. We provide a detailed analysis of ergodicity, particle collisions,\nand boundary attainment, and use these results to study the associated\nfinancial markets. Their properties include (1) stability of the capital\ndistribution curve and (2) unleveraged and explicit growth optimal strategies.\nThe sub-class of rank Jacobi models are additionally shown to (3) serve as the\nworst-case model for a robust asymptotic growth problem under model ambiguity\nand (4) exhibit stability in the large-$d$ limit. Our definition of an open\nmarket is a relaxation of existing definitions which is essential to make the\nanalysis tractable.\n"
    },
    {
        "paper_id": 2110.1429,
        "authors": "Neil M Davies, Jackie Grant, and Chin Yang Shapland",
        "title": "The USS Trustee's risky strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How much risk, and what types of risk, is the Universities Superannuation\nScheme (USS) taking? This is a critical question for universities across the UK\nand many of their employees. Will the fund have enough money to pay for all our\npensions? Will it run out? Or is there a significant risk that we are\ncollectively overpaying? In September 2021, David Miles and James Sefton, from\nImperial College Business School, stepped into this vacuum, publishing 'How\nmuch risk is the USS taking?'. The paper presents important, accessible and\nhighly readable analysis which estimates how likely the USS is to default over\ntime. Their work is particularly relevant to the current UCU dispute with 69\nemployers over the benefit cuts that Universities UK (UUK) is planning to\nimplement on the basis of the 2020 USS valuation. In this Brief, we assess the\nassumptions, replicate the results, explore further their model and consider\npotential extensions. We demonstrate that for a cautious model with reasonable\nassumptions for assets and asset growth, the fund has a less than 7% chance of\ndefaulting for the duration that pensions promises are due, but a greater than\n80% chance of being over funded by at least {\\pounds}100bn, and nearly 50%\nchance of having over {\\pounds}400bn. We offer warm thanks to David Miles and\nJames Sefton for sharing their code and data, for their helpful conversations\nand clarification. Their analysis is infinitely clearer, better and more\ncredible than anything the USS has produced. We hope this paper will be the\nbeginning of more work in this area. All errors are our own.\n"
    },
    {
        "paper_id": 2110.14317,
        "authors": "M. Eren Akbiyik, Mert Erkul, Killian Kaempf, Vaiva Vasiliauskaite,\n  Nino Antulov-Fantulin",
        "title": "Ask \"Who\", Not \"What\": Bitcoin Volatility Forecasting with Twitter Data",
        "comments": "11 pages, 11 figures, 6 tables. In Proceedings of the Sixteenth ACM\n  International Conference on Web Search and Data Mining (WSDM '23), February\n  27-March 3, 2023, Singapore, Singapore. ACM, New York, NY, USA",
        "journal-ref": null,
        "doi": "10.1145/3539597.3570387",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Understanding the variations in trading price (volatility), and its response\nto exogenous information, is a well-researched topic in finance. In this study,\nwe focus on finding stable and accurate volatility predictors for a relatively\nnew asset class of cryptocurrencies, in particular Bitcoin, using deep learning\nrepresentations of public social media data obtained from Twitter. For our\nexperiments, we extracted semantic information and user statistics from over 30\nmillion Bitcoin-related tweets, in conjunction with 15-minute frequency price\ndata over a horizon of 144 days. Using this data, we built several deep\nlearning architectures that utilized different combinations of the gathered\ninformation. For each model, we conducted ablation studies to assess the\ninfluence of different components and feature sets over the prediction\naccuracy. We found statistical evidences for the hypotheses that: (i) temporal\nconvolutional networks perform significantly better than both classical\nautoregressive models and other deep learning-based architectures in the\nliterature, and (ii) tweet author meta-information, even detached from the\ntweet itself, is a better predictor of volatility than the semantic content and\ntweet volume statistics. We demonstrate how different information sets gathered\nfrom social media can be utilized in different architectures and how they\naffect the prediction results. As an additional contribution, we make our\ndataset public for future research.\n"
    },
    {
        "paper_id": 2110.14405,
        "authors": "Atilla Aras",
        "title": "Solution to the Equity Premium Puzzle Using the Sufficiency Factor of\n  the Model",
        "comments": "36 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study provides the solution to the equity premium puzzle. The new model\nwas developed by including the behavior of investors toward risk in financial\nmarkets in prior studies. The calculations of this newly tested model show that\nthe value of the coefficient of relative risk aversion is 1.033526 by assuming\nthe value of the subjective time discount factor to be 0.99. Since these values\nare compatible with the existing empirical studies, they confirm the validity\nof the newly derived model that provides the solution to the equity premium\npuzzle.\n"
    },
    {
        "paper_id": 2110.14771,
        "authors": "Selim Amrouni, Aymeric Moulin, Jared Vann, Svitlana Vyetrenko, Tucker\n  Balch and Manuela Veloso",
        "title": "ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation\n  and Application to Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3490354.3494433",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Model-free Reinforcement Learning (RL) requires the ability to sample\ntrajectories by taking actions in the original problem environment or a\nsimulated version of it. Breakthroughs in the field of RL have been largely\nfacilitated by the development of dedicated open source simulators with easy to\nuse frameworks such as OpenAI Gym and its Atari environments. In this paper we\npropose to use the OpenAI Gym framework on discrete event time based Discrete\nEvent Multi-Agent Simulation (DEMAS). We introduce a general technique to wrap\na DEMAS simulator into the Gym framework. We expose the technique in detail and\nimplement it using the simulator ABIDES as a base. We apply this work by\nspecifically using the markets extension of ABIDES, ABIDES-Markets, and develop\ntwo benchmark financial markets OpenAI Gym environments for training daily\ninvestor and execution agents. As a result, these two environments describe\nclassic financial problems with a complex interactive market behavior response\nto the experimental agent's action.\n"
    },
    {
        "paper_id": 2110.14914,
        "authors": "Nestoras Chalkidis, Rahul Savani",
        "title": "Trading via Selective Classification",
        "comments": "(8 pages, 6 figures, 4 tables, ICAIF'21)",
        "journal-ref": null,
        "doi": "10.1145/3490354.3494379",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A binary classifier that tries to predict if the price of an asset will\nincrease or decrease naturally gives rise to a trading strategy that follows\nthe prediction and thus always has a position in the market. Selective\nclassification extends a binary or many-class classifier to allow it to abstain\nfrom making a prediction for certain inputs, thereby allowing a trade-off\nbetween the accuracy of the resulting selective classifier against coverage of\nthe input feature space. Selective classifiers give rise to trading strategies\nthat do not take a trading position when the classifier abstains. We\ninvestigate the application of binary and ternary selective classification to\ntrading strategy design. For ternary classification, in addition to classes for\nthe price going up or down, we include a third class that corresponds to\nrelatively small price moves in either direction, and gives the classifier\nanother way to avoid making a directional prediction. We use a walk-forward\ntrain-validate-test approach to evaluate and compare binary and ternary,\nselective and non-selective classifiers across several different feature sets\nbased on four classification approaches: logistic regression, random forests,\nfeed-forward, and recurrent neural networks. We then turn these classifiers\ninto trading strategies for which we perform backtests on commodity futures\nmarkets. Our empirical results demonstrate the potential of selective\nclassification for trading.\n"
    },
    {
        "paper_id": 2110.14938,
        "authors": "Liqun Liu",
        "title": "Domestic Constraints in Crisis Bargaining",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I study how political bias and audience costs impose domestic institutional\nconstraints that affect states' capacity to reach peaceful agreements during\ncrises. With a mechanism design approach, I show that the existence of peaceful\nagreements hinges crucially on whether the resource being divided can appease\ntwo sides of the highest type (i.e. the maximum war capacity). The derivation\nhas two major implications. On the one hand, if war must be averted, then\npolitical leaders are not incentivized by audience costs to communicate private\ninformation; they will pool on the strategy that induces the maximum bargaining\ngains. On the other hand, political bias matters for the scope of peace because\nit alters a state's expected war payoff.\n"
    },
    {
        "paper_id": 2110.15025,
        "authors": "Anindya Goswami, Nimit Rana and Tak Kuen Siu",
        "title": "Regime Switching Optimal Growth Model with Risk Sensitive Preferences",
        "comments": "27 pages, 2 figures. Key words: Regime switching models, Growth\n  models, Risk sensitive Preferences, Optimal consumption, Euler equation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a risk-sensitive optimization of consumption-utility on infinite\ntime horizon where the one-period investment gain depends on an underlying\neconomic state whose evolution over time is assumed to be described by a\ndiscrete-time, finite-state, Markov chain. We suppose that the production\nfunction also depends on a sequence of i.i.d. random shocks. For the sake of\ngenerality, the utility and the production functions are allowed to be\nunbounded from above. Under the Markov regime-switching model, it is shown that\nthe value function of optimization problem satisfies an optimality equation and\nthat the optimality equation has a unique solution in a particular class of\nfunctions. Furthermore, we show that an optimal policy exists in the class of\nstationary policies. We also derive the Euler equation of optimal consumption.\nFurthermore, the existence of the unique joint stationary distribution of the\noptimal growth process and the underlying regime process is examined. Finally,\nwe present a numerical solution by considering power utility and some\nhypothetical values of parameters in a regime switching extension of\nCobb-Douglas production rate function.\n"
    },
    {
        "paper_id": 2110.15102,
        "authors": "Siyi Wang, Xing Yan, Bangqi Zheng, Hu Wang, Wangli Xu, Nanbo Peng, Qi\n  Wu",
        "title": "Risk and return prediction for pricing portfolios of non-performing\n  consumer credit",
        "comments": "Accepted by 2nd ACM International Conference on AI in Finance\n  (ICAIF'21)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We design a system for risk-analyzing and pricing portfolios of\nnon-performing consumer credit loans. The rapid development of credit lending\nbusiness for consumers heightens the need for trading portfolios formed by\noverdue loans as a manner of risk transferring. However, the problem is\nnontrivial technically and related research is absent. We tackle the challenge\nby building a bottom-up architecture, in which we model the distribution of\nevery single loan's repayment rate, followed by modeling the distribution of\nthe portfolio's overall repayment rate. To address the technical issues\nencountered, we adopt the approaches of simultaneous quantile regression,\nR-copula, and Gaussian one-factor copula model. To our best knowledge, this is\nthe first study that successfully adopts a bottom-up system for analyzing\ncredit portfolio risks of consumer loans. We conduct experiments on a vast\namount of data and prove that our methodology can be applied successfully in\nreal business tasks.\n"
    },
    {
        "paper_id": 2110.15133,
        "authors": "Mohamed Ben Alaya and Ahmed Kebaier and Djibril Sarr",
        "title": "Deep Calibration of Interest Rates Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For any financial institution it is a necessity to be able to apprehend the\nbehavior of interest rates. Despite the use of Deep Learning that is growing\nvery fastly, due to many reasons (expertise, ease of use, ...) classic rates\nmodels such as CIR, or the Gaussian family are still being used widely. We\npropose to calibrate the five parameters of the G2++ model using Neural\nNetworks. To achieve that, we construct synthetic data sets of parameters drawn\nuniformly from a reference set of parameters calibrated from the market. From\nthose parameters, we compute Zero-Coupon and Forward rates and their\ncovariances and correlations. Our first model is a Fully Connected Neural\nnetwork and uses only covariances and correlations. We show that covariances\nare more suited to the problem than correlations. The second model is a\nConvulutional Neural Network using only Zero-Coupon rates with no\ntransformation. The methods we propose perform very quickly (less than 0.3\nseconds for 2 000 calibrations) and have low errors and good fitting.\n"
    },
    {
        "paper_id": 2110.15229,
        "authors": "Martin Dumav",
        "title": "Moral Hazard, Dynamic Incentives, and Ambiguous Perceptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers dynamic moral hazard settings, in which the consequences\nof the agent's actions are not precisely understood. In a new continuous-time\nmoral hazard model with drift ambiguity, the agent's unobservable action\ntranslates to drift set that describe the evolution of output. The agent and\nthe principal have imprecise information about the technology, and both seek\nrobust performance from a contract in relation to their respective worst-case\nscenarios. We show that the optimal long-term contract aligns the parties'\npessimistic expectations and broadly features compressing of the high-powered\nincentives. Methodologically, we provide a tractable way to formulate and\ncharacterize optimal long-run contracts with drift ambiguity. Substantively,\nour results provide some insights into the formal link between robustness and\nsimplicity of dynamic contracts, in particular high-powered incentives become\nless effective in the presence of ambiguity.\n"
    },
    {
        "paper_id": 2110.15239,
        "authors": "Michael Isichenko",
        "title": "Costly Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We revisit optimal execution of an active portfolio in the presence of\nslippage (aka linear, proportional, or absolute-value) costs. Market efficiency\nimplies a close balance between active alphas and trading costs, so even small\nchanges to trading optimization can make a big difference. It has been observed\nfor some time that optimal trading involves a pattern of a no-trade zone with\nwidth $\\Delta$ increasing with slippage cost parameter $c$. In a setting of a\nreasonably stable (non-stochastic) forecast of future returns and a quadratic\nrisk aversion, it is shown that $\\Delta\\sim c^{1/2}$, which differs from the\n$\\Delta\\sim c^{1/3}$ scaling reported for stochastic settings. Analysis of\noptimal trading employs maximization of a utility including projected\nalpha-based profits, slippage costs, and risk aversion and borrows from a\nphysical analogy of forced motion in the presence of friction.\n"
    },
    {
        "paper_id": 2110.1531,
        "authors": "Talia Gillis, Bryce McLaughlin, Jann Spiess",
        "title": "On the Fairness of Machine-Assisted Human Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When machine-learning algorithms are used in high-stakes decisions, we want\nto ensure that their deployment leads to fair and equitable outcomes. This\nconcern has motivated a fast-growing literature that focuses on diagnosing and\naddressing disparities in machine predictions. However, many machine\npredictions are deployed to assist in decisions where a human decision-maker\nretains the ultimate decision authority. In this article, we therefore consider\nin a formal model and in a lab experiment how properties of machine predictions\naffect the resulting human decisions. In our formal model of statistical\ndecision-making, we show that the inclusion of a biased human decision-maker\ncan revert common relationships between the structure of the algorithm and the\nqualities of resulting decisions. Specifically, we document that excluding\ninformation about protected groups from the prediction may fail to reduce, and\nmay even increase, ultimate disparities. In the lab experiment, we demonstrate\nhow predictions informed by gender-specific information can reduce average\ngender disparities in decisions. While our concrete theoretical results rely on\nspecific assumptions about the data, algorithm, and decision-maker, and the\nexperiment focuses on a particular prediction task, our findings show more\nbroadly that any study of critical properties of complex decision systems, such\nas the fairness of machine-assisted human decisions, should go beyond focusing\non the underlying algorithmic predictions in isolation.\n"
    },
    {
        "paper_id": 2110.15312,
        "authors": "Upasak Das, Karan Singhal",
        "title": "Solving it correctly Prevalence and Persistence of Gender Gap in Basic\n  Mathematics in rural India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Mathematical ability is among the most important determinants of prospering\nin the labour market. Using multiple representative datasets with learning\noutcomes of over 2 million children from rural India in the age group 8 to 16\nyears, the paper examines the prevalence of gender gap in performance in\nmathematics and its temporal variation from 2010 to 2018. Our findings from the\nregressions show significant gender gap in mathematics, which is not observable\nfor reading skills. This difference in mathematics scores remains prevalent\nacross households of different socio-economic and demographic groups. This gap\nis found to be persistent over time and it appears to increase as the children\nget older. We also find significant inter-state variation with the north Indian\nstates lagging behind considerably and the south Indian states showing a\nreverse gender gap. As an explanation to this, we observe evidence of a robust\nassociation between pre-existing gender norms at the household and district\nlevel with higher gender gap. The findings, in light of other available\nevidence on the consequences of such gaps, call for the need to understand\nthese gender specific differences more granularly and periodically to inform\ngender-specific interventions.\n"
    },
    {
        "paper_id": 2110.1575,
        "authors": "Chinmay Ghoroi, Jay Shah, Devanshu Thakar, Sakshi Baheti",
        "title": "Process Design and Economics of Production of p-Aminophenol",
        "comments": "23 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Para-Aminophenol is one of the key chemicals required for the synthesis of\nParacetamol, an analgesic and antipyretic drug. Data shows a large fraction of\nIndia's demand for Para-Aminophenol being met through imports from China. The\nuncertainty in the India-China relations would affect the supply and price of\nthis \"Key Starting Material.\" This report is a detailed business plan for\nsetting up a plant and producing Para-Aminophenol in India at a competitive\nprice. The plant is simulated in AspenPlus V8 and different Material Balances\nand Energy Balances calculations are carried out. The plant produces 22.7 kmols\nPara-Aminophenol per hour with a purity of 99.9%. Along with the simulation,\neconomic analysis is carried out for this plant to determine the financial\nparameters like Payback Period and Return on Investment.\n"
    },
    {
        "paper_id": 2111.00274,
        "authors": "Chenyu Zhao, Misha van Beek, Peter Spreij, Makhtar Ba",
        "title": "Polynomial Approximation of Discounted Moments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an approximation strategy for the discounted moments of a\nstochastic process that can, for a large class of problems, approximate the\ntrue moments. These moments appear in pricing formulas of financial products\nsuch as bonds and credit derivatives. The approximation relies on high-order\npower series expansion of the infinitesimal generator, and draws parallels with\nthe theory of polynomial processes. We demonstrate applications to bond pricing\nand credit derivatives. In the special cases that allow for an analytical\nsolution the approximation error decreases to around 10 to 100 times machine\nprecision for higher orders. When no analytical solution exists we tie out the\napproximation with Monte Carlo simulations.\n"
    },
    {
        "paper_id": 2111.00319,
        "authors": "Plamen Nikolov, Leila Salarpour, David Titus",
        "title": "Skill Downgrading Among Refugees and Economic Immigrants in Germany",
        "comments": "Keywords: downgrading, immigrants, refugees, Germany, labor markets,\n  wages, employment",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Upon arrival to a new country, many immigrants face job downgrading, a\nphenomenon describing workers being in jobs below the ones they have based on\nthe skills they possess. Moreover, in the presence of downgrading immigrants\nreceiving lower wage returns to the same skills compared to natives. The level\nof downgrading could depend on the immigrant type and numerous other factors.\nThis study examines the determinants of skill downgrading among two types of\nimmigrants - refugees and economic immigrants - in the German labor markets\nbetween 1984 and 2018. We find that refugees downgrade more than economic\nimmigrants, and this discrepancy between the two groups persists over time. We\nshow that language skill improvements exert a strong influence on subsequent\nlabor market outcomes of both groups.\n"
    },
    {
        "paper_id": 2111.00348,
        "authors": "Marc Geha and Antoine Jacquier and Zan Zuric",
        "title": "Large and moderate deviations for importance sampling in the Heston\n  model",
        "comments": "32 pages, 12 Figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a detailed importance sampling analysis for variance reduction in\nstochastic volatility models. The optimal change of measure is obtained using a\nvariety of results from large and moderate deviations: small-time, large-time,\nsmall-noise. Specialising the results to the Heston model, we derive many\nclosed-form solutions, making the whole approach easy to implement. We support\nour theoretical results with a detailed numerical analysis of the variance\nreduction gains.\n"
    },
    {
        "paper_id": 2111.00451,
        "authors": "Yan Dolinsky and Shir Moshe",
        "title": "Utility Indifference Pricing with High Risk Aversion and Small Linear\n  Price Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the Bachelier model with linear price impact. Exponential utility\nindifference prices are studied for vanilla European options and we compute\ntheir non-trivial scaling limit for a vanishing price impact which is inversely\nproportional to the risk aversion. Moreover, we find explicitly a family of\nportfolios which are asymptotically optimal.\n"
    },
    {
        "paper_id": 2111.00522,
        "authors": "Liqun Liu",
        "title": "The Politics of (No) Compromise: Information Acquisition, Policy\n  Discretion, and Reputation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Precise information is essential for making good policies, especially those\nregarding reform decisions. However, decision-makers may hesitate to gather\nsuch information if certain decisions could have negative impacts on their\nfuture careers. We model how decision-makers with career concerns may acquire\npolicy-relevant information and carry out reform decisions when their policy\ndiscretion can be limited ex ante. Typically, decision-makers with career\nconcerns have weaker incentives to acquire information compared to\ndecision-makers without such concerns. In this context, we demonstrate that the\npublic can encourage information acquisition by eliminating either the\n\"moderate policy\" or the status quo from decision-makers' discretion. We also\nanalyze when reform decisions should be strategically delegated to\ndecision-makers with or without career concerns.\n"
    },
    {
        "paper_id": 2111.00526,
        "authors": "Asier Guti\\'errez-Fandi\\~no, Miquel Noguer i Alonso, Petter Kolm,\n  Jordi Armengol-Estap\\'e",
        "title": "FinEAS: Financial Embedding Analysis of Sentiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new language representation model in finance called Financial\nEmbedding Analysis of Sentiment (FinEAS). In financial markets, news and\ninvestor sentiment are significant drivers of security prices. Thus, leveraging\nthe capabilities of modern NLP approaches for financial sentiment analysis is a\ncrucial component in identifying patterns and trends that are useful for market\nparticipants and regulators. In recent years, methods that use transfer\nlearning from large Transformer-based language models like BERT, have achieved\nstate-of-the-art results in text classification tasks, including sentiment\nanalysis using labelled datasets. Researchers have quickly adopted these\napproaches to financial texts, but best practices in this domain are not\nwell-established. In this work, we propose a new model for financial sentiment\nanalysis based on supervised fine-tuned sentence embeddings from a standard\nBERT model. We demonstrate our approach achieves significant improvements in\ncomparison to vanilla BERT, LSTM, and FinBERT, a financial domain specific\nBERT.\n"
    },
    {
        "paper_id": 2111.00529,
        "authors": "Moritz Jirak",
        "title": "Edgeworth expansions for volatility models",
        "comments": "minor extension, some typos removed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated from option and derivative pricing, this note develops Edgeworth\nexpansions both in the Kolmogorov and Wasserstein metric for many different\ntypes of discrete time volatility models and their possible transformations.\nThis includes, among others, H\\\"{o}lder-type functions of (augmented) Garch\nprocesses of any order, iterated random functions or Volterra-processes.\n"
    },
    {
        "paper_id": 2111.00566,
        "authors": "Kazem Biabany Khameneh, Reza Najarzadeh, Hassan Dargahi, Lotfali\n  Agheli",
        "title": "The Role of Global Value Chains in Carbon Intensity Convergence: A\n  Spatial Econometrics Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The expansion of trade agreements has provided a potential basis for trade\nintegration and economic convergence of different countries. Moreover,\ndeveloping and expanding global value chains (GVCs) have provided more\nopportunities for knowledge and technology spillovers and the potential\nconvergence of production techniques. This can result in conceivable\nenvironmental outcomes in developed and developing countries. This study\ninvestigates whether GVCs can become a basis for the carbon intensity (CI)\nconvergence of different countries. To answer this question, data from 101\ncountries from 1997 to 2014 are analyzed using spatial panel data econometrics.\nThe results indicate a spatial correlation between GVCs trade partners in terms\nof CI growth, and they confirm the GVCs-based conditional CI convergence of the\ncountries. Moreover, estimates indicate that expanding GVCs even stimulates\nbridging the CI gap between countries, i.e., directly and indirectly through\nspillover effects. According to the results, GVCs have the potential capacity\nto improve the effectiveness of carbon efficiency policies. Therefore,\ndifferent dimensions of GVCs and their benefits should be taken into account\nwhen devising environmental policies.\n"
    },
    {
        "paper_id": 2111.00835,
        "authors": "Pavel V. Shevchenko, Daisuke Murakami, Tomoko Matsui, Tor A. Myrvoll",
        "title": "Impact of COVID-19 type events on the economy and climate under the\n  stochastic DICE model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The classical DICE model is a widely accepted integrated assessment model for\nthe joint modeling of economic and climate systems, where all model state\nvariables evolve over time deterministically. We reformulate and solve the DICE\nmodel as an optimal control dynamic programming problem with six state\nvariables (related to the carbon concentration, temperature, and economic\ncapital) evolving over time deterministically and affected by two controls\n(carbon emission mitigation rate and consumption). We then extend the model by\nadding a discrete stochastic shock variable to model the economy in the\nstressed and normal regimes as a jump process caused by events such as the\nCOVID-19 pandemic. These shocks reduce the world gross output leading to a\nreduction in both the world net output and carbon emission. The extended model\nis solved under several scenarios as an optimal stochastic control problem,\nassuming that the shock events occur randomly on average once every 100 years\nand last for 5 years. The results show that, if the world gross output recovers\nin full after each event, the impact of the COVID-19 events on the temperature\nand carbon concentration will be immaterial even in the case of a conservative\n10\\% drop in the annual gross output over a 5-year period. The impact becomes\nnoticeable, although still extremely small (long-term temperature drops by\n$0.1^\\circ \\mathrm{C}$), in a presence of persistent shocks of a 5\\% output\ndrop propagating to the subsequent time periods through the recursively reduced\nproductivity. If the deterministic DICE model policy is applied in a presence\nof stochastic shocks (i.e. when this policy is suboptimal), then the drop in\ntemperature is larger (approximately $0.25^\\circ \\mathrm{C}$), that is, the\nlower economic activities owing to shocks imply that more ambitious mitigation\ntargets are now feasible at lower costs.\n"
    },
    {
        "paper_id": 2111.00987,
        "authors": "Alexander Kell",
        "title": "Modelling the transition to a low-carbon energy supply",
        "comments": "PhD thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A transition to a low-carbon electricity supply is crucial to limit the\nimpacts of climate change. Reducing carbon emissions could help prevent the\nworld from reaching a tipping point, where runaway emissions are likely.\nRunaway emissions could lead to extremes in weather conditions around the world\n-- especially in problematic regions unable to cope with these conditions.\nHowever, the movement to a low-carbon energy supply can not happen\ninstantaneously due to the existing fossil-fuel infrastructure and the\nrequirement to maintain a reliable energy supply. Therefore, a low-carbon\ntransition is required, however, the decisions various stakeholders should make\nover the coming decades to reduce these carbon emissions are not obvious. This\nis due to many long-term uncertainties, such as electricity, fuel and\ngeneration costs, human behaviour and the size of electricity demand. A well\nchoreographed low-carbon transition is, therefore, required between all of the\nheterogenous actors in the system, as opposed to changing the behaviour of a\nsingle, centralised actor. The objective of this thesis is to create a novel,\nopen-source agent-based model to better understand the manner in which the\nwhole electricity market reacts to different factors using state-of-the-art\nmachine learning and artificial intelligence methods. In contrast to other\nworks, this thesis looks at both the long-term and short-term impact that\ndifferent behaviours have on the electricity market by using these\nstate-of-the-art methods.\n"
    },
    {
        "paper_id": 2111.00992,
        "authors": "David Karpa, Torben Klarl, Michael Rochlitz",
        "title": "Artificial Intelligence, Surveillance, and Big Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The most important resource to improve technologies in the field of\nartificial intelligence is data. Two types of policies are crucial in this\nrespect: privacy and data-sharing regulations, and the use of surveillance\ntechnologies for policing. Both types of policies vary substantially across\ncountries and political regimes. In this chapter, we examine how authoritarian\nand democratic political institutions can influence the quality of research in\nartificial intelligence, and the availability of large-scale datasets to\nimprove and train deep learning algorithms. We focus mainly on the Chinese\ncase, and find that -- ceteris paribus -- authoritarian political institutions\ncontinue to have a negative effect on innovation. They can, however, have a\npositive effect on research in deep learning, via the availability of\nlarge-scale datasets that have been obtained through government surveillance.\nWe propose a research agenda to study which of the two effects might dominate\nin a race for leadership in artificial intelligence between countries with\ndifferent political institutions, such as the United States and China.\n"
    },
    {
        "paper_id": 2111.01038,
        "authors": "Anthony Enisan Akinlo, Segun Michael Ojo",
        "title": "Economic consequences of covid-19 pandemic to the sub-Saharan Africa: an\n  historical perspective",
        "comments": "23 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the economic consequences of the COVID-19 pandemic to\nsub-Saharan Africa (SSA) using the historical approach by analyzing the policy\nresponses of the region to past crises and their economic consequences. The\nstudy employs the manufacturing-value-added share of GDP as a performance\nindicator. The analysis shows that wrong policy intervention to past crises,\nlead the African sub-region into the deplorable economic situation. The study\nobserved that the region leapfrogged prematurely to import substitution, export\npromotion, and global value chains. Based on these past experiences, the region\nshould adopt a gradual approach in responding to the COVID-19 economic\nconsequences. The sub-region should first address relevant areas of\nsustainability, including proactive investment in research and development to\ndevelop home-grown technology, upgrade essential infrastructural facilities,\ndevelop security infrastructures, and strengthen the financial sector.\n"
    },
    {
        "paper_id": 2111.01137,
        "authors": "Ananda Chatterjee, Hrisav Bhowmick, and Jaydip Sen",
        "title": "Stock Price Prediction Using Time Series, Econometric, Machine Learning,\n  and Deep Learning Models",
        "comments": "This is the accepted version of our paper in the international\n  conference, IEEE Mysurucon'21, which was organized in Hassan, Karnataka,\n  India from October 24, 2021 to October 25, 2021. The paper is 8 pages long,\n  and it contains 20 figures and 22 tables. This is the preprint of the\n  conference paper",
        "journal-ref": "Proc. of IEEE Mysore Sub Section International Conference\n  (MysuruCon), October 24-25, 2021, pp. 289-296, Hassan, Karnataka, India",
        "doi": "10.1109/MysuruCon52639.2021.9641610",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a long-time, researchers have been developing a reliable and accurate\npredictive model for stock price prediction. According to the literature, if\npredictive models are correctly designed and refined, they can painstakingly\nand faithfully estimate future stock values. This paper demonstrates a set of\ntime series, econometric, and various learning-based models for stock price\nprediction. The data of Infosys, ICICI, and SUN PHARMA from the period of\nJanuary 2004 to December 2019 was used here for training and testing the models\nto know which model performs best in which sector. One time series model\n(Holt-Winters Exponential Smoothing), one econometric model (ARIMA), two\nmachine Learning models (Random Forest and MARS), and two deep learning-based\nmodels (simple RNN and LSTM) have been included in this paper. MARS has been\nproved to be the best performing machine learning model, while LSTM has proved\nto be the best performing deep learning model. But overall, for all three\nsectors - IT (on Infosys data), Banking (on ICICI data), and Health (on SUN\nPHARMA data), MARS has proved to be the best performing model in sales\nforecasting.\n"
    },
    {
        "paper_id": 2111.01234,
        "authors": "F. Habib, H. Huang, A. Mauskopf, B. Nikolic, T.S. Salisbury",
        "title": "Optimal allocation to deferred income annuities",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics 90 (2020), pp. 94-104",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we employ a lifecycle model that uses utility of consumption\nand bequest to determine an optimal Deferred Income Annuity (DIA) purchase\npolicy. We lay out a mathematical framework to formalize the optimization\nprocess. The method and implementation of the optimization is explained, and\nthe results are then analyzed. We extend our model to control for asset\nallocation and show how the purchase policy changes when one is allowed to vary\nasset allocation. Our results indicate that (i.) refundable DIAs are less\nappealing than non-refundable DIAs because of the loss of mortality credits;\n(ii.) the DIA allocation region is larger under the fixed asset allocation\nstrategy due to it becoming a proxy for fixed-income allocation; and (iii.)\nwhen the investor is allowed to change asset-allocation, DIA allocation becomes\nless appealing. However, a case for higher DIA allocation can be made for those\nindividuals who perceive their longevity to be higher than the population.\n"
    },
    {
        "paper_id": 2111.01239,
        "authors": "Moshe A. Milevsky and Thomas S. Salisbury",
        "title": "Refundable income annuities: Feasibility of money-back guarantees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Refundable income annuities (IA), such as cash-refund and instalment-refund,\ndiffer in material ways from the life-only version beloved by economists. In\naddition to lifetime income they guarantee the annuitant or beneficiary will\nreceive their money back albeit slowly over time. We document that refundable\nIAs now represent the majority of sales in the U.S., yet they are mostly\nignored by insurance and pension economists. And, although their pricing,\nduration, and money's-worth-ratio is complicated by recursivity which will be\nexplained, we offer a path forward to make refundable IAs tractable.\n  A key result concerns the market price of cash-refund IAs, when the actuarial\npresent value is grossed-up by an insurance loading. We prove that price is\ncounterintuitively no longer a declining function of age and older buyers might\npay more than younger ones. Moreover, there exists a threshold valuation rate\nbelow which no price is viable. This may also explain why inflation-adjusted\nIAs have all but disappeared.\n"
    },
    {
        "paper_id": 2111.01248,
        "authors": "Mikhail Freer and Marco Castillo",
        "title": "A General Revealed Preference Test for Quasilinear Preferences: Theory\n  and Experiments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a generalized revealed preference test for quasilinear\npreferences. The test applies to nonlinear budget sets and non-convex\npreferences as those found in taxation and nonlinear pricing contexts. We study\nthe prevalence of quasilinear preferences in a laboratory real-effort task\nexperiment with nonlinear wages. The experiment demonstrates the empirical\nrelevance of our test. We find support for either convex (non-separable)\npreferences or quasilinear preferences but weak support for the hypothesis of\nboth quasilinear and convex preferences.\n"
    },
    {
        "paper_id": 2111.01529,
        "authors": "Bernardo D'Auria and Jos\\'e A. Salmer\\'on",
        "title": "Anticipative information in a Brownian-Poissonmarket: the binary\n  information",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The binary information collects all those events that may or may not occur.\nWith this kind of variables, a large amount of information can be captured, in\nparticular, about financial assets and their future trends. In our paper, we\nassume the existence of some anticipative information of this type in a market\nwhose risky asset dynamics evolve according to a Brownian motion and a Poisson\nprocess. Using Malliavin calculus and filtration enlargement techniques, we\ncompute the semimartingale decomposition of the mentioned processes and, in the\npure jump case, we give the exact value of the information. Many examples are\nshown, where the anticipative information is related to some conditions that\nthe constituent processes or their running maximum may or may not verify.\n"
    },
    {
        "paper_id": 2111.01598,
        "authors": "Hanwoong Kim, Haewon McJeon, Dawoon Jung, Hanju Lee, Candelaria\n  Bergero, Jiyong Eom",
        "title": "Integrated Assessment Modeling of Korea 2050 Carbon Neutrality\n  Technology Pathways",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This integrated assessment modeling research analyzes what Korea's 2050\ncarbon neutrality would require for the national energy system and the role of\nthe power sector concerning the availability of critical mitigation\ntechnologies. Our scenario-based assessments show that Korea's current policy\nfalls short of what the nation's carbon-neutrality ambition would require.\nAcross all technology scenarios examined in this study, extensive and rapid\nenergy system transition is imperative, requiring the large-scale deployment of\nrenewables and carbon capture & storage (CCS) early on and negative emission\ntechnologies (NETs) by the mid-century. Importantly, rapid decarbonization of\nthe power sector that goes with rapid electrification of end-uses seems to be a\nrobust national decarbonization strategy. Furthermore, we contextualize our\nnet-zero scenario results using policy costs, requirements for natural\nresources, and the expansion rate of zero-carbon technologies. We find that the\navailability of nuclear power lowers the required expansion rate of renewables\nand CCS, alleviating any stress on terrestrial and geological systems. By\ncontrast, the limited availability of CCS without nuclear power necessarily\ndemands a very high penetration of renewables and significantly high policy\ncompliance costs, which would decrease the feasibility of achieving the carbon\nneutrality target.\n"
    },
    {
        "paper_id": 2111.01678,
        "authors": "Konrad Menzel",
        "title": "Central Limit Theory for Models of Strategic Network Formation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide asymptotic approximations to the distribution of statistics that\nare obtained from network data for limiting sequences that let the number of\nnodes (agents) in the network grow large. Network formation is permitted to be\nstrategic in that agents' incentives for link formation may depend on the ego\nand alter's positions in that endogenous network. Our framework does not limit\nthe strength of these interaction effects, but assumes that the network is\nsparse. We show that the model can be approximated by a sampling experiment in\nwhich subnetworks are generated independently from a common equilibrium\ndistribution, and any dependence across subnetworks is captured by state\nvariables at the level of the entire network. Under many-player asymptotics,\nthe leading term of the approximation error to the limiting model established\nin Menzel (2015b) is shown to be Gaussian, with an asymptotic bias and variance\nthat can be estimated consistently from a single network.\n"
    },
    {
        "paper_id": 2111.01762,
        "authors": "Francisco Estrada, Oscar Calder\\'on-Bustamante, Wouter Botzen,\n  Juli\\'an A. Velasco and Richard S.J. Tol",
        "title": "AIRCC-Clim: a user-friendly tool for generating regional probabilistic\n  climate change scenarios and risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex physical models are the most advanced tools available for producing\nrealistic simulations of the climate system. However, such levels of realism\nimply high computational cost and restrictions on their use for policymaking\nand risk assessment. Two central characteristics of climate change are\nuncertainty and that it is a dynamic problem in which international actions can\nsignificantly alter climate projections and information needs, including\npartial and full compliance of global climate goals. Here we present\nAIRCC-Clim, a simple climate model emulator that produces regional\nprobabilistic climate change projections of monthly and annual temperature and\nprecipitation, as well as risk measures, based both on standard and\nuser-defined emissions scenarios for six greenhouse gases. AIRCC-Clim emulates\n37 atmosphere-ocean coupled general circulation models with low computational\nand technical requirements for the user. This standalone, user-friendly\nsoftware is designed for a variety of applications including impact\nassessments, climate policy evaluation and integrated assessment modelling.\n"
    },
    {
        "paper_id": 2111.01778,
        "authors": "Angela E. Kilby and Charlie Denhart",
        "title": "Location inference on social media data for agile monitoring of public\n  health crises: An application to opioid use and abuse during the Covid-19\n  pandemic",
        "comments": "10 pages, 7 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The Covid-19 pandemic has intersected with the opioid epidemic to create a\nunique public health crisis, with the health and economic consequences of the\nvirus and associated lockdowns compounding pre-existing social and economic\nstressors associated with rising opioid and heroin use and abuse. In order to\nbetter understand these interlocking crises, we use social media data to\nextract qualitative and quantitative insights on the experiences of opioid\nusers during the Covid-19 pandemic. In particular, we use an unsupervised\nlearning approach to create a rich geolocated data source for public health\nsurveillance and analysis. To do this we first infer the location of 26,000\nReddit users that participate in opiate-related sub-communities (subreddits) by\ncombining named entity recognition, geocoding, density-based clustering, and\nheuristic methods. Our strategy achieves 63 percent accuracy at state-level\nlocation inference on a manually-annotated reference dataset. We then leverage\nthe geospatial nature of our user cohort to answer policy-relevant questions\nabout the impact of varying state-level policy approaches that balance economic\nversus health concerns during Covid-19. We find that state government\nstrategies that prioritized economic reopening over curtailing the spread of\nthe virus created a markedly different environment and outcomes for opioid\nusers. Our results demonstrate that geospatial social media data can be used\nfor agile monitoring of complex public health crises.\n"
    },
    {
        "paper_id": 2111.01783,
        "authors": "Christa Cuchiero, Christoph Reisinger, Stefan Rigger",
        "title": "Optimal bailout strategies resulting from the drift controlled\n  supercooled Stefan problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem faced by a central bank which bails out distressed\nfinancial institutions that pose systemic risk to the banking sector. In a\nstructural default model with mutual obligations, the central agent seeks to\ninject a minimum amount of cash in order to limit defaults to a given\nproportion of entities. We prove that the value of the central agent's control\nproblem converges as the number of defaultable institutions goes to infinity,\nand that it satisfies a drift controlled version of the supercooled Stefan\nproblem. We compute optimal strategies in feedback form by solving numerically\na regularized version of the corresponding mean field control problem using a\npolicy gradient method. Our simulations show that the central agent's optimal\nstrategy is to subsidise banks whose equity values lie in a non-trivial\ntime-dependent region.\n"
    },
    {
        "paper_id": 2111.01874,
        "authors": "Christian Bayer, Chiheb Ben Hammouda, Ra\\'ul Tempone",
        "title": "Numerical Smoothing with Hierarchical Adaptive Sparse Grids and\n  Quasi-Monte Carlo Methods for Efficient Option Pricing",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2003.05708",
        "journal-ref": null,
        "doi": "10.1080/14697688.2022.2135455",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When approximating the expectations of a functional of a solution to a\nstochastic differential equation, the numerical performance of deterministic\nquadrature methods, such as sparse grid quadrature and quasi-Monte Carlo (QMC)\nmethods, may critically depend on the regularity of the integrand. To overcome\nthis issue and improve the regularity structure of the problem, we consider\ncases in which analytic smoothing (bias-free mollification) cannot be performed\nand introduce a novel numerical smoothing approach by combining a root-finding\nmethod with a one-dimensional numerical integration with respect to a single\nwell-chosen variable. We prove that, under appropriate conditions, the\nresulting function of the remaining variables is highly smooth, potentially\naffording the improved efficiency of adaptive sparse grid quadrature (ASGQ) and\nQMC methods, particularly when combined with hierarchical transformations (ie.,\nthe Brownian bridge and Richardson extrapolation on the weak error). This\napproach facilitates the effective treatment of high dimensionality. Our study\nis motivated by option pricing problems, focusing on dynamics where the\ndiscretization of the asset price is necessary. Based on our analysis and\nnumerical experiments, we demonstrate the advantages of combining numerical\nsmoothing with the ASGQ and QMC methods over these methods without smoothing\nand the Monte Carlo approach. Finally, our approach is generic and can be\napplied to solve a broad class of problems, particularly approximating\ndistribution functions, computing financial Greeks, and estimating risk\nquantities.\n"
    },
    {
        "paper_id": 2111.01911,
        "authors": "Simerjot Kaur, Ivan Brugere, Andrea Stefanucci, Armineh Nourbakhsh,\n  Sameena Shah, Manuela Veloso",
        "title": "Parameterized Explanations for Investor / Company Matching",
        "comments": "8 pages, 7 figures, 4 tables, 2 algorithms",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Matching companies and investors is usually considered a highly specialized\ndecision making process. Building an AI agent that can automate such\nrecommendation process can significantly help reduce costs, and eliminate human\nbiases and errors. However, limited sample size of financial data-sets and the\nneed for not only good recommendations, but also explaining why a particular\nrecommendation is being made, makes this a challenging problem. In this work we\npropose a representation learning based recommendation engine that works\nextremely well with small datasets and demonstrate how it can be coupled with a\nparameterized explanation generation engine to build an explainable\nrecommendation system for investor-company matching. We compare the performance\nof our system with human generated recommendations and demonstrate the ability\nof our algorithm to perform extremely well on this task. We also highlight how\nexplainability helps with real-life adoption of our system.\n"
    },
    {
        "paper_id": 2111.01931,
        "authors": "Xiaofei Shi, Daran Xu, Zhanhao Zhang",
        "title": "Deep Learning Algorithms for Hedging with Frictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work studies the deep learning-based numerical algorithms for optimal\nhedging problems in markets with general convex transaction costs on the\ntrading rates, focusing on their scalability of trading time horizon. Based on\nthe comparison results of the FBSDE solver by Han, Jentzen, and E (2018) and\nthe Deep Hedging algorithm by Buehler, Gonon, Teichmann, and Wood (2019), we\npropose a Stable Transfer Hedging (ST-Hedging) algorithm, to aggregate the\nconvenience of the leading-order approximation formulas and the accuracy of the\ndeep learning-based algorithms. Our ST-Hedging algorithm achieves the same\nstate-of-the-art performance in short and moderately long time horizon as FBSDE\nsolver and Deep Hedging, and generalize well to long time horizon when previous\nalgorithms become suboptimal. With the transfer learning technique, ST-Hedging\ndrastically reduce the training time, and shows great scalability to\nhigh-dimensional settings. This opens up new possibilities in model-based deep\nlearning algorithms in economics, finance, and operational research, which\ntakes advantages of the domain expert knowledge and the accuracy of the\nlearning-based methods.\n"
    },
    {
        "paper_id": 2111.01957,
        "authors": "Shreya Bose and Ibrahim Ekren",
        "title": "Multidimensional Kyle-Back model with a risk averse informed trader",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the continuous time Kyle-Back model with a risk averse informed\ntrader.We show that in a market with multiple assets and non-Gaussian prices an\nequilibrium exists. The equilibrium is constructed by considering a\nFokker-Planck equation and a system of partial differential equations that are\ncoupled with an optimal transport type constraint at maturity.\n"
    },
    {
        "paper_id": 2111.02067,
        "authors": "Edgardo Brigatti and Estevan Augusto Amazonas Mendes",
        "title": "Testing macroecological theories in cryptocurrency market: neutral\n  models can not describe diversity patterns and their variation",
        "comments": "18 pages, 8 figures",
        "journal-ref": "R. Soc. Open Sci. 9,212005 (2022)",
        "doi": "10.1098/rsos.212005",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop an analysis of the cryptocurrency market borrowing methods and\nconcepts from ecology. This approach makes it possible to identify specific\ndiversity patterns and their variation, in close analogy with ecological\nsystems, and to characterize the cryptocurrency market in an effective way. At\nthe same time, it shows how non-biological systems can have an important role\nin contrasting different ecological theories and in testing the use of neutral\nmodels. The study of the cryptocurrencies abundance distribution and the\nevolution of the community structure strongly indicates that these statistical\npatterns are not consistent with neutrality. In particular, the necessity to\nincrease the temporal change in community composition when the number of\ncryptocurrencies grows, suggests that their interactions are not necessarily\nweak. The analysis of the intraspecific and interspecific interdependency\nsupports this fact and demonstrates the presence of a market sector influenced\nby mutualistic relations. These latest findings challenge the hypothesis of\nweakly interacting symmetric species, the postulate at the heart of neutral\nmodels.\n"
    },
    {
        "paper_id": 2111.02112,
        "authors": "Charlotte Liotta, Vincent Vigui\\'e, Quentin Lepetit",
        "title": "Testing the monocentric standard urban model in a global sample of\n  cities",
        "comments": null,
        "journal-ref": "Regional Science and Urban Economics, 2022",
        "doi": "10.1016/j.regsciurbeco.2022.103832",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a unique dataset containing gridded data on population densities,\nrents, housing sizes, and transportation in 192 cities worldwide, we\ninvestigate the empirical relevance of the monocentric standard urban model\n(SUM). Overall, the SUM seems surprisingly capable of capturing the inner\nstructure of cities, both in developed and developing countries. As expected,\ncities spread out when they are richer, more populated, and when transportation\nor farmland is cheaper. Respectively 100% and 87% of the cities exhibit the\nexpected negative density and rent gradients: on average, a 1% decrease in\nincome net of transportation costs leads to a 21% decrease in densities and a\n3% decrease in rents per m2. We also investigate the heterogeneity between\ncities of different characteristics in terms of monocentricity, informality,\nand amenities.\n"
    },
    {
        "paper_id": 2111.02328,
        "authors": "Anibal Sanjab, Yuting Mou, Ana Virag, Kris Kessels",
        "title": "A Linear Model for Distributed Flexibility Markets and DLMPs: A\n  Comparison with the SOCP Formulation",
        "comments": "CIRED'21",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the performance trade-offs between an introduced linear\nflexibility market model for congestion management and a benchmark second-order\ncone programming (SOCP) formulation. The linear market model incorporates\nvoltage magnitudes and reactive powers, while providing a simpler formulation\nthan the SOCP model, which enables its practical implementation. The paper\nprovides a structured comparison of the two formulations relying on developed\ndeterministic and statistical Monte Carlo case analyses using two distribution\ntest systems (the Matpower 69-bus and 141-bus systems). The case analyses show\nthat with the increasing spread of offered flexibility throughout the system,\nthe linear formulation increasingly preserves the reliability of the computed\nsystem variables as compared to the SOCP formulation, while more lenient\nimposed voltage limits can improve the approximation of prices and power flows\nat the expense of a less accurate computation of voltage magnitudes.\n"
    },
    {
        "paper_id": 2111.02554,
        "authors": "David Hobson, Gechun Liang, Edward Wang",
        "title": "Callable convertible bonds under liquidity constraints and hybrid\n  priorities",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the callable convertible bond problem in the presence\nof a liquidity constraint modelled by Poisson signals. We assume that neither\nthe bondholder nor the firm has absolute priority when they stop the game\nsimultaneously, but instead, a proportion $m\\in[0,1]$ of the bond is converted\nto the firm's stock and the rest is called by the firm. The paper thus\ngeneralizes the special case studied in [Liang and Sun, Dynkin games with\nPoisson random intervention times, SIAM Journal on Control and Optimization, 57\n(2019), 2962-2991] where the bondholder has priority ($m=1$), and presents a\ncomplete solution to the callable convertible bond problem with liquidity\nconstraint. The callable convertible bond is an example of a Dynkin game, but\nfalls outside the standard paradigm since the payoffs do not depend in an\nordered way upon which agent stops the game. We show how to deal with this\nnon-ordered situation by introducing a new technique which may be of interest\nin its own right, and then apply it to the bond problem.\n"
    },
    {
        "paper_id": 2111.02633,
        "authors": "Xiufeng Yan, Qi Tang",
        "title": "Network analysis regarding international trade network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the effect of globalization of world economy between 1980 and 2010\nby using network analysis technics on trade and GDP data of 71 countries in the\nworld. We draw results distinguishing relatively developing and relatively\ndeveloped countries during this period of time and point out the standing out\neconomies among the BRICS countries during the years of globalization: within\nour context of study, China and Russia are the countries that already exhibit\ndeveloped economy characters, India is next in line but have some unusual\nfeatures, while Brazil and South Africa still have erratic behaviors\n"
    },
    {
        "paper_id": 2111.02834,
        "authors": "T. N. Li and A. Tourin",
        "title": "Optimal Pairs Trading with Time-Varying Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a pairs trading model that incorporates a time-varying volatility\nof the Constant Elasticity of Variance type. Our approach is based on\nstochastic control techniques; given a fixed time horizon and a portfolio of\ntwo co-integrated assets, we define the trading strategies as the portfolio\nweights maximizing the expected power utility from terminal wealth. We compute\nthe optimal pairs strategies by using a Finite Difference method. Finally, we\nillustrate our results by conducting tests on historical market data at daily\nfrequency. The parameters are estimated by the Generalized Method of Moments.\n"
    },
    {
        "paper_id": 2111.0285,
        "authors": "Bashir Ahmed Bhuiyan, Mohammad Shahansha Molla, and Masud Alam",
        "title": "Managing Innovation in Technical Education: Revisiting the Developmental\n  Strategies of Politeknik Brunei",
        "comments": "14",
        "journal-ref": "Annals of Contemporary Developments in Management & HR\n  (ACDMHR),pp. 44-57, Vol. 3, No. 4, 1st November 2021",
        "doi": "10.33166/ACDMHR.2021.04.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present study aims at exploring the strategies for managing innovation in\ntechnical education by using blended learning philosophy and practices with\nspecial reference to Politeknik Brunei. Based on literature review and desk\nresearch, the study found out salient characteristics, explored constraining\nfactors, elicited strategies of Politeknik Brunei, and suggested some options\nand a framework for innovations management and development of effective blended\nteaching and learning. The limiting factors identified are the unwillingness of\nthe top-level management, lack of structural support, dearth of readiness of\nthe stakeholders, the gap between teacher's expectations and changed students\ncharacteristics, and blended teaching myopia on the way of effective\napplication of blended learning strategies. Notable suggestions for strategic\ndevelopment are developing wide-angle vision and self-renewal processes,\nanalyzing the environment for needs determination. Clarity of purpose and\ntasks, technological adaptability, data-driven decision making, prompt\nfeedback, flipped classroom, and development of learning clusters are other\ndimensions that may go a long way toward innovating teaching-learning and the\noverall development of an academic institution. Finally, the study suggested\nimportant guidelines for applying the strategies and proposed framework for\nquality blended learning and managing innovations in technical education.\n"
    },
    {
        "paper_id": 2111.02872,
        "authors": "Minwoo Hyun, Aleh Cherp, Jessica Jewell, Yeong Jae Kim, Jiyong Eom",
        "title": "Feasibility trade-offs in decarbonisation of power sector with high coal\n  dependence: A case of Korea",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Decarbonisation of the power sector requires feasible strategies for rapid\nphase-out of fossil fuels and expansion of low-carbon sources. This study\ndevelops and uses a model with an explicit account of power plant stocks to\nexplore plausible decarbonization scenarios of the power sector in the Republic\nof Korea through 2050 and 2060. The results show that achieving zero emissions\nfrom the power sector by the mid-century requires either ambitious expansion of\nrenewables backed by gas-fired generation equipped with carbon capture and\nstorage or significant expansion of nuclear power. The first strategy implies\nreplicating and maintaining for decades maximum growth rates of solar power\nachieved in leading countries and becoming an early and ambitious adopter of\nthe CCS technology. The alternative expansion of nuclear power has historical\nprecedents in Korea and other countries but may not be acceptable in the\ncurrent political and regulatory environment.\n"
    },
    {
        "paper_id": 2111.03366,
        "authors": "Matteo Malavasi (1), Gareth W. Peters (2,1), Pavel V. Shevchenko (1),\n  Stefan Tr\\\"uck (1), Jiwook Jang (1), Georgy Sofronov (3) ((1) Department of\n  Actuarial Studies and Business Analytics, Macquarie University, Australia (2)\n  Department of Statistics and Applied Probability, University of California\n  Santa Barbara, USA (3) Department of Mathematics and Statistics, Macquarie\n  University, Australia)",
        "title": "Cyber Risk Frequency, Severity and Insurance Viability",
        "comments": "42 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study an exploration of insurance risk transfer is undertaken for the\ncyber insurance industry in the United States of America, based on the leading\nindustry dataset of cyber events provided by Advisen. We seek to address two\ncore unresolved questions. First, what factors are the most significant\ncovariates that may explain the frequency and severity of cyber loss events and\nare they heterogeneous over cyber risk categories? Second, is cyber risk\ninsurable in regards to the required premiums, risk pool sizes and how would\nthis decision vary with the insured companies industry sector and size? We\naddress these questions through a combination of regression models based on the\nclass of Generalised Additive Models for Location Shape and Scale (GAMLSS) and\na class of ordinal regressions. These models will then form the basis for our\nanalysis of frequency and severity of cyber risk loss processes. We investigate\nthe viability of insurance for cyber risk using a utility modelling framework\nwith premium calculated by classical certainty equivalence analysis utilising\nthe developed regression models. Our results provide several new key insights\ninto the nature of insurability of cyber risk and rigorously address the two\ninsurance questions posed in a real data driven case study analysis.\n"
    },
    {
        "paper_id": 2111.03477,
        "authors": "Jie Chen, Lingfei Li",
        "title": "Data-driven Hedging of Stock Index Options via Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop deep learning models to learn the hedge ratio for S&P500 index\noptions directly from options data. We compare different combinations of\nfeatures and show that a feedforward neural network model with time to\nmaturity, Black-Scholes delta and a sentiment variable (VIX for calls and index\nreturn for puts) as input features performs the best in the out-of-sample test.\nThis model significantly outperforms the standard hedging practice that uses\nthe Black-Scholes delta and a recent data-driven model. Our results demonstrate\nthe importance of market sentiment for hedging efficiency, a factor previously\nignored in developing hedging strategies.\n"
    },
    {
        "paper_id": 2111.03603,
        "authors": "Marcos Escobar-Anel, Yevhen Havrylenko, Michel Kschonnek, Rudi Zagst",
        "title": "Decrease of capital guarantees in life insurance products: can\n  reinsurance stop it?",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We analyze the potential of reinsurance for reversing the current trend of\ndecreasing capital guarantees in life insurance products. Providing an insurer\nwith an opportunity to shift part of the financial risk to a reinsurer, we\nsolve the insurer's dynamic investment-reinsurance optimization problem under\nsimultaneous Value-at-Risk and no-short-selling constraints. We introduce the\nconcept of guarantee-equivalent utility gain and use it to compare life\ninsurance products with and without reinsurance. Our numerical studies indicate\nthat the optimally managed reinsurance allows the insurer to offer\nsignificantly higher capital guarantees to clients without any loss in the\ninsurer's expected utility. The longer the investment horizon and the less\nrisk-averse the insurer, the more prominent the reinsurance benefit.\n"
    },
    {
        "paper_id": 2111.03662,
        "authors": "Giacomo De Giorgi, Matthew Harding, Gabriel Vasconcelos",
        "title": "Predicting Mortality from Credit Reports",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Data on hundreds of variables related to individual consumer finance behavior\n(such as credit card and loan activity) is routinely collected in many\ncountries and plays an important role in lending decisions. We postulate that\nthe detailed nature of this data may be used to predict outcomes in seemingly\nunrelated domains such as individual health. We build a series of machine\nlearning models to demonstrate that credit report data can be used to predict\nindividual mortality. Variable groups related to credit cards and various\nloans, mostly unsecured loans, are shown to carry significant predictive power.\nLags of these variables are also significant thus indicating that dynamics also\nmatters. Improved mortality predictions based on consumer finance data can have\nimportant economic implications in insurance markets but may also raise privacy\nconcerns.\n"
    },
    {
        "paper_id": 2111.03713,
        "authors": "Valentin Tissot-Daguette",
        "title": "Projection of Functionals and Fast Pricing of Exotic Options",
        "comments": "14 pages, 6 figures. Forthcoming in \"SIAM Journal on Financial\n  Mathematics\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the approximation of path functionals. In particular, we\nadvocate the use of the Karhunen-Lo\\`eve expansion, the continuous analogue of\nPrincipal Component Analysis, to extract relevant information from the image of\na functional. Having accurate estimate of functionals is of paramount\nimportance in the context of exotic derivatives pricing, as presented in the\npractical applications. Specifically, we show how a simulation-based procedure,\nwhich we call the Karhunen-Lo\\`eve Monte Carlo (KLMC) algorithm, allows fast\nand efficient computation of the price of path-dependent options. We also\nexplore the path signature as an alternative tool to project both paths and\nfunctionals.\n"
    },
    {
        "paper_id": 2111.03724,
        "authors": "Giorgio Ferrari, Patrick Schuhmann, Shihao Zhu",
        "title": "Optimal Dividends under Markov-Modulated Bankruptcy Level",
        "comments": "32 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes and studies an optimal dividend problem in which a\ntwo-state regime-switching environment affects the dynamics of the company's\ncash surplus and, as a novel feature, also the bankruptcy level. The aim is to\nmaximize the total expected profits from dividends until bankruptcy. The\ncompany's optimal dividend payout is therefore influenced by four factors\nsimultaneously: Brownian fluctuations in the cash surplus, as well as regime\nchanges in drift, volatility and bankruptcy levels. In particular, the average\nprofitability can assume different signs in the two regimes. We find a rich\nstructure of the optimal strategy, which, depending on the interaction of the\nmodel's parameters, can be either of barrier-type or of liquidation-barrier\ntype. Furthermore, we provide explicit expressions of the optimal policies and\nvalue functions. Finally, we complement our theoretical results by a detailed\nnumerical study, where also a thorough analysis of the sensitivities of the\noptimal dividend policy with respect to the problem's parameters is performed.\n"
    },
    {
        "paper_id": 2111.03995,
        "authors": "Mao Guan, Xiao-Yang Liu",
        "title": "Explainable Deep Reinforcement Learning for Portfolio Management: An\n  Empirical Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep reinforcement learning (DRL) has been widely studied in the portfolio\nmanagement task. However, it is challenging to understand a DRL-based trading\nstrategy because of the black-box nature of deep neural networks. In this\npaper, we propose an empirical approach to explain the strategies of DRL agents\nfor the portfolio management task. First, we use a linear model in hindsight as\nthe reference model, which finds the best portfolio weights by assuming knowing\nactual stock returns in foresight. In particular, we use the coefficients of a\nlinear model in hindsight as the reference feature weights. Secondly, for DRL\nagents, we use integrated gradients to define the feature weights, which are\nthe coefficients between reward and features under a linear regression model.\nThirdly, we study the prediction power in two cases, single-step prediction and\nmulti-step prediction. In particular, we quantify the prediction power by\ncalculating the linear correlations between the feature weights of a DRL agent\nand the reference feature weights, and similarly for machine learning methods.\nFinally, we evaluate a portfolio management task on Dow Jones 30 constituent\nstocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a\nDRL agent exhibits a stronger multi-step prediction power than machine learning\nmethods.\n"
    },
    {
        "paper_id": 2111.04038,
        "authors": "Battulga Gankhuu",
        "title": "Equity-Linked Life Insurances on Maximum of Several Assets",
        "comments": "21 pages. arXiv admin note: substantial text overlap with\n  arXiv:2109.05998",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economic variables play important roles in any economic model, and sudden and\ndramatic changes exist in the financial market and economy. For this reason, to\nprice and hedge equity-linked life insurance products, including segregated\nfunds and unit-linked life insurance products on maximum price of several\nassets, this paper introduces Bayesian Markov-Switching Vector Autoregressive\n(MS-VAR) process. By assuming that a regime-switching process is generated by a\nhomogeneous Markov process and a residual process follows a heteroscedastic\nmodel, we obtain joint distribution of endogenous variables and insured's\nfuture lifetime random variable under risk-neutral probability probability\nmeasure. Using the distribution function, we obtain net single premiums and\nhedging formulas of the equity-linked life insurance products. An advantage of\nour model is it depends on economic variables and is not complicated as\ncompared to previous papers.\n"
    },
    {
        "paper_id": 2111.04165,
        "authors": "Jakob Mokander",
        "title": "On the Limits of Design: What Are the Conceptual Constraints on\n  Designing Artificial Intelligence for Social Good?",
        "comments": "Artificial Intelligence, Design, Infosphere, Philosophy of\n  Information, Governance, Policy",
        "journal-ref": "Chapter 5 In: Cowls J., Morley J. (eds). The 2020 Yearbook of the\n  Digital Ethics Lab. Digital Ethics Lab Yearbook. Springer (2021)",
        "doi": "10.1007/978-3-030-80083-3_5",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial intelligence AI can bring substantial benefits to society by\nhelping to reduce costs, increase efficiency and enable new solutions to\ncomplex problems. Using Floridi's notion of how to design the 'infosphere' as a\nstarting point, in this chapter I consider the question: what are the limits of\ndesign, i.e. what are the conceptual constraints on designing AI for social\ngood? The main argument of this chapter is that while design is a useful\nconceptual tool to shape technologies and societies, collective efforts towards\ndesigning future societies are constrained by both internal and external\nfactors. Internal constraints on design are discussed by evoking Hardin's\nthought experiment regarding 'the Tragedy of the Commons'. Further, Hayek's\nclassical distinction between 'cosmos' and 'taxis' is used to demarcate\nexternal constraints on design. Finally, five design principles are presented\nwhich are aimed at helping policymakers manage the internal and external\nconstraints on design. A successful approach to designing future societies\nneeds to account for the emergent properties of complex systems by allowing\nspace for serendipity and socio-technological coevolution.\n"
    },
    {
        "paper_id": 2111.04172,
        "authors": "Aditya Kuvalekar, Jo\\~ao Ramos, Johannes Schneider",
        "title": "The Wrong Kind of Information",
        "comments": null,
        "journal-ref": "RAND Journal of Economics, 2023",
        "doi": "10.1111/1756-2171.12440",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agents, some with a bias, decide between undertaking a risky project and a\nsafe alternative based on information about the project's efficiency. Only a\npart of that information is verifiable. Unbiased agents want to undertake only\nefficient projects, while biased agents want to undertake any project. If the\nproject causes harm, a court examines the verifiable information, forms a\nbelief about the agent's type, and decides the punishment. Tension arises\nbetween deterring inefficient projects and a chilling effect on using the\nunverifiable information. Improving the unverifiable information always\nincreases overall efficiency, but improving the verifiable information may\nreduce efficiency.\n"
    },
    {
        "paper_id": 2111.04311,
        "authors": "Nuerxiati Abudurexiti, Kai He, Dongdong Hu, Svetlozar T. Rachev,\n  Hasanjan Sayit, Ruoyu Sun",
        "title": "Portfolio analysis with mean-CVaR and mean-CVaR-skewness criteria based\n  on mean-variance mixture models",
        "comments": "25pages, 1 figure, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper Zhao et al. (2015) shows that mean-CVaR-skewness portfolio\noptimization problems based on asymetric Laplace (AL) distributions can be\ntransformed into quadratic optimization problems under which closed form\nsolutions can be found. In this note, we show that such result also holds for\nmean-risk-skewness portfolio optimization problems when the underlying\ndistribution is a larger class of normal mean-variance mixture (NMVM) models\nthan the class of AL distributions. We then study the value at risk (VaR) and\nconditional value at risk (CVaR) risk measures on portfolios of returns with\nNMVM distributions. They have closed form expressions for portfolios of normal\nand more generally elliptically distributed returns as discussed in Rockafellar\n& Uryasev (2000) and in Landsman & Valdez (2003). When the returns have general\nNMVM distributions, these risk measures do not give closed form expressions. In\nthis note, we give approximate closed form expressions for VaR and CVaR of\nportfolios of returns with NMVM distributions. Numerical tests show that our\nclosed form formulas give accurate values for VaR and CVaR and shortens the\ncomputational time for portfolio optimization problems associated with VaR and\nCVaR considerably.\n"
    },
    {
        "paper_id": 2111.04391,
        "authors": "Ren\\'e A\\\"id, Ofelia Bonesini, Giorgia Callegaro, Luciano Campi",
        "title": "A McKean-Vlasov game of commodity production, consumption and trading",
        "comments": "29 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a model where a producer and a consumer can affect the price\ndynamics of some commodity controlling drift and volatility of, respectively,\nthe production rate and the consumption rate. We assume that the producer has a\nshort position in a forward contract on \\lambda units of the underlying at a\nfixed price F, while the consumer has the corresponding long position.\nMoreover, both players are risk-averse with respect to their financial position\nand their risk aversions are modelled through an integrated-variance\npenalization. We study the impact of risk aversion on the interaction between\nthe producer and the consumer as well as on the derivative price. In\nmathematical terms, we are dealing with a two-player linear-quadratic\nMcKean-Vlasov stochastic differential game. Using methods based on the\nmartingale optimality principle and BSDEs, we find a Nash equilibrium and\ncharacterize the corresponding strategies and payoffs in semi-explicit form.\nFurthermore, we compute the two indifference prices (one for the producer and\none for the consumer) induced by that equilibrium and we determine the quantity\n\\lambda such that the players agree on the price. Finally, we illustrate our\nresults with some numerics. In particular, we focus on how the risk aversions\nand the volatility control costs of the players affect the derivative price.\n"
    },
    {
        "paper_id": 2111.04483,
        "authors": "Isaiah Hull and Or Sattath",
        "title": "Revisiting the Properties of Money",
        "comments": "27 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The properties of money commonly referenced in the economics literature were\noriginally identified by Jevons (1876) and Menger (1892) in the late 1800s and\nwere intended to describe physical currencies, such as commodity money,\nmetallic coins, and paper bills. In the digital era, many non-physical\ncurrencies have either entered circulation or are under development, including\ndemand deposits, cryptocurrencies, stablecoins, central bank digital currencies\n(CBDCs), in-game currencies, and quantum money. These forms of money have novel\nproperties that have not been studied extensively within the economics\nliterature, but may be important determinants of the monetary equilibrium that\nemerges in the forthcoming era of heightened currency competition. This paper\nmakes the first exhaustive attempt to identify and define the properties of all\nphysical and digital forms of money. It reviews both the economics and computer\nscience literatures and categorizes properties within an expanded version of\nthe original functions-and-properties framework of money that includes societal\nand regulatory objectives.\n"
    },
    {
        "paper_id": 2111.04626,
        "authors": "Gaurab Aryal, Hanna Charankevich, Seungwon Jeong, Dong-Hyuk Kim",
        "title": "Procurements with Bidder Asymmetry in Cost and Risk-Aversion",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/07350015.2022.2115497",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an empirical method to analyze data from first-price procurements\nwhere bidders are asymmetric in their risk-aversion (CRRA) coefficients and\ndistributions of private costs. Our Bayesian approach evaluates the likelihood\nby solving type-symmetric equilibria using the boundary-value method and\nintegrates out unobserved heterogeneity through data augmentation. We study a\nnew dataset from Russian government procurements focusing on the category of\nprinting papers. We find that there is no unobserved heterogeneity (presumably\nbecause the job is routine), but bidders are highly asymmetric in their cost\nand risk-aversion. Our counterfactual study shows that choosing a type-specific\ncost-minimizing reserve price marginally reduces the procurement cost; however,\ninviting one more bidder substantially reduces the cost, by at least 5.5%.\nFurthermore, incorrectly imposing risk-neutrality would severely mislead\ninference and policy recommendations, but the bias from imposing homogeneity in\nrisk-aversion is small.\n"
    },
    {
        "paper_id": 2111.04709,
        "authors": "Jaydip Sen, Abhishek Dutta, and Sidra Mehtab",
        "title": "Stock Portfolio Optimization Using a Deep Learning LSTM Model",
        "comments": "This is the accepted version of our paper in the international\n  conference, IEEE Mysurucon'21, which was organized in Hassan, Karnataka,\n  India from October 24, 2021 to October 25, 2021. The paper is 9 pages long,\n  and it contains 19 figures and 19 tables. This is the preprint of the\n  conference paper",
        "journal-ref": "Proc. of IEEE Mysore Sub Section International Conference\n  (MysuruCon), October 24-25, 2021, pp. 263-271, Hassan, Karnataka, India",
        "doi": "10.1109/MysuruCon52639.2021.9641662",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting future stock prices and their movement patterns is a complex\nproblem. Hence, building a portfolio of capital assets using the predicted\nprices to achieve the optimization between its return and risk is an even more\ndifficult task. This work has carried out an analysis of the time series of the\nhistorical prices of the top five stocks from the nine different sectors of the\nIndian stock market from January 1, 2016, to December 31, 2020. Optimum\nportfolios are built for each of these sectors. For predicting future stock\nprices, a long-and-short-term memory (LSTM) model is also designed and\nfine-tuned. After five months of the portfolio construction, the actual and the\npredicted returns and risks of each portfolio are computed. The predicted and\nthe actual returns of each portfolio are found to be high, indicating the high\nprecision of the LSTM model.\n"
    },
    {
        "paper_id": 2111.04951,
        "authors": "Songqiao Han, Hailiang Huang, Jiangwei Liu, Shengsheng Xiao",
        "title": "American Hate Crime Trends Prediction with Event Extraction",
        "comments": "12 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social media platforms may provide potential space for discourses that\ncontain hate speech, and even worse, can act as a propagation mechanism for\nhate crimes. The FBI's Uniform Crime Reporting (UCR) Program collects hate\ncrime data and releases statistic report yearly. These statistics provide\ninformation in determining national hate crime trends. The statistics can also\nprovide valuable holistic and strategic insight for law enforcement agencies or\njustify lawmakers for specific legislation. However, the reports are mostly\nreleased next year and lag behind many immediate needs. Recent research mainly\nfocuses on hate speech detection in social media text or empirical studies on\nthe impact of a confirmed crime. This paper proposes a framework that first\nutilizes text mining techniques to extract hate crime events from New York\nTimes news, then uses the results to facilitate predicting American\nnational-level and state-level hate crime trends. Experimental results show\nthat our method can significantly enhance the prediction performance compared\nwith time series or regression methods without event-related factors. Our\nframework broadens the methods of national-level and state-level hate crime\ntrends prediction.\n"
    },
    {
        "paper_id": 2111.04976,
        "authors": "Jaydip Sen, Saikat Mondal, and Sidra Mehtab",
        "title": "Analysis of Sectoral Profitability of the Indian Stock Market Using an\n  LSTM Regression Model",
        "comments": "This was accepted for oral presentation and publication in the\n  proceedings of the Deep Learning Developers' Conference (DLDC'2021) organized\n  online from September 23 - September 24, 2021 by Analytics India Magazine,\n  INIDA. The paper si 8 pages long, and it contains 15 figures and 14 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predictive model design for accurately predicting future stock prices has\nalways been considered an interesting and challenging research problem. The\ntask becomes complex due to the volatile and stochastic nature of the stock\nprices in the real world which is affected by numerous controllable and\nuncontrollable variables. This paper presents an optimized predictive model\nbuilt on long-and-short-term memory (LSTM) architecture for automatically\nextracting past stock prices from the web over a specified time interval and\npredicting their future prices for a specified forecast horizon, and forecasts\nthe future stock prices. The model is deployed for making buy and sell\ntransactions based on its predicted results for 70 important stocks from seven\ndifferent sectors listed in the National Stock Exchange (NSE) of India. The\nprofitability of each sector is derived based on the total profit yielded by\nthe stocks in that sector over a period from Jan 1, 2010 to Aug 26, 2021. The\nsectors are compared based on their profitability values. The prediction\naccuracy of the model is also evaluated for each sector. The results indicate\nthat the model is highly accurate in predicting future stock prices.\n"
    },
    {
        "paper_id": 2111.05072,
        "authors": "Gabriele D'Acunto, Paolo Bajardi, Francesco Bonchi, Gianmarco De\n  Francisci Morales",
        "title": "The Evolving Causal Structure of Equity Risk Factors",
        "comments": null,
        "journal-ref": "ACM International Conference on AI in Finance, 2021",
        "doi": "10.1145/3490354.3494370",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, multi-factor strategies have gained increasing popularity in\nthe financial industry, as they allow investors to have a better understanding\nof the risk drivers underlying their portfolios. Moreover, such strategies\npromise to promote diversification and thus limit losses in times of financial\nturmoil. However, recent studies have reported a significant level of\nredundancy between these factors, which might enhance risk contagion among\nmulti-factor portfolios during financial crises. Therefore, it is of\nfundamental importance to better understand the relationships among factors.\n  Empowered by recent advances in causal structure learning methods, this paper\npresents a study of the causal structure of financial risk factors and its\nevolution over time. In particular, the data we analyze covers 11 risk factors\nconcerning the US equity market, spanning a period of 29 years at daily\nfrequency.\n  Our results show a statistically significant sparsifying trend of the\nunderlying causal structure. However, this trend breaks down during periods of\nfinancial stress, in which we can observe a densification of the causal network\ndriven by a growth of the out-degree of the market factor node. Finally, we\npresent a comparison with the analysis of factors cross-correlations, which\nfurther confirms the importance of causal analysis for gaining deeper insights\nin the dynamics of the factor system, particularly during economic downturns.\n  Our findings are especially significant from a risk-management perspective.\nThey link the evolution of the causal structure of equity risk factors with\nmarket volatility and a worsening macroeconomic environment, and show that, in\ntimes of financial crisis, exposure to different factors boils down to exposure\nto the market risk factor.\n"
    },
    {
        "paper_id": 2111.05188,
        "authors": "Zechu Li and Xiao-Yang Liu and Jiahao Zheng and Zhaoran Wang and Anwar\n  Walid and Jian Guo",
        "title": "FinRL-Podracer: High Performance and Scalable Deep Reinforcement\n  Learning for Quantitative Finance",
        "comments": null,
        "journal-ref": "ACM International Conference on AI in Finance, 2021",
        "doi": "10.1145/3490354.3494413",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Machine learning techniques are playing more and more important roles in\nfinance market investment. However, finance quantitative modeling with\nconventional supervised learning approaches has a number of limitations. The\ndevelopment of deep reinforcement learning techniques is partially addressing\nthese issues. Unfortunately, the steep learning curve and the difficulty in\nquick modeling and agile development are impeding finance researchers from\nusing deep reinforcement learning in quantitative trading. In this paper, we\npropose an RLOps in finance paradigm and present a FinRL-Podracer framework to\naccelerate the development pipeline of deep reinforcement learning (DRL)-driven\ntrading strategy and to improve both trading performance and training\nefficiency. FinRL-Podracer is a cloud solution that features high performance\nand high scalability and promises continuous training, continuous integration,\nand continuous delivery of DRL-driven trading strategies, facilitating a rapid\ntransformation from algorithmic innovations into a profitable trading strategy.\nFirst, we propose a generational evolution mechanism with an ensemble strategy\nto improve the trading performance of a DRL agent, and schedule the training of\na DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out\nthe training of DRL components with high-performance optimizations on GPUs.\nFinally, we evaluate the FinRL-Podracer framework for a stock trend prediction\ntask on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular\nDRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\sim 35%\nimprovements in annual return, 0.1 \\sim 0.6 improvements in Sharpe ratio and 3\ntimes \\sim 7 times speed-up in training time. We show the high scalability by\ntraining a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100\nconstituent stocks with minute-level data over 10 years.\n"
    },
    {
        "paper_id": 2111.05272,
        "authors": "Meenakshi Balakrishna and Kenneth C. Wilbur",
        "title": "Do Firearm Markets Comply with Firearm Restrictions? How the\n  Massachusetts Assault Weapons Ban Enforcement Notice Changed Firearm Sales",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2102.02884",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How well do firearm markets comply with firearm restrictions? The\nMassachusetts Attorney General issued an Enforcement Notice in 2016 to announce\na new interpretation of the key phrase \"copies and duplicates\" in the state's\nassault weapons ban. The Enforcement Notice increased assault rifle sales by\n1,349 (+560%) within five days, followed by a reduction of 211 (-58%) over the\nnext three weeks. Assault rifle sales were 64-66% lower in 2017 than in\ncomparable earlier periods, suggesting that the Enforcement Notice reduced\nassault weapon sales but also that many banned weapons continued to be sold.\n"
    },
    {
        "paper_id": 2111.05455,
        "authors": "Michelle Escobar Carias, David Johnston, Rachel Knott, Rohan Sweeney",
        "title": "Flood Disasters and Health Among the Urban Poor",
        "comments": "41 pages, 1 figure, 6 tables, 10 Appendix tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Billions of people live in urban poverty, with many forced to reside in\ndisaster-prone areas. Research suggests that such disasters harm child\nnutrition and increase adult morbidity. However, little is known about impacts\non mental health, particularly of people living in slums. In this paper we\nestimate the effects of flood disasters on the mental and physical health of\npoor adults and children in urban Indonesia. Our data come from the Indonesia\nFamily Life Survey and new surveys of informal settlement residents. We find\nthat urban poor populations experience increases in acute morbidities and\ndepressive symptoms following floods, that the negative mental health effects\nlast longer, and that the urban wealthy show no health effects from flood\nexposure. Further analysis suggests that worse economic outcomes may be partly\nresponsible. Overall, the results provide a more nuanced understanding of the\nmorbidities experienced by populations most vulnerable to increased disaster\noccurrence.\n"
    },
    {
        "paper_id": 2111.05686,
        "authors": "Itzhak Rasooly",
        "title": "Going... going... wrong: a test of the level-k (and cognitive hierarchy)\n  models of bidding behaviour",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we design and implement an experiment aimed at testing the\nlevel-k model of auctions. We begin by asking which (simple) environments can\nbest disentangle the level-k model from its leading rival, Bayes-Nash\nequilibrium. We find two environments that are particularly suited to this\npurpose: an all-pay auction with uniformly distributed values, and a\nfirst-price auction with the possibility of cancelled bids. We then implement\nboth of these environments in a virtual laboratory in order to see which theory\ncan best explain observed bidding behaviour. We find that, when plausibly\ncalibrated, the level-k model substantially under-predicts the observed bids\nand is clearly out-performed by equilibrium. Moreover, attempting to fit the\nlevel-k model to the observed data results in implausibly high estimated\nlevels, which in turn bear no relation to the levels inferred from a game known\nto trigger level-k reasoning. Finally, subjects almost never appeal to iterated\nreasoning when asked to explain how they bid. Overall, these findings suggest\nthat, despite its notable success in predicting behaviour in other strategic\nsettings, the level-k model (and its close cousin cognitive hierarchy) cannot\nexplain behaviour in auctions.\n"
    },
    {
        "paper_id": 2111.05783,
        "authors": "Sandro Provenzano and Hannah Bull",
        "title": "The Local Economic Impact of Mineral Mining in Africa: Evidence from\n  Four Decades of Satellite Imagery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using state-of-the-art techniques in computer vision, we analyze one million\nsatellite images covering 12% of the African continent between 1984 and 2019 to\ntrack local development around 1,658 mineral deposits. We use stacked event\nstudies and difference-in-difference models to estimate the impact of mine\nopenings and closings. The magnitude of the effect of mine openings is\nconsiderable - after 15 years, urban areas within 20km of an opening mine\nalmost double in size. We find strong evidence of a political resource curse at\nthe local level. Although mining boosts the local economy in democratic\ncountries, these gains are meager in autocracies and come at the expense of\ntripling the likelihood of conflict relative to prior the onset of mining.\nFurthermore, our results suggest that the growth acceleration in mining areas\nis only temporary and diminishes with the closure of the mine.\n"
    },
    {
        "paper_id": 2111.05843,
        "authors": "Mohammad Firouz, Linda Li, Daizy Ahmed, Abdulaziz Ahmed",
        "title": "An Integrated Vaccination Site Selection and Dose Allocation Problem\n  with Fairness Concerns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fairness in vaccination is not only important from a social justice point of\nview, but experience has shown that a fair distribution of vaccine proves more\neffective in public immunization by preventing highly-concentrated infected\nareas to form among the population. In this paper, we address fairness from two\nsimultaneous points of view: equity and accessibility. Equity in our setting\nmeans that as far as possible, each demand zone should receive a fair-share of\nthe total doses available. On the other hand, accessibility means that as far\nas possible, each demand zone should have equal travel distance to access their\nassigned vaccination site.\n"
    },
    {
        "paper_id": 2111.05935,
        "authors": "Damian Kisiel and Denise Gorse",
        "title": "A Meta-Method for Portfolio Management Using Machine Learning for\n  Adaptive Strategy Selection",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": "10.1145/3507623.3507635",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work proposes a novel portfolio management technique, the Meta Portfolio\nMethod (MPM), inspired by the successes of meta approaches in the field of\nbioinformatics and elsewhere. The MPM uses XGBoost to learn how to switch\nbetween two risk-based portfolio allocation strategies, the Hierarchical Risk\nParity (HRP) and more classical Na\\\"ive Risk Parity (NRP). It is demonstrated\nthat the MPM is able to successfully take advantage of the best characteristics\nof each strategy (the NRP's fast growth during market uptrends, and the HRP's\nprotection against drawdowns during market turmoil). As a result, the MPM is\nshown to possess an excellent out-of-sample risk-reward profile, as measured by\nthe Sharpe ratio, and in addition offers a high degree of interpretability of\nits asset allocation decisions.\n"
    },
    {
        "paper_id": 2111.06042,
        "authors": "Baron Law",
        "title": "Correlation Estimation in Hybrid Systems",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0219024923500085",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A simple method is proposed to estimate the instantaneous correlations\nbetween state variables in a hybrid system from the empirical correlations\nbetween observable market quantities such as spot rate, stock price and implied\nvolatility. The new algorithm is extremely fast since only low-dimension linear\nsystems are involved. If the resulting matrix from the linear systems is not\npositive semidefinite, the shrinking method, which requires only\nbisection-style iterations, is recommended to convert the matrix to positive\nsemidefinite. The square of short-term at-the-money implied volatility is\nsuggested as the proxy for the unobservable stochastic variance. When the\nimplied volatility is not available, a simple trick is provided to fill in the\nmissing correlations. Numerical study shows that the estimates are reasonably\naccurate, when using more than 1,000 data points. In addition, the algorithm is\nrobust to misspecified interest rate model parameters and the\nshort-sampling-period assumption. G2++ and Heston are used for illustration but\nthe method can be extended to other affine term structure, local volatility and\njump diffusion models, with or without stochastic interest rate.\n"
    },
    {
        "paper_id": 2111.06062,
        "authors": "Michael Thaler",
        "title": "The Supply of Motivated Beliefs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When people choose what messages to send to others, they often consider how\nothers will interpret the messages. A sender may expect a receiver to engage in\nmotivated reasoning, leading the receiver to trust good news more than bad\nnews, relative to a Bayesian. This paper experimentally studies how motivated\nreasoning affects information transmission in political settings. Senders are\nrandomly matched with receivers whose political party's stances happen to be\naligned or misaligned with the truth, and either face incentives to be rated as\ntruthful or face no incentives. Incentives to be rated as truthful cause\nsenders to be less truthful; when incentivized, senders send false information\nto align messages with receivers' politically-motivated beliefs. The adverse\neffect of incentives is not appreciated by receivers, who rate senders in both\nconditions as being equally likely to be truthful. A complementary experiment\nfurther identifies senders' beliefs about receivers' motivated reasoning as the\nmechanism driving these results. Senders are additionally willing to pay to\nlearn the politics of their receivers, and use this information to send more\nfalse messages.\n"
    },
    {
        "paper_id": 2111.06224,
        "authors": "Wanetha Sudswong, Anon Plangprasopchok, and Chainarong\n  Amornbunchornvej",
        "title": "Occupational Income Inequality of Thailand: A Case Study of Exploratory\n  Data Analysis beyond Gini Coefficient",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Income inequality is an important issue that has to be solved in order to\nmake progress in our society. The study of income inequality is well received\nthrough the Gini coefficient, which is used to measure degrees of inequality in\ngeneral. While this method is effective in several aspects, the Gini\ncoefficient alone inevitably overlooks minority subpopulations (e.g.\noccupations) which results in missing undetected patterns of inequality in\nminority.\n  In this study, the surveys of incomes and occupations from more than 12\nmillions households across Thailand have been analyzed by using both Gini\ncoefficient and network densities of income domination networks to get insight\nregarding the degrees of general and occupational income inequality issues. The\nresults show that, in agricultural provinces, there are less issues in both\ntypes of inequality (low Gini coefficients and network densities), while some\nnon-agricultural provinces face an issue of occupational income inequality\n(high network densities) without any symptom of general income inequality (low\nGini coefficients). Moreover, the results also illustrate the gaps of income\ninequality using estimation statistics, which not only support whether income\ninequality exists, but that we are also able to tell the magnitudes of income\ngaps among occupations. These results cannot be obtained via Gini coefficients\nalone. This work serves as a use case of analyzing income inequality from both\ngeneral population and subpopulations perspectives that can be utilized in\nstudies of other countries.\n"
    },
    {
        "paper_id": 2111.06253,
        "authors": "Sigurd Bjarghov and Hossein Farahmand and Gerard Doorman",
        "title": "Grid Tariffs Based on Capacity Subscription: Multi Year Analysis on\n  Metered Consumer Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While volume-based grid tariffs have been the norm for residential consumers,\ncapacity-based tariffs will become more relevant with the increasing\nelectrification of society. A further development is capacity subscription,\nwhere consumers are financially penalised for exceeding their subscribed\ncapacity, or alternatively their demand is limited to the subscribed level. The\npenalty or limitation can either be static (always active) or dynamic, meaning\nthat it is only activated when there are active grid constraints. We\ninvestigate the cost impact for static and dynamic capacity subscription\ntariffs, for 84 consumers based on six years of historical load data. We use\nseveral approaches for finding the optimal subscription level ex ante. The\nresults show that annual costs remain both stable and similar for most\nconsumers, with a few exceptions for those that have high peak demand. In the\ncase of a physical limitation, it is important to use a stochastic approach for\nthe optimal subscription level to avoid excessive demand limitations. Facing\nincreased peak loads due to electrification, regulators should consider a move\nto capacity-based tariffs in order to reduce cross-subsidisation between\nconsumers and increase cost reflectivity without impacting the DSO cost\nrecovery.\n"
    },
    {
        "paper_id": 2111.06365,
        "authors": "Yong Cai, Santiago Camara, Nicholas Capel",
        "title": "It's not always about the money, sometimes it's about sending a message:\n  Evidence of Informational Content in Monetary Policy Announcements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a transparent framework to identify the informational\ncontent of FOMC announcements. We do so by modelling the expectations of the\nFOMC and private sector agents using state of the art computational linguistic\ntools on both FOMC statements and New York Times articles. We identify the\ninformational content of FOMC announcements as the projection of high frequency\nmovements in financial assets onto differences in expectations. Our recovered\nseries is intuitively reasonable and shows that information disclosure has a\nsignificant impact on the yields of short-term government bonds.\n"
    },
    {
        "paper_id": 2111.06371,
        "authors": "Christian Esposito and Marco Gortan and Lorenzo Testa and Francesca\n  Chiaromonte and Giorgio Fagiolo and Andrea Mina and Giulio Rossetti",
        "title": "Can you always reap what you sow? Network and functional data analysis\n  of VC investments in health-tech companies",
        "comments": "12 pages, 4 figures, accepted for publication in the proceedings of\n  the 10th International Conference on Complex Networks and Their Applications",
        "journal-ref": "Proceedings of 10th International Conference of Complex Networks\n  and their applications 2021",
        "doi": "10.1007/978-3-030-93409-5_61",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \"Success\" of firms in venture capital markets is hard to define, and its\ndeterminants are still poorly understood. We build a bipartite network of\ninvestors and firms in the healthcare sector, describing its structure and its\ncommunities. Then, we characterize \"success\" introducing progressively more\nrefined definitions, and we find a positive association between such\ndefinitions and the centrality of a company. In particular, we are able to\ncluster funding trajectories of firms into two groups capturing different\n\"success\" regimes and to link the probability of belonging to one or the other\nto their network features (in particular their centrality and the one of their\ninvestors). We further investigate this positive association by introducing\nscalar as well as functional \"success\" outcomes, confirming our findings and\ntheir robustness.\n"
    },
    {
        "paper_id": 2111.06462,
        "authors": "Krist\\'of Kutasi, J\\'ulia Koltai, \\'Agnes Szab\\'o-Morvai, Gergely\n  R\\\"ost, M\\'arton Karsai, P\\'eter Bir\\'o, Bal\\'azs Lengyel",
        "title": "Understanding hesitancy with revealed preferences across COVID-19\n  vaccine types",
        "comments": "29 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many countries have secured larger quantities of COVID-19 vaccines than their\npopulace is willing to take. This abundance and variety of vaccines created a\nhistorical moment to understand vaccine hesitancy better. Never before were\nmore types of vaccines available for an illness and the intensity of\nvaccine-related public discourse is unprecedented. Yet, the heterogeneity of\nhesitancy by vaccine types has been neglected so far, even though factual or\nbelieved vaccine characteristics and patient attributes are known to influence\nacceptance. We address this problem by analysing acceptance and assessment of\nfive vaccine types using information collected with a nationally representative\nsurvey at the end of the third wave of the COVID-19 pandemic in Hungary, where\na unique portfolio of vaccines were available to the public in large\nquantities. Our special case enables us to quantify revealed preferences across\nvaccine types since one could evaluate a vaccine unacceptable and even could\nreject an assigned vaccine to wait for another type. We find that the source of\ninformation that respondents trust characterizes their attitudes towards\nvaccine types differently and leads to divergent vaccine hesitancy. Believers\nof conspiracy theories were significantly more likely to evaluate the mRNA\nvaccines (Pfizer and Moderna) unacceptable while those who follow the advice of\npoliticians evaluate vector-based (AstraZeneca and Sputnik) or whole-virus\nvaccines (Sinopharm) acceptable with higher likelihood. We illustrate that the\nrejection of non-desired and re-selection of preferred vaccines fragments the\npopulation by the mRNA versus other type of vaccines while it generally\nimproves the assessment of the received vaccine. These results highlight that\ngreater variance of available vaccine types and individual free choice are\ndesirable conditions that can widen the acceptance of vaccines in societies.\n"
    },
    {
        "paper_id": 2111.06631,
        "authors": "Nhan Huynh and Mike Ludkovski",
        "title": "Joint Models for Cause-of-Death Mortality in Multiple Populations",
        "comments": "27 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate jointly modeling Age-specific rates of various causes of death\nin a multinational setting. We apply Multi-Output Gaussian Processes (MOGP), a\nspatial machine learning method, to smooth and extrapolate multiple\ncause-of-death mortality rates across several countries and both genders. To\nmaintain flexibility and scalability, we investigate MOGPs with\nKronecker-structured kernels and latent factors. In particular, we develop a\ncustom multi-level MOGP that leverages the gridded structure of mortality\ntables to efficiently capture heterogeneity and dependence across different\nfactor inputs. Results are illustrated with datasets from the Human\nCause-of-Death Database (HCD). We discuss a case study involving cancer\nvariations in three European nations, and a US-based study that considers eight\ntop-level causes and includes comparison to all-cause analysis. Our models\nprovide insights into the commonality of cause-specific mortality trends and\ndemonstrate the opportunities for respective data fusion.\n"
    },
    {
        "paper_id": 2111.06655,
        "authors": "Ilyas El Ghordaf, Abdelbari El Khamlichi (UCD, IAE - UCA)",
        "title": "Profit warnings and stock returns: Evidence from moroccan stock exchange",
        "comments": null,
        "journal-ref": "2nd International conference on organization's performance, May\n  2021, El jadida, Morocco",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is an important literature focused on profit warnings and its impact on\nstock returns. We provide evidence from Moroccan stock market which aims to\nbecome an African financial hub. Despite this practical improvement, academic\nresearches that focused on this market are scarce and our study is a first\ninvestigation in this context. Using the event study methodology and a sample\nof companies listed in Casablanca Stock Exchange for the period of 2009 to\n2016, we examined whether the effect of qualitative warning is more negative\ncompared to quantitative warnings in a short event window. Our empirical\nfindings show that the average abnormal return on the date of announcement is\nnegative and statistically significant. The magnitude of this negative abnormal\nreturn is greater for qualitative warnings than quantitative ones.\n"
    },
    {
        "paper_id": 2111.06663,
        "authors": "Tim Ritmeester and Hildegard Meyer-Ortmanns",
        "title": "The cavity method for minority games between arbitrageurs on financial\n  markets",
        "comments": "36 pages, 7 figures",
        "journal-ref": "J. Stat. Mech. (2022) 043403",
        "doi": "10.1088/1742-5468/ac6030",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We use the cavity method from statistical physics for analyzing the transient\nand stationary dynamics of a minority game that is played by agents performing\nmarket arbitrage. On the level of linear response the method allows to include\nthe reaction of the market to individual actions of the agents as well as the\nreaction of the agents to individual information items of the market. This way\nwe derive a self-consistent solution to the minority game. In particular we\nanalyze the impact of general nonlinear price functions on the amount of\narbitrage if noise from external fluctuations is present. We identify the\nconditions under which arbitrage gets reduced due to the presence of noise.\nWhen the cavity method is extended to time dependent response of the market\nprice to previous actions of the agents, the individual contributions of noise\ncan be pursued over different time scales in the transient dynamics until a\nstationary state is reached and when the stationary state is reached. The\ncontributions are from external fluctuations in price and information and from\nnoise due to the choice of strategies. The dynamics explains the time evolution\nof scores of the agents' strategies: it changes from initially a random walk to\nnon-Markovian dynamics and bounded excursions on an intermediate time scale to\neffectively random switching in the choice between strategies on long time\nscales. In contrast to the Curie-Weiss level of a mean-field approach, the\nmarket response included by the cavity method captures the realistic feature\nthat the agents can have a preference for a certain choice of strategies\nwithout getting stuck to a single choice. The breakdown of the method in the\nphase transition region indicates possible market mechanisms leading to\ncritical volatility and a possible regime shift.\n"
    },
    {
        "paper_id": 2111.06837,
        "authors": "Wen Hsu, Bing-Fang Hwang, Chau-Ren Jung, Yau-Huo Jimmy Shr",
        "title": "Can Air Pollution Save Lives? Air Quality and Risky Behaviors on Roads",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Air pollution has been linked to elevated levels of risk aversion. This paper\nprovides the first evidence showing that such effect reduces life-threatening\nrisky behaviors. We study the impact of air pollution on traffic accidents\ncaused by risky driving behaviors, using the universe of accident records and\nhigh-resolution air quality data of Taiwan from 2009 to 2015. We find that air\npollution significantly decreases accidents caused by driver violations, and\nthat this effect is nonlinear. In addition, our results suggest that air\npollution primarily reduces road users' risky behaviors through visual channels\nrather than through the respiratory system.\n"
    },
    {
        "paper_id": 2111.06886,
        "authors": "Hugo Inzirillo and R\\'emi Genet",
        "title": "Performance vs Persistence : Assess the alpha to identify outperformers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The number of pension funds has multiplied exponentially over the last\ndecade. Active portfolio management requires a precise analysis of the\nperformance drivers. Several risk and performance attribution metrics have been\ndeveloped since the 70s to guide investors in their investment choices. Based\non the study made by Fama and French (2010) we reproduce the experiment they\nhad carried out in order to complete their work using additionnal features.\nThroughout this study we draw a parallel between the results obtained by Fama\nand French (2010) with the 3-factor model. The aim of this paper is to assess\nthe usefulness of two additional factors in the analysis of the persistence of\nalphas. We also look at the quality of the manager through his investment\nchoices in order to generate alpha considering the environment in which he\noperates.\n"
    },
    {
        "paper_id": 2111.07075,
        "authors": "Magomet Yandiev",
        "title": "Risk-Free Rate in the Covid-19 Pandemic: Application Mistakes and\n  Conclusions for Traders",
        "comments": "11 pages, 1 figure, 1 Graph, 1 Tabl",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This short paper is intended to demonstrate a crucial omission made by\ntraders in setting the risk-free interest rate, especially in times of crisis:\ninstead of increasing the risk-free rate, traders undercut it en masse on the\ncontrary. This results in incorrect investment and financial decisions,\nespecially those involving CAPM models, option pricing models and portfolio\ntheory.\n"
    },
    {
        "paper_id": 2111.07508,
        "authors": "Feras A. Batarseh, Munisamy Gopinath, Anderson Monken, Zhengrong Gu",
        "title": "Public Policymaking for International Agricultural Trade using\n  Association Rules and Ensemble Machine Learning",
        "comments": "Paper published at Elsevier's Journal of Machine Learning with\n  Applications\n  https://www.sciencedirect.com/science/article/pii/S2666827021000232",
        "journal-ref": "Machine Learning with Applications, Volume 5, 2021, 100046, ISSN\n  2666-8270",
        "doi": "10.1016/j.mlwa.2021.100046",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  International economics has a long history of improving our understanding of\nfactors causing trade, and the consequences of free flow of goods and services\nacross countries. The recent shocks to the free trade regime, especially trade\ndisputes among major economies, as well as black swan events, such as trade\nwars and pandemics, raise the need for improved predictions to inform policy\ndecisions. AI methods are allowing economists to solve such prediction problems\nin new ways. In this manuscript, we present novel methods that predict and\nassociate food and agricultural commodities traded internationally. Association\nRules (AR) analysis has been deployed successfully for economic scenarios at\nthe consumer or store level, such as for market basket analysis. In our work\nhowever, we present analysis of imports and exports associations and their\neffects on commodity trade flows. Moreover, Ensemble Machine Learning methods\nare developed to provide improved agricultural trade predictions, outlier\nevents' implications, and quantitative pointers to policy makers.\n"
    },
    {
        "paper_id": 2111.07844,
        "authors": "Hans Buehler, Phillip Murray, Mikko S. Pakkanen, Ben Wood",
        "title": "Deep Hedging: Learning to Remove the Drift under Trading Frictions with\n  Minimal Equivalent Near-Martingale Measures",
        "comments": "21 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a machine learning approach for finding minimal equivalent\nmartingale measures for markets simulators of tradable instruments, e.g. for a\nspot price and options written on the same underlying. We extend our results to\nmarkets with frictions, in which case we find \"near-martingale measures\" under\nwhich the prices of hedging instruments are martingales within their bid/ask\nspread.\n  By removing the drift, we are then able to learn using Deep Hedging a \"clean\"\nhedge for an exotic payoff which is not polluted by the trading strategy trying\nto make money from statistical arbitrage opportunities. We correspondingly\nhighlight the robustness of this hedge vs estimation error of the original\nmarket simulator. We discuss applications to two market simulators.\n"
    },
    {
        "paper_id": 2111.0806,
        "authors": "Faizal Hafiz, Jan Broekaert, Davide La Torre, Akshya Swain",
        "title": "A Multi-criteria Approach to Evolve Sparse Neural Architectures for\n  Stock Market Forecasting",
        "comments": "29 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes a new framework to evolve efficacious yet parsimonious\nneural architectures for the movement prediction of stock market indices using\ntechnical indicators as inputs. In the light of a sparse signal-to-noise ratio\nunder the Efficient Market hypothesis, developing machine learning methods to\npredict the movement of a financial market using technical indicators has shown\nto be a challenging problem. To this end, the neural architecture search is\nposed as a multi-criteria optimization problem to balance the efficacy with the\ncomplexity of architectures. In addition, the implications of different\ndominant trading tendencies which may be present in the pre-COVID and\nwithin-COVID time periods are investigated. An $\\epsilon-$ constraint framework\nis proposed as a remedy to extract any concordant information underlying the\npossibly conflicting pre-COVID data. Further, a new search paradigm,\nTwo-Dimensional Swarms (2DS) is proposed for the multi-criteria neural\narchitecture search, which explicitly integrates sparsity as an additional\nsearch dimension in particle swarms. A detailed comparative evaluation of the\nproposed approach is carried out by considering genetic algorithm and several\ncombinations of empirical neural design rules with a filter-based feature\nselection method (mRMR) as baseline approaches. The results of this study\nconvincingly demonstrate that the proposed approach can evolve parsimonious\nnetworks with better generalization capabilities.\n"
    },
    {
        "paper_id": 2111.08115,
        "authors": "Eric Forgy and Leo Lau",
        "title": "A Family of Multi-Asset Automated Market Makers",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a family of multi-asset automated market makers whose liquidity\ncurves are derived from the financial principles of self financing transactions\nand rebalancing. The constant product market maker emerges as a special case.\n"
    },
    {
        "paper_id": 2111.08294,
        "authors": "Maria Arduca and Cosimo Munari",
        "title": "Risk measures beyond frictionless markets",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general theory of risk measures that determines the optimal\namount of capital to raise and invest in a portfolio of reference traded\nsecurities in order to meet a pre-specified regulatory requirement. The\ndistinguishing feature of our approach is that we embed portfolio constraints\nand transaction costs into the securities market. As a consequence, we have to\ndispense with the property of translation invariance, which plays a key role in\nthe classical theory. We provide a comprehensive analysis of relevant\nproperties such as star shapedness, positive homogeneity, convexity,\nquasiconvexity, subadditivity, and lower semicontinuity. In addition, we\nestablish dual representations for convex and quasiconvex risk measures. In the\nconvex case, the absence of a special kind of arbitrage opportunities allows to\nobtain dual representations in terms of pricing rules that respect market\nbid-ask spreads and assign a strictly positive price to each nonzero position\nin the regulator's acceptance set.\n"
    },
    {
        "paper_id": 2111.08311,
        "authors": "M\\'ed\\'eric Motte (LPSM), Huy\\^en Pham (LPSM)",
        "title": "Optimal bidding strategies for digital advertising",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the emergence of new online channels and information technology, digital\nadvertising tends to substitute more and more to traditional advertising by\noffering the opportunity to companies to target the consumers/users that are\nreally interested by their products or services. We introduce a novel framework\nfor the study of optimal bidding strategies associated to different types of\nadvertising, namely, commercial advertising for triggering purchases or\nsubscriptions, and social marketing for alerting population about unhealthy\nbehaviours (anti-drug, vaccination, road-safety campaigns). Our continuoustime\nmodels are based on a common framework encoding users online behaviours via\ntheir web-browsing at random times, and the targeted advertising auction\nmechanism widely used on Internet, the objective being to efficiently diffuse\nadvertising information by means of digital channels. Our main results are to\nprovide semi-explicit formulas for the optimal value and bidding policy for\neach of these problems. We show some sensitivity properties of the solution\nwith respect to model parameters, and analyse how the different sources of\ndigital information accessible to users including the social interactions\naffect the optimal bid for advertising auctions. We also study how to\nefficiently combine targeted advertising and non-targeted advertising\nmechanisms. Finally, some classes of examples with fully explicit formulas are\nderived.\n"
    },
    {
        "paper_id": 2111.08338,
        "authors": "Nicole El Karoui and Kaouther Hadji and Sarah Kaakai",
        "title": "Simulating long-term impacts of mortality shocks: learning from the\n  cholera pandemic",
        "comments": "25 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this paper is to study the long-term consequence on longevity of a\nmortality shock. We adopt an historical and modeling approach to study how the\npopulation evolution following a mortality shock such as the COVID-19 pandemic\ncould impact future mortality rates. In the first of part the paper, we study\nthe several cholera epidemics in France and in England starting from the 1830s,\nand their impact on the major development of public health at the end of the\nnineteenth century. In the second part, we present the mathematical modeling of\nstochastic Individual-Based models. Using the R package IBMPopSim, this\nflexible framework is then applied to simulate the long-term impact of a\nmortality shock, using a toy model where nonlinear population compositional\nchanges affect future mortality rates.\n"
    },
    {
        "paper_id": 2111.08359,
        "authors": "Luca Di Persio, Alessandro Gnoatto, Marco Patacca",
        "title": "A change of measure formula for recursive conditional expectations",
        "comments": "25 pages. Minor typos removed",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we derive a representation for the value process associated to\nthe solutions of FBSDEs in a jump-diffusion setting under multiple probability\nmeasures. Motivated by concrete financial problems, the latter representations\nare then applied to devise a generalization of the change of num\\'eraire\ntechnique allowing to obtain recursive pricing formulas in the presence of\nmultiple interest rates and collateralization.\n"
    },
    {
        "paper_id": 2111.0839,
        "authors": "Tatsuru Kikuchi, Toranosuke Onishi and Kenichi Ueda",
        "title": "Price Stability of Cryptocurrencies as a Medium of Exchange",
        "comments": "127 pages",
        "journal-ref": null,
        "doi": "10.7566/JPSCP.36.011001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present positive evidence of price stability of cryptocurrencies as a\nmedium of exchange. For the sample years from 2016 to 2020, the prices of major\ncryptocurrencies are found to be stable, relative to major financial assets.\nSpecifically, after filtering out the less-than-one-month cycles, we\ninvestigate the daily returns in US dollars of the major cryptocurrencies\n(i.e., Bitcoin, Ethereum, and Ripple) as well as their comparators (i.e., major\nlegal tenders, the Euro and Japanese yen, and the major stock indexes, S&P 500\nand MSCI World Index). We examine the stability of the filtered daily returns\nusing three different measures. First, the Pearson correlations increased in\nlater years in our sample. Second, based on the dynamic time-warping method\nthat allows lags and leads in relations, the similarities in the daily returns\nof cryptocurrencies with their comparators have been present even since 2016.\nThird, we check whether the cumulative sum of errors to predict cryptocurrency\nprices, assuming stable relations with comparators' daily returns, does not\nexceeds the bounds implied by the Black-Scholes model. This test, in other\nwords, does not reject the efficient market hypothesis.\n"
    },
    {
        "paper_id": 2111.08601,
        "authors": "Matthieu Stigler, David Lobell",
        "title": "Optimal index insurance and basis risk decomposition: an application to\n  Kenya",
        "comments": "Forthcoming in American Journal of Agricultural Economics",
        "journal-ref": null,
        "doi": "10.1111/ajae.12375",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Index insurance is a promising tool to reduce the risk faced by farmers, but\nhigh basis risk, which arises from imperfect correlation between the index and\nindividual farm yields, has limited its adoption to date. Basis risk arises\nfrom two fundamental sources: the intrinsic heterogeneity within an insurance\nzone (zonal risk), and the lack of predictive accuracy of the index (design\nrisk). Whereas previous work has focused almost exclusively on design risk, a\ntheoretical and empirical understanding of the role of zonal risk is still\nlacking.\n  Here we investigate the relative roles of zonal and design risk, using the\ncase of maize yields in Kenya. Our first contribution is to derive a formal\ndecomposition of basis risk, providing a simple upper bound on the insurable\nbasis risk that any index can reach within a given zone. Our second\ncontribution is to provide the first large-scale empirical analysis of the\nextent of zonal versus design risk. To do so, we use satellite estimates of\nyields at 10m resolution across Kenya, and investigate the effect of using\nsmaller zones versus using different indices. Our results show a strong local\nheterogeneity in yields, underscoring the challenge of implementing index\ninsurance in smallholder systems, and the potential benefits of low-cost yield\nmeasurement approaches that can enable more local definitions of insurance\nzones.\n"
    },
    {
        "paper_id": 2111.08631,
        "authors": "Santiago Camara",
        "title": "Spillovers of US Interest Rates: Monetary Policy & Information Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper quantifies the international spillovers of US monetary policy by\nexploiting the high-frequency movement of multiple financial assets around FOMC\nannouncements. I use the identification strategy introduced by Jarocinski &\nKaradi (2022) to identify two FOMC shocks: a pure US monetary policy and an\ninformation disclosure shock. These two FOMC shocks have intuitive and very\ndifferent international spillovers. On the one hand, a US tightening caused by\na pure US monetary policy shock leads to an economic recession, an exchange\nrate depreciation and tighter financial conditions. On the other hand, a\ntightening of US monetary policy caused by the FOMC disclosing positive\ninformation about the state of the US economy leads to an economic expansion,\nan exchange rate appreciation and looser financial conditions. Ignoring the\ndisclosure of information by the FOMC biases the impact of a US monetary policy\ntightening and may explain recent atypical findings.\n"
    },
    {
        "paper_id": 2111.08654,
        "authors": "Karl Naumann-Woleske, Max Sina Knicker, Michael Benzaquen,\n  Jean-Philippe Bouchaud",
        "title": "Exploration of the Parameter Space in Macroeconomic Agent-Based Models",
        "comments": "20 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Agent-Based Models (ABM) are computational scenario-generators, which can be\nused to predict the possible future outcomes of the complex system they\nrepresent. To better understand the robustness of these predictions, it is\nnecessary to understand the full scope of the possible phenomena the model can\ngenerate. Most often, due to high-dimensional parameter spaces, this is a\ncomputationally expensive task. Inspired by ideas coming from systems biology,\nwe show that for multiple macroeconomic models, including an agent-based model\nand several Dynamic Stochastic General Equilibrium (DSGE) models, there are\nonly a few stiff parameter combinations that have strong effects, while the\nother sloppy directions are irrelevant. This suggest an algorithm that\nefficiently explores the space of parameters by primarily moving along the\nstiff directions. We apply our algorithm to a medium-sized agent-based model,\nand show that it recovers all possible dynamics of the unemployment rate. The\napplication of this method to Agent-based Models may lead to a more thorough\nand robust understanding of their features, and provide enhanced parameter\nsensitivity analyses. Several promising paths for future research are\ndiscussed.\n"
    },
    {
        "paper_id": 2111.08805,
        "authors": "Vishwajit Hegde, Arvind S. Menon, L.A. Prashanth, and Krishna\n  Jagannathan",
        "title": "Online Estimation and Optimization of Utility-Based Shortfall Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Utility-Based Shortfall Risk (UBSR) is a risk metric that is increasingly\npopular in financial applications, owing to certain desirable properties that\nit enjoys. We consider the problem of estimating UBSR in a recursive setting,\nwhere samples from the underlying loss distribution are available\none-at-a-time. We cast the UBSR estimation problem as a root finding problem,\nand propose stochastic approximation-based estimations schemes. We derive\nnon-asymptotic bounds on the estimation error in the number of samples. We also\nconsider the problem of UBSR optimization within a parameterized class of\nrandom variables. We propose a stochastic gradient descent based algorithm for\nUBSR optimization, and derive non-asymptotic bounds on its convergence.\n"
    },
    {
        "paper_id": 2111.09032,
        "authors": "Zixin Feng and Dejian Tian",
        "title": "Optimal consumption and portfolio selection with Epstein-Zin utility\n  under general constraints",
        "comments": "32 pages, to appear in Probability, Uncertainty and Quantitative Risk",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper investigates the consumption-investment problem for an investor\nwith Epstein-Zin utility in an incomplete market. Closed, not necessarily\nconvex, constraints are imposed on strategies. The optimal consumption and\ninvestment strategies are characterized via a quadratic backward stochastic\ndifferential equation (BSDE). Due to the stochastic market environment, the\nsolution to this BSDE is unbounded and thereby the BMO argument breaks down.\nAfter establishing the martingale optimality criterion, by delicately selecting\nLyapunov functions, the verification theorem is ultimately obtained. Besides,\nseveral examples and numerical simulations for the optimal strategies are\nprovided and illustrated.\n"
    },
    {
        "paper_id": 2111.09057,
        "authors": "Vaiva Vasiliauskaite, Fabrizio Lillo, Nino Antulov-Fantulin",
        "title": "Information dynamics of price and liquidity around the 2017 Bitcoin\n  markets crash",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1063/5.0080462",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the information dynamics between the largest Bitcoin exchange\nmarkets during the bubble in 2017-2018. By analysing high-frequency\nmarket-microstructure observables with different information theoretic measures\nfor dynamical systems, we find temporal changes in information sharing across\nmarkets. In particular, we study the time-varying components of predictability,\nmemory, and synchronous coupling, measured by transfer entropy, active\ninformation storage, and multi-information. By comparing these empirical\nfindings with several models we argue that some results could relate to\nintra-market and inter-market regime shifts, and changes in direction of\ninformation flow between different market observables.\n"
    },
    {
        "paper_id": 2111.09111,
        "authors": "Jiangwei Liu and Xiaohong Huang",
        "title": "Forecasting Crude Oil Price Using Event Extraction",
        "comments": "14 pages, 5 figures, 5 tables",
        "journal-ref": "in IEEE Access, vol. 9, pp. 149067-149076, 2021",
        "doi": "10.1109/ACCESS.2021.3124802",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research on crude oil price forecasting has attracted tremendous attention\nfrom scholars and policymakers due to its significant effect on the global\neconomy. Besides supply and demand, crude oil prices are largely influenced by\nvarious factors, such as economic development, financial markets, conflicts,\nwars, and political events. Most previous research treats crude oil price\nforecasting as a time series or econometric variable prediction problem.\nAlthough recently there have been researches considering the effects of\nreal-time news events, most of these works mainly use raw news headlines or\ntopic models to extract text features without profoundly exploring the event\ninformation. In this study, a novel crude oil price forecasting framework,\nAGESL, is proposed to deal with this problem. In our approach, an open domain\nevent extraction algorithm is utilized to extract underlying related events,\nand a text sentiment analysis algorithm is used to extract sentiment from\nmassive news. Then a deep neural network integrating the news event features,\nsentimental features, and historical price features is built to predict future\ncrude oil prices. Empirical experiments are performed on West Texas\nIntermediate (WTI) crude oil price data, and the results show that our approach\nobtains superior performance compared with several benchmark methods.\n"
    },
    {
        "paper_id": 2111.0917,
        "authors": "Chao Zhang, Zihao Zhang, Mihai Cucuringu, Stefan Zohren",
        "title": "A Universal End-to-End Approach to Portfolio Optimization via Deep\n  Learning",
        "comments": "12 pages,",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a universal end-to-end framework for portfolio optimization where\nasset distributions are directly obtained. The designed framework circumvents\nthe traditional forecasting step and avoids the estimation of the covariance\nmatrix, lifting the bottleneck for generalizing to a large amount of\ninstruments. Our framework has the flexibility of optimizing various objective\nfunctions including Sharpe ratio, mean-variance trade-off etc. Further, we\nallow for short selling and study several constraints attached to objective\nfunctions. In particular, we consider cardinality, maximum position for\nindividual instrument and leverage. These constraints are formulated into\nobjective functions by utilizing several neural layers and gradient ascent can\nbe adopted for optimization. To ensure the robustness of our framework, we test\nour methods on two datasets. Firstly, we look at a synthetic dataset where we\ndemonstrate that weights obtained from our end-to-end approach are better than\nclassical predictive methods. Secondly, we apply our framework on a real-life\ndataset with historical observations of hundreds of instruments with a testing\nperiod of more than 20 years.\n"
    },
    {
        "paper_id": 2111.09192,
        "authors": "Stefan Loesch and Nate Hindman and Mark B Richardson and Nicholas\n  Welch",
        "title": "Impermanent Loss in Uniswap v3",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  AMMs are autonomous smart contracts deployed on a blockchain that make\nmarkets between different assets that live on that chain. In this paper we are\nexamining a specific class of AMMs called Constant Function Market Makers whose\ntrading profile, ignoring fees, is determined by their bonding curve. This\nclass of AMM suffers from what is commonly referred to as Impermanent Loss,\nwhich we have previously identified as the Gamma component of the associated\nself-financing trading strategy and which is the risk that LP providers wager\nagainst potential fee earnings.\n  The recent Uniswap v3 release has popularized the concept of leveraged\nliquidity provision - wherein the trading range in which liquidity is provided\nis reduced and achieves a higher degree of capital efficiency through\nelimination of unused collateral. This leverage increases the fees earned, but\nit also increases the risk taken, ie the IL. Fee levels on Uniswap v3 are well\npublicized so, in this paper, we focus on calculating the IL.\n  We found that for the 17 pools we analyzed, covering 43% of TVL and chosen by\nsize, composite tokens and data availability, total fees earned since inception\nuntil the cut-off date was $199.3m. We also found that the total IL suffered by\nLPs during this period was USD 260.1m, meaning that in aggregate those LPs\nwould have been better off by USD 60.8m had they simply HODLd.\n"
    },
    {
        "paper_id": 2111.09395,
        "authors": "Xiao-Yang Liu and Hongyang Yang and Jiechao Gao and Christina Dan Wang",
        "title": "FinRL: Deep Reinforcement Learning Framework to Automate Trading in\n  Quantitative Finance",
        "comments": "ACM International Conference on AI in Finance",
        "journal-ref": "ACM International Conference on AI in Finance, 2021",
        "doi": "10.1145/3490354.3494366",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Deep reinforcement learning (DRL) has been envisioned to have a competitive\nedge in quantitative finance. However, there is a steep development curve for\nquantitative traders to obtain an agent that automatically positions to win in\nthe market, namely \\textit{to decide where to trade, at what price} and\n\\textit{what quantity}, due to the error-prone programming and arduous\ndebugging. In this paper, we present the first open-source framework\n\\textit{FinRL} as a full pipeline to help quantitative traders overcome the\nsteep learning curve. FinRL is featured with simplicity, applicability and\nextensibility under the key principles, \\textit{full-stack framework,\ncustomization, reproducibility} and \\textit{hands-on tutoring}.\n  Embodied as a three-layer architecture with modular structures, FinRL\nimplements fine-tuned state-of-the-art DRL algorithms and common reward\nfunctions, while alleviating the debugging workloads. Thus, we help users\npipeline the strategy design at a high turnover rate. At multiple levels of\ntime granularity, FinRL simulates various markets as training environments\nusing historical data and live trading APIs. Being highly extensible, FinRL\nreserves a set of user-import interfaces and incorporates trading constraints\nsuch as market friction, market liquidity and investor's risk-aversion.\nMoreover, serving as practitioners' stepping stones, typical trading tasks are\nprovided as step-by-step tutorials, e.g., stock trading, portfolio allocation,\ncryptocurrency trading, etc.\n"
    },
    {
        "paper_id": 2111.09407,
        "authors": "Patrick Mellacher",
        "title": "Growth, Inequality and Declining Business Dynamism in a Unified\n  Schumpeter Mark I + II Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  I develop a simple Schumpeterian agent-based model where the entry and exit\nof firms, their productivity and markup, the birth of new industries and the\nsocial structure of the population are endogenous and use it to study the\ncauses of rising inequality and \"declining business dynamism\" since the 1980s.\nMy hybrid model combines features of i) the so-called Schumpeter Mark I\n(centering around the entrepreneur), ii) the Mark II model (emphasizing the\ninnovative capacities of firms), and iii) Cournot competition, with firms using\nOLS learning to estimate the market environment and the behavior of their\ncompetitors. A scenario which is quantitatively calibrated to US data on growth\nand inequality replicates a large number of stylized facts regarding the\nindustry life-cycle, growth, inequality and all ten stylized facts on\n\"declining business dynamism\" proposed by Akcigit and Ates (AEJ:Macro, 2021).\nCounterfactual simulations show that antitrust policy is highly effective at\ncombatting inequality and increasing business dynamism and growth, but is\nsubject to a conflict of interest between workers and firm owners, as GDP and\nwages grow at the expense of profits. Technological factors, on the other hand,\nare much less effective in combatting declining business dynamism in my model.\n"
    },
    {
        "paper_id": 2111.09408,
        "authors": "Patrick Mellacher",
        "title": "Opinion Dynamics with Conflicting Interests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I develop a rather simple agent-based model to capture a co-evolution of\nopinion formation, political decision making and economic outcomes. I use this\nmodel to study how societies form opinions if their members have opposing\ninterests. Agents are connected in a social network and exchange opinions, but\ndiffer with regard to their interests and ability to gain information about\nthem. I show that inequality in information and economic resources can have a\ndrastic impact on aggregated opinion. In particular, my model illustrates how a\ntiny, but well-informed minority can influence group decisions to their favor.\nThis effect is amplified if these agents are able to command more economic\nresources to advertise their views and if they can target their advertisements\nefficiently, as made possible by the rise of information technology. My results\ncontribute to the understanding of pressing questions such as climate change\ndenial and highlight the dangers that economic and information inequality can\npose for democracies.\n"
    },
    {
        "paper_id": 2111.09458,
        "authors": "Philip Protter and Alejandra Quintos",
        "title": "Stopping Times Occurring Simultaneously",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stopping times are used in applications to model random arrivals. A standard\nassumption in many models is that they are conditionally independent, given an\nunderlying filtration. This is a widely useful assumption, but there are\ncircumstances where it seems to be unnecessarily strong. We use a modified Cox\nconstruction along with the bivariate exponential introduced by Marshall and\nOlkin (1967) to create a family of stopping times, which are not necessarily\nconditionally independent, allowing for a positive probability for them to be\nequal. We show that our initial construction only allows for positive\ndependence between stopping times, but we also propose a joint distribution\nthat allows for negative dependence while preserving the property of non-zero\nprobability of equality. We indicate applications to modeling COVID-19\ncontagion (and epidemics in general), civil engineering, and to credit risk\n"
    },
    {
        "paper_id": 2111.09655,
        "authors": "Minseog Oh, Donggyu Kim",
        "title": "Effect of the U.S.--China Trade War on Stock Markets: A Financial\n  Contagion Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the effect of the U.S.--China trade war on\nstock markets from a financial contagion perspective, based on high-frequency\nfinancial data. Specifically, to account for risk contagion between the U.S.\nand China stock markets, we develop a novel jump-diffusion process. For\nexample, we consider three channels for volatility contagion--such as\nintegrated volatility, positive jump variation, and negative jump\nvariation--and each stock market is able to affect the other stock market as an\novernight risk factor. We develop a quasi-maximum likelihood estimator for\nmodel parameters and establish its asymptotic properties. Furthermore, to\nidentify contagion channels and test the existence of a structural break, we\npropose hypothesis test procedures. From the empirical study, we find evidence\nof financial contagion from the U.S. to China and evidence that the risk\ncontagion channel has changed from integrated volatility to negative jump\nvariation.\n"
    },
    {
        "paper_id": 2111.09773,
        "authors": "Francesco Cesarone, Manuel L Martino, Fabio Tardella",
        "title": "Mean-Variance-VaR portfolios: MIQP formulation and performance analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Value-at-Risk is one of the most popular risk management tools in the\nfinancial industry. Over the past 20 years several attempts to include VaR in\nthe portfolio selection process have been proposed. However, using VaR as a\nrisk measure in portfolio optimization models leads to problems that are\ncomputationally hard to solve. In view of this, few practical applications of\nVaR in portfolio selection have appeared in the literature up to now. In this\npaper, we propose to add the VaR criterion to the classical Mean-Variance\napproach in order to better address the typical regulatory constraints of the\nfinancial industry. We thus obtain a portfolio selection model characterized by\nthree criteria: expected return, variance, and VaR at a specified confidence\nlevel. The resulting optimization problem consists in minimizing variance with\nparametric constraints on the levels of expected return and VaR. This model can\nbe formulated as a Mixed-Integer Quadratic Programming (MIQP) problem. An\nextensive empirical analysis on seven real-world datasets demonstrates the\npractical applicability of the proposed approach. Furthermore, the\nout-of-sample performance of the optimal Mean-Variance-VaR portfolios seems to\nbe generally better than that of the optimal Mean-Variance and Mean-VaR\nportfolios.\n"
    },
    {
        "paper_id": 2111.09846,
        "authors": "David McCune and Lori McCune",
        "title": "The Curious Case of the 2021 Minneapolis Ward 2 City Council Election",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we explain why the November 2021 election for the Ward 2 city\ncouncil seat in Minneapolis, MN, may be the mathematically most interesting\nranked choice election in US history.\n"
    },
    {
        "paper_id": 2111.09866,
        "authors": "M.Moore",
        "title": "Collaboration in Coworking Spaces: Impact on Firm Innovativeness and\n  Business Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Purpose: The purpose of this paper is to contribute to the debate as to\nwhether collaboration in coworking spaces contributes to firm innovativeness\nand impacts the business models of organizations in a positive manner.\n  Methodology: This paper includes primary data from 75 organizations in 17\ncoworking spaces and uses quantitative research methods. The methodology\nincludes multiple statistical methods, such as principal component analysis,\ncorrelation analysis as well as linear and binary regression analysis.\n  Results: The results show a positive interrelation between collaboration and\ninnovation, indicating that coworkers are able to improve their innovative\ncapabilities by making use of strategic partnerships in coworking spaces.\nFurther, this study shows that business models are significantly affected by\nthe level of collaboration in coworking spaces, which suggests that coworking\nis a promoting force for business model development or business model\ninnovation. Contributions: The paper contributes to management literature and\nrepresents the first empirical investigations which focuses on the effects of\ncollaboration on a firm-level in coworking spaces.\n  Practical implications: The results indicate that organizations in coworking\nspaces should embrace a collaborative mindset and should actively seek out\ncollaborative alliances and partnerships, as doing such is shown to increase\ntheir innovativeness and/or develop their business model.\n  Future Research: Future research should focus on the antecedents of\ncollaboration or could investigate the effects of collaboration in coworking\nspaces on a community level.\n"
    },
    {
        "paper_id": 2111.09902,
        "authors": "Kamesh Korangi, Christophe Mues, Cristi\\'an Bravo",
        "title": "A transformer-based model for default prediction in mid-cap corporate\n  markets",
        "comments": "38 pages, 6 figures, V4 published",
        "journal-ref": "European Journal of Operational Research, 308, 306-320 (2023)",
        "doi": "10.1016/j.ejor.2022.10.032",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we study mid-cap companies, i.e. publicly traded companies\nwith less than US $10 billion in market capitalisation. Using a large dataset\nof US mid-cap companies observed over 30 years, we look to predict the default\nprobability term structure over the medium term and understand which data\nsources (i.e. fundamental, market or pricing data) contribute most to the\ndefault risk. Whereas existing methods typically require that data from\ndifferent time periods are first aggregated and turned into cross-sectional\nfeatures, we frame the problem as a multi-label time-series classification\nproblem. We adapt transformer models, a state-of-the-art deep learning model\nemanating from the natural language processing domain, to the credit risk\nmodelling setting. We also interpret the predictions of these models using\nattention heat maps. To optimise the model further, we present a custom loss\nfunction for multi-label classification and a novel multi-channel architecture\nwith differential training that gives the model the ability to use all input\ndata efficiently. Our results show the proposed deep learning architecture's\nsuperior performance, resulting in a 13% improvement in AUC (Area Under the\nreceiver operating characteristic Curve) over traditional models. We also\ndemonstrate how to produce an importance ranking for the different data sources\nand the temporal relationships using a Shapley approach specific to these\nmodels.\n"
    },
    {
        "paper_id": 2111.10033,
        "authors": "Bin Xie, Weiping Li, Nan Liang",
        "title": "Pricing S&P 500 Index Options with L\\'evy Jumps",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze various jumps for Heston model, non-IID model and three L\\'evy\njump models for S&P 500 index options. The L\\'evy jump for the S&P 500 index\noptions is inevitable from empirical studies. We estimate parameters from\nin-sample pricing through SSE for the BS, SV, SVJ, non-IID and L\\'evy (GH, NIG,\nCGMY) models by the method of Bakshi et al. (1997), and utilize them for\nout-of-sample pricing and compare these models. The sensitivities of the call\noption pricing for the L\\'evy models with respect to parameters are presented.\nEmpirically, we show that the NIG model, SV and SVJ models with estimated\nvolatilities outperform other models for both in-sample and out-of-sample\nperiods. Using the in-sample optimized parameters, we find that the NIG model\nhas the least SSE and outperforms the rest models on one-day prediction.\n"
    },
    {
        "paper_id": 2111.10164,
        "authors": "Jens Robben and Katrien Antonio and Sander Devriendt",
        "title": "Assessing the impact of the COVID-19 shock on a stochastic\n  multi-population mortality model",
        "comments": "31 pages, 15 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We aim to assess the impact of a pandemic data point on the calibration of a\nstochastic multi-population mortality projection model and its resulting\nprojections for future mortality rates. Throughout the paper we put focus on\nthe Li & Lee mortality model, which has become a standard for projecting\nmortality in Belgium and the Netherlands. We calibrate this mortality model on\nannual deaths and exposures at the level of individual ages. This type of\nmortality data is typically collected, produced and reported with a significant\ndelay of -- for some countries -- several years on a platform such as the Human\nMortality Database. To enable a timely evaluation of the impact of a pandemic\ndata point we have to rely on other data sources (e.g. the Short-Term Mortality\nFluctuations Data series) that swiftly publish weekly mortality data collected\nin age buckets. To be compliant with the design and calibration strategy of the\nLi & Lee model, we have to transform the weekly mortality data collected in age\nbuckets to yearly, age-specific observations. Therefore, our paper constructs a\nprotocol to ungroup the deaths and exposures registered in age buckets to\nindividual ages. To evaluate the impact of a pandemic shock, like COVID-19 in\nthe year 2020, we weigh this data point in either the calibration or projection\nstep. Obviously, the more weight we place on this data point, the more impact\nwe observe on future estimated mortality rates and life expectancies. Our paper\nallows to quantify this impact and provides actuaries and actuarial\nassociations with a framework to generate scenarios of future mortality under\nvarious assessments of the pandemic data point.\n"
    },
    {
        "paper_id": 2111.10301,
        "authors": "Xiyue Han and Alexander Schied",
        "title": "The roughness exponent and its model-free estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by pathwise stochastic calculus, we say that a continuous\nreal-valued function $x$ admits the roughness exponent $R$ if the\n$p^{\\text{th}}$ variation of $x$ converges to zero if $p>1/R$ and to infinity\nif $p<1/R$. For the sample paths of many stochastic processes, such as\nfractional Brownian motion, the roughness exponent exists and equals the\nstandard Hurst parameter. In our main result, we provide a mild condition on\nthe Faber--Schauder coefficients of $x$ under which the roughness exponent\nexists and is given as the limit of the classical Gladyshev estimates $\\widehat\nR_n(x)$. This result can be viewed as a strong consistency result for the\nGladyshev estimators in an entirely model-free setting, because it works\nstrictly trajectory-wise and requires no probabilistic assumptions.\nNonetheless, our proof is probabilistic and relies on a martingale that is\nhidden in the Faber--Schauder expansion of $x$. Since the Gladyshev estimators\nare not scale-invariant, we construct several scale-invariant estimators that\nare derived from the sequence $(\\widehat R_n)_{n\\in\\mathbb N}$. We also discuss\nhow a dynamic change in the roughness parameter of a time series can be\ndetected. Finally, we extend our results to the case in which the\n$p^{\\text{th}}$ variation of $x$ is defined over a sequence of unequally spaced\npartitions. Our results are illustrated by means of high-frequency financial\ntime series.\n"
    },
    {
        "paper_id": 2111.10554,
        "authors": "Dominik Grafenhofer, Wolfgang Kuhle",
        "title": "Observing Actions in Global Games",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1904.10744",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study Bayesian coordination games where agents receive noisy private\ninformation over the game's payoffs, and over each others' actions. If private\ninformation over actions is of low quality, equilibrium uniqueness obtains in a\nmanner similar to a global games setting. On the contrary, if private\ninformation over actions (and thus over the game's payoff coefficient) is\nprecise, agents can coordinate on multiple equilibria. We argue that our\nresults apply to phenomena such as bank-runs, currency crises, recessions, or\nriots and revolutions, where agents monitor each other closely.\n"
    },
    {
        "paper_id": 2111.10627,
        "authors": "Yang Hu, Zhui Zhu, Sirui Song, Xue Liu, Yang Yu",
        "title": "Calculus of Consent via MARL: Legitimating the Collaborative Governance\n  Supplying Public Goods",
        "comments": "Accepted to NeurIPS'2021 PERLS Workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public policies that supply public goods, especially those involve\ncollaboration by limiting individual liberty, always give rise to controversies\nover governance legitimacy. Multi-Agent Reinforcement Learning (MARL) methods\nare appropriate for supporting the legitimacy of the public policies that\nsupply public goods at the cost of individual interests. Among these policies,\nthe inter-regional collaborative pandemic control is a prominent example, which\nhas become much more important for an increasingly inter-connected world facing\na global pandemic like COVID-19. Different patterns of collaborative strategies\nhave been observed among different systems of regions, yet it lacks an\nanalytical process to reason for the legitimacy of those strategies. In this\npaper, we use the inter-regional collaboration for pandemic control as an\nexample to demonstrate the necessity of MARL in reasoning, and thereby\nlegitimizing policies enforcing such inter-regional collaboration. Experimental\nresults in an exemplary environment show that our MARL approach is able to\ndemonstrate the effectiveness and necessity of restrictions on individual\nliberty for collaborative supply of public goods. Different optimal policies\nare learned by our MARL agents under different collaboration levels, which\nchange in an interpretable pattern of collaboration that helps to balance the\nlosses suffered by regions of different types, and consequently promotes the\noverall welfare. Meanwhile, policies learned with higher collaboration levels\nyield higher global rewards, which illustrates the benefit of, and thus\nprovides a novel justification for the legitimacy of, promoting inter-regional\ncollaboration. Therefore, our method shows the capability of MARL in\ncomputationally modeling and supporting the theory of calculus of consent,\ndeveloped by Nobel Prize winner J. M. Buchanan.\n"
    },
    {
        "paper_id": 2111.11016,
        "authors": "Koichi Miyamoto",
        "title": "Quantum algorithms for numerical differentiation of expected values with\n  respect to parameters",
        "comments": "17 pages, no figure, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The quantum algorithms for Monte Carlo integration (QMCI), which are based on\nquantum amplitude estimation (QAE), speed up expected value calculation\ncompared with classical counterparts, and have been widely investigated along\nwith their applications to industrial problems such as financial derivative\npricing. In this paper, we consider an expected value of a function of a\nstochastic variable and a real-valued parameter, and how to calculate\nderivatives of the expectation with respect to the parameter. This problem is\nrelated to calculating sensitivities of financial derivatives, and so of\nindustrial importance. Based on QMCI and the general-order central difference\nformula for numerical differentiation, we propose two quantum methods for this\nproblem, and evaluate their complexities. The first one, which we call the\nnaive iteration method, simply calculates the formula by iterative computations\nand additions of the terms in it, and then estimates its expected value by QAE.\nThe second one, which we name the sum-in-QAE method, performs the summation of\nthe terms at the same time as the sum over the possible values of the\nstochastic variable in a single QAE. We see that, depending on the smoothness\nof the function and the number of qubits available, either of two methods is\nbetter than the other. In particular, when the function is nonsmooth or we want\nto save the qubit number, the sum-in-QAE method can be advantageous.\n"
    },
    {
        "paper_id": 2111.11022,
        "authors": "Nick James and Kevin Chin",
        "title": "On the systemic nature of global inflation, its association with equity\n  markets and financial portfolio implications",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.126895",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper uses new and recently introduced mathematical techniques to\nundertake a data-driven study on the systemic nature of global inflation. We\nstart by investigating country CPI inflation over the past 70 years. There, we\nhighlight the systemic nature of global inflation with a judicious application\nof eigenvalue analysis and determine which countries exhibit most \"centrality\"\nwith an inner-product based optimization method. We then turn to inflationary\nimpacts on financial market securities, where we explore country equity\nindices' equity robustness and the varied performance of equity sectors during\nperiods of significant inflationary pressure. Finally, we implement a\ntime-varying portfolio optimization to determine which asset classes were most\nbeneficial in increasing portfolio Sharpe ratio when an investor must hold a\ncore (and constant) allocation to equities.\n"
    },
    {
        "paper_id": 2111.11128,
        "authors": "Matthieu Garcin and Maxime L. D. Nicolas",
        "title": "Nonparametric estimator of the tail dependence coefficient: balancing\n  bias and variance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A theoretical expression is derived for the mean squared error of a\nnonparametric estimator of the tail dependence coefficient, depending on a\nthreshold that defines which rank delimits the tails of a distribution. We\npropose a new method to optimally select this threshold. It combines the\ntheoretical mean squared error of the estimator with a parametric estimation of\nthe copula linking observations in the tails. Using simulations, we compare\nthis semiparametric method with other approaches proposed in the literature,\nincluding the plateau-finding algorithm.\n"
    },
    {
        "paper_id": 2111.11211,
        "authors": "Jos\\'e Alejandro Mendoza, Faustino Prieto, Jos\\'e Mar\\'ia Sarabia",
        "title": "Inequality in the use frequency of patent technology codes",
        "comments": "This is a preprint (18 pages, 2 tables, 7 figures)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technology codes are assigned to each patent for classification purposes and\nto identify the components of its novelty. Not all the technology codes are\nused with the same frequency - if we study the use frequency of codes in a\nyear, we can find predominant technologies used in many patents and technology\ncodes not so frequent as part of a patent. In this paper, we measure that\ninequality in the use frequency of patent technology codes. First, we analyze\nthe total inequality in that use frequency considering the patent applications\nfiled under the Patent Co-operation Treaty at international phase, with the\nEuropean Patent Office as designated office, in the period 1977-2018, on a\nyearly basis. Then, we analyze the decomposition of that inequality by grouping\nthe technology codes by productive economic activities. We show that total\ninequality had an initial period of growth followed by a phase of relative\nstabilization, and that it tends to be persistently high. We also show that\ntotal inequality was mainly driven by inequality within productive economic\nactivities, with a low contribution of the between-activities component.\n"
    },
    {
        "paper_id": 2111.11232,
        "authors": "Yanwei Jia and Xun Yu Zhou",
        "title": "Policy Gradient and Actor-Critic Learning in Continuous Time and Space:\n  Theory and Algorithms",
        "comments": "52 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study policy gradient (PG) for reinforcement learning in continuous time\nand space under the regularized exploratory formulation developed by Wang et\nal. (2020). We represent the gradient of the value function with respect to a\ngiven parameterized stochastic policy as the expected integration of an\nauxiliary running reward function that can be evaluated using samples and the\ncurrent value function. This effectively turns PG into a policy evaluation (PE)\nproblem, enabling us to apply the martingale approach recently developed by Jia\nand Zhou (2021) for PE to solve our PG problem. Based on this analysis, we\npropose two types of the actor-critic algorithms for RL, where we learn and\nupdate value functions and policies simultaneously and alternatingly. The first\ntype is based directly on the aforementioned representation which involves\nfuture trajectories and hence is offline. The second type, designed for online\nlearning, employs the first-order condition of the policy gradient and turns it\ninto martingale orthogonality conditions. These conditions are then\nincorporated using stochastic approximation when updating policies. Finally, we\ndemonstrate the algorithms by simulations in two concrete examples.\n"
    },
    {
        "paper_id": 2111.11256,
        "authors": "Jos\\'e Manuel Aburto, Ugofilippo Basellini, Annette Baudisch,\n  Francisco Villavicencio",
        "title": "Drewnowski's index to measure lifespan variation: Revisiting the Gini\n  coefficient of the life table",
        "comments": "28 pages, 5 figures",
        "journal-ref": "Theor. Popul. Biol. 148 (2022) 1-10",
        "doi": "10.1016/j.tpb.2022.08.003",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Gini coefficient of the life table is a concentration index that provides\ninformation on lifespan variation. Originally proposed by economists to measure\nincome and wealth inequalities, it has been widely used in population studies\nto investigate variation in ages at death. We focus on a complementary\nindicator, Drewnowski's index, which is as a measure of equality. We study its\nmathematical properties and analyze how changes over time relate to changes in\nlife expectancy. Further, we identify the threshold age below which mortality\nimprovements are translated into decreasing lifespan variation and above which\nthese improvements translate into increasing lifespan inequality. We illustrate\nour theoretical findings simulating scenarios of mortality improvement in the\nGompertz model. Our experiments demonstrate how Drewnowski's index can serve as\nan indicator of the shape of mortality patterns. These properties, along with\nour analytical findings, support studying lifespan variation alongside life\nexpectancy trends in multiple species.\n"
    },
    {
        "paper_id": 2111.11286,
        "authors": "Yajie Yang, Longfeng Zhao, Lin Chen, Chao Wang, Jihui Han",
        "title": "Portfolio optimization with idiosyncratic and systemic risks for\n  financial networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we propose a new multi-objective portfolio optimization with\nidiosyncratic and systemic risks for financial networks. The two risks are\nmeasured by the idiosyncratic variance and the network clustering coefficient\nderived from the asset correlation networks, respectively. We construct three\ntypes of financial networks in which nodes indicate assets and edges are based\non three correlation measures. Starting from the multi-objective model, we\nformulate and solve the asset allocation problem. We find that the optimal\nportfolios obtained through the multi-objective with networked approach have a\nsignificant over-performance in terms of return measures in an out-of-sample\nframework. This is further supported by the less drawdown during the periods of\nthe stock market fluctuating downward. According to analyzing different\ndatasets, we also show that improvements made to portfolio strategies are\nrobust.\n"
    },
    {
        "paper_id": 2111.11315,
        "authors": "Misha Perepelitsa",
        "title": "Investing in crypto: speculative bubbles and cyclic stochastic price\n  pumps",
        "comments": "12 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The problem of investing into a cryptocurrency market requires good\nunderstanding of the processes that regulate the price of the currency. In this\npaper we offer a view of a cryptocurrency market as an environment for\nrealization of a self-organized speculative scheme that results in a formation\nof a characteristic price bubble as a transient phenomenon. We use microscale,\nagent-based models to simulate the system behavior and derive macroscale ODE\nmodels to estimate such parameters as the return rate and the market value of\ninvestments. We provide the formula for the total risk of the system as a sum\nof two independent components, one being characteristic of the price bubble and\nthe other of the investor behavior.\n"
    },
    {
        "paper_id": 2111.11459,
        "authors": "Heng Z. Chen and Stephen R. Cosslett",
        "title": "Semi-nonparametric Estimation of Operational Risk Capital with Extreme\n  Loss Events",
        "comments": "There are 32 pages, including tables, figures, appendix and\n  reference. The research was presented at the MATLAB Annual Computational\n  Finance Conference, September 27-30, 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bank operational risk capital modeling using the Basel II advanced\nmeasurement approach (AMA) often lead to a counter-intuitive capital estimate\nof value at risk at 99.9% due to extreme loss events. To address this issue, a\nflexible semi-nonparametric (SNP) model is introduced using the change of\nvariables technique to enrich the family of distributions to handle extreme\nloss events. The SNP models are proved to have the same maximum domain of\nattraction (MDA) as the parametric kernels, and it follows that the SNP models\nare consistent with the extreme value theory peaks over threshold method but\nwith different shape and scale parameters from the kernels. By using the\nsimulation dataset generated from a mixture of distributions with both light\nand heavy tails, the SNP models in the Frechet and Gumbel MDAs are shown to fit\nthe tail dataset satisfactorily through increasing the number of model\nparameters. The SNP model quantile estimates at 99.9 percent are not overly\nsensitive towards the body-tail threshold change, which is in sharp contrast to\nthe parametric models. When applied to a bank operational risk dataset with\nthree Basel event types, the SNP model provides a significant improvement in\nthe goodness of fit to the two event types with heavy tails, yielding an\nintuitive capital estimate that is in the same magnitude as the event type\ntotal loss. Since the third event type does not have a heavy tail, the\nparametric model yields an intuitive capital estimate, and the SNP model cannot\nprovide additional improvement. This research suggests that the SNP model may\nenable banks to continue with the AMA or its partial use to obtain an intuitive\noperational risk capital estimate when the simple non-model based Basic\nIndicator Approach or Standardized Approach are not suitable per Basel\nCommittee Banking Supervision OPE10 (2019).\n"
    },
    {
        "paper_id": 2111.11609,
        "authors": "Sidharth Mallik",
        "title": "Pricing cryptocurrencies : Modelling the ETHBTC spot-quotient variation\n  as a diffusion process",
        "comments": "6 tables, under journal submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This research proposes a model for the intraday variation between the ETHBTC\nspot and the quotient of ETHUSDT and BTCUSDT traded on Binance. Under\nconditions of no-arbitrage, perfect accuracy and no microstructure effects, the\nvariation must be equal to its theoretically computed value of 0. We conduct\nour research on 4 years of data. We find that the variation is not constantly\n0. The variation shows a fluctuating behaviour on either side of 0.\nFurthermore, the deviations tend to be larger in the first year than the rest\nof the years. We test the sample for the nature of diffusion where we find\nevidence of mean-reversion. We model the variation using an Ornstein-Uhlenbeck\nprocess. A maximum likelihood estimation procedure is used. From the accuracy\nof the sampling distribution of the parameters obtained, we conclude that the\nvariation may be accurately modelled as an Ornstein-Uhlenbeck process. From the\nparameters obtained, the long-term mean is shown to have a negative sign and\ndiffers from the theoretical value of 0 at 1e-05 precision. We take note of the\nresults in light of efficiency of the markets to price publicly known\ninformation.\n"
    },
    {
        "paper_id": 2111.11875,
        "authors": "Kamalanathan Ganesan, Jo\\~ao Tom\\'e Saraiva and Ricardo J. Bessa",
        "title": "Functional Model of Residential Consumption Elasticity under Dynamic\n  Tariffs",
        "comments": "28 pages, 19 figures, journal paper - Elsevier: Energy & Buildings",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  One of the major barriers for the retailers is to understand the consumption\nelasticity they can expect from their contracted demand response (DR) clients.\nThe current trend of DR products provided by retailers are not\nconsumer-specific, which poses additional barriers for the active engagement of\nconsumers in these programs. The elasticity of consumers demand behavior varies\nfrom individual to individual. The utility will benefit from knowing more\naccurately how changes in its prices will modify the consumption pattern of its\nclients. This work proposes a functional model for the consumption elasticity\nof the DR contracted consumers. The model aims to determine the load adjustment\nthe DR consumers can provide to the retailers or utilities for different price\nlevels. The proposed model uses a Bayesian probabilistic approach to identify\nthe actual load adjustment an individual contracted client can provide for\ndifferent price levels it can experience. The developed framework provides the\nretailers or utilities with a tool to obtain crucial information on how an\nindividual consumer will respond to different price levels. This approach is\nable to quantify the likelihood with which the consumer reacts to a DR signal\nand identify the actual load adjustment an individual contracted DR client\nprovides for different price levels they can experience. This information can\nbe used to maximize the control and reliability of the services the retailer or\nutility can offer to the System Operators.\n"
    },
    {
        "paper_id": 2111.12237,
        "authors": "Ron van der Meyden",
        "title": "A Game Theoretic Analysis of Liquidity Events in Convertible Instruments",
        "comments": "28 pp",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Convertible instruments are contracts, used in venture financing, which give\ninvestors the right to receive shares in the venture in certain circumstances.\nIn liquidity events, investors may have the option to either receive back their\nprincipal investment, or to receive a proportional payment after conversion of\nthe contract to a shareholding. In each case, the value of the payment may\ndepend on the choices made by other investors who hold such convertible\ncontracts. A liquidity event therefore sets up a game theoretic optimization\nproblem. The paper defines a general model for such games, which is shown to\ncover all instances of the Y Combinator Simple Agreement for Future Equity\n(SAFE) contracts, a type of convertible instrument that is commonly used to\nfinance startup ventures. The paper shows that, in general, pure strategy Nash\nequilibria do not necessarily exist in this model, and there may not exist an\noptimum pure strategy Nash equilibrium in cases where pure strategy Nash\nequilibria do exist. However, it is shown when all contracts are uniformly one\nof the SAFE contract types, an optimum pure strategy Nash equilibrium exists.\nPolynomial time algorithms for computing (optimum) pure strategy Nash\nequilibria in these cases are developed.\n"
    },
    {
        "paper_id": 2111.12248,
        "authors": "Jiarui Chu and Ludovic Tangpi",
        "title": "Non-asymptotic estimation of risk measures using stochastic gradient\n  Langevin dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we will study the approximation of arbitrary law invariant risk\nmeasures. As a starting point, we approximate the average value at risk using\nstochastic gradient Langevin dynamics, which can be seen as a variant of the\nstochastic gradient descent algorithm. Further, the Kusuoka's spectral\nrepresentation allows us to bootstrap the estimation of the average value at\nrisk to extend the algorithm to general law invariant risk measures. We will\npresent both theoretical, non-asymptotic convergence rates of the approximation\nalgorithm and numerical simulations.\n"
    },
    {
        "paper_id": 2111.12459,
        "authors": "Michael J. B\\\"ohm and Hans-Martin von Gaudecker",
        "title": "The Performance of Recent Methods for Estimating Skill Prices in Panel\n  Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores different methods to estimate prices paid per efficiency\nunit of labor in panel data. We study the sensitivity of skill price estimates\nto different assumptions regarding workers' choice problem, identification\nstrategies, the number of occupations considered, skill accumulation processes,\nand estimation strategies. In order to do so, we conduct careful Monte Carlo\nexperiments designed to generate similar features as in German panel data. We\nfind that once skill accumulation is appropriately modelled, skill price\nestimates are generally robust to modelling choices when the number of\noccupations is small, i.e., switches between occupations are rare. When\nswitching is important, subtle issues emerge and the performance of different\nmethods varies more strongly.\n"
    },
    {
        "paper_id": 2111.12509,
        "authors": "Nikitas Stamatopoulos, Guglielmo Mazzola, Stefan Woerner and William\n  J. Zeng",
        "title": "Towards Quantum Advantage in Financial Market Risk using Quantum\n  Gradient Algorithms",
        "comments": null,
        "journal-ref": "Quantum 6, 770 (2022)",
        "doi": "10.22331/q-2022-07-20-770",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a quantum algorithm to compute the market risk of financial\nderivatives. Previous work has shown that quantum amplitude estimation can\naccelerate derivative pricing quadratically in the target error and we extend\nthis to a quadratic error scaling advantage in market risk computation. We show\nthat employing quantum gradient estimation algorithms can deliver a further\nquadratic advantage in the number of the associated market sensitivities,\nusually called greeks. By numerically simulating the quantum gradient\nestimation algorithms on financial derivatives of practical interest, we\ndemonstrate that not only can we successfully estimate the greeks in the\nexamples studied, but that the resource requirements can be significantly lower\nin practice than what is expected by theoretical complexity bounds. This\nadditional advantage in the computation of financial market risk lowers the\nestimated logical clock rate required for financial quantum advantage from\nChakrabarti et al. [Quantum 5, 463 (2021)] by a factor of ~7, from 50MHz to\n7MHz, even for a modest number of greeks by industry standards (four).\nMoreover, we show that if we have access to enough resources, the quantum\nalgorithm can be parallelized across 60 QPUs, in which case the logical clock\nrate of each device required to achieve the same overall runtime as the serial\nexecution would be ~100kHz. Throughout this work, we summarize and compare\nseveral different combinations of quantum and classical approaches that could\nbe used for computing the market risk of financial derivatives.\n"
    },
    {
        "paper_id": 2111.12532,
        "authors": "Taras Bodnar, Nestor Parolya and Erik Thors\\'en",
        "title": "Is the empirical out-of-sample variance an informative risk measure for\n  the high-dimensional portfolios?",
        "comments": "21 pages, 5 figures",
        "journal-ref": "Finance Research Letters, Volume 54, June 2023, 103807",
        "doi": "10.1016/j.frl.2023.103807",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main contribution of this paper is the derivation of the asymptotic\nbehaviour of the out-of-sample variance, the out-of-sample relative loss, and\nof their empirical counterparts in the high-dimensional setting, i.e., when\nboth ratios $p/n$ and $p/m$ tend to some positive constants as $m\\to\\infty$ and\n$n\\to\\infty$, where $p$ is the portfolio dimension, while $n$ and $m$ are the\nsample sizes from the in-sample and out-of-sample periods, respectively. The\nresults are obtained for the traditional estimator of the global minimum\nvariance (GMV) portfolio, for the two shrinkage estimators introduced by\n\\cite{frahm2010} and \\cite{bodnar2018estimation}, and for the equally-weighted\nportfolio, which is used as a target portfolio in the specification of the two\nconsidered shrinkage estimators. We show that the behaviour of the empirical\nout-of-sample variance may be misleading is many practical situations. On the\nother hand, this will never happen with the empirical out-of-sample relative\nloss, which seems to provide a natural normalization of the out-of-sample\nvariance in the high-dimensional setup. As a result, an important question\narises if this risk measure can safely be used in practice for portfolios\nconstructed from a large asset universe.\n"
    },
    {
        "paper_id": 2111.12564,
        "authors": "Aihua Li",
        "title": "Conditional Estimates of Diffusion Processes for Evaluating the Positive\n  Feedback Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Positive feedback trading, which buys when prices rise and sells when prices\nfall, has long been criticized for being destabilizing as it moves prices away\nfrom the fundamentals. Motivated by the relationship between positive feedback\ntrading and investors cognitive bias, this paper provides a quantitative\nmeasurement of the bias based on the conditional estimates of diffusion\nprocesses. We prove the asymptotic properties of the estimates, which helps to\ninterpret the investment behaviors that if a feedback trader finds a security\nperform better than his expectation, he will expect the future return to be\nhigher, while in the long term, this bias will converge to zero. Furthermore,\nthe observed deviations between the return forecast and its realized value lead\nto adaptive expectations in reality, for which we raise an exponential\nsmoothing model as an adjustment method. In the empirical study on the stock\nmarket in China, we show the effectiveness of the ES method in bringing the\nbiased expectation closer to the fundamental level, and suggest that the\nfeedback traders, who are often over-optimistic about the return, are likely to\nsuffer from downside risk and aggravate the speculative bubbles in the market.\n"
    },
    {
        "paper_id": 2111.1264,
        "authors": "Olaf Dreyer, Horst K\\\"ohler, Thomas Streuer",
        "title": "Completing correlation matrices",
        "comments": "16 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a way to complete a correlation matrix that is not fully\nspecified. Such matrices often arise in financial applications when the number\nof stochastic variables becomes large or when several smaller models are\ncombined in a larger model. We argue that the proper completion to consider is\nthe matrix that maximizes the entropy of the distribution described by the\nmatrix. We then give a way to construct this matrix starting from the graph\nassociated with the incomplete matrix. If this graph is chordal our\nconstruction will result in a proper correlation matrix. We give a detailed\ndescription of the construction for a cross-currency model with six stochastic\nvariables and describe extensions to larger models involving more currencies.\n"
    },
    {
        "paper_id": 2111.12658,
        "authors": "Jonathan Raimana Chan, Thomas Huckle, Antoine Jacquier, Aitor Muguruza",
        "title": "Portfolio optimisation with options",
        "comments": "25 pages, 18 pictures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new analysis for portfolio optimisation with options, tackling\nthe three fundamental issues with this problem: asymmetric options'\ndistributions, high dimensionality and dependence structure. To do so, we\npropose a new dependency matrix, built upon conditional probabilities between\noptions' payoffs, and show how it can be computed in closed form given a copula\nstructure of the underlying asset prices. The empirical evidence we provide\nhighlights that this approach is efficient, fast and easily scalable to large\nportfolios of (mixed) options.\n"
    },
    {
        "paper_id": 2111.12799,
        "authors": "Francesco Furno",
        "title": "The Macroeconomic Effects of Corporate Tax Reforms",
        "comments": "37 pages, 13 figures, replication material at\n  https://ffurno.github.io/JMP_Corporate_Tax_Reforms_Replication.zip",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper extends a standard general equilibrium framework with a corporate\ntax code featuring two key elements: tax depreciation policy and the\ndistinction between c-corporations and pass-through businesses. In the model,\nthe stimulative effect of a tax rate cut on c-corporations is smaller when tax\ndepreciation policy is accelerated, and is further diluted in the aggregate by\nthe presence of pass-through entities. Because of a highly accelerated tax\ndepreciation policy and a large share of pass-through activity in 2017, the\nmodel predicts small stimulus, large payouts to shareholders, and a dramatic\nloss of corporate tax revenues following the Tax Cuts and Jobs Act (TCJA-17).\nThese predictions are consistent with novel micro- and macro-level evidence\nfrom professional forecasters and sectoral tax returns. At the same time,\nbecause of less-accelerated tax depreciation and a lower pass-through share in\nthe early 1960s, the model predicts sizable stimulus in response to the\nKennedy's corporate tax cuts - also supported by the data. The model-implied\ncorporate tax multipliers for Trump's TCJA-17 and Kennedy's tax cuts are +0.6\nand +2.5, respectively.\n"
    },
    {
        "paper_id": 2111.1283,
        "authors": "Anibal Sanjab, H\\'el\\`ene Le Cadre, Yuting Mou",
        "title": "TSO-DSOs Stable Cost Allocation for the Joint Procurement of\n  Flexibility: A Cooperative Game Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a transmission-distribution systems flexibility market is\nintroduced, in which system operators (SOs) jointly procure flexibility from\ndifferent systems to meet their needs (balancing and congestion management)\nusing a common market. This common market is, then, formulated as a cooperative\ngame aiming at identifying a stable and efficient split of costs of the jointly\nprocured flexibility among the participating SOs to incentivize their\ncooperation. The non-emptiness of the core of this game is then mathematically\nproven, implying the stability of the game and the naturally-arising incentive\nfor cooperation among the SOs. Several cost allocation mechanisms are then\nintroduced, while characterizing their mathematical properties. Numerical\nresults focusing on an interconnected system (composed of the IEEE 14-bus\ntransmission system and the Matpower 18-bus, 69-bus, and 141-bus distributions\nsystems) showcase the cooperation-induced reduction in system-wide flexibility\nprocurement costs, and identifies the varying costs borne by different SOs\nunder various cost allocations methods.\n"
    },
    {
        "paper_id": 2111.12967,
        "authors": "Julian Jetses and Marcus C. Christiansen",
        "title": "A General Surplus Decomposition Principle in Life Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In with-profit life insurance, the prudent valuation of future insurance\nliabilities leads to systematic surplus that mainly belongs to the\npolicyholders and is redistributed as bonus. For a fair and lawful\nredistribution of surplus the insurer needs to decompose the total portfolio\nsurplus with respect to the contributions of individual policies and with\nrespect to different risk sources. For this task, actuaries have a number of\nheuristic decomposition formulas, but an overarching decomposition principle is\nstill missing. This paper fills that gap by introducing a so-called ISU\ndecomposition principle that bases on infinitesimal sequential updates of the\ninsurer's valuation basis. It is shown that the existing heuristic\ndecomposition formulas can be replicated as ISU decompositions. Furthermore,\nalternative decomposition principles and their relation to the ISU\ndecomposition principle are discussed. The generality of the ISU concept makes\nit a useful tool also beyond classical surplus decompositions in life\ninsurance.\n"
    },
    {
        "paper_id": 2111.13109,
        "authors": "Christian Bongiorno, Damien Challet, Gr\\'egoire Loeper",
        "title": "Cleaning the covariance matrix of strongly nonstationary systems with\n  time-independent eigenvalues",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a data-driven way to reduce the noise of covariance matrices of\nnonstationary systems. In the case of stationary systems, asymptotic approaches\nwere proved to converge to the optimal solutions. Such methods produce\neigenvalues that are highly dependent on the inputs, as common sense would\nsuggest. Our approach proposes instead to use a set of eigenvalues totally\nindependent from the inputs and that encode the long-term averaging of the\ninfluence of the future on present eigenvalues. Such an influence can be the\npredominant factor in nonstationary systems. Using real and synthetic data, we\nshow that our data-driven method outperforms optimal methods designed for\nstationary systems for the filtering of both covariance matrix and its inverse,\nas illustrated by financial portfolio variance minimization, which makes out\nmethod generically relevant to many problems of multivariate inference.\n"
    },
    {
        "paper_id": 2111.13164,
        "authors": "Luxuan Yang, Ting Gao, Yubin Lu, Jinqiao Duan and Tao Liu",
        "title": "Neural network stochastic differential equation models with applications\n  to financial data forecasting",
        "comments": "18 pages, 38 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, we employ a collection of stochastic differential equations\nwith drift and diffusion coefficients approximated by neural networks to\npredict the trend of chaotic time series which has big jump properties. Our\ncontributions are, first, we propose a model called L\\'evy induced stochastic\ndifferential equation network, which explores compounded stochastic\ndifferential equations with $\\alpha$-stable L\\'evy motion to model complex time\nseries data and solve the problem through neural network approximation. Second,\nwe theoretically prove that the numerical solution through our algorithm\nconverges in probability to the solution of corresponding stochastic\ndifferential equation, without curse of dimensionality. Finally, we illustrate\nour method by applying it to real financial time series data and find the\naccuracy increases through the use of non-Gaussian L\\'evy processes. We also\npresent detailed comparisons in terms of data patterns, various models,\ndifferent shapes of L\\'evy motion and the prediction lengths.\n"
    },
    {
        "paper_id": 2111.13228,
        "authors": "Wujiang Lou",
        "title": "Securities Lending Haircuts and Indemnification Pricing",
        "comments": "17 pages, 2 figures, 4 tables, to appear in Risk",
        "journal-ref": null,
        "doi": "10.2139/ssrn.3682930",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Securities borrowing and lending are critical to proper functioning of\nsecurities markets. To alleviate securities owners' exposure to borrower\ndefault risk, overcollateralization and indemnification are provided by the\nborrower and the lending agent respectively. Haircuts as the level of\novercollateralization and the cost of indemnification are naturally\ninterrelated: the higher haircut is, the lower cost shall become. This article\npresents a method of quantifying their relationship. Borrower dependent\nhaircuts satisfying the lender's credit risk appetite are computed for US\nTreasuries and main equities by applying a repo haircut model to bilateral\nsecurities lending transactions. Indemnification is designed to fulfill a\ntriple-A risk appetite when the transaction haircut fails to deliver. The cost\nof indemnification consists of a risk charge, a capital charge, and a funding\ncharge, each corresponding to the expected loss, the economic capital, and the\nredundant fund needed to arrive at the triple-A haircut.\n"
    },
    {
        "paper_id": 2111.13334,
        "authors": "Bin Xie, Weiping Li",
        "title": "The Parameter Sensitivities of a Jump-diffusion Process in Basic Credit\n  Risk Analysis",
        "comments": "9 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We detect the parameter sensitivities of bond pricing which is driven by a\nBrownian motion and a compound Poisson process as the discontinuous case in\ncredit risk research. The strict mathematical deductions are given\ntheoretically due to the explicit call price formula. Furthermore, we\nillustrate Matlab simulation to verify these conclusions.\n"
    },
    {
        "paper_id": 2111.13443,
        "authors": "S\\\"oren Christensen, Albrecht Irle, Julian Peter Lemburg",
        "title": "Flexible forward improvement iteration for infinite time horizon\n  Markovian optimal stopping problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose an extension of the forward improvement iteration\nalgorithm, originally introduced in Irle (2006) and recently reconsidered in\nMiclo and Villeneuve (2021). The main new ingredient is a flexible window\nparameter describing the look-ahead distance in the improvement step. We\nconsider the framework of a Markovian optimal stopping problem in discrete time\nwith random discounting and infinite time horizon. We prove convergence and\nshow that the additional flexibility may significantly reduce the runtime.\n"
    },
    {
        "paper_id": 2111.13519,
        "authors": "Edward Turner",
        "title": "Graph Auto-Encoders for Financial Clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Deep learning has shown remarkable results on Euclidean data (e.g. audio,\nimages, text) however this type of data is limited in the amount of relational\ninformation it can hold. In mathematics we can model more general relational\ndata in a graph structure while retaining Euclidean data as associated node or\nedge features. Due to the ubiquity of graph data, and its ability to hold\nmultiple dimensions of information, graph deep learning has become a fast\nemerging field. We look at applying and optimising graph deep learning on a\nfinance graph to produce more informed clusters of companies. Having clusters\nproduced from multiple streams of data can be highly useful in quantitative\nfinance; not only does it allow clusters to be tailored to the specific task\nbut the culmination of multiple streams allows for cross source pattern\nrecognition that would have otherwise gone unnoticed. This can provide\nfinancial institutions with an edge over competitors which is crucial in the\nheavily optimised world of trading. In this paper we use news co-occurrence and\nstock price for our data combination. We optimise our model to achieve an\naverage testing precision of 78% and find a clear improvement in clustering\ncapabilities when dual data sources are used; cluster purity rises from 32% for\njust vertex data and 42% for just edge data to 64% when both are used in\ncomparisons to ground-truth Bloomberg clusters. The framework we provide\nutilises unsupervised learning which we view as key for future work due to the\nvolume of unlabelled data in financial markets.\n"
    },
    {
        "paper_id": 2111.1369,
        "authors": "Maria A. Shishanina, Anatoly A. Sidorov",
        "title": "Management of Social and Economic Development of Municipalities",
        "comments": "2 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper discusses the process of social and economic development of\nmunicipalities. A conclusion is made that developing an adequate model of\nsocial and economic development using conventional approaches presents a\nconsiderable challenge. It is proposed to use semantic modeling to represent\nthe social and economic development of municipalities, and cognitive mapping to\nidentify the set of connections that occur among indicators and that have a\ndirect impact on social and economic development.\n"
    },
    {
        "paper_id": 2111.13692,
        "authors": "Martin Popp",
        "title": "Minimum Wages in Concentrated Labor Markets",
        "comments": "112 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Economists increasingly refer to monopsony power to reconcile the absence of\nnegative employment effects of minimum wages with theory. However, systematic\nevidence for the monopsony argument is scarce. In this paper, I perform a\ncomprehensive test of monopsony theory by using labor market concentration as a\nproxy for monopsony power. Labor market concentration turns out substantial in\nGermany. Absent wage floors, a 10 percent increase in labor market\nconcentration makes firms reduce wages by 0.5 percent and employment by 1.6\npercent, reflecting monopsonistic exploitation. In line with perfect\ncompetition, sectoral minimum wages lead to negative employment effects in\nslightly concentrated labor markets. This effect weakens with increasing\nconcentration and, ultimately, becomes positive in highly concentrated or\nmonopsonistic markets. Overall, the results lend empirical support to the\nmonopsony argument, implying that conventional minimum wage effects on\nemployment conceal heterogeneity across market forms.\n"
    },
    {
        "paper_id": 2111.1374,
        "authors": "Guillermo Angeris, Alex Evans, Tarun Chitra",
        "title": "Replicating Monotonic Payoffs Without Oracles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we show that any monotonic payoff can be replicated using only\nliquidity provider shares in constant function market makers (CFMMs), without\nthe need for additional collateral or oracles. Such payoffs include\ncash-or-nothing calls and capped calls, among many others, and we give an\nexplicit method for finding a trading function matching these payoffs. For\nexample, this method provides an easy way to show that the trading function for\nmaintaining a portfolio where 50% of the portfolio is allocated in one asset\nand 50% in the other is exactly the constant product market maker (e.g.,\nUniswap) from first principles. We additionally provide a simple formula for\nthe total earnings of an arbitrageur who is arbitraging against these CFMMs.\n"
    },
    {
        "paper_id": 2111.13901,
        "authors": "Leonardo Perotti, Lech A. Grzelak",
        "title": "Fast Sampling from Time-Integrated Bridges using Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a methodology to sample from time-integrated stochastic bridges,\nnamely random variables defined as $\\int_{t_1}^{t_2} f(Y(t))dt$ conditioned on\n$Y(t_1)\\!=\\!a$ and $Y(t_2)\\!=\\!b$, with $a,b\\in R$. The Stochastic Collocation\nMonte Carlo sampler and the Seven-League scheme are applied for this purpose.\nNotably, the distribution of the time-integrated bridge is approximated\nutilizing a polynomial chaos expansion built on a suitable set of stochastic\ncollocation points. Furthermore, artificial neural networks are employed to\nlearn the collocation points. The result is a robust, data-driven procedure for\nthe Monte Carlo sampling from conditional time-integrated processes, which\nguarantees high accuracy and generates thousands of samples in milliseconds.\nApplications, with a focus on finance, are presented here as well.\n"
    },
    {
        "paper_id": 2111.14502,
        "authors": "Anna Aksamit, Ivan Guo, Shidan Liu, Zhou Zhou",
        "title": "Superhedging duality for multi-action options under model uncertainty\n  with information delay",
        "comments": "23",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the superhedging price of an exotic option under nondominated\nmodel uncertainty in discrete time in which the option buyer chooses some\naction from an (uncountable) action space at each time step. By introducing an\nenlarged space we reformulate the superhedging problem for such an exotic\noption as a problem for a European option, which enables us to prove the\npricing-hedging duality. Next, we present a duality result that, when the\noption buyers action is observed by the seller up to $l$ periods later, the\nsuperhedging price equals the model-based price where the option buyer has the\npower to look into the future for $l$ periods.\n"
    },
    {
        "paper_id": 2111.14521,
        "authors": "Selina Gangl",
        "title": "Do soda taxes affect the consumption and health of school-aged children?\n  Evidence from France and Hungary",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the effect of two different soda taxes on consumption\nbehaviour and health of school-aged children in Europe: Hungary imposed a\nPublic Health Product Tax (PHPT) on several unhealthy products in 2011. France\nintroduced solely a soda tax, containing sugar or artificial sweeteners, in\n2012. In order to exploit spatial variation, I use a semi-parametric\nDifference-in-Differences (DID) approach. Since the policies differ in Hungary\nand France, I analyse the effects separately by using a neighbouring country\nwithout a soda tax as a control group. The results suggest a counter-intuitive\npositive effect of the tax on soda consumption in Hungary. The reason for this\nfinding could be the substitution of other unhealthy beverages, which are taxed\nat a higher rate, by sodas. The effect of the soda tax in France is as expected\nnegative, but insignificant which might be caused by a low tax rate. The body\nmass index (BMI) is not affected by the tax in any country. Consequently,\npolicy makers should think carefully about the design and the tax rate before\nimplementing a soda tax.\n"
    },
    {
        "paper_id": 2111.14524,
        "authors": "Selina Gangl and Martin Huber",
        "title": "From homemakers to breadwinners? How mandatory kindergarten affects\n  maternal labour market outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyse the effect of mandatory kindergarten attendance for four-year-old\nchildren on maternal labour market outcomes in Switzerland. To determine the\ncausal effect of this policy, we combine two different datasets and\nquasi-experiments in this paper: Firstly, we investigate a large administrative\ndataset and apply a non-parametric regression discontinuity design (RDD) to\nevaluate the effect of the reform at the birthday cut-off for entering the\nkindergarten in the same versus in the following year. Secondly, we complement\nthis analysis by exploiting spatial variation and staggered treatment\nimplementation of the reform across cantons (administrative units in\nSwitzerland) in a difference-in-differences (DiD) approach based on a household\nsurvey. All in all, the results suggest that if anything, mandatory\nkindergarten increases the labour market outcomes of mothers very moderately.\nThe effects are driven by previous non-employed mothers and by older rather\nthan younger mothers.\n"
    },
    {
        "paper_id": 2111.14613,
        "authors": "Hannes Wallimann, Kevin Bl\\\"attler and Widar von Arx",
        "title": "Do price reductions attract customers in urban public transport? A\n  synthetic control approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we assess the demand effects of lower public transport fares\nin Geneva, an urban area in Switzerland. Considering a unique sample based on\ntransport companies' annual reports, we find that, when reducing the costs of\nannual season tickets, day tickets and hourly tickets (by up to 29%, 6% and\n20%, respectively), demand increases by, on average, over five years, about\n10.6%. To the best of our knowledge, we are the first to show how the synthetic\ncontrol method (Abadie and Gardeazabal, 2003, Abadie, Diamond, and Hainmueller,\n2010) can be used to assess such (for policy-makers) important price reduction\neffects in urban public transport. Furthermore, we propose an aggregate metric\nthat inherits changes in public transport supply (e.g., frequency increases) to\nassess these demand effects, namely passenger trips per vehicle kilometre. This\nmetric helps us to isolate the impact of price reductions by ensuring that\ncompanies' frequency increases do not affect estimators of interest. In\naddition, we show how to investigate the robustness of results in similar\nsettings. Using a recent statistical method and a different study design, i.e.,\nnot blocking off supply changes as an alternate explanation of the effect,\nleads us to a lower bound of the effect, amounting to an increase of 3.7%.\nFinally, as far as we know, it is the first causal estimate of price reduction\non urban public transport initiated by direct democracy.\n"
    },
    {
        "paper_id": 2111.1462,
        "authors": "Siyuan Liu and Mehmet Orcun Yalcin and Hsuan Fu and Xiuyi Fan",
        "title": "An Investigation of the Impact of COVID-19 Non-Pharmaceutical\n  Interventions and Economic Support Policies on Foreign Exchange Markets with\n  Explainable AI Techniques",
        "comments": "8 Pages, 7 Figures, Presented at Workshop on Explainable AI in\n  Finance at ICAIF 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since the onset of the the COVID-19 pandemic, many countries across the world\nhave implemented various non-pharmaceutical interventions (NPIs) to contain the\nspread of virus, as well as economic support policies (ESPs) to save their\neconomies. The pandemic and the associated NPIs have triggered unprecedented\nwaves of economic shocks to the financial markets, including the foreign\nexchange (FX) markets. Although there are some studies exploring the impact of\nthe NPIs and ESPs on FX markets, the relative impact of individual NPIs or ESPs\nhas not been studied in a combined framework. In this work, we investigate the\nrelative impact of NPIs and ESPs with Explainable AI (XAI) techniques.\nExperiments over exchange rate data of G10 currencies during the period from\nJanuary 1, 2020 to January 13, 2021 suggest strong impacts on exchange rate\nmarkets by all measures of the strict lockdown, such as stay at home\nrequirements, workplace closing, international travel control, and restrictions\non internal movement. Yet, the impact of individual NPI and ESP can vary across\ndifferent currencies. To the best of our knowledge, this is the first work that\nuses XAI techniques to study the relative impact of NPIs and ESPs on the FX\nmarket. The derived insights can guide governments and policymakers to make\ninformed decisions when facing with the ongoing pandemic and a similar\nsituation in the near future.\n"
    },
    {
        "paper_id": 2111.14631,
        "authors": "Christian Meyer",
        "title": "Model Risk in Credit Portfolio Models",
        "comments": "12 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Model risk in credit portfolio models is a serious issue for banks but has so\nfar not been tackled comprehensively. We will demonstrate how to deal with\nuncertainty in all model parameters in an all-embracing, yet easy-to-implement\nway.\n"
    },
    {
        "paper_id": 2111.14665,
        "authors": "Mohammad Ebrahim Sadeghi, Hamed Nozari, Hadi Khajezadeh Dezfoli, Mehdi\n  Khajezadeh",
        "title": "Ranking of different of investment risk in high-tech projects using\n  TOPSIS method in fuzzy environment based on linguistic variables",
        "comments": null,
        "journal-ref": "Journal of Fuzzy Extension & Applications, 2021, Volume 2, Issue 3",
        "doi": "10.22105/JFEA.2021.298002.1159",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Examining the trend of the global economy shows that global trade is moving\ntowards high-tech products. Given that these products generate very high added\nvalue, countries that can produce and export these products will have high\ngrowth in the industrial sector. The importance of investing in advanced\ntechnologies for economic and social growth and development is so great that it\nis mentioned as one of the strong levers to achieve development. It should be\nnoted that the policy of developing advanced technologies requires\nconsideration of various performance aspects, risks and future risks in the\ninvestment phase. Risk related to high-tech investment projects has a meaning\nother than financial concepts only. In recent years, researchers have focused\non identifying, analyzing, and prioritizing risk. There are two important\ncomponents in measuring investment risk in high-tech industries, which include\nidentifying the characteristics and criteria for measuring system risk and how\nto measure them. This study tries to evaluate and rank the investment risks in\nadvanced industries using fuzzy TOPSIS technique based on verbal variables.\n"
    },
    {
        "paper_id": 2111.15204,
        "authors": "Christian Meyer",
        "title": "Estimation of inter-sector asset correlations",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Asset correlations are an intuitive and therefore popular way to incorporate\nevent dependence into event risk, e.g., default risk, modeling. In this paper\nwe study the case of estimation of inter-sector asset correlations by\nseparation of cross-sectional dimension and time dimension.\n"
    },
    {
        "paper_id": 2111.15248,
        "authors": "Leonardo Niccol\\`o Ialongo, Camille de Valk, Emiliano Marchese, Fabian\n  Jansen, Hicham Zmarrou, Tiziano Squartini, Diego Garlaschelli",
        "title": "Reconstructing firm-level interactions: the Dutch input-output network",
        "comments": "main text: 26 pages, 9 figures; supplementary material 21 pages, 9\n  figures",
        "journal-ref": "Sci. Rep. 12 (11847) (2022)",
        "doi": "10.1038/s41598-022-13996-3",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent crises have shown that the knowledge of the structure of input-output\nnetworks at the firm level is crucial when studying economic resilience from\nthe microscopic point of view of firms that rewire their connections under\nsupply and demand shocks. Unfortunately, empirical inter-firm network data are\nrarely accessible and protected by confidentiality. The available methods of\nnetwork reconstruction from partial information, which have been devised for\nfinancial exposures, are inadequate for inter-firm relationships because they\ntreat all pairs of nodes as potentially interacting, thereby overestimating the\nrewiring capabilities of the system. Here we use two big data sets of\ntransactions in the Netherlands to represent a large portion of the Dutch\ninter-firm network and document the properties of one of the few analysed\nnetworks of this kind. We, then, introduce a generalized maximum-entropy\nreconstruction method that preserves the production function of each firm in\nthe data, i.e. the input and output flows of each node for each product type.\nWe confirm that the new method becomes increasingly more reliable as a finer\nproduct resolution is considered and can therefore be used as a generative\nmodel of inter-firm networks with fine production constraints. The likelihood\nof the model, being related to the entropy, proxies the rewiring capability of\nthe system for a fixed input-output configuration.\n"
    },
    {
        "paper_id": 2111.15255,
        "authors": "Zongmin Liu",
        "title": "Double Fuzzy Probabilistic Interval Linguistic Term Set and a Dynamic\n  Fuzzy Decision Making Model based on Markov Process with tts Application in\n  Multiple Criteria Group Decision Making",
        "comments": "submitted to IEEE Transactions on Fuzzy Systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The probabilistic linguistic term has been proposed to deal with probability\ndistributions in provided linguistic evaluations. However, because it has some\nfundamental defects, it is often difficult for decision-makers to get\nreasonable information of linguistic evaluations for group decision making. In\naddition, weight information plays a significant role in dynamic information\nfusion and decision making process. However, there are few research methods to\ndetermine the dynamic attribute weight with time. In this paper, I propose the\nconcept of double fuzzy probability interval linguistic term set (DFPILTS).\nFirstly, fuzzy semantic integration, DFPILTS definition, its preference\nrelationship, some basic algorithms and aggregation operators are defined.\nThen, a fuzzy linguistic Markov matrix with its network is developed. Then, a\nweight determination method based on distance measure and information entropy\nto reducing the inconsistency of DFPILPR and obtain collective priority vector\nbased on group consensus is developed. Finally, an aggregation-based approach\nis developed, and an optimal investment case from a financial risk is used to\nillustrate the application of DFPILTS and decision method in multi-criteria\ndecision making.\n"
    },
    {
        "paper_id": 2111.15327,
        "authors": "Shuguang Xiao, Xinglin Lai, Jiamin Peng",
        "title": "China's Easily Overlooked Monetary Transmission Mechanism: Monetary\n  Reservoir",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The traditional monetary transmission mechanism usually views the equity\nmarkets as the monetary reservoir that absorbs over-issued money, but due to\nChina's unique fiscal and financial system, the real estate sector has become\nan \"invisible\" non-traditional monetary reservoir in China for many years.\nFirst, using data from Chinese housing market and central bank for parameter\nestimation, we constructs a dynamic general equilibrium model that includes\nfiscal expansion and financial accelerator to reveal the mechanism of monetary\nreservoir. An asset can be called a loan product, which worked as financed\nasset for local fiscal expansion, as long as it satisfies the following three\nconditions: leveraged trading system, balance commitment payment, and the\nexistence of the utility of local governments. This paper refers to this\nmechanism as the monetary reservoir that will push up the premium of loan\nproduct, form asset bubbles and has a significant impact on the effectiveness\nof monetary policy. Local governments leverage the sector of the loan product\nto obtain short-term growth by influencing the balance sheets of financial\nintermediaries through fiscal financing, expenditure and also investment, but\nthis mechanism undermines the foundations of long-term growth by crowding out\nhuman capital and technological accumulation.\n"
    },
    {
        "paper_id": 2111.15332,
        "authors": "Jo\\~ao F. Doriguello, Alessandro Luongo, Jinge Bao, Patrick\n  Rebentrost, Miklos Santha",
        "title": "Quantum algorithm for stochastic optimal stopping problems with\n  applications in finance",
        "comments": "46 pages; v2: title slightly changed, typos fixed, references added;\n  v3: corrected the classical runtime by including a $O(T^2)$ term; v4:\n  constants slightly improved and simplified in Section 4 and several typos\n  corrected",
        "journal-ref": "Proceedings of TQC 2022, LIPIcs, vol. 232, 2:1--2:24 (2022)",
        "doi": "10.4230/LIPIcs.TQC.2022.2",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The famous least squares Monte Carlo (LSM) algorithm combines linear least\nsquare regression with Monte Carlo simulation to approximately solve problems\nin stochastic optimal stopping theory. In this work, we propose a quantum LSM\nbased on quantum access to a stochastic process, on quantum circuits for\ncomputing the optimal stopping times, and on quantum techniques for Monte\nCarlo. For this algorithm, we elucidate the intricate interplay of function\napproximation and quantum algorithms for Monte Carlo. Our algorithm achieves a\nnearly quadratic speedup in the runtime compared to the LSM algorithm under\nsome mild assumptions. Specifically, our quantum algorithm can be applied to\nAmerican option pricing and we analyze a case study for the common situation of\nBrownian motion and geometric Brownian motion processes.\n"
    },
    {
        "paper_id": 2111.15351,
        "authors": "Noriyuki Kunimoto and Kazuhiko Kakamu",
        "title": "Is Bitcoin really a currency? A viewpoint of a stochastic volatility\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/00036846.2021.1951441",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the asymmetric stochastic volatility model, this study investigates the\nday-of-the-week and holiday effects on the returns and volatility of Bitcoin\nfrom January 1, 2013 to August 31, 2019; in this context, we also discuss the\ncharacteristics of Bitcoin as a financial asset. The results of the estimation\nare threefold. First, the finding shows a small day-of-the week effect in\nvolatility on Saturday and Sunday than in the rest of the week. Second,\nalthough the holiday effects are examined in active trading countries, namely\nJapan, China, Germany, and the United States, the positive post-holiday effect\non the returns and weak positive pre-holiday effect on the volatility are only\nobserved in the United States. Finally, the asymmetry effect is not observed. A\ncomparison of Bitcoin to several assets such as stock, currency, and gold shows\nBitcoin's positioning between stock, currency, and gold in relation to the week\nand holiday effects, its reaction to federal funds and medium of exchange\ncharacteristics, and the lack of asymmetry effect.\n"
    },
    {
        "paper_id": 2111.15354,
        "authors": "Yizhuo Li, Peng Zhou, Fangyi Li, Xiao Yang",
        "title": "An Improved Reinforcement Learning Model Based on Sentiment Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the development of artificial intelligence technology, quantitative\ntrading systems represented by reinforcement learning have emerged in the stock\ntrading market. The authors combined the deep Q network in reinforcement\nlearning with the sentiment quantitative indicator ARBR to build a\nhigh-frequency stock trading model for the share market. To improve the\nperformance of the model, the PCA algorithm is used to reduce the\ndimensionality feature vector while incorporating the influence of market\nsentiment on the long-short power into the spatial state of the trading model\nand uses the LSTM layer to replace the fully connected layer to solve the\ntraditional DQN model due to limited empirical data storage. Through the use of\ncumulative income, Sharpe ratio to evaluate the performance of the model and\nthe use of double moving averages and other strategies for comparison. The\nresults show that the improved model proposed by authors is far superior to the\ncomparison model in terms of income, achieving a maximum annualized rate of\nreturn of 54.5%, which is proven to be able to increase reinforcement learning\nperformance significantly in stock trading.\n"
    },
    {
        "paper_id": 2111.15355,
        "authors": "Peng Zhou, Fangyi Li",
        "title": "Prediction of Fund Net Value Based on ARIMA-LSTM Hybrid Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The net value of the fund is affected by performance and market, and the\nresearchers try to quantify these effects to predict the future net value by\nestablishing different models. The current prediction models usually can only\nreflect the linear variation law, poorly handled or selectively ignore their\nnonlinear characteristics, so the prediction results are usually less accurate.\nThis paper uses a fund prediction method based on the ARIMA-LSTM hybrid model.\nAfter preprocessing the historical data, the first filter out the linear data\ncharacteristics with the ARIMA model, then pass the data to the LSTM model to\nextract the nonlinear characteristic by residual, and finally superposition the\nrespective prediction values of the two models to obtain the prediction results\nof the hybrid model. Empirically shows that the methods in the paper are more\naccurate and applicable than traditional fund prediction methods.\n"
    },
    {
        "paper_id": 2111.15356,
        "authors": "Peng Zhou, Jingling Tang",
        "title": "Improved Method of Stock Trading under Reinforcement Learning Based on\n  DRQN and Sentiment Indicators ARBR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the application of artificial intelligence in the financial field,\nquantitative trading is considered to be profitable. Based on this, this paper\nproposes an improved deep recurrent DRQN-ARBR model because the existing\nquantitative trading model ignores the impact of irrational investor behavior\non the market, making the application effect poor in an environment where the\nstock market in China is non-efficiency. By changing the fully connected layer\nin the original model to the LSTM layer and using the emotion indicator ARBR to\nconstruct a trading strategy, this model solves the problems of the traditional\nDQN model with limited memory for empirical data storage and the impact of\nobservable Markov properties on performance. At the same time, this paper also\nimproved the shortcomings of the original model with fewer stock states and\nchose more technical indicators as the input values of the model. The\nexperimental results show that the DRQN-ARBR algorithm proposed in this paper\ncan significantly improve the performance of reinforcement learning in stock\ntrading.\n"
    },
    {
        "paper_id": 2111.15365,
        "authors": "Carl Remlinger, Bri\\`ere Marie, Alasseur Cl\\'emence, Joseph Mikael",
        "title": "Expert Aggregation for Financial Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning algorithms dedicated to financial time series forecasting\nhave gained a lot of interest. But choosing between several algorithms can be\nchallenging, as their estimation accuracy may be unstable over time. Online\naggregation of experts combine the forecasts of a finite set of models in a\nsingle approach without making any assumption about the models. In this paper,\na Bernstein Online Aggregation (BOA) procedure is applied to the construction\nof long-short strategies built from individual stock return forecasts coming\nfrom different machine learning models. The online mixture of experts leads to\nattractive portfolio performances even in environments characterised by\nnon-stationarity. The aggregation outperforms individual algorithms, offering a\nhigher portfolio Sharpe Ratio, lower shortfall, with a similar turnover.\nExtensions to expert and aggregation specialisations are also proposed to\nimprove the overall mixture on a family of portfolio evaluation metrics.\n"
    },
    {
        "paper_id": 2111.15367,
        "authors": "Jianian Wang, Sheng Zhang, Yanghua Xiao, Rui Song",
        "title": "A Review on Graph Neural Network Methods in Financial Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With multiple components and relations, financial data are often presented as\ngraph data, since it could represent both the individual features and the\ncomplicated relations. Due to the complexity and volatility of the financial\nmarket, the graph constructed on the financial data is often heterogeneous or\ntime-varying, which imposes challenges on modeling technology. Among the graph\nmodeling technologies, graph neural network (GNN) models are able to handle the\ncomplex graph structure and achieve great performance and thus could be used to\nsolve financial tasks. In this work, we provide a comprehensive review of GNN\nmodels in recent financial context. We first categorize the commonly-used\nfinancial graphs and summarize the feature processing step for each node. Then\nwe summarize the GNN methodology for each graph type, application in each area,\nand propose some potential research areas.\n"
    },
    {
        "paper_id": 2111.15389,
        "authors": "Federico Nutarelli, Massimo Riccaboni, Andrea Morescalchi",
        "title": "Product recalls, market size and innovation in the pharmaceutical\n  industry",
        "comments": "Conference paper at ASSA (2022)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The idea that research investments respond to market rewards is well\nestablished in the literature on markets for innovation (Schmookler, 1966;\nAcemoglu and Linn, 2004; Bryan and Williams, 2021). Empirical evidence tells us\nthat a change in market size, such as the one measured by demographical shifts,\nis associated with an increase in the number of new drugs available (Acemoglu\nand Linn, 2004; Dubois et al., 2015). However, the debate about potential\nreverse causality is still open (Cerda et al., 2007). In this paper we analyze\nmarket size's effect on innovation as measured by active clinical trials. The\nidea is to exploit product recalls an innovative instrument tested to be sharp,\nstrong, and unexpected. The work analyses the relationship between US market\nsize and innovation at ATC-3 level through an original dataset and the two-step\nIV methodology proposed by Wooldridge et al. (2019). The results reveal a\nrobust and significantly positive response of number of active trials to market\nsize.\n"
    },
    {
        "paper_id": 2111.15598,
        "authors": "Liqun Liu, Tusi (\\\"Undes) Wen",
        "title": "Inefficient Peace or Preventive War?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a model of two-player bargaining game in the shadow of a preventive\ntrade war that examines why states deliberately maintain trade barriers in the\nage of globalization. Globalization can induce substantial power shifts between\nstates, which makes the threat of a preventive trade war salient. In this\nsituation, there may exist \"healthy\" levels of trade barriers that dampen the\nwar incentives by reducing states' expected payoffs from such a war. Thus, we\ndemonstrate that trade barriers can sometimes serve as breaks and cushions\nnecessary to sustain inefficient yet peaceful economic cooperation between\nstates. We assess the theoretical implications by examining the US-China trade\nrelations since 1972.\n"
    },
    {
        "paper_id": 2111.15618,
        "authors": "Karinna Moura Boaviagem and Jos\\'e Ricardo Bezerra Nogueira",
        "title": "Analise Demografica e Socioeconomica do Uso e do Acesso a Medicamentos\n  Antidepressivos no Brasil",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Depressive disorders, in addition to causing direct negative impacts on\nhealth, are also responsible for imposing substantial costs on society. In\nrelation to the treatment of depression, antidepressants have proven effective,\nand, to the World Health Organization, access to psychotropic drugs for people\nwith mental illnesses offers a chance of improved health and an opportunity for\nreengagement in society. The aim of this study is to analyze the use of and\naccess to antidepressants in Brazil, according to macro-regions and to\ndemographic, social and economic conditions of the population, using the\nNational Survey on Access, Use and Promotion of Rational Use of Medicines\n(PNAUM 2013/2014). The results show that there is a high prevalence of\nantidepressant use in individuals with depression in Brazil. The main profile\nof use of these drugs is: female individuals, between 20 and 59 years old,\nwhite, from the Southeast region, of the economic class D/E, with a high\nschooling level, in a marital situation, without health insurance coverage,\nwithout limitations derived from depression, and who self-evaluated health as\nregular.\n"
    },
    {
        "paper_id": 2111.15634,
        "authors": "MohammadAmin Fazli, Parsa Alian, Ali Owfi, Erfan Loghmani",
        "title": "RPS: Portfolio Asset Selection using Graph based Representation Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Portfolio optimization is one of the essential fields of focus in finance.\nThere has been an increasing demand for novel computational methods in this\narea to compute portfolios with better returns and lower risks in recent years.\nWe present a novel computational method called Representation Portfolio\nSelection (RPS) by redefining the distance matrix of financial assets using\nRepresentation Learning and Clustering algorithms for portfolio selection to\nincrease diversification. RPS proposes a heuristic for getting closer to the\noptimal subset of assets. Using empirical results in this paper, we demonstrate\nthat widely used portfolio optimization algorithms, such as MVO, CLA, and HRP,\ncan benefit from our asset subset selection.\n"
    },
    {
        "paper_id": 2112.00375,
        "authors": "Bastien Baldacci, Philippe Bergault",
        "title": "Optimal incentives in a limit order book: a SPDE control approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the fragmentation of electronic markets, exchanges are now competing in\norder to attract trading activity on their platform. Consequently, they\ndeveloped several regulatory tools to control liquidity provision / consumption\non their liquidity pool. In this paper, we study the problem of an exchange\nusing incentives in order to increase market liquidity. We model the limit\norder book as the solution of a stochastic partial differential equation (SPDE)\nas in [12]. The incentives proposed to the market participants are functions of\nthe time and the distance of their limit order to the mid-price. We formulate\nthe control problem of the exchange who wishes to modify the shape of the order\nbook by increasing the volume at specific limits. Due to the particular nature\nof the SPDE control problem, we are able to characterize the solution with a\nclassic Feynman-Kac representation theorem. Moreover, when studying the\nasymptotic behavior of the solution, a specific penalty function enables the\nexchange to obtain closed-form incentives at each limit of the order book. We\nstudy numerically the form of the incentives and their impact on the shape of\nthe order book, and analyze the sensitivity of the incentives to the market\nparameters.\n"
    },
    {
        "paper_id": 2112.00415,
        "authors": "Abhijit Chakraborty and Tobias Reisch and Christian Diem and Stefan\n  Thurner",
        "title": "Inequality in economic shock exposures across the global firm-level\n  supply network",
        "comments": "19 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For centuries, national economies created wealth by engaging in international\ntrade and production. The resulting international supply networks not only\nincrease wealth for countries, but also create systemic risk: economic shocks,\ntriggered by company failures in one country, may propagate to other countries.\nUsing global supply network data on the firm-level, we present a method to\nestimate a country's exposure to direct and indirect economic losses caused by\nthe failure of a company in another country. We show the network of systemic\nrisk-flows across the world. We find that rich countries expose poor countries\nmuch more to systemic risk than the other way round. We demonstrate that higher\nsystemic risk levels are not compensated with a risk premium in GDP, nor do\nthey correlate with economic growth. Systemic risk around the globe appears to\nbe distributed more unequally than wealth. These findings put the often praised\nbenefits for developing countries from globalized production in a new light,\nsince they relate them to the involved risks in the production processes.\nExposure risks present a new dimension of global inequality, that most affects\nthe poor in supply shock crises. It becomes fully quantifiable with the\nproposed method.\n"
    },
    {
        "paper_id": 2112.00439,
        "authors": "Gongqiu Zhang, Lingfei Li",
        "title": "A General Approach for Lookback Option Pricing under Markov Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a very efficient method for pricing various types of lookback\noptions under Markov models. We utilize the model-free representations of\nlookback option prices as integrals of first passage probabilities. We combine\nefficient numerical quadrature with continuous-time Markov chain approximation\nfor the first passage problem to price lookbacks. Our method is applicable to a\nvariety of models, including one-dimensional time-homogeneous and\ntime-inhomogeneous Markov processes, regime-switching models and stochastic\nlocal volatility models. We demonstrate the efficiency of our method through\nvarious numerical examples.\n"
    },
    {
        "paper_id": 2112.00562,
        "authors": "Chengxiu Ling, Jiayi Li, Yixuan Liu, Zhiyan Cai",
        "title": "Extremal Analysis of Flooding Risk and Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Catastrophic losses caused by natural disasters receive a growing concern\nabout the severe rise in magnitude and frequency. The constructions of\ninsurance and financial management scheme become increasingly necessary to\ndiversify the disaster risks. Given the frequency and severity of floods in\nChina, this paper investigates the extreme analysis of flood-related huge\nlosses and extreme precipitations using Peaks-Over-Threshold method and Point\nProcess (PP) model. These findings are further utilized for both designs of\nflood zoning insurance and flooding catastrophic bond: (1) Using the\nextrapolation approach in Extreme Value Theory (EVT), the estimated\nValue-at-Risk (VaR) and conditional VaR (CVaR) are given to determine the\ncross-regional insurance premium together with the Grey Relational Analysis\n(GRA) and the Technique for Order Preference by Similarity to an Ideal Solution\n(TOPSIS). The flood risk vulnerability and threat are analyzed with both the\ngeography and economic factors into considerations, leading to the three\nlayered premium levels of the 19 flood-prone provinces. (2) To hedge the risk\nfor insurers and reinsurers to the financial market, we design a flooding\ncatastrophe bond with considerate trigger choices and the pricing mechanism to\nbalance the benefits of both reinsurers and investors. To reflect both the\nmarket price of catastrophe risk and the low-correlated financial interest\nrisk, we utilize the pricing mechanism of Tang and Yuan (2021) to analyze the\npricing sensitivity against the tail risk of the flooding disaster and the\ndistortion magnitude and the market risk through the distortion magnitude\ninvolved in Wang's transform. Finally, constructive suggestions and policies\nare proposed concerning the flood risk warning and prevention.\n"
    },
    {
        "paper_id": 2112.00564,
        "authors": "Teemu Makkonen and Timo Mitze",
        "title": "Geo-political conflicts, economic sanctions and international knowledge\n  flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We address the question how sensitive international knowledge flows respond\nto geo-political conflicts taking the politico-economic tensions between\nEU-Russia since the Ukraine crisis 2014 as case study. We base our econometric\nanalysis on comprehensive data covering more than 500 million scientific\npublications and 8 million international co-publications between 1995 and 2018.\nOur findings indicate that the imposition of EU sanctions and Russian\ncounter-sanctions from 2014 onwards has significant negative effects on\nbilateral international scientific co-publication rates between EU countries\nand Russia. Depending on the chosen control group and sectors considered,\neffect size ranges from 15% to 70%. Effects are also observed to grow over\ntime.\n"
    }
]