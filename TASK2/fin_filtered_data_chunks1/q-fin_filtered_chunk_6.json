[
    {
        "paper_id": 1511.08591,
        "authors": "Stefan Rass",
        "title": "On Game-Theoretic Risk Management (Part Two) -- Algorithms to Compute\n  Nash-Equilibria in Games with Distributions as Payoffs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The game-theoretic risk management framework put forth in the precursor work\n\"Towards a Theory of Games with Payoffs that are Probability-Distributions\"\n(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how\nto compute equilibria in games where the payoffs are probability distributions.\nOur approach is \"data driven\" in the sense that we assume empirical data\n(measurements, simulation, etc.) to be available that can be compiled into\ndistribution models, which are suitable for efficient decisions about\npreferences, and setting up and solving games using these as payoffs. While\npreferences among distributions turn out to be quite simple if nonparametric\nmethods (kernel density estimates) are used, computing Nash-equilibria in games\nusing such models is discovered as inefficient (if not impossible). In fact, we\ngive a counterexample in which fictitious play fails to converge for the\n(specifically unfortunate) choice of payoff distributions in the game, and\nintroduce a suitable tail approximation of the payoff densities to tackle the\nissue. The overall procedure is essentially a modified version of fictitious\nplay, and is herein described for standard and multicriteria games, to\niteratively deliver an (approximate) Nash-equilibrium. An exact method using\nlinear programming is also given.\n"
    },
    {
        "paper_id": 1511.08622,
        "authors": "Emanuele Pugliese, Guido L. Chiarotti, Andrea Zaccaria, and Luciano\n  Pietronero",
        "title": "Complex economies have a lateral escape from the poverty trap",
        "comments": "12 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0168540",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the decisive role played by the complexity of economic systems at\nthe onset of the industrialization process of countries over the past 50 years.\nOur analysis of the input growth dynamics, based on a recently introduced\nmeasure of economic complexity, reveals that more differentiated and more\ncomplex economies face a lower barrier (in terms of GDP per capita) when\nstarting the transition towards industrialization. Moreover, adding the\ncomplexity dimension to the industrialization process description helps to\nreconcile current theories with empirical findings.\n"
    },
    {
        "paper_id": 1511.08666,
        "authors": "Tatiana Belkina, Nadezhda Konyukhova, and Sergey Kurochkin",
        "title": "Singular Problems for Integro-Differential Equations in Dynamic\n  Insurance Models",
        "comments": "17 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1007/978-1-4614-7333-6_3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A second order linear integro-differential equation with Volterra integral\noperator and strong singularities at the endpoints (zero and infinity) is\nconsidered. Under limit conditions at the singular points, and some natural\nassumptions, the problem is a singular initial problem with limit normalizing\nconditions at infinity. An existence and uniqueness theorem is proved and\nasymptotic representations of the solution are given. A numerical algorithm for\nevaluating the solution is proposed, calculations and their interpretation are\ndiscussed. The main singular problem under study describes the survival\n(non-ruin) probability of an insurance company on infinite time interval (as a\nfunction of initial surplus) in the Cramer-Lundberg dynamic insurance model\nwith an exponential claim size distribution and certain company's strategy at\nthe financial market assuming investment of a fixed part of the surplus\n(capital) into risky assets (shares) and the rest of it into a risk free asset\n(bank deposit). Accompanying \"degenerate\" problems are also considered that\nhave an independent meaning in risk theory\n"
    },
    {
        "paper_id": 1511.08718,
        "authors": "Yiran Cui, Sebastian del Ba\\~no Rollin, Guido Germano",
        "title": "Full and fast calibration of the Heston stochastic volatility model",
        "comments": "30 pages, 8 figures, submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an algorithm for a complete and efficient calibration of\nthe Heston stochastic volatility model. We express the calibration as a\nnonlinear least squares problem. We exploit a suitable representation of the\nHeston characteristic function and modify it to avoid discontinuities caused by\nbranch switchings of complex functions. Using this representation, we obtain\nthe analytical gradient of the price of a vanilla option with respect to the\nmodel parameters, which is the key element of all variants of the objective\nfunction. The interdependency between the components of the gradient enables an\nefficient implementation which is around ten times faster than a numerical\ngradient. We choose the Levenberg-Marquardt method to calibrate the model and\ndo not observe multiple local minima reported in previous research.\nTwo-dimensional sections show that the objective function is shaped as a narrow\nvalley with a flat bottom. Our method is the fastest calibration of the Heston\nmodel developed so far and meets the speed requirement of practical trading.\n"
    },
    {
        "paper_id": 1511.0883,
        "authors": "Paolo Barucca and Fabrizio Lillo",
        "title": "Disentangling bipartite and core-periphery structure in financial\n  networks",
        "comments": "8 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2016.02.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A growing number of systems are represented as networks whose architecture\nconveys significant information and determines many of their properties.\nExamples of network architecture include modular, bipartite, and core-periphery\nstructures. However inferring the network structure is a non trivial task and\ncan depend sometimes on the chosen null model. Here we propose a method for\nclassifying network structures and ranking its nodes in a statistically\nwell-grounded fashion. The method is based on the use of Belief Propagation for\nlearning through Entropy Maximization on both the Stochastic Block Model (SBM)\nand the degree-corrected Stochastic Block Model (dcSBM). As a specific\napplication we show how the combined use of the two ensembles -SBM and dcSBM-\nallows to disentangle the bipartite and the core-periphery structure in the\ncase of the e-MID interbank network. Specifically we find that, taking into\naccount the degree, this interbank network is better described by a bipartite\nstructure, while using the SBM the core-periphery structure emerges only when\ndata are aggregated for more than a week.\n"
    },
    {
        "paper_id": 1511.08997,
        "authors": "Tetsuya Takaishi",
        "title": "Realized Volatility Analysis in A Spin Model of Financial Markets",
        "comments": "4 pages, 5 figures",
        "journal-ref": "JPS Conf. Proc. 1, 019007 (2014)",
        "doi": "10.7566/JPSCP.1.019007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We calculate the realized volatility in the spin model of financial markets\nand examine the returns standardized by the realized volatility. We find that\nmoments of the standardized returns agree with the theoretical values of\nstandard normal variables. This is the first evidence that the return dynamics\nof the spin financial market is consistent with the view of the\nmixture-of-distribution hypothesis that also holds in the real financial\nmarkets.\n"
    },
    {
        "paper_id": 1511.09041,
        "authors": "Roxana Dumitrescu, Marie-Claire Quenez and Agn\\`es Sulem",
        "title": "Game options in an imperfect market with default",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study pricing and superhedging strategies for game options in an imperfect\nmarket with default. We extend the results obtained by Kifer in \\cite{Kifer} in\nthe case of a perfect market model to the case of an imperfect market with\ndefault, when the imperfections are taken into account via the nonlinearity of\nthe wealth dynamics. We introduce the {\\em seller's price} of the game option\nas the infimum of the initial wealths which allow the seller to be superhedged.\nWe {prove} that this price coincides with the value function of an associated\n{\\em generalized} Dynkin game, recently introduced in \\cite{DQS2}, expressed\nwith a nonlinear expectation induced by a nonlinear BSDE with default jump. We\nmoreover study the existence of superhedging strategies.\n  We then address the case of ambiguity on the model, - for example ambiguity\non the default probability - and characterize the robust seller's price of a\ngame option as the value function of a {\\em mixed generalized} Dynkin game.\n  We study the existence of a cancellation time and a trading strategy which\nallow the seller to be super-hedged, whatever the model is.\n"
    },
    {
        "paper_id": 1511.09054,
        "authors": "Zachary Feinstein",
        "title": "It's a Trap: Emperor Palpatine's Poison Pill",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the financial repercussions of the destruction of two\nfully armed and operational moon-sized battle stations (\"Death Stars\") in a\n4-year period and the dissolution of the galactic government in Star Wars. The\nemphasis of this work is to calibrate and simulate a model of the banking and\nfinancial systems within the galaxy. Along these lines, we measure the level of\nsystemic risk that may have been generated by the death of Emperor Palpatine\nand the destruction of the second Death Star. We conclude by finding the\neconomic resources the Rebel Alliance would need to have in reserve in order to\nprevent a financial crisis from gripping the galaxy through an optimally\nallocated banking bailout.\n"
    },
    {
        "paper_id": 1511.09203,
        "authors": "Marco Bardoscia, Giacomo Livan, Matteo Marsili",
        "title": "Statistical mechanics of complex economies",
        "comments": "30 pages, 10 figures",
        "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment (2017)\n  P043401",
        "doi": "10.1088/1742-5468/aa6688",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the pursuit of ever increasing efficiency and growth, our economies have\nevolved to remarkable degrees of complexity, with nested production processes\nfeeding each other in order to create products of greater sophistication from\nless sophisticated ones, down to raw materials. The engine of such an expansion\nhave been competitive markets that, according to General Equilibrium Theory\n(GET), achieve efficient allocations under specific conditions. We study large\nrandom economies within the GET framework, as templates of complex economies,\nand we find that a non-trivial phase transition occurs: the economy freezes in\na state where all production processes collapse when either the number of\nprimary goods or the number of available technologies fall below a critical\nthreshold. As in other examples of phase transitions in large random systems,\nthis is an unintended consequence of the growth in complexity. Our findings\nsuggest that the Industrial Revolution can be regarded as a sharp transition\nbetween different phases, but also imply that well developed economies can\ncollapse if too many intermediate goods are introduced.\n"
    },
    {
        "paper_id": 1511.09323,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the GDP/cap Data",
        "comments": "16 pages, 8 figures, 5945 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical properties of the historical GDP/cap distributions are discussed\nand explained. These distributions are frequently incorrectly interpreted and\nthe Unified Growth Theory is an outstanding example of such common\nmisconceptions. It is shown here that the fundamental postulates of this theory\nare contradicted by the data used in its formulation. The postulated three\nregimes of growth did not exist and there was no takeoff at any time. It is\ndemonstrated that features interpreted as three regimes of growth represent\njust mathematical properties of a single, monotonically-increasing\ndistribution, indicating that a single mechanism should be used to explain the\nhistorical economic growth. It is shown that using different socio-economic\nconditions for different perceived parts of the historical GDP/cap data is\nirrelevant and scientifically unjustified. The GDP/cap growth was indeed\nincreasing slowly over a long time and fast over a short time but these\nfeatures represent a single, uniform and uninterrupted growth process, which\nshould be interpreted as whole using a single mechanism of growth.\n"
    },
    {
        "paper_id": 1512.00227,
        "authors": "Takanori Adachi",
        "title": "A Framework for Analyzing Stochastic Jumps in Finance based on Belief\n  and Knowledge",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a formal language IE that is a variant of the language PAL\ndeveloped in [van Benthem 2011] by adding a belief operator and a common belief\noperator,specializing to stochastic analysis. A constant symbol in the language\ndenotes a stochastic process so that we can represent several financial events\nas formulae in the language, which is expected to be clues of analyzing the\nmoments that some stochastic jumps such as financial crises occur based on\nknowledge and belief of individuals or those shared within groups of\nindividuals. In order to represent beliefs, we use sigma-complete Boolean\nalgebras as generalized sigma-algebras. We use the representation for\nconstructing a model in which the interpretations of the formulae written in\nthe language IE reside. The model also uses some new categories for integrating\nseveral components appeared in the theory into one.\n"
    },
    {
        "paper_id": 1512.0123,
        "authors": "Kartik Ahuja, Mihaela van der Schaar, William R. Zame",
        "title": "A Theory of Individualism, Collectivism and Economic Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a dynamic model to study the impact on the economic\noutcomes in different societies during the Malthusian Era of individualism\n(time spent working alone) and collectivism (complementary time spent working\nwith others). The model is driven by opposing forces: a greater degree of\ncollectivism provides a higher safety net for low quality workers but a greater\ndegree of individualism allows high quality workers to leave larger bequests.\nThe model suggests that more individualistic societies display smaller\npopulations, greater per capita income and greater income inequality. Some\n(limited) historical evidence is consistent with these predictions.\n"
    },
    {
        "paper_id": 1512.01267,
        "authors": "Vera Zaporozhets, Mar\\'ia Garc\\'ia-Vali\\~nas, and Sascha Kurz",
        "title": "Key drivers of EU budget allocation: Does power matter?",
        "comments": "32 pages, 9 tables",
        "journal-ref": null,
        "doi": "10.1016/j.ejpoleco.2016.02.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the determinants of the EU budget expenditures allocation among\ndifferent countries. In line with earlier literature, we consider two\nalternative explanations for the EU budget distribution: political power vs.\nneeds view. Extending the original data set from Kauppi and Widgr\\'en (2004),\nwe analyze the robustness of their predictions when applying a different\nmeasure of power and more sophisticated econometric techniques. We conclude\nthat the nucleolus is a good alternative to the Shapley-Shubik index in\ndistributive situations such as the case of EU budget allocation. Our results\nalso show that when explaining budget shares, the relative weight of political\npower based on the nucleolus is lower than the predictions of previous studies\nbased on the Shapley-Shubik index.\n"
    },
    {
        "paper_id": 1512.01488,
        "authors": "Matteo Burzoni",
        "title": "Arbitrage and Hedging in model-independent markets with frictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a Fundamental Theorem of Asset Pricing and a Superhedging Theorem\nfor a model independent discrete time financial market with proportional\ntransaction costs. We consider a probability-free version of the Robust No\nArbitrage condition introduced in Schachermayer ['04] and show that this is\nequivalent to the existence of Consistent Price Systems. Moreover, we prove\nthat the superhedging price for a claim g coincides with the frictionless\nsuperhedging price of g for a suitable process in the bid-ask spread.\n"
    },
    {
        "paper_id": 1512.01527,
        "authors": "Peter Carr and Zura Kakushadze",
        "title": "FX Options in Target Zone",
        "comments": "25 pages; to appear in Quantitative Finance",
        "journal-ref": "Quantitative Finance 17(10) (2017) 1477-1486, Featured Article",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we discuss - in what is intended to be a pedagogical fashion -\nFX option pricing in target zones with attainable boundaries. The boundaries\nmust be reflecting. The no-arbitrage requirement implies that the differential\n(foreign minus domestic) short-rate is not deterministic. When the band is\nnarrow, we can pick the functional form of the FX rate process based on\ncomputational convenience. With a thoughtful choice, the FX option pricing\nproblem can be solved analytically. The European option prices are expressed\nvia (fast converging) series of elementary functions. We discuss the general\napproach to solving the pricing PDE and explicit examples, including\nanalytically tractable models with (non-Ornstein-Uhlenbeck) mean-reversion.\n"
    },
    {
        "paper_id": 1512.01626,
        "authors": "Su-Mei Chen and Ling-Yun He",
        "title": "Optimal environmental tax swaps and double dividend hypothesis",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Taking environmental tax rate as given, is there an optimal allocation of tax\nrevenues to benefit economic variables? This paper analyzes this issue in an\noverlapping-generations model with the pollution-related health damage. It\nfinds the optimal allocations towards pollution abatement and labor income to\nmaximize the steady-state lifetime welfare and per-worker output, respectively.\nMoreover, a greater shift towards labor income might enhance steady-state\nwelfare while reducing per-worker output.\n"
    },
    {
        "paper_id": 1512.01676,
        "authors": "Yue-Jun Zhang, Ting Yao, Ling-Yun He",
        "title": "Forecasting crude oil market volatility: can the Regime Switching GARCH\n  model beat the single-regime GARCH models?",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to obtain a reasonable and reliable forecast method for crude oil\nprice volatility, this paper evaluates the forecast performance of\nsingle-regime GARCH models (including the standard linear GARCH model and the\nnonlinear GJR-GARCH and EGARCH models) and the two-regime Markov Regime\nSwitching GARCH (MRS-GARCH) model for crude oil price volatility at different\ndata frequencies and time horizons. The results indicate that, first, the\ntwo-regime MRS-GARCH model beats other three single-regime GARCH type models in\nin-sample data estimation under most evaluation criteria, although it appears\ninferior under a few of other evaluation criteria. Second, the two-regime\nMRS-GARCH model overall provides more accurate volatility forecast for daily\ndata but this superiority dies way for weekly and monthly data. Third, among\nthe three single-regime GARCH type models, the volatility forecast of the\nnonlinear GARCH models exhibit greater accuracy than the linear GARCH model for\ndaily data at longer time horizons. Finally, the linear single-regime GARCH\nmodel overall performs better than other three nonlinear GARCH type models in\nValue-at-Risk (VaR) forecast.\n"
    },
    {
        "paper_id": 1512.01698,
        "authors": "Vladimir Vovk",
        "title": "Purely pathwise probability-free Ito integral",
        "comments": "23 pages; this version strengthens the main results, simplifies their\n  proofs, and adds a preliminary result about non-cadlag integrands",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper gives several simple constructions of the pathwise Ito integral\n$\\int_0^t\\phi d\\omega$ for an integrand $\\phi$ and a price path $\\omega$ as\nintegrator, with $\\phi$ and $\\omega$ satisfying various topological and\nanalytical conditions. The definitions are purely pathwise in that neither\n$\\phi$ nor $\\omega$ are assumed to be paths of stochastic processes, and the\nIto integral exists almost surely in a non-probabilistic financial sense. For\nexample, one of the results shows the existence of $\\int_0^t\\phi d\\omega$ for a\ncadlag integrand $\\phi$ and a cadlag integrator $\\omega$ with jumps bounded in\na predictable manner.\n"
    },
    {
        "paper_id": 1512.01742,
        "authors": "Sheng Yang, Ling-Yun He",
        "title": "Oil price shocks, road transport pollution emissions and residents'\n  health losses in China",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.trd.2015.10.019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China's rapid economic growth resulted in serious air pollution, which caused\nsubstantial losses to economic development and residents' health. In\nparticular, the road transport sector has been blamed to be one of the major\nemitters. During the past decades, fluctuation in the international oil prices\nhas imposed significant impacts on the China's road transport sector.\nTherefore, we propose an assumption that China's provincial economies are\nindependent \"economic entities\". Based on this assumption, we investigate the\nChina's road transport fuel (i.e., gasoline and diesel) demand system by using\nthe panel data of all 31 Chinese provinces except Hong Kong, Macau and Taiwan.\nTo connect the fuel demand system and the air pollution emissions, we propose\nthe concept of pollution emissions elasticities to estimate the air pollution\nemissions from the road transport sector, and residents' health losses by a\nsimplified approach consisting of air pollution concentrations and health loss\nassessment models under different scenarios based on real-world oil price\nfluctuations. Our framework, to the best of our knowledge, is the first attempt\nto address the transmission mechanism between the fuel demand system in road\ntransport sector and residents' health losses in the transitional China.\n"
    },
    {
        "paper_id": 1512.01758,
        "authors": "Mario Sikic",
        "title": "Financial market models in discrete time beyond the concave case",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we propose a study of market models starting from a set of\naxioms, as one does in the case of risk measures. We define a market model\nsimply as a mapping from the set of adapted strategies to the set of random\nvariables describing the outcome of trading. We do not make any concavity\nassumptions. The first result is that under sequential upper-semicontinuity the\nmarket model can be represented as a normal integrand. We then extend the\nconcept of no-arbitrage to this setup and study its consequences as the\nsuper-hedging theorem and utility maximization. Finally, we show how to extend\nthe concepts and results to the case of vector-valued market models, an example\nof which is the Kabanov model of currency markets.\n"
    },
    {
        "paper_id": 1512.01806,
        "authors": "Mihaly Ormos and Dusan Timotity",
        "title": "Generalized asset pricing: Expected Downside Risk-Based Equilibrium\n  Modelling",
        "comments": "55 pages, 15 figures, 1 table, 3 appandices, Econ. Model. (2015)",
        "journal-ref": null,
        "doi": "10.1016/j.econmod.2015.10.036",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an equilibrium asset pricing model, which we build on the\nrelationship between a novel risk measure, the Expected Downside Risk (EDR) and\nthe expected return. On the one hand, our proposed risk measure uses a\nnonparametric approach that allows us to get rid of any assumption on the\ndistribution of returns. On the other hand, our asset pricing model is based on\nloss-averse investors of Prospect Theory, through which we implement the\nrisk-seeking behaviour of investors in a dynamic setting. By including EDR in\nour proposed model unrealistic assumptions of commonly used equilibrium models\n- such as the exclusion of risk-seeking or price-maker investors and the\nassumption of unlimited leverage opportunity for a unique interest rate - can\nbe omitted. Therefore, we argue that based on more realistic assumptions our\nmodel is able to describe equilibrium expected returns with higher accuracy,\nwhich we support by empirical evidence as well.\n"
    },
    {
        "paper_id": 1512.01905,
        "authors": "Hannah Cheng Juan Zhan, William Rea, Alethea Rea",
        "title": "A Comparision of Three Network Portfolio Selection Methods -- Evidence\n  from the Dow Jones",
        "comments": "39 pages, 13 tables, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare three network portfolio selection methods; hierarchical clustering\ntrees, minimum spanning trees and neighbor-Nets, with random and industry group\nselection methods on twelve years of data from the 30 Dow Jones Industrial\nAverage stocks from 2001 to 2013 for very small private investor sized\nportfolios. We find that the three network methods perform on par with randomly\nselected portfolios.\n"
    },
    {
        "paper_id": 1512.01916,
        "authors": "Xin Li and Carlos F. Tolmasky",
        "title": "An asymmetric ARCH model and the non-stationarity of Clustering and\n  Leverage effects",
        "comments": "16 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new volatility model based on two stylized facts of the\nvolatility in the stock market: clustering and leverage effect. We calibrate\nour model parameters, in the leading order, with 77 years Dow Jones Industrial\nAverage data. We find in the short time scale (10 to 50 days) the future\nvolatility is sensitive to the sign of past returns, i.e. asymmetric feedback\nor leverage effect. However, in the long time scale (300 to 1000 days)\nclustering becomes the main factor. We study non-stationary features by using\nmoving windows and find that clustering and leverage effects display time\nevolutions that are rather nontrivial. The structure of our model allows us to\nshed light on a few surprising facts recently found by Chicheportiche and\nBouchaud.\n"
    },
    {
        "paper_id": 1512.02233,
        "authors": "Guillermo Garc\\'ia-P\\'erez, Mari\\'an Bogu\\~n\\'a, Antoine Allard, and\n  M. \\'Angeles Serrano",
        "title": "The hidden hyperbolic geometry of international trade: World Trade Atlas\n  1870-2013",
        "comments": "World Trade Atlas 1870-2013 interactive tool at\n  http://morfeo.ffn.ub.edu/wta1870-2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here, we present the World Trade Atlas 1870-2013, a collection of annual\nworld trade maps in which distance combines economic size and the different\ndimensions that affect international trade beyond mere geography. Trade\ndistances, which are based on a gravity model predicting the existence of\nsignificant trade channels, are such that the closer countries are in trade\nspace, the greater their chance of becoming connected. The atlas provides us\nwith information regarding the long-term evolution of the international trade\nsystem and demonstrates that, in terms of trade, the world is not flat but\nhyperbolic, as a reflection of its complex architecture. The departure from\nflatness has been increasing since World War I, meaning that differences in\ntrade distances are growing and trade networks are becoming more hierarchical.\nSmaller-scale economies are moving away from other countries except for the\nlargest economies; meanwhile those large economies are increasing their chances\nof becoming connected worldwide. At the same time, Preferential Trade\nAgreements do not fit in perfectly with natural communities within the trade\nspace and have not necessarily reduced internal trade barriers. We discuss an\ninterpretation in terms of globalization, hierarchization, and localization;\nthree simultaneous forces that shape the international trade system.\n"
    },
    {
        "paper_id": 1512.0231,
        "authors": "David Puelz, P. Richard Hahn, Carlos M. Carvalho",
        "title": "Sparse Mean-Variance Portfolios: A Penalized Utility Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers mean-variance optimization under uncertainty,\nspecifically when one desires a sparsified set of optimal portfolio weights.\nFrom the standpoint of a Bayesian investor, our approach produces a small\nportfolio from many potential assets while acknowledging uncertainty in asset\nreturns and parameter estimates. We demonstrate the procedure using static and\ndynamic models for asset returns.\n"
    },
    {
        "paper_id": 1512.02454,
        "authors": "Assaf Almog, Tiziano Squartini and Diego Garlaschelli",
        "title": "The double role of GDP in shaping the structure of the International\n  Trade Network",
        "comments": "11 pages, 5 figures. Contribution submitted for the International\n  Workshop on Computational Economics and Econometrics - see also\n  arXiv:1409.6649",
        "journal-ref": "Int. J. Comput. Econ. Econom., 7 (4), 381-398 (2017)",
        "doi": "10.1504/IJCEE.2017.10003511",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The International Trade Network (ITN) is the network formed by trade\nrelationships between world countries. The complex structure of the ITN impacts\nimportant economic processes such as globalization, competitiveness, and the\npropagation of instabilities. Modeling the structure of the ITN in terms of\nsimple macroeconomic quantities is therefore of paramount importance. While\ntraditional macroeconomics has mainly used the Gravity Model to characterize\nthe magnitude of trade volumes, modern network theory has predominantly focused\non modeling the topology of the ITN. Combining these two complementary\napproaches is still an open problem. Here we review these approaches and\nemphasize the double role played by GDP in empirically determining both the\nexistence and the volume of trade linkages. Moreover, we discuss a unified\nmodel that exploits these patterns and uses only the GDP as the relevant\nmacroeconomic factor for reproducing both the topology and the link weights of\nthe ITN.\n"
    },
    {
        "paper_id": 1512.02478,
        "authors": "Robert Fernholz",
        "title": "Variations on an example of Karatzas and Ruf",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markets composed of stocks with capitalization processes represented by\npositive continuous semimartingales are studied under the condition that the\nmarket excess growth rate is bounded away from zero. The following examples of\nthese markets are given: i) a market with a singular covariance matrix and\ninstantaneous relative arbitrage; ii) a market with a singular covariance\nmatrix and no arbitrage; iii) a market with a nonsingular covariance matrix and\nno arbitrage; iv) a market with a nonsingular covariance matrix and relative\narbitrage over an arbitrary time horizon.\n"
    },
    {
        "paper_id": 1512.02529,
        "authors": "Bertram D\\\"uring, James Miles",
        "title": "High-order ADI scheme for option pricing in stochastic volatility models",
        "comments": "18 pages",
        "journal-ref": "J. Comput. Appl. Math. 316 (2017), 109-121",
        "doi": "10.1016/j.cam.2016.09.040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new high-order alternating direction implicit (ADI) finite\ndifference scheme for the solution of initial-boundary value problems of\nconvection-diffusion type with mixed derivatives and non-constant coefficients,\nas they arise from stochastic volatility models in option pricing. Our approach\ncombines different high-order spatial discretisations with Hundsdorfer and\nVerwer's ADI time-stepping method, to obtain an efficient method which is\nfourth-order accurate in space and second-order accurate in time. Numerical\nexperiments for the European put option pricing problem using Heston's\nstochastic volatility model confirm the high-order convergence.\n"
    },
    {
        "paper_id": 1512.02859,
        "authors": "Antonios Garas, Celine Rozenblat and Frank Schweitzer",
        "title": "The network structure of city-firm relations",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How are economic activities linked to geographic locations? To answer this\nquestion, we use a data-driven approach that builds on the information about\nlocation, ownership and economic activities of the world's 3,000 largest firms\nand their almost one million subsidiaries. From this information we generate a\nbipartite network of cities linked to economic activities. Analysing the\nstructure of this network, we find striking similarities with nested networks\nobserved in ecology, where links represent mutualistic interactions between\nspecies. This motivates us to apply ecological indicators to identify the\nunbalanced deployment of economic activities. Such deployment can lead to an\nover-representation of specific economic sectors in a given city, and poses a\nsignificant thread for the city's future especially in times when the\nover-represented activities face economic uncertainties. If we compare our\nanalysis with external rankings about the quality of life in a city, we find\nthat the nested structure of the city-firm network also reflects such\ninformation about the quality of life, which can usually be assessed only via\ndedicated survey-based indicators.\n"
    },
    {
        "paper_id": 1512.02912,
        "authors": "H. Pollitt and J.-F. Mercure",
        "title": "The role of money and the financial sector in energy-economy models used\n  for assessing climate policy",
        "comments": "6000 words, 12 pages",
        "journal-ref": "Climate Policy 2017",
        "doi": "10.1080/14693062.2016.1277685",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper outlines a critical gap in the assessment methodology used to\nestimate the macroeconomic costs and benefits of climate policy. It shows that\nthe vast majority of models used for assessing climate policy use assumptions\nabout the financial system that sit at odds with the observed reality. In\nparticular, the models' assumptions lead to `crowding out' of capital, which\ncause them to show negative impacts from climate policy in virtually all cases.\nWe compare this approach with that of the E3ME model, which follows\nnon-equilibrium economic theory and adopts a more empirical approach. While the\nnon-equilibrium model also has limitations, its treatment of the financial\nsystem is more consistent with reality and it shows that green investment need\nnot crowd out investment in other parts of the economy -- and may therefore\noffer an economic stimulus.\n  The implication of this finding is that standard CGE models consistently\nover-estimate the costs of climate policy in terms of GDP and welfare,\npotentially by a substantial amount. These findings overly restrict the range\nof possible emission pathways accessible using climate policy from the\nviewpoint of the decision-maker, and may also lead to misleading information\nused for policy making. Improvements in both modelling approaches should be\nsought with some urgency -- both to provide a better assessment of potential\nclimate policy and to improve understanding of the dynamics of the global\nfinancial system more generally.\n"
    },
    {
        "paper_id": 1512.03164,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the Economic Growth in Africa",
        "comments": "4 figures, 8 pages, 2990 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the fundamental postulates of the Unified Growth Theory is the claimed\nexistence of three distinctly different regimes of economic growth governed by\nthree distinctly different mechanisms of growth. However, Galor also proposed\nthat the timing of these regimes is different for developed countries and for\nless-developed countries. Africa is the perfect example of economic growth in\nless-developed countries. The data used by Galor, but never properly\ninvestigated, are now analysed. They turn out to be in dramatic contradiction\nof this theory.\n"
    },
    {
        "paper_id": 1512.03173,
        "authors": "Micha{\\l} Barski",
        "title": "Monotonicity of the collateralized debt obligations term structure model",
        "comments": "28 pages",
        "journal-ref": "Stochastics, 86,5, 835-864 (2015)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of existence of arbitrage free and monotone CDO term structure\nmodels is studied. Conditions for positivity and monotonicity of the\ncorresponding Heath-Jarrow-Morton-Musiela equation for the $x$-forward rates\nwith the use of the Milian type result are formulated. Two state spaces are\ntaken into account - of square integrable functions and a Sobolev space. For\nthe first the regularity results concerning pointwise monotonicity are proven.\nArbitrage free and monotone models are characterized in terms of the volatility\nof the model and characteristics of the driving L\\'evy process.\n"
    },
    {
        "paper_id": 1512.03259,
        "authors": "Zorana Grbac, Laura Meneghello, Wolfgang J. Runggaldier",
        "title": "Derivative pricing for a multi-curve extension of the Gaussian,\n  exponentially quadratic short rate model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent financial crisis has led to so-called multi-curve models for the\nterm structure. Here we study a multi-curve extension of short rate models\nwhere, in addition to the short rate itself, we introduce short rate spreads.\nIn particular, we consider a Gaussian factor model where the short rate and the\nspreads are second order polynomials of Gaussian factor processes. This leads\nto an exponentially quadratic model class that is less well known than the\nexponentially affine class. In the latter class the factors enter linearly and\nfor positivity one considers square root factor processes. While the square\nroot factors in the affine class have more involved distributions, in the\nquadratic class the factors remain Gaussian and this leads to various\nadvantages, in particular for derivative pricing. After some preliminaries on\nmartingale modeling in the multi-curve setup, we concentrate on pricing of\nlinear and optional derivatives. For linear derivatives, we exhibit an\nadjustment factor that allows one to pass from pre-crisis single curve values\nto the corresponding post-crisis multi-curve values.\n"
    },
    {
        "paper_id": 1512.03292,
        "authors": "Stefan Waldenberger",
        "title": "Time-inhomogeneous affine processes and affine market models",
        "comments": "Doctoral Dissertation (PhD thesis), Graz University of Technology,\n  approved November 13, 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis is devoted to the study of affine processes and their\napplications in financial mathematics. In the first part we consider the theory\nof time-inhomogeneous affine processes on general state spaces. We present a\nconcise setup for time-inhomogeneous Markov processes. For stochastically\ncontinuous affine processes we show that there always exists a c\\`adl\\`ag\nmodification. Afterwards we consider the regularity and the semimartingale\nproperty of affine processes. Contrary to the time-homogeneous case,\ntime-inhomogeneous affine processes are in general neither regular nor\nsemimartingales and the time-inhomogeneous case raises many new and interesting\nquestions. Assuming that an affine process is a semimartingale, we show that\neven without regularity the parameter functions satisfy generalized Riccati\nintegral equations. This generalizes an important result for time-homogeneous\naffine processes. We also show that stochastically continuous affine\nsemimartingales are essentially generated by deterministic time-changes of what\nwe call absolutely continuously affine semimartingales. These processes\ngeneralize time-homogeneous regular affine processes.\n  In the second part we consider the class of affine LIBOR market models. We\ncontribute to this class of models in two ways. First, we modify the original\nsetup of the affine LIBOR market models in such a way that next to nonnegative\naffine processes real-valued affine processes can also be used. Numerical\nexamples show that this allows for more flexible implied volatility surfaces.\nSecond, we introduce the class of affine inflation market models, an extension\nof the affine LIBOR market models. A calibration example shows that these\nmodels perform very well in fitting market-observed prices of inflation\nderivatives.\n"
    },
    {
        "paper_id": 1512.03492,
        "authors": "Martin D. Gould, Julius Bonart",
        "title": "Queue Imbalance as a One-Tick-Ahead Price Predictor in a Limit Order\n  Book",
        "comments": "30 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate whether the bid/ask queue imbalance in a limit order book\n(LOB) provides significant predictive power for the direction of the next\nmid-price movement. We consider this question both in the context of a simple\nbinary classifier, which seeks to predict the direction of the next mid-price\nmovement, and a probabilistic classifier, which seeks to predict the\nprobability that the next mid-price movement will be upwards. To implement\nthese classifiers, we fit logistic regressions between the queue imbalance and\nthe direction of the subsequent mid-price movement for each of 10 liquid stocks\non Nasdaq. In each case, we find a strongly statistically significant\nrelationship between these variables. Compared to a simple null model, which\nassumes that the direction of mid-price changes is uncorrelated with the queue\nimbalance, we find that our logistic regression fits provide a considerable\nimprovement in binary and probabilistic classification for large-tick stocks,\nand provide a moderate improvement in binary and probabilistic classification\nfor small-tick stocks. We also perform local logistic regression fits on the\nsame data, and find that this semi-parametric approach slightly outperform our\nlogistic regression fits, at the expense of being more computationally\nintensive to implement.\n"
    },
    {
        "paper_id": 1512.03537,
        "authors": "Libin Yang and William Rea and and Alethea Rea",
        "title": "Identifying Highly Correlated Stocks Using the Last Few Principal\n  Components",
        "comments": "16 pages, 1 table, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the last few components in principal component analysis of the\ncorrelation matrix of a group of stocks may contain useful financial\ninformation by identifying highly correlated pairs or larger groups of stocks.\nThe results of this type of analysis can easily be included in the information\nan investor uses to manage their portfolio.\n"
    },
    {
        "paper_id": 1512.03618,
        "authors": "Jeroen Rozendaal, Yannick Malevergne and Didier Sornette",
        "title": "Macroeconomic Dynamics of Assets, Leverage and Trust",
        "comments": "32 pages with 10 figures",
        "journal-ref": null,
        "doi": "10.1142/S0218127416501339",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A macroeconomic model based on the economic variables (i) assets, (ii)\nleverage (defined as debt over asset) and (iii) trust (defined as the maximum\nsustainable leverage) is proposed to investigate the role of credit in the\ndynamics of economic growth, and how credit may be associated with both\neconomic performance and confidence. Our first notable finding is the mechanism\nof reward/penalty associated with patience, as quantified by the return on\nassets. In regular economies where the EBITA/Assets ratio is larger than the\ncost of debt, starting with a trust higher than leverage results in the highest\nlong-term return on assets (which can be seen as a proxy for economic growth).\nOur second main finding concerns a recommendation for the reaction of a central\nbank to an external shock that affects negatively the economic growth. We find\nthat late policy intervention in the model economy results in the highest\nlong-term return on assets and largest asset value. But this comes at the cost\nof suffering longer from the crisis until the intervention occurs. The\nphenomenon can be ascribed to the fact that postponing intervention allows\ntrust to increase first, and it is most effective to intervene when trust is\nhigh. These results derive from two fundamental assumptions underlying our\nmodel: (a) trust tends to increase when it is above leverage; (b) economic\nagents learn optimally to adjust debt for a given level of trust and amount of\nassets. Using a Markov Switching Model for the EBITA/Assets ratio, we have\nsuccessfully calibrated our model to the empirical data of the return on equity\nof the EURO STOXX 50 for the time period 2000-2013. We find that dynamics of\nleverage and trust can be highly non-monotonous with curved trajectories, as a\nresult of the nonlinear coupling between the variables.\n"
    },
    {
        "paper_id": 1512.03641,
        "authors": "Elisa Mastrogiacomo and Emanuela Rosazza Gianin",
        "title": "Time-consistency of cash-subadditive risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main goal of this paper is to investigate under which conditions\ncash-subadditive convex dynamic risk measures are time-consistent. Proceeding\nas in Detlefsen and Scandolo \\cite{detlef-scandolo} and inspired by their\nresult, we give a dual representation of dynamic cash-subadditive convex risk\nmeasures (that can also be seen as particular case of the dual quasiconvex\nrepresentation). The main result of the paper consists in providing, in the\ncash-subadditive case, a sufficient condition for strong time-consistency (or\nrecursivity) in terms of a generalized cocycle condition. On one hand, our\nresult can be seen as an extension to cash-subadditive convex dynamic risk\nmeasures of Theorem 2.5 in Bion-Nadal \\cite{bion-nadal-FS}; on the other hand,\nit is weaker since strong time-consistency is not fully characterized. Finally,\nwe exploit the relation between different notions of time-consistency.\n"
    },
    {
        "paper_id": 1512.03677,
        "authors": "Christian Bayer and John Schoenmakers",
        "title": "Option pricing in affine generalized Merton models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider affine generalizations of the Merton jump\ndiffusion model [Merton, J. Fin. Econ., 1976] and the respective pricing of\nEuropean options. On the one hand, the Brownian motion part in the Merton model\nmay be generalized to a log-Heston model, and on the other hand, the jump part\nmay be generalized to an affine process with possibly state dependent jumps.\nWhile the characteristic function of the log-Heston component is known in\nclosed form, the characteristic function of the second component may be unknown\nexplicitly. For the latter component we propose an approximation procedure\nbased on the method introduced in [Belomestny et al., J. Func. Anal., 2009]. We\nconclude with some numerical examples.\n"
    },
    {
        "paper_id": 1512.03743,
        "authors": "Joao da Gama Batista, Domenico Massaro, Jean-Philippe Bouchaud, Damien\n  Challet, Cars Hommes",
        "title": "Do investors trade too much? A laboratory experiment",
        "comments": "57 pages, 18 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We run experimental asset markets to investigate the emergence of excess\ntrading and the occurrence of synchronised trading activity leading to crashes\nin the artificial markets. The market environment favours early investment in\nthe risky asset and no posterior trading, i.e. a buy-and-hold strategy with a\nmost probable return of over 600%. We observe that subjects trade too much, and\ndue to the market impact that we explicitly implement, this is detrimental to\ntheir wealth. The asset market experiment was followed by risk aversion\nmeasurement. We find that preference for risk systematically leads to higher\nactivity rates (and lower final wealth). We also measure subjects' expectations\nof future prices and find that their actions are fully consistent with their\nexpectations. In particular, trading subjects try to beat the market and make\nprofits by playing a buy low, sell high strategy. Finally, we have not detected\nany major market crash driven by collective panic modes, but rather a weaker\nbut significant tendency of traders to synchronise their entry and exit points\nin the market.\n"
    },
    {
        "paper_id": 1512.03896,
        "authors": "Frank Gehmlich and Thorsten Schmidt",
        "title": "A generalized intensity based framework for single-name credit risk",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1411.4851",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The intensity of a default time is obtained by assuming that the default\nindicator process has an absolutely continuous compensator. Here we drop the\nassumption of absolute continuity with respect to the Lebesgue measure and only\nassume that the compensator is absolutely continuous with respect to a general\n$\\sigma$-finite measure. This allows for example to incorporate the\nMerton-model in the generalized intensity based framework. An extension of the\nBlack-Cox model is also considered. We propose a class of generalized Merton\nmodels and study absence of arbitrage by a suitable modification of the forward\nrate approach of Heath-Jarrow-Morton (1992). Finally, we study affine term\nstructure models which fit in this class. They exhibit stochastic\ndiscontinuities in contrast to the affine models previously studied in the\nliterature.\n"
    },
    {
        "paper_id": 1512.03963,
        "authors": "Micha{\\l} Barski",
        "title": "Incompleteness of the bond market with L\\'evy noise under the physical\n  measure",
        "comments": "25 pages",
        "journal-ref": "Advances in Mathematics of Finance, Banach Center Publications,\n  vol. 104, str. 61-83; (2015)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of completeness of the forward rate based bond market model\ndriven by a L\\'evy process under the physical measure is examined. The\nincompleteness of market in the case when the L\\'evy measure has a density\nfunction is shown. The required elements of the theory of stochastic\nintegration over the compensated jump measure under a martingale measure is\npresented and the corresponding integral representation of local martingales is\nproven.\n"
    },
    {
        "paper_id": 1512.0446,
        "authors": "Marco Bardoscia, Fabio Caccioli, Juan Ignacio Perotti, Gianna Vivaldo,\n  Guido Caldarelli",
        "title": "Distress propagation in complex networks: the case of non-linear\n  DebtRank",
        "comments": "9 pages, 5 figures",
        "journal-ref": "PLoS ONE 11(10): e0163825 (2016)",
        "doi": "10.1371/journal.pone.0163825",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a dynamical model of distress propagation on complex networks,\nwhich we apply to the study of financial contagion in networks of banks\nconnected to each other by direct exposures. The model that we consider is an\nextension of the DebtRank algorithm, recently introduced in the literature. The\nmechanics of distress propagation is very simple: When a bank suffers a loss,\ndistress propagates to its creditors, who in turn suffer losses, and so on. The\noriginal DebtRank assumes that losses are propagated linearly between connected\nbanks. Here we relax this assumption and introduce a one-parameter family of\nnon-linear propagation functions. As a case study, we apply this algorithm to a\ndata-set of 183 European banks, and we study how the stability of the system\ndepends on the non-linearity parameter under different stress-test scenarios.\nWe find that the system is characterized by a transition between a regime where\nsmall shocks can be amplified and a regime where shocks do not propagate, and\nthat the overall stability of the system increases between 2008 and 2013.\n"
    },
    {
        "paper_id": 1512.04583,
        "authors": "Yusong Li and Harry Zheng",
        "title": "Constrained Quadratic Risk Minimization via Forward and Backward\n  Stochastic Differential Equations",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a continuous-time stochastic linear quadratic control\nproblem arising from mathematical finance. We model the asset dynamics with\nrandom market coefficients and portfolio strategies with convex constraints.\nFollowing the convex duality approach, we show that the necessary and\nsufficient optimality conditions for both the primal and dual problems can be\nwritten in terms of processes satisfying a system of FBSDEs together with other\nconditions. We characterise explicitly the optimal wealth and portfolio\nprocesses as functions of adjoint processes from the dual FBSDEs in a dynamic\nfashion and vice versa. We apply the results to solve quadratic risk\nminimization problems with cone-constraints and derive the explicit\nrepresentations of solutions to the extended stochastic Riccati equations for\nsuch problems.\n"
    },
    {
        "paper_id": 1512.04714,
        "authors": "Micha{\\l} Barski, Jerzy Zabczyk",
        "title": "Heath-Jarrow-Morton-Musiela equation with L\\'evy perturbation",
        "comments": "42 pages",
        "journal-ref": "Journal of Differential Equations, (2012), 253, 9, p. 2657-2697",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies the Heath-Jarrow-Morton-Musiela equation of the bond\nmarket. The equation is analyzed in weighted spaces of functions defined on\n$[0,+\\infty)$. Sufficient conditions for local and global existence are\nobtained . For equation with the linear diffusion term the conditions for\nglobal existence are close to the necessary ones.\n"
    },
    {
        "paper_id": 1512.04716,
        "authors": "Mark Podolskij, Bezirgen Veliyev, Nakahiro Yoshida",
        "title": "Edgeworth expansion for the pre-averaging estimator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the Edgeworth expansion for a pre-averaging estimator\nof quadratic variation in the framework of continuous diffusion models observed\nwith noise. More specifically, we obtain a second order expansion for the joint\ndensity of the estimators of quadratic variation and its asymptotic variance.\nOur approach is based on martingale embedding, Malliavin calculus and stable\ncentral limit theorems for continuous diffusions. Moreover, we derive the\ndensity expansion for the studentized statistic, which might be applied to\nconstruct asymptotic confidence regions.\n"
    },
    {
        "paper_id": 1512.04741,
        "authors": "Damiano Brigo, Camilla Pisani, Francesco Rapisarda",
        "title": "The Multivariate Mixture Dynamics Model: Shifted dynamics and\n  correlation skew",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Multi Variate Mixture Dynamics model is a tractable, dynamical,\narbitrage-free multivariate model characterized by transparency on the\ndependence structure, since closed form formulae for terminal correlations,\naverage correlations and copula function are available. It also allows for\ncomplete decorrelation between assets and instantaneous variances. Each single\nasset is modelled according to a lognormal mixture dynamics model, and this\nunivariate version is widely used in the industry due to its flexibility and\naccuracy. The same property holds for the multivariate process of all assets,\nwhose density is a mixture of multivariate basic densities. This allows for\nconsistency of single asset and index/portfolio smile. In this paper, we\ngeneralize the MVMD model by introducing shifted dynamics and we propose a\ndefinition of implied correlation under this model. We investigate whether the\nmodel is able to consistently reproduce the implied volatility of FX cross\nrates once the single components are calibrated to univariate shifted lognormal\nmixture dynamics models. We consider in particular the case of the Chinese\nrenminbi FX rate, showing that the shifted MVMD model correctly recovers the\nCNY/EUR smile given the EUR/USD smile and the USD/CNY smile, thus highlighting\nthat the model can also work as an arbitrage free volatility smile\nextrapolation tool for cross currencies that may not be liquid or fully\nobservable. We compare the performance of the shifted MVMD model in terms of\nimplied correlation with those of the shifted Simply Correlated Mixture\nDynamics model where the dynamics of the single assets are connected naively by\nintroducing correlation among their Brownian motions. Finally, we introduce a\nmodel with uncertain volatilities and correlation. The Markovian projection of\nthis model is a generalization of the shifted MVMD model.\n"
    },
    {
        "paper_id": 1512.04916,
        "authors": "Ruoxuan Xiong, Eric P. Nichols and Yuan Shen",
        "title": "Deep Learning Stock Volatility with Google Domestic Trends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have applied a Long Short-Term Memory neural network to model S&P 500\nvolatility, incorporating Google domestic trends as indicators of the public\nmood and macroeconomic factors. In a held-out test set, our Long Short-Term\nMemory model gives a mean absolute percentage error of 24.2%, outperforming\nlinear Ridge/Lasso and autoregressive GARCH benchmarks by at least 31%. This\nevaluation is based on an optimal observation and normalization scheme which\nmaximizes the mutual information between domestic trends and daily volatility\nin the training set. Our preliminary investigation shows strong promise for\nbetter predicting stock behavior via deep learning and neural network models.\n"
    },
    {
        "paper_id": 1512.05015,
        "authors": "Christopher W. Miller, Insoon Yang",
        "title": "Optimal Control of Conditional Value-at-Risk in Continuous Time",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization,\" 55(2), pp.856-884, 2017",
        "doi": "10.1137/16M1058492",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider continuous-time stochastic optimal control problems featuring\nConditional Value-at-Risk (CVaR) in the objective. The major difficulty in\nthese problems arises from time-inconsistency, which prevents us from directly\nusing dynamic programming. To resolve this challenge, we convert to an\nequivalent bilevel optimization problem in which the inner optimization problem\nis standard stochastic control. Furthermore, we provide conditions under which\nthe outer objective function is convex and differentiable. We compute the outer\nobjective's value via a Hamilton-Jacobi-Bellman equation and its gradient via\nthe viscosity solution of a linear parabolic equation, which allows us to\nperform gradient descent. The significance of this result is that we provide an\nefficient dynamic programming-based algorithm for optimal control of CVaR\nwithout lifting the state-space. To broaden the applicability of the proposed\nalgorithm, we propose convergent approximation schemes in cases where our key\nassumptions do not hold and characterize relevant suboptimality bounds. In\naddition, we extend our method to a more general class of risk metrics, which\nincludes mean-variance and median-deviation. We also demonstrate a concrete\napplication to portfolio optimization under CVaR constraints. Our results\ncontribute an efficient framework for solving time-inconsistent CVaR-based\nsequential optimization.\n"
    },
    {
        "paper_id": 1512.05066,
        "authors": "Hiroyasu Inoue",
        "title": "Analyses of Aggregate Fluctuations of Firm Network Based on the\n  Self-Organized Criticality Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examine the difference in the size of avalanches among industries\ntriggered by demand shocks, which can be rephrased by control of the economy or\nfiscal policy, and by using the production-inventory model and observed data.\nWe obtain the following results. (1) The size of avalanches follows power law.\n(2) The mean sizes of avalanches for industries are diverse but their standard\ndeviations highly overlap. (3) We compare the simulation with an input-output\ntable and with the actual policies. They are compatible.\n"
    },
    {
        "paper_id": 1512.05074,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the Economic Growth in Asia",
        "comments": "9 pages, 2 figures, 3848 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical economic growth in Asia (excluding Japan) is analysed. It is shown\nthat Unified Growth Theory is contradicted by the data, which were used (but\nnot analysed) during the formulation of this theory. Unified Growth Theory does\nnot explain the mechanism of economic growth. It explains the mechanism of\nMalthusian stagnation, which did not exist and it explains the mechanism of the\ntransition from stagnation to growth that did not happen. The data show that\nthe economic growth in Asia was never stagnant but hyperbolic. The alleged\ndramatic takeoff around 1900 or around any other time did not happen. However,\nthe theory contains also a dangerous and strongly-misleading concept that after\na long epoch of stagnation we have now entered the epoch of sustained economic\ngrowth, the concept creating the sense of security. The opposite is true. After\nthe epoch of sustained and secure economic growth we have now entered the epoch\nof a fast-increasing and insecure economic growth.\n"
    },
    {
        "paper_id": 1512.05321,
        "authors": "Micha{\\l} Barski, Jerzy Zabczyk",
        "title": "Forward rate models with linear volatilities",
        "comments": "21 pages. arXiv admin note: text overlap with arXiv:0911.1119",
        "journal-ref": "Finance and Stochastics, (2012), 16,3, 537-560",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existence of solutions to the Heath-Jarrow-Morton equation of the bond market\nwith linear volatility and general L\\'evy random factor is studied. Conditions\nfor existence and non-existence of solutions in the class of bounded fields are\npresented. For the existence of solutions the L\\'evy process should necessarily\nbe without the Gaussian part and without negative jumps. If this is the case\nthen necessary and sufficient conditions for the existence are formulated\neither in terms of the behavior of the L\\'evy measure of the noise near the\norigin or the behavior of the Laplace exponent of the noise at infinity.\n"
    },
    {
        "paper_id": 1512.05343,
        "authors": "Tobias Baltensperger, Rudolf M. F\\\"uchslin, Pius Kr\\\"utli, and John\n  Lygeros",
        "title": "European Union gas market development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recently announced Energy Union by the European Commission is the most\nrecent step in a series of developments aiming at integrating the EU's gas\nmarkets to increase social welfare (SW) and security of gas supply. Based on a\nspatial partial equilibrium model, we analyze the changes in consumption,\nprices, and SW up to 2022 induced by the infrastructure expansions planned for\nthis period. We find that wholesale prices decrease slightly and converge at\nWestern European levels, the potential of suppliers to exert market power\ndecreases significantly, and consumer surplus increases by 15.9% in the EU. Our\nresults allow us to distinguish three categories of projects: (i) New gas\nsources developed and brought to the EU markets. These projects decrease prices\nand increase SW in a large number of countries. The only project in this\ncategory is the Trans-Anatolian Gas Pipeline; (ii) Existing gas sources made\navailable to additional countries. This leads to an increase of SW in the newly\nconnected countries, and a decrease everywhere else. These projects mainly\ninvolve pipeline and regasification terminal capacity enhancements; (iii)\nProjects with a marginal effect on the (fully functioning) market. Most storage\nexpansion projects fall into this category, plus the recently announced Turkish\nStream. Our results indicate that if all proposed infrastructure projects are\nrealized, the EU's single market will become a reality in 2019. However, we\nalso find that SW can only be increased significantly for the EU as a whole if\nnew gas sources become accessible. Consequently, we suggest that the EU should\nemphasize on measures to increase the available volumes, in particular once the\nintegration of the market is completed. At the same time, efficiency gains,\nalbeit decreasing SW, help to improve the situation of consumers and decrease\nthe dependency of the EU as a whole on external suppliers.\n"
    },
    {
        "paper_id": 1512.05377,
        "authors": "Mauricio Contreras, Rely Pellicer, Daniel Santiagos and Marcelo\n  Villena",
        "title": "Calibration and simulation of arbitrage effects in a non-equilibrium\n  quantum Black-Scholes model by using semiclassical methods",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An interacting Black-Scholes model for option pricing, where the usual\nconstant interest rate r is replaced by a stochastic time dependent rate r(t)\nof the form r(t)=r+f(t) dW/dt, accounting for market imperfections and prices\nnon-alignment, was developed in [1]. The white noise amplitude f(t), called\narbitrage bubble, generates a time dependent potential U(t) which changes the\nusual equilibrium dynamics of the traditional Black-Scholes model. The purpose\nof this article is to tackle the inverse problem, that is, is it possible to\nextract the time dependent potential U(t) and its associated bubble shape f(t)\nfrom the real empirical financial data? In order to give an answer to this\nquestion, the interacting Black-Scholes equation must be interpreted as a\nquantum Schrodinger equation with hamiltonian operator H=H0+U(t), where H0 is\nthe equilibrium Black-Scholes hamiltonian and U(t) is the interaction term. If\nthe U(t) term is small enough, the interaction potential can be thought as a\nperturbation, so one can compute the solution of the interacting Black-Scholes\nequation in an approximate form by perturbation theory. In [2] by applying the\nsemi-classical considerations, an approximate solution of the non equilibrium\nBlack-Scholes equation for an arbitrary bubble shape f(t) was developed. Using\nthis semi-classical solution and the knowledge about the mispricing of the\nfinancial data, one can determinate an equation, which solutions permit obtain\nthe functional form of the potential term U(t) and its associated bubble f(t).\nIn all the studied cases, the non equilibrium model performs a better\nestimation of the real data than the usual equilibrium model. It is expected\nthat this new and simple methodology for calibrating and simulating option\npricing solutions in the presence of market imperfections, could help to\nimprove option pricing estimations.\n"
    },
    {
        "paper_id": 1512.05924,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "Quadratic-exponential growth BSDEs with Jumps and their Malliavin's\n  Differentiability",
        "comments": "Forthcoming in Stochastic Processes and their Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a class of quadratic-exponential growth BSDEs with jumps. The\nquadratic structure introduced by Barrieu & El Karoui (2013) yields the\nuniversal bounds on the possible solutions. With local Lipschitz continuity and\nthe so-called A_gamma-condition for the comparison principle to hold, we prove\nthe existence of a unique solution under the general quadratic-exponential\nstructure. We have also shown that the strong convergence occurs under more\ngeneral (not necessarily monotone) sequence of drivers, which is then applied\nto give the sufficient conditions for the Malliavin's differentiability.\n"
    },
    {
        "paper_id": 1512.05983,
        "authors": "Fred Espen Benth, Paul Kr\\\"uhner",
        "title": "Approximation of forward curve models in commodity markets with\n  arbitrage-free finite dimensional models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show how to approximate a Heath-Jarrow-Morton dynamics for\nthe forward prices in commodity markets with arbitrage-free models which have a\nfinite dimensional state space. Moreover, we recover a closed form\nrepresentation of the forward price dynamics in the approximation models and\nderive the rate of convergence uniformly over an interval of time to maturity\nto the true dynamics under certain additional smoothness conditions. In the\nMarkovian case we can strengthen the convergence to be uniform over time as\nwell. Our results are based on the construction of a convenient Riesz basis on\nthe state space of the term structure dynamics.\n"
    },
    {
        "paper_id": 1512.06151,
        "authors": "Oleksii Patsiuk, Sergii Kovalenko",
        "title": "Symmetry reduction and exact solutions of the non-linear Black--Scholes\n  equation",
        "comments": "Published version, 16 pages (in preprint, 10 pages in print), 2\n  figures",
        "journal-ref": "Commun Nonlinear Sci Numer Simulat 62 (2018) 164--173",
        "doi": "10.1016/j.cnsns.2018.02.028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the non-linear Black--Scholes equation:\n$$u_t+ax^2u_{xx}+bx^3u_{xx}^2+c(xu_x-u)=0,\\quad a,b>0,\\ c\\geq0.$$ and show that\nthe one can be reduced to the equation $$u_t+(u_{xx}+u_x)^2=0$$ by an\nappropriate point transformation of variables. For the resulting equation, we\nstudy the group-theoretic properties, namely, we find the maximal algebra of\ninvariance of its in Lie sense, carry out the symmetry reduction and seek for a\nnumber of exact group-invariant solutions of the equation. Using the results\nobtained, we get a number of exact solutions of the Black--Scholes equation\nunder study and apply the ones to resolving several boundary value problems\nwith appropriate from the economic point of view terminal and boundary\nconditions.\n"
    },
    {
        "paper_id": 1512.06159,
        "authors": "Richard Y. Chen, Per A. Mykland",
        "title": "Model-Free Approaches to Discern Non-Stationary Microstructure Noise and\n  Time-Varying Liquidity in High-Frequency Data",
        "comments": null,
        "journal-ref": "Journal of Econometrics, Volume 200, Issue 1, September 2017,\n  Pages 79-103",
        "doi": "10.1016/j.jeconom.2017.05.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide non-parametric statistical tools to test\nstationarity of microstructure noise in general hidden Ito semimartingales, and\ndiscuss how to measure liquidity risk using high frequency financial data. In\nparticular, we investigate the impact of non-stationary microstructure noise on\nsome volatility estimators, and design three complementary tests by exploiting\nedge effects, information aggregation of local estimates and high-frequency\nasymptotic approximation. The asymptotic distributions of these tests are\navailable under both stationary and non-stationary assumptions, thereby enable\nus to conservatively control type-I errors and meanwhile ensure the proposed\ntests enjoy the asymptotically optimal statistical power. Besides it also\nenables us to empirically measure aggregate liquidity risks by these test\nstatistics. As byproducts, functional dependence and endogenous microstructure\nnoise are briefly discussed. Simulation with a realistic configuration\ncorroborates our theoretical results, and our empirical study indicates the\nprevalence of non-stationary microstructure noise in New York Stock Exchange.\n"
    },
    {
        "paper_id": 1512.06228,
        "authors": "Abhijit Sharang and Chetan Rao",
        "title": "Using machine learning for medium frequency derivative portfolio trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use machine learning for designing a medium frequency trading strategy for\na portfolio of 5 year and 10 year US Treasury note futures. We formulate this\nas a classification problem where we predict the weekly direction of movement\nof the portfolio using features extracted from a deep belief network trained on\ntechnical indicators of the portfolio constituents. The experimentation shows\nthat the resulting pipeline is effective in making a profitable trade.\n"
    },
    {
        "paper_id": 1512.06247,
        "authors": "Chris Kenyon, Andrew Green and Mourad Berrahoui",
        "title": "Which measure for PFE? The Risk Appetite Measure, A",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Potential Future Exposure (PFE) is a standard risk metric for managing\nbusiness unit counterparty credit risk but there is debate on how it should be\ncalculated. The debate has been whether to use one of many historical\n(\"physical\") measures (one per calibration setup), or one of many risk-neutral\nmeasures (one per numeraire). However, we argue that limits should be based on\nthe bank's own risk appetite provided that this is consistent with regulatory\nbacktesting and that whichever measure is used it should behave (in a sense\nmade precise) like a historical measure. Backtesting is only required by\nregulators for banks with IMM approval but we expect that similar methods are\npart of limit maintenance generally. We provide three methods for computing the\nbank price of risk from readily available business unit data, i.e. business\nunit budgets (rate of return) and limits (e.g. exposure percentiles). Hence we\ndefine and propose a Risk Appetite Measure, A, for PFE and suggest that this is\nuniquely consistent with the bank's Risk Appetite Framework as required by\nsound governance.\n"
    },
    {
        "paper_id": 1512.06295,
        "authors": "Ljudmila A. Bordag, Ivan P. Yamshchikov",
        "title": "Optimization problem for a portfolio with an illiquid asset: Lie group\n  analysis",
        "comments": "46 pages",
        "journal-ref": "J. Math. Anal. Appl. 453, pp. 668-699, 2017",
        "doi": "10.1016/j.jmaa.2017.04.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Management of a portfolio that includes an illiquid asset is an important\nproblem of modern mathematical finance. One of the ways to model illiquidity\namong others is to build an optimization problem and assume that one of the\nassets in a portfolio can not be sold until a certain finite, infinite or\nrandom moment of time. This approach arises a certain amount of models that are\nactively studied at the moment.\n  Working in the Merton's optimal consumption framework with continuous time we\nconsider an optimization problem for a portfolio with an illiquid, a risky and\na risk-free asset. Our goal in this paper is to carry out a complete Lie group\nanalysis of PDEs describing value function and investment and consumption\nstrategies for a portfolio with an illiquid asset that is sold in an exogenous\nrandom moment of time with a prescribed liquidation time distribution. The\nproblem of such type leads to three dimensional nonlinear\nHamilton-Jacobi-Bellman (HJB) equations. Such equations are not only tedious\nfor analytical methods but are also quite challenging form a numeric point of\nview. To reduce the three-dimensional problem to a two-dimensional one or even\nto an ODE one usually uses some substitutions, yet the methods used to find\nsuch substitutions are rarely discussed by the authors.\n  We find the admitted Lie algebra for a broad class of liquidation time\ndistributions in cases of HARA and log utility functions and formulate\ncorresponding theorems for all these cases. We use found Lie algebras to obtain\nreductions of the studied equations. Several of similar substitutions were used\nin other papers before whereas others are new to our knowledge. This method\ngives us the possibility to provide a complete set of non-equivalent\nsubstitutions and reduced equations.\n"
    },
    {
        "paper_id": 1512.06309,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the Economic Growth in the Former\n  USSR",
        "comments": "8 pages, 3 figures, 3379 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical economic growth in countries of the former USSR is analysed. It is\nshown that Unified Growth Theory is contradicted by the data, which were used,\nbut not analysed, during the formulation of this theory. Unified Growth Theory\ndoes not explain the mechanism of economic growth. It explains the mechanism of\nMalthusian stagnation, which did not exist and it explains the mechanism of the\ntransition from stagnation to growth that did not happen. Unified Growth Theory\nis full of stories but it is hard to decide which of them are reliable because\nthey are based on unprofessional examination of data. The data show that the\neconomic growth in the former USSR was never stagnant but hyperbolic.\nIndustrial Revolution did not boost the economic growth in the former USSR.\nUnified Growth Theory needs to be revised or replaced by a reliable theory to\nreconcile it with data and to avoid creating the unwarranted sense of security\nabout the current economic growth.\n"
    },
    {
        "paper_id": 1512.06449,
        "authors": "V.A. Kalyagin, P.A. Koldanov, P.M. Pardalos",
        "title": "Optimal decision for the market graph identification problem in sign\n  similarity network",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investigation of the market graph attracts a growing attention in market\nnetwork analysis. One of the important problem connected with market graph is\nto identify it from observations. Traditional way for the market graph\nidentification is to use a simple procedure based on statistical estimations of\nPearson correlations between pairs of stocks. Recently a new class of\nstatistical procedures for the market graph identification was introduced and\noptimality of these procedures in Pearson correlation Gaussian network was\nproved. However the obtained procedures have a high reliability only for\nGaussian multivariate distributions of stocks attributes. One of the way to\ncorrect this drawback is to consider a different networks generated by\ndifferent measures of pairwise similarity of stocks. A new and promising model\nin this context is the sign similarity network. In the present paper the market\ngraph identification problem in sign similarity network is considered. A new\nclass of statistical procedures for the market graph identification is\nintroduced and optimality of these procedures is proved. Numerical experiments\ndetect essential difference in quality of optimal procedures in sign similarity\nand Pearson correlation networks. In particular it is observed that the quality\nof optimal identification procedure in sign similarity network is not sensitive\nto the assumptions on distribution of stocks attributes.\n"
    },
    {
        "paper_id": 1512.06454,
        "authors": "Philipp Harms, David Stefanovits, Josef Teichmann, Mario V. W\\\"uthrich",
        "title": "Consistent Re-Calibration of the Discrete-Time Multifactor Vasi\\v{c}ek\n  Model",
        "comments": "29 pages, 16 figures, 2 tables",
        "journal-ref": "Risks 4, 3 (2016), pp. 1-31",
        "doi": "10.3390/risks4030018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The discrete-time multifactor Vasi\\v{c}ek model is a tractable Gaussian spot\nrate model. Typically, two- or three-factor versions allow one to capture the\ndependence structure between yields with different times to maturity in an\nappropriate way. In practice, re-calibration of the model to the prevailing\nmarket conditions leads to model parameters that change over time. Therefore,\nthe model parameters should be understood as being time-dependent or even\nstochastic. Following the consistent re-calibration (CRC) approach, we\nconstruct models as concatenations of yield curve increments of Hull-White\nextended multifactor Vasi\\v{c}ek models with different parameters. The CRC\napproach provides attractive tractable models that preserve the no-arbitrage\npremise. As a numerical example, we fit Swiss interest rates using CRC\nmultifactor Vasi\\v{c}ek models.\n"
    },
    {
        "paper_id": 1512.06486,
        "authors": "Libin Yang and William Rea and Alethea Rea",
        "title": "How much diversification potential is there in a single market? Evidence\n  from the Australian Stock Exchange",
        "comments": "19 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present four methods of assessing the diversification potential within a\nstock market, two of these are based on principal component analysis. They were\napplied to the Australian stock exchange for the years 2000 to 2014 and all\nshow a consistent picture. The potential for diversification declined almost\nmonotonically in the three years prior to the 2008 financial crisis. On one of\nthe measures the diversification potential declined even further in the 2011\nEuropean debt crisis and the American credit downgrade.\n"
    },
    {
        "paper_id": 1512.06582,
        "authors": "Micha{\\l} Barski",
        "title": "Asymptotic pricing in large financial markets",
        "comments": "18 pages",
        "journal-ref": "Mathematical Methods of Operation Research, 2007, 66, 1-20",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of hedging and pricing sequences of contingent claims in large\nfinancial markets is studied. Connection between asymptotic arbitrage and\nbehavior of the $\\alpha$~-~quantile price is shown. The large Black-Scholes\nmodel is carefully examined.\n"
    },
    {
        "paper_id": 1512.06812,
        "authors": "Michael R. Tehranchi",
        "title": "Uniform bounds for Black--Scholes implied volatility",
        "comments": null,
        "journal-ref": "SIAM Journal on Financial Mathematics 7(1): 893-916 (2016)",
        "doi": "10.1137/14095248X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, Black--Scholes implied volatility is expressed in terms of\nvarious optimisation problems. From these representations, upper and lower\nbounds are derived which hold uniformly across moneyness and call price.\nVarious symmetries of the Black--Scholes formula are exploited to derive new\nbounds from old. These bounds are used to reprove asymptotic formulae for\nimplied volatility at extreme strikes and/or maturities.\n"
    },
    {
        "paper_id": 1512.0696,
        "authors": "Demian Pouzo and Ignacio Presno",
        "title": "Sovereign Default Risk and Uncertainty Premia",
        "comments": "Accepted for publication at American Economic Journal: Macroeconomics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies how international investors' concerns about model\nmisspecification affect sovereign bond spreads. We develop a general\nequilibrium model of sovereign debt with endogenous default wherein investors\nfear that the probability model of the underlying state of the borrowing\neconomy is misspecified. Consequently, investors demand higher returns on their\nbond holdings to compensate for the default risk in the context of uncertainty.\nIn contrast with the existing literature on sovereign default, we match the\nbond spreads dynamics observed in the data together with other business cycle\nfeatures for Argentina, while preserving the default frequency at historical\nlow levels.\n"
    },
    {
        "paper_id": 1512.07087,
        "authors": "B Bouchard (CEREMADE), G Loeper (FiQuant), Y Zou (CEREMADE)",
        "title": "Hedging of covered options with linear market impact and gamma\n  constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within a financial model with linear price impact, we study the problem of\nhedging a covered European option under gamma constraint. Using stochastic\ntarget and partial differential equation smoothing techniques, we prove that\nthe super-replication price is the viscosity solution of a fully non-linear\nparabolic equation. As a by-product, we show how $\\epsilon$-optimal strategies\ncan be constructed. Finally, a numerical resolution scheme is proposed.\n"
    },
    {
        "paper_id": 1512.07256,
        "authors": "Damiano Brigo, Nicola Pede, Andrea Petrelli",
        "title": "Multi Currency Credit Default Swaps Quanto effects and FX devaluation\n  jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit Default Swaps (CDS) on a reference entity may be traded in multiple\ncurrencies, in that protection upon default may be offered either in the\ndomestic currency where the entity resides, or in a more liquid and global\nforeign currency. In this situation currency fluctuations clearly introduce a\nsource of risk on CDS spreads. For emerging markets, but in some cases even in\nwell developed markets, the risk of dramatic Foreign Exchange (FX) rate\ndevaluation in conjunction with default events is relevant. We address this\nissue by proposing and implementing a model that considers the risk of foreign\ncurrency devaluation that is synchronous with default of the reference entity.\n  Preliminary results indicate that perceived risks of devaluation can induce a\nsignificant basis across domestic and foreign CDS quotes. For the Republic of\nItaly, a USD CDS spread quote of 440 bps can translate into a EUR quote of 350\nbps in the middle of the Euro-debt crisis in the first week of May 2012. More\nrecently, from June 2013, the basis spreads between the EUR quotes and the USD\nquotes are in the range around 40 bps.\n  We explain in detail the sources for such discrepancies. Our modeling\napproach is based on the reduced form framework for credit risk, where the\ndefault time is modeled in a Cox process setting with explicit diffusion\ndynamics for default intensity/hazard rate and exponential jump to default. For\nthe FX part, we include an explicit default-driven jump in the FX dynamics. As\nour results show, such a mechanism provides a further and more effective way to\nmodel credit / FX dependency than the instantaneous correlation that can be\nimposed among the driving Brownian motions of default intensity and FX rates,\nas it is not possible to explain the observed basis spreads during the\nEuro-debt crisis by using the latter mechanism alone.\n"
    },
    {
        "paper_id": 1512.07337,
        "authors": "Wujiang Lou",
        "title": "MVA Transfer Pricing",
        "comments": "18 pages, 6 tables, 1 figure",
        "journal-ref": "Risk, July 2016, pp 72-77",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article prices OTC derivatives with either an exogenously determined\ninitial margin profile or endogenously approximated initial margin. In the\nformer case, margin valuation adjustment (MVA) is defined as the liability-side\ndiscounted expected margin profile, while in the latter, an extended partial\ndifferential equation is derived and solved for an all-in fair value,\ndecomposable into coherent CVA, FVA and MVA. For uncollateralized customer\ntrades, MVA can be transferred to the customer via an extension of the\nliability-side pricing theory. For BCBS-IOSCO covered OTC derivatives, a market\nmaker has to charge financial counterparties a bid-ask spread to transfer its\nfunding cost. An IM multiplier is applied to calibrate to external IM models to\nallow portfolio incremental pricing. In particular, a link to ISDA SIMM for\nequity, commodity and fx risks is established through the PDE with its vega and\ncurvature IM components captured fully. Numerical examples are given for swaps\nand equity portfolios and offer a plausible attribution of recent CME-LCH basis\nspread widening to elevated MVA accompanying dealers' hedging of customer\nflows.\n"
    },
    {
        "paper_id": 1512.0734,
        "authors": "Wujiang Lou",
        "title": "Liability-side Pricing of Swaps and Coherent CVA and FVA by\n  Regression/Simulation",
        "comments": "19 pages, 5 tables, 2 figures; Global Derivatives USA Conference\n  (2014), Chicago",
        "journal-ref": "Liability-side pricing of swaps, RISK, April 2016, pp 66-71",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An uncollateralized swap hedged back-to-back by a CCP swap is used to\nintroduce FVA. The open IR01 of FVA, however, is a sure sign of risk not being\nfully hedged, a theoretical no-arbitrage pricing concern, and a bait to lure\nmarket risk capital, a practical business concern. By dynamically trading the\nCCP swap, with the liability-side counterparty provides counterparty exposure\nhedge and swap funding, we find that the uncollateralized swap can be fully\nreplicated, leaving out no IR01 leakage. The fair value of the swap is obtained\nby applying to swap's net cash flows a discount rate switching to\ncounterparty's bond curve if the swap is a local asset or one's own curve if a\nliability, and the total valuation adjustment is the present value of cost of\nfunding the risk-free price discounted at the same switching rate. FVA is\nredefined as a liquidity or funding basis component of total valuation\nadjustment, coherent with CVA, the default risk component. A Longstaff-Schwartz\nstyle least-square regression and simulation is introduced to compute the\nrecursive fair value and adjustments. A separately developed finite difference\nscheme is used to test and find regression necessary to decouple the discount\nrate switch. Preliminary results show the impact of counterparty risk to swap\nhedge ratios, swap bid/ask spreads, and valuation adjustments, and considerable\nerrors of calculating CVA by discounting cash flow or potential future\nexposure.\n"
    },
    {
        "paper_id": 1512.08037,
        "authors": "Louis R. Eeckhoudt and Roger J. A. Laeven",
        "title": "Risk Aversion in the Small and in the Large under Rank-Dependent Utility",
        "comments": "15 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under expected utility the local index of absolute risk aversion has played a\ncentral role in many applications. Besides, its link with the \"global\" concepts\nof the risk and probability premia has reinforced its attractiveness. This\npaper shows that, with an appropriate approach, similar developments can be\nachieved in the framework of Yaari's dual theory and, more generally, under\nrank-dependent utility.\n"
    },
    {
        "paper_id": 1512.08067,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the Economic Growth in Europe",
        "comments": "10 pages, 6 figures, 3989 words; fourth paper in the series of 5\n  discussing economic growth in Africa, Asia, former USSR, Europe and Latin\n  America",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical economic growth in Western and Eastern Europe is analysed. These\nregions should have produced the best and the most convincing confirmation of\nthe Unified Growth Theory because they, and in particular Western Europe, were\nthe centre of the Industrial Revolution, which according to Galor was the prime\nengine of economic growth. However, the data for Western and Eastern Europe\nshow a remarkable disagreement with the Unified Growth Theory. There is no\nconnection, whatever, between the data and the Unified Growth Theory. The data\nshow that there was never a transition from stagnation to growth because there\nwas no stagnation. Industrial Revolution, which should have the strongest\ninfluence in these regions, had absolutely no impact on changing the economic\ngrowth trajectories. The alleged remarkable or stunning escape from Malthusian\ntrap did not happen because there was no trap. Unified Growth Theory does not\nexplain the mechanism of the economic growth because its explanations are based\non mythical features, which did not exist, the features contradicted by data.\nThis theory needs to be either thoroughly revised or most likely replaced by a\ntheory supported by a professional analysis of economic growth data.\n"
    },
    {
        "paper_id": 1512.08098,
        "authors": "Valentin Vankov Iliev",
        "title": "On a Generalization of Markowitz Preference Relation",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given two families of continuous functions $u$ and $v$ on a topological space\n$X$, we define a preorder $R=R(u,v)$ on $X$ by the condition that any member of\n$u$ is an $R$-increasing and any member of $v$ is an $R$-decreasing function.\nIt turns out that if the topological space $X$ is quasi-compact and\nsequentially compact, then any element of $X$ is $R$-dominated by an\n$R$-maximal element of $X$. In particular, since the $(n-1)$-dimensional\nsimplex is a compact subset of the real $n$-dimensional vector space, then\nconsidering its members as portfolios consisting of $n$ financial assets, we\nobtain the classical 1952 result of Harry Markowitz that any portfolio is\ndominated by an efficient portfolio. Moreover, several other examples of\npossible application of this general setup are presented.\n"
    },
    {
        "paper_id": 1512.08381,
        "authors": "Nils Bertschinger and Oliver Pfante",
        "title": "Inferring Volatility in the Heston Model and its Relatives -- an\n  Information Theoretical Approach",
        "comments": "26 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:0804.2589 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic volatility models describe asset prices $S_t$ as driven by an\nunobserved process capturing the random dynamics of volatility $\\sigma_t$.\nHere, we quantify how much information about $\\sigma_t$ can be inferred from\nasset prices $S_t$ in terms of Shannon's mutual information $I(S_t :\n\\sigma_t)$. This motivates a careful numerical and analytical study of\ninformation theoretic properties of the Heston model. In addition, we study a\ngeneral class of discrete time models motivated from a machine learning\nperspective. In all cases, we find a large uncertainty in volatility estimates\nfor quite fundamental information theoretic reasons.\n"
    },
    {
        "paper_id": 1512.08792,
        "authors": "Valerii Salov",
        "title": "The Role of Time in Making Risky Decisions and the Function of Choice",
        "comments": "52 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The prospects of Kahneman and Tversky, Mega Million and Powerball lotteries,\nSt. Petersburg paradox, premature profits and growing losses criticized by\nLivermore are reviewed under an angle of view comparing mathematical\nexpectations with awards received. Original prospects have been formulated as a\none time opportunity. An award value depends on the number of times the game is\nplayed. The random sample mean is discussed as a universal award. The role of\ntime in making a risky decision is important as long as the frequency of games\nand playing time affect their number. A function of choice mapping properties\nof two-point random variables to fractions of respondents choosing them is\nproposed.\n"
    },
    {
        "paper_id": 1512.08866,
        "authors": "Wai-Ki Ching, Jia-Wen Gu, Qing-Qing Yang and Tak-Kuen Siu",
        "title": "On Optimal Pricing Model for Multiple Dealers in a Competitive Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the optimal pricing strategy in Avellande-Stoikov's for a\nmonopolistic dealer is extended to a general situation where multiple dealers\nare present in a competitive market. The dealers' trading intensities, their\noptimal bid and ask prices and therefore their spreads are derived when the\ndealers are informed the severity of the competition. The effects of various\nparameters on the bid-ask quotes and profits of the dealers in a competitive\nmarket are also discussed. This study gives some insights on the average\nspread, profit of the dealers in a competitive trading environment.\n"
    },
    {
        "paper_id": 1512.0928,
        "authors": "A.K.M. Azhar, Vincent B.Y. Gan, W.A.T. Wan Abdullah, H. Zainuddin",
        "title": "On the Fractal Geometry of the Balance Sheet and the Fractal Index of\n  Insolvency Risk",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper reviews the economic and theoretical foundations of insolvency\nrisk measurement and capital adequacy rules. The proposed new measure of\ninsolvency risk is constructed by disentangling assets, debt and equity at the\nmicro-prudential firm level. This new risk index is the Firm Insolvency Risk\nIndex (FIRI) which is symmetrical, proportional and scale invariant. We\ndemonstrate that the balance sheet can be shown to evolve with a fractal\npattern. As such we construct a fractal index that can measure the risk of\nassets. This index can differentiate between the similarity and dissimilarity\nin asset risk, and it will also possess the properties of being self-similar\nand invariant to firm characteristics that make up its asset composition hence\ninvariant to all types of risk derived from assets. Self-similarity and scale\ninvariance across the cross section allows direct comparison of degrees of risk\nin assets. This is by comparing the risk dissimilarity of assets. Being\nnaturally bounded to its highest upper bound, (0,2], the fractal index is able\nto serve like a risk thermometer. We assign geometric probabilities of\ninsolvency P (equity is equal or less than 0 conditional on debt being greater\nthan 0).\n"
    },
    {
        "paper_id": 1601.00085,
        "authors": "Ravi Kashyap",
        "title": "Dynamic Multi-Factor Bid-Offer Adjustment Model: A Feedback Mechanism\n  for Dealers (Market Makers) to Deal (Grapple) with the Uncertainty Principle\n  of the Social Sciences",
        "comments": null,
        "journal-ref": "The Journal of Trading, 9(3) 2014, 42-55",
        "doi": "10.3905/jot.2014.9.3.042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The author seeks to develop a model to alter the bid-offer spread, currently\nquoted by market makers, that varies with the market and trading conditions.\nThe dynamic nature of financial markets and trading, as with the rest of social\nsciences, where changes can be observed and decisions can be made by\nparticipants to influence the system, means that this model has to be adaptive\nand include a feedback loop that alters the bid-offer adjustment based on the\nmodifications observed in the market and trading conditions, without a\nsignificant time delay.\n  The factors used to adjust the spread are price volatility, which is publicly\nobservable, and trade count and volume, which are generally known only to the\nmarket maker, in various instruments over different historical durations in\ntime. The contributions of each factor to the bid-offer adjustment are computed\nseparately and then consolidated to produce a very adaptive bid-offer\nquotation. The author uses the currency markets to build the sample model\nbecause they are extremely liquid and trading in them is not as transparent as\nother financial instruments, such as equities. Simulating the number of trades\nand the average size of trades from a lognormal distribution, the parameters of\nthe lognormal distributions are chosen such that the total volume in a certain\ninterval matches the volume publicly mentioned by currency trading firms. This\nmethodology can easily be extended to other financial instruments and possibly\nto any product with the ability to make electronic price quotations, or can\neven be used to periodically perform manual price updates on products that are\ntraded non-electronically.\n"
    },
    {
        "paper_id": 1601.00092,
        "authors": "M.A. Szybisz and L. Szybisz",
        "title": "Hyperinflation in Brazil, Israel, and Nicaragua revisited",
        "comments": "10 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.07.052",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this work is to address the description of hyperinflation regimes\nin economy. The spirals of hyperinflation developed in Brazil, Israel, and\nNicaragua are revisited. This new analysis of data indicates that the episodes\noccurred in Brazil and Nicaragua can be understood within the frame of the\nmodel available in the literature, which is based on a nonlinear feedback (NLF)\ncharacterized by an exponent $\\beta>0$. In the NLF model the accumulated\nconsumer price index carries a finite time singularity of the type\n$1/(t_c-t)^{(1- \\beta)/\\beta}$ determining a critical time $t_c$ at which the\neconomy would crash. It is shown that in the case of Brazil the entire episode\ncannot be described with a unique set of parameters because the time series was\nstrongly affected by a change of policy. This fact gives support to the \"so\ncalled\" Lucas critique, who stated that model's parameters usually change once\npolicy changes. On the other hand, such a model is not able to provide any\n$t_c$ in the case of the weaker hyperinflation occurred in Israel. It is shown\nthat in this case the fit of data yields $\\beta \\to 0$. This limit leads to the\nlinear feedback formulation which does not predict any $t_c$. An extension for\nthe NLF model is suggested.\n"
    },
    {
        "paper_id": 1601.00175,
        "authors": "Dmitry B. Rokhlin",
        "title": "Minimax perfect stopping rules for selling an asset near its ultimate\n  maximum",
        "comments": "13 pages; major revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of selling an asset near its ultimate maximum in the\nminimax setting. The regret-based notion of a perfect stopping time is\nintroduced. A perfect stopping time is uniquely characterized by its optimality\nproperties and has the following form: one should sell the asset if its price\ndeviates from the running maximum by a certain time-dependent quantity. The\nrelated selling rule improves any earlier one and cannot be improved by further\ndelay. The results, which are applicable to a quite general price model, are\nillustrated by several examples.\n"
    },
    {
        "paper_id": 1601.00229,
        "authors": "Roberto Mota Navarro, Hern\\'an Larralde Ridaura",
        "title": "A detailed heterogeneous agent model for a single asset financial market\n  with trading via an order book",
        "comments": "25 pages, 24 figures and 1 table",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0170766",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an agent based model of a single asset financial market that is\ncapable of replicating several non-trivial statistical properties observed in\nreal financial markets, generically referred to as stylized facts. While\nprevious models reported in the literature are also capable of replicating some\nof these statistical properties, in general, they tend to oversimplify either\nthe trading mechanisms or the behavior of the agents. In our model, we strived\nto capture the most important characteristics of both aspects to create agents\nthat employ strategies inspired on those used in real markets, and, at the same\ntime, a more realistic trade mechanism based on a double auction order book. We\nstudy the role of the distinct types of trader on the return statistics:\nspecifically, correlation properties (or lack thereof), volatilty clustering,\nheavy tails, and the degree to which the distribution can be described by a\nlog-normal. Further, by introducing the practice of profit taking, our model is\nalso capable of replicating the stylized fact related to an asymmetry in the\ndistribution of losses and gains.\n"
    },
    {
        "paper_id": 1601.00233,
        "authors": "Timothy J. Garrett",
        "title": "Long-run evolution of the global economy - Part 2: Hindcasts of\n  innovation and growth",
        "comments": null,
        "journal-ref": "Earth Syst. Dynam., 6, 673-688, 2015",
        "doi": "10.5194/esd-6-673-2015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long-range climate forecasts use integrated assessment models to link the\nglobal economy to greenhouse gas emissions. This paper evaluates an alternative\neconomic framework outlined in part 1 of this study (Garrett, 2014) that\napproaches the global economy using purely physical principles rather than\nexplicitly resolved societal dynamics. If this model is initialized with\neconomic data from the 1950s, it yields hindcasts for how fast global economic\nproduction and energy consumption grew between 2000 and 2010 with skill scores\n> 90 % relative to a model of persistence in trends. The model appears to\nattain high skill partly because there was a strong impulse of discovery of\nfossil fuel energy reserves in the mid-twentieth century that helped\ncivilization to grow rapidly as a deterministic physical response. Forecasting\nthe coming century may prove more of a challenge because the effect of the\nenergy impulse appears to have nearly run its course. Nonetheless, an\nunderstanding of the external forces that drive civilization may help\ndevelopment of constrained futures for the coupled evolution of civilization\nand climate during the Anthropocene.\n"
    },
    {
        "paper_id": 1601.00263,
        "authors": "Zhongxing Wang, Yan Yan, Xiaosong Chen",
        "title": "Time and Frequency Structure of Causal Correlation Network in China Bond\n  Market",
        "comments": "9 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2017-70049-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are more than eight hundred interest rates published in China bond\nmarket every day. Which are the benchmark interest rates that have broad\ninfluences on most interest rates is a major concern for economists. In this\npaper, multi-variable Granger causality test is developed and applied to\nconstruct a directed network of interest rates, whose important nodes, regarded\nas key interest rates, are evaluated with inverse Page Rank scores. The results\nindicate that some short-term interest rates have larger influences on the most\nkey interest rates, while repo rates are the benchmark of short-term rates. It\nis also found that central bank bills'rates are in the core position of\nmid-term interest rates'network, and treasury bond rates are leading the\nlong-term bonds rates. The evolution of benchmark interest rates is also\nstudied from 2008 to 2014, and it's found that SHIBOR has generally become the\nbenchmark interest rate in China. In the frequency domain we detect the\nproperties of information flows between interest rates and the result confirms\nthe existence of market segmentation in China bond market.\n"
    },
    {
        "paper_id": 1601.00354,
        "authors": "Krzysztof Echaust, Krzysztof Piasecki",
        "title": "Black-Litterman model with intuitionistic fuzzy posterior return",
        "comments": "SSRN Electronic Journal 2016",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2010280",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main objective is to present a some variant of the Black - Litterman\nmodel. We consider the canonical case when priori return is determined by means\nsuch excess return from the CAPM market portfolio which is derived using\nreverse optimization method. Then the a priori return is at risk quantified\nuncertainty. On the side, intensive discussion shows that the experts' views\nare under knightian uncertainty. For this reason, we propose such variant of\nthe Black - Litterman model in which the experts' views are described as\nintuitionistic fuzzy number. The existence of posterior return is proved for\nthis case.We show that then posterior return is an intuitionistic fuzzy\nprobabilistic set.\n"
    },
    {
        "paper_id": 1601.00566,
        "authors": "Lev B Klebanov",
        "title": "No Stable Distributions in Finance, please!",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Failure of the main argument for the use of heavy tailed distribution in\nFinance is given. More precisely, one cannot observe so many outliers for\nCauchy or for symmetric stable distributions as we have in reality.\nkeywords:outliers; financial indexes; heavy tails; Cauchy distribution; stable\ndistributions\n"
    },
    {
        "paper_id": 1601.00679,
        "authors": "Antoine Kornprobst",
        "title": "Essay on the State of Research and Innovation in France and the European\n  Union",
        "comments": "26 pages. Modifications in the new version : Reference [7] was\n  invalid (it was a report that was never voted and published by the French\n  Senate, contrary to what I assumed) and has been replaced. The corresponding\n  part has been modified in consequence. Also, minor corrections",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Innovation in the economy is an important engine of growth and no economy,\nwhatever its complexity and degree of advancement, whether it is based on\nindustry, agriculture, high tech or the providing of services, can be truly\nhealthy without innovating actors within it. The aim of this work, done by an\napplied mathematician working in finance, not by an economist or a lawyer,\nisn't to provide an exhaustive view of the all the mechanisms in France and in\nEurope that aim at fostering innovation in the economy and to offer solutions\nfor removing all the roadblocks that still hinder innovation; indeed such a\nstudy would go far beyond the scope of this study. What I modestly attempted to\nachieve in this study was firstly to draw a panorama of what is working and\nwhat needs to perfected as far as innovation is concerned in France and Europe,\nthen secondly to offer some solutions and personal thoughts to boost\ninnovation.\n"
    },
    {
        "paper_id": 1601.00712,
        "authors": "Robert Bassett and Khoa Le",
        "title": "Multistage Portfolio Optimization: A Duality Result in Conic Market\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove a general duality result for multi-stage portfolio optimization\nproblems in markets with proportional transaction costs. The financial market\nis described by Kabanov's model of foreign exchange markets over a finite\nprobability space and finite-horizon discrete time steps. This framework allows\nus to compare vector-valued portfolios under a partial ordering, so that our\nmodel does not require liquidation into some numeraire at terminal time.\n  We embed the vector-valued portfolio problem into the set-optimization\nframework, and generate a problem dual to portfolio optimization. Using recent\nresults in the development of set optimization, we then show that a strong\nduality relationship holds between the problems.\n"
    },
    {
        "paper_id": 1601.00822,
        "authors": "Aur\\'elien Hazan",
        "title": "Volume of the steady-state space of financial flows in a monetary\n  stock-flow-consistent model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.01.050",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that a steady-state stock-flow consistent macro-economic model can be\nrepresented as a Constraint Satisfaction Problem (CSP).The set of solutions is\na polytope, which volume depends on the constraintsapplied and reveals the\npotential fragility of the economic circuit,with no need to study the dynamics.\nSeveral methods to compute the volume are compared, inspired by operations\nresearch methods and theanalysis of metabolic networks, both exact and\napproximate.We also introduce a random transaction matrix, and study the\nparticularcase of linear flows with respect to money stocks.\n"
    },
    {
        "paper_id": 1601.00903,
        "authors": "John Goddard, Enrico Onali",
        "title": "Long memory and multifractality: A joint test",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2015.12.166",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The properties of statistical tests for hypotheses concerning the parameters\nof the multifractal model of asset returns (MMAR) are investigated, using Monte\nCarlo techniques. We show that, in the presence of multifractality,\nconventional tests of long memory tend to over-reject the null hypothesis of no\nlong memory. Our test addresses this issue by jointly estimating long memory\nand multifractality. The estimation and test procedures are applied to exchange\nrate data for 12 currencies. In 11 cases, the exchange rate returns are\naccurately described by compounding a NIID series with a multifractal\ntime-deformation process. There is no evidence of long memory.\n"
    },
    {
        "paper_id": 1601.00919,
        "authors": "Andrei Cozma and Christoph Reisinger",
        "title": "Exponential integrability properties of Euler discretization schemes for\n  the Cox-Ingersoll-Ross process",
        "comments": "24 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze exponential integrability properties of the Cox-Ingersoll-Ross\n(CIR) process and its Euler discretizations with various types of truncation\nand reflection at 0. These properties play a key role in establishing the\nfiniteness of moments and the strong convergence of numerical approximations\nfor a class of stochastic differential equations arising in finance. We prove\nthat both implicit and explicit Euler-Maruyama discretizations for the CIR\nprocess preserve the exponential integrability of the exact solution for a wide\nrange of parameters, and find lower bounds on the explosion time.\n"
    },
    {
        "paper_id": 1601.0094,
        "authors": "D. Jason Gibson, Aaron Wingo",
        "title": "Pricing barrier options with discrete dividends",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The presence of discrete dividends complicates the derivation and form of\npricing formulas even for vanilla options. Existing analytic, numerical, and\ntheoretical approximations provide results of varying quality and performance.\nHere, we compare the analytic approach, developed and effective for European\nputs and calls, of Buryak and Guo with the formulas, designed in the context of\nbarrier option pricing, of Dai and Chiu.\n"
    },
    {
        "paper_id": 1601.00991,
        "authors": "Zura Kakushadze",
        "title": "101 Formulaic Alphas",
        "comments": "22 pages; no changes (excepting this line); to appear in Wilmott\n  Magazine",
        "journal-ref": "Wilmott Magazine 2016(84) (2016) 72-80",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present explicit formulas - that are also computer code - for 101\nreal-life quantitative trading alphas. Their average holding period\napproximately ranges 0.6-6.4 days. The average pair-wise correlation of these\nalphas is low, 15.9%. The returns are strongly correlated with volatility, but\nhave no significant dependence on turnover, directly confirming an earlier\nresult based on a more indirect empirical analysis. We further find empirically\nthat turnover has poor explanatory power for alpha correlations.\n"
    },
    {
        "paper_id": 1601.01128,
        "authors": "Sergii Kuchuk-Iatsenko, Yuliya Mishura",
        "title": "Option pricing in the model with stochastic volatility driven by\n  Ornstein--Uhlenbeck process. Simulation",
        "comments": "Published at http://dx.doi.org/10.15559/15-VMSTA43 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2015, Vol. 2, No. 4,\n  355-369",
        "doi": "10.15559/15-VMSTA43",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete-time approximation of paths of an Ornstein--Uhlenbeck\nprocess as a mean for estimation of a price of European call option in the\nmodel of financial market with stochastic volatility. The Euler--Maruyama\napproximation scheme is implemented. We determine the estimates for the option\nprice for predetermined sets of parameters. The rate of convergence of the\nprice and an average volatility when discretization intervals tighten are\ndetermined. Discretization precision is analyzed for the case where the exact\nvalue of the price can be derived.\n"
    },
    {
        "paper_id": 1601.01352,
        "authors": "Kathrin Glau, Zorana Grbac, Antonis Papapantoleon",
        "title": "A unified view of LIBOR models",
        "comments": "26 pages. Revised version, fortcoming in \"Advanced Modelling in\n  Mathematical Finance -- In honour of Ernst Eberlein\", Springer",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a unified framework for modeling LIBOR rates using general\nsemimartingales as driving processes and generic functional forms to describe\nthe evolution of the dynamics. We derive sufficient conditions for the model to\nbe arbitrage-free which are easily verifiable, and for the LIBOR rates to be\ntrue martingales under the respective forward measures. We discuss when the\nconditions are also necessary and comment on further desirable properties such\nas those leading to analytical tractability and positivity of rates. This\nframework allows to consider several popular models in the literature, such as\nLIBOR market models driven by Brownian motion or jump processes, the L\\'evy\nforward price model as well as the affine LIBOR model, under one umbrella.\nMoreover, we derive structural results about LIBOR models and show, in\nparticular, that only models where the forward price is an exponentially affine\nfunction of the driving process preserve their structure under different\nforward measures.\n"
    },
    {
        "paper_id": 1601.01553,
        "authors": "Gurjeet Dhesi and Marcel Ausloos",
        "title": "Modelling and Measuring the Irrational behaviour of Agents in Financial\n  Markets: Discovering the Psychological Soliton",
        "comments": "14 pages, 5 figures, 33 references; prepared for a special issue of\n  Chaos, Solitons & Fractals",
        "journal-ref": "Chaos, Solitons & Fractals 88, 119-125 (2016)",
        "doi": "10.1016/j.chaos.2015.12.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following a Geometrical Brownian Motion extension into an Irrational\nFractional Brownian Motion model, we re-examine agent behaviour reacting to\ntime dependent news on the log-returns thereby modifying a financial market\nevolution. We specifically discuss the role of financial news or economic\ninformation positive or negative feedback of such irrational (or contrarian)\nagents upon the price evolution. We observe a kink-like effect reminiscent of\nsoliton behaviour, suggesting how analysts' forecasts errors induce stock\nprices to adjust accordingly, thereby proposing a measure of the irrational\nforce in a market.\n"
    },
    {
        "paper_id": 1601.0171,
        "authors": "Anatoliy Swishchuk and Nelson Vadori",
        "title": "A Semi-Markovian Modeling of Limit Order Markets",
        "comments": "27 pages, 1 figure, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  R. Cont and A. de Larrard (SIAM J. Finan. Math, 2013) introduced a tractable\nstochastic model for the dynamics of a limit order book, computing various\nquantities of interest such as the probability of a price increase or the\ndiffusion limit of the price process. As suggested by empirical observations,\nwe extend their framework to 1) arbitrary distributions for book events\ninter-arrival times (possibly non-exponential) and 2) both the nature of a new\nbook event and its corresponding inter-arrival time depend on the nature of the\nprevious book event. We do so by resorting to Markov renewal processes to model\nthe dynamics of the bid and ask queues. We keep analytical tractability via\nexplicit expressions for the Laplace transforms of various quantities of\ninterest. We justify and illustrate our approach by calibrating our model to\nthe five stocks Amazon, Apple, Google, Intel and Microsoft on June 21^{st}\n2012. As in R. Cont and A. de Larrard, the bid-ask spread remains constant\nequal to one tick, only the bid and ask queues are modeled (they are\nindependent from each other and get reinitialized after a price change), and\nall orders have the same size.\n"
    },
    {
        "paper_id": 1601.01753,
        "authors": "Xing Li, Tian Qiu, Guang Chen, Li-Xin Zhong, Xiong-Fei Jiang",
        "title": "Geography and distance effect on financial dynamics in the Chinese stock\n  market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.03.058",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geography effect is investigated for the Chinese stock market including the\nShanghai and Shenzhen stock markets, based on the daily data of individual\nstocks. The Shanghai city and the Guangdong province can be identified in the\nstock geographical sector. By investigating a geographical correlation on a\ngeographical parameter, the stock location is found to have an impact on the\nfinancial dynamics, except for the financial crisis time of the Shenzhen\nmarket. Stock distance effect is further studied, with a crossover behavior\nobserved for the stock distance distribution. The probability of the short\ndistance is much greater than that of the long distance. The average stock\ncorrelation is found to weakly decay with the stock distance for the Shanghai\nstock market, but stays nearly stable for different stock distance for the\nShenzhen stock market.\n"
    },
    {
        "paper_id": 1601.01771,
        "authors": "Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Teaching Economics and Providing Visual \"Big Pictures\"",
        "comments": "The 14th Annual Cambridge Business & Economics Conference, University\n  of Cambridge, Cambridge, UK, 1-2 July, 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to investigate the importance of providing visual\n\"big pictures\" in the teaching of economics. The plurality and variety of\nconcepts, variables, diagrams, and models involved in economics can be a source\nof confusion for many economics students. However, reviewing the existing\nliterature on the importance of providing visual \"big pictures\" in the process\nof learning suggests that furnishing students with a visual \"big picture\" that\nillustrates the ways through which those numerous, diverse concepts are\nconnected to each other could be an effective solution to clear up the\nmentioned mental chaos. As a practical example, this paper introduces a \"big\npicture\" that can be used as a good resource in intermediate macroeconomics\nclasses. This figure presents twenty-seven commonly-discussed macroeconomic\ndiagrams in the intermediate macroeconomics course, and gives little detail on\nsome of these diagrams, aiming at helping students to get the whole picture at\nonce on a single piece of paper. This macroeconomics big picture mostly focuses\non the routes through which common diagrams in macroeconomics are connected to\neach other, and finally introduces the general macroeconomic equilibrium that\nis graphically derived through those connections.\n"
    },
    {
        "paper_id": 1601.01804,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the Economic Growth in Latin\n  America",
        "comments": "7 pages, 2 figures, 3185 words.Corrected two parameters. Conclusions\n  remain unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historical economic growth in Latin America is analysed using the data of\nMaddison. Unified Growth Theory is found to be contradicted by these data in\nthe same way as it is contradicted by the economic growth in Africa, Asia,\nformer USSR, Western Europe, Eastern Europe and by the world economic growth.\nParadoxically, Unified Growth Theory is repeatedly and consistently\ncontradicted by the same data, which were used, but never properly analysed,\nduring the formulation of this theory. Unified Growth Theory does not explain\nthe mechanism of the economic growth because it explains features contradicted\nby data. This theory is based fundamentally on the unfortunate lack of\nunderstanding of the properties of hyperbolic distribution and on the\nunscientific analysis of data. There was no transition from stagnation to\ngrowth at the end of the alleged Malthusian regime because the economic growth\nwas hyperbolic. There was no escape from Malthusian trap because there was no\ntrap. There was no takeoff. On the contrary, at the time of the alleged takeoff\neconomic growth started to be diverted to a slower trajectory. Unified Growth\nTheory is dissociated from the reality. This theory needs to be revised or\nreplaced. In its present form, it is a collection of irrelevant stories based\non impressions and on the unscientific use of data.\n"
    },
    {
        "paper_id": 1601.01811,
        "authors": "Matteo Ludovico Bedini, Rainer Buckdahn, Hans-J\\\"urgen Engelbert",
        "title": "Brownian Bridges on Random Intervals",
        "comments": "29 pages, submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The issue of giving an explicit description of the flow of information\nconcerning the time of bankruptcy of a company (or a state) arriving on the\nmarket is tackled by defining a bridge process starting from zero and\nconditioned to be equal to zero when the default occurs. This enables to catch\nsome empirical facts on the behavior of financial markets: when the bridge\nprocess is away from zero, investors can be relatively sure that the default\nwill not happen immediately. However, when the information process is close to\nzero, market agents should be aware of the risk of an imminent default. In this\nsense the bridge process leaks information concerning the default before it\noccurs. The objective of this first paper on Brownian bridges on stochastic\nintervals is to provide the basic properties of these processes.\n"
    },
    {
        "paper_id": 1601.0198,
        "authors": "Lucas Lacasa, Ryan Flanagan",
        "title": "Irreversibility of financial time series: a graph-theoretical approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relation between time series irreversibility and entropy production has\nbeen recently investigated in thermodynamic systems operating away from\nequilibrium. In this work we explore this concept in the context of financial\ntime series. We make use of visibility algorithms to quantify in\ngraph-theoretical terms time irreversibility of 35 financial indices evolving\nover the period 1998-2012. We show that this metric is complementary to\nstandard measures based on volatility and exploit it to both classify periods\nof financial stress and to rank companies accordingly. We then validate this\napproach by finding that a projection in principal components space of\nfinancial years based on time irreversibility features clusters together\nperiods of financial stress from stable periods. Relations between\nirreversibility, efficiency and predictability are briefly discussed.\n"
    },
    {
        "paper_id": 1601.01987,
        "authors": "Justin Sirignano",
        "title": "Deep Learning for Limit Order Books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a new neural network architecture for modeling spatial\ndistributions (i.e., distributions on R^d) which is computationally efficient\nand specifically designed to take advantage of the spatial structure of limit\norder books. The new architecture yields a low-dimensional model of price\nmovements deep into the limit order book, allowing more effective use of\ninformation from deep in the limit order book (i.e., many levels beyond the\nbest bid and best ask). This \"spatial neural network\" models the joint\ndistribution of the state of the limit order book at a future time conditional\non the current state of the limit order book. The spatial neural network\noutperforms other models such as the naive empirical model, logistic regression\n(with nonlinear features), and a standard neural network architecture. Both\nneural networks strongly outperform the logistic regression model. Due to its\nmore effective use of information deep in the limit order book, the spatial\nneural network especially outperforms the standard neural network in the tail\nof the distribution, which is important for risk management applications. The\nmodels are trained and tested on nearly 500 stocks. Techniques from deep\nlearning such as dropout are employed to improve performance. Due to the\nsignificant computational challenges associated with the large amount of data,\nmodels are trained with a cluster of 50 GPUs.\n"
    },
    {
        "paper_id": 1601.02149,
        "authors": "Robert Howley and Robert Storer and Juan Vera and Luis F. Zuluaga",
        "title": "Computing semiparametric bounds on the expected payments of insurance\n  instruments via column generation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been recently shown that numerical semiparametric bounds on the\nexpected payoff of fi- nancial or actuarial instruments can be computed using\nsemidefinite programming. However, this approach has practical limitations.\nHere we use column generation, a classical optimization technique, to address\nthese limitations. From column generation, it follows that practical univari-\nate semiparametric bounds can be found by solving a series of linear programs.\nIn addition to moment information, the column generation approach allows the\ninclusion of extra information about the random variable; for instance,\nunimodality and continuity, as well as the construction of corresponding\nworst/best-case distributions in a simple way.\n"
    },
    {
        "paper_id": 1601.02156,
        "authors": "Matt V. Leduc, Sebastian Poledna and Stefan Thurner",
        "title": "Systemic Risk Management in Financial Networks with Credit Default Swaps",
        "comments": "21 pages, 9 figures",
        "journal-ref": "Journal of Network Theory in Finance (2017)",
        "doi": "10.21314/JNTF.2017.034",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study insolvency cascades in an interbank system when banks are allowed to\ninsure their loans with credit default swaps (CDS) sold by other banks. We show\nthat, by properly shifting financial exposures from one institution to another,\na CDS market can be designed to rewire the network of interbank exposures in a\nway that makes it more resilient to insolvency cascades. A regulator can use\ninformation about the topology of the interbank network to devise a systemic\ninsurance surcharge that is added to the CDS spread. CDS contracts are thus\neffectively penalized according to how much they contribute to increasing\nsystemic risk. CDS contracts that decrease systemic risk remain untaxed. We\nsimulate this regulated CDS market using an agent-based model (CRISIS\nmacro-financial model) and we demonstrate that it leads to an interbank system\nthat is more resilient to insolvency cascades.\n"
    },
    {
        "paper_id": 1601.02246,
        "authors": "Jozef Kiselak, Philipp Hermann and Milan Stehlik",
        "title": "Negative interest rates: why and how?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The interest rates (or nominal yields) can be negative, this is an\nunavoidable fact which has already been visible during the Great Depression\n(1929-39). Nowadays we can find negative rates easily by e.g. auditing. Several\ntheoretical and practical ideas how to model and eventually overcome empirical\nnegative rates can be suggested, however, they are far beyond a simple\npractical realization. In this paper we discuss the dynamical reasons why\nnegative interest rates can happen in the second order differential dynamics\nand how they can influence the variance and expectation of the interest rate\nprocess. Such issues are highly practical, involving e.g. banking sector and\npension securities.\n"
    },
    {
        "paper_id": 1601.02407,
        "authors": "Jaydip Sen and Tamal Datta Chaudhuri",
        "title": "Decomposition of Time Series Data of Stock Markets and its Implications\n  for Prediction: An Application for the Indian Auto Sector",
        "comments": "14 pages, 2 figures, 4 tables. The paper is published in the\n  Proceedings of the 2nd National Conference on Advances in Business Research\n  and Management Practices (ABRMP'2016), January 8-9, 2016, Kolkata, INDIA",
        "journal-ref": null,
        "doi": "10.13140/RG.2.1.3232.0241",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the rapid development and evolution of sophisticated algorithms for\nstatistical analysis of time series data, the research community has started\nspending considerable effort in technical analysis of such data. Forecasting is\nalso an area which has witnessed a paradigm shift in its approach. In this\nwork, we have used the time series of the index values of the Auto sector in\nIndia during January 2010 to December 2015 for a deeper understanding of the\nbehavior of its three constituent components, e.g., the Trend, the Seasonal\ncomponent, and the Random component. Based on this structural analysis, we have\nalso designed three approaches for forecasting and also computed their accuracy\nin prediction using suitably chosen training and test data sets. The results\nclearly demonstrate the accuracy of our decomposition results and efficiency of\nour forecasting techniques, even in presence of a dominant Random component in\nthe time series.\n"
    },
    {
        "paper_id": 1601.02463,
        "authors": "Anindya S. Chakrabarti, Arnab Chatterjee, Tushar K. Nandi, Asim Ghosh,\n  Anirban Chakraborti",
        "title": "Quantifying invariant features of within-group inequality in consumption\n  across groups",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11403-017-0189-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study unit-level expenditure on consumption across multiple countries and\nmultiple years, in order to extract invariant features of consumption\ndistribution. We show that the bulk of it is lognormally distributed, followed\nby a power law tail at the limit. The distributions coincide with each other\nunder normalization by mean expenditure and log scaling even though the data is\nsampled across multiple dimension including, e.g., time, social structure and\nlocations. This phenomenon indicates that the dispersions in consumption\nexpenditure across various social and economic groups are significantly similar\nsubject to suitable scaling and normalization. Further, the results provide a\nmeasurement of the core distributional features. Other descriptive factors\nincluding those of sociological, demographic and political nature, add further\nlayers of variation on the this core distribution. We present a stochastic\nmultiplicative model to quantitatively characterize the invariance and the\ndistributional features.\n"
    },
    {
        "paper_id": 1601.02677,
        "authors": "Subarna Basnet, Christopher L. Magee",
        "title": "Dependence of technological improvement on artifact interactions",
        "comments": "Main manuscript: 18 pages including, 5 figures; supplemental\n  information: 30 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical research has shown performance improvement of many different\ntechnological domains occurs exponentially but with widely varying improvement\nrates. What causes some technologies to improve faster than others do? Previous\nquantitative modeling research has identified artifact interactions, where a\ndesign change in one component influences others, as an important determinant\nof improvement rates. The models predict that improvement rate for a domain is\nproportional to the inverse of the domain interaction parameter. However, no\nempirical research has previously studied and tested the dependence of\nimprovement rates on artifact interactions. A challenge to testing the\ndependence is that any method for measuring interactions has to be applicable\nto a wide variety of technologies. Here we propose a patent-based method that\nis both technology domain-agnostic and less costly than alternative methods. We\nuse textual content from patent sets in 27 domains to find the influence of\ninteractions on improvement rates. Qualitative analysis identified six specific\nkeywords that signal artifact interactions. Patent sets from each domain were\nthen examined to determine the total count of these 6 keywords in each domain,\ngiving an estimate of artifact interactions in each domain. It is found that\nimprovement rates are positively correlated with the inverse of the total count\nof keywords with correlation coefficient of +0.56 with a p-value of 0.002. The\nempirical results agree with model predictions and support the suggestion that\ndomains with higher number of artifacts interactions (higher complexity) will\nimprove at a slower pace.\n"
    },
    {
        "paper_id": 1601.0299,
        "authors": "Serge Galam",
        "title": "The invisible hand and the rational agent are behind bubbles and crashes",
        "comments": "20 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2016.03.011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The substantial turmoil created by both 2000 dot-com crash and 2008 subprime\ncrisis has fueled the belief that the two classical paradigms of economics,\nwhich are the invisible hand and the rational agent, are not appropriate to\ndescribe market dynamics and should be abandoned at the benefit of alternative\nnew theoretical concepts. At odd with such a view, using a simple model of\nchoice dynamics from sociophysics, the invisible hand and the rational agent\nparadigms are given a new legitimacy. Indeed, it is sufficient to introduce the\nholding of a few intermediate mini market aggregations by agents sharing their\nown private information, to recenter the invisible hand and the rational agent\nat the heart of market self regulation including the making of bubbles and\ntheir subsequent crashes. In so doing, an elasticity is discovered in the\nmarket efficiency mechanism due to the existence of agents anticipation. This\nelasticity is found to create spontaneous bubbles, which are rationally\nfounded, and at the same time, it provokes crashes when the limit of elasticity\nis reached. Although the findings disclose a path to put an end to the\nbubble-crash phenomena, it is argued to be rationality not feasible.\n"
    },
    {
        "paper_id": 1601.03015,
        "authors": "Thilo A. Schmitt and Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Credit risk: Taking fluctuating asset correlations into account",
        "comments": "appears in Journal of Credit Risk, Volume 11, Number 3, 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In structural credit risk models, default events and the ensuing losses are\nboth derived from the asset values at maturity. Hence it is of utmost\nimportance to choose a distribution for these asset values which is in\naccordance with empirical data. At the same time, it is desirable to still\npreserve some analytical tractability. We achieve both goals by putting forward\nan ensemble approach for the asset correlations. Consistently with the data, we\nview them as fluctuating quantities, for which we may choose the average\ncorrelation as homogeneous. Thereby we can reduce the number of parameters to\ntwo, the average correlation between assets and the strength of the\nfluctuations around this average value. Yet, the resulting asset value\ndistribution describes the empirical data well. This allows us to derive the\ndistribution of credit portfolio losses. With Monte-Carlo simulations for the\nValue at Risk and Expected Tail Loss we validate the assumptions of our\napproach and demonstrate the necessity of taking fluctuating correlations into\naccount.\n"
    },
    {
        "paper_id": 1601.03067,
        "authors": "Stefano Peluso, Antonietta Mira, Pietro Muliere, Alessandro Lomi",
        "title": "International Trade: a Reinforced Urn Network Model",
        "comments": "18 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a unified modelling framework that theoretically justifies the\nmain empirical regularities characterizing the international trade network.\nEach country is associated to a Polya urn whose composition controls the\npropensity of the country to trade with other countries. The urn composition is\nupdated through the walk of the Reinforced Urn Process of Muliere et al.\n(2000). The model implies a local preferential attachment scheme and a power\nlaw right tail behaviour of bilateral trade flows. Different assumptions on the\nurns' reinforcement parameters account for local clustering, path-shortening\nand sparsity. Likelihood-based estimation approaches are facilitated by\nfeasible likelihood analytical derivation in various network settings. A\nsimulated example and the empirical results on the international trade network\nare discussed.\n"
    },
    {
        "paper_id": 1601.03171,
        "authors": "Yumiharu Nakano",
        "title": "On a law of large numbers for insurance risks",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note presents a kind of the strong law of large numbers for an insurance\nrisk caused by a single catastrophic event rather than by an accumulation of\nindependent and identically distributed risks. We derive this result by a large\ndiversification effect resulting from optimal allocation of the risk to many\nreinsurers or investors.\n"
    },
    {
        "paper_id": 1601.0338,
        "authors": "Micha{\\l} Barski",
        "title": "Quantile hedging on markets with proportional transaction costs",
        "comments": "15 pages",
        "journal-ref": "Applicationes Mathematicae, 2003, 30, 193-208",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper a problem of risk measures on a discrete-time market model with\ntransaction costs is studied. Strategy effectiveness and shortfall risk is\nintroduced. This paper is a generalization of quantile hedging presented in\n[4].\n"
    },
    {
        "paper_id": 1601.03388,
        "authors": "Micha{\\l} Barski",
        "title": "Large losses - probability minimizing approach",
        "comments": "13 pages",
        "journal-ref": "Applicationes Mathematicae, 2004, 31, 243-257",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The probability minimizing problem of large losses of portfolio in discrete\nand continuous time models is studied. This gives a generalization of quantile\nhedging presented in [3].\n"
    },
    {
        "paper_id": 1601.03435,
        "authors": "Arash Fahim, Lingjiong Zhu",
        "title": "Asymptotic Analysis for Optimal Dividends in a Dual Risk Model",
        "comments": "31 pages",
        "journal-ref": "Stochastic Models 2022, Volume 38, Issue 4, 605-637",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dual risk model is a popular model in finance and insurance, which is\noften used to model the wealth process of a venture capital or high tech\ncompany. Optimal dividends have been extensively studied in the literature for\na dual risk model. It is well known that the value function of this optimal\ncontrol problem does not yield closed-form solutions except in some special\ncases. In this paper, we study the asymptotics of the optimal dividend problem\nwhen the parameters of the model go to either zero or infinity. Our results\nprovide insights to the optimal strategies and the optimal values when the\nparameters are extreme.\n"
    },
    {
        "paper_id": 1601.03562,
        "authors": "Anis Matoussi and Hao Xing",
        "title": "Convex duality for stochastic differential utility",
        "comments": "22 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a dual problem to study a continuous-time consumption\nand investment problem with incomplete markets and stochastic differential\nutility. For Epstein-Zin utility, duality between the primal and dual problems\nis established. Consequently the optimal strategy of the consumption and\ninvestment problem is identified without assuming several technical conditions\non market model, utility specification, and agent's admissible strategy.\nMeanwhile the minimizer of the dual problem is identified as the utility\ngradient of the primal value and is economically interpreted as the \"least\nfavorable\" completion of the market.\n"
    },
    {
        "paper_id": 1601.03574,
        "authors": "Nicholas Gonchar",
        "title": "Generalization of Doob decomposition Theorem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, we introduce the notion of a local regular supermartingale\nrelative to a convex set of equivalent measures and prove for it an optional\nDoob decomposition in the discrete case. This Theorem is a generalization of\nthe famous Doob decomposition onto the case of supermartingales relative to a\nconvex set of equivalent measures.\n"
    },
    {
        "paper_id": 1601.03688,
        "authors": "Constantino Tsallis",
        "title": "Inter-occurrence times and universal laws in finance, earthquakes and\n  genomes",
        "comments": "Invited review to appear in Chaos, Solitons and Fractals. It has 37\n  pages, including one Table and 12 figures",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2015.12.025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A plethora of natural, artificial and social systems exist which do not\nbelong to the Boltzmann-Gibbs (BG) statistical-mechanical world, based on the\nstandard additive entropy $S_{BG}$ and its associated exponential BG factor.\nFrequent behaviors in such complex systems have been shown to be closely\nrelated to $q$-statistics instead, based on the nonadditive entropy $S_q$ (with\n$S_1=S_{BG}$), and its associated $q$-exponential factor which generalizes the\nusual BG one. In fact, a wide range of phenomena of quite different nature\nexist which can be described and, in the simplest cases, understood through\nanalytic (and explicit) functions and probability distributions which exhibit\nsome universal features. Universality classes are concomitantly observed which\ncan be characterized through indices such as $q$. We will exhibit here some\nsuch cases, namely concerning the distribution of inter-occurrence (or\ninter-event) times in the areas of finance, earthquakes and genomes.\n"
    },
    {
        "paper_id": 1601.03968,
        "authors": "Marvin S. Mueller",
        "title": "A stochastic Stefan-type problem under first-order boundary conditions",
        "comments": "37 pages, preprint of the version published in the Annals of Applied\n  Probability",
        "journal-ref": "Ann. Appl. Probab., Volume 28, Number 4 (2018), 2335-2369",
        "doi": "10.1214/17-AAP1359",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Moving boundary problems allow to model systems with phase transition at an\ninner boundary. Driven by problems in economics and finance, in particular\nmodeling of limit order books, we consider a stochastic and non-linear\nextension of the classical Stefan-problem in one space dimension, where the\npaths of the moving interface might have unbounded variation. Working on the\ndistribution space, Ito-Wentzell formula for SPDEs allows to transform these\nmoving boundary problems into partial differential equations on fixed domains.\nRewriting the equations into the framework of stochastic evolution equations,\nwe apply results based on stochastic maximal $L^p$-regularity to obtain\nexistence, uniqueness and regularity of local solutions. Moreover, we observe\nthat explosion might take place due to the boundary interaction even when the\ncoefficients of the original problem have linear growths.\n"
    },
    {
        "paper_id": 1601.04028,
        "authors": "Steffen Lange, Peter P\\\"utz and Thomas Kopp",
        "title": "Do Mature Economies Grow Exponentially?",
        "comments": null,
        "journal-ref": "Ecolocigal Economics 147 (2018) 123-133",
        "doi": "10.1016/j.ecolecon.2018.01.011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most models that try to explain economic growth indicate exponential growth\npaths. In recent years, however, a lively discussion has emerged considering\nthe validity of this notion. In the empirical literature dealing with drivers\nof economic growth, the majority of articles is based upon an implicit\nassumption of exponential growth. Few scholarly articles have addressed this\nissue so far. In order to shed light on this issue, we estimate autoregressive\nintegrated moving average time series models based on Gross Domestic Product\nPer Capita data for 18 mature economies from 1960 to 2013. We compare the\nadequacy of linear and exponential growth models and conduct several robustness\nchecks. Our fndings cast doubts on the widespread belief of exponential growth\nand suggest a deeper discussion on alternative economic grow theories.\n"
    },
    {
        "paper_id": 1601.04043,
        "authors": "Ravi Kashyap",
        "title": "Fighting Uncertainty with Uncertainty: A Baby Step",
        "comments": null,
        "journal-ref": null,
        "doi": "10.4236/tel.2017.75097",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We can overcome uncertainty with uncertainty. Using randomness in our choices\nand in what we control, and hence in the decision making process, could\npotentially offset the uncertainty inherent in the environment and yield better\noutcomes. The example we develop in greater detail is the news-vendor inventory\nmanagement problem with demand uncertainty. We briefly discuss areas, where\nsuch an approach might be helpful, with the common prescription, \"Don't Simply\nOptimize, Also Randomize; perhaps best described by the term -\nRandoptimization\".\n  1. News-vendor Inventory Management\n  2. School Admissions\n  3. Journal Submissions\n  4. Job Candidate Selection\n  5. Stock Picking\n  6. Monetary Policy\n  This methodology is suitable for the social sciences since the primary source\nof uncertainty are the members of the system themselves and presently, no\nmethods are known to fully determine the outcomes in such an environment, which\nperhaps would require being able to read the minds of everyone involved and to\nanticipate their actions continuously. Admittedly, we are not qualified to\nrecommend whether such an approach is conducive for the natural sciences,\nunless perhaps, bounds can be established on the levels of uncertainty in a\nsystem and it is shown conclusively that a better understanding of the system\nand hence improved decision making will not alter the outcomes.\n"
    },
    {
        "paper_id": 1601.04093,
        "authors": "Ricardo T. Fernholz",
        "title": "A Statistical Model of Inequality",
        "comments": "46 pages, 9 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a nonparametric statistical model of wealth distribution\nthat imposes little structure on the fluctuations of household wealth. In this\nsetting, we use new techniques to obtain a closed-form household-by-household\ncharacterization of the stable distribution of wealth and show that this\ndistribution is shaped entirely by two factors - the reversion rates (a measure\nof cross-sectional mean reversion) and idiosyncratic volatilities of wealth\nacross different ranked households. By estimating these factors, our model can\nexactly match the U.S. wealth distribution. This provides information about the\ncurrent trajectory of inequality as well as estimates of the distributional\neffects of progressive capital taxes. We find evidence that the U.S. wealth\ndistribution might be on a temporarily unstable trajectory, thus suggesting\nthat further increases in top wealth shares are likely in the near future. For\ncapital taxes, we find that a small tax levied on just 1% of households\nsubstantially reshapes the distribution of wealth and reduces inequality.\n"
    },
    {
        "paper_id": 1601.04188,
        "authors": "M. Fern\\'andez-Mart\\'inez, M.A S\\'anchez-Granero, Mar\\'ia Jos\\'e\n  Mu\\~noz Torrecillas and Bill McKelvey",
        "title": "A comparison among some Hurst exponent approaches to predict nascent\n  bubbles in $500$ company stocks",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": "10.1142/S0218348X17500062",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, three approaches to calculate the self-similarity exponent of\na time series are compared in order to determine which one performs best to\nidentify the transition from random efficient market behavior (EM) to herding\nbehavior (HB) and hence, to find out the beginning of a market bubble. In\nparticular, classical Detrended Fluctuation Analysis (DFA), Generalized Hurst\nExponent (GHE) and GM2 (one of Geometric Method-based algorithms) were applied\nfor self-similarity exponent calculation purposes. Traditionally, researchers\nhave been focused on identifying the beginning of a crash. Instead of this, we\nare pretty interested in identifying the beginning of the transition process\nfrom EM to a market bubble onset, what we consider could be more interesting.\nThe relevance of self-similarity index in such a context lies on the fact that\nit becomes a suitable indicator which allows to identify the raising of HB in\nfinancial markets. Overall, we could state that the greater the self-similarity\nexponent in financial series, the more likely the transition process to HB\ncould start. This fact is illustrated through actual S\\&P500 stocks.\n"
    },
    {
        "paper_id": 1601.0421,
        "authors": "Tim Leung, Jiao Li, Xin Li, Zheng Wang",
        "title": "Speculative Futures Trading under Mean Reversion",
        "comments": "22 pages,13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the problem of trading futures with transaction costs when\nthe underlying spot price is mean-reverting. Specifically, we model the spot\ndynamics by the Ornstein-Uhlenbeck (OU), Cox-Ingersoll-Ross (CIR), or\nexponential Ornstein-Uhlenbeck (XOU) model. The futures term structure is\nderived and its connection to futures price dynamics is examined. For each\nfutures contract, we describe the evolution of the roll yield, and compute\nexplicitly the expected roll yield. For the futures trading problem, we\nincorporate the investor's timing option to enter or exit the market, as well\nas a chooser option to long or short a futures upon entry. This leads us to\nformulate and solve the corresponding optimal double stopping problems to\ndetermine the optimal trading strategies. Numerical results are presented to\nillustrate the optimal entry and exit boundaries under different models. We\nfind that the option to choose between a long or short position induces the\ninvestor to delay market entry, as compared to the case where the investor\npre-commits to go either long or short.\n"
    },
    {
        "paper_id": 1601.04341,
        "authors": "Alexey Fomin, Andrey Korotayev, Julia Zinkina",
        "title": "Negative oil price bubble is likely to burst in March - May 2016. A\n  forecast on the basis of the law of log-periodical dynamics",
        "comments": "14 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data analysis with log-periodical parametrization of the Brent oil price\ndynamics has allowed to estimate (very approximately) the date when the dashing\ncollapse of the Brent oil price will achieve the absolute minimum level\n(corresponding to the so-called singularity point), after which there will\noccur a rather rapid rebound, whereas the accelerating fall of the oil prices\nwhich started in mid-2014 will come to an end. This is likely to happen in the\nperiod between March, 24th and May, 15th, 2016. An analogous estimate (though a\nmore exact one) was made for the date of the burst of the nearest negative\n\"sub-bubble\", which is likely to occur between 19.01 and 02.02.2016\n(importantly, this estimate will allow to verify the robustness of the\ndeveloped forecast in the very nearest days). However, this will not mean a\nstart of a new uninterrupted global growth - the fall will soon continue,\nbreaking new \"anti-records\". The fall will only finally stop after passing the\nabovementioned point of the main negative bubble singularity somewhere between\nMarch 24th and May 15th, 2016 (if, of course, the oil market remains at the\ndisposal of speculators, and no massive interventions of macro actors are\nmade). Importantly, our calculations have also shown that after mid-2014 we are\ndealing not with an antibubble (when price collapse goes on in a damped and\nalmost unstoppable regime) in the world oil market, but with a negative bubble,\nwhen prices collapse in an accelerated mode, and there can be particularly\npowerful collapses with particularly strong destabilizing effect near the\nsingularity point. On the other hand, negative bubbles can be better\nmanipulated by the actions of the macro actors.\n"
    },
    {
        "paper_id": 1601.04351,
        "authors": "Fran\\c{c}ois Dufresne, Enkelejd Hashorva, Gildas Ratovomirija and\n  Youssouf Toukourou",
        "title": "On bivariate lifetime modelling in life insurance applications",
        "comments": "22 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Insurance and annuity products covering several lives require the modelling\nof the joint distribution of future lifetimes. In the interest of simplifying\ncalculations, it is common in practice to assume that the future lifetimes\namong a group of people are independent. However, extensive research over the\npast decades suggests otherwise. In this paper, a copula approach is used to\nmodel the dependence between lifetimes within a married couple \\eH{using data\nfrom a large Canadian insurance company}. As a novelty, the age difference and\nthe \\eH{gender} of the elder partner are introduced as an argument of the\ndependence parameter. \\green{Maximum likelihood techniques are} thus\nimplemented for the parameter estimation. Not only do the results make clear\nthat the correlation decreases with age difference, but also the dependence\nbetween the lifetimes is higher when husband is older than wife. A\ngoodness-of-fit procedure is applied in order to assess the validity of the\nmodel. Finally, considering several products available on the life insurance\nmarket, the paper concludes with practical illustrations.\n"
    },
    {
        "paper_id": 1601.04478,
        "authors": "Jean-Philippe Bouchaud, Stefano Ciliberti, Augustin Landier, Guillaume\n  Simon, David Thesmar",
        "title": "The Excess Returns of \"Quality\" Stocks: A Behavioral Anomaly",
        "comments": "11 pages, 1 figure, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note investigates the causes of the quality anomaly, which is one of the\nstrongest and most scalable anomalies in equity markets. We explore two\npotential explanations. The \"risk view\", whereby investing in high quality\nfirms is somehow riskier, so that the higher returns of a quality portfolio are\na compensation for risk exposure. This view is consistent with the Efficient\nMarket Hypothesis. The other view is the \"behavioral view\", which states that\nsome investors persistently underestimate the true value of high quality firms.\nWe find no evidence in favor of the \"risk view\": The returns from investing in\nquality firms are abnormally high on a risk-adjusted basis, and are not prone\nto crashes. We provide novel evidence in favor of the \"behavioral view\": In\ntheir forecasts of future prices, and while being overall overoptimistic,\nanalysts systematically underestimate the future return of high quality firms,\ncompared to low quality firms.\n"
    },
    {
        "paper_id": 1601.04535,
        "authors": "Th\\'arsis T. P. Souza and Tomaso Aste",
        "title": "A nonlinear impact: evidences of causal effects of social media on\n  market prices",
        "comments": "17 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online social networks offer a new way to investigate financial markets'\ndynamics by enabling the large-scale analysis of investors' collective\nbehavior. We provide empirical evidence that suggests social media and stock\nmarkets have a nonlinear causal relationship. We take advantage of an extensive\ndata set composed of social media messages related to DJIA index components. By\nusing information-theoretic measures to cope for possible nonlinear causal\ncoupling between social media and stock markets systems, we point out stunning\ndifferences in the results with respect to linear coupling. Two main\nconclusions are drawn: First, social media significant causality on stocks'\nreturns are purely nonlinear in most cases; Second, social media dominates the\ndirectional coupling with stock market, an effect not observable within linear\nmodeling. Results also serve as empirical guidance on model adequacy in the\ninvestigation of sociotechnical and financial systems.\n"
    },
    {
        "paper_id": 1601.04557,
        "authors": "Jonas Hirz, Uwe Schmock, Pavel V. Shevchenko",
        "title": "Crunching Mortality and Life Insurance Portfolios with extended\n  CreditRisk+",
        "comments": "18 pages, 7 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1505.04757",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using an extended version of the credit risk model CreditRisk+, we develop a\nflexible framework with numerous applications amongst which we find stochastic\nmortality modelling, forecasting of death causes as well as profit and loss\nmodelling of life insurance and annuity portfolios which can be used in\n(partial) internal models under Solvency II. Yet, there exists a fast and\nnumerically stable algorithm to derive loss distributions exactly, even for\nlarge portfolios. We provide various estimation procedures based on publicly\navailable data. Compared to the Lee-Carter model, we have a more flexible\nframework, get tighter bounds and can directly extract several sources of\nuncertainty. Straight-forward model validation techniques are available.\n"
    },
    {
        "paper_id": 1601.04686,
        "authors": "Ron W Nielsen",
        "title": "Unified Growth Theory Contradicted by the Absence of Takeoffs in the\n  Gross Domestic Product",
        "comments": "15 pages, 7 figures, 4320 words. One figure corrected. Conclusions\n  remain unchanged. arXiv admin note: substantial text overlap with\n  arXiv:1509.06612, arXiv:1601.07291",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data describing historical economic growth are analysed. They demonstrate\nconvincingly that the takeoffs from stagnation to growth, claimed in the\nUnified Growth Theory, never happened. This theory is again contradicted by\ndata, which were used, but never properly analysed, during its formulation. The\nabsence of the claimed takeoffs demonstrates also that the postulate of the\ndifferential takeoffs is contradicted by data.\n"
    },
    {
        "paper_id": 1601.04949,
        "authors": "Nicholas S. Gonchar, Wolodymyr H. Kozyrski, Anatol S. Zhokhin",
        "title": "General Equilibrium and Recession Phenomenon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theorems we proved describe the structure of economic equilibrium in the\nexchange economy model. We have studied the structure of property vectors under\ngiven structure of demand vectors at which given price vector is equilibrium\none. On this ground, we describe the general structure of the equilibrium state\nand give characteristic of equilibrium state describing economic recession. The\ntheory developed is applied to explain the state of the economy in some\nEuropean countries.\n"
    },
    {
        "paper_id": 1601.05012,
        "authors": "Sabiou Inoua",
        "title": "A Simple Measure of Economic Complexity",
        "comments": null,
        "journal-ref": "Volume 52, Issue 7, September 2023, 104793",
        "doi": "10.1016/j.respol.2023.104793",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Contrary to conventional economic growth theory, which reduces a country's\noutput to one aggregate variable (GDP), product diversity is central to\neconomic development, as recent 'economic complexity' research suggests. A\ncountry's product diversity reflects its diversity of knowhow or\n'capabilities'. Researchers proposed the Economic Complexity Index (ECI) and\nthe country Fitness index to estimate a country's number of capabilities from\ninternational export data; these measures predict economic growth better than\nconventional variables such as human capital. This paper offers a simpler\nmeasure of a country's knowhow, Log Product Diversity (or LPD, the logarithm of\na country's number of products), which can be derived from a one-parameter\ncombinatorial model of production in which a set of knowhows combine with some\nprobability to turn raw materials into a product. ECI and log-fitness can be\ninterpreted theoretically (using the combinatorial model) and empirically as\npotentially noisy estimates of LPD; moreover, controlling for natural\nresources, the simple measure better explains the cross-country differences in\nGDP and in GDP per capita.\n"
    },
    {
        "paper_id": 1601.05081,
        "authors": "Rzoska Agata Angelika",
        "title": "Econo- and socio- physics based remarks on the economical growth of the\n  World",
        "comments": "16 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been shown that the long term evolution of the Gross Product of the\nWorld after World War II can be well portrayed by the exponential function with\nthe crossover at the year 1973, cinsiding with the Oil Crisis onset. For the\nthe Standard and Poor 500 index the single exponential behavior extends down to\nat least the mid of the nineteen century. It is notable that the detailed\nshort-term insight focused on the last quarter of century revealed the\nemergence of the power like dependence. However, such dependences can be\nintroduced only when taking into account the behavior at reference-baselines\nyears. The possible relationship to the growth/death evolution of\nmicroorganisms is also discussed. The report proposes the new discussion of the\npast and nowadays time of the global economy. It recalls econonophysics and\nsociophysics as disciplines within which the effective parameterization of\ntrends is possible. Finally, possible future trends are discussed.\n"
    },
    {
        "paper_id": 1601.05199,
        "authors": "Mauro Bernardi and Leopoldo Catania",
        "title": "Portfolio Optimisation Under Flexible Dynamic Dependence Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Signals coming from multivariate higher order conditional moments as well as\nthe information contained in exogenous covariates, can be effectively exploited\nby rational investors to allocate their wealth among different risky investment\nopportunities. This paper proposes a new flexible dynamic copula model being\nable to explain and forecast the time-varying shape of large dimensional asset\nreturns distributions. Moreover, we let the univariate marginal distributions\nto be driven by an updating mechanism based on the scaled score of the\nconditional distribution. This framework allows us to introduce time-variation\nin up to the fourth moment of the conditional distribution. The time-varying\ndependence pattern is subsequently modelled as function of a latent Markov\nSwitching process, allowing also for the inclusion of exogenous covariates in\nthe dynamic updating equation. We empirically assess that the proposed model\nsubstantially improves the optimal portfolio allocation of rational investors\nmaximising their expected utility.\n"
    },
    {
        "paper_id": 1601.05306,
        "authors": "Zhenyu Cui, Chihoon Lee, Yanchu Liu",
        "title": "On \"A General Framework for Pricing Asian Options Under Markov\n  Processes\"",
        "comments": "working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cai, Song and Kou (2015) [Cai, N., Y. Song, S. Kou (2015) A general framework\nfor pricing Asian options under Markov processes. Oper. Res. 63(3): 540-554]\nmade a breakthrough by proposing a general framework for pricing both\ndiscretely and continuously monitored Asian options under one-dimensional\nMarkov processes. In this note, under the setting of continuous-time Markov\nchain (CTMC), we explicitly carry out the inverse Z-transform and the inverse\nLaplace transform respectively for the discretely and the continuously\nmonitored cases. The resulting explicit single Laplace transforms improve their\nTheorem 2, p.543, and numerical studies demonstrate the gain in efficiency.\n"
    },
    {
        "paper_id": 1601.0566,
        "authors": "Antonios Garas and Athanasios Lapatinas",
        "title": "The role of consumer networks in firms' multi-characteristics\n  competition and market-share inequality",
        "comments": "33 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a location analysis spatial model of firms' competition in\nmulti-characteristics space, where consumers' opinions about the firms'\nproducts are distributed on multilayered networks. Firms do not compete on\nprice but only on location upon the products' multi-characteristics space, and\nthey aim to attract the maximum number of consumers. Boundedly rational\nconsumers have distinct ideal points/tastes over the possible available firm\nlocations but, crucially, they are affected by the opinions of their neighbors.\nProposing a dynamic agent-based analysis on firms' location choice we\ncharacterize multi-dimensional product differentiation competition as adaptive\nlearning by firms' managers and we argue that such a complex systems approach\nadvances the analysis in alternative ways, beyond game-theoretic calculations.\n"
    },
    {
        "paper_id": 1601.05872,
        "authors": "Philip Ernst, L.C.G. Rogers, and Quan Zhou",
        "title": "The value of foresight",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Suppose you have one unit of stock, currently worth 1, which you must sell\nbefore time $T$. The Optional Sampling Theorem tells us that whatever stopping\ntime we choose to sell, the expected discounted value we get when we sell will\nbe 1. Suppose however that we are able to see $a$ units of time into the\nfuture, and base our stopping rule on that; we should be able to do better than\nexpected value 1. But how much better can we do? And how would we exploit the\nadditional information? The optimal solution to this problem will never be\nfound, but in this paper we establish remarkably close bounds on the value of\nthe problem, and we derive a fairly simple exercise rule that manages to\nextract most of the value of foresight.\n"
    },
    {
        "paper_id": 1601.06204,
        "authors": "J\\'ozsef Mezei and Peter Sarlin",
        "title": "RiskRank: Measuring interconnected risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes RiskRank as a joint measure of cyclical and\ncross-sectional systemic risk. RiskRank is a general-purpose aggregation\noperator that concurrently accounts for risk levels for individual entities and\ntheir interconnectedness. The measure relies on the decomposition of systemic\nrisk into sub-components that are in turn assessed using a set of risk measures\nand their relationships. For this purpose, motivated by the development of the\nChoquet integral, we employ the RiskRank function to aggregate risk measures,\nallowing for the integration of the interrelation of different factors in the\naggregation process. The use of RiskRank is illustrated through a real-world\ncase in a European setting, in which we show that it performs well in\nout-of-sample analysis. In the example, we provide an estimation of systemic\nrisk from country-level risk and cross-border linkages.\n"
    },
    {
        "paper_id": 1601.0642,
        "authors": "Vaibhav Srivastava and Philip Holmes and Patrick Simen",
        "title": "Explicit moments of decision times for single- and double-threshold\n  drift-diffusion processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive expressions for the first three moments of the decision time (DT)\ndistribution produced via first threshold crossings by sample paths of a\ndrift-diffusion equation. The \"pure\" and \"extended\" diffusion processes are\nwidely used to model two-alternative forced choice decisions, and, while simple\nformulae for accuracy, mean DT and coefficient of variation are readily\navailable, third and higher moments and conditioned moments are not generally\navailable. We provide explicit formulae for these, describe their behaviors as\ndrift rates and starting points approach interesting limits, and, with the\nsupport of numerical simulations, discuss how trial-to-trial variability of\ndrift rates, starting points, and non-decision times affect these behaviors in\nthe extended diffusion model. Both unconditioned moments and those conditioned\non correct and erroneous responses are treated. We argue that the results will\nassist in exploring mechanisms of evidence accumulation and in fitting\nparameters to experimental data.\n"
    },
    {
        "paper_id": 1601.06477,
        "authors": "Likuan Qin and Vadim Linetsky and Yutian Nie",
        "title": "Long Forward Probabilities, Recovery and the Term Structure of Bond Risk\n  Premiums",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the martingale component in the long-term factorization of the\nstochastic discount factor due to Alvarez and Jermann (2005) and Hansen and\nScheinkman (2009) is highly volatile, produces a downward-sloping term\nstructure of bond Sharpe ratios, and implies that the long bond is far from\ngrowth optimality. In contrast, the long forward probabilities forecast an\nupward sloping term structure of bond Sharpe ratios that starts from zero for\nshort-term bonds and implies that the long bond is growth optimal. Thus,\ntransition independence and degeneracy of the martingale component are\nimplausible assumptions in the bond market.\n"
    },
    {
        "paper_id": 1601.06651,
        "authors": "Jonas Hallgren, Timo Koski",
        "title": "Testing for Causality in Continuous Time Bayesian Network Models of\n  High-Frequency Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Continuous time Bayesian networks are investigated with a special focus on\ntheir ability to express causality. A framework is presented for doing\ninference in these networks. The central contributions are a representation of\nthe intensity matrices for the networks and the introduction of a causality\nmeasure. A new model for high-frequency financial data is presented. It is\ncalibrated to market data and by the new causality measure it performs better\nthan older models.\n"
    },
    {
        "paper_id": 1601.06979,
        "authors": "Thomas Knispel and Roger J. A. Laeven and Gregor Svindland",
        "title": "Robust Optimal Risk Sharing and Risk Premia in Expanding Pools",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2016.05.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal risk sharing in a pool of cooperative\nagents. We analyze the asymptotic behavior of the certainty equivalents and\nrisk premia associated with the Pareto optimal risk sharing contract as the\npool expands. We first study this problem under expected utility preferences\nwith an objectively or subjectively given probabilistic model. Next, we develop\na robust approach by explicitly taking uncertainty about the probabilistic\nmodel (ambiguity) into account. The resulting robust certainty equivalents and\nrisk premia compound risk and ambiguity aversion. We provide explicit results\non their limits and rates of convergence, induced by Pareto optimal risk\nsharing in expanding pools.\n"
    },
    {
        "paper_id": 1601.06995,
        "authors": "Sidi Mohamed Aly",
        "title": "Moment explosions, implied volatility and local volatility at extreme\n  strikes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic volatility model where the moment generating\nfunction of the logarithmic price is finite only on part of the real line.\nUsing a new Tauberian result obtained in [1] and [2], we show that the\nknowledge of the moment generating function near its critical moment gives a\nsharp asymptotic expansion (with an error of order o(1)) of the local\nvolatility and implied volatility for small and large strikes. We apply our\ntheoretical estimates to Gatheral's SVI parametrization of the implied\nvolatility and Heston's model.\n"
    },
    {
        "paper_id": 1601.07593,
        "authors": "Peter Harremo\\\"es",
        "title": "Sufficiency on the Stock Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that there are a number of relations between theoretical\nfinance theory and information theory. Some of these relations are exact and\nsome are approximate. In this paper we will explore some of these relations and\ndetermine under which conditions the relations are exact. It turns out that\nportfolio theory always leads to Bregman divergences. The Bregman divergence is\nonly proportional to information divergence in situations that are essentially\nequal to the type of gambling studied by Kelly. This can be related an abstract\nsufficiency condition.\n"
    },
    {
        "paper_id": 1601.07626,
        "authors": "Vassilios Papathanakos",
        "title": "Trading-profit attribution for the size factor",
        "comments": "15 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An algorithm was recently introduced by INTECH for the purposes of estimating\nthe trading-profit contribution of systematic rebalancing to the relative\nreturn of rules-based investment strategies. We apply this methodology to\nanalyze the size factor through the use of equal-weighted portfolios. These\nstrategies combine a natural exposure to the size factor with a simple\nunderstanding within the framework of Stochastic Portfolio Theory, furnishing a\nnatural test subject for the attribution algorithm.\n"
    },
    {
        "paper_id": 1601.07628,
        "authors": "Vassilios Papathanakos",
        "title": "Portfolio Optimization in the Stochastic Portfolio Theory Framework",
        "comments": "15 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I discuss some theoretical results with a view to motivate some practical\nchoices in portfolio optimization. Even though the setting is not completely\ngeneral (for example, the covariance matrix is assumed to be non-singular), I\nattempt to highlight the features that have practical relevance. The\nmathematical setting is Stochastic Portfolio Theory, which is flexible enough\nto describe most realistic assets, and it has been successfully employed for\nmanaging equity portfolios since 1987.\n"
    },
    {
        "paper_id": 1601.07707,
        "authors": "Maximilian Seyrich and Didier Sornette",
        "title": "Micro-foundation using percolation theory of the finite-time singular\n  behavior of the crash hazard rate in a class of rational expectation bubbles",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1142/S0129183116501138",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a plausible micro-founded model for the previously postulated\npower law finite time singular form of the crash hazard rate in the\nJohansen-Ledoit-Sornette model of rational expectation bubbles. The model is\nbased on a percolation picture of the network of traders and the concept that\nclusters of connected traders share the same opinion. The key ingredient is the\nnotion that a shift of position from buyer to seller of a sufficiently large\ngroup of traders can trigger a crash. This provides a formula to estimate the\ncrash hazard rate by summation over percolation clusters above a minimum size\nof a power sa (with a > 1) of the cluster sizes s, similarly to a generalized\npercolation susceptibility. The power sa of cluster sizes emerges from the\nsuper-linear dependence of group activity as a function of group size,\npreviously documented in the literature. The crash hazard rate exhibits\nexplosive finite-time singular behaviors when the control parameter (fraction\nof occupied sites, or density of traders in the network) approaches the\npercolation threshold pc. Realistic dynamics are generated by modelling the\ndensity of traders on the percolation network by an Ornstein-Uhlenbeck process,\nwhose memory controls the spontaneous excursion of the control parameter close\nto the critical region of bubble formation. Our numerical simulations recover\nthe main stylized properties of the JLS model with intermittent explosive\nsuper-exponential bubbles interrupted by crashes.\n"
    },
    {
        "paper_id": 1601.07716,
        "authors": "Michael Dittmar (ETH Zurich, Institute of Particle Physics)",
        "title": "Regional Oil Extraction and Consumption: A simple production model for\n  the next 35 years Part I",
        "comments": "25 pages, 3 Figures and 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The growing conflicts in and about oil exporting regions and speculations\nabout volatile oil prices during the last decade have renewed the public\ninterest in predictions for the near future oil production and consumption.\nUnfortunately, studies from only 10 years ago, which tried to forecast the oil\nproduction during the next 20-30 years, failed to make accurate predictions for\ntoday's global oil production and consumption. Forecasts using economic growth\nscenarios, overestimated the actual oil production, while models which tried to\nestimate the maximum future oil production/year, using the official country oil\nreserve data, predicted a too low production.\n  In this paper, a new approach to model the maximal future regional and thus\nglobal oil production (part I) and consumption (part II) during the next\ndecades is proposed.\n  Our analysis of the regional oil production data during past decades shows\nthat, in contrast to periods when production was growing and growth rates\nvaried greatly from one country to another, remarkable similarities are found\nduring the plateau and decline periods of different countries. Following this\nmodel, the particular production phase of each major oil producing country and\nregion is determined essentially only from the recent past oil production data.\nUsing these data, the model is then used to predict the production from all\nmajor oil producing countries, regions and continents up to the year 2050. The\nlimited regional and global potential to compensate this decline with\nunconventional oil and oil-equivalents is also presented.\n"
    },
    {
        "paper_id": 1601.07776,
        "authors": "Angelo Antoci, Alexia Delfino, Fabio Paglieri, Fabio Sabatini",
        "title": "The ecology of social interactions in online and offline environments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rise in online social networking has brought about a revolution in social\nrelations. However, its effects on offline interactions and its implications\nfor collective well-being are still not clear and are under-investigated. We\nstudy the ecology of online and offline interaction in an evolutionary game\nframework where individuals can adopt different strategies of socialization.\nOur main result is that the spreading of self-protective behaviors to cope with\nhostile social environments can lead the economy to non-socially optimal\nstationary states.\n"
    },
    {
        "paper_id": 1601.07792,
        "authors": "John J. Nay, Yevgeniy Vorobeychik",
        "title": "Predicting Human Cooperation",
        "comments": "Added references. New inline citation style. Added small portions of\n  text. Re-compiled Rmarkdown file with updated ggplot2 so small aesthetic\n  changes to plots",
        "journal-ref": "PLoS ONE 11(5): e0155656 (2016)",
        "doi": "10.1371/journal.pone.0155656",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Prisoner's Dilemma has been a subject of extensive research due to its\nimportance in understanding the ever-present tension between individual\nself-interest and social benefit. A strictly dominant strategy in a Prisoner's\nDilemma (defection), when played by both players, is mutually harmful.\nRepetition of the Prisoner's Dilemma can give rise to cooperation as an\nequilibrium, but defection is as well, and this ambiguity is difficult to\nresolve. The numerous behavioral experiments investigating the Prisoner's\nDilemma highlight that players often cooperate, but the level of cooperation\nvaries significantly with the specifics of the experimental predicament. We\npresent the first computational model of human behavior in repeated Prisoner's\nDilemma games that unifies the diversity of experimental observations in a\nsystematic and quantitatively reliable manner. Our model relies on data we\nintegrated from many experiments, comprising 168,386 individual decisions. The\ncomputational model is composed of two pieces: the first predicts the\nfirst-period action using solely the structural game parameters, while the\nsecond predicts dynamic actions using both game parameters and history of play.\nOur model is extremely successful not merely at fitting the data, but in\npredicting behavior at multiple scales in experimental designs not used for\ncalibration, using only information about the game structure. We demonstrate\nthe power of our approach through a simulation analysis revealing how to best\npromote human cooperation.\n"
    },
    {
        "paper_id": 1601.07864,
        "authors": "Nikolaos Halidias",
        "title": "On construction of boundary preserving numerical schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our aim in this note is to extend the semi discrete technique by combine it\nwith the split step method. We apply our new method to the Ait-Sahalia model\nand propose an explicit and positivity preserving numerical scheme.\n"
    },
    {
        "paper_id": 1601.079,
        "authors": "I.A. Molotkov and N.A. Ryabova",
        "title": "Critical value of the total debt in view of the debts durations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Parastatistic distribution of a total debt owed to a large number of\ncreditors considered in relation to the duration of these debts. The process of\ndebt calculation depends on the fractal dimension of economic system in which\nthis process takes place. Two actual variants of these dimensions are\ninvestigated. Critical values for these variants are determined. These critical\nvalues represent the levels after that borrower bankruptcy occurs. The\ncalculation of the critical value is performed by two independent methods: as\nthe point where the entropy of the system reaches its maximum value, and as the\npoint where the chemical potential is zero, which corresponds to the\ntermination of payments on the debt. Both methods lead to the same critical\nvalue. When the velocity of money circulation decrease, it is found for what\ndimensions critical debt value is increased and for what it is decreased in the\ncase when the velocity of money circulation is increased.\n"
    },
    {
        "paper_id": 1601.07961,
        "authors": "Juan M. Romero and Jorge Bautista",
        "title": "Exact solutions for optimal execution of portfolios transactions and the\n  Riccati equation",
        "comments": "15 pages, non figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose two methods to obtain exact solutions for the Almgren-Chriss model\nabout optimal execution of portfolio transactions. In the first method we\nrewrite the Almgren-Chriss equation and find two exact solutions. In the second\nmethod, employing a general reparametrized time, we show that the\nAlmgren-Chriss equation can be reduced to some known equations which can be\nexactly solved in different cases.For this last case we obtain a quantity\nconserved. In addition, we show that in both methods the Almgren-Chriss\nequation is equivalent to a Riccati equation.\n"
    },
    {
        "paper_id": 1601.08099,
        "authors": "Adil Yilmaz, Gazanfer Unal",
        "title": "Chaos in Fractionally Integrated Generalized Autoregressive Conditional\n  Heteroskedastic Processes",
        "comments": "20 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fractionally integrated generalized autoregressive conditional\nheteroskedasticity (FIGARCH) arises in modeling of financial time series.\nFIGARCH is essentially governed by a system of nonlinear stochastic difference\nequations ${u_t}$ = ${z_t}$ $(1-\\sum\\limits_{j=1}^q \\beta_j L^j)\\sigma_{t}^2 =\n\\omega+(1-\\sum\\limits_{j=1}^q \\beta_j L^j - (\\sum\\limits_{k=1}^p \\varphi_k L^k)\n(1-L)^d) u_t^2$, where $\\omega\\in$ R, and $\\beta_j\\in$ R are constant\nparameters, $\\{u_t\\}_{{t\\in}^+}$ and $\\{\\sigma_t\\}_{{t\\in}^+}$ are the discrete\ntime real valued stochastic processes which represent FIGARCH (p,d,q) and\nstochastic volatility, respectively. Moreover, L is the backward shift\noperator, i.e. $L^d u_t \\equiv u_{t-d}$ (d is the fractional differencing\nparameter 0$<$d$<$1).\n  In this work, we have studied the chaoticity properties of FIGARCH (p,d,q)\nprocesses by computing mutual information, correlation dimensions, FNNs (False\nNearest Neighbour), the Lyapunov exponents, and for both the stochastic\ndifference equation given above and for the financial time series. We have\nobserved that maximal Lyapunov exponents are negative, therefore, it can be\nsuggested that FIGARCH (p,d,q) is not deterministic chaotic process.\n"
    },
    {
        "paper_id": 1601.08155,
        "authors": "J\\\"orn Sass, Dorothee Westphal, Ralf Wunderlich",
        "title": "Expert Opinions and Logarithmic Utility Maximization for Multivariate\n  Stock Returns with Gaussian Drift",
        "comments": "Minor changes to earlier version, still 30 pages",
        "journal-ref": "International Journal of Theoretical and Applied Finance 20 (4),\n  2017",
        "doi": "10.1142/S0219024917500224",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates optimal trading strategies in a financial market with\nmultidimensional stock returns where the drift is an unobservable multivariate\nOrnstein-Uhlenbeck process. Information about the drift is obtained by\nobserving stock returns and expert opinions. The latter provide unbiased\nestimates on the current state of the drift at discrete points in time.\n  The optimal trading strategy of investors maximizing expected logarithmic\nutility of terminal wealth depends on the filter which is the conditional\nexpectation of the drift given the available information. We state filtering\nequations to describe its dynamics for different information settings. Between\nexpert opinions this is the Kalman filter. The conditional covariance matrices\nof the filter follow ordinary differential equations of Riccati type. We rely\non basic theory about matrix Riccati equations to investigate their properties.\nFirstly, we consider the asymptotic behaviour of the covariance matrices for an\nincreasing number of expert opinions on a finite time horizon. Secondly, we\nstate conditions for the convergence of the covariance matrices on an infinite\ntime horizon with regularly arriving expert opinions.\n  Finally, we derive the optimal trading strategy of an investor. The optimal\nexpected logarithmic utility of terminal wealth, the value function, is a\nfunctional of the conditional covariance matrices. Hence, our analysis of the\ncovariance matrices allows us to deduce properties of the value function.\n"
    },
    {
        "paper_id": 1602.0009,
        "authors": "Christopher L. Magee and Tessaleno C. Devezas",
        "title": "A Simple extension of Dematerialization Theory: Incorporation of\n  Technical Progress and the Rebound Effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Dematerialization is the reduction in the quantity of materials needed to\nproduce something useful over time. Dematerialization fundamentally derives\nfrom ongoing increases in technical performance but it can be counteracted by\ndemand rebound - increases in usage because of increased value (or decreased\ncost) that also results from increasing technical performance. A major question\nthen is to what extent technological performance improvement can offset and is\noffsetting continuously increasing economic consumption. This paper contributes\nto answering this question by offering some simple quantitative extensions to\nthe theory of dematerialization. The paper then empirically examines the\nmaterials consumption trends as well as cost trends for a large set of\nmaterials and a few modern artifacts over the past decades. In each of 57 cases\nexamined, the particular combinations of demand elasticity and technical\nperformance rate improvement are not consistent with dematerialization.\nOverall, the theory extension and empirical examination indicate that there is\nno dematerialization occurring even for cases of information technology with\nrapid technical progress. Thus, a fully passive policy stance that relies on\nunfettered technological change is not supported by our results.\n"
    },
    {
        "paper_id": 1602.00094,
        "authors": "Jos\\'e Manuel Corcuera, Arturo Valdivia",
        "title": "CoCos under short-term uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/17442508.2016.1149590",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyze an extension of the Jeanblanc and Valchev (2005)\nmodel by considering a short-term uncertainty model with two noises. It is a\ncombination of the ideas of Duffie and Lando (2001) and Jeanblanc and Valchev\n(2005): share quotations of the firm are available at the financial market, and\nthese can be seen as noisy information about the fundamental value, or the\nfirm's asset, from which a low level produces the credit event. We assume there\nare also reports of the firm, release times, where this short-term uncertainty\ndisappears. This credit event model is used to describe conversion and default\nin a CoCo bond.\n"
    },
    {
        "paper_id": 1602.00125,
        "authors": "Rui-Qi Han (ECUST), Wen-Jie Xie (ECUST), Xiong Xiong (TJU), Wei Zhang\n  (TJU), Wei-Xing Zhou (ECUST)",
        "title": "Market correlation structure changes around the Great Crash",
        "comments": "6 pages including 5 figures",
        "journal-ref": "Fluctuation and Noise Letters 16 (2), 1750018 (2017)",
        "doi": "10.1142/S0219477517500183",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a comparative analysis of the Chinese stock market around the\noccurrence of the 2008 crisis based on the random matrix analysis of\nhigh-frequency stock returns of 1228 stocks listed on the Shanghai and Shenzhen\nstock exchanges. Both raw correlation matrix and partial correlation matrix\nwith respect to the market index in two time periods of one year are\ninvestigated. We find that the Chinese stocks have stronger average correlation\nand partial correlation in 2008 than in 2007 and the average partial\ncorrelation is significantly weaker than the average correlation in each\nperiod. Accordingly, the largest eigenvalue of the correlation matrix is\nremarkably greater than that of the partial correlation matrix in each period.\nMoreover, each largest eigenvalue and its eigenvector reflect an evident market\neffect, while other deviating eigenvalues do not. We find no evidence that\ndeviating eigenvalues contain industrial sectorial information. Surprisingly,\nthe eigenvectors of the second largest eigenvalues in 2007 and of the third\nlargest eigenvalues in 2008 are able to distinguish the stocks from the two\nexchanges. We also find that the component magnitudes of the some largest\neigenvectors are proportional to the stocks' capitalizations.\n"
    },
    {
        "paper_id": 1602.00159,
        "authors": "Ricardo T. Fernholz",
        "title": "Empirical Methods for Dynamic Power Law Distributions in the Social\n  Sciences",
        "comments": "33 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1601.04093",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces nonparametric econometric methods that characterize\ngeneral power law distributions under basic stability conditions. These methods\nextend the literature on power laws in the social sciences in several\ndirections. First, we show that any stationary distribution in a random growth\nsetting is shaped entirely by two factors - the idiosyncratic volatilities and\nreversion rates (a measure of cross-sectional mean reversion) for different\nranks in the distribution. This result is valid regardless of how growth rates\nand volatilities vary across different economic agents, and hence applies to\nGibrat's law and its extensions. Second, we present techniques to estimate\nthese two factors using panel data. Third, we show how our results offer a\nstructural explanation for a generalized size effect in which higher-ranked\nprocesses grow more slowly than lower-ranked processes on average. Finally, we\nemploy our empirical methods using panel data on commodity prices and show that\nour techniques accurately describe the empirical distribution of relative\ncommodity prices. We also show the existence of a generalized \"size\" effect for\ncommodities, as predicted by our econometric theory.\n"
    },
    {
        "paper_id": 1602.00235,
        "authors": "Carol Alexander and Johannes Rauch",
        "title": "Model-Free Discretisation-Invariant Swap Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Realised pay-offs for discretisation-invariant swaps are those which satisfy\na restricted `aggregation property' of Neuberger [2012] for twice continuously\ndifferentiable deterministic functions of a multivariate martingale. They are\ninitially characterised as solutions to a second-order system of PDEs, then\nthose pay-offs based on martingale and log-martingale processes alone form a\nvector space. Hence there exist an infinite variety of other variance and\nhigher-moment risk premia that are less prone to bias than standard variance\nswaps because their option replication portfolios have no discrete-monitoring\nor jump errors. Their fair values are also independent of the monitoring\npartition. A sub-class consists of pay-offs with fair values that are further\nfree from numerical integration errors over option strikes. Here exact pricing\nand hedging is possible via dynamic trading strategies on a few vanilla puts\nand calls. An S&P 500 empirical study on higher-moment and other DI swaps\nconcludes.\n"
    },
    {
        "paper_id": 1602.00256,
        "authors": "Lev B. Klebanov, Greg Temnov, Ashot V. Kakosyan",
        "title": "Some Contra-Arguments for the Use of Stable Distributions in Financial\n  Modeling",
        "comments": "keywords:outliers, financial indexes, heavy tails, stable\n  distributions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, we discuss contra-arguments concerning the use of\nPareto-Lev\\'y distributions for modeling in Finance. It appears that such\nprobability laws do not provide sufficient number of outliers observed in real\ndata. Connection with the classical limit theorem for heavy-tailed\ndistributions with such type of models is also questionable. The idea of\nalternative modeling is given.\n"
    },
    {
        "paper_id": 1602.00358,
        "authors": "Wai-Ki Ching, Jia-Wen Gu, Tak-Kuen Siu and Qing-Qing Yang",
        "title": "Trading Strategy with Stochastic Volatility in a Limit Order Book Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we employ the Heston stochastic volatility model to describe\nthe stock's volatility and apply the model to derive and analyze the optimal\ntrading strategies for dealers in a security market. We also extend our study\nto option market making for options written on stocks in the presence of\nstochastic volatility. Mathematically, the problem is formulated as a\nstochastic optimal control problem and the controlled state process is the\ndealer's mark-to-market wealth. Dealers in the security market can optimally\ndetermine their ask and bid quotes on the underlying stocks or options\ncontinuously over time. Their objective is to maximize an expected profit from\ntransactions with a penalty proportional to the variance of cumulative\ninventory cost.\n"
    },
    {
        "paper_id": 1602.0057,
        "authors": "Imke Redeker and Ralf Wunderlich",
        "title": "Portfolio optimization under dynamic risk constraints: continuous vs.\n  discrete time trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor facing a classical portfolio problem of optimal\ninvestment in a log-Brownian stock and a fixed-interest bond, but constrained\nto choose portfolio and consumption strategies that reduce a dynamic shortfall\nrisk measure. For continuous- and discrete-time financial markets we\ninvestigate the loss in expected utility of intermediate consumption and\nterminal wealth caused by imposing a dynamic risk constraint. We derive the\ndynamic programming equations for the resulting stochastic optimal control\nproblems and solve them numerically. Our numerical results indicate that the\nloss of portfolio performance is not too large while the risk is notably\nreduced. We then investigate time discretization effects and find that the loss\nof portfolio performance resulting from imposing a risk constraint is typically\nbigger than the loss resulting from infrequent trading.\n"
    },
    {
        "paper_id": 1602.00619,
        "authors": "Parsiad Azimzadeh",
        "title": "Stock loans with liquidation",
        "comments": "10 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a \"semi-analytic\" solution for a stock loan in which the lender\nforces liquidation when the loan-to-collateral ratio drops beneath a certain\nthreshold. We use this to study the sensitivity of the contract to model\nparameters.\n"
    },
    {
        "paper_id": 1602.00629,
        "authors": "Alessandro Stringhi, Silvia Figini",
        "title": "How to improve accuracy for DFA technique",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the existing literature on empirical estimation of the\nconfidence intervals associated to the Detrended Fluctuation Analysis (DFA). We\nused Montecarlo simulation to evaluate the confidence intervals. Varying the\nparameters in DFA technique, we point out the relationship between those and\nthe standard deviation of H. The parameters considered are the finite time\nlength L, the number of divisors d used and the values of those. We found that\nall these parameters play a crucial role, determining the accuracy of the\nestimation of H.\n"
    },
    {
        "paper_id": 1602.00731,
        "authors": "Hai-Chuan Xu (ECUST), Wei Chen (SZSE), Xiong Xiong (TJU), Wei Zhang\n  (TJU), Wei-Xing Zhou (ECUST) and H Eugene Stanley (BU)",
        "title": "Limit-order book resiliency after effective market orders: Spread, depth\n  and intensity",
        "comments": "18 pages inclusing 8 figures",
        "journal-ref": "Journal of Statistical Mechanics 2017 (7), 073404 (2017)",
        "doi": "10.1088/1742-5468/aa7a3e",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order-driven markets, limit-order book (LOB) resiliency is an important\nmicroscopic indicator of market quality when the order book is hit by a\nliquidity shock and plays an essential role in the design of optimal submission\nstrategies of large orders. However, the evolutionary behavior of LOB\nresilience around liquidity shocks is not well understood empirically. Using\norder flow data sets of Chinese stocks, we quantify and compare the LOB\ndynamics characterized by the bid-ask spread, the LOB depth and the order\nintensity surrounding effective market orders with different aggressiveness. We\nfind that traders are more likely to submit effective market orders when the\nspreads are relatively low, the same-side depth is high, and the opposite-side\ndepth is low. Such phenomenon is especially significant when the initial spread\nis 1 tick. Although the resiliency patterns show obvious diversity after\ndifferent types of market orders, the spread and depth can return to the sample\naverage within 20 best limit updates. The price resiliency behavior is dominant\nafter aggressive market orders, while the price continuation behavior is\ndominant after less-aggressive market orders. Moreover, the effective market\norders produce asymmetrical stimulus to limit orders when the initial spreads\nequal to 1 tick. Under this case, effective buy market orders attract more buy\nlimit orders and effective sell market orders attract more sell limit orders.\nThe resiliency behavior of spread and depth is linked to limit order intensity.\n"
    },
    {
        "paper_id": 1602.00782,
        "authors": "Philip Ernst, James Thompson, Yinsen Miao",
        "title": "Portfolio Selection: The Power of Equal Weight",
        "comments": "11 pages, 5 figures",
        "journal-ref": "Models and Reality: A Festschrift for James R. Thompson , pp.\n  225-236 (2017)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We empirically show the superiority of the equally weighted S\\&P 500\nportfolio over Sharpe's market capitalization weighted S\\&P 500 portfolio. We\nproceed to consider the MaxMedian rule, a non-proprietary rule designed for the\ninvestor who wishes to do his/her own investing on a laptop with the purchase\nof only 20 stocks. Rather surprisingly, over the 1958-2016 horizon, the\ncumulative returns of MaxMedian beat those of the equally weighted S\\&P 500\nportfolio by a factor of 1.15.\n"
    },
    {
        "paper_id": 1602.00839,
        "authors": "Ravi Kashyap",
        "title": "A Tale of Two Consequences: Intended and Unintended Outcomes of the\n  Japan TOPIX Tick Size Changes",
        "comments": "This version has a mathematical proof to show conditions under which\n  endogeneity is resolved when doing a regression. The idea is consider the\n  changes in the coefficients before and after an event. This theoretical proof\n  is illustrated using the data from our event study",
        "journal-ref": "Journal of Trading, Vol. 10, No. 4 (Fall 2015), pp. 51-95",
        "doi": "10.3905/jot.2015.10.4.051",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We look at the effect of the tick size changes on the TOPIX 100 index names\nmade by the Tokyo Stock Exchange on Jan-14-2014 and Jul-22-2104. The intended\nconsequence of the change is price improvement and shorter time to execution.\nWe look at security level metrics that include the spread, trading volume,\nnumber of trades and the size of trades to establish whether this goal is\naccomplished. An unintended effect might be the reduction in execution sizes,\nwhich would then mean that institutions with large orders would have greater\ndifficulty in sourcing liquidity. We look at a sample of real orders to see if\nthe execution costs have gone up across the orders since the implementation of\nthis change.\n  We study the mechanisms that affect how securities are traded on an exchange,\nbefore delving into the specifics of the TSE tick size events. Some of the\ntopics we explore are: The Venue Menu and How to Increase Revenue; To Automate\nor Not to Automate; Microstructure under the Microscope; The Price of\nConnections to High (and Faraway) Places; Speed Thrills but Kills; Pick a Size\nfor the Perfect Tick; TSE Tick Size Experiments, Then and Now; Sergey Bubka and\nthe Regulators; Bird`s Eye View; Deep Dive; Possibilities for a Deeper Dive;\nDoes Tick Size Matter? Tick Size Does Matter!\n"
    },
    {
        "paper_id": 1602.00865,
        "authors": "Johannes Rauch, Carol Alexander",
        "title": "Tail Risk Premia for Long-Term Equity Investors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the P&L on a particular class of swaps, representing variance and\nhigher moments for log returns, as estimators in our empirical study on the\nS&P500 that investigates the factors determining variance and higher-moment\nrisk premia. This class is the discretisation invariant sub-class of swaps with\nNeuberger's aggregating characteristics. Besides the market excess return,\nmomentum is the dominant driver for both skewness and kurtosis risk premia,\nwhich exhibit a highly significant negative correlation. By contrast, the\nvariance risk premium responds positively to size and negatively to growth, and\nthe correlation between variance and tail risk premia is relatively low\ncompared with previous research, particularly at high sampling frequencies.\nThese findings extend prior research on determinants of these risk premia.\nFurthermore, our meticulous data-construction methodology avoids unwanted\nartefacts which distort results.\n"
    },
    {
        "paper_id": 1602.00899,
        "authors": "Dariusz Zawisza",
        "title": "Smooth solutions to discounted reward control problems with unbounded\n  discount rate and financial applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discounted reward control problem in continuous time stochastic\nenvironment where the discount rate might be an unbounded function of the\ncontrol process. We provide a set of general assumptions to ensure that there\nexists a smooth classical solution to the corresponding HJB equation. Moreover,\nsome verification reasoning are provided and the possible extension to dynamic\ngames is discussed. At the end of the paper consumption - investment problems\narising in financial economics are considered.\n"
    },
    {
        "paper_id": 1602.00931,
        "authors": "Sebastien Valeyre, Denis Grebenkov, Sofiane Aboura, and Francois\n  Bonnin",
        "title": "Should employers pay their employees better? An asset pricing approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We uncover a new anomaly in asset pricing that is linked to the remuneration:\nthe more a company spends on salaries and benefits per employee, the better its\nstock performs, on average. Moreover, the companies adopting similar\nremuneration policies share a common risk, which is comparable to that of the\nvalue premium. For this purpose,we set up an original methodology that uses\nfirm financial characteristics to build factors that are less correlated than\nin the standard asset pricing methodology. We quantify the importance of these\nfactors from an asset pricing perspective by introducing the factor correlation\nlevel as a directly accessible proxy of eigenvalues of the correlation matrix.\nA rational explanation of the remuneration anomaly involves the positive\ncorrelation between pay and employee performance.\n"
    },
    {
        "paper_id": 1602.0107,
        "authors": "Lingqi Gu and Yiqing Lin and Junjian Yang",
        "title": "A note on utility maximization with transaction costs and random\n  endoment: num\\'eraire-based model and convex duality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we study the utility maximization problem on the terminal\nwealth under proportional transaction costs and bounded random endowment. In\nparticular, we restrict ourselves to the num\\'eraire-based model and work with\nutility functions only supporting R+. Under the assumption of existence of\nconsistent price systems and natural regularity conditions, standard convex\nduality results are established. Precisely, we first enlarge the dual domain\nfrom the collection of martingale densities associated with consistent price\nsystems to a set of finitely additive measures; then the dual formulation of\nthe utility maximization problem can be regarded as an extension of the paper\nof Cvitani\\'c-Schachermayer-Wang (2001) to the context under proportional\ntransaction costs.\n"
    },
    {
        "paper_id": 1602.01109,
        "authors": "Lingqi Gu and Yiqing Lin and Junjian Yang",
        "title": "On the existence of shadow prices for optimal investment with random\n  endowment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a num\\'eraire-based utility maximization problem\nunder constant proportional transaction costs and random endowment. Assuming\nthat the agent cannot short sell assets and is endowed with a strictly positive\ncontingent claim, a primal optimizer of this utility maximization problem\nexists. Moreover, we observe that the original market with transaction costs\ncan be replaced by a frictionless shadow market that yields the same\noptimality. On the other hand, we present an example to show that in some case\nwhen these constraints are relaxed, the existence of shadow prices is still\nwarranted.\n"
    },
    {
        "paper_id": 1602.01271,
        "authors": "Di Molfetta Giuseppe",
        "title": "On the parameter identifiability problem in Agent Based economical\n  models",
        "comments": "Master 2 Recherche dissertation in Economie Theorique et Empirique,\n  Universite Paris 1 Pantheon - Sorbonne and Supervised by Jean-Bernard\n  Chatelain (Director) and Antoine Mandel (Co-Director)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifiability of parameters is a fundamental prerequisite for model\nidentification. It concerns uniqueness of the model parameters determined from\nexperimental or simulated observations. This dissertation specifically deals\nwith structural or a priori identifiability: whether or not parameters can be\nidentified from a given model structure and experimental measurements. We\nbriefly present the identifiability problem in linear and non linear dynamical\nmodel. We compare DSGE and Agent Based model (ABM) in terms of identifiability\nof the structural parameters and we finally discuss limits and perspective of\nnumerical protocols to test global identifiability in case of ergodic and\nmarkovian economical systems.\n"
    },
    {
        "paper_id": 1602.01578,
        "authors": "Giulia Carra, Ismir Mulalic, Mogens Fosgerau, Marc Barthelemy",
        "title": "Modeling the relation between income and commuting distance",
        "comments": "9 pages, 3 figures",
        "journal-ref": "J. R. Soc. Interface 13:20160306 (2016)",
        "doi": "10.1098/rsif.2016.0306",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the distribution of commuting distances and its relation to\nincome. Using data from Denmark, the UK, and the US, we show that the commuting\ndistance is (i) broadly distributed with a slow decaying tail that can be\nfitted by a power law with exponent $\\gamma \\approx 3$ and (ii) an average\ngrowing slowly as a power law with an exponent less than one that depends on\nthe country considered. The classical theory for job search is based on the\nidea that workers evaluate the wage of potential jobs as they arrive\nsequentially through time, and extending this model with space, we obtain\npredictions that are strongly contradicted by our empirical findings. We\npropose an alternative model that is based on the idea that workers evaluate\npotential jobs based on a quality aspect and that workers search for jobs\nsequentially across space. We also assume that the density of potential jobs\ndepends on the skills of the worker and decreases with the wage. The predicted\ndistribution of commuting distances decays as $1/r^{3}$ and is independent of\nthe distribution of the quality of jobs. We find our alternative model to be in\nagreement with our data. This type of approach opens new perspectives for the\nmodeling of mobility.\n"
    },
    {
        "paper_id": 1602.0196,
        "authors": "Emre Kahraman and Gazanfer \\\"Unal",
        "title": "Multiple Wavelet Coherency Analysis and Forecasting of Metal Prices",
        "comments": "15 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The assessment of co-movement among metals is crucial to better understand\nthe behaviors of the metal prices and the interactions with others that affect\nthe changes in prices. In this study, both Wavelet Analysis and VARMA (Vector\nAutoregressive Moving Average) models are utilized. First, Multiple Wavelet\nCoherence (MWC), where Wavelet Analysis is needed, is utilized to determine\ndynamic correlation time interval and scales. VARMA is then used for\nforecasting which results in reduced errors.\n  The daily prices of steel, aluminium, copper and zinc between 10.05.2010 and\n29.05.2014 are analyzed via wavelet analysis to highlight the interactions.\nResults uncover interesting dynamics between mentioned metals in the\ntime-frequency space. VARMA (1,1) model forecasting is carried out considering\nthe daily prices between 14.11.2011 and 16.11.2012 where the interactions are\nquite high and prediction errors are found quite limited with respect to\nARMA(1.1). It is shown that dynamic co-movement detection via four variables\nwavelet coherency analysis in the determination of VARMA time interval enables\nto improve forecasting power of ARMA by decreasing forecasting errors.\n"
    },
    {
        "paper_id": 1602.02011,
        "authors": "Andreas Lager{\\aa}s and Mathias Lindholm",
        "title": "Issues with the Smith-Wilson method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of the present paper is to analyse various features of the\nSmith-Wilson method used for discounting under the EU regulation Solvency II,\nwith special attention to hedging. In particular, we show that all key rate\nduration hedges of liabilities beyond the Last Liquid Point will be peculiar.\nMoreover, we show that there is a connection between the occurrence of negative\ndiscount factors and singularities in the convergence criterion used to\ncalibrate the model. The main tool used for analysing hedges is a novel\nstochastic representation of the Smith-Wilson method. Further, we provide\nnecessary conditions needed in order to construct similar, but hedgeable,\ndiscount curves.\n"
    },
    {
        "paper_id": 1602.02185,
        "authors": "Michael Ho, Jack Xin",
        "title": "Sparse Kalman Filtering Approaches to Covariance Estimation from High\n  Frequency Data in the Presence of Jumps",
        "comments": null,
        "journal-ref": "J. Math. Program. (2019)",
        "doi": "10.1007/s10107-019-01371-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimation of the covariance matrix of asset returns from high frequency data\nis complicated by asynchronous returns, market mi- crostructure noise and\njumps. One technique for addressing both asynchronous returns and market\nmicrostructure is the Kalman-EM (KEM) algorithm. However the KEM approach\nassumes log-normal prices and does not address jumps in the return process\nwhich can corrupt estimation of the covariance matrix.\n  In this paper we extend the KEM algorithm to price models that include jumps.\nWe propose two sparse Kalman filtering approaches to this problem. In the first\napproach we develop a Kalman Expectation Conditional Maximization (KECM)\nalgorithm to determine the un- known covariance as well as detecting the jumps.\nFor this algorithm we consider Laplace and the spike and slab jump models, both\nof which promote sparse estimates of the jumps. In the second method we take a\nBayesian approach and use Gibbs sampling to sample from the posterior\ndistribution of the covariance matrix under the spike and slab jump model.\nNumerical results using simulated data show that each of these approaches\nprovide for improved covariance estima- tion relative to the KEM method in a\nvariety of settings where jumps occur.\n"
    },
    {
        "paper_id": 1602.02192,
        "authors": "Anatolii A. Puhalskii, Michael Jay Stutzer",
        "title": "On minimising a portfolio's shortfall probability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain a lower asymptotic bound on the decay rate of the probability of a\nportfolio's underperformance against a benchmark over a large time horizon. It\nis assumed that the prices of the securities are governed by geometric Brownian\nmotions with the coefficients depending on an economic factor, possibly\nnonlinearly. The economic factor is modelled with a general Ito equation. The\nbound is shown to be tight. More specifically, epsilon-optimal portfolios are\nobtained under additional conditions.\n"
    },
    {
        "paper_id": 1602.02348,
        "authors": "Inga Ivanova, Oivind Strand, Duncan Kushnir, and Loet Leydesdorff",
        "title": "Economic and Technological Complexity: A Model Study of Indicators of\n  Knowledge-based Innovation Systems",
        "comments": null,
        "journal-ref": "Technological Forecasting and Social Change 120 (July 2017) 77-89",
        "doi": "10.1016/j.techfore.2017.04.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Economic Complexity Index (ECI; Hidalgo & Hausmann, 2009) measures the\ncomplexity of national economies in terms of product groups. Analogously to\nECI, a Patent Complexity Index (PatCI) can be developed on the basis of a\nmatrix of nations versus patent classes. Using linear algebra, the three\ndimensions: countries, product groups, and patent classes can be combined into\na measure of \"Triple Helix\" complexity (THCI) including the trilateral\ninteraction terms between knowledge production, wealth generation, and\n(national) control. THCI can be expected to capture the extent of systems\nintegration between the global dynamics of markets (ECI) and technologies\n(PatCI) in each national system of innovation. We measure ECI, PatCI, and THCI\nduring the period 2000-2014 for the 34 OECD member states, the BRICS countries,\nand a group of emerging and affiliated economies (Argentina, Hong Kong,\nIndonesia, Malaysia, Romania, and Singapore). The three complexity indicators\nare correlated between themselves; but the correlations with GDP per capita are\nvirtually absent. Of the world's major economies, Japan scores highest on all\nthree indicators, while China has been increasingly successful in combining\neconomic and technological complexity. We could not reproduce the correlation\nbetween ECI and average income that has been central to the argument about the\nfruitfulness of the economic complexity approach.\n"
    },
    {
        "paper_id": 1602.02542,
        "authors": "Leopoldo Catania and Anna Gloria Bill\\'e",
        "title": "Dynamic Spatial Autoregressive Models with Autoregressive and\n  Heteroskedastic Disturbances",
        "comments": "old version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new class of models specifically tailored for spatio-temporal\ndata analysis. To this end, we generalize the spatial autoregressive model with\nautoregressive and heteroskedastic disturbances, i.e. SARAR(1,1), by exploiting\nthe recent advancements in Score Driven (SD) models typically used in time\nseries econometrics. In particular, we allow for time-varying spatial\nautoregressive coefficients as well as time-varying regressor coefficients and\ncross-sectional standard deviations. We report an extensive Monte Carlo\nsimulation study in order to investigate the finite sample properties of the\nMaximum Likelihood estimator for the new class of models as well as its\nflexibility in explaining several dynamic spatial dependence processes. The new\nproposed class of models are found to be economically preferred by rational\ninvestors through an application in portfolio optimization.\n"
    },
    {
        "paper_id": 1602.02735,
        "authors": "Damian Eduardo Taranto, Giacomo Bormetti, Jean-Philippe Bouchaud,\n  Fabrizio Lillo and Bence Toth",
        "title": "Linear models for the impact of order flow on prices I. Propagators:\n  Transient vs. History Dependent Impact",
        "comments": "22 pages, 9 figures, and 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market impact is a key concept in the study of financial markets and several\nmodels have been proposed in the literature so far. The Transient Impact Model\n(TIM) posits that the price at high frequency time scales is a linear\ncombination of the signs of the past executed market orders, weighted by a\nso-called propagator function. An alternative description -- the History\nDependent Impact Model (HDIM) -- assumes that the deviation between the\nrealised order sign and its expected level impacts the price linearly and\npermanently. The two models, however, should be extended since prices are a\npriori influenced not only by the past order flow, but also by the past\nrealisation of returns themselves. In this paper, we propose a two-event\nframework, where price-changing and non price-changing events are considered\nseparately. Two-event propagator models provide a remarkable improvement of the\ndescription of the market impact, especially for large tick stocks, where the\nevents of price changes are very rare and very informative. Specifically the\nextended approach captures the excess anti-correlation between past returns and\nsubsequent order flow which is missing in one-event models. Our results\ndocument the superior performances of the HDIMs even though only in minor\nrelative terms compared to TIMs. This is somewhat surprising, because HDIMs are\nwell grounded theoretically, while TIMs are, strictly speaking, inconsistent.\n"
    },
    {
        "paper_id": 1602.02907,
        "authors": "Fred Espen Benth, Heidar Eyjolfsson",
        "title": "Simulation of volatility modulated Volterra processes using hyperbolic\n  stochastic partial differential equations",
        "comments": "Published at http://dx.doi.org/10.3150/14-BEJ675 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)",
        "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 774-793",
        "doi": "10.3150/14-BEJ675",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a finite difference scheme to simulate solutions to a certain type\nof hyperbolic stochastic partial differential equation (HSPDE). These solutions\ncan in turn estimate so called volatility modulated Volterra (VMV) processes\nand L\\'{e}vy semistationary (LSS) processes, which is a class of processes that\nhave been employed to model turbulence, tumor growth and electricity forward\nand spot prices. We will see that our finite difference scheme converges to the\nsolution of the HSPDE as we take finer and finer partitions for our finite\ndifference scheme in both time and space. Finally, we demonstrate our method\nwith an example from the energy finance literature.\n"
    },
    {
        "paper_id": 1602.03011,
        "authors": "Michael Benzaquen, Jonathan Donier, Jean-Philippe Bouchaud",
        "title": "Unravelling the trading invariance hypothesis",
        "comments": "11 pages, 9 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We confirm and substantially extend the recent empirical result of Andersen\net al. \\cite{Andersen2015}, where it is shown that the amount of risk $W$\nexchanged in the E-mini S\\&P futures market (i.e. price times volume times\nvolatility) scales like the 3/2 power of the number of trades $N$. We show that\nthis 3/2-law holds very precisely across 12 futures contracts and 300 single US\nstocks, and across a wide range of time scales. However, we find that the\n\"trading invariant\" $I=W/N^{3/2}$ proposed by Kyle and Obizhaeva is in fact\nquite different for different contracts, in particular between futures and\nsingle stocks. Our analysis suggests $I/{\\cal C}$ as a more natural candidate,\nwhere $\\cal C$ is the average spread cost of a trade, defined as the average of\nthe trade size times the bid-ask spread. We also establish two more complex\nscaling laws for the volatility $\\sigma$ and the traded volume $V$ as a\nfunction of $N$, that reveal the existence of a characteristic number of trades\n$N_0$ above which the expected behaviour $\\sigma \\sim \\sqrt{N}$ and $V \\sim N$\nhold, but below which strong deviations appear, induced by the size of\nthe~tick.\n"
    },
    {
        "paper_id": 1602.03043,
        "authors": "Bence Toth, Zoltan Eisler, Jean-Philippe Bouchaud",
        "title": "The square-root impact law also holds for option markets",
        "comments": "short note, 3 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many independent studies on stocks and futures contracts have established\nthat market impact is proportional to the square-root of the executed volume.\nIs market impact quantitatively similar for option markets as well? In order to\nanswer this question, we have analyzed the impact of a large proprietary data\nset of option trades. We find that the square-root law indeed holds in that\ncase. This finding supports the argument for a universal underlying mechanism.\n"
    },
    {
        "paper_id": 1602.03238,
        "authors": "Pavel V. Shevchenko and Xiaolin Luo",
        "title": "Valuation of Variable Annuities with Guaranteed Minimum Withdrawal\n  Benefit under Stochastic Interest Rate",
        "comments": "arXiv admin note: text overlap with arXiv:1410.8609",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A variable annuity contract with Guaranteed Minimum Withdrawal Benefit (GMWB)\npromises to return the entire initial investment through cash withdrawals\nduring the contract plus the remaining account balance at maturity, regardless\nof the portfolio performance. Under the optimal(dynamic) withdrawal strategy of\na policyholder, GMWB pricing becomes an optimal stochastic control problem that\ncan be solved by backward recursion of Bellman equation. In this paper we\ndevelop a very efficient new algorithm for pricing these contracts in the case\nof stochastic interest rate not considered previously in the literature.\nPresently our method is applied to the Vasicek interest rate model, but it is\ngenerally applicable to any model when transition density or moments of the\nunderlying asset and interest rate are known in closed form or can be evaluated\nefficiently. Using bond price as a numeraire the required expectations in the\nbackward recursion are reduced to two-dimensional integrals calculated through\na high order Gauss-Hermite quadrature applied on a two-dimensional cubic spline\ninterpolation. Numerical results from the new algorithm for a series of GMWB\ncontracts for both static and optimal cases are presented. As a validation,\nresults of the algorithm are compared with the closed form solutions for simple\nvanilla options, and with Monte Carlo and finite difference results for the\nstatic GMWB. The comparison demonstrates that the new algorithm is\nsignificantly faster than finite difference or Monte Carlo for all the\ntwo-dimensional problems tested so far. For dynamic GMWB pricing, we found that\nfor positive correlation between the underlying asset and interest rate, the\nGMWB price under the stochastic interest rate is significantly higher compared\nto the case of deterministic interest rate, while for negative correlation the\ndifference is less but still significant.\n"
    },
    {
        "paper_id": 1602.03271,
        "authors": "Semei Coronado and Omar Rojas",
        "title": "A study of co-movements between oil price, stock index and exchange rate\n  under a cross-bicorrelation perspective: the case of Mexico",
        "comments": "14 pages, accepted to be published in the book Modelado de\n  Fen\\'omenos Econ\\'omicos y Financieros: Una Visi\\'on Contempor\\'anea, Vol. 1\n  (C.E. Castillo Ram\\'irez, F. L\\'opez Herrera and F. Venegas Mart\\'inez\n  (eds.))",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this chapter we studied the nonlinear co-movements between the Mexican\nCrude Oil price, the Mexican Stock Market Index and the USD/MXN Exchange Rate,\nfor the sample period from 1994 to date. We used a battery of nonlinear tests,\ncf. (Patterson & Ashley, 2000) and one multivariate test, in order to determine\nthe dynamic co-movement exerted from the oil prices to the stock and exchange\nrate markets. Such co-movement and time windows are exposed using the Brooks &\nHinich (1999) cross- bicorrelation statistical test. The effects of oil spills\non other markets have been studied from different angles and on several\nfinancial assets. In this study, we focus our attention on the detection, not\nonly of the correlations amongst markets but on the epochs in which such\nnonlinear dependence might occur. This is important in order to understand\nbetter, how the markets that drive the economy interact with each other. We\nhope to contribute to the literature with such findings, filling a gap in the\nemerging markets context, in particular, for the Mexican case.\n"
    },
    {
        "paper_id": 1602.03402,
        "authors": "Maren Diane Schmeck",
        "title": "Pricing options on forwards in energy markets: the role of mean\n  reversion's speed",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider the problem of pricing options on forwards in energy markets, when\nspot prices follow a geometric multi-factor model in which several rates of\nmean reversion appear. In this paper we investigate the role played by slow\nmean reversion when pricing and hedging options. In particular, we determine\nboth upper and lower bounds for the error one makes neglecting low rates of\nmean reversion in the spot price dynamics.\n"
    },
    {
        "paper_id": 1602.03505,
        "authors": "Sebastian Poledna, Olaf Bochmann and Stefan Thurner",
        "title": "Basel III capital surcharges for G-SIBs fail to control systemic risk\n  and can cause pro-cyclical side effects",
        "comments": "12 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1401.8026",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In addition to constraining bilateral exposures of financial institutions,\nthere are essentially two options for future financial regulation of systemic\nrisk (SR): First, financial regulation could attempt to reduce the financial\nfragility of global or domestic systemically important financial institutions\n(G-SIBs or D-SIBs), as for instance proposed in Basel III. Second, future\nfinancial regulation could attempt strengthening the financial system as a\nwhole. This can be achieved by re-shaping the topology of financial networks.\nWe use an agent-based model (ABM) of a financial system and the real economy to\nstudy and compare the consequences of these two options. By conducting three\n\"computer experiments\" with the ABM we find that re-shaping financial networks\nis more effective and efficient than reducing leverage. Capital surcharges for\nG-SIBs can reduce SR, but must be larger than those specified in Basel III in\norder to have a measurable impact. This can cause a loss of efficiency. Basel\nIII capital surcharges for G-SIBs can have pro-cyclical side effects.\n"
    },
    {
        "paper_id": 1602.03944,
        "authors": "Ioane Muni Toke and Nakahiro Yoshida",
        "title": "Modelling intensities of order flows in a limit order book",
        "comments": "30 pages, 15 figures, 4 tables",
        "journal-ref": "Quantitative Finance, 17(5), 683-701 (2017)",
        "doi": "10.1080/14697688.2016.1236210",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a parametric model for the simulation of limit order books. We\nassume that limit orders, market orders and cancellations are submitted\naccording to point processes with state-dependent intensities. We propose new\nfunctional forms for these intensities, as well as new models for the placement\nof limit orders and cancellations. For cancellations, we introduce the concept\nof \"priority index\" to describe the selection of orders to be cancelled in the\norder book. Parameters of the model are estimated using likelihood\nmaximization. We illustrate the performance of the model by providing extensive\nsimulation results, with a comparison to empirical data and a standard Poisson\nreference.\n"
    },
    {
        "paper_id": 1602.04352,
        "authors": "Stephanie Rend\\'on de la Torre, Jaan Kalda, Robert Kitt, J\\\"uri\n  Engelbrecht",
        "title": "On the topologic structure of economic complex networks: Empirical\n  evidence from large scale payment network of Estonia",
        "comments": "Chaos, Solitons and Fractals: the interdisciplinary journal of\n  Nonlinear Science, and Nonequilibrium and Complex Phenomena, 2016",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2016.01.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the first topological analysis of the economic structure\nof an entire country based on payments data obtained from Swedbank. This data\nset is exclusive in its kind because around 80% of Estonia's bank transactions\nare done through Swedbank, hence, the economic structure of the country can be\nreconstructed. Scale-free networks are commonly observed in a wide array of\ndifferent contexts such as nature and society. In this paper, the nodes are\ncomprised by customers of the bank (legal entities) and the links are\nestablished by payments between these nodes. We study the scaling-free and\nstructural properties of this network. We also describe its topology,\ncomponents and behaviors. We show that this network shares typical structural\ncharacteristics known in other complex networks: degree distributions follow a\npower law, low clustering coefficient and low average shortest path length. We\nidentify the key nodes of the network and perform simulations of resiliency\nagainst random and targeted attacks of the nodes with two different approaches.\nWith this, we find that by identifying and studying the links between the nodes\nis possible to perform vulnerability analysis of the Estonian economy with\nrespect to economic shocks.\n"
    },
    {
        "paper_id": 1602.04363,
        "authors": "Masayuki Hattori, Sumiyoshi Abe",
        "title": "Path probability of stochastic motion: A functional approach",
        "comments": "19 pages, 2 figures",
        "journal-ref": "Physica A 451 (2016) 198-204",
        "doi": "10.1016/j.physa.2016.01.053",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The path probability of a particle undergoing stochastic motion is studied by\nthe use of functional technique, and the general formula is derived for the\npath probability distribution functional. The probability of finding paths\ninside a tube/band, the center of which is stipulated by a given path, is\nanalytically evaluated in a way analogous to continuous measurements in quantum\nmechanics. Then, the formalism developed here is applied to the stochastic\ndynamics of stock price in finance.\n"
    },
    {
        "paper_id": 1602.04372,
        "authors": "Vinicius Albani, Uri M. Ascher and Jorge P. Zubelli",
        "title": "Local Volatility Models in Commodity Markets and Online Calibration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a local volatility model for the valuation of options on\ncommodity futures by using European vanilla option prices. The corresponding\ncalibration problem is addressed within an online framework, allowing the use\nof multiple price surfaces. Since uncertainty in the observation of the\nunderlying future prices translates to uncertainty in data locations, we\npropose a model-based adjustment of such prices that improves reconstructions\nand smile adherence. In order to tackle the ill-posedness of the calibration\nproblem we incorporate a priori information through a judiciously designed\nTikhonov-type regularization. Extensive empirical tests with market as well as\nsynthetic data are used to demonstrate the effectiveness of the methodology and\nalgorithms.\n"
    },
    {
        "paper_id": 1602.04423,
        "authors": "Vladislav Gennadievich Malyshkin",
        "title": "Market Dynamics. On Supply and Demand Concepts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The disbalance of Supply and Demand is typically considered as the driving\nforce of the markets. However, the measurement or estimation of Supply and\nDemand at price different from the execution price is not possible even after\nthe transaction. An approach in which Supply and Demand are always matched, but\nthe rate $I=dv/dt$ (number of units traded per unit time) of their matching\nvaries, is proposed. The state of the system is determined not by a price $p$,\nbut by a probability distribution defined as the square of a wavefunction\n$\\psi(p)$. The equilibrium state $\\psi^{[H]}$ is postulated to be the one\ngiving maximal $I$ and obtained from maximizing the matching rate functional\n$<I\\psi^2(p)>/<\\psi^2(p)>$, i.e. solving the dynamic equation of the form\n\"future price tend to the value maximizing the number of shares traded per unit\ntime\". An application of the theory in a quasi--stationary case is\ndemonstrated. This transition from Supply and Demand concept to Liquidity\nDeficit concept, described by the matching rate $I$, allows to operate only\nwith observable variables, and have a theory applicable to practical problems.\n"
    },
    {
        "paper_id": 1602.04466,
        "authors": "Eric Lavallee",
        "title": "Mediation with near insolvent defaulting suppliers: a linear\n  optimisation model to find an optimal outcome",
        "comments": "21 pages,5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a model to describe contractual dispute resolution by\nmediation in situations where a defaulting supplier is near insolvent. While\neach party has internal constraints, and if alternate performances are\navailable, such as more costly alternative goods, the proposed approach allows\nthe mediator to find an optimal solution. The notion of optimality is presented\nas adherence to the initial contract, therefore optimising a value function for\nthe non defaulting party. The proposed model includes describing the evolution\nover time of each party's perceived constraints using a phasor like approach\nwith a modulation to the core constraints phasing out of the real part and\nphasing in the imaginary part of complex numbers. The offers related to\nalternative performances by the defaulting party are modelled by a Gompertz\nfunction, being an exponential learning curve of the supplier in regards to the\nreaction to its offers, limited by another exponential function when\napproaching its internal constraints. Furthermore, the model takes into account\nthe discount associated to the delay in the delivery time of the alternative\nperformances.\n"
    },
    {
        "paper_id": 1602.0458,
        "authors": "Matija Vidmar",
        "title": "Ruin under stochastic dependence between premium and claim arrivals",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate, focusing on the ruin probability, an adaptation of the\nCramer-Lundberg model for the surplus process of an insurance company, in\nwhich, conditionally on their intensities, the two mixed Poisson processes\ngoverning the arrival times of the premiums and of the claims respectively, are\nindependent. Such a model exhibits a stochastic dependence between the\naggregate premium and claim amount processes. An explicit expression for the\nruin probability is obtained when the claim and premium sizes are exponentially\ndistributed.\n"
    },
    {
        "paper_id": 1602.04656,
        "authors": "Michaela Sz\\\"olgyenyi",
        "title": "Dividend maximization in a hidden Markov switching model",
        "comments": "Forthcoming in Statistics & Risk Modeling",
        "journal-ref": "Statistics & Risk Modeling, 32(3-4):143-158, 2016",
        "doi": "10.1515/strm-2015-0019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the valuation problem of an insurance company by\nmaximizing the expected discounted future dividend payments in a model with\npartial information that allows for a changing economic environment. The\nsurplus process is modeled as a Brownian motion with drift. This drift depends\non an underlying Markov chain the current state of which is assumed to be\nunobservable. The different states of the Markov chain thereby represent\ndifferent phases of the economy. We apply results from filtering theory to\novercome uncertainty and then we give an analytic characterization of the\noptimal value function. Finally, we present a numerical study covering various\nscenarios to get a clear picture of how dividends should be paid out.\n"
    },
    {
        "paper_id": 1602.0466,
        "authors": "Gunther Leobacher, Michaela Sz\\\"olgyenyi, Stefan Thonhauser",
        "title": "Bayesian Dividend Optimization and Finite Time Ruin Probabilities",
        "comments": null,
        "journal-ref": "Stochastic Models, Vol. 30, Issue 2, p. 216-249, 2014",
        "doi": "10.1080/15326349.2014.900390",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the valuation problem of an (insurance) company under partial\ninformation. Therefore we use the concept of maximizing discounted future\ndividend payments. The firm value process is described by a diffusion model\nwith constant and observable volatility and constant but unknown drift\nparameter. For transforming the problem to a problem with complete information,\nwe derive a suitable filter. The optimal value function is characterized as the\nunique viscosity solution of the associated Hamilton-Jacobi-Bellman equation.\nWe state a numerical procedure for approximating both the optimal dividend\nstrategy and the corresponding value function. Furthermore, threshold\nstrategies are discussed in some detail. Finally, we calculate the probability\nof ruin in the uncontrolled and controlled situation.\n"
    },
    {
        "paper_id": 1602.04662,
        "authors": "Anton A. Shardin, Michaela Sz\\\"olgyenyi",
        "title": "Optimal Control of an Energy Storage Facility Under a Changing Economic\n  Environment and Partial Information",
        "comments": "Forthcoming in International Journal of Theoretical and Applied\n  Finance",
        "journal-ref": "International Journal of Theoretical and Applied Finance,\n  19(4):1-27, 2016",
        "doi": "10.1142/S0219024916500266",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider an energy storage optimization problem in finite\ntime in a model with partial information that allows for a changing economic\nenvironment. The state process consists of the storage level controlled by the\nstorage manager and the energy price process, which is a diffusion process the\ndrift of which is assumed to be unobservable. We apply filtering theory to find\nan alternative state process which is adapted to our observation filtration.\nFor this alternative state process we derive the associated\nHamilton-Jacobi-Bellman equation and solve the optimization problem\nnumerically. This results in a candidate for the optimal policy for which it is\na-priori not clear whether the controlled state process exists. Hence, we prove\nan existence and uniqueness result for a class of time-inhomogeneous stochastic\ndifferential equations with discontinuous drift and singular diffusion\ncoefficient. Finally, we apply our result to prove admissibility of the\ncandidate optimal control.\n"
    },
    {
        "paper_id": 1602.04848,
        "authors": "Hanno Gottschalk, Elpida Nizami and Marius Schubert",
        "title": "Option Pricing in Markets with Unknown Stochastic Dynamics",
        "comments": "20 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider arbitrage free valuation of European options in Black-Scholes and\nMerton markets, where the general structure of the market is known, however the\nspecific parameters are not known. In order to reflect this subjective\nuncertainty of a market participant, we follow a Bayesian approach to option\npricing. Here we use historic discrete or continuous observations of the market\nto set up posterior distributions for the future market. Given a subjective\nphysical measure for the market dynamics, we derive the existence of arbitrage\nfree pricing rules by constructing subjective option pricing measures. The\nnon-uniqueness of such measures can be proven using the freedom of choice of\nprior distributions. The subjective market measure thus turns out to model an\nincomplete market. In addition, for the Black-Scholes market we prove that in\nthe high frequency limit (or the long time limit) of observations, Bayesian\noption prices converge to the standard BS-Option price with the true\nvolatility. In contrast to this, in the Merton market with normally distributed\njumps Bayesian prices do not converge to standard Merton prices with the true\nparameters, as only a finite number of jump events can be observed in finite\ntime. However, we prove that this convergence holds true in the limit of long\nobservation times.\n"
    },
    {
        "paper_id": 1602.04902,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Multifactor Risk Models and Heterotic CAPM",
        "comments": "49 pages; one reference updated; to appear in The Journal of\n  Investment Strategies. arXiv admin note: substantial text overlap with\n  arXiv:1508.04883",
        "journal-ref": "The Journal of Investment Strategies 5(4) (2016) 1-49",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give a complete algorithm and source code for constructing general\nmultifactor risk models (for equities) via any combination of style factors,\nprincipal components (betas) and/or industry factors. For short horizons we\nemploy the Russian-doll risk model construction to obtain a nonsingular factor\ncovariance matrix. This generalizes the heterotic risk model construction to\ninclude arbitrary non-industry risk factors as well as industry risk factors\nwith generic \"weights\". The aim of sharing our proprietary know-how with the\ninvestment community is to encourage organic risk model building. The\npresentation is intended to be essentially self-contained and pedagogical. So,\nstop wasting money and complaining, start building risk models and enjoy!\n"
    },
    {
        "paper_id": 1602.04946,
        "authors": "Candia Riga",
        "title": "A pathwise approach to continuous-time trading",
        "comments": "33 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a mathematical framework for the analysis of\ncontinuous-time trading strategies which, in contrast to the classical setting\nof continuous-time mathematical finance, does not rely on stochastic integrals\nor other probabilistic notions. Our purely analytic framework allows for the\nderivation of a pathwise self-financial condition for continuous-time trading\nstrategies, which is consistent with the classical definition in case a\nprobability model is introduced. Our first proposition provides us with a\npathwise definition of the gain process for a large class of continuous-time,\npath-dependent, self-finacing trading strategies, including the important class\nof 'delta-hedging' strategies, and is based on the recently developed\n'non-anticipative functional calculus'. Two versions of the statement involve\nrespectively continuous and c\\`adl\\`ag price paths. The second proposition is a\npathwise replication result that generalizes the ones obtained in the classical\nframework of diffusion models. Moreover, it gives an explicit and purely\npathwise formula for the hedging error of delta-hedging strategies for\npath-dependent derivatives across a given set of scenarios. We also provide an\neconomic justification of our main assumption on price paths.\n"
    },
    {
        "paper_id": 1602.0495,
        "authors": "Michael Harvey, Dieter Hendricks, Tim Gebbie, Diane Wilcox",
        "title": "Deviations in expected price impact for small transaction volumes under\n  fee restructuring",
        "comments": "11 pages, 13 figures",
        "journal-ref": "Physica A 471(1) 2017, 416-426",
        "doi": "10.1016/j.physa.2016.11.042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report on the occurrence of an anomaly in the price impacts of small\ntransaction volumes following a change in the fee structure of an electronic\nmarket. We first review evidence for the existence of a master curve for price\nimpact on the Johannesburg Stock Exchange (JSE). On attempting to re-estimate a\nmaster curve after fee reductions, it is found that the price impact\ncorresponding to smaller volume trades is greater than expected relative to\nprior estimates for a range of listed stocks. We show that a master curve for\nprice impact can be found following rescaling by an appropriate liquidity\nproxy, providing a means for practitioners to approximate price impact curves\nwithout onerous processing of tick data.\n"
    },
    {
        "paper_id": 1602.04975,
        "authors": "Chi Kin Lam, Yuhong Xu, Guosheng Yin",
        "title": "Dynamic portfolio selection without risk-free assets",
        "comments": "41 pages,8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the mean--variance portfolio optimization problem under the game\ntheoretic framework and without risk-free assets. The problem is solved\nsemi-explicitly by applying the extended Hamilton--Jacobi--Bellman equation.\nAlthough the coefficient of risk aversion in our model is a constant, the\noptimal amounts of money invested in each stock still depend on the current\nwealth in general. The optimal solution is obtained by solving a system of\nordinary differential equations whose existence and uniqueness are proved and a\nnumerical algorithm as well as its convergence speed are provided. Different\nfrom portfolio selection with risk-free assets, our value function is quadratic\nin the current wealth, and the equilibrium allocation is linearly sensitive to\nthe initial wealth. Numerical results show that this model performs better than\nboth the classical one and the variance model in a bull market.\n"
    },
    {
        "paper_id": 1602.05323,
        "authors": "Vikram Krishnamurthy, Elisabeth Leoff, J\\\"orn Sass",
        "title": "Filterbased Stochastic Volatility in Continuous-Time Hidden Markov\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regime-switching models, in particular Hidden Markov Models (HMMs) where the\nswitching is driven by an unobservable Markov chain, are widely-used in\nfinancial applications, due to their tractability and good econometric\nproperties. In this work we consider HMMs in continuous time with both constant\nand switching volatility. In the continuous-time model with switching\nvolatility the underlying Markov chain could be observed due to this stochastic\nvolatility, and no estimation (filtering) of it is needed (in theory), while in\nthe discretized model or the model with constant volatility one has to filter\nfor the underlying Markov chain. The motivations for continuous-time models are\nexplicit computations in finance. To have a realistic model with unobservable\nMarkov chain in continuous time and good econometric properties we introduce a\nregime-switching model where the volatility depends on the filter for the\nunderlying chain and state the filtering equations. We prove an approximation\nresult for a fixed information filtration and further motivate the model by\nconsidering social learning arguments. We analyze its relation to the switching\nvolatility model and present a convergence result for the discretized model. We\nthen illustrate its econometric properties by considering numerical\nsimulations.\n"
    },
    {
        "paper_id": 1602.05356,
        "authors": "Marcel Ausloos and Roy Cerqueti",
        "title": "Studies on Regional Wealth Inequalities: the case of Italy",
        "comments": "17 pages; 2 tables; 7 figures; 25 references; prepared for Acta\n  Physica Polonica (FENS 2015)",
        "journal-ref": "Acta Physica Polonica A 129 (2016) 959-964",
        "doi": "10.12693/APhysPolA.129.959",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper contains a short review of techniques examining regional wealth\ninequalities based on recently published research work but is also presenting\nunpublished features.\n  The data pertains to Italy (IT), over the period 2007-2011: the number of\ncities in regions, the number of inhabitants in cities and in regions, as well\nas the aggregated tax income of the cities and of regions. Frequency-size plots\nand cumulative distribution function plots, scatter plots and rank-size plots\nare displayed. The rank-size rule of a few cases is discussed. Yearly data of\nthe aggregated tax income is transformed into a few indicators: the Gini,\nTheil, and Herfindahl-Hirschman indices. Numerical results confirm that IT is\ndivided into very different regional realities. One region is selected for a\nshort discussion: Molise.\n  A note on the \"first digit Benford law\" for testing data validity is\npresented.\n"
    },
    {
        "paper_id": 1602.05385,
        "authors": "Ladislav Kristoufek",
        "title": "Power-law cross-correlations estimation under heavy tails",
        "comments": "19 pages, 6 figures",
        "journal-ref": "Communications in Nonlinear Science and Numerical Simulation,\n  Volume 40, November 2016, Pages 163-172",
        "doi": "10.1016/j.cnsns.2016.04.010",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the performance of six estimators of the power-law\ncross-correlations -- the detrended cross-correlation analysis, the detrending\nmoving-average cross-correlation analysis, the height cross-correlation\nanalysis, the averaged periodogram estimator, the cross-periodogram estimator\nand the local cross-Whittle estimator -- under heavy-tailed distributions. The\nselection of estimators allows to separate these into the time and frequency\ndomain estimators. By varying the characteristic exponent of the\n$\\alpha$-stable distributions which controls the tails behavior, we report\nseveral interesting findings. First, the frequency domain estimators are\npractically unaffected by heavy tails bias-wise. Second, the time domain\nestimators are upward biased for heavy tails but they have lower estimator\nvariance than the other group for short series. Third, specific estimators are\nmore appropriate depending on distributional properties and length of the\nanalyzed series. In addition, we provide a discussion of implications of these\nresults for empirical applications as well as theoretical explanations.\n"
    },
    {
        "paper_id": 1602.05471,
        "authors": "Francesca Biagini and Jacopo Mancin",
        "title": "Robust Financial Bubbles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the concept of financial bubble in a market model endowed with a set\nof probability measures, typically mutually singular to each other. In this\nsetting we introduce the notions of robust bubble and robust fundamental value\nin a consistent way with the existing literature in the case a unique prior\nexists. The notion of no dominance is also investigated under the uncertainty\nframework. Finally, we provide concrete examples illustrating our results.\n"
    },
    {
        "paper_id": 1602.05477,
        "authors": "Pablo Koch-Medina, Cosimo Munari, Gregor Svindland",
        "title": "Which eligible assets are compatible with comonotonic capital\n  requirements?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the context of capital adequacy, we study comonotonicity of risk\nmeasures in terms of the primitives of the theory: acceptance sets and\neligible, or reference, assets. We show that comonotonicity cannot be\ncharacterized by the properties of the acceptance set alone and heavily depends\non the choice of the eligible asset. In fact, in many important cases,\ncomonotonicity is only compatible with risk-free eligible assets. The\nincompatibility with risky eligible assets is systematic whenever the\nacceptability criterion is based on Value at Risk or any convex distortion risk\nmeasure such as Expected Shortfall. These findings qualify and arguably call\nfor a critical appraisal of the meaning and the role of comonotonicity within a\ncapital adequacy context.\n"
    },
    {
        "paper_id": 1602.05484,
        "authors": "Francesca Biagini, Jacopo Mancin and Thilo Meyer Brandis",
        "title": "Robust Mean-Variance Hedging via G-Expectation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study mean-variance hedging under the G-expectation\nframework. Our analysis is carried out by exploiting the G-martingale\nrepresentation theorem and the related probabilistic tools, in a contin- uous\nfinancial market with two assets, where the discounted risky one is modeled as\na symmetric G-martingale. By tackling progressively larger classes of\ncontingent claims, we are able to explicitly compute the optimal strategy under\ngeneral assumptions on the form of the contingent claim.\n"
    },
    {
        "paper_id": 1602.05489,
        "authors": "Jozef Barunik and Lukas Vacha",
        "title": "Do co-jumps impact correlations in currency markets?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify how co-jumps impact correlations in currency markets. To\ndisentangle the continuous part of quadratic covariation from co-jumps, and\nstudy the influence of co-jumps on correlations, we propose a new wavelet-based\nestimator. The proposed estimation framework is able to localize the co-jumps\nvery precisely through wavelet coefficients and identify statistically\nsignificant co-jumps. Empirical findings reveal the different behaviors of\nco-jumps during Asian, European and U.S. trading sessions. Importantly, we\ndocument that co-jumps significantly influence correlation in currency markets.\n"
    },
    {
        "paper_id": 1602.05541,
        "authors": "Ying Jiao (ISFA), Chunhua Ma, Simone Scotti (LPMA)",
        "title": "Alpha-CIR Model with Branching Processes in Sovereign Interest Rate\n  Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a class of interest rate models, called the $\\alpha$-CIR model,\nwhich gives a natural extension of the standard CIR model by adopting the\n$\\alpha$-stable L{\\'e}vy process and preserving the branching property. This\nmodel allows to describe in a unified and parsimonious way several recent\nobservations on the sovereign bond market such as the persistency of low\ninterest rate together with the presence of large jumps at local extent. We\nemphasize on a general integral representation of the model by using random\nfields, with which we establish the link to the CBI processes and the affine\nmodels. Finally we analyze the jump behaviors and in particular the large\njumps, and we provide numerical illustrations.\n"
    },
    {
        "paper_id": 1602.05718,
        "authors": "Ron W Nielsen",
        "title": "The Postulate of the Three Regimes of Economic Growth Contradicted by\n  Data",
        "comments": "37 pages,20 figures,10,987 words. Corrected two parameters.\n  Conclusions remain unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic growth in Western Europe, Eastern Europe, Asia, countries of the\nformer USSR, Africa and Latin America were analysed. It is demonstrated that\nthe fundamental postulate of the Unified Growth Theory about the existence of\nthe three regimes of growth (Malthusian regime, post-Malthusian regime and\nsustained-growth regime) is contradicted by data. These regimes did not exist.\nIn particular, there was no escape from the Malthusian trap because there was\nno trap. Economic growth in all these regions was not stagnant but hyperbolic.\nUnified Growth Theory is fundamentally incorrect. However, this theory is also\ndangerously misleading because it claims a transition from the endless epoch of\nstagnation to the new era of sustained economic growth, the interpretation\ncreating the sense of security and a promise of prosperity. The data show that\nthe opposite is true. Economic growth in the past was sustained and secure.\nNow, it is supported by the increasing ecological deficit. The long-term\nsustained and secure economic growth has yet to be created. It did not happen\nautomatically, as suggested incorrectly by the Unified Growth Theory.\n"
    },
    {
        "paper_id": 1602.05749,
        "authors": "Stavros Stavroyiannis",
        "title": "Value-at-Risk and backtesting with the APARCH model and the standardized\n  Pearson type IV distribution",
        "comments": "21 pages, 3 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the efficiency of the Asymmetric Power ARCH (APARCH) model in the\ncase where the residuals follow the standardized Pearson type IV distribution.\nThe model is tested with a variety of loss functions and the efficiency is\nexamined via application of several statistical tests and risk measures. The\nresults indicate that the APARCH model with the standardized Pearson type IV\ndistribution is accurate, within the general financial risk modeling\nperspective, providing the financial analyst with an additional skewed\ndistribution for incorporation in the risk management tools.\n"
    },
    {
        "paper_id": 1602.05758,
        "authors": "Miklos Rasonyi",
        "title": "On optimal strategies for utility maximizers in the Arbitrage Pricing\n  Model",
        "comments": "12 pages, slightly revised",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a popular model of microeconomics with countably many assets: the\nArbitrage Pricing Model. We study the problem of optimal investment under an\nexpected utility criterion and look for conditions ensuring the existence of\noptimal strategies. Previous results required a certain restrictive hypothesis\non the tails of asset return distributions. Using a different method, we manage\nto remove this hypothesis, at the price of stronger assumptions on the moments\nof asset returns.\n"
    },
    {
        "paper_id": 1602.05858,
        "authors": "Peng Huang, Tianxiang Wang",
        "title": "On the Profitability of Optimal Mean Reversion Trading Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the profitability of optimal mean reversion trading strategies in\nthe US equity market. Different from regular pair trading practice, we apply\nmaximum likelihood method to construct the optimal static pairs trading\nportfolio that best fits the Ornstein-Uhlenbeck process, and rigorously\nestimate the parameters. Therefore, we ensure that our portfolios match the\nmean-reverting process before trading. We then generate contrarian trading\nsignals using the model parameters. We also optimize the thresholds and the\nlength of in-sample period by multiple tests. In nine good pair examples, we\ncan see that our pairs exhibit high Sharpe ratio (above 1.9) over the in-sample\nperiod and out-of-sample period. In particular, Crown Castle International\nCorp. (CCI) and HCP, Inc. (HCP) achieve a Sharpe ratio of 2.326 during\nin-sample period and a Sharpe ratio of 2.425 in out-of-sample test. Crown\nCastle International Corp. (CCI) and Realty Income Corporation (O) achieve a\nSharpe ratio of 2.405 and 2.903 respectively during in-sample period and\nout-of-sample period.\n"
    },
    {
        "paper_id": 1602.05883,
        "authors": "Marco Bardoscia, Stefano Battiston, Fabio Caccioli, Guido Caldarelli",
        "title": "Pathways towards instability in financial networks",
        "comments": "9 pages, 3 figures. Supplementary Material: 12 pages, 5 figures",
        "journal-ref": "Nature Communications 8, 14416 (2017)",
        "doi": "10.1038/ncomms14416",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the financial crisis of 2007-2008, a deep analogy between the\norigins of instability in financial systems and complex ecosystems has been\npointed out: in both cases, topological features of network structures\ninfluence how easily distress can spread within the system. However, in\nfinancial network models, the details of how financial institutions interact\ntypically play a decisive role, and a general understanding of precisely how\nnetwork topology creates instability remains lacking. Here we show how\nprocesses that are widely believed to stabilise the financial system, i.e.\nmarket integration and diversification, can actually drive it towards\ninstability, as they contribute to create cyclical structures which tend to\namplify financial distress, thereby undermining systemic stability and making\nlarge crises more likely. This result holds irrespective of the details of how\ninstitutions interact, showing that policy-relevant analysis of the factors\naffecting financial stability can be carried out while abstracting away from\nsuch details.\n"
    },
    {
        "paper_id": 1602.05998,
        "authors": "Damiano Brigo, Cristin Buescu, Marek Rutkowski",
        "title": "Funding, repo and credit inclusive valuation as modified option pricing",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We take the holistic approach of computing an OTC claim value that\nincorporates credit and funding liquidity risks and their interplays, instead\nof forcing individual price adjustments: CVA, DVA, FVA, KVA. The resulting\nnonlinear mathematical problem features semilinear PDEs and FBSDEs. We show\nthat for the benchmark vulnerable claim there is an analytical solution, and we\nexpress it in terms of the Black-Scholes formula with dividends. This allows\nfor a detailed valuation analysis, stress testing and risk analysis via\nsensitivities.\n"
    },
    {
        "paper_id": 1602.06101,
        "authors": "Thibaut Mastrolia (CEREMADE)",
        "title": "Density analysis of non-Markovian BSDEs and applications to biology and\n  finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide conditions which ensure that stochastic Lipschitz\nBSDEs admit Malliavin differentiable solutions. We investigate the problem of\nexistence of densities for the first components of solutions to general\npath-dependent stochastic Lipschitz BSDEs and obtain results for the second\ncomponents in particular cases. We apply these results to both the study of a\ngene expression model in biology and to the classical pricing problems in\nmathematical finance.\n"
    },
    {
        "paper_id": 1602.06177,
        "authors": "Patrick Cheridito, Michael Kupper and Ludovic Tangpi",
        "title": "Duality formulas for robust pricing and hedging in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive robust super- and subhedging dualities for contingent\nclaims that can depend on several underlying assets. In addition to strict\nsuper- and subhedging, we also consider relaxed versions which, instead of\neliminating the shortfall risk completely, aim to reduce it to an acceptable\nlevel. This yields robust price bounds with tighter spreads. As examples we\nstudy strict super- and subhedging with general convex transaction costs and\ntrading constraints as well as risk-based hedging with respect to robust\nversions of the average value at risk and entropic risk measure. Our approach\nis based on representation results for increasing convex functionals and allows\nfor general financial market structures. As a side result it yields a robust\nversion of the fundamental theorem of asset pricing.\n"
    },
    {
        "paper_id": 1602.06186,
        "authors": "Dirk Paulsen and Jakob S\\\"ohl",
        "title": "Noise Fit, Estimation Error and a Sharpe Information Criterion",
        "comments": "38 pages, 7 figures, 1 table",
        "journal-ref": "Quant. Finance 20(6) (2020) 1027-1043",
        "doi": "10.1080/14697688.2020.1718746",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When the in-sample Sharpe ratio is obtained by optimizing over a\nk-dimensional parameter space, it is a biased estimator for what can be\nexpected on unseen data (out-of-sample). We derive (1) an unbiased estimator\nadjusting for both sources of bias: noise fit and estimation error. We then\nshow (2) how to use the adjusted Sharpe ratio as model selection criterion\nanalogously to the Akaike Information Criterion (AIC). Selecting a model with\nthe highest adjusted Sharpe ratio selects the model with the highest estimated\nout-of-sample Sharpe ratio in the same way as selection by AIC does for the\nlog-likelihood as measure of fit.\n"
    },
    {
        "paper_id": 1602.06188,
        "authors": "Bruce Knuteson",
        "title": "Blunt Honesty, Incentives, and Knowledge Exchange",
        "comments": "3 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple mechanism to facilitate the buying and selling of useful,\nbluntly honest information. The for-profit, arm's length knowledge exchange\nthis mechanism enables may dramatically increase the pace of scientific\nprogress.\n"
    },
    {
        "paper_id": 1602.06189,
        "authors": "Alexey Bakshaev",
        "title": "Accrual valuation and mark to market adjustment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides intuition on the relationship of accrual and\nmark-to-market valuation for cash and forward interest rate trades. Discounted\ncashflow valuation is compared to spread-based valuation for forward trades,\nwhich explains the trader's view on valuation. This is followed by Taylor\nseries approximation for cash trades, uncovering simple intuition behind\naccrual valuation and mark-to-market adjustment. It is followed by the PNL\nexample modelled in R. Within the Taylor approximation framework, theta and\ndelta are explained. The concept of deferral is explained taking Forward Rate\nAgreement (FRA) as an example.\n"
    },
    {
        "paper_id": 1602.06213,
        "authors": "Li-Xin Wang",
        "title": "Modeling Stock Price Dynamics with Fuzzy Opinion Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a mathematical model for the word-of-mouth communications among\nstock investors through social networks and explore how the changes of the\ninvestors' social networks influence the stock price dynamics and vice versa.\nAn investor is modeled as a Gaussian fuzzy set (a fuzzy opinion) with the\ncenter and standard deviation as inputs and the fuzzy set itself as output.\nInvestors are connected in the following fashion: the center input of an\ninvestor is taken as the average of the neighbors' outputs, where two investors\nare neighbors if their fuzzy opinions are close enough to each other, and the\nstandard deviation (uncertainty) input is taken with local, global or external\nreference schemes to model different scenarios of how investors define\nuncertainties. The centers and standard deviations of the fuzzy opinions are\nthe expected prices and their uncertainties, respectively, that are used as\ninputs to the price dynamic equation. We prove that with the local reference\nscheme the investors converge to different groups in finite time, while with\nthe global or external reference schemes all investors converge to a consensus\nwithin finite time and the consensus may change with time in the external\nreference case. We show how to model trend followers, contrarians and\nmanipulators within this mathematical framework and prove that the biggest\nenemy of a manipulator is the other manipulators. We perform Monte Carlo\nsimulations to show how the model parameters influence the price dynamics, and\nwe apply a modified version of the model to the daily closing prices of fifteen\ntop banking and real estate stocks in Hong Kong for the recent two years from\nDec. 5, 2013 to Dec. 4, 2015 and discover that a sharp increase of the combined\nuncertainty is a reliable signal to predict the reversal of the current price\ntrend.\n"
    },
    {
        "paper_id": 1602.06234,
        "authors": "Costas Efthimiou, Adam Wearne",
        "title": "Household Income Distribution in the USA",
        "comments": "to appear in Eur. Phys. J. B",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2016-60670-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we present an alternative model for the distribution of\nhousehold incomes in the United States. We provide arguments from two differing\nperspectives which both yield the proposed income distribution curve, and then\nfit this curve to empirical data on household income distribution obtained from\nthe United States Census Bureau.\n"
    },
    {
        "paper_id": 1602.06295,
        "authors": "C\\'edric Join, Michel Fliess, Cyril Voyant, Fr\\'ed\\'eric Chaxel",
        "title": "Solar energy production: Short-term forecasting and risk management",
        "comments": "8th IFAC Conference on Manufacturing Modelling, Management & Control\n  (Troyes, France, June 2016)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Electricity production via solar energy is tackled via short-term forecasts\nand risk management. Our main tool is a new setting on time series. It allows\nthe definition of \"confidence bands\" where the Gaussian assumption, which is\nnot satisfied by our concrete data, may be abandoned. Those bands are quite\nconvenient and easily implementable. Numerous computer simulations are\npresented.\n"
    },
    {
        "paper_id": 1602.06585,
        "authors": "Tore Opsahl and William Newton",
        "title": "Credit risk and companies' inter-organizational networks: Assessing\n  impact of suppliers and buyers on CDS spreads",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Companies do not operate in a vacuum. As companies move towards an\nincreasingly specialized production function and their reach is becoming truly\nglobal, their aptitude in managing and shaping their inter-organizational\nnetwork is a determining factor in measuring their health. Current models of\ncompany financial health often lack variables explaining the\ninter-organizational network, and as such, assume that (1) all networks are the\nsame and (2) the performance of partners do not impact companies. This paper\naims to be a first step in the direction of removing these assumptions.\nSpecifically, the impact is illustrated by examining the effects of customer\nand supplier concentrations and partners' credit risk on credit-default swap\n(CDS) spreads while controlling for credit risk and size. We rely upon\nsupply-chain data from Bloomberg that provides insight into companies'\nrelationships. The empirical results show that a well diversified customer\nnetwork lowers CDS spread, while having stable partners with low default\nprobabilities increase spreads. The latter result suggests that successful\ncompanies do not focus on building a stable eco-system around themselves, but\ninstead focus on their own profit maximization at the cost of the financial\nhealth of their suppliers' and customers'. At a more general level, the results\nindicate the importance of considering the inter-organizational networks, and\nhighlight the value of including network variables in credit risk models.\n"
    },
    {
        "paper_id": 1602.06685,
        "authors": "Romain Blanchard, Laurence Carassus, Mikl\\'os R\\'asonyi",
        "title": "Non-concave optimal investment and no-arbitrage: a measure theoretical\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider non-concave and non-smooth random utility functions with do- main\nof definition equal to the non-negative half-line. We use a dynamic pro-\ngramming framework together with measurable selection arguments to establish\nboth the no-arbitrage condition characterization and the existence of an\noptimal portfolio in a (generically incomplete) discrete-time financial market\nmodel with finite time horizon. In contrast to the existing literature, we\npropose to consider a probability space which is not necessarily complete.\n"
    },
    {
        "paper_id": 1602.06765,
        "authors": "Giorgio Ferrari, Shuzhen Yang",
        "title": "On an Optimal Extraction Problem with Regime Switching",
        "comments": "34 pages, 2 figures. Thorough revision of the manuscript, added new\n  results, and removed assumptions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a finite-fuel two-dimensional degenerate singular\nstochastic control problem under regime switching that is motivated by the\noptimal irreversible extraction problem of an exhaustible commodity. A company\nextracts a natural resource from a reserve with finite capacity, and sells it\nin the market at a spot price that evolves according to a Brownian motion with\nvolatility modulated by a two-state Markov chain. In this setting, the company\naims at finding the extraction rule that maximizes its expected discounted cash\nflow, net of the costs of extraction and maintenance of the reserve. We provide\nexpressions both for the value function and for the optimal control. On the one\nhand, if the running cost for the maintenance of the reserve is a convex\nfunction of the reserve level, the optimal extraction rule prescribes a\nSkorokhod reflection of the (optimally) controlled state process at a certain\nstate and price dependent threshold. On the other hand, in presence of a\nconcave running cost function it is optimal to instantaneously deplete the\nreserve at the time at which the commodity's price exceeds an endogenously\ndetermined critical level. In both cases, the threshold triggering the optimal\ncontrol is given in terms of the optimal stopping boundary of an auxiliary\nfamily of perpetual optimal selling problems with regime switching.\n"
    },
    {
        "paper_id": 1602.06855,
        "authors": "Abner D. Soares (1), Newton J. Moura Jr. (2), Marcelo B. Ribeiro (3\n  and 4) ((1) Comiss\\~ao Nacional de Energia Nuclear - CNEN, Rio de Janeiro,\n  (2) Instituto Brasileiro de Geografia e Estat\\'istica - IBGE, Rio de Janeiro,\n  (3) Instituto de F\\'isica, Universidade Federal do Rio de Janeiro - UFRJ, (4)\n  Observat\\'orio do Valongo, Universidade Federal do Rio de Janeiro - UFRJ)",
        "title": "Tsallis statistics in the income distribution of Brazil",
        "comments": "10 figures, 39 graphs, 17 pages, LaTeX. Minor changes to match\n  version sent to publisher. To appear in \"Chaos, Solitons & Fractals\"",
        "journal-ref": "Chaos, Solitons and Fractals 88 (2016) 158-171",
        "doi": "10.1016/j.chaos.2016.02.026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the empirical evidence of Tsallis statistical functions\nin the personal income distribution of Brazil. Yearly samples from 1978 to 2014\nwere linearized by the q-logarithm and straight lines were fitted to the entire\nrange of the income data in all samples, producing a two-parameters-only single\nfunction representation of the whole distribution in every year. The results\nshowed that the time evolution of the parameters is periodic and plotting one\nin terms of the other reveals a cycle mostly clockwise. It was also found that\nthe empirical data oscillate periodically around the fitted straight lines with\nthe amplitude growing as the income values increase. Since the entire income\ndata range can be fitted by a single function, this raises questions on\nprevious results claiming that the income distribution is constituted by a well\ndefined two-classes-base income structure, since such a division in two very\ndistinct income classes might not be an intrinsic property of societies, but a\nconsequence of an a priori fitting-choice procedure that may leave aside\npossibly important income dynamics at the intermediate levels.\n"
    },
    {
        "paper_id": 1602.06935,
        "authors": "Adri\\'an Carro, Ra\\'ul Toral, Maxi San Miguel",
        "title": "The noisy voter model on complex networks",
        "comments": "28 pages, 9 figures",
        "journal-ref": "Scientific Reports 6, 24775 (2016)",
        "doi": "10.1038/srep24775",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new analytical method to study stochastic, binary-state models\non complex networks. Moving beyond the usual mean-field theories, this\nalternative approach is based on the introduction of an annealed approximation\nfor uncorrelated networks, allowing to deal with the network structure as\nparametric heterogeneity. As an illustration, we study the noisy voter model, a\nmodification of the original voter model including random changes of state. The\nproposed method is able to unfold the dependence of the model not only on the\nmean degree (the mean-field prediction) but also on more complex averages over\nthe degree distribution. In particular, we find that the degree heterogeneity\n---variance of the underlying degree distribution--- has a strong influence on\nthe location of the critical point of a noise-induced, finite-size transition\noccurring in the model, on the local ordering of the system, and on the\nfunctional form of its temporal correlations. Finally, we show how this latter\npoint opens the possibility of inferring the degree heterogeneity of the\nunderlying network by observing only the aggregate behavior of the system as a\nwhole, an issue of interest for systems where only macroscopic, population\nlevel variables can be measured.\n"
    },
    {
        "paper_id": 1602.06943,
        "authors": "A.V. Kavokin, A.S. Sheremet, M.Yu. Petrov",
        "title": "Bunching of numbers in a non-ideal roulette: the key to winning\n  strategies",
        "comments": "7 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Chances of a gambler are always lower than chances of a casino in the case of\nan ideal, mathematically perfect roulette, if the capital of the gambler is\nlimited and the minimum and maximum allowed bets are limited by the casino.\nHowever, a realistic roulette is not ideal: the probabilities of realisation of\ndifferent numbers slightly deviate. Describing this deviation by a statistical\ndistribution with a width {\\delta} we find a critical {\\delta} that equalizes\nchances of gambler and casino in the case of a simple strategy of the game: the\ngambler always puts equal bets to the last N numbers. For up-critical {\\delta}\nthe expected return of the roulette becomes positive. We show that the dramatic\nincrease of gambler's chances is a manifestation of bunching of numbers in a\nnon-ideal roulette. We also estimate the critical starting capital needed to\nensure the low risk game for an indefinite time.\n"
    },
    {
        "paper_id": 1602.06968,
        "authors": "Alberto Bicci",
        "title": "Limit Order Book and its modelling in terms of Gibbs Grand-Canonical\n  Ensemble",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.07.040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the domain of the so called Econophysics some attempts already have been\nmade for applying the theory of Thermodynamics and Statistical Mechanics to\neconomics and financial markets. In this paper a similar approach is made from\na different perspective, trying to model the limit order book and price\nformation process of a given stock by the Grand-Canonical Gibbs Ensemble for\nthe bid and ask processes. As a consequence we can define in a meaningful way\nexpressions for the temperatures of the ensembles of bid orders and of ask\norders, which are a function of maximum bid, minimum ask and closure prices of\nthe stock as well as of the exchanged volume of shares. It is demonstrated that\nthe difference between the ask and bid orders temperatures can be related to\nthe VAO (Volume Accumulation Oscillator) indicator, empirically defined in\nTechnical Analysis of stock markets. Furthermore the distributions for bid and\nask orders derived by the theory can be subject to well defined validations\nagainst real data, giving a falsifiable character to the model.\n"
    },
    {
        "paper_id": 1602.06998,
        "authors": "Jin Hyuk Choi",
        "title": "Optimal consumption and investment with liquid and illiquid assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an optimal consumption/investment problem to maximize expected\nutility from consumption. In this market model, the investor is allowed to\nchoose a portfolio which consists of one bond, one liquid risky asset (no\ntransaction costs) and one illiquid risky asset (proportional transaction\ncosts). We fully characterize the optimal consumption and trading strategies in\nterms of the solution of the free boundary ODE with an integral constraint. We\nfind an explicit characterization of model parameters for the well-posedness of\nthe problem, and show that the problem is well-posed if and only if there\nexists a shadow price process. Finally, we describe how the investor's optimal\nstrategy is affected by the additional opportunity of trading the liquid risky\nasset, compared to the simpler model with one bond and one illiquid risky\nasset.\n"
    },
    {
        "paper_id": 1602.073,
        "authors": "Jo\\~ao Pedro Jerico, Fran\\c{c}ois P. Landes, Matteo Marsili, Isaac\n  P\\'erez Castillo and Valerio Volpati",
        "title": "When does inequality freeze an economy?",
        "comments": "12 pages (plus 9 pages appendixes), 8 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/2016/07/073402",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inequality and its consequences are the subject of intense recent debate.\nUsing a simplified model of the economy, we address the relation between\ninequality and liquidity, the latter understood as the frequency of economic\nexchanges. Assuming a Pareto distribution of wealth for the agents, that is\nconsistent with empirical findings, we find an inverse relation between wealth\ninequality and overall liquidity. We show that an increase in the inequality of\nwealth results in an even sharper concentration of the liquid financial\nresources. This leads to a congestion of the flow of goods and the arrest of\nthe economy when the Pareto exponent reaches one.\n"
    },
    {
        "paper_id": 1602.07452,
        "authors": "Lucia Bellenzier, J{\\o}rgen Vitting Andersen, and Giulia Rotundo",
        "title": "Contagion in the world's stock exchanges seen as a set of coupled\n  oscillators",
        "comments": "38 pages - jpg figures conveeted to pdf in this version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how the phenomenon of contagion can take place in the network of the\nworld's stock exchanges due to the behavioral trait \"blindeness to small\nchanges\". On large scale individual, the delay in the collective response may\nsignificantly change the dynamics of the overall system. We explicitely insert\na term describing the behavioral phenomenon in a system of equations that\ndescribe the build and release of stress across the worldwide stock markets. In\nthe mathematical formulation of the model, each stock exchange acts as an\nintegrate-and-fire oscillator. Calibration on market data validate the model.\n  One advantage of the integrate-and-fire dynamics is that it enables for a\ndirect identification of cause and effect of price movements, without the need\nfor statistical tests such as for example Granger causality tests often used in\nthe identification of causes of contagion. Our methodology can thereby identify\nthe most relevant nodes with respect to onset of contagion in the network of\nstock exchanges, as well as identify potential periods of high vulnerability of\nthe network. The model is characterized by a separation of time scales created\nby a slow build up of stresses, for example due to (say monthly/yearly)\nmacroeconomic factors, and then a fast (say hourly/daily) release of stresses\nthrough \"price-quakes\" of price movements across the worlds network of stock\nexchanges.\n"
    },
    {
        "paper_id": 1602.07599,
        "authors": "Jacopo Corbetta and Ilaria Peri",
        "title": "Backtesting Lambda Value at Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new risk measure, the lambda value at risk (Lambda VaR), has been recently\nproposed from a theoretical point of view as a generalization of the value at\nrisk (VaR). The Lambda VaR appears attractive for its potential ability to\nsolve several problems of the VaR. In this paper we propose three nonparametric\nbacktesting methodologies for the Lambda VaR which exploit different features.\nTwo of these tests directly assess the correctness of the level of coverage\npredicted by the model. One of these tests is bilateral and provides an\nasymptotic result. A third test assess the accuracy of the Lambda VaR that\ndepends on the choice of the P&L distribution. However, this test requires the\nstorage of more information. Finally, we perform a backtesting exercise and we\ncompare our results with the ones from Hitaj and Peri (2015)\n"
    },
    {
        "paper_id": 1602.07628,
        "authors": "Yuval Rabani and Leonard J. Schulman",
        "title": "The Invisible Hand of Laplace: the Role of Market Structure in Price\n  Convergence and Oscillation",
        "comments": null,
        "journal-ref": "J. Math. Econ. 2021, page 102475",
        "doi": "10.1016/j.jmateco.2021.102475",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A fundamental question about a market is under what conditions, and then how\nrapidly, does price signaling cause price equilibration. Qualitatively, this\nought to depend on how well-connected the market is. We address this question\nquantitatively for a certain class of Arrow-Debreu markets with continuous-time\nproportional t\\^{a}tonnement dynamics. We show that the algebraic connectivity\nof the market determines the effectiveness of price signaling equilibration.\nThis also lets us study the rate of external noise that a market can tolerate\nand still maintain near-equilibrium prices.\n"
    },
    {
        "paper_id": 1602.07663,
        "authors": "Marcello Rambaldi, Emmanuel Bacry, Fabrizio Lillo",
        "title": "The role of volume in order book dynamics: a multivariate Hawkes process\n  analysis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2016.1260759",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that multivariate Hawkes processes coupled with the nonparametric\nestimation procedure first proposed in Bacry and Muzy (2015) can be\nsuccessfully used to study complex interactions between the time of arrival of\norders and their size, observed in a limit order book market. We apply this\nmethodology to high-frequency order book data of futures traded at EUREX.\nSpecifically, we demonstrate how this approach is amenable not only to analyze\ninterplay between different order types (market orders, limit orders,\ncancellations) but also to include other relevant quantities, such as the order\nsize, into the analysis, showing also that simple models assuming the\nindependence between volume and time are not suitable to describe the data.\n"
    },
    {
        "paper_id": 1602.0791,
        "authors": "Francesca Biagini, Yinglin Zhang",
        "title": "Polynomial Diffusion Models for Life Insurance Liabilities",
        "comments": null,
        "journal-ref": "Insurance Mathematics and Economics 71 (2016) 114-129",
        "doi": "10.1016/j.insmatheco.2016.08.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the pricing and hedging problem of a portfolio of life\ninsurance products under the benchmark approach, where the reference market is\nmodelled as driven by a state variable following a polynomial diffusion on a\ncompact state space. Such a model guarantees not only the positivity of the OIS\nshort rate and the mortality intensity, but also the possibility of\napproximating both pricing formula and hedging strategy of a large class of\nlife insurance products by explicit formulas.\n"
    },
    {
        "paper_id": 1602.0807,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Statistical Risk Models",
        "comments": "44 pages; a trivial typo corrected, references updated; to appear in\n  The Journal of Investment Strategies. arXiv admin note: text overlap with\n  arXiv:1602.04902, arXiv:1508.04883, arXiv:1604.08743",
        "journal-ref": "The Journal of Investment Strategies 6(2) (2017) 1-40",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give complete algorithms and source code for constructing statistical risk\nmodels, including methods for fixing the number of risk factors. One such\nmethod is based on eRank (effective rank) and yields results similar to (and\nfurther validates) the method set forth in an earlier paper by one of us. We\nalso give a complete algorithm and source code for computing eigenvectors and\neigenvalues of a sample covariance matrix which requires i) no costly\niterations and ii) the number of operations linear in the number of returns.\nThe presentation is intended to be pedagogical and oriented toward practical\napplications.\n"
    },
    {
        "paper_id": 1602.08258,
        "authors": "Vladimir Filimonov, Guilherme Demos and Didier Sornette",
        "title": "Modified Profile Likelihood Inference and Interval Forecast of the Burst\n  of Financial Bubbles",
        "comments": "40 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We present a detailed methodological study of the application of the modified\nprofile likelihood method for the calibration of nonlinear financial models\ncharacterised by a large number of parameters. We apply the general approach to\nthe Log-Periodic Power Law Singularity (LPPLS) model of financial bubbles. This\nmodel is particularly relevant because one of its parameters, the critical time\n$t_c$ signalling the burst of the bubble, is arguably the target of choice for\ndynamical risk management. However, previous calibrations of the LPPLS model\nhave shown that the estimation of $t_c$ is in general quite unstable. Here, we\nprovide a rigorous likelihood inference approach to determine $t_c$, which\ntakes into account the impact of the other nonlinear (so-called \"nuisance\")\nparameters for the correct adjustment of the uncertainty on $t_c$. This\nprovides a rigorous interval estimation for the critical time, rather than a\npoint estimation in previous approaches. As a bonus, the interval estimations\ncan also be obtained for the nuisance parameters ($m,\\omega$, damping), which\ncan be used to improve filtering of the calibration results. We show that the\nuse of the modified profile likelihood method dramatically reduces the number\nof local extrema by constructing much simpler smoother log-likelihood\nlandscapes. The remaining distinct solutions can be interpreted as genuine\nscenarios that unfold as the time of the analysis flows, which can be compared\ndirectly via their likelihood ratio. Finally, we develop a multi-scale profile\nlikelihood analysis to visualize the structure of the financial data at\ndifferent scales (typically from 100 to 750 days). We test the methodology\nsuccessfully on synthetic price time series and on three well-known historical\nfinancial bubbles.\n"
    },
    {
        "paper_id": 1602.0827,
        "authors": "Alessio Emanuele Biondo, Alessandro Pluchino, Andrea Rapisarda",
        "title": "Order Book, Financial Markets and Self-Organized Criticality",
        "comments": "18 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple order book mechanism that regulates an artificial\nfinancial market with self-organized criticality dynamics and fat tails of\nreturns distribution. The model shows the role played by individual imitation\nin determining trading decisions, while fruitfully replicates typical aggregate\nmarket behavior as the \"self-fulfilling prophecy\". We also address the role of\nrandom traders as a possible decentralized solution to dampen market\nfluctuations.\n"
    },
    {
        "paper_id": 1602.08297,
        "authors": "G\\'abor Papp, Fabio Caccioli, Imre Kondor",
        "title": "Bias-variance trade-off in portfolio optimization under Expected\n  Shortfall with $\\ell_2$ regularization",
        "comments": "14 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The optimization of a large random portfolio under the Expected Shortfall\nrisk measure with an $\\ell_2$ regularizer is carried out by analytical\ncalculation. The regularizer reins in the large sample fluctuations and the\nconcomitant divergent estimation error, and eliminates the phase transition\nwhere this error would otherwise blow up. In the data-dominated region, where\nthe number $N$ of different assets in the portfolio is much less than the\nlength $T$ of the available time series, the regularizer plays a negligible\nrole even if its strength $\\eta$ is large, while in the opposite limit, where\nthe size of samples is comparable to, or even smaller than the number of\nassets, the optimum is almost entirely determined by the regularizer. We\nconstruct the contour map of estimation error on the $N/T$ vs. $\\eta$ plane and\nfind that for a given value of the estimation error the gain in $N/T$ due to\nthe regularizer can reach a factor of about 4 for a sufficiently strong\nregularizer.\n"
    },
    {
        "paper_id": 1602.08374,
        "authors": "Alessandro Fiasconaro, Emanuele Strano, Vincenzo Nicosia, Sergio\n  Porta, Vito Latora",
        "title": "Spatio-temporal analysis of micro economic activities in Rome reveals\n  patterns of mixed-use urban evolution",
        "comments": "14 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0151681",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding urban growth is one with understanding how society evolves to\nsatisfy the needs of its individuals in sharing a common space and adapting to\nthe territory. We propose here a quantitative analysis of the historical\ndevelopment of a large urban area by investigating the spatial distribution and\nthe age of commercial activities in the whole city of Rome. We find that the\nage of activities of various categories presents a very interesting double\nexponential trend, with a transition possibly related to the long-term\neconomical effects determined by the oil crisis of the Seventies. The\ndiversification of commercial categories, studied through various measures of\nentropy, shows, among other interesting features, a saturating behaviour with\nthe density of activities. Moreover, different couples of commercial categories\nexhibit over the years a tendency to attract in space. Our results demonstrate\nthat the spatio-temporal distribution of commercial activities can provide\nimportant insights on the urbanisation processes at work, revealing specific\nand not trivial socio-economical dynamics, as the presence of crisis periods\nand expansion trends, and contributing to the characterisation of the maturity\nof urban areas.\n"
    },
    {
        "paper_id": 1602.08429,
        "authors": "D.L.Wilcox",
        "title": "No such thing as a risk-neutral market",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A very brief history of relative valuation in neoclassical finance since 1973\nis presented, with attention to core currency issues for emerging economies.\nPrice formation is considered in the context of hierarchical causality, with\ndiscussion focussed on identifying mathematical modelling challenges for robust\nand transparent regulation of interactions.\n"
    },
    {
        "paper_id": 1602.08442,
        "authors": "Marina Dolfin, Dami\\'an Knopoff, Leone Leonida, Dario Maimone Ansaldo\n  Patti",
        "title": "Escaping the trap of 'blocking': a kinetic model linking economic\n  development and political competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a kinetic model with stochastic game-type\ninteractions, analyzing the relationship between the level of political\ncompetition in a society and the degree of economic liberalization. The above\nissue regards the complex interactions between economy and institutional\npolicies intended to introduce technological innovations in a society, where\ntechnological innovations are intended in a broad sense comprehending reforms\ncritical to production. A special focus is placed on the political replacement\neffect described in a macroscopic model by Acemoglu and Robinson (AR-model,\nhenceforth), which can determine the phenomenon of innovation 'blocking',\npossibly leading to economic backwardness. One of the goals of our modelization\nis to obtain a mesoscopic dynamical model whose macroscopic outputs are\nqualitatively comparable with stylized facts of the AR-model. A set of\nnumerical solutions is presented showing the non monotonous relationship\nbetween economic liberization and political competition, which can be\nconsidered as an emergent phenomenon of the complex socio-economic interaction\ndynamic.\n"
    },
    {
        "paper_id": 1602.08467,
        "authors": "M.L. Bertotti, G. Modanese",
        "title": "Microscopic models for the study of taxpayer audit effects",
        "comments": "18 pages, 1 figure. To appear in Int. J. Mod. Phys. C",
        "journal-ref": "International Journal of Modern Physics C, 27-9 (2016), art. no.\n  1650100",
        "doi": "10.1142/S012918311650100X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A microscopic dynamic model is here constructed and analyzed, describing the\nevolution of the income distribution in the presence of taxation and\nredistribution in a society in which also tax evasion and auditing processes\noccur. The focus is on effects of enforcement regimes, characterized by\ndifferent choices of the audited taxpayer fraction and of the penalties imposed\nto noncompliant individuals. A complex systems perspective is adopted: society\nis considered as a system composed by a large number of heterogeneous\nindividuals. These are divided into income classes and may as well have\ndifferent tax evasion behaviors. The variation in time of the number of\nindividuals in each class is described by a system of nonlinear differential\nequations of the kinetic discretized Boltzmann type involving transition\nprobabilities.\n"
    },
    {
        "paper_id": 1602.08533,
        "authors": "Ricardo T. Fernholz and Robert Fernholz",
        "title": "A Rank-Based Approach to Zipf's Law",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An Atlas model is a rank-based system of continuous semimartingales for which\nthe steady-state values of the processes follow a power law, or Pareto\ndistribution. For a power law, the log-log plot of these steady-state values\nversus rank is a straight line. Zipf's law is a power law for which the slope\nof this line is -1. In this note, rank-based conditions are found under which\nan Atlas model will follow Zipf's law. An advantage of this rank-based approach\nis that it provides information about the dynamics of systems that result in\nZipf's law.\n"
    },
    {
        "paper_id": 1602.08894,
        "authors": "Thibaut Lux, Antonis Papapantoleon",
        "title": "Improved Fr\\'echet$-$Hoeffding bounds on $d$-copulas and applications in\n  model-free finance",
        "comments": "36 pages, 2 figures, final version, forthcoming in Ann. Appl. Probab",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive upper and lower bounds on the expectation of $f(\\mathbf{S})$ under\ndependence uncertainty, i.e. when the marginal distributions of the random\nvector $\\mathbf{S}=(S_1,\\dots,S_d)$ are known but their dependence structure is\npartially unknown. We solve the problem by providing improved \\FH bounds on the\ncopula of $\\mathbf{S}$ that account for additional information. In particular,\nwe derive bounds when the values of the copula are given on a compact subset of\n$[0,1]^d$, the value of a functional of the copula is prescribed or different\ntypes of information are available on the lower dimensional marginals of the\ncopula. We then show that, in contrast to the two-dimensional case, the bounds\nare quasi-copulas but fail to be copulas if $d>2$. Thus, in order to translate\nthe improved \\FH bounds into bounds on the expectation of $f(\\mathbf{S})$, we\ndevelop an alternative representation of multivariate integrals with respect to\ncopulas that admits also quasi-copulas as integrators. By means of this\nrepresentation, we provide an integral characterization of orthant orders on\nthe set of quasi-copulas which relates the improved \\FH bounds to bounds on the\nexpectation of $f(\\mathbf{S})$. Finally, we apply these results to compute\nmodel-free bounds on the prices of multi-asset options that take partial\ninformation on the dependence structure into account, such as correlations or\nmarket prices of other traded derivatives. The numerical results show that the\nadditional information leads to a significant improvement of the option price\nbounds compared to the situation where only the marginal distributions are\nknown.\n"
    },
    {
        "paper_id": 1602.09071,
        "authors": "Pierluigi Gallo, Francesco Randazzo, Ignazio Gallo",
        "title": "Fairs for e-commerce: the benefits of aggregating buyers and sellers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, many new and interesting models of successful online\nbusiness have been developed. Many of these are based on the competition\nbetween users, such as online auctions, where the product price is not fixed\nand tends to rise. Other models, including group-buying, are based on\ncooperation between users, characterized by a dynamic price of the product that\ntends to go down. There is not yet a business model in which both sellers and\nbuyers are grouped in order to negotiate on a specific product or service. The\npresent study investigates a new extension of the group-buying model, called\nfair, which allows aggregation of demand and supply for price optimization, in\na cooperative manner. Additionally, our system also aggregates products and\ndestinations for shipping optimization. We introduced the following new\nrelevant input parameters in order to implement a double-side aggregation: (a)\nprice-quantity curves provided by the seller; (b) waiting time, that is, the\nlonger buyers wait, the greater discount they get; (c) payment time, which\ndetermines if the buyer pays before, during or after receiving the product; (d)\nthe distance between the place where products are available and the place of\nshipment, provided in advance by the buyer or dynamically suggested by the\nsystem. To analyze the proposed model we implemented a system prototype and a\nsimulator that allow to study effects of changing some input parameters. We\nanalyzed the dynamic price model in fairs having one single seller and a\ncombination of selected sellers. The results are very encouraging and motivate\nfurther investigation on this topic.\n"
    },
    {
        "paper_id": 1602.09078,
        "authors": "Ludovic Gouden\\`ege, Andrea Molent and Antonino Zanette",
        "title": "Pricing and Hedging GMWB in the Heston and in the Black-Scholes with\n  Stochastic Interest Rate Models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1509.02686",
        "journal-ref": "Computational Management Science. February 2019, Volume 16, Issue\n  1-2, pp 217-248",
        "doi": "10.1007/s10287-018-0304-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Valuing Guaranteed Minimum Withdrawal Benefit (GMWB) has attracted\nsignificant attention from both the academic field and real world financial\nmarkets. As remarked by Yang and Dai, the Black and Scholes framework seems to\nbe inappropriate for such a long maturity products. Also Chen Vetzal and\nForsyth in showed that the price of these products is very sensitive to\ninterest rate and volatility parameters. We propose here to use a stochastic\nvolatility model (Heston model) and a Black Scholes model with stochastic\ninterest rate (Hull White model). For this purpose we present four numerical\nmethods for pricing GMWB variables annuities: a hybrid tree-finite difference\nmethod and a Hybrid Monte Carlo method, an ADI finite difference scheme, and a\nStandard Monte Carlo method. These methods are used to determine the\nno-arbitrage fee for the most popular versions of the GMWB contract, and to\ncalculate the Greeks used in hedging. Both constant withdrawal, optimal\nsurrender and optimal withdrawal strategies are considered. Numerical results\nare presented which demonstrate the sensitivity of the no-arbitrage fee to\neconomic, contractual and longevity assumptions.\n"
    },
    {
        "paper_id": 1603.00527,
        "authors": "Christa Cuchiero and Claudio Fontana and Alessandro Gnoatto",
        "title": "Affine multiple yield curve models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a general and tractable framework under which all multiple yield\ncurve modeling approaches based on affine processes, be it short rate, Libor\nmarket, or HJM modeling, can be consolidated. We model a numeraire process and\nmultiplicative spreads between Libor rates and simply compounded OIS rates as\nfunctions of an underlying affine process. Besides allowing for ordered spreads\nand an exact fit to the initially observed term structures, this general\nframework leads to tractable valuation formulas for caplets and swaptions and\nembeds all existing multi-curve affine models. The proposed approach also gives\nrise to new developments, such as a short rate type model driven by a Wishart\nprocess, for which we derive a closed-form pricing formula for caplets. The\nempirical performance of two specifications of our framework is illustrated by\ncalibration to market data.\n"
    },
    {
        "paper_id": 1603.00568,
        "authors": "Andr\\'es Riquelme and Marcela Parada",
        "title": "The Value of A Statistical Life in Absence of Panel Data: What can we\n  do?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper I show how reliable estimates of the Value of a Statistical\nLife (VSL) can be obtained using cross sectional data using Garen's\ninstrumental variable (IV) approach. The increase in the range confidence\nintervals due to the IV setup can be reduced by a factor of 3 by using a proxy\nto risk attitude. In order state the \"precision\" of the cross sectional VSL\nestimates I estimate the VSL using Chilean panel data and use them as benchmark\nfor different cross sectional specifications. The use of the proxy eliminates\nneed for using hard-to-find instruments for the job risk level and narrows the\nconfidence intervals for the workers in the Chilean labor market for the year\n2009.\n"
    },
    {
        "paper_id": 1603.00736,
        "authors": "Ron W Nielsen",
        "title": "Puzzling properties of the historical growth rate of income per capita\n  explained",
        "comments": "16 pages, 7 figures, 6,451 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Galor discovered many mysteries of the growth process. He lists them in his\nUnified Growth Theory and wonders how they can be explained. Close inspection\nof his mysteries reveals that they are of his own creation. They do not exist.\nHe created them by his habitually distorted presentation of data. One of his\nself-created mysteries is the mystery of the alleged sudden spurt in the growth\nrate of income per capita. This sudden spurt never happened. In order to\nunderstand the growth rate of income per capita, its mathematical properties\nare now explored and explained. The explanation is illustrated using the\nhistorical world economic growth. Galor also wonders about the sudden spurt in\nthe growth rate of population. We show that this sudden spurt was also created\nby the distorted presentation of data. The mechanism of the historical economic\ngrowth and of the growth of human population is yet to be explained but it\nwould be unproductive to try to explain the non-existing and self-created\nmysteries of the growth process described in the scientifically unacceptable\nUnified Growth Theory. However, the problem is much deeper than just the\nexamination of this theory. Demographic Growth Theory is based on the incorrect\nbut deeply entrenched postulates developed by accretion over many years and now\ngenerally accepted in the economic and demographic research, postulates\nrevolving around the concept of Malthusian stagnation and around a transition\nfrom stagnation to growth. The study presented here and earlier similar\npublications show that these postulates need to be replaced by interpretations\nbased on the mathematical analysis of data and on the correct understanding of\nhyperbolic distributions.\n"
    },
    {
        "paper_id": 1603.00751,
        "authors": "Nikola Milosevic",
        "title": "Equity forecast: Predicting long term stock price movement using machine\n  learning",
        "comments": "9 pages, 3 tables, computational finance, algorithmic finance",
        "journal-ref": "Journal of Economics Library, 3(2), 2016, 288-294",
        "doi": "10.1453/jel.v3i2.750",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Long term investment is one of the major investment strategies. However,\ncalculating intrinsic value of some company and evaluating shares for long term\ninvestment is not easy, since analyst have to care about a large number of\nfinancial indicators and evaluate them in a right manner. So far, little help\nin predicting the direction of the company value over the longer period of time\nhas been provided from the machines. In this paper we present a machine\nlearning aided approach to evaluate the equity's future price over the long\ntime. Our method is able to correctly predict whether some company's value will\nbe 10% higher or not over the period of one year in 76.5% of cases.\n"
    },
    {
        "paper_id": 1603.0085,
        "authors": "Robert E. Kopp, Rachael Shwom, Gernot Wagner, Jiacan Yuan",
        "title": "Tipping elements and climate-economic shocks: Pathways toward integrated\n  assessment",
        "comments": "43 pages, 2 figure, 2 tables. Published in Earth's Future",
        "journal-ref": null,
        "doi": "10.1002/2016EF000362",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The literature on the costs of climate change often draws a link between\nclimatic 'tipping points' and large economic shocks, frequently called\n'catastrophes'. The use of the phrase 'tipping points' in this context can be\nmisleading. In popular and social scientific discourse, 'tipping points'\ninvolve abrupt state changes. For some climatic 'tipping points,' the\ncommitment to a state change may occur abruptly, but the change itself may be\nrate-limited and take centuries or longer to realize. Additionally, the\nconnection between climatic 'tipping points' and economic losses is tenuous,\nthough emerging empirical and process-model-based tools provide pathways for\ninvestigating it. We propose terminology to clarify the distinction between\n'tipping points' in the popular sense, the critical thresholds exhibited by\nclimatic and social 'tipping elements,' and 'economic shocks'. The last may be\nassociated with tipping elements, gradual climate change, or non-climatic\ntriggers. We illustrate our proposed distinctions by surveying the literature\non climatic tipping elements, climatically sensitive social tipping elements,\nand climate-economic shocks, and we propose a research agenda to advance the\nintegrated assessment of all three.\n"
    },
    {
        "paper_id": 1603.00984,
        "authors": "Ravi Kashyap",
        "title": "David vs Goliath (You against the Markets), A Dynamic Programming\n  Approach to Separate the Impact and Timing of Trading Costs",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 545 (May\n  2020), 122848",
        "doi": "10.1016/j.physa.2019.122848",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a fundamentally different stochastic dynamic programming model of\ntrading costs. Built on a strong theoretical foundation, our model provides\ninsights to market participants by splitting the overall move of the security\nprice during the duration of an order into the Market Impact (price move caused\nby their actions) and Market Timing (price move caused by everyone else)\ncomponents. We derive formulations of this model under different laws of motion\nof the security prices, starting with a simple benchmark scenario and extending\nthis to include multiple sources of uncertainty, liquidity constraints due to\nvolume curve shifts and relating trading costs to the spread. We develop a\nnumerical framework that can be used to obtain optimal executions under any law\nof motion of prices and demonstrate the tremendous practical applicability of\nour theoretical methodology including the powerful numerical techniques to\nimplement them. Our decomposition of trading costs into Market Impact and\nMarket Timing allows us to deduce the zero sum game nature of trading costs. It\nholds numerous lessons for dealing with complex systems, wherein reducing the\ncomplexity by splitting the many sources of uncertainty can lead to better\ninsights in the decision process.\n"
    },
    {
        "paper_id": 1603.00987,
        "authors": "Ravi Kashyap",
        "title": "Securities Lending Strategies: Exclusive Valuations and Auction Bids",
        "comments": "Incomplete Early Draft - Soliciting suggestions, omissions and other\n  feedback",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive valuations of a portfolio of financial instruments from a\nsecurities lending perspective, under different assumptions, and show a\nweighting scheme that converges to the true valuation. We illustrate conditions\nunder which our alternative weighting scheme converges faster to the true\nvaluation when compared to the minimum variance weighting. This weighting\nscheme is applicable in any situation where multiple forecasts are made and we\nneed a methodology to combine them. Our valuations can be useful either to\nderive a bidding strategy for an exclusive auction or to design an appropriate\nauction mechanism, depending on which side of the fence a participant sits\n(whether the interest is to procure the rights to use a portfolio for making\nstock loans such as for a lending desk, or, to obtain additional revenue from a\nportfolio such as from the point of view of a long only asset management firm).\nLastly, we run simulations to establish numerical examples for the set of\nvaluations and for various bidding strategies corresponding to different\nauction settings.\n"
    },
    {
        "paper_id": 1603.00991,
        "authors": "Ravi Kashyap",
        "title": "Financial Services, Economic Growth and Well-Being: A Four-Pronged Study",
        "comments": "Contains Supplementary Material (Research Questions, Methodology and\n  Data Sources) in the Appendix",
        "journal-ref": "Indian Journal of Finance, Vol. 9, No. 1 (2015), pp. 9-22",
        "doi": "10.17010//2015/v9i1/71531",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A four-pronged approach to dealing with Social Science Phenomenon is\noutlined. This methodology is applied to Financial Services, Economic Growth\nand Well-Being. The four prongs are like the four directions for an army\ngeneral looking for victory. Just like the four directions, we need to be aware\nthat there is a degree of interconnectedness in the below four prongs.\n-Uncertainty Principle of the Social Sciences -Responsibilities of Fiscal\nJanitors -Need for Smaller Organizations -Redirecting Growth that Generates\nGarbage The importance of gaining a more profound comprehension of welfare and\ndelineating its components into those that result from an increase in goods and\nservices, and hence can be attributed to economic growth, and into those that\nare not related to economic growth but lead to a better quality of life, is\nhighlighted. The reasoning being that economic growth alone is an inadequate\nindicator of well-being. Hand in hand with a better understanding of the\ncharacteristics of welfare, comes the need to consider the metrics we currently\nhave that gauge economic growth and supplement those with measures that capture\nwell-being more holistically.\n"
    },
    {
        "paper_id": 1603.01041,
        "authors": "Gareth W. Peters, Wilson Y. Chen, Richard H. Gerlach",
        "title": "Estimating Quantile Families of Loss Distributions for Non-Life\n  Insurance Modelling via L-moments",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses different classes of loss models in non-life insurance\nsettings. It then overviews the class Tukey transform loss models that have not\nyet been widely considered in non-life insurance modelling, but offer\nopportunities to produce flexible skewness and kurtosis features often required\nin loss modelling. In addition, these loss models admit explicit quantile\nspecifications which make them directly relevant for quantile based risk\nmeasure calculations. We detail various parameterizations and sub-families of\nthe Tukey transform based models, such as the g-and-h, g-and-k and g-and-j\nmodels, including their properties of relevance to loss modelling.\n  One of the challenges with such models is to perform robust estimation for\nthe loss model parameters that will be amenable to practitioners when fitting\nsuch models. In this paper we develop a novel, efficient and robust estimation\nprocedure for estimation of model parameters in this family Tukey transform\nmodels, based on L-moments. It is shown to be more robust and efficient than\ncurrent state of the art methods of estimation for such families of loss models\nand is simple to implement for practical purposes.\n"
    },
    {
        "paper_id": 1603.01103,
        "authors": "Marcel Ausloos, Rosella Castellano and Roy Cerqueti",
        "title": "Regularities and Discrepancies of Credit Default Swaps: a Data Science\n  approach through Benford's Law",
        "comments": "20 pages, 6 tables, 1 figure, Chaos, Solitons and Fractals, 2016",
        "journal-ref": "Chaos, Solitons & Fractals 90 (2016) 8-17",
        "doi": "10.1016/j.chaos.2016.03.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we search whether the Benford's law is applicable to monitor\ndaily changes in sovereign Credit Default Swaps (CDS) quotes, which are\nacknowledged to be complex systems of economic content. This test is of\nparamount importance since the CDS of a country proxy its health and\nprobability to default, being associated to an insurance against the event of\nits default. We fit the Benford's law to the daily changes in sovereign CDS\nspreads for 13 European countries, - both inside and outside the European Union\nand European Monetary Union. Two different tenors for the sovereign CDS\ncontracts are considered: 5 yrs and 10 yrs, - the former being the reference\nand most liquid one. The time period under investigation is 2008-2015 which\nincludes the period of distress caused by the European sovereign debt crisis.\nMoreover, (i) an analysis over relevant sub-periods is carried out, (ii)\nseveral insights are provided also by implementing the tracking of the\nBenford's law over moving windows. The main test for checking the conformance\nto Benford's law is - as usual - the $\\chi^{2}$ test, whose values are\npresented and discussed for all cases. The analysis is further completed by\nelaborations based on Chebyshev's distance and Kullback and Leibler's\ndivergence. The results highlight differences by countries and tenors. In\nparticular, these results suggest that liquidity seems to be associated to\nhigher levels of distortion. Greece - representing a peculiar case - shows a\nvery different path with respect to the other European countries.\n"
    },
    {
        "paper_id": 1603.01231,
        "authors": "Claudiu Albulescu (UPT), Christian Aubin (CRIEF), Daniel Goyeau\n  (CRIEF)",
        "title": "Stock prices, inflation and inflation uncertainty in the U.S.: Testing\n  the long-run relationship considering Dow Jones sector indexes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test for the long-run relationship between stock prices, inflation and its\nuncertainty for different U.S. sector stock indexes, over the period 2002M7 to\n2015M10. For this purpose we use a cointegration analysis with one structural\nbreak to capture the crisis effect, and we assess the inflation uncertainty\nbased on a time-varying unobserved component model. In line with recent\nempirical studies we discover that in the long-run, the inflation and its\nuncertainty negatively impact the stock prices, opposed to the well-known\nFisher effect. In addition we show that for several sector stock indexes the\nnegative effect of inflation and its uncertainty vanishes after the crisis\nsetup. However, in the short-run the results provide evidence in the favor of a\nnegative impact of uncertainty, while the inflation has no significant\ninfluence on stock prices, except for the consumption indexes. The\nconsideration of business cycle effects confirms our findings, which proves\nthat the results are robust, both for the long-and the short-run relationships.\n"
    },
    {
        "paper_id": 1603.01288,
        "authors": "Niushan Gao, Foivos Xanthos",
        "title": "Option spanning beyond $L_p$-models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \\begin{abstract} The aim of this paper is to study the spanning power of\noptions in a static financial market that allows non-integrable assets. Our\nfindings extend and unify the results in [8,9,18] for $L_p$-models. We also\napply the spanning power properties to the pricing problem. In particular, we\nshow that prices on call and put options of a limited liability asset can be\nuniquely extended by arbitrage to all marketed contingent claims written on the\nasset.\n"
    },
    {
        "paper_id": 1603.01308,
        "authors": "Leopoldo Catania",
        "title": "Dynamic Adaptive Mixture Models",
        "comments": "old version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a new class of Dynamic Mixture Models (DAMMs) being\nable to sequentially adapt the mixture components as well as the mixture\ncomposition using information coming from the data. The information driven\nnature of the proposed class of models allows to exactly compute the full\nlikelihood and to avoid computer intensive simulation schemes. An extensive\nMonte Carlo experiment reveals that the new proposed model can accurately\napproximate the more complicated Stochastic Dynamic Mixture Model previously\nintroduced in the literature as well as other kind of models. The properties of\nthe new proposed class of models are discussed through the paper and an\napplication in financial econometrics is reported.\n"
    },
    {
        "paper_id": 1603.01341,
        "authors": "Ravi Kashyap",
        "title": "Hong Kong -- Shanghai Connect / Hong Kong -- Beijing Disconnect (?):\n  Scaling the Great Wall of Chinese Securities Trading Costs",
        "comments": "arXiv admin note: text overlap with arXiv:1603.00984,\n  arXiv:1602.00839",
        "journal-ref": "Journal of Trading, Vol. 11, No. 3 (Summer 2016), pp. 81-134",
        "doi": "10.3905/jot.2016.11.3.081",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We utilize a fundamentally different model of trading costs to look at the\neffect of the opening of the Hong Kong Shanghai Connect that links the stock\nexchanges in the two cities, arguably the biggest event in international\nbusiness and finance since Christopher Columbus set sail for India. We design a\nnovel methodology that compensates for the lack of data on trading costs in\nChina. We estimate trading costs across similar positions on the dual listed\nset of securities in Hong Kong and China, hoping to provide useful pieces of\ninformation to help scale 'The Great Wall of Chinese Securities Trading Costs'.\nWe then compare actual and estimated trading costs on a sample of real orders\nacross the Hong Kong securities in the dual listed pair to establish the\naccuracy of our measurements. The primary question we seek to address is 'Which\nmarket would be better to trade to gain exposure to the same (or similar) set\nof securities or sectors?' We find that trading costs on Shanghai, which might\nhave been lower than Hong Kong, might have become higher leading up to the\nConnect. What remains to be seen is whether this increase in trading costs is a\ntemporary equilibrium due to the frenzy to gain exposure to Chinese securities\nor whether this phenomenon will persist once the two markets start becoming\nmore and more tightly coupled. It would be interesting to see if this\npioneering policy will lead to securities exchanges across the globe linking up\none another, creating a trade anything, anywhere and anytime marketplace.\nLooking beyond mere trading costs, such studies can be used to gather some\nevidence on what effect the mode of governance and other aspects of life in one\ncountry have on another country, once they start joining up their financial\nmarkets.\n"
    },
    {
        "paper_id": 1603.01397,
        "authors": "Sunil Kumar",
        "title": "Latent class analyisis for reliable measure of inflation expectation in\n  the indian public",
        "comments": "16 pages, 04 Tables and 03 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main aim of this paper is to inspect the properties of survey based on\nhouseholds inflation expectations, conducted by Reserve Bank of India. It is\ntheorized that the respondents answers are exaggerated by extreme response\nbias. Latent class analysis has been hailed as a promising technique for\nstudying measurement errors in surveys, because the model produces estimates of\nthe error rates associated with a given question of the questionnaire. I have\nidentified a model with optimum performance and hence categorize the objective\nas well as reliable classifiers or otherwise.\n"
    },
    {
        "paper_id": 1603.01416,
        "authors": "Atif Ansar, Bent Flyvbjerg, Alexander Budzier, and Daniel Lunn",
        "title": "Big is Fragile: An Attempt at Theorizing Scale",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we characterise the propensity of big capital investments to\nsystematically deliver poor outcomes as \"fragility,\" a notion suggested by\nNassim Taleb. A thing or system that is easily harmed by randomness is fragile.\nWe argue that, contrary to their appearance, big capital investments break\neasily - i.e. deliver negative net present value - due to various sources of\nuncertainty that impact them during their long gestation, implementation, and\noperation periods. We do not refute the existence of economies of scale and\nscope. Instead we argue that big capital investments have a disproportionate\n(non-linear) exposure to uncertainties that deliver poor or negative returns\nabove and beyond their economies of scale and scope. We further argue that to\nsucceed, leaders of capital projects need to carefully consider where scaling\npays off and where it does not. To automatically assume that \"bigger is\nbetter,\" which is common in megaproject management, is a recipe for failure.\n"
    },
    {
        "paper_id": 1603.0158,
        "authors": "Shanshan Wang, Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Cross-response in correlated financial markets: individual stocks",
        "comments": "stock pairs, unaveraged, see arXiv:1510.03205",
        "journal-ref": "Eur. Phys. J. B (2016) 89: 105",
        "doi": "10.1140/epjb/e2016-60818-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous studies of the stock price response to trades focused on the\ndynamics of single stocks, i.e. they addressed the self-response. We\nempirically investigate the price response of one stock to the trades of other\nstocks in a correlated market, i.e. the cross-responses. How large is the\nimpact of one stock on others and vice versa? -- This impact of trades on the\nprice change across stocks appears to be transient instead of permanent as we\ndiscuss from the viewpoint of market efficiency. Furthermore, we compare the\nself-responses on different scales and the self- and cross-responses on the\nsame scale. We also find that the cross-correlation of the trade signs turns\nout to be a short-memory process.\n"
    },
    {
        "paper_id": 1603.01586,
        "authors": "Shanshan Wang, Rudi Sch\\\"afer and Thomas Guhr",
        "title": "Average cross-responses in correlated financial market",
        "comments": "stock pairs, averaged, see arXiv:1510.03205",
        "journal-ref": "Eur. Phys. J. B (2016) 89: 207",
        "doi": "10.1140/epjb/e2016-70137-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are non-vanishing price responses across different stocks in correlated\nfinancial markets. We further study this issue by performing different\naverages, which identify active and passive cross-responses. The two average\ncross-responses show different characteristic dependences on the time lag. The\npassive cross-response exhibits a shorter response period with sizeable\nvolatilities, while the corresponding period for the active cross-response is\nlonger. The average cross-responses for a given stock are evaluated either with\nrespect to the whole market or to different sectors. Using the response\nstrength, the influences of individual stocks are identified and discussed.\nMoreover, the various cross-responses as well as the average cross-responses\nare compared with the self-responses. In contrast, the short memory of trade\nsign cross-correlation for stock pairs, the sign cross-correlation has long\nmemory when averaged over different pairs of stocks.\n"
    },
    {
        "paper_id": 1603.01685,
        "authors": "Ron W Nielsen",
        "title": "Mathematical analysis of historical income per capita distributions",
        "comments": "20 pages, 8 figures, 1 table, 8331 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data describing historical growth of income per capita [Gross Domestic\nProduct per capita (GDP/cap)] for the world economic growth and for the growth\nin Western Europe, Eastern Europe, Asia, former USSR, Africa and Latin America\nare analysed. They follow closely the linearly-modulated hyperbolic\ndistributions represented by the ratios of hyperbolic distributions obtained by\nfitting the GDP and population data. Results of this analysis demonstrate that\nincome per capita was increasing monotonically. There was no stagnation and\nthere were no transitions from stagnation to growth. The usually postulated\ndramatic escapes from the Malthusian trap never happened because there was no\ntrap. Unified Growth Theory is fundamentally incorrect because its central\npostulates are contradicted repeatedly by data, which were used but never\nanalysed during the formulation of this theory. The large body of\nreadily-available data opens new avenues for the economic and demographic\nresearch. They show that certain fundamental postulates revolving around the\nconcept of Malthusian stagnation need to be replaced by the evidence-based\ninterpretations. Within the range of analysable data, which for the growth of\npopulation extends down to 10,000 BC, growth of human population and economic\ngrowth were hyperbolic. There was no Malthusian stagnation and there were no\ntransitions to distinctly faster trajectories. Industrial Revolution had no\nimpact on changing growth trajectories.\n"
    },
    {
        "paper_id": 1603.01865,
        "authors": "Soumik Pal",
        "title": "Exponentially concave functions and high dimensional stochastic\n  portfolio theory",
        "comments": "31 pages, 5 figures, minor editing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the following problem in stochastic portfolio theory. Are there\nportfolios that are relative arbitrages with respect to the market portfolio\nover very short periods of time under realistic assumptions? We answer a\nslightly relaxed question affirmative in the following high dimensional sense,\nwhere dimension refers to the number of stocks being traded. Very roughly,\nsuppose that for every dimension we have a continuous semimartingale market\nsuch that (i) the vector of market weights in decreasing order has a stationary\nregularly varying tail with an index between $-1$ and $-1/2$ and (ii) zero is\nnot a limit point of the relative volatilities of the stocks. Then, given a\nprobability $\\eta < 1$ arbitrarily close to one, two arbitrarily small\n$\\epsilon, \\delta >0$, and an arbitrarily high positive amount $M$, for all\nhigh enough dimensions, it is possible to construct a functionally generated\nportfolio such that, with probability at least $\\eta$, its relative value with\nrespect to the market at time $\\delta$ is at least $M$, and never goes below\n$(1-\\epsilon)$ during $[0, \\delta]$. There are two phase transitions; if the\nindex of the tail is less than $-1$ or larger than $-1/2$. The construction\nuses properties of regular variation, high-dimensional convex geometry and\nconcentration of measure under Dirichlet distributions. We crucially use the\nnotion of $(K,N)$ convex functions introduced by Erbar, Kuwada, Sturm in the\ncontext of curvature-dimension conditions and Bochner's inequalities.\n"
    },
    {
        "paper_id": 1603.02354,
        "authors": "Hannah Cheng, Juan Zhan, William Rea, Alethea Rea",
        "title": "Stock Selection as a Problem in Phylogenetics -- Evidence from the ASX",
        "comments": "33 page, 13 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:1511.07945",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report the results of fifteen sets of portfolio selection simulations\nusing stocks in the ASX200 index for the period May 2000 to December 2013. We\ninvestigated five portfolio selection methods, randomly and from within\nindustrial groups, and three based on neighbor-Net phylogenetic networks. We\nreport that using random, industrial groups, or neighbor-Net phylogenetic\nnetworks alone rarely produced statistically significant reduction in risk,\nthough in four out of the five cases in which it did so, the portfolios\nselected using the phylogenetic networks had the lowest risk. However, we\nreport that when using the neighbor-Net phylogenetic networks in combination\nwith industry group selection that substantial reductions in portfolio return\nspread were achieved.\n"
    },
    {
        "paper_id": 1603.02438,
        "authors": "Gopal K. Basak, Pranab Kumar Das and Allena Rohit",
        "title": "A Mathematical Model of Foreign Capital Inflow",
        "comments": "32 pages, 4 sets of figures each set consists of capital inflow,\n  interest rate and exchange rate",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper models foreign capital inflow from the developed to the developing\ncountries in a stochastic dynamic programming (SDP) framework. Under some\nregularity conditions, the existence of the solutions to the SDP problem is\nproved and they are then obtained by numerical technique because of the\nnon-linearity of the related functions. A number of comparative dynamic\nanalyses explore the impact of parameters of the model on dynamic paths of\ncapital inflow, interest rate in the international loan market and the exchange\nrate.\n"
    },
    {
        "paper_id": 1603.02615,
        "authors": "Marcin Pitera and Thorsten Schmidt",
        "title": "Unbiased estimation of risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The estimation of risk measures recently gained a lot of attention, partly\nbecause of the backtesting issues of expected shortfall related to\nelicitability. In this work we shed a new and fundamental light on optimal\nestimation procedures of risk measures in terms of bias. We show that once the\nparameters of a model need to be estimated, one has to take additional care\nwhen estimating risks. The typical plug-in approach, for example, introduces a\nbias which leads to a systematic underestimation of risk. In this regard, we\nintroduce a novel notion of unbiasedness to the estimation of risk which is\nmotivated by economic principles. In general, the proposed concept does not\ncoincide with the well-known statistical notion of unbiasedness. We show that\nan appropriate bias correction is available for many well-known estimators. In\nparticular, we consider value-at-risk and expected shortfall (tail\nvalue-at-risk). In the special case of normal distributions, closed-formed\nsolutions for unbiased estimators can be obtained. We present a number of\nmotivating examples which show the outperformance of unbiased estimators in\nmany circumstances. The unbiasedness has a direct impact on backtesting and\ntherefore adds a further viewpoint to established statistical properties.\n"
    },
    {
        "paper_id": 1603.02867,
        "authors": "Teemu Pennanen and Ari-Pekka Perkki\\\"o",
        "title": "Convex duality in optimal investment and contingent claim valuation in\n  illiquid markets",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies convex duality in optimal investment and contingent claim\nvaluation in markets where traded assets may be subject to nonlinear trading\ncosts and portfolio constraints. Under fairly general conditions, the dual\nexpressions decompose into tree terms, corresponding to the agent's risk\npreferences, trading costs and portfolio constraints, respectively. The dual\nrepresentations are shown to be valid when the market model satisfies an\nappropriate generalization of the no-arbitrage condition and the agent's\nutility function satisfies an appropriate generalization of asymptotic\nelasticity conditions. When applied to classical liquid market models or models\nwith bid-ask spreads, we recover well-known pricing formulas in terms of\nmartingale measures and consistent price systems. Building on the general\ntheory of convex stochastic optimization, we also derive optimality conditions\nin terms of an extended notion of a \"shadow price\".\n"
    },
    {
        "paper_id": 1603.02874,
        "authors": "Aurelio F. Bariviera, M. Belen Guercio, Lisana B. Martinez, Osvaldo A.\n  Rosso",
        "title": "Libor at crossroads: stochastic switching detection using information\n  theory quantifiers",
        "comments": "17 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1508.04748, arXiv:1509.00217",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2016.02.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the 28 time series of Libor rates, classified in seven\nmaturities and four currencies), during the last 14 years. The analysis was\nperformed using a novel technique in financial economics: the\nComplexity-Entropy Causality Plane. This planar representation allows the\ndiscrimination of different stochastic and chaotic regimes. Using a temporal\nanalysis based on moving windows, this paper unveals an abnormal movement of\nLibor time series arround the period of the 2007 financial crisis. This\nalteration in the stochastic dynamics of Libor is contemporary of what press\ncalled \"Libor scandal\", i.e. the manipulation of interest rates carried out by\nseveral prime banks. We argue that our methodology is suitable as a market\nwatch mechanism, as it makes visible the temporal redution in informational\nefficiency of the market.\n"
    },
    {
        "paper_id": 1603.02896,
        "authors": "Martin Forde, Hongzhong Zhang",
        "title": "Small-time asymptotics for basket options -- the bi-variate SABR model\n  and the hyperbolic heat kernel on $\\mathbb{H}^3$",
        "comments": "24 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1137/15M1029795",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compute a sharp small-time estimate for the price of a basket call under a\nbi-variate SABR model with both $\\beta$ parameters equal to $1$ and three\ncorrelation parameters, which extends the work of Bayer,Friz&Laurence [BFL14]\nfor the multivariate Black-Scholes flat vol model. The result follows from the\nheat kernel on hyperbolic space for $n=3$ combined with the Bellaiche [Bel81]\nheat kernel expansion and Laplace's method, and we give numerical results which\ncorroborate our asymptotic formulae. Similar to the Black-Scholes case, we find\nthat there is a phase transition from one \"most-likely\" path to two most-likely\npaths beyond some critical $K^*$.\n"
    },
    {
        "paper_id": 1603.02902,
        "authors": "Feng-Hui Yu, Wai-Ki Ching, Jia-Wen Gu and Tak-Kuen Siu",
        "title": "Interacting Default Intensity with Hidden Markov Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a reduced-form intensity-based credit risk model\nwith a hidden Markov state process. A filtering method is proposed for\nextracting the underlying state given the observation processes. The method may\nbe applied to a wide range of problems. Based on this model, we derive the\njoint distribution of multiple default times without imposing stringent\nassumptions on the form of default intensities. Closed-form formulas for the\ndistribution of default times are obtained which are then applied to solve a\nnumber of practical problems such as hedging and pricing credit derivatives.\nThe method and numerical algorithms presented may be applicable to various\nforms of default intensities.\n"
    },
    {
        "paper_id": 1603.03012,
        "authors": "Claudio Albanese, Simone Caenazzo (LaMME), St\\'ephane Cr\\'epey (LaMME)",
        "title": "Capital Valuation Adjustment and Funding Valuation Adjustment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the aftermath of the 2007 global financial crisis, banks started\nreflecting into derivative pricing the cost of capital and collateral funding\nthrough XVA metrics. Here XVA is a catch-all acronym whereby X is replaced by a\nletter such as C for credit, D for debt, F for funding, K for capital and so\non, and VA stands for valuation adjustment. This behaviour is at odds with\neconomies where markets for contingent claims are complete, whereby trades\nclear at fair valuations and the costs for capital and collateral are both\nirrelevant to investment decisions. In this paper, we set forth a mathematical\nformalism for derivative portfolio management in incomplete markets for banks.\nA particular emphasis is given to the problem of finding optimal strategies for\nretained earnings which ensure a sustainable dividend policy.\n"
    },
    {
        "paper_id": 1603.03198,
        "authors": "Claudio Fontana and Thorsten Schmidt",
        "title": "General dynamic term structures under default risk",
        "comments": "minor changes, 31 pages. To appear in Stochastic Processes and their\n  Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of modelling the term structure of defaultable bonds,\nunder minimal assumptions on the default time. In particular, we do not assume\nthe existence of a default intensity and we therefore allow for the possibility\nof default at predictable times. It turns out that this requires the\nintroduction of an additional term in the forward rate approach by Heath,\nJarrow and Morton (1992). This term is driven by a random measure encoding\ninformation about those times where default can happen with positive\nprobability. In this framework, we derive necessary and sufficient conditions\nfor a reference probability measure to be a local martingale measure for the\nlarge financial market of credit risky bonds, also considering general recovery\nschemes.\n"
    },
    {
        "paper_id": 1603.03458,
        "authors": "Leonardo dos Santos Pinheiro and Flavio Codeco Coelho",
        "title": "Financial contagion in investment funds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many new models for measuring financial contagion have been presented\nrecently. While these models have not been specified for investment funds\ndirectly, there are many similarities that could be explored to extend the\nmodels. In this work we explore ideas developed about financial contagion to\ncreate a network of investment funds using both cross-holding of quotas and a\nbipartite network of funds and assets. Using data from the Brazilian asset\nmanagement market we analyze not only the contagion pattern but also the\nstructure of this network and how this model can be used to assess the\nstability of the market.\n"
    },
    {
        "paper_id": 1603.03538,
        "authors": "Jean-Pierre Fouque and Ruimeng Hu",
        "title": "Asymptotic Optimal Strategy for Portfolio Optimization in a Slowly\n  Varying Stochastic Environment",
        "comments": "39pages, 3figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the portfolio optimization problem with general\nutility functions and when the return and volatility of underlying asset are\nslowly varying. An asymptotic optimal strategy is provided within a specific\nclass of admissible controls under this problem setup. Specifically, we first\nestablish a rigorous first order approximation of the value function associated\nto a fixed zeroth order suboptimal trading strategy, which is given by the\nheuristic argument in [J.-P. Fouque, R. Sircar and T. Zariphopoulou, {\\it\nMathematical Finance}, 2016]. Then, we show that this zeroth order suboptimal\nstrategy is asymptotically optimal in a specific family of admissible trading\nstrategies. Finally, we show that our assumptions are satisfied by a particular\nfully solvable model.\n"
    },
    {
        "paper_id": 1603.03747,
        "authors": "Ale\\v{s} \\v{C}ern\\'y",
        "title": "Discrete-Time Quadratic Hedging of Barrier Options in Exponential\n  L\\'{e}vy Model",
        "comments": "19 pages",
        "journal-ref": "2016, in J. Kallsen and A. Papapantoleon (eds.), Advanced Modeling\n  in Mathematical Finance, 257-275, Springer",
        "doi": "10.1007/978-3-319-45875-5_12",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine optimal quadratic hedging of barrier options in a discretely\nsampled exponential L\\'{e}vy model that has been realistically calibrated to\nreflect the leptokurtic nature of equity returns. Our main finding is that the\nimpact of hedging errors on prices is several times higher than the impact of\nother pricing biases studied in the literature.\n"
    },
    {
        "paper_id": 1603.03874,
        "authors": "Daniel Sevcovic and Magdalena Zitnanska",
        "title": "Analysis of the nonlinear option pricing model under variable\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyze a nonlinear Black--Scholes model for option pricing\nunder variable transaction costs. The diffusion coefficient of the nonlinear\nparabolic equation for the price $V$ is assumed to be a function of the\nunderlying asset price and the Gamma of the option. We show that the\ngeneralizations of the classical Black--Scholes model can be analyzed by means\nof transformation of the fully nonlinear parabolic equation into a quasilinear\nparabolic equation for the second derivative of the option price. We show\nexistence of a classical smooth solution and prove useful bounds on the option\nprices. Furthermore, we construct an effective numerical scheme for\napproximation of the solution. The solutions are obtained by means of the\nefficient numerical discretization scheme of the Gamma equation. Several\ncomputational examples are presented.\n"
    },
    {
        "paper_id": 1603.04017,
        "authors": "Gautier Marti, S\\'ebastien Andler, Frank Nielsen, Philippe Donnat",
        "title": "Clustering Financial Time Series: How Long is Enough?",
        "comments": "Accepted at IJCAI 2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Researchers have used from 30 days to several years of daily returns as\nsource data for clustering financial time series based on their correlations.\nThis paper sets up a statistical framework to study the validity of such\npractices. We first show that clustering correlated random variables from their\nobserved values is statistically consistent. Then, we also give a first\nempirical answer to the much debated question: How long should the time series\nbe? If too short, the clusters found can be spurious; if too long, dynamics can\nbe smoothed out.\n"
    },
    {
        "paper_id": 1603.04099,
        "authors": "Seyyed Mostafa Mousavi, Robert Mackay, Alistair Tucker",
        "title": "Contagion and Stability in Financial Networks",
        "comments": "Wealth IJMBF, Volume 3, Issue 1, Jan-June 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates two mechanisms of financial contagion that are,\nfirstly, the correlated exposure of banks to the same source of risk, and\nsecondly the direct exposure of banks in the interbank market. It will consider\na random network of banks which are connected through the inter-bank market and\nwill discuss the desirable level of banks exposure to the same sources of risk,\nthat is investment in similar portfolios, for different levels of network\nconnectivity when peering through the lens of the systemic cost incurred to the\neconomy from the banks simultaneous failure. It demonstrates that for all\nlevels of network connectivity, certain levels of diversifying individual banks\ndiversifications are not optimum under any condition. So, given an acceptable\nlevel of systemic cost, the regulator could let banks decrease their capital\nbuffers by moving away from the non-optimum area.\n"
    },
    {
        "paper_id": 1603.04364,
        "authors": "Jo\\\"el Bun, Jean-Philippe Bouchaud, Marc Potters",
        "title": "On the overlaps between eigenvectors of correlated random matrices",
        "comments": "12 pages, 5 figures",
        "journal-ref": "Phys. Rev. E 98, 052145 (2018)",
        "doi": "10.1103/PhysRevE.98.052145",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain general, exact formulas for the overlaps between the eigenvectors\nof large correlated random matrices, with additive or multiplicative noise.\nThese results have potential applications in many different contexts, from\nquantum thermalisation to high dimensional statistics. We find that the\noverlaps only depend on measurable quantities, and do not require the knowledge\nof the underlying \"true\" (noiseless) matrices. We apply our results to the case\nof empirical correlation matrices, that allow us to estimate reliably the width\nof the spectrum of the true correlation matrix, even when the latter is very\nclose to the identity. We illustrate our results on the example of stock\nreturns correlations, that clearly reveal a non trivial structure for the bulk\neigenvalues. We also apply our results to the problem of matrix denoising in\nhigh dimension.\n"
    },
    {
        "paper_id": 1603.05142,
        "authors": "Pawe{\\l} Smaga, Mateusz Wili\\'nski, Piotr Ochnicki, Piotr Arendarski\n  and Tomasz Gubiec",
        "title": "Can banks default overnight? Modeling endogenous contagion on O/N\n  interbank market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model of the liquidity driven banking system focusing on\novernight interbank loans. This significant branch of the interbank market is\ncommonly neglected in the banking system modeling and systemic risk analysis.\nWe construct a model where banks are allowed to use both the interbank and the\nsecurities markets to manage their liquidity demand and supply as driven by\nprudential requirements in a volatile environment. The network of interbank\nloans is dynamic and simulated every day. We show how only the intrasystem cash\nfluctuations, without any external shocks, may lead to systemic defaults, what\nmay be a symptom of the self-organized criticality of the system. We also\nanalyze the impact of different prudential regulations and market conditions on\nthe interbank market resilience. We confirm that central bank's asset purchase\nprograms, limiting the declines in government bond prices, can successfully\nstabilize bank's liquidity demand. The model can be used to analyze the\ninterbank market impact of macroprudential tools.\n"
    },
    {
        "paper_id": 1603.05181,
        "authors": "Kyu-Min Lee and Kwang-Il Goh",
        "title": "Strength of weak layers in cascading failures on multiplex networks:\n  case of the international trade network",
        "comments": "10 pages, 5 figures",
        "journal-ref": "Scientific Reports 6, 26346 (2016)",
        "doi": "10.1038/srep26346",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many real-world complex systems across natural, social, and economical\ndomains consist of manifold layers to form multiplex networks. The multiple\nnetwork layers give rise to nonlinear effect for the emergent dynamics of\nsystems. Especially, weak layers that can potentially play significant role in\namplifying the vulnerability of multiplex networks might be shadowed in the\naggregated single-layer network framework which indiscriminately accumulates\nall layers. Here we present a simple model of cascading failure on multiplex\nnetworks of weight-heterogeneous layers. By simulating the model on the\nmultiplex network of international trades, we found that the multiplex model\nproduces more catastrophic cascading failures which are the result of emergent\ncollective effect of coupling layers, rather than the simple sum thereof.\nTherefore risks can be systematically underestimated in single-layer network\nanalyses because the impact of weak layers can be overlooked. We anticipate\nthat our simple theoretical study can contribute to further investigation and\ndesign of optimal risk-averse real-world complex systems.\n"
    },
    {
        "paper_id": 1603.05294,
        "authors": "Ekaterina Sorokina",
        "title": "Modeling and Estimation of the Risk When Choosing a Provider",
        "comments": "4 pages, 1 figures, 1 table, 14 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper provides an algorithm for the risk estimation when a company\nselects an outsourcing service provider for innovation product. Calculations\nare based on expert surveys conducted among customers and among providers of\noutsourcing. The surveys assessed the degree of materiality of species at risk.\n"
    },
    {
        "paper_id": 1603.05313,
        "authors": "Vladislav Gennadievich Malyshkin and Ray Bakhramov",
        "title": "Market Dynamics vs. Statistics: Limit Order Book Example",
        "comments": "Grammar fixes. Best price level orders execution pattern\n  clarification (10% execution, 90% cancellation)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Commonly used limit order book attributes are empirically considered based on\nNASDAQ ITCH data. It is shown that some of them have the properties drastically\ndifferent from the ones assumed in many market dynamics study. Because of this\ndifference we propose to make a transition from \"Statistical\" type of order\nbook study (typical for academics) to \"Dynamical\" type of study (typical for\nmarket practitioners). Based on market data analysis we conclude, that most of\nmarket dynamics information is contained in attributes with spikes (e.g.\nexecuted trades flow $I=dv/dt$), there is no any \"stationary case\" on the\nmarket and typical market dynamics is a \"fast excitation and then slow\nrelaxation\" type of behavior with a wide distribution of excitation frequencies\nand relaxation times. A computer code, providing full depth order book\ninformation and recently executed trades is available from authors [1].\n"
    },
    {
        "paper_id": 1603.05373,
        "authors": "Chuancun Yin, Dan Zhu",
        "title": "Sharp convex bounds on the aggregate sums--An alternative proof",
        "comments": "11pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that a random vector with given marginal distributions is\ncomonotonic if and only if it has the largest sum with respect to the convex\norder [ Kaas, Dhaene, Vyncke, Goovaerts, Denuit (2002), A simple geometric\nproof that comonotonic risks have the convex-largest sum, ASTIN Bulletin 32,\n71-80. Cheung (2010), Characterizing a comonotonic random vector by the\ndistribution of the sum of its components, Insurance: Mathematics and Economics\n47(2), 130-136] and that a random vector with given marginal distributions is\nmutually exclusive if and only if it has the minimal convex sum [Cheung and Lo\n(2014), Characterizing mutual exclusivity as the strongest negative\nmultivariate dependence structure, Insurance: Mathematics and Economics 55,\n180-190]. In this note, we give a new proof of this two results using the\ntheories of distortion risk measure and expected utility.\n"
    },
    {
        "paper_id": 1603.05513,
        "authors": "Claudio Altafini",
        "title": "The geometric phase of stock trading",
        "comments": "15 pages, 12 figures",
        "journal-ref": "PLoS ONE, vol 11, e0161538, 2016",
        "doi": "10.1371/journal.pone.0161538",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric phases describe how in a continuous-time dynamical system the\ndisplacement of a variable (called phase variable) can be related to other\nvariables (shape variables) undergoing a cyclic motion, according to an area\nrule. The aim of this paper is to show that geometric phases can exist also for\ndiscrete-time systems, and even when the cycles in shape space have zero area.\nA context in which this principle can be applied is stock trading. A zero-area\ncycle in shape space represents the type of trading operations normally carried\nout by high-frequency traders (entering and exiting a position on a fast\ntime-scale), while the phase variable represents the cash balance of a trader.\nUnder the assumption that trading impacts stock prices, even zero-area cyclic\ntrading operations can induce geometric phases, i.e., profits or losses,\nwithout affecting the stock quote.\n"
    },
    {
        "paper_id": 1603.0567,
        "authors": "Samuel R\\\"onnqvist, Peter Sarlin",
        "title": "Bank distress in the news: Describing events through deep learning",
        "comments": "Forthcoming in Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:1507.07870 [in version 1]",
        "journal-ref": "Neurocomputing, 264, 2017",
        "doi": "10.1016/j.neucom.2016.12.11",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While many models are purposed for detecting the occurrence of significant\nevents in financial systems, the task of providing qualitative detail on the\ndevelopments is not usually as well automated. We present a deep learning\napproach for detecting relevant discussion in text and extracting natural\nlanguage descriptions of events. Supervised by only a small set of event\ninformation, comprising entity names and dates, the model is leveraged by\nunsupervised learning of semantic vector representations on extensive text\ndata. We demonstrate applicability to the study of financial risk based on news\n(6.6M articles), particularly bank distress and government interventions (243\nevents), where indices can signal the level of bank-stress-related reporting at\nthe entity level, or aggregated at national or European level, while being\ncoupled with explanations. Thus, we exemplify how text, as timely, widely\navailable and descriptive data, can serve as a useful complementary source of\ninformation for financial and systemic risk analytics.\n"
    },
    {
        "paper_id": 1603.057,
        "authors": "Yoann Potiron, Per Mykland",
        "title": "Local Parametric Estimation in High Frequency Data",
        "comments": "67 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we give a general time-varying parameter model, where the\nmultidimensional parameter possibly includes jumps. The quantity of interest is\ndefined as the integrated value over time of the parameter process $\\Theta =\nT^{-1} \\int_0^T \\theta_t^* dt$. We provide a local parametric estimator (LPE)\nof $\\Theta$ and conditions under which we can show the central limit theorem.\nRoughly speaking those conditions correspond to some uniform limit theory in\nthe parametric version of the problem. The framework is restricted to the\nspecific convergence rate $n^{1/2}$. Several examples of LPE are studied:\nestimation of volatility, powers of volatility, volatility when incorporating\ntrading information and time-varying MA(1).\n"
    },
    {
        "paper_id": 1603.05828,
        "authors": "Angelo Antoci, Fabio Sabatini, Francesco Sarracino",
        "title": "Online Networks, Social Interaction and Segregation: An Evolutionary\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have developed an evolutionary game model, where agents can choose between\ntwo forms of social participation: interaction via online social networks and\ninteraction by exclusive means of face-to-face encounters. We illustrate the\nsocietal dynamics that the model predicts, in light of the empirical evidence\nprovided by previous literature. We then assess their welfare implications. We\nshow that dynamics, starting from a world in which online social interaction is\nless gratifying than offline encounters, will lead to the extinction of the\nsub-population of online networks users, thereby making Facebook and alike\ndisappear in the long run. Furthermore, we show that the higher the propensity\nfor discrimination between the two sub-populations of socially active\nindividuals, the greater the probability that individuals will ultimately\nsegregate themselves, making society fall into a social poverty trap.\n"
    },
    {
        "paper_id": 1603.05914,
        "authors": "Stanislao Gualdi, Giulio Cimini, Kevin Primicerio, Riccardo Di\n  Clemente, Damien Challet",
        "title": "Statistically validated network of portfolio overlaps and systemic risk",
        "comments": null,
        "journal-ref": "Scientific Reports 6, 39467 (2016)",
        "doi": "10.1038/srep39467",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Common asset holding by financial institutions, namely portfolio overlap, is\nnowadays regarded as an important channel for financial contagion with the\npotential to trigger fire sales and thus severe losses at the systemic level.\nIn this paper we propose a method to assess the statistical significance of the\noverlap between pairs of heterogeneously diversified portfolios, which then\nallows us to build a validated network of financial institutions where links\nindicate potential contagion channels due to realized portfolio overlaps. The\nmethod is implemented on a historical database of institutional holdings\nranging from 1999 to the end of 2013, but can be in general applied to any\nbipartite network where the presence of similar sets of neighbors is of\ninterest. We find that the proportion of validated network links (i.e., of\nstatistically significant overlaps) increased steadily before the 2007-2008\nglobal financial crisis and reached a maximum when the crisis occurred. We\nargue that the nature of this measure implies that systemic risk from fire\nsales liquidation was maximal at that time. After a sharp drop in 2008,\nsystemic risk resumed its growth in 2009, with a notable acceleration in 2013,\nreaching levels not seen since 2007. We finally show that market trends tend to\nbe amplified in the portfolios identified by the algorithm, such that it is\npossible to have an informative signal about financial institutions that are\nabout to suffer (enjoy) the most significant losses (gains).\n"
    },
    {
        "paper_id": 1603.05937,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "How to Combine a Billion Alphas",
        "comments": "23 pages; a reference updated, no other changes; to appear in Journal\n  of Asset Management",
        "journal-ref": "Journal of Asset Management 18(1) (2017) 64-80",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit algorithm and source code for computing optimal weights\nfor combining a large number N of alphas. This algorithm does not cost O(N^3)\nor even O(N^2) operations but is much cheaper, in fact, the number of required\noperations scales linearly with N. We discuss how in the absence of binary or\nquasi-binary clustering of alphas, which is not observed in practice, the\noptimization problem simplifies when N is large. Our algorithm does not require\ncomputing principal components or inverting large matrices, nor does it require\niterations. The number of risk factors it employs, which typically is limited\nby the number of historical observations, can be sizably enlarged via using\nposition data for the underlying tradables.\n"
    },
    {
        "paper_id": 1603.06034,
        "authors": "Ravi Kashyap",
        "title": "Solving Society's Big Ills, A Small Step",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1603.00991",
        "journal-ref": "Asian Social Science, (April 2017), Canadian Center of Science and\n  Education, Vol. 13, No. 4, pp. 175-191",
        "doi": "10.5539/ass.v13n4p175",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We look at a collection of conjectures with the unifying message that smaller\nsocial systems, tend to be less complex and can be aligned better, towards\nfulfilling their intended objectives. We touch upon a framework, referred to as\nthe four pronged approach that can aid the analysis of social systems. The four\nprongs are:\n  1. The Uncertainty Principle of the Social Sciences\n  2. The Objectives of a Social System or the Responsibilities of the Players\n  3. The Need for Smaller Organizations\n  4. Redirecting Unintended Outcomes\n  Smaller organizations mitigating the disruptive effects of corruption is\ndiscussed and also the need for organizations, whose objective is to foster the\ndevelopment of other smaller organizations. We consider a way of life, which is\nabout respect for knowledge and a desire to seek it. Knowledge can help\neradicate ignorance, but the accumulation of knowledge can lead to\noverconfidence. Hence it becomes important to instill an attitude that does not\nknowledge too seriously, along with the thirst for knowledge. All of this is\nimportant to create an environment that is conducive for smaller organizations\nand can be viewed as a natural extension of studies that fall under the wider\ncategory of understanding factors and policies aimed at increasing the welfare\nor well-being to society.\n"
    },
    {
        "paper_id": 1603.06047,
        "authors": "Ravi Kashyap",
        "title": "The Circle of Investment: Connecting the Dots of the Portfolio\n  Management Cycle...",
        "comments": "Expanded into a book \"The Circle of Investment\", Available at\n  http://www.amazon.com",
        "journal-ref": "International Journal of Economics and Finance, Vol. 6, No. 5\n  (2014), pp. 244-263",
        "doi": "10.5539/ijef.v6n5p244",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We will look at the entire cycle of the investment process relating to all\naspects of, formulating an investment hypothesis, constructing a portfolio\nbased on that, executing the trades to implement it, on-going risk management,\nperiodically measuring the performance of the portfolio, and rebalancing the\nportfolio either due to an increase in the risk parameters or due to a\ndeviation from the intended asset allocation. We provide several illustrative\nanalogies that are meant to intuitively explain the pleasures and the pitfalls\nthat can arise while managing a portfolio. If we consider the entire investment\nmanagement procedure as being akin to connecting the dots of a circle, then the\nCircle of Investment can be represented as a dotted circle with many dots\nfalling approximately on the circumference and with no clue about the exact\nlocation of the centre or the length of the radius. We represent the investment\nprocess as a dotted circle since there is a lot of ambiguity in the various\nsteps involved. The circle also indicates the repetitive nature of many steps\nthat are continuously carried out while investing. This work introduces two new\npoints pertaining to this dotted circle and improves the ability, to understand\nhow far-off this dotted circle is, from a more well-defined circle and, to\ncreate a well-formed circle. The two innovations we introduce are:\n  1. The first, relating to the limitations that apply to any finding in the\nsocial sciences, would be the additional point we introduce that lies near the\ncentre of the circle. We title this as, 'The Uncertainty Principle of the\nSocial Sciences'.\n  2. The second, relating to establishing confidence levels in a systematic\nmanner for each view we associate with a security or group of securities as\nrequired by the Black-Litterman framework, would be the new point we present\nnear the circumference of the circle.\n"
    },
    {
        "paper_id": 1603.0605,
        "authors": "Philip Ernst, James Thompson, and Yinsen Miao",
        "title": "Tukey's transformational ladder for portfolio management",
        "comments": "32 pages",
        "journal-ref": "Financial Markets and Portfolio Management (2017) 31(3): 317-355",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Over the past half-century, the empirical finance community has produced vast\nliterature on the advantages of the equally weighted S\\&P 500 portfolio as well\nas the often overlooked disadvantages of the market capitalization weighted\nStandard and Poor's (S\\&P 500) portfolio (see \\cite{Bloom}, \\cite{Uppal},\n\\cite{Jacobs}, \\cite{Treynor}). However, portfolio allocation based on Tukey's\ntransformational ladde have, rather surprisingly, remained absent from the\nliterature. In this work, we consider the S\\&P 500 portfolio over the 1958-2015\ntime horizon weighted by Tukey's transformational ladder (\\cite{Tukey2}):\n$1/x^2,\\,\\, 1/x,\\,\\, 1/\\sqrt{x},\\,\\, \\text{log}(x),\\,\\, \\sqrt{x},\\,\\, x,\\,\\,\n\\text{and} \\,\\, x^2$, where $x$ is defined as the market capitalization\nweighted S\\&P 500 portfolio. Accounting for dividends and transaction fees, we\nfind that the 1/$x^2$ weighting strategy produces cumulative returns that\nsignificantly dominates all other portfolios, achieving a compound annual\ngrowth rate of 18\\% over the 1958-2015 horizon. Our story is furthered by a\nstartling phenomenon: both the cumulative and annual returns of the $1/x^2$\nweighting strategy are superior to those of the $1/x$ weighting strategy, which\nare in turn superior to those of the 1/$\\sqrt{x}$ weighted portfolio, and so\nforth, ending with the $x^2$ transformation, whose cumulative returns are the\nlowest of the seven transformations of Tukey's transformational ladder. The\norder of cumulative returns precisely follows that of Tukey's transformational\nladder. To the best of our knowledge, we are the first to discover this\nphenomenon.\n"
    },
    {
        "paper_id": 1603.06183,
        "authors": "Enzo Busseti, Ernest K. Ryu, Stephen Boyd",
        "title": "Risk-Constrained Kelly Gambling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the classic Kelly gambling problem with general distribution of\noutcomes, and an additional risk constraint that limits the probability of a\ndrawdown of wealth to a given undesirable level. We develop a bound on the\ndrawdown probability; using this bound instead of the original risk constraint\nyields a convex optimization problem that guarantees the drawdown risk\nconstraint holds. Numerical experiments show that our bound on drawdown\nprobability is reasonably close to the actual drawdown risk, as computed by\nMonte Carlo simulation. Our method is parametrized by a single parameter that\nhas a natural interpretation as a risk-aversion parameter, allowing us to\nsystematically trade off asymptotic growth rate and drawdown risk. Simulations\nshow that this method yields bets that out perform fractional-Kelly bets for\nthe same drawdown risk level or growth rate. Finally, we show that a natural\nquadratic approximation of our convex problem is closely connected to the\nclassical mean-variance Markowitz portfolio selection problem.\n"
    },
    {
        "paper_id": 1603.06196,
        "authors": "Sgouris Sgouridis, Abdulla Kaya, Denes Csala",
        "title": "Switching Economics for Physics and the Carbon Price Inflation: Problems\n  in Integrated Assessment Models and their Implications",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Integrated Assessment Models (IAMs) are mainstay tools for assessing the\nlong-term interactions between climate and the economy and for deriving optimal\npolicy responses in the form of carbon prices. IAMs have been criticized for\ncontroversial discount rate assumptions, arbitrary climate damage functions,\nand the inadequate handling of potentially catastrophic climate outcomes. We\nreview these external shortcomings for prominent IAMs before turning our focus\non an internal modeling fallacy: the widespread misapplication of the Constant\nElasticity of Substitution (CES) function for the technology transitions\nmodeled by IAMs. Applying CES, an economic modeling approach, on technical\nfactor inputs over long periods where an entire factor (the greenhouse gas\nemitting fossil fuel inputs) must be substituted creates artifacts that fail to\nmatch the S-curve patterns observed historically. A policy critical result, the\nmonotonically increasing cost of carbon, a universal feature of IAMs, is called\ninto question by showing that it is unrealistic as it is an artifact of the\nmodeling approach and not representative of the technical substitutability\npotential nor of the expected cost of the technologies. We demonstrate this\nfirst through a simple but representative example of CES application on the\nenergy system and with a sectoral discussion of the actual fossil substitution\ncosts. We propose a methodological modification using dynamically varying\nelasticity of substitution as a plausible alternative to model the energy\ntransition in line with the historical observations and technical realities\nwithin the existing modeling systems. Nevertheless, a fundamentally different\napproach based on physical energy principles would be more appropriate.\n"
    },
    {
        "paper_id": 1603.06202,
        "authors": "Sid Ghoshal, Stephen Roberts",
        "title": "Extracting Predictive Information from Heterogeneous Data Streams using\n  Gaussian Processes",
        "comments": "15 pages, 5 figures, accepted for publication in Algorithmic Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are notoriously complex environments, presenting vast\namounts of noisy, yet potentially informative data. We consider the problem of\nforecasting financial time series from a wide range of information sources\nusing online Gaussian Processes with Automatic Relevance Determination (ARD)\nkernels. We measure the performance gain, quantified in terms of Normalised\nRoot Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson\ncorrelation, from fusing each of four separate data domains: time series\ntechnicals, sentiment analysis, options market data and broker recommendations.\nWe show evidence that ARD kernels produce meaningful feature rankings that help\nretain salient inputs and reduce input dimensionality, providing a framework\nfor sifting through financial complexity. We measure the performance gain from\nfusing each domain's heterogeneous data streams into a single probabilistic\nmodel. In particular our findings highlight the critical value of options data\nin mapping out the curvature of price space and inspire an intuitive, novel\ndirection for research in financial prediction.\n"
    },
    {
        "paper_id": 1603.06312,
        "authors": "Erhan Bayraktar and Yuchong Zhang",
        "title": "A rank based mean field game in the strong formulation",
        "comments": "Final version. To appear in Electronic Communications in Probability.\n  Keywords: Mean Field Games, competition/tournaments, common noise, rank\n  dependent interaction, non-local interaction, strong formulation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a natural game of competition and solve the corresponding mean\nfield game with \\emph{common noise} when agents' rewards are \\emph{rank\ndependent}. We use this solution to provide an approximate Nash equilibrium for\nthe finite player game and obtain the rate of convergence.\n"
    },
    {
        "paper_id": 1603.06389,
        "authors": "Sergey Badikov, Antoine Jacquier, Daphne Qing Liu, Patrick Roome",
        "title": "No-arbitrage bounds for the forward smile given marginals",
        "comments": "20 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the robust replication of forward-start straddles given quoted\n(Call and Put options) market data. One approach to this problem classically\nfollows semi-infinite linear programming arguments, and we propose a\ndiscretisation scheme to reduce its dimensionality and hence its complexity.\nAlternatively, one can consider the dual problem, consisting in finding optimal\nmartingale measures under which the upper and the lower bounds are attained.\nSemi-analytical solutions to this dual problem were proposed by Hobson and\nKlimmek (2013) and by Hobson and Neuberger (2008). We recast this dual approach\nas a finite dimensional linear programme, and reconcile numerically, in the\nBlack-Scholes and in the Heston model, the two approaches.\n"
    },
    {
        "paper_id": 1603.06407,
        "authors": "Rui-Jie Wu, Gui-Yuan Shi, Yi-Cheng Zhang, Manuel Sebastian Mariani",
        "title": "The mathematics of non-linear metrics for nested networks",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 460 (2016):\n  254-269",
        "doi": "10.1016/j.physa.2016.05.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Numerical analysis of data from international trade and ecological networks\nhas shown that the non-linear fitness-complexity metric is the best candidate\nto rank nodes by importance in bipartite networks that exhibit a nested\nstructure. Despite its relevance for real networks, the mathematical properties\nof the metric and its variants remain largely unexplored. Here, we perform an\nanalytic and numeric study of the fitness-complexity metric and a new variant,\ncalled minimal extremal metric. We rigorously derive exact expressions for node\nscores for perfectly nested networks and show that these expressions explain\nthe non-trivial convergence properties of the metrics. A comparison between the\nfitness-complexity metric and the minimal extremal metric on real data reveals\nthat the latter can produce improved rankings if the input data are reliable.\n"
    },
    {
        "paper_id": 1603.06498,
        "authors": "Dirk Becherer, Todor Bilarev, Peter Frentrup",
        "title": "Optimal Liquidation under Stochastic Liquidity",
        "comments": "Appeared in Finance and Stochastics as \"online first\". The final\n  publication is available at link.springer.com",
        "journal-ref": "Finance Stoch (2018), 22/1: 39-68",
        "doi": "10.1007/s00780-017-0346-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve explicitly a two-dimensional singular control problem of finite fuel\ntype for infinite time horizon. The problem stems from the optimal liquidation\nof an asset position in a financial market with multiplicative and transient\nprice impact. Liquidity is stochastic in that the volume effect process, which\ndetermines the inter-temporal resilience of the market in spirit of Predoiu,\nShaikhet and Shreve (2011), is taken to be stochastic, being driven by own\nrandom noise. The optimal control is obtained as the local time of a diffusion\nprocess reflected at a non-constant free boundary. To solve the HJB variational\ninequality and prove optimality, we need a combination of probabilistic\narguments and calculus of variations methods, involving Laplace transforms of\ninverse local times for diffusions reflected at elastic boundaries.\n"
    },
    {
        "paper_id": 1603.06558,
        "authors": "Richard J Martin",
        "title": "Universal trading under proportional transaction costs",
        "comments": "arXiv admin note: text overlap with arXiv:1204.6488",
        "journal-ref": "RISK 27(8):54-59 (2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theory of optimal trading under proportional transaction costs has been\nconsidered from a variety of perspectives. In this paper, we show that all the\nresults can be interpreted using a universal law, illustrating the results in\ntrading algorithm design.\n"
    },
    {
        "paper_id": 1603.06805,
        "authors": "Dieter Hendricks",
        "title": "Using real-time cluster configurations of streaming asynchronous\n  features as online state descriptors in financial markets",
        "comments": "19 pages, 6 figures, 3 tables, under review at Pattern Recognition\n  Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a scheme for online, unsupervised state discovery and detection\nfrom streaming, multi-featured, asynchronous data in high-frequency financial\nmarkets. Online feature correlations are computed using an unbiased, lossless\nFourier estimator. A high-speed maximum likelihood clustering algorithm is then\nused to find the feature cluster configuration which best explains the\nstructure in the correlation matrix. We conjecture that this feature\nconfiguration is a candidate descriptor for the temporal state of the system.\nUsing a simple cluster configuration similarity metric, we are able to\nenumerate the state space based on prevailing feature configurations. The\nproposed state representation removes the need for human-driven data\npre-processing for state attribute specification, allowing a learning agent to\nfind structure in streaming data, discern changes in the system, enumerate its\nperceived state space and learn suitable action-selection policies.\n"
    },
    {
        "paper_id": 1603.06825,
        "authors": "Nikolai Dokuchaev",
        "title": "First Order BSPDEs in higher dimension for optimal control problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies the First Order BSPDEs (Backward Stochastic Partial\nDifferential Equations) suggested earlier for a case of multidimensional state\ndomain with a boundary. These equations represent analogs of\nHamilton-Jacobi-Bellman equations and allow to construct the value function for\nstochastic optimal control problems with unspecified dynamics where the\nunderlying processes do not necessarily satisfy stochastic differential\nequations of a known kind with a given structure. The problems considered arise\nin financial modelling.\n"
    },
    {
        "paper_id": 1603.06888,
        "authors": "F. Knobloch and J.-F. Mercure",
        "title": "The behavioural aspect of green technology investments: a general\n  positive model in the context of heterogeneous agents",
        "comments": "21 pages, 6 figures, 7 tables, to appear in Environmental Innovation\n  and Societal Transitions",
        "journal-ref": "Environmental Innovation and Societal Transitions 21 (2016) 39-55",
        "doi": "10.1016/j.eist.2016.03.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Studies report that firms do not invest in cost-effective green technologies.\nWhile economic barriers can explain parts of the gap, behavioural aspects cause\nfurther under-valuation. This could be partly due to systematic deviations of\ndecision-making agents' perceptions from normative benchmarks, and partly due\nto their diversity. This paper combines available behavioural knowledge into a\nsimple model of technology adoption. Firms are modelled as heterogeneous agents\nwith different behavioural responses. To quantify the gap, the model simulates\ntheir investment decisions from different theoretical perspectives. While\nrelevant parameters are uncertain at the micro-level, using distributed agent\nperspectives provides a realistic representation of the macro adoption rate.\nThe model is calibrated using audit data for proposed investments in energy\nefficient electric motors. The inclusion of behavioural factors reduces\nsignificantly expected adoption rates: from 81% using a normative optimisation\nperspective, down to 20% using a behavioural perspective. The effectiveness of\nvarious policies is tested.\n"
    },
    {
        "paper_id": 1603.07019,
        "authors": "Pablo Azcue, Nora Muler, Zbigniew Palmowski",
        "title": "Optimal dividend payments for a two-dimensional insurance risk process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a two-dimensional optimal dividend problem in the context of two\nbranches of an insurance company with compound Poisson surplus processes\ndividing claims and premia in some specified proportions. We solve the\nstochastic control problem of maximizing expected cumulative discounted\ndividend payments (among all admissible dividend strategies) until ruin of at\nleast one company. We prove that the value function is the smallest viscosity\nsupersolution of the respective Hamilton-Jacobi-Bellman equation and we\ndescribe the optimal strategy. We analize some numerical examples.\n"
    },
    {
        "paper_id": 1603.0702,
        "authors": "Tomas Krehlik and Jozef Barunik",
        "title": "Cyclical properties of supply-side and demand-side shocks in oil-based\n  commodity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Oil markets profoundly influence world economies through determination of\nprices of energy and transports. Using novel methodology devised in frequency\ndomain, we study the information transmission mechanisms in oil-based commodity\nmarkets. Taking crude oil as a supply-side benchmark and heating oil and\ngasoline as demand-side benchmarks, we document new stylized facts about\ncyclical properties of the transmission mechanism generated by volatility\nshocks with heterogeneous frequency responses. Our first key finding is that\nshocks to volatility with response shorter than one week are increasingly\nimportant to the transmission mechanism over the studied period. Second,\ndemand-side shocks to volatility are becoming increasingly important in\ncreating short-run connectedness. Third, the supply-side shocks to volatility\nresonating in both the long run and short run are important sources of\nconnectedness.\n"
    },
    {
        "paper_id": 1603.07074,
        "authors": "Tiexin Guo, Erxin Zhang, Mingzhi Wu, Bixuan Yang, George Yuan and\n  Xiaolin Zeng",
        "title": "On random convex analysis",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, based on the idea of randomizing space theory, random convex\nanalysis has been being developed in order to deal with the corresponding\nproblems in random environments such as analysis of conditional convex risk\nmeasures and the related variational problems and optimization problems. Random\nconvex analysis is convex analysis over random locally convex modules. Since\nrandom locally convex modules have the more complicated topological and\nalgebraic structures than ordinary locally convex spaces, establishing random\nconvex analysis will encounter harder mathematical challenges than classical\nconvex analysis so that there are still a lot of fundamentally important\nunsolved problems in random convex analysis. This paper is devoted to solving\nsome important theoretic problems. First, we establish the inferior limit\nbehavior of a proper lower semicontinuous $L^0$--convex function on a random\nlocally convex module endowed with the locally $L^0$--convex topology, which\nmakes perfect the Fenchel--Moreau duality theorem for such functions. Then, we\ninvestigate the relations among continuity, locally $L^0$--Lipschitzian\ncontinuity and almost surely sequent continuity of a proper $L^0$--convex\nfunction. And then, we establish the elegant relationships among\nsubdifferentiability, G\\^ateaux--differentiability and\nFr\\'ech\\'et--differentiability for a proper $L^0$--convex function defined on\nrandom normed modules. At last, based on the Ekeland's variational principle\nfor a proper lower semicontinuous $\\bar{L}^0$--valued function, we show that\n$\\varepsilon$--subdifferentials can be approximated by subdifferentials. We\nwould like to emphasize that the success of this paper lies in simultaneously\nconsidering the $(\\varepsilon, \\lambda)$--topology and the locally\n$L^0$--convex topology for a random locally convex module.\n"
    },
    {
        "paper_id": 1603.07225,
        "authors": "Maya Briani, Lucia Caramellino, Giulia Terenzi, Antonino Zanette",
        "title": "Numerical stability of a hybrid method for pricing options",
        "comments": "arXiv admin note: text overlap with arXiv:1503.03705",
        "journal-ref": null,
        "doi": "10.1142/S0219024919500365",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop and study stability properties of a hybrid approximation of\nfunctionals of the Bates jump model with stochastic interest rate that uses a\ntree method in the direction of the volatility and the interest rate and a\nfinite-difference approach in order to handle the underlying asset price\nprocess. We also propose hybrid simulations for the model, following a binomial\ntree in the direction of both the volatility and the interest rate, and a\nspace-continuous approximation for the underlying asset price process coming\nfrom a Euler-Maruyama type scheme. We show that our methods allow to obtain\nefficient and accurate European and American option prices. Numerical\nexperiments are provided, and show the reliability and the efficiency of the\nalgorithms.\n"
    },
    {
        "paper_id": 1603.07488,
        "authors": "Fr\\'ed\\'eric Vrins and Monique Jeanblanc",
        "title": "Conic Martingales from Stochastic Integrals",
        "comments": "34 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce the concept of conic martingales}. This class\nrefers to stochastic processes having the martingale property, but that evolve\nwithin given (possibly time-dependent) boundaries. We first review some results\nabout the martingale property of solution to driftless stochastic differential\nequations. We then provide a simple way to construct and handle such processes.\nSpecific attention is paid to martingales in $[0,1]$. One of these martingales\nproves to be analytically tractable. It is shown that up to shifting and\nrescaling constants, it is the only martingale (with the trivial constant,\nBrownian motion and Geometric Brownian motion) having a separable coefficient\n$\\sigma(t,y)=g(t)h(y)$ and that can be obtained via a time-homogeneous mapping\nof Gaussian diffusions. The approach is exemplified to the modeling of\nstochastic conditional survival probabilities in the univariate (both\nconditional and unconditional to survival) and bivariate cases.\n"
    },
    {
        "paper_id": 1603.07532,
        "authors": "Nassim Nicholas Taleb",
        "title": "A Short Note on P-Value Hacking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the expected values from p-value hacking as a choice of the\nminimum p-value among $m$ independents tests, which can be considerably lower\nthan the \"true\" p-value, even with a single trial, owing to the extreme\nskewness of the meta-distribution.\n  We first present an exact probability distribution (meta-distribution) for\np-values across ensembles of statistically identical phenomena. We derive the\ndistribution for small samples $2<n \\leq n^*\\approx 30$ as well as the limiting\none as the sample size $n$ becomes large. We also look at the properties of the\n\"power\" of a test through the distribution of its inverse for a given p-value\nand parametrization.\n  The formulas allow the investigation of the stability of the reproduction of\nresults and \"p-hacking\" and other aspects of meta-analysis.\n  P-values are shown to be extremely skewed and volatile, regardless of the\nsample size $n$, and vary greatly across repetitions of exactly same protocols\nunder identical stochastic copies of the phenomenon; such volatility makes the\nminimum $p$ value diverge significantly from the \"true\" one. Setting the power\nis shown to offer little remedy unless sample size is increased markedly or the\np-value is lowered by at least one order of magnitude.\n"
    },
    {
        "paper_id": 1603.07615,
        "authors": "Julia Eisenberg, Paul Kr\\\"uhner",
        "title": "A Note on the Optimal Dividends Paid in a Foreign Currency",
        "comments": "8 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an insurance entity endowed with an initial capital and a surplus\nprocess modelled as a Brownian motion with drift. It is assumed that the\ncompany seeks to maximise the cumulated value of expected discounted dividends,\nwhich are declared or paid in a foreign currency. The currency fluctuation is\nmodelled as a L\\'evy process. We consider both cases: restricted and\nunrestricted dividend payments. It turns out that the value function and the\noptimal strategy can be calculated explicitly.\n"
    },
    {
        "paper_id": 1603.07682,
        "authors": "Robert Kleinberg, Bo Waggoner, E. Glen Weyl",
        "title": "Descending Price Optimally Coordinates Search",
        "comments": "JEL Classification: D44, D47, D82, D83. 117 pages, of which 74 are\n  appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investigating potential purchases is often a substantial investment under\nuncertainty. Standard market designs, such as simultaneous or English auctions,\ncompound this with uncertainty about the price a bidder will have to pay in\norder to win. As a result they tend to confuse the process of search both by\nleading to wasteful information acquisition on goods that have already found a\ngood purchaser and by discouraging needed investigations of objects,\npotentially eliminating all gains from trade. In contrast, we show that the\nDutch auction preserves all of its properties from a standard setting without\ninformation costs because it guarantees, at the time of information\nacquisition, a price at which the good can be purchased. Calibrations to\nstart-up acquisition and timber auctions suggest that in practice the social\nlosses through poor search coordination in standard formats are an order of\nmagnitude or two larger than the (negligible) inefficiencies arising from\nex-ante bidder asymmetries.\n"
    },
    {
        "paper_id": 1603.07822,
        "authors": "Gautier Marti, Frank Nielsen, Philippe Donnat, S\\'ebastien Andler",
        "title": "On clustering financial time series: a need for distances between\n  dependent random variables",
        "comments": "Work presented during a workshop on Information Geometry at the\n  International Centre for Mathematical Sciences, Edinburgh, UK",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The following working document summarizes our work on the clustering of\nfinancial time series. It was written for a workshop on information geometry\nand its application for image and signal processing. This workshop brought\nseveral experts in pure and applied mathematics together with applied\nresearchers from medical imaging, radar signal processing and finance. The\nauthors belong to the latter group. This document was written as a long\nintroduction to further development of geometric tools in financial\napplications such as risk or portfolio analysis. Indeed, risk and portfolio\nanalysis essentially rely on covariance matrices. Besides that the Gaussian\nassumption is known to be inaccurate, covariance matrices are difficult to\nestimate from empirical data. To filter noise from the empirical estimate,\nMantegna proposed using hierarchical clustering. In this work, we first show\nthat this procedure is statistically consistent. Then, we propose to use\nclustering with a much broader application than the filtering of empirical\ncovariance matrices from the estimate correlation coefficients. To be able to\ndo that, we need to obtain distances between the financial time series that\nincorporate all the available information in these cross-dependent random\nprocesses.\n"
    },
    {
        "paper_id": 1603.08114,
        "authors": "Tetsuya Takaishi",
        "title": "GPU Computing in Bayesian Inference of Realized Stochastic Volatility\n  Model",
        "comments": "5 pages, 3 figures",
        "journal-ref": "Journal of Physics: Conference Series 574 (2015) 012143",
        "doi": "10.1088/1742-6596/574/1/012143",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The realized stochastic volatility (RSV) model that utilizes the realized\nvolatility as additional information has been proposed to infer volatility of\nfinancial time series. We consider the Bayesian inference of the RSV model by\nthe Hybrid Monte Carlo (HMC) algorithm. The HMC algorithm can be parallelized\nand thus performed on the GPU for speedup. The GPU code is developed with CUDA\nFortran. We compare the computational time in performing the HMC algorithm on\nGPU (GTX 760) and CPU (Intel i7-4770 3.4GHz) and find that the GPU can be up to\n17 times faster than the CPU. We also code the program with OpenACC and find\nthat appropriate coding can achieve the similar speedup with CUDA Fortran.\n"
    },
    {
        "paper_id": 1603.08142,
        "authors": "Mikhail Timonin",
        "title": "Conjoint axiomatization of the Choquet integral for heterogeneous\n  product sets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an axiomatization of the Choquet integral model for the general\ncase of a heterogeneous product set $X = X_1 \\times \\ldots \\times X_n$. In MCDA\nelements of $X$ are interpreted as alternatives, characterized by criteria\ntaking values from the sets $X_i$. Previous axiomatizations of the Choquet\nintegral have been given for particular cases $X = Y^n$ and $X = \\mathbb{R}^n$.\nHowever, within multicriteria context such identicalness, hence\ncommensurateness, of criteria cannot be assumed a priori. This constitutes the\nmajor difference of this paper from the earlier axiomatizations. In particular,\nthe notion of \"comonotonicity\" cannot be used in a heterogeneous structure, as\nthere does not exist a \"built-in\" order between elements of sets $X_i$ and\n$X_j$. However, such an order is implied by the representation model. Our\napproach does not assume commensurateness of criteria. We construct the\nrepresentation and study its uniqueness properties.\n"
    },
    {
        "paper_id": 1603.08169,
        "authors": "Agostino Capponi and Lijun Bo",
        "title": "Robust Optimization of Credit Portfolios",
        "comments": "Keywords: robust control, default contagion, HJB equation, relative\n  entropy, Mathematics of Operations Research. Forthcoming, 2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a dynamic credit portfolio framework where optimal investment\nstrategies are robust against misspecifications of the reference credit model.\nThe risk-averse investor models his fear of credit risk misspecification by\nconsidering a set of plausible alternatives whose expected log likelihood\nratios are penalized. We provide an explicit characterization of the optimal\nrobust bond investment strategy, in terms of default state dependent value\nfunctions associated with the max-min robust optimization criterion. The value\nfunctions can be obtained as the solutions of a recursive system of HJB\nequations. We show that each HJB equation is equivalent to a suitably truncated\nequation admitting a unique bounded regular solution. The truncation technique\nrelies on estimates for the solution of the master HJB equation that we\nestablish.\n"
    },
    {
        "paper_id": 1603.08216,
        "authors": "Maximilian Ga{\\ss} and Kathrin Glau",
        "title": "A Flexible Galerkin Scheme for Option Pricing in L\\'evy Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One popular approach to option pricing in L\\'evy models is through solving\nthe related partial integro differential equation (PIDE). For the numerical\nsolution of such equations powerful Galerkin methods have been put forward e.g.\nby Hilber et al. (2013). As in practice large classes of models are maintained\nsimultaneously, flexibility in the driving L\\'evy model is crucial for the\nimplementation of these powerful tools. In this article we provide such a\nflexible finite element Galerkin method. To this end we exploit the Fourier\nrepresentation of the infinitesimal generator, i.e. the related symbol, which\nis explicitly available for the most relevant L\\'evy models. Empirical studies\nfor the Merton, NIG and CGMY model confirm the numerical feasibility of the\nmethod.\n"
    },
    {
        "paper_id": 1603.08245,
        "authors": "Ioannis Karatzas and Johannes Ruf",
        "title": "Trading Strategies Generated by Lyapunov Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Functional portfolio generation, initiated by E.R. Fernholz almost twenty\nyears ago, is a methodology for constructing trading strategies with controlled\nbehavior. It is based on very weak and descriptive assumptions on the\ncovariation structure of the underlying market model, and needs no estimation\nof model parameters. In this paper, the corresponding generating functions $G$\nare interpreted as Lyapunov functions for the vector process $\\mu(\\cdot)$ of\nmarket weights; that is, via the property that $G(\\mu(\\cdot))$ is a\nsupermartingale under an appropriate change of measure. This point of view\nunifies, generalizes, and simplifies several existing results, and allows the\nformulation of conditions under which it is possible to outperform the market\nportfolio over appropriate time-horizons. From a probabilistic point of view,\nthe present paper yields results concerning the interplay of stochastic\ndiscount factors and concave transformations of semimartingales on compact\ndomains.\n"
    },
    {
        "paper_id": 1603.08289,
        "authors": "Jiling Cao, Teh Raihana Nazirah Roslan and Wenjun Zhang",
        "title": "Pricing variance swaps in a hybrid model of stochastic volatility and\n  interest rate with regime-switching",
        "comments": "16 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of pricing discretely-sampled variance\nswaps based on a hybrid model of stochastic volatility and stochastic interest\nrate with regime-switching. Our modelling framework extends the Heston\nstochastic volatility model by including the CIR stochastic interest rate and\nmodel parameters that switch according to a continuous-time observable Markov\nchain process. A semi-closed form pricing formula for variance swaps is\nderived. The pricing formula is assessed through numerical implementations, and\nthe impact of including regime-switching on pricing variance swaps is also\ndiscussed.\n"
    },
    {
        "paper_id": 1603.08311,
        "authors": "Michael Coopersmith and Pascal J. Gambardella",
        "title": "Interest Rates and Inflation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is an extension of the work of one of us (Coopersmith, 2011) in\nderiving the relationship between certain interest rates and the inflation rate\nof a two component economic system. We use the well-known Fisher relation\nbetween the difference of the nominal interest rate and its inflation adjusted\nvalue to eliminate the inflation rate and obtain a delay differential equation.\nWe provide computer simulated solutions for this equation over regimes of\ninterest. This paper could be of interest to three audiences: those in\nEconomics who are interested in interest and inflation; those in Mathematics\nwho are interested in examining a detailed analysis of a delay differential\nequation, which includes a summary of existing results, simulations, and an\nexact solution; and those in Physics who are interested in non-traditional\napplications of traditional methods of modeling.\n"
    },
    {
        "paper_id": 1603.08344,
        "authors": "Ron W Nielsen",
        "title": "The unresolved mystery of the great divergence is solved",
        "comments": "26pages, 19 figures, 9282 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The so-called great divergence in the income per capita is described in the\nUnified Growth Theory as the mind-boggling and unresolved mystery about the\ngrowth process. This mystery has now been solved: the great divergence never\nhappened. It was created by the manipulation of data. Economic growth in\nvarious regions is at different levels of development but it follows similar,\nnon-divergent trajectories. Unified Growth Theory is shown yet again to be\nincorrect and scientifically unacceptable. It promotes incorrect and even\npotentially dangerous concepts. The distorted presentation of data supporting\nthe concept of the great divergence shows that economic growth is now\ndeveloping along moderately-increasing trajectories but mathematical analysis\nof the same data and even their undistorted presentation shows that these\ntrajectories are now increasing approximately vertically with time. So, while\nthe distorted presentation of data used in the Unified Growth Theory suggests\ngenerally sustainable and secure economic growth, the undistorted presentation\nof data demonstrates that the growth is unsustainable and insecure. The concept\nof takeoffs from stagnation to the sustained-growth regime promoted in the\nUnified Growth Theory is also dangerously misleading because it suggests a\nsustainable and prosperous future while the mathematical analysis of data shows\nthat the current economic growth is insecure and unsustainable.\n"
    },
    {
        "paper_id": 1603.08383,
        "authors": "Elvis Oltean",
        "title": "Modelling income, wealth, and expenditure data by use of Econophysics",
        "comments": "arXiv admin note: text overlap with arXiv:0709.3662 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, we identify several distributions from Physics and\nstudy their applicability to phenomena such as distribution of income, wealth,\nand expenditure. Firstly, we apply logistic distribution to these data and we\nfind that it fits very well the annual data for the entire income interval\nincluding for upper income segment of population. Secondly, we apply\nFermi-Dirac distribution to these data. We seek to explain possible\ncorrelations and analogies between economic systems and statistical\nthermodynamics systems. We try to explain their behavior and properties when we\ncorrelate physical variables with macroeconomic aggregates and indicators. Then\nwe draw some analogies between parameters of the Fermi-Dirac distribution and\nmacroeconomic variables. Thirdly, as complex systems are modeled using\npolynomial distributions, we apply polynomials to the annual sets of data and\nwe find that it fits very well also the entire income interval. Fourthly, we\ndevelop a new methodology to approach dynamically the income, wealth, and\nexpenditure distribution similarly with dynamical complex systems. This\nmethodology was applied to different time intervals consisting of consecutive\nyears up to 35 years. Finally, we develop a mathematical model based on a\nHamiltonian that maximizes utility function applied to Ramsey model using\nFermi-Dirac and polynomial utility functions. We find some theoretical\nconnections with time preference theory. We apply these distributions to a\nlarge pool of data from countries with different levels of development, using\ndifferent methods for calculation of income, wealth, and expenditure.\n"
    },
    {
        "paper_id": 1603.08828,
        "authors": "Umut \\c{C}etin",
        "title": "Financial equilibrium with asymmetric information and random horizon",
        "comments": "Few typos have been corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study in detail and explicitly solve the version of Kyle's model\nintroduced in a specific case in \\cite{BB}, where the trading horizon is given\nby an exponentially distributed random time. The first part of the paper is\ndevoted to the analysis of time-homogeneous equilibria using tools from the\ntheory of one-dimensional diffusions. It turns out that such an equilibrium is\nonly possible if the final payoff is Bernoulli distributed as in \\cite{BB}. We\nshow in the second part that the signal of the market makers use in the general\ncase is a time-changed version of the one that they would have used had the\nfinal payoff had a Bernoulli distribution. In both cases we characterise\nexplicitly the equilibrium price process and the optimal strategy of the\ninformed trader. Contrary to the original Kyle model it is found that the\nreciprocal of market's depth, i.e. Kyle's lambda, is a uniformly integrable\nsupermartingale. While Kyle's lambda is a potential, i.e. converges to $0$, for\nthe Bernoulli distributed final payoff, its limit in general is different than\n$0$.\n"
    },
    {
        "paper_id": 1603.08961,
        "authors": "John J. Nay, Martin Van der Linden, Jonathan M. Gilligan",
        "title": "Betting and Belief: Prediction Markets and Attribution of Climate Change",
        "comments": "All code and data for the model is available at\n  http://johnjnay.com/predMarket/. Forthcoming in Proceedings of the 2016\n  Winter Simulation Conference. IEEE Press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite much scientific evidence, a large fraction of the American public\ndoubts that greenhouse gases are causing global warming. We present a\nsimulation model as a computational test-bed for climate prediction markets.\nTraders adapt their beliefs about future temperatures based on the profits of\nother traders in their social network. We simulate two alternative climate\nfutures, in which global temperatures are primarily driven either by carbon\ndioxide or by solar irradiance. These represent, respectively, the scientific\nconsensus and a hypothesis advanced by prominent skeptics. We conduct\nsensitivity analyses to determine how a variety of factors describing both the\nmarket and the physical climate may affect traders' beliefs about the cause of\nglobal climate change. Market participation causes most traders to converge\nquickly toward believing the \"true\" climate model, suggesting that a climate\nmarket could be useful for building public consensus.\n"
    },
    {
        "paper_id": 1603.0903,
        "authors": "Tomasz R. Bielecki and Igor Cialenco and Marcin Pitera",
        "title": "A survey of time consistency of dynamic risk measures and dynamic\n  performance measures in discrete time: LM-measure perspective",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1409.7028",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we give a comprehensive overview of the time consistency\nproperty of dynamic risk and performance measures, focusing on a the discrete\ntime setup. The two key operational concepts used throughout are the notion of\nthe LM-measure and the notion of the update rule that, we believe, are the key\ntools for studying time consistency in a unified framework.\n"
    },
    {
        "paper_id": 1603.09049,
        "authors": "Erwan Pierre, St\\'ephane Villeneuve, Xavier Warin",
        "title": "Numerical approximation of a cash-constrained firm value with investment\n  opportunities",
        "comments": "30 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a singular control problem with regime switching that arises in\nproblems of optimal investment decisions of cash-constrained firms. The value\nfunction is proved to be the unique viscosity solution of the associated\nHamilton-Jacobi-Bellman equation.\n  Moreover, we give regularity properties of the value function as well as a\ndescription of the shape of the control regions. Based on these theoretical\nresults, a numerical deterministic approximation of the related HJB variational\ninequality is provided. We finally show that this numerical approximation\nconverges to the value function. This allows us to describe the investment and\ndividend optimal policies.\n"
    },
    {
        "paper_id": 1603.0906,
        "authors": "Ravi Kashyap",
        "title": "The Perfect Marriage and Much More: Combining Dimension Reduction,\n  Distance Measures and Covariance",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.04.174",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel methodology based on the marriage between the\nBhattacharyya distance, a measure of similarity across distributions of random\nvariables, and the Johnson-Lindenstrauss Lemma, a technique for dimension\nreduction. The resulting technique is a simple yet powerful tool that allows\ncomparisons between data-sets representing any two distributions. The degree to\nwhich different entities, (markets, universities, hospitals, cities, groups of\nsecurities, etc.), have different distance measures of their corresponding\ndistributions tells us the extent to which they are different, aiding\nparticipants looking for diversification or looking for more of the same thing.\nWe demonstrate a relationship between covariance and distance measures based on\na generic extension of Stein's Lemma. We consider an asset pricing application\nand then briefly discuss how this methodology lends itself to numerous\nmarket-structure studies and even applications outside the realm of finance /\nsocial sciences by illustrating a biological application. We provide numerical\nillustrations using security prices, volumes and volatilities of both these\nvariables from six different countries.\n"
    },
    {
        "paper_id": 1603.09149,
        "authors": "Milan Kumar Das, Anindya Goswami, Nimit Rana",
        "title": "Risk Sensitive Portfolio Optimization in a Jump Diffusion Model with\n  Regimes",
        "comments": "29 pages, 3 figures",
        "journal-ref": "SIAM J. Control Optim. 56 (2018), 1550-576",
        "doi": "10.1137/17M1121809",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article studies a portfolio optimization problem, where the market\nconsisting of several stocks is modeled by a multi-dimensional jump-diffusion\nprocess with age-dependent semi-Markov modulated coefficients. We study risk\nsensitive portfolio optimization on the finite time horizon. We study the\nproblem by using a probabilistic approach to establish the existence and\nuniqueness of the classical solution to the corresponding\nHamilton-Jacobi-Bellman (HJB) equation. We also implement a numerical scheme to\ninvestigate the behavior of solutions for different values of the initial\nportfolio wealth, the maturity, and the risk of aversion parameter.\n"
    },
    {
        "paper_id": 1603.09324,
        "authors": "Mohamed Amine Lkabous, Irmina Czarna, Jean-Fran\\c{c}ois Renaud",
        "title": "Parisian ruin for a refracted L\\'evy process",
        "comments": "Typos corrected; proof of Theorem 2 fixed; numerical examples added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate Parisian ruin for a L\\'evy surplus process with\nan adaptive premium rate, namely a refracted L\\'evy process. More general\nParisian boundary-crossing problems with a deterministic implementation delay\nare also considered. Our main contribution is a generalization of the result in\nLoeffen et al. (2013) for the probability of Parisian ruin of a standard L\\'evy\ninsurance risk process. Despite the more general setup considered here, our\nmain result is as compact and has a similar structure. Examples are provided.\n"
    },
    {
        "paper_id": 1603.09329,
        "authors": "Djilali Ait Aoudia, Jean-Fran\\c{c}ois Renaud",
        "title": "Pricing occupation-time options in a mixed-exponential jump-diffusion\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short paper, in order to price occupation-time options, such as\n(double-barrier) step options and quantile options, we derive various joint\ndistributions of a mixed-exponential jump-diffusion process and its occupation\ntimes of intervals.\n"
    },
    {
        "paper_id": 1603.09406,
        "authors": "Bikramjit Das and Vicky Fasen",
        "title": "Risk contagion under regular variation and asymptotic tail independence",
        "comments": "26 pages; 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk contagion concerns any entity dealing with large scale risks. Suppose\n(X,Y) denotes a risk vector pertaining to two components in some system. A\nrelevant measurement of risk contagion would be to quantify the amount of\ninfluence of high values of Y on X. This can be measured in a variety of ways.\nIn this paper, we study two such measures: the quantity E[max(X-t,0)|Y > t]\ncalled Marginal Mean Excess (MME) as well as the related quantity E[X|Y > t]\ncalled Marginal Expected Shortfall (MES). Both quantities are indicators of\nrisk contagion and useful in various applications ranging from finance,\ninsurance and systemic risk to environmental and climate risk. We work under\nthe assumptions of multivariate regular variation, hidden regular variation and\nasymptotic tail independence for the risk vector (X,Y). Many broad and useful\nmodel classes satisfy these assumptions. We present several examples and derive\nthe asymptotic behavior of both MME and MES as the threshold t tends to\ninfinity. We observe that although we assume asymptotic tail independence in\nthe models, MME and MES converge to 1 under very general conditions; this\nreflects that the underlying weak dependence in the model still remains\nsignificant. Besides the consistency of the empirical estimators, we introduce\nan extrapolation method based on extreme value theory to estimate both MME and\nMES for high thresholds t where little data are available. We show that these\nestimators are consistent and illustrate our methodology in both simulated and\nreal data sets.\n"
    },
    {
        "paper_id": 1603.09491,
        "authors": "Matteo Burzoni, Ilaria Peri, Chiara Maria Ruffo",
        "title": "On the properties of the Lambda value at risk: robustness, elicitability\n  and consistency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, financial industry and regulators have enhanced the debate on the\ngood properties of a risk measure. A fundamental issue is the evaluation of the\nquality of a risk estimation. On the one hand, a backtesting procedure is\ndesirable for assessing the accuracy of such an estimation and this can be\nnaturally achieved by elicitable risk measures. For the same objective, an\nalternative approach has been introduced by Davis (2016) through the so-called\nconsistency property. On the other hand, a risk estimation should be less\nsensitive with respect to small changes in the available data set and exhibit\nqualitative robustness. A new risk measure, the Lambda value at risk (Lambda\nVaR), has been recently proposed by Frittelli et al. (2014), as a\ngeneralization of VaR with the ability to discriminate the risk among P&L\ndistributions with different tail behaviour. In this article, we show that\nLambda VaR also satisfies the properties of robustness, elicitability and\nconsistency under some conditions.\n"
    },
    {
        "paper_id": 1603.09519,
        "authors": "Julia Eisenberg",
        "title": "Deterministic Income with Deterministic and Stochastic Interest Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an individual or household endowed with an initial capital and an\nincome, modeled as a deterministic process with a continuous drift rate. At\nfirst, we model the discounting rate as the price of a zero-coupon bond at zero\nunder the assumption of a short rate evolving as an Ornstein-Uhlenbeck process.\nThen, a geometric Brownian motion as the preference function and an\nOrnstein-Uhlenbeck process as the short rate are taken into consideration. It\nis assumed that the primal interest of the economic agent is to maximise the\ncumulated value of (expected) discounted consumption from a given time up to a\nfinite deterministic time horizon $T\\in\\R_+$ or, in a stochastic setting,\ninfinite time horizon. We find an explicit expression for the value function\nand for the optimal strategy in the first two cases. In the third case, we have\nto apply the viscosity ansatz.\n"
    },
    {
        "paper_id": 1603.09666,
        "authors": "Enrico Scalas, Fabio Rapallo, Tijana Radivojevi\\'c",
        "title": "Low-traffic limit and first-passage times for a simple model of the\n  continuous double auction",
        "comments": "17 pages, 7 figures, 1 appendix",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.05.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a simplified model of the continuous double auction where prices\nare integers varying from $1$ to $N$ with limit orders and market orders, but\nquantity per order limited to a single share. For this model, the order process\nis equivalent to two $M/M/1$ queues. We study the behaviour of the auction in\nthe low-traffic limit where limit orders are immediately transformed into\nmarket orders. In this limit, the distribution of prices can be computed\nexactly and gives a reasonable approximation of the price distribution when the\nratio between the rate of order arrivals and the rate of order executions is\nbelow $1/2$. This is further confirmed by the analysis of the first passage\ntime in $1$ or $N$.\n"
    },
    {
        "paper_id": 1604.00105,
        "authors": "Josselin Garnier and Knut Solna",
        "title": "Option pricing under fast-varying long-memory stochastic volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent empirical studies suggest that the volatility of an underlying price\nprocess may have correlations that decay slowly under certain market\nconditions. In this paper, the volatility is modeled as a stationary process\nwith long-range correlation properties in order to capture such a situation,\nand we consider European option pricing. This means that the volatility process\nis neither a Markov process nor a martingale. However, by exploiting the fact\nthat the price process is still a semimartingale and accordingly using the\nmartingale method, we can obtain an analytical expression for the option price\nin the regime where the volatility process is fast mean-reverting. The\nvolatility process is modeled as a smooth and bounded function of a fractional\nOrnstein-Uhlenbeck process. We give the expression for the implied volatility,\nwhich has a fractional term structure.\n"
    },
    {
        "paper_id": 1604.00148,
        "authors": "Mikio Ito, Kiyotaka Maeda, Akihiko Noda",
        "title": "Market Integration in the Prewar Japanese Rice Markets",
        "comments": "41 pages, 8 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the integration process of the Japanese major rice\nmarkets (Tokyo and Osaka) from 1881 to 1932. Using a non-Bayesian time-varying\nvector error correction model, we argue that the process strongly depended on\nthe government's policy on the network system of the telegram and telephone;\nrice traders with an intention to use modern communication tools were usually\naffected by the changes in policy. We find that (i) the Japanese rice markets\nhad been integrated in the 1910s; (ii) increasing use of telegraphs had\naccelerated rice market integration from the Meiji period in Japan; and (iii)\nlocal telephone system, which reduced the time spent by urban users sending and\nreceiving telegrams, promoted market integration.\n"
    },
    {
        "paper_id": 1604.00254,
        "authors": "Russell Barker, Andrew Dickinson, Alex Lipton, Rajeev Virmani",
        "title": "Systemic Risks in CCP Networks",
        "comments": "15 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for the credit and liquidity risks faced by clearing\nmembers of Central Counterparty Clearing houses (CCPs). This model aims to\ncapture the features of: gap risk; feedback between clearing member default,\nmarket volatility and margining requirements; the different risks faced by\nvarious types of market participant and the changes in margining requirements a\nclearing member faces as the system evolves. By considering the entire network\nof CCPs and clearing members, we investigate the distribution of losses to\ndefault fund contributions and contingent liquidity requirements for each\nclearing member; further, we identify wrong-way risks between defaults of\nclearing members and market turbulence.\n"
    },
    {
        "paper_id": 1604.00283,
        "authors": "Juan C. Correa and Klaus Jaffe",
        "title": "Corruption and Wealth: Unveiling a national prosperity syndrome in\n  Europe",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data mining revealed a cluster of economic, psychological, social and\ncultural indicators that in combination predicted corruption and wealth of\nEuropean nations. This prosperity syndrome of self-reliant citizens, efficient\ndivision of labor, a sophisticated scientific community, and respect for the\nlaw, was clearly distinct from that of poor countries that had a diffuse\nrelationship between high corruption perception, low GDP/capita, high social\ninequality, low scientific development, reliance on family and friends, and\nlanguages with many words for guilt. This suggests that there are many ways for\na nation to be poor, but few ones to become rich, supporting the existence of\nsynergistic interactions between the components in the prosperity syndrome\nfavoring economic growth. No single feature was responsible for national\nprosperity. Focusing on synergies rather than on single features should improve\nour understanding of the transition from poverty and corruption to prosperity\nin European nations and elsewhere.\n"
    },
    {
        "paper_id": 1604.00369,
        "authors": "Tomas Skovranek",
        "title": "The Mittag-Leffler Fitting of the Phillips Curve",
        "comments": "20 pages, 4 figures, 3 tables, 7 numbered equations",
        "journal-ref": "Mathematics 7(7), paper ID 589 (2019)",
        "doi": "10.3390/math7070589",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a mathematical model based on the one-parameter Mittag-Leffler\nfunction is proposed to be used for the first time to describe the relation\nbetween unemployment rate and inflation rate, also known as the Phillips curve.\nThe Phillips curve is in the literature often represented by an\nexponential-like shape. On the other hand, Phillips in his fundamental paper\nused a power function in the model definition. Considering that the ordinary as\nwell as generalised Mittag-Leffler function behaves between a purely\nexponential function and a power function it is natural to implement it in the\ndefinition of the model used to describe the relation between the data\nrepresenting the Phillips curve. For the modelling purposes the data of two\ndifferent European economies, France and Switzerland, were used and an\n\"out-of-sample\" forecast was done to compare the performance of the\nMittag-Leffler model to the performance of the power-type and exponential-type\nmodel. The results demonstrate that the ability of the Mittag-Leffler function\nto fit data that manifest signs of stretched exponentials, oscillations or even\ndamped oscillations can be of use when describing economic relations and\nphenomenons, such as the Phillips curve.\n"
    },
    {
        "paper_id": 1604.00525,
        "authors": "Michael Mania and Revaz Tevzadze",
        "title": "On regularity of primal and dual dynamic value functions related to\n  investment problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study regularity properties of the dynamic value functions of primal and\ndual problems of optimal investing for utility functions defined on the whole\nreal line. Relations between decomposition terms of value processes of primal\nand dual problems and between optimal solutions of basic and conditional\nutility maximization problems are established. These properties are used to\nshow that the value function satisfies a corresponding backward stochastic\npartial differential equation.\n  In the case of complete markets we give conditions on the utility function\nwhen this equation admits a solution.\n"
    },
    {
        "paper_id": 1604.00596,
        "authors": "Vladimir Vovk",
        "title": "Getting rich quick with the Axiom of Choice",
        "comments": "21 pages. This version takes account of the feedback from reviewers",
        "journal-ref": "Finance and Stochastics (2017) 21:719-739",
        "doi": "10.1007/s00780-017-0336-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes new get-rich-quick schemes that involve trading in a\nfinancial security with a non-degenerate price path. For simplicity the\ninterest rate is assumed zero. If the price path is assumed continuous, the\ntrader can become infinitely rich immediately after it becomes non-constant (if\nit ever does). If it is assumed positive, he can become infinitely rich\nimmediately after reaching a point in time such that the variation of the log\nprice is infinite in any right neighbourhood of that point (whereas reaching a\npoint in time such that the variation of the log price is infinite in any left\nneighbourhood of that point is not sufficient). The practical value of these\nschemes is tempered by their use of the Axiom of Choice.\n"
    },
    {
        "paper_id": 1604.00976,
        "authors": "Yaneer Bar-Yam",
        "title": "From Big Data To Important Information",
        "comments": "24 pages, 7 figures (Complexity, in press), New England Complex\n  Systems Institute Report 04-01-2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Advances in science are being sought in newly available opportunities to\ncollect massive quantities of data about complex systems. While key advances\nare being made in detailed mapping of systems, how to relate this data to\nsolving many of the challenges facing humanity is unclear. The questions we\noften wish to address require identifying the impact of interventions on the\nsystem and that impact is not apparent in the detailed data that is available.\nHere we review key concepts and motivate a general framework for building\nlarger scale views of complex systems and for characterizing the importance of\ninformation in physical, biological and social systems. We provide examples of\nits application to evolutionary biology with relevance to ecology,\nbiodiversity, pandemics, and human lifespan, and in the context of social\nsystems with relevance to ethnic violence, global food prices, and stock market\npanic. Framing scientific inquiry as an effort to determine what is important\nand unimportant is a means for advancing our understanding and addressing many\npractical concerns, such as economic development or treating disease.\n"
    },
    {
        "paper_id": 1604.01224,
        "authors": "Luca Barbaglia, Ines Wilms and Christophe Croux",
        "title": "Commodity Dynamics: A Sparse Multi-class Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The correct understanding of commodity price dynamics can bring relevant\nimprovements in terms of policy formulation both for developing and developed\ncountries. Agricultural, metal and energy commodity prices might depend on each\nother: although we expect few important effects among the total number of\npossible ones, some price effects among different commodities might still be\nsubstantial. Moreover, the increasing integration of the world economy suggests\nthat these effects should be comparable for different markets. This paper\nintroduces a sparse estimator of the Multi-class Vector AutoRegressive model to\ndetect common price effects between a large number of commodities, for\ndifferent markets or investment portfolios. In a first application, we consider\nagricultural, metal and energy commodities for three different markets. We show\na large prevalence of effects involving metal commodities in the Chinese and\nIndian markets, and the existence of asymmetric price effects. In a second\napplication, we analyze commodity prices for five different investment\nportfolios, and highlight the existence of important effects from energy to\nagricultural commodities. The relevance of biofuels is hereby confirmed.\nOverall, we find stronger similarities in commodity price effects among\nportfolios than among markets.\n"
    },
    {
        "paper_id": 1604.01281,
        "authors": "Peter Friz, Stefan Gerhold, Arpad Pinter",
        "title": "Option Pricing in the Moderate Deviations Regime",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider call option prices in diffusion models close to expiry, in an\nasymptotic regime (\"moderately out of the money\") that interpolates between the\nwell-studied cases of at-the-money options and out-of-the-money fixed-strike\noptions. First and higher order small-time moderate deviation estimates of call\nprices and implied volatility are obtained. The expansions involve only simple\nexpressions of the model parameters, and we show in detail how to calculate\nthem for generic local and stochastic volatility models. Some numerical\nexamples for the Heston model illustrate the accuracy of our results.\n"
    },
    {
        "paper_id": 1604.01322,
        "authors": "Hiroyasu Inoue",
        "title": "Controllability Analyses on Firm Networks Based on Comprehensive Data",
        "comments": "arXiv admin note: text overlap with arXiv:1512.05066",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since governments give stimulus to firms and expect the spillover effect by\nfiscal policies, it is important to know the effectiveness that they can\ncontrol the economy. To clarify the controllability of the economy, we\ninvestigate a firm production network observed exhaustively in Japan and what\nfirms should be directly or indirectly controlled by using control theory. By\ncontrol theory, we can classify firms into three different types: (a) firms\nthat should be directly controlled; (b) firms that should be indirectly\ncontrolled; (c) neither of them (ordinary). Since there is a direction\n(supplier and client) in the production network, we can consider controls of\ntwo different directions: demand and supply sides. As analyses results, we\nobtain the following results: (1) Each industry has diverse share of firms that\nshould be controlled directly or indirectly. The configurations of the shares\nin industries are different between demand- and supply-sides; (2) Advancement\nof industries, such like, primary industries or other advanced industries, does\nnot show apparent difference in controllability; (3) If we clip a network in\ndescending order of capital size, we do not lose the control effect for both\ndemand- and supply-sides.\n"
    },
    {
        "paper_id": 1604.01338,
        "authors": "Fabrizio Cipollini and Robert F. Engle and Giampiero M. Gallo",
        "title": "Copula--based Specification of vector MEMs",
        "comments": "Financial support from Italian MIUR (PRIN 2010-2011 MISURA)\n  gratefully acknowledged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Multiplicative Error Model (Engle (2002)) for nonnegative valued\nprocesses is specified as the product of a (conditionally autoregressive) scale\nfactor and an innovation process with nonnegative support. A multivariate\nextension allows for the innovations to be contemporaneously correlated. We\novercome the lack of sufficiently flexible probability density functions for\nsuch processes by suggesting a copula function approach to estimate the\nparameters of the scale factors and of the correlations of the innovation\nprocesses. We illustrate this vector MEM with an application to the\ninteractions between realized volatility, volume and the number of trades. We\nshow that significantly superior realized volatility forecasts are delivered in\nthe presence of other trading activity indicators and contemporaneous\ncorrelations.\n"
    },
    {
        "paper_id": 1604.01447,
        "authors": "Juan M. Romero and Ilse B. Zubieta-Mart\\'inez",
        "title": "Relativistic Quantum Finance",
        "comments": "9 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Employing the Klein-Gordon equation, we propose a generalized Black-Scholes\nequation. In addition, we found a limit where this generalized equation is\ninvariant under conformal transformations, in particular invariant under scale\ntransformations. In this limit, we show that the stock prices distribution is\ngiven by a Cauchy distribution, instead of a normal distribution.\n"
    },
    {
        "paper_id": 1604.01557,
        "authors": "Mario Guti\\'errez-Roig, Carlota Segura, Jordi Duch, Josep Perell\\'o",
        "title": "Market Imitation and Win-Stay Lose-Shift strategies emerge as unintended\n  patterns in market direction guesses",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0159078",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decisions taken in our everyday lives are based on a wide variety of\ninformation so it is generally very difficult to assess what are the strategies\nthat guide us. Stock market therefore provides a rich environment to study how\npeople take decision since responding to market uncertainty needs a constant\nupdate of these strategies. For this purpose, we run a lab-in-the-field\nexperiment where volunteers are given a controlled set of financial information\n-based on real data from worldwide financial indices- and they are required to\nguess whether the market price would go up or down in each situation. From the\ndata collected we explore basic statistical traits, behavioural biases and\nemerging strategies. In particular, we detect unintended patterns of behavior\nthrough consistent actions which can be interpreted as {\\it Market Imitation}\nand {\\it Win-Stay Lose-Shift} emerging strategies, being {\\it Market Imitation}\nthe most dominant one. We also observe that these strategies are affected by\nexternal factors: the expert advice, the lack of information or an information\noverload reinforce the use of these intuitive strategies, while the probability\nto follow them significantly decreases when subjects spends more time to take a\ndecision. The cohort analysis shows that women and children are more prone to\nuse such strategies although their performance is not undermined. Our results\nare of interest for better handling clients expectations of trading companies,\navoiding behavioural anomalies in financial analysts decisions and improving\nnot only the design of markets but also the trading digital interfaces where\ninformation is set down. Strategies and behavioural biases observed can also be\ntranslated into new agent based modelling or stochastic price dynamics to\nbetter understand financial bubbles or the effects of asymmetric risk\nperception to price drops.\n"
    },
    {
        "paper_id": 1604.01819,
        "authors": "Nina Anchugina, Matthew Ryan, Arkadii Slinko",
        "title": "Aggregating time preferences with decreasing impatience",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that for a group of time-consistent decision makers their\ncollective time preferences may become time-inconsistent. Jackson and Yariv\n(2014) demonstrated that the result of aggregation of exponential discount\nfunctions always exhibits present bias. We show that when preferences satisfy\nthe axioms of Fishburn and Rubinstein (1982), present bias is equivalent to\ndecreasing impatience (DI). Applying the notion of comparative DI introduced by\nPrelec (2004), we generalize the result of Jackson and Yariv (2014). We prove\nthat the aggregation of distinct discount functions from comparable DI classes\nresults in the collective discount function which is strictly more DI than the\nleast DI of the functions being aggregated. We also prove an analogue of\nWeitzman's (1998) result, for hyperbolic rather than exponential discount\nfunctions. We show that if a decision maker is uncertain about her hyperbolic\ndiscount rate, then long-term costs and benefits will be discounted at a rate\nwhich is the probability-weighted harmonic mean of the possible hyperbolic\ndiscount rates.\n"
    },
    {
        "paper_id": 1604.01824,
        "authors": "Roger Martins, Dieter Hendricks",
        "title": "The statistical significance of multivariate Hawkes processes fitted to\n  limit order book data",
        "comments": "22 pages, 11 figures, 10 tables, added more detailed results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hawkes processes have seen a number of applications in finance, due to their\nability to capture event clustering behaviour typically observed in financial\nsystems. Given a calibrated Hawkes process, of concern is the statistical fit\nto empirical data, particularly for the accurate quantification of self- and\nmutual-excitation effects. We investigate the application of a multivariate\nHawkes process with a sum-of-exponentials kernel and piecewise-linear\nexogeneity factors, fitted to liquidity demand and replenishment events\nextracted from limit order book data. We consider one-, two- and\nthree-exponential kernels, applying various tests to ascertain goodness-of-fit\nand stationarity of residuals, as well as stability of the calibration\nprocedure. In line with prior research, it is found that performance across all\ntests improves as the number of exponentials is increased, with a\nsum-of-three-exponentials yielding the best fit to the given set of coupled\npoint processes.\n"
    },
    {
        "paper_id": 1604.02237,
        "authors": "Areski Cousin (SAF), Hassan Maatouk (GdR MASCOT-NUM, LIMOS,\n  DEMO-ENSMSE), Didier Rulli\\`ere (SAF)",
        "title": "Kriging of financial term-structures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to the lack of reliable market information, building financial\nterm-structures may be associated with a significant degree of uncertainty. In\nthis paper, we propose a new term-structure interpolation method that extends\nclassical spline techniques by additionally allowing for quantification of\nuncertainty. The proposed method is based on a generalization of kriging models\nwith linear equality constraints (market-fit conditions) and shape-preserving\nconditions such as monotonicity or positivity (no-arbitrage conditions). We\ndefine the most likely curve and show how to build confidence bands. The\nGaussian process covariance hyper-parameters under the construction constraints\nare estimated using cross-validation techniques. Based on observed market\nquotes at different dates, we demonstrate the efficiency of the method by\nbuilding curves together with confidence intervals for term-structures of OIS\ndiscount rates, of zero-coupon swaps rates and of CDS implied default\nprobabilities. We also show how to construct interest-rate surfaces or default\nprobability surfaces by considering time (quotation dates) as an additional\ndimension.\n"
    },
    {
        "paper_id": 1604.02269,
        "authors": "David Hobson and Anthony Neuberger",
        "title": "On the value of being American",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The virtue of an American option is that it can be exercised at any time.\nThis right is particularly valuable when there is model uncertainty. Yet almost\nall the extensive literature on American options assumes away model\nuncertainty. This paper quantifies the potential value of this flexibility by\nidentifying the supremum on the price of an American option when no model is\nimposed on the data, but rather any model is required to be consistent with a\nfamily of European call prices. The bound is enforced by a hedging strategy\ninvolving these call options which is robust to model error.\n"
    },
    {
        "paper_id": 1604.02274,
        "authors": "David Hobson and Anthony Neuberger",
        "title": "More on hedging American options under model uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this note is to reconcile two different results concerning the\nmodel-free upper bound on the price of an American option, given a set of\nEuropean option prices. Neuberger (2007, `Bounds on the American option') and\nHobson and Neuberger (2016, `On the value of being American') argue that the\ncost of the cheapest super-replicating strategy is equal to the highest\nmodel-based price, where we search over all models which price correctly the\ngiven European options. Bayraktar, Huang and Zhou (2015, `On hedging American\noptions under model uncertainty', SIAM J. Financial Math ematics) argue that\nthe cost of the cheapest super-replicating strategy can strictly exceed the\nhighest model-based price. We show that the reason for the difference in\nconclusion is that Bayraktar et al do not search over a rich enough class of\nmodels.\n"
    },
    {
        "paper_id": 1604.0237,
        "authors": "Jie Li, Bruce M. Boghosian, Chengli Li",
        "title": "The Affine Wealth Model: An agent-based model of asset exchange that\n  allows for negative-wealth agents and its empirical validation",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a stochastic, agent-based, binary-transaction Asset-Exchange Model\n(AEM) for wealth distribution that allows for agents with negative wealth. This\nmodel retains certain features of prior AEMs such as redistribution and\nwealth-attained advantage, but it also allows for shifts as well as scalings of\nthe agent density function. We derive the Fokker-Planck equation describing its\ntime evolution and we describe its numerical solution, including a methodology\nfor solving the inverse problem of finding the model parameters that best match\nempirical data. Using this methodology, we compare the steady-state solutions\nof the Fokker-Planck equation with data from the United States Survey of\nConsumer Finances over a time period of 27 years. In doing so, we demonstrate\nagreement with empirical data of an average error less than 0.16\\% over this\ntime period. We present the model parameters for the US wealth distribution\ndata as a function of time under the assumption that the distribution responds\nto their variation adiabatically. We argue that the time series of model\nparameters thus obtained provides a valuable new diagnostic tool for analyzing\nwealth inequality.\n"
    },
    {
        "paper_id": 1604.02759,
        "authors": "Ioane Muni Toke",
        "title": "Reconstruction of Order Flows using Aggregated Data",
        "comments": "32 pages, 7 tables, 14 figures",
        "journal-ref": "Market microstructure and liquidity, 2(02), 1650007 (2016)",
        "doi": "10.1142/S2382626616500076",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we investigate tick-by-tick data provided by the TRTH database\nfor several stocks on three different exchanges (Paris - Euronext, London and\nFrankfurt - Deutsche B\\\"orse) and on a 5-year span. We use a simple algorithm\nthat helps the synchronization of the trades and quotes data sources, providing\nenhancements to the basic procedure that, depending on the time period and the\nexchange, are shown to be significant. We show that the analysis of the\nperformance of this algorithm turns out to be a a forensic tool assessing the\nquality of the aggregated database: we are able to track through the data some\nsignificant technical changes that occurred on the studied exchanges. We also\nillustrate the fact that the choices made when reconstructing order flows have\nconsequences on the quantitative models that are calibrated afterwards on such\ndata. Our study also provides elements on the trade signature, and we are able\nto give a more refined look at the standard Lee-Ready procedure, giving new\nelements on the way optimal lags should be chosen when using this method. The\nfindings are in line with both financial reasoning and the analysis of an\nillustrative Poisson model of the order flow.\n"
    },
    {
        "paper_id": 1604.03042,
        "authors": "Erhan Bayraktar and Christopher W. Miller",
        "title": "Distribution-Constrained Optimal Stopping",
        "comments": "Final version. To appear in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve the problem of optimal stopping of a Brownian motion subject to the\nconstraint that the stopping time's distribution is a given measure consisting\nof finitely-many atoms. In particular, we show that this problem can be\nconverted to a finite sequence of state-constrained optimal control problems\nwith additional states corresponding to the conditional probability of stopping\nat each possible terminal time. The proof of this correspondence relies on a\nnew variation of the dynamic programming principle for state-constrained\nproblems which avoids measurable selection. We emphasize that distribution\nconstraints lead to novel and interesting mathematical problems on their own,\nbut also demonstrate an application in mathematical finance to model-free\nsuperhedging with an outlook on volatility.\n"
    },
    {
        "paper_id": 1604.03317,
        "authors": "J\\'er\\^ome Lelong",
        "title": "Pricing American options using martingale bases",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose an algorithm to price American options by directly\nsolving the dual minimization problem introduced by Rogers. Our approach relies\non approximating the set of uniformly square integrable martingales by a finite\ndimensional Wiener chaos expansion. Then, we use a sample average approximation\ntechnique to efficiently solve the optimization problem. Unlike all the\nregression based methods, our method can transparently deal with path dependent\noptions without extra computations and a parallel implementation writes easily\nwith very little communication and no centralized work. We test our approach on\nseveral multi--dimensional options with up to 40 assets and show the impressive\nscalability of the parallel implementation.\n"
    },
    {
        "paper_id": 1604.03337,
        "authors": "Dominique Pepin (CRIEF)",
        "title": "The subjective discount factor and the coefficient of relative risk\n  aversion under time-additive isoelastic expected utility model",
        "comments": null,
        "journal-ref": "Economics Bulletin, Economics Bulletin, 2016, 36 (2)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By analysing the restrictions that ensure the existence of capital market\nequilibrium, we show that the coefficient of relative risk aversion and the\nsubjective discount factor cannot be high simultaneously as they are supposed\nto be to make the standard asset pricing consistent with financial stylised\nfacts.\n"
    },
    {
        "paper_id": 1604.03522,
        "authors": "Tanya Ara\\'ujo and M. Ennes Ferreira",
        "title": "The Topology of African Exports: emerging patterns on spanning trees",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.06.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is a contribution to interweaving two lines of research that have\nprogressed in separate ways: network analyses of international trade and the\nliterature on African trade and development. Gathering empirical data on\nAfrican countries has important limitations and so does the space occupied by\nAfrican countries in the analyses of trade networks. Here, these limitations\nare dealt with by the definition of two independent bipartite networks: a\ndestination share network and\\ a\\ commodity share network. These networks -\ntogether with their corresponding minimal spanning trees - allow to uncover\nsome ordering emerging from African exports in the broader context of\ninternational trade. The emerging patterns help to understand important\ncharacteristics of African exports and its binding relations to other economic,\ngeographic and organizational concerns as the recent literature on African\ntrade, development and growth has shown.\n"
    },
    {
        "paper_id": 1604.03776,
        "authors": "Daniel Kosiorowski, Jerzy P. Rydlewski, Ma{\\l}gorzata Snarska",
        "title": "Detecting a Structural Change in Functional Time Series Using Local\n  Wilcoxon Statistic",
        "comments": "17 pages, 19 figures, LaTeX svjour3 class The final publication is\n  available at link.springer.com DOI: 10.1007/s00362-017-0891-y",
        "journal-ref": "Statistical Papers, October 2019, Volume 60, Issue 5, pp 1677 -\n  1698",
        "doi": "10.1007/s00362-017-0891-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Functional data analysis (FDA) is a part of modern multivariate statistics\nthat analyses data providing information about curves, surfaces or anything\nelse varying over a certain continuum. In economics and empirical finance we\noften have to deal with time series of functional data, where we cannot easily\ndecide, whether they are to be considered as homogeneous or heterogeneous. At\npresent a discussion on adequate tests of homogenity for functional data is\ncarried. We propose a novel statistic for detetecting a structural change in\nfunctional time series based on a local Wilcoxon statistic induced by a local\ndepth function proposed by Paindaveine and Van Bever (2013).\n"
    },
    {
        "paper_id": 1604.03906,
        "authors": "Erhan Bayraktar and Jiaqi Li",
        "title": "Stochastic Perron for Stochastic Target Problems",
        "comments": "Final version. To appear in the Journal of Optimization Theory and\n  Applications. Keywords: The stochastic target problem, stochastic Perron's\n  method, jump-diffusion processes, viscosity solutions, unbounded controls",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we adapt stochastic Perron's method to analyze a stochastic\ntarget problem with unbounded controls in a jump diffusion set-up. With this\nmethod, we construct a viscosity sub-solution and super-solution to the\nassociated Hamiltonian-Jacobi-Bellman (HJB) equations. Under comparison\nprinciples, uniqueness of the viscosity solutions holds and the value function\ncoincides with the unique solution in the parabolic interior. Since classical\ncontrol problems can be analyzed under the framework of stochastic target\nproblems (with unbounded controls), we use our results to generalize the\nresults in ArXiv:1212.2170 to problems with controlled jumps.\n"
    },
    {
        "paper_id": 1604.03996,
        "authors": "Leopoldo S\\'anchez-Cant\\'u, Carlos Arturo Soto-Campos, Andriy Kryvko",
        "title": "Evidence of Self-Organization in Time Series of Capital Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A methodology is developed to identify, as units of study, each decrease in\nthe value of a stock from a given maximum price level. A critical level in the\namount of price declines is found to separate a segment operating under a\nrandom walk from a segment operating under a power law. This level is\ninterpreted as a point of phase transition into a self-organized system.\nEvidence of self-organization was found in all the stock market indices studied\nbut in none of the control synthetic random series. Findings partially explain\nthe fractal structure characteristic of financial time series and suggest that\nprice fluctuations adopt two different operating regimes. We propose to\nidentify downward movements larger than the critical level apparently subject\nto the power law, as self-organized states, and price decreases smaller than\nthe critical level, as a random walk with the Markov property.\n"
    },
    {
        "paper_id": 1604.04223,
        "authors": "Andrea C. Levi, Ubaldo Garibaldi",
        "title": "On the survival of poor peasants",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previously, in underdeveloped countries, people tried to keep the prices of\nfood products artificially low, in order to help the poor to buy their food.\nBut it became soon clear that such system, although helpful for the city poor,\nwas disastrous for the peasants (who usually are even poorer), so that hunger\nincreased, instead of decreasing. More recently, thus, higher prices have been\nimposed. But a high-price system does not solve the problems. It helps, indeed,\na peasant to buy in the city non-edible products, but not to buy (more\nexpensive) food products from other peasants. The question is discussed here in\nmore detail starting from the simplest conceivable case of two peasants\nproducing each a different food product (bread and cheese, say), then\ngeneralizing to several food items and to any number of peasants producing a\ngiven food item j. Like in every economic system which wants to be sustainable,\nor able to reproduce itself in a stationary state at least, prices are\ndetermined by the necessity of exchanging \"means of production\" among\n\"industries\", except that here industriesare replaced by working peasants and\nmeans of production are replaced by food. It is found that prices must obey\ncertain inequalities related to the minimal amount of each food item necessary\nfor survival. Inequalities may be rewritten as equations and, in an important\nspecial case, such equations give rise to a simple version of the matrix\nequation used by famous authors to describe the economy.\n"
    },
    {
        "paper_id": 1604.04312,
        "authors": "Eamon Duede and Victor Zhorin",
        "title": "Convergence of Economic Growth and the Great Recession as Seen From a\n  Celestial Observatory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Macroeconomic theories of growth and wealth distribution have an outsized\ninfluence on national and international social and economic policies. Yet, due\nto a relative lack of reliable, system wide data, many such theories remain, at\nbest, unvalidated and, at worst, misleading. In this paper, we introduce a\nnovel economic observatory and framework enabling high resolution comparisons\nand assessments of the distributional impact of economic development through\nthe remote sensing of planet earth's surface. Striking visual and empirical\nvalidation is observed for a broad, global macroeconomic sigma-convergence in\nthe period immediately following the end of the Cold War. What is more, we\nobserve strong empirical evidence that the mechanisms driving sigma-convergence\nfailed immediately after the financial crisis and the start of the Great\nRecession. Nevertheless, analysis of both cross-country and cross-state samples\nindicates that, globally, disproportionately high growth levels and excessively\nhigh decay levels have become rarer over time. We also see that urban areas,\nespecially concentrated within short distances of major capital cities were\nmore likely than rural or suburban areas to see relatively high growth in the\naftermath of the financial crisis. Observed changes in growth polarity can be\nattributed plausibly to post-crisis government intervention and subsidy\npolicies introduced around the world. Overall, the data and techniques we\npresent here make economic evidence for the rise of China, the decline of U.S.\nmanufacturing, the euro crisis, the Arab Spring, and various, recent, Middle\nEast conflicts visually evident for the first time.\n"
    },
    {
        "paper_id": 1604.04608,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "Super-hedging American Options with Semi-static Trading Strategies under\n  Model Uncertainty",
        "comments": "Final version. To appear in the International Journal of Theoretical\n  and Applied Finance. Keywords: American options, super-hedging, model\n  uncertainty, semi-static trading strategies, randomized models",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the super-hedging price of an American option in a discrete-time\nmarket in which stocks are available for dynamic trading and European options\nare available for static trading. We show that the super-hedging price $\\pi$ is\ngiven by the supremum over the prices of the American option under randomized\nmodels. That is, $\\pi=\\sup_{(c_i,Q_i)_i}\\sum_ic_i\\phi^{Q_i}$, where $c_i \\in\n\\mathbb{R}_+$ and the martingale measure $Q^i$ are chosen such that $\\sum_i\nc_i=1$ and $\\sum_i c_iQ_i$ prices the European options correctly, and\n$\\phi^{Q_i}$ is the price of the American option under the model $Q_i$. Our\nresult generalizes the example given in ArXiv:1604.02274 that the highest model\nbased price can be considered as a randomization over models.\n"
    },
    {
        "paper_id": 1604.04872,
        "authors": "Ravi Kashyap",
        "title": "Solving the Equity Risk Premium Puzzle and Inching Towards a Theory of\n  Everything",
        "comments": null,
        "journal-ref": "Journal of Private Equity, Spring 2018, Institutional Investor\n  Journals, New York, USA, Vol. 21, No. 2, pp. 45-63;",
        "doi": "10.3905/jpe.2018.21.2.045",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The equity risk premium puzzle is that the return on equities has far\nexceeded the average return on short-term risk-free debt and cannot be\nexplained by conventional representative-agent consumption based equilibrium\nmodels. We review a few attempts done over the years to explain this anomaly:\n1. Inclusion of highly unlikely events with low probability (Ugly state along\nwith Good and Bad), or market crashes / Black Swans. 2. Slow moving habit, or\ntime-varying subsistence level, added to the basic power utility function. 3. A\nseparation of the inter-temporal elasticity of substitution and risk aversion,\ncombined with long run risks which captures time varying economic uncertainty.\nWe explore whether a fusion of the above approaches supplemented with better\nmethods to handle the below reservations would provide a more realistic and yet\ntractable framework to tackle the various conundrums in the social sciences: 1.\nUnlimited ability of individuals to invest as compared to their ability to\nconsume. 2. Lack of an objective measuring stick of value 3. Unintended\nconsequences due to the dynamic nature of social systems 4. Relaxation of the\ntransversality condition to avoid the formation of asset price bubbles 5. How\ndurable is durable? Accounting for durable goods since nothing lasts forever\nThe world we live in produces fascinating phenomenon despite (or perhaps, due\nto) being a hotchpotch of varying doses of the above elements. The rationale\nfor a unified theory is that beauty can emerge from chaos since the best test\nfor a stew is its taste. Many long standing puzzles seem to have been resolved\nusing different techniques. The various explanations need to stand the test of\ntime before acceptance; but then unexpected outcomes set in and new puzzles\nemerge. As real analysis and limits tell us: We are getting Closer and Closer;\nYet it seems we are still Far Far Away...\n"
    },
    {
        "paper_id": 1604.04963,
        "authors": "Brian Bulthuis and Julio Concha and Tim Leung and Brian Ward",
        "title": "Optimal Execution of Limit and Market Orders with Trade Director, Speed\n  Limiter, and Fill Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal execution of market and limit orders with permanent and\ntemporary price impacts as well as uncertainty in the filling of limit orders.\nOur continuous-time model incorporates a trade speed limiter and a trader\ndirector to provide better control on the trading rates. We formulate a\nstochastic control problem to determine the optimal dynamic strategy for trade\nexecution, with a quadratic terminal penalty to ensure complete liquidation. In\naddition, we identify conditions on the model parameters to ensure optimality\nof the controls and finiteness of the associated value functions. For\ncomparison, we also solve the schedule-following optimal execution problem that\npenalizes deviations from an order schedule. Numerical results are provided to\nillustrate the optimal market and limit orders over time.\n"
    },
    {
        "paper_id": 1604.05178,
        "authors": "Yuri M. Dimitrov, Lubin G. Vulkov",
        "title": "High order finite difference schemes on non-uniform meshes for the\n  time-fractional Black-Scholes equation",
        "comments": "9 pages, 2 figure, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a three-point compact finite difference scheme on a non-uniform\nmesh for the time-fractional Black-Scholes equation. We show that for special\ngraded meshes used in finance, the Tavella-Randall and the quadratic meshes the\nnumerical solution has a fourth-order accuracy in space. Numerical experiments\nare discussed.\n"
    },
    {
        "paper_id": 1604.05404,
        "authors": "Wujiang Lou",
        "title": "Repo Haircuts and Economic Capital: A Theory of Repo Pricing",
        "comments": "45 pages, 5 figures, 9 data tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A repurchase agreement lets investors borrow cash to buy securities.\nFinancier only lends to securities' market value after a haircut and charges\ninterest. Repo pricing is characterized with its puzzling dual pricing\nmeasures: repo haircut and repo spread. This article develops a repo haircut\nmodel by designing haircuts to achieve high credit criteria, and identifies\neconomic capital for repo's default risk as the main driver of repo pricing. A\nsimple repo spread formula is obtained that relates spread to haircuts negative\nlinearly. An investor wishing to minimize all-in funding cost can settle at an\noptimal combination of haircut and repo rate. The model empirically reproduces\nrepo haircut hikes concerning asset backed securities during the financial\ncrisis. It explains tri-party and bilateral repo haircut differences,\nquantifies shortening tenor's risk reduction effect, and sets a limit on excess\nliquidity intermediating dealers can extract between money market funds and\nhedge funds.\n"
    },
    {
        "paper_id": 1604.05406,
        "authors": "Wujiang Lou",
        "title": "Gap Risk KVA and Repo Pricing: An Economic Capital Approach in the\n  Black-Scholes-Merton Framework",
        "comments": "25 pages, 2 figures, Risk, November 2016, under the title of \"Gap\n  Risk KVA and Repo Pricing\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although not a formal pricing consideration, gap risk or hedging errors are\nthe norm of derivatives businesses. Starting with the gap risk during a margin\nperiod of risk of a repurchase agreement (repo), this article extends the\nBlack-Scholes-Merton option pricing framework by introducing a reserve capital\napproach to the hedging error's irreducible variability. An extended partial\ndifferential equation is derived with two new terms for expected gap loss and\neconomic capital charge, leading to the gap risk economic value adjustment and\ncapital valuation adjustment (KVA) respectively. Practical repo pricing\nformulae is obtained showing that the break-even repo rate decomposes into cost\nof fund and economic capital charge in KVA. At zero haircut, a one-year term\nrepo on main equities could command a capital charge as large as 50 basis\npoints for a 'BBB' rated borrower.\n"
    },
    {
        "paper_id": 1604.05517,
        "authors": "Anna Aksamit and Shuoqing Deng and Jan Ob\\l\\'oj and Xiaolu Tan",
        "title": "Robust pricing--hedging duality for American options in discrete time\n  financial markets",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate pricing-hedging duality for American options in discrete time\nfinancial models where some assets are traded dynamically and others, e.g. a\nfamily of European options, only statically. In the first part of the paper we\nconsider an abstract setting, which includes the classical case with a fixed\nreference probability measure as well as the robust framework with a\nnon-dominated family of probability measures. Our first insight is that by\nconsidering a (universal) enlargement of the space, we can see American options\nas European options and recover the pricing-hedging duality, which may fail in\nthe original formulation. This may be seen as a weak formulation of the\noriginal problem. Our second insight is that lack of duality is caused by the\nlack of dynamic consistency and hence a different enlargement with dynamic\nconsistency is sufficient to recover duality: it is enough to consider\n(fictitious) extensions of the market in which all the assets are traded\ndynamically. In the second part of the paper we study two important examples of\nrobust framework: the setup of Bouchard and Nutz (2015) and the martingale\noptimal transport setup of Beiglb\\\"ock et al. (2013), and show that our general\nresults apply in both cases and allow us to obtain pricing-hedging duality for\nAmerican options.\n"
    },
    {
        "paper_id": 1604.05584,
        "authors": "Thai Nguyen",
        "title": "Optimal investment and consumption with downside risk constraint in\n  jump-diffusion models",
        "comments": "32 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the results of the article [C. Kl\\\"{u}ppelberg and S. M.\nPergamenchtchikov. Optimal consumption and investment with bounded downside\nrisk for power utility functions. In Optimality and Risk: {\\it Modern Trends in\nMathematical Finance. The Kabanov Festschrift}, pages 133-169, 2009] to a\njump-diffusion setting. We show that under the assumption that only positive\njumps in the asset prices are allowed, the explicit optimal strategy can be\nfound in the subset of admissible strategies satisfying the same risk\nconstraint as in the pure diffusion setting. When negative jumps probably\nhappen, the regulator should be more conservative. In that case, we suggest to\nimpose on the investor's portfolio a stricter constraint which depends on the\nprobability of having negative jumps in the assets during the whole considered\nhorizon.\n"
    },
    {
        "paper_id": 1604.05598,
        "authors": "Holger Fink, Yulia Klimova, Claudia Czado, Jakob St\\\"ober",
        "title": "Regime switching vine copula models for global equity and volatility\n  indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For nearly every major stock market there exist equity and implied volatility\nindices. These play important roles within finance: be it as a benchmark, a\nmeasure of general uncertainty or a way of investing or hedging. It is well\nknown in the academic literature, that correlations and higher moments between\ndifferent indices tend to vary in time. However, to the best of our knowledge,\nno one has yet considered a global setup including both, equity and implied\nvolatility indices of various continents, and allowing for a changing\ndependence structure. We aim to close this gap by applying Markov-switching\n$R$-vine models to investigate the existence of different, global dependence\nregimes. In particular, we identify times of \"normal\" and \"abnormal\" states\nwithin a data set consisting of North-American, European and Asian indices. Our\nresults confirm the existence of joint points in time at which global regime\nswitching takes place.\n"
    },
    {
        "paper_id": 1604.05672,
        "authors": "Julien Blasco, Graciela Chichilnisky",
        "title": "Risk Aversion and Catastrophic Risks: the Pill Experiment",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article focuses on the work of O. Chanel and G. Chichilnisky (2013) on\nthe flaws of expected utility theory while assessing the value of life.\nExpected utility is a fundamental tool in decision theory. However, it does not\nfit with the experimental results when it comes to catastrophic outcomes\n---see, for example, Chichilnisky (2009) for more details. In the experiments\nconducted by Olivier Chanel in 1998 and 2009, several subjects are ask to\nimagine they are presented 1 billion identical pills. They are paid \\$220,000\nto take and swallow one, knowing that one out of 1 billion is deadly. The\nobjective of this article is to show that risk aversion phenomenon cannot\nexplain the experimental results found. This is an additional reason why a new\nkind of utility function is necessary: the axioms proposed by Graciela\nChichilnisky will be briefly presented, and it will be shown that it better\nfits with experiments than any risk aversion utility function.\n"
    },
    {
        "paper_id": 1604.05771,
        "authors": "Pierre-Andr\\'e Chiappori, Robert McCann, Brendan Pass",
        "title": "Multidimensional matching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general analysis of multidimensional matching problems with\ntransferable utility, paying particular attention to the case in which the\ndimensions of heterogeneity on the two sides of the market are unequal. A\nparticular emphasis is put on problems where agents on one side of the market\nare multidimensional and agents on the other side are uni-dimensional, we\ndescribe a general approach to solve such problems. Lastly, we analyze several\nexamples, including an hedonic model with differentiated products, a marriage\nmarket model where wives are differentiated in income and fertility, and a\ncompetitive variation of the Rochet-Chon\\'e problem. In the latter example, we\nshow that the bunching phenomena, observed by Rochet and Chon\\'e in the\nmonopoly context, do not occur in the competitive context\n"
    },
    {
        "paper_id": 1604.05896,
        "authors": "Antti J. Tanskanen, Jani Lukkarinen, Kari Vatanen",
        "title": "Random selection of factors preserves the correlation structure in a\n  linear factor model to a high degree",
        "comments": "27 pages, 7 figures, ver 2: includes major revision of the abstract,\n  introduction and discussion, ver 3: title changed to match the final\n  published version (available as an open access file following the DOI link)",
        "journal-ref": "PLoS ONE (2018) 13(12): e0206551",
        "doi": "10.1371/journal.pone.0206551",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a very high-dimensional vector space, two randomly-chosen vectors are\nalmost orthogonal with high probability. Starting from this observation, we\ndevelop a statistical factor model, the random factor model, in which factors\nare chosen at random based on the random projection method. Randomness of\nfactors has the consequence that covariance matrix is well preserved in a\nlinear factor representation. It also enables derivation of probabilistic\nbounds for the accuracy of the random factor representation of time-series,\ntheir cross-correlations and covariances. As an application, we analyze\nreproduction of time-series and their cross-correlation coefficients in the\nwell-diversified Russell 3,000 equity index.\n"
    },
    {
        "paper_id": 1604.06284,
        "authors": "Viktor Stojkoski, Zoran Utkovski, Ljupco Kocarev",
        "title": "The Impact of Services on Economic Complexity: Service Sophistication as\n  Route for Economic Growth",
        "comments": "High quality figures available upon request",
        "journal-ref": "PLoS ONE 11(8) 2016",
        "doi": "10.1371/journal.pone.0161633",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic complexity reflects the amount of knowledge that is embedded in the\nproductive structure of an economy. By combining tools from network science and\neconometrics, a robust and stable relationship between a country's productive\nstructure and its economic growth has been established. Here we report that not\nonly goods but also services are important for predicting the rate at which\ncountries will grow. By adopting a terminology which classifies manufactured\ngoods and delivered services as products, we investigate the influence of\nservices on the country's productive structure. In particular, we provide\nevidence that complexity indices for services are in general higher than those\nfor goods, which is reflected in a general tendency to rank countries with\ndeveloped service sector higher than countries with economy centred on\nmanufacturing of goods. By focusing on country dynamics based on experimental\ndata, we investigate the impact of services on the economic complexity of\ncountries measured in the product space (consisting of both goods and\nservices). Importantly, we show that diversification of service exports and its\nsophistication can provide an additional route for economic growth in both\ndeveloping and developed countries.\n"
    },
    {
        "paper_id": 1604.06342,
        "authors": "N Baradel (CEREMADE, CREST), B Bouchard (CEREMADE), Ngoc Minh Dang",
        "title": "Optimal trading with online parameters revisions",
        "comments": "arXiv admin note: text overlap with arXiv:1604.06340",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to explain how parameters adjustments can be\nintegrated in the design or the control of automates of trading. Typically, we\nare interested by the online estimation of the market impacts generated by\nrobots or single orders, and how they/the controller should react in an optimal\nway to the informations generated by the observation of the realized impacts.\nThis can be formulated as an optimal impulse control problem with unknown\nparameters, on which a prior is given. We explain how a mix of the classical\nBayesian updating rule and of optimal control techniques allows one to derive\nthe dynamic programming equation satisfied by the corresponding value function,\nfrom which the optimal policy can be inferred. We provide an example of\nconvergent finite difference scheme and consider typical examples of\napplications.\n"
    },
    {
        "paper_id": 1604.06609,
        "authors": "Huy\\^en Pham (LPMA, CREST)",
        "title": "Linear quadratic optimal control of conditional McKean-Vlasov equation\n  with random coefficients and applications *",
        "comments": "to appear in Probability, Uncertainty and Quantitative Risk",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal control problem for a linear conditional\nMcKean-Vlasov equation with quadratic cost functional. The coefficients of the\nsystem and the weigh-ting matrices in the cost functional are allowed to be\nadapted processes with respect to the common noise filtration. Semi closed-loop\nstrategies are introduced, and following the dynamic programming approach in\n[32], we solve the problem and characterize time-consistent optimal control by\nmeans of a system of decoupled backward stochastic Riccati differential\nequations. We present several financial applications with explicit solutions,\nand revisit in particular optimal tracking problems with price impact, and the\nconditional mean-variance portfolio selection in incomplete market model.\n"
    },
    {
        "paper_id": 1604.06629,
        "authors": "Giulio Cimini and Matteo Serri",
        "title": "Entangling credit and funding shocks in interbank markets",
        "comments": null,
        "journal-ref": "PLoS ONE 11(8): e0161642 (2016)",
        "doi": "10.1371/journal.pone.0161642",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit and liquidity risks represent main channels of financial contagion for\ninterbank lending markets. On one hand, banks face potential losses whenever\ntheir counterparties are under distress and thus unable to fulfill their\nobligations. On the other hand, solvency constraints may force banks to recover\nlost fundings by selling their illiquid assets, resulting in effective losses\nin the presence of fire sales - that is, when funding shortcomings are\nwidespread over the market. Because of the complex structure of the network of\ninterbank exposures, these losses reverberate among banks and eventually get\namplified, with potentially catastrophic consequences for the whole financial\nsystem. Building on Debt Rank [Battiston et al., 2012], in this work we define\na systemic risk metric that estimates the potential amplification of losses in\ninterbank markets accounting for both credit and liquidity contagion channels:\nthe Debt-Solvency Rank. We implement this framework on a dataset of 183\nEuropean banks that were publicly traded between 2004 and 2013, showing indeed\nthat liquidity spillovers substantially increase systemic risk, and thus cannot\nbe neglected in stress-test scenarios. We also provide additional evidence that\nthe interbank market was extremely fragile up to the 2008 financial crisis,\nbecoming slightly more robust only afterwards.\n"
    },
    {
        "paper_id": 1604.06892,
        "authors": "Ewa Marciniak and Zbigniew Palmowski",
        "title": "On the Optimal Dividend Problem for Insurance Risk Models with\n  Surplus-Dependent Premiums",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns an optimal dividend distribution problem for an insurance\ncompany with surplus-dependent premium. In the absence of dividend payments,\nsuch a risk process is a particular case of so-called piecewise deterministic\nMarkov processes. The control mechanism chooses the size of dividend payments.\nThe objective consists in maximazing the sum of the expected cumulative\ndiscounted dividend payments received until the time of ruin and a penalty\npayment at the time of ruin, which is an increasing function of the size of the\nshortfall at ruin. A complete solution is presented to the corresponding\nstochastic control problem. We identify the associated Hamilton-Jacobi-Bellman\nequation and find necessary and sufficient conditions for optimality of a\nsingle dividend-band strategy, in terms of particular Gerber-Shiu functions. A\nnumber of concrete examples are analyzed.\n"
    },
    {
        "paper_id": 1604.06917,
        "authors": "Joachim Sicking, Thomas Guhr and Rudi Sch\\\"afer",
        "title": "Concurrent Credit Portfolio Losses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of concurrent portfolio losses in two non-overlapping\ncredit portfolios. In order to explore the full statistical dependence\nstructure of such portfolio losses, we estimate their empirical pairwise\ncopulas. Instead of a Gaussian dependence, we typically find a strong asymmetry\nin the copulas. Concurrent large portfolio losses are much more likely than\nsmall ones. Studying the dependences of these losses as a function of portfolio\nsize, we moreover reveal that not only large portfolios of thousands of\ncontracts, but also medium-sized and small ones with only a few dozens of\ncontracts exhibit notable portfolio loss correlations. Anticipated\nidiosyncratic effects turn out to be negligible. These are troublesome insights\nnot only for investors in structured fixed-income products, but particularly\nfor the stability of the financial sector.\n"
    },
    {
        "paper_id": 1604.07042,
        "authors": "Sylvia Gottschalk",
        "title": "Entropy and credit risk in highly correlated markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.02.083",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare two models of corporate default by calculating the\nJeffreys-Kullback-Leibler divergence between their predicted default\nprobabilities when asset correlations are either high or low. Our main results\nshow that the divergence between the two models increases in highly correlated,\nvolatile, and large markets, but that it is closer to zero in small markets,\nwhen asset correlations are low and firms are highly leveraged. These findings\nsuggest that during periods of financial instability the single-and\nmulti-factor models of corporate default will generate increasingly\ninconsistent predictions.\n"
    },
    {
        "paper_id": 1604.07556,
        "authors": "Damian Eduardo Taranto, Giacomo Bormetti, Jean-Philippe Bouchaud,\n  Fabrizio Lillo, Bence Toth",
        "title": "Linear models for the impact of order flow on prices II. The Mixture\n  Transition Distribution model",
        "comments": "23 pages, 7 figures, and 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling the impact of the order flow on asset prices is of primary\nimportance to understand the behavior of financial markets. Part I of this\npaper reported the remarkable improvements in the description of the price\ndynamics which can be obtained when one incorporates the impact of past returns\non the future order flow. However, impact models presented in Part I consider\nthe order flow as an exogenous process, only characterized by its two-point\ncorrelations. This assumption seriously limits the forecasting ability of the\nmodel. Here we attempt to model directly the stream of discrete events with a\nso-called Mixture Transition Distribution (MTD) framework, introduced\noriginally by Raftery (1985). We distinguish between price-changing and non\nprice-changing events and combine them with the order sign in order to reduce\nthe order flow dynamics to the dynamics of a four-state discrete random\nvariable. The MTD represents a parsimonious approximation of a full high-order\nMarkov chain. The new approach captures with adequate realism the conditional\ncorrelation functions between signed events for both small and large tick\nstocks and signature plots. From a methodological viewpoint, we discuss a novel\nand flexible way to calibrate a large class of MTD models with a very large\nnumber of parameters. In spite of this large number of parameters, an\nout-of-sample analysis confirms that the model does not overfit the data.\n"
    },
    {
        "paper_id": 1604.0769,
        "authors": "Jani Lukkarinen, Mikko S. Pakkanen",
        "title": "Arbitrage without borrowing or short selling?",
        "comments": "14 pages, 1 figure, v2: minor revision, to appear in Mathematics and\n  Financial Economics",
        "journal-ref": "Mathematics and Financial Economics 2017, Vol. 11, No. 3, 263-274",
        "doi": "10.1007/s11579-016-0180-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that a trader, who starts with no initial wealth and is not allowed\nto borrow money or short sell assets, is theoretically able to attain positive\nwealth by continuous trading, provided that she has perfect foresight of future\nasset prices, given by a continuous semimartingale. Such an arbitrage strategy\ncan be constructed as a process of finite variation that satisfies a seemingly\ninnocuous self-financing condition, formulated using a pathwise\nRiemann-Stieltjes integral. Our result exemplifies the potential intricacies of\nformulating economically meaningful self-financing conditions in continuous\ntime, when one leaves the conventional arbitrage-free framework.\n"
    },
    {
        "paper_id": 1604.07782,
        "authors": "Leno S. Rocha, Frederico S. A. Rocha and Th\\'arsis T. P. Souza",
        "title": "Is the public sector of your country a diffusion borrower? Empirical\n  evidence from Brazil",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0185257",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose a diffusion process to describe the global dynamic evolution of\ncredit operations at a national level given observed operations at a\nsubnational level in a sovereign country. Empirical analysis with a unique\ndataset from Brazilian federate constituents supports the conclusions. Despite\nthe heterogeneity observed in credit operations at a subnational level, the\naggregated dynamics at a national level were accurately described with the\nproposed model. Results may guide management of public finances, particularly\ndebt manager authorities in charge of reaching surplus targets.\n"
    },
    {
        "paper_id": 1604.07969,
        "authors": "Keren Shen, Jianfeng Yao and Wai Keung Li",
        "title": "On the Surprising Explanatory Power of Higher Realized Moments in\n  Practice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Realized moments of higher order computed from intraday returns are\nintroduced in recent years. The literature indicates that realized skewness is\nan important factor in explaining future asset returns. However, the literature\nmainly focuses on the whole market and on the monthly or weekly scale. In this\npaper, we conduct an extensive empirical analysis to investigate the\nforecasting abilities of realized skewness and realized kurtosis towards\nindividual stock's future return and variance in the daily scale. It is found\nthat realized kurtosis possesses significant forecasting power for the stock's\nfuture variance. In the meanwhile, realized skewness is lack of explanatory\npower for the future daily return for individual stocks with a short horizon,\nin contrast with the existing literature.\n"
    },
    {
        "paper_id": 1604.08037,
        "authors": "Martijn Pistorius and Mitja Stadje",
        "title": "On Dynamic Deviation Measures and Continuous-Time Portfolio Optimisation",
        "comments": "28pp",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose the notion of dynamic deviation measure, as a\ndynamic time-consistent extension of the (static) notion of deviation measure.\nTo achieve time-consistency we require that a dynamic deviation measures\nsatisfies a generalised conditional variance formula. We show that, under a\ndomination condition, dynamic deviation measures are characterised as the\nsolutions to a certain class of backward SDEs. We establish for any dynamic\ndeviation measure an integral representation, and derive a dual\ncharacterisation result in terms of additively $m$-stable dual sets. Using this\nnotion of dynamic deviation measure we formulate a dynamic mean-deviation\nportfolio optimisation problem in a jump-diffusion setting and identify a\nsubgame-perfect Nash equilibrium strategy that is linear as function of wealth\nby deriving and solving an associated extended HJB equation.\n"
    },
    {
        "paper_id": 1604.0807,
        "authors": "Birgit Rudloff",
        "title": "Convex Hedging in Incomplete Markets",
        "comments": null,
        "journal-ref": "Applied Mathematical Finance 14 (5), 437 - 452, 2007",
        "doi": "10.1080/13504860701352206",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In incomplete financial markets not every contingent claim can be replicated\nby a self-financing strategy. The risk of the resulting shortfall can be\nmeasured by convex risk measures, recently introduced by F\\\"ollmer, Schied\n(2002). The dynamic optimization problem of finding a self-financing strategy\nthat minimizes the convex risk of the shortfall can be split into a static\noptimization problem and a representation problem. It follows that the optimal\nstrategy consists in superhedging the modified claim $\\widetilde{\\varphi}H$,\nwhere $H$ is the payoff of the claim and $\\widetilde{\\varphi}$ is the solution\nof the static optimization problem, the optimal randomized test.\n  In this paper, we will deduce necessary and sufficient optimality conditions\nfor the static problem using convex duality methods. The solution of the static\noptimization problem turns out to be a randomized test with a typical\n$0$-$1$-structure.\n"
    },
    {
        "paper_id": 1604.08224,
        "authors": "Yiqing Lin and Junjian Yang",
        "title": "Utility maximization problem with random endowment and transaction\n  costs: when wealth may become negative",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the problem of maximizing expected utility from the\nterminal wealth with proportional transaction costs and random endowment. In\nthe context of the existence of consistent price systems, we consider the\nduality between the primal utility maximization problem and the dual one, which\nis set up on the domain of finitely additive measures. In particular, we prove\nduality results for utility functions supporting possibly negative values.\nMoreover, we construct the shadow market by the dual optimal process and\nconsider the utility based pricing for random endowment.\n"
    },
    {
        "paper_id": 1604.08677,
        "authors": "Du Nguyen",
        "title": "An Explicit Formula for Likelihood Function for Gaussian Vector\n  Autoregressive Moving-Average Model Conditioned on Initial Observables with\n  Application to Model Calibration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an explicit formula for likelihood function for Gaussian VARMA\nmodel conditioned on initial observables where the moving-average (MA)\ncoefficients are scalar. For fixed MA coefficients the likelihood function is\noptimized in the autoregressive variables $\\Phi$'s by a closed form formula\ngeneralizing regression calculation of the VAR model with the introduction of\nan inner product defined by MA coefficients. We show the assumption of scalar\nMA coefficients is not restrictive and this formulation of the VARMA model\nshares many nice features of VAR and MA model. The gradient and Hessian could\nbe computed analytically. The likelihood function is preserved under the root\ninvertion maps of the MA coefficients. We discuss constraints on the gradient\nof the likelihood function with moving average unit roots. With the help of FFT\nthe likelihood function could be computed in $O((kp+1)^2T +ckT\\log(T))$ time.\nNumerical calibration is required for the scalar MA variables only. The\napproach can be generalized to include additional drifts as well as integrated\ncomponents. We discuss a relationship with the Borodin-Okounkov formula and the\ncase of infinite MA components.\n"
    },
    {
        "paper_id": 1604.08735,
        "authors": "Anastasia Borovykh, Cornelis W. Oosterlee, Andrea Pascucci",
        "title": "Pricing Bermudan options under local L\\'evy models with default",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a defaultable asset whose risk-neutral pricing dynamics are\ndescribed by an exponential L\\'evy-type martingale. This class of models allows\nfor a local volatility, local default intensity and a locally dependent L\\'evy\nmeasure. We present a pricing method for Bermudan options based on an\nanalytical approximation of the characteristic function combined with the COS\nmethod. Due to a special form of the obtained characteristic function the price\ncan be computed using a Fast Fourier Transform-based algorithm resulting in a\nfast and accurate calculation. The Greeks can be computed at almost no\nadditional computational cost. Error bounds for the approximation of the\ncharacteristic function as well as for the total option price are given.\n"
    },
    {
        "paper_id": 1604.08743,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Factor Models for Cancer Signatures",
        "comments": "70 pages, 21 figures; a few trivial typos corrected",
        "journal-ref": "Physica A 462 (2016) 527-559",
        "doi": "10.1016/j.physa.2016.06.089",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel method for extracting cancer signatures by applying\nstatistical risk models (http://ssrn.com/abstract=2732453) from quantitative\nfinance to cancer genome data. Using 1389 whole genome sequenced samples from\n14 cancers, we identify an \"overall\" mode of somatic mutational noise. We give\na prescription for factoring out this noise and source code for fixing the\nnumber of signatures. We apply nonnegative matrix factorization (NMF) to genome\ndata aggregated by cancer subtype and filtered using our method. The resultant\nsignatures have substantially lower variability than those from unfiltered\ndata. Also, the computational cost of signature extraction is cut by about a\nfactor of 10. We find 3 novel cancer signatures, including a liver cancer\ndominant signature (96% contribution) and a renal cell carcinoma signature (70%\ncontribution). Our method accelerates finding new cancer signatures and\nimproves their overall stability. Reciprocally, the methods for extracting\ncancer signatures could have interesting applications in quantitative finance.\n"
    },
    {
        "paper_id": 1604.08824,
        "authors": "Radu T. Pruna, Maria Polukarov and Nicholas R. Jennings",
        "title": "A new structural stochastic volatility model of asset pricing and its\n  stylized facts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Building on a prominent agent-based model, we present a new structural\nstochastic volatility asset pricing model of fundamentalists vs. chartists\nwhere the prices are determined based on excess demand. Specifically, this\nallows for modelling stochastic interactions between agents, based on a herding\nprocess corrected by a price misalignment, and incorporating strong noise\ncomponents in the agents' demand. The model's parameters are estimated using\nthe method of simulated moments, where the moments reflect the basic properties\nof the daily returns of a stock market index. In addition, for the first time\nwe apply a (parametric) bootstrap method in a setting where the switching\nbetween strategies is modelled using a discrete choice approach. As we\ndemonstrate, the resulting dynamics replicate a rich set of the stylized facts\nof the daily financial data including: heavy tails, volatility clustering, long\nmemory in absolute returns, as well as the absence of autocorrelation in raw\nreturns, volatility-volume correlations, aggregate Gaussianity, concave price\nimpact and extreme price events.\n"
    },
    {
        "paper_id": 1604.08895,
        "authors": "Christian Mueller-Kademann",
        "title": "The puzzle that just isn't",
        "comments": "25 pages, 2 figures, with a quote from Wonko the Sane",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In his stimulating article on the reasons for two puzzling observations about\nthe behaviour of interest rates, exchange rates and the rate of inflation,\nCharles Engel (2016) puts forward an explanation that rests on the concept of a\nnon-pecuniary liquidity return on assets. Albeit intriguing the analysis\nstruggles to account for a number of facts which are familiar to participants\nof the foreign exchange and bond markets. Reconciling these facts in\nconjunction with a careful dissection of the \"puzzle\" to begin with, shows that\nthe forward premium puzzle just does not exist, at least not in its canonical\nform.\n"
    },
    {
        "paper_id": 1605.00039,
        "authors": "Ren\\'e A\\\"id, Matteo Basei, Giorgia Callegaro, Luciano Campi, Tiziano\n  Vargiolu",
        "title": "Nonzero-sum stochastic differential games with impulse controls: a\n  verification theorem with applications",
        "comments": "7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general nonzero-sum impulse game with two players. The main\nmathematical contribution of the paper is a verification theorem which\nprovides, under some regularity conditions, a suitable system of\nquasi-variational inequalities for the value functions and the optimal\nstrategies of the two players. As an application, we study an impulse game with\na one-dimensional state variable, following a real-valued scaled Brownian\nmotion, and two players with linear and symmetric running payoffs. We fully\ncharacterize a Nash equilibrium and provide explicit expressions for the\noptimal strategies and the value functions. We also prove some asymptotic\nresults with respect to the intervention costs. Finally, we consider two\nfurther non-symmetric examples where a Nash equilibrium is found numerically.\n"
    },
    {
        "paper_id": 1605.0008,
        "authors": "Brendon Farrell",
        "title": "Depreciation and the Time Value of Money",
        "comments": "Third Draft - Suggestions and feedback will be highly appreciated",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generally accepted depreciation methods do not compute the intrinsic value of\nan asset, as they do not factor for the Time Value of Money, a key principle\nwithin financial theory. This is disadvantageous, as knowing the intrinsic\nvalue of an asset can assist with making effective purchase and sale decisions.\n  By applying the Time Value of Money principle to deprecation and book\nvaluation, methods can be formulated to approximate the intrinsic valuation of\na depreciable asset, which improves the capacity for buyers and sellers of\nassets to make rational decisions.\n  A deprecation method is formulated within, which aims to better match book\nvalue with intrinsic value. While this method makes many assumptions and thus\nhas limitations, more complex formulas, which factor for a greater number of\nvariables, can be created using a similar approach, to produce better\napproximations for intrinsic value.\n"
    },
    {
        "paper_id": 1605.00173,
        "authors": "Ahmed Bel Hadj Ayed, Gr\\'egoire Loeper, Fr\\'ed\\'eric Abergel",
        "title": "Robustness of mathematical models and technical analysis strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to compare the performances of the optimal strategy\nunder parameters mis-specification and of a technical analysis trading\nstrategy. The setting we consider is that of a stochastic asset price model\nwhere the trend follows an unobservable Ornstein-Uhlenbeck process. For both\nstrategies, we provide the asymptotic expectation of the logarithmic return as\na function of the model parameters. Finally, numerical examples find that an\ninvestment strategy using the cross moving averages rule is more robust than\nthe optimal strategy under parameters mis-specification.\n"
    },
    {
        "paper_id": 1605.0023,
        "authors": "Leopoldo Catania and Nima Nonejad",
        "title": "Density Forecasts and the Leverage Effect: Some Evidence from\n  Observation and Parameter-Driven Volatility Models",
        "comments": "36 pages, 2 figures, 20 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The leverage effect refers to the well-established relationship between\nreturns and volatility. When returns fall, volatility increases. We examine the\nrole of the leverage effect with regards to generating density forecasts of\nequity returns using well-known observation and parameter-driven volatility\nmodels. These models differ in their assumptions regarding: The parametric\nspecification, the evolution of the conditional volatility process and how the\nleverage effect is accounted for. The ability of a model to generate accurate\ndensity forecasts when the leverage effect is incorporated or not as well as a\ncomparison between different model-types is carried out using a large number of\nfinancial time-series. We find that, models with the leverage effect generally\ngenerate more accurate density forecasts compared to their no-leverage\ncounterparts. Moreover, we also find that our choice with regards to how to\nmodel the leverage effect and the conditional log-volatility process is\nimportant in generating accurate density forecasts\n"
    },
    {
        "paper_id": 1605.00307,
        "authors": "Jan Kuklinski and Kevin Tyloo",
        "title": "Semi-analytic path integral solution of SABR and Heston equations:\n  pricing Vanilla and Asian options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We discuss a semi-analytical method for solving SABR-type equations based on\npath integrals. In this approach, one set of variables is integrated\nanalytically while the second set is integrated numerically via Monte-Carlo.\nThis method, known in the literature as Conditional Monte-Carlo, leads to\ncompact expressions functional on three correlated stochastic variables. The\nmethodology is practical and efficient when solving Vanilla pricing in the\nSABR, Heston and Bates models with time depending parameters. Further, it can\nalso be practically applied to pricing Asian options in the $\\beta=0$ SABR\nmodel and to other $\\beta=0$ type models.\n"
    },
    {
        "paper_id": 1605.00339,
        "authors": "Pavel V. Shevchenko and Xiaolin Luo",
        "title": "A unified pricing of variable annuity guarantees under the optimal\n  stochastic control framework",
        "comments": "Keywords: variable annuity, guaranteed living and death benefits,\n  guaranteed minimum accumulation benefit, optimal stochastic control, direct\n  integration method",
        "journal-ref": "Risks 2016, 4(3), 22:1-22:31",
        "doi": "10.3390/risks4030022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we review pricing of variable annuity living and death\nguarantees offered to retail investors in many countries. Investors purchase\nthese products to take advantage of market growth and protect savings. We\npresent pricing of these products via an optimal stochastic control framework,\nand review the existing numerical methods. For numerical valuation of these\ncontracts, we develop a direct integration method based on Gauss-Hermite\nquadrature with a one-dimensional cubic spline for calculation of the expected\ncontract value, and a bi-cubic spline interpolation for applying the jump\nconditions across the contract cashflow event times. This method is very\nefficient when compared to the partial differential equation methods if the\ntransition density (or its moments) of the risky asset underlying the contract\nis known in closed form between the event times. We also present accurate\nnumerical results for pricing of a Guaranteed Minimum Accumulation Benefit\n(GMAB) guarantee available on the market that can serve as a benchmark for\npractitioners and researchers developing pricing of variable annuity\nguarantees.\n"
    },
    {
        "paper_id": 1605.00634,
        "authors": "Jean-Philippe Bouchaud and Damien Challet",
        "title": "Why have asset price properties changed so little in 200 years",
        "comments": "16 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We first review empirical evidence that asset prices have had episodes of\nlarge fluctuations and been inefficient for at least 200 years. We briefly\nreview recent theoretical results as well as the neurological basis of trend\nfollowing and finally argue that these asset price properties can be attributed\nto two fundamental mechanisms that have not changed for many centuries: an\ninnate preference for trend following and the collective tendency to exploit as\nmuch as possible detectable price arbitrage, which leads to destabilizing\nfeedback loops.\n"
    },
    {
        "paper_id": 1605.00762,
        "authors": "Philip Ernst and Larry Shepp",
        "title": "Revisiting a Theorem of L.A. Shepp on Optimal Stopping",
        "comments": "5 pages",
        "journal-ref": "Communications on Stochastic Analysis (2015) 9(3): 419-423",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Using a bondholder who seeks to determine when to sell his bond as our\nmotivating example, we revisit one of Larry Shepp's classical theorems on\noptimal stopping. We offer a novel proof of Theorem 1 from from \\cite{Shepp}.\nOur approach is that of guessing the optimal control function and proving its\noptimality with martingales. Without martingale theory one could hardly prove\nour guess to be correct.\n"
    },
    {
        "paper_id": 1605.00868,
        "authors": "Mikkel Bennedsen and Ulrich Hounyo and Asger Lunde and Mikko S.\n  Pakkanen",
        "title": "The Local Fractional Bootstrap",
        "comments": null,
        "journal-ref": "Scandinavian Journal of Statistics 2018, Vol. 46, No. 1, 329-359",
        "doi": "10.1111/sjos.12355",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a bootstrap procedure for high-frequency statistics of Brownian\nsemistationary processes. More specifically, we focus on a hypothesis test on\nthe roughness of sample paths of Brownian semistationary processes, which uses\nan estimator based on a ratio of realized power variations. Our new resampling\nmethod, the local fractional bootstrap, relies on simulating an auxiliary\nfractional Brownian motion that mimics the fine properties of high frequency\ndifferences of the Brownian semistationary process under the null hypothesis.\nWe prove the first order validity of the bootstrap method and in simulations we\nobserve that the bootstrap-based hypothesis test provides considerable\nfinite-sample improvements over an existing test that is based on a central\nlimit theorem. This is important when studying the roughness properties of time\nseries data; we illustrate this by applying the bootstrap method to two\nempirical data sets: we assess the roughness of a time series of high-frequency\nasset prices and we test the validity of Kolmogorov's scaling law in\natmospheric turbulence data.\n"
    },
    {
        "paper_id": 1605.01028,
        "authors": "Philip Ernst, Dean Foster, and Larry Shepp",
        "title": "On Optimal Retirement (How to Retire Early)",
        "comments": "14 pages, 2 figures",
        "journal-ref": "Journal of Applied Probability (2014), 51(2): 333-345",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We pose an optimal control problem arising in a perhaps new model for\nretirement investing. Given a control function $f$ and our current net worth as\n$X(t)$ for any $t$, we invest an amount $f(X(t))$ in the market. We need a\nfortune of $M$ \"superdollars\" to retire and want to retire as early as\npossible. We model our change in net worth over each infinitesimal time\ninterval by the Ito process $dX(t)= (1+f(X(t))dt+ f(X(t))dW(t)$. We show how to\nchoose the optimal $f=f_0$ and show that the choice of $f_0$ is optimal among\nall nonanticipative investment strategies, not just among Markovian ones.\n"
    },
    {
        "paper_id": 1605.01052,
        "authors": "Damien Challet",
        "title": "Regrets, learning and wisdom",
        "comments": "9 pages, 1 figure. Opinion paper submitted to European Physical\n  Journal - Special Topics \"Can economics be a physical science?\"",
        "journal-ref": null,
        "doi": "10.1140/epjst/e2016-60122-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This contribution discusses in what respect Econophysics may be able to\ncontribute to the rebuilding of economics theory. It focuses on aggregation,\nindividual vs collective learning and functional wisdom of the crowds.\n"
    },
    {
        "paper_id": 1605.01071,
        "authors": "A. Paliathanasis, R.M. Morris and P.G.L. Leach",
        "title": "Lie symmetries of (1+2) nonautonomous evolution equations in Financial\n  Mathematics",
        "comments": "15 pages, 1 figure, to be published in Mathematics in the Special\n  issue \"Mathematical Finance\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse two classes of $(1+2)$ evolution equations which are of special\ninterest in Financial Mathematics, namely the Two-dimensional Black-Scholes\nEquation and the equation for the Two-factor Commodities Problem. Our approach\nis that of Lie Symmetry Analysis. We study these equations for the case in\nwhich they are autonomous and for the case in which the parameters of the\nequations are unspecified functions of time. For the autonomous Black-Scholes\nEquation we find that the symmetry is maximal and so the equation is reducible\nto the $(1+2)$ Classical Heat Equation. This is not the case for the\nnonautonomous equation for which the number of symmetries is submaximal. In the\ncase of the two-factor equation the number of symmetries is submaximal in both\nautonomous and nonautonomous cases. When the solution symmetries are used to\nreduce each equation to a $(1+1)$ equation, the resulting equation is of\nmaximal symmetry and so equivalent to the $(1+1)$ Classical Heat Equation.\n"
    },
    {
        "paper_id": 1605.01327,
        "authors": "Erhan Bayraktar and Zhou Zhou",
        "title": "No-arbitrage and hedging with liquid American options",
        "comments": "Final version. To appear in Mathematics of Operations Research.\n  Keywords: semi-static trading strategies, Liquid American options,\n  Fundamental theorem of asset pricing, sub/super hedging dualities",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since most of the traded options on individual stocks is of American type it\nis of interest to generalize the results obtained in semi-static trading to the\ncase when one is allowed to statically trade American options. However, this\nproblem has proved to be elusive so far because of the asymmetric nature of the\npositions of holding versus shorting such options. Here we provide a unified\nframework and generalize the fundamental theorem of asset pricing (FTAP) and\nhedging dualities in arXiv:1502.06681 (to appear in Annals of Applied\nProbability) to the case where the investor can also short American options.\nFollowing arXiv:1502.06681, we assume that the longed American options are\ndivisible. As for the shorted American options, we show that the divisibility\nplays no role regarding arbitrage property and hedging prices. Then using the\nmethod of enlarging probability spaces proposed in arXiv:1604.05517, we convert\nthe shorted American options to European options, and establish the FTAP and\nsub- and super-hedging dualities in the enlarged space both with and without\nmodel uncertainty.\n"
    },
    {
        "paper_id": 1605.01343,
        "authors": "Siamak F. Shahandashti",
        "title": "Electoral Systems Used around the World",
        "comments": "This is a personally archived version of a chapter by the same title\n  contributed to the book \"Real-World Electronic Voting: Design, Analysis and\n  Deployment\", Feng Hao and Peter Y. A. Ryan (editors), Series in Security,\n  Privacy and Trust, CRC Press, 2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an overview of the diverse electoral systems used in local, national,\nor super-national elections around the world. We discuss existing methods for\nselecting single and multiple winners and give real-world examples for some\nmore elaborate systems. Eventually, we elaborate on some of the better known\nstrengths and weaknesses of various methods from both the theoretical and\npractical points of view.\n"
    },
    {
        "paper_id": 1605.01354,
        "authors": "Luisanna Cocco and Michele Marchesi",
        "title": "Modeling and Simulation of the Economics of Mining in the Bitcoin Market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0164603",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In January 3, 2009, Satoshi Nakamoto gave rise to the \"Bitcoin Block Chain\"\ncreating the first block of the chain hashing on his computers central\nprocessing unit (CPU). Since then, the hash calculations to mine Bitcoin have\nbeen getting more and more complex, and consequently the mining hardware\nevolved to adapt to this increasing difficulty. Three generations of mining\nhardware have followed the CPU's generation. They are GPU's, FPGA's and ASIC's\ngenerations. This work presents an agent based artificial market model of the\nBitcoin mining process and of the Bitcoin transactions. The goal of this work\nis to model the economy of the mining process, starting from GPU's generation,\nthe first with economic significance. The model reproduces some \"stylized\nfacts\" found in real time price series and some core aspects of the mining\nbusiness. In particular, the computational experiments performed are able to\nreproduce the unit root property, the fat tail phenomenon and the volatility\nclustering of Bitcoin price series. In addition, under proper assumptions, they\nare able to reproduce the price peak at the end of November 2013, its next fall\nin April 2014, the generation of Bitcoins, the hashing capability, the power\nconsumption, and the mining hardware and electrical energy expenses of the\nBitcoin network.\n"
    },
    {
        "paper_id": 1605.01862,
        "authors": "Olivier Gu\\'eant",
        "title": "Optimal market making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market makers provide liquidity to other market participants: they propose\nprices at which they stand ready to buy and sell a wide variety of assets. They\nface a complex optimization problem with both static and dynamic components.\nThey need indeed to propose bid and offer/ask prices in an optimal way for\nmaking money out of the difference between these two prices (their bid-ask\nspread). Since they seldom buy and sell simultaneously, and therefore hold long\nand/or short inventories, they also need to mitigate the risk associated with\nprice changes, and subsequently skew their quotes dynamically. In this paper,\n(i) we propose a general modeling framework which generalizes (and reconciles)\nthe various modeling approaches proposed in the literature since the\npublication of the seminal paper \"High-frequency trading in a limit order book\"\nby Avellaneda and Stoikov, (ii) we prove new general results on the existence\nand the characterization of optimal market making strategies, (iii) we obtain\nnew closed-form approximations for the optimal quotes, (iv) we extend the\nmodeling framework to the case of multi-asset market making and we obtain\ngeneral closed-form approximations for the optimal quotes of a multi-asset\nmarket maker, and (v) we show how the model can be used in practice in the\nspecific (and original) case of two credit indices.\n"
    },
    {
        "paper_id": 1605.0192,
        "authors": "Arnab Chatterjee",
        "title": "Is it \"natural\" to expect Economics to become a part of the Natural\n  Sciences?",
        "comments": "7 pages; EPJ-ST special issue on 'Can Economics be a Physical\n  Science?' Edited by S. Sinha, A. S. Chakrabarti & M. Mitra",
        "journal-ref": "Eur. Phys. J. Special Topics 225 (2016) 3145-3149",
        "doi": "10.1140/epjst/e2016-60157-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We are in the middle of a complex debate as to whether Economics is really a\nproper natural science. The 'Discussion & Debate' issue of this Euro. Phys. J.\nSpecial Topic volume is: 'Can economics be a Physical Science?' I discuss some\naspects here.\n"
    },
    {
        "paper_id": 1605.01949,
        "authors": "Belal Baaquie, Bertrand M. Roehner, Qinghai Wang",
        "title": "The wage transition in developed countries and its implications for\n  China",
        "comments": "32 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.11.092",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The expression \"wage transition\" refers to the fact that over the past two or\nthree decades in all developed economies wage increases have levelled off.\nThere has been a widening divergence and decoupling between wages on the one\nhand and GDP per capita on the other hand. Yet, in China wages and GDP per\ncapita climbed in sync (at least up to now). In the first part of the paper we\npresent comparative statistical evidence which measures the extent of the wage\ntransition effect. In a second part we consider the reasons of this phenomenon,\nin particular we explain how the transfers of labor from low productivity\nsectors (such as agriculture) to high productivity sectors (such as\nmanufacturing) are the driver of productivity growth, particularly through\ntheir synergetic effects. Although rural flight represents only one of these\neffects, it is certainly the most visible because of the geographical\nrelocation that it implies; it is also the most well-defined statistically.\nMoreover, it will be seen that it is a good indicator of the overall\nproductivity and attractivity of the non-agricultural sector. Because this\nmodel accounts fairly well for the observed evolution in industrialized\ncountries, we use it to predict the rate of Chinese economic growth in the\ncoming decades. Our forecast for the average annual growth of real wages ranges\nfrom 4% to 6% depending on how well China will control the development of its\nhealthcare industry.\n"
    },
    {
        "paper_id": 1605.01976,
        "authors": "Andrea Flori, Giuseppe Pappalardo, Michelangelo Puliga, Alessandro\n  Chessa, Fabio Pammolli",
        "title": "The Accounting Network: how financial institutions react to systemic\n  crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The role of Network Theory in the study of the financial crisis has been\nwidely spotted in the latest years. It has been shown how the network topology\nand the dynamics running on top of it can trigger the outbreak of large\nsystemic crisis. Following this methodological perspective we introduce here\nthe Accounting Network, i.e. the network we can extract through vector\nsimilarities techniques from companies' financial statements. We build the\nAccounting Network on a large database of worldwide banks in the period\n2001-2013, covering the onset of the global financial crisis of mid-2007. After\na careful data cleaning, we apply a quality check in the construction of the\nnetwork, introducing a parameter (the Quality Ratio) capable of trading off the\nsize of the sample (coverage) and the representativeness of the financial\nstatements (accuracy). We compute several basic network statistics and check,\nwith the Louvain community detection algorithm, for emerging communities of\nbanks. Remarkably enough sensible regional aggregations show up with the\nJapanese and the US clusters dominating the community structure, although the\npresence of a geographically mixed community points to a gradual convergence of\nbanks into similar supranational practices. Finally, a Principal Component\nAnalysis procedure reveals the main economic components that influence\ncommunities' heterogeneity. Even using the most basic vector similarity\nhypotheses on the composition of the financial statements, the signature of the\nfinancial crisis clearly arises across the years around 2008. We finally\ndiscuss how the Accounting Networks can be improved to reflect the best\npractices in the financial statement analysis.\n"
    },
    {
        "paper_id": 1605.01998,
        "authors": "Louis Paulot",
        "title": "Unbiased Monte Carlo Simulation of Diffusion Processes",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monte Carlo simulations of diffusion processes often introduce bias in the\nfinal result, due to time discretization. Using an auxiliary Poisson process,\nit is possible to run simulations which are unbiased. In this article, we\npropose such a Monte Carlo scheme which converges to the exact value. We manage\nto keep the simulation variance finite in all cases, so that the strong law of\nlarge numbers guarantees the convergence. Moreover, the simulation noise is a\ndecreasing function of the Poisson process intensity. Our method handles\nmultidimensional processes with nonconstant drifts and nonconstant\nvariance-covariance matrices. It also encompasses stochastic interest rates.\n"
    },
    {
        "paper_id": 1605.02188,
        "authors": "Donya Rahmani, Saeed Heravi, Hossein Hassani, Mansi Ghodsi",
        "title": "Forecasting time series with structural breaks with Singular Spectrum\n  Analysis, using a general form of recurrent formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study extends and evaluates the forecasting performance of the Singular\nSpectrum Analysis (SSA) technique using a general non-linear form for the re-\ncurrent formula. In this study, we consider 24 series measuring the monthly\nseasonally adjusted industrial production of important sectors of the German,\nFrench and UK economies. This is tested by comparing the performance of the new\nproposed model with basic SSA and the SSA bootstrap forecasting, especially\nwhen there is evidence of structural breaks in both in-sample and out-of-sample\nperiods. According to root mean-square error (RMSE), SSA using the general\nrecursive formula outperforms both the SSA and the bootstrap forecasting at\nhorizons of up to a year. We found no significant difference in predicting the\ndirection of change between these methods. Therefore, it is suggested that the\nSSA model with the general recurrent formula should be chosen by users in the\ncase of structural breaks in the series.\n"
    },
    {
        "paper_id": 1605.02283,
        "authors": "Shangmei Zhao, Qiuchao Xie, Qing Lu, Xin Jiang and Wei Chen",
        "title": "Coherence and incoherence collective behavior in financial market",
        "comments": "6 pages, 6 figures",
        "journal-ref": "EPL (Europhysics Letters), Volume 112, Number 2,2015",
        "doi": "10.1209/0295-5075/112/28002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets have been extensively studied as highly complex evolving\nsystems. In this paper, we quantify financial price fluctuations through a\ncoupled dynamical system composed of phase oscillators. We find a Financial\nCoherence and Incoherence (FCI) coexistence collective behavior emerges as the\nsystem evolves into the stable state, in which the stocks split into two\ngroups: one is represented by coherent, phase-locked oscillators, the other is\ncomposed of incoherent, drifting oscillators. It is demonstrated that the size\nof the coherent stock groups fluctuates during the economic periods according\nto real-world financial instabilities or shocks. Further, we introduce the\ncoherent characteristic matrix to characterize the involvement dynamics of\nstocks in the coherent groups. Clustering results on the matrix provides a\nnovel manifestation of the correlations among stocks in the economic periods.\nOur analysis for components of the groups is consistent with the Global\nIndustry Classification Standard (GICS) classification and can also figure out\nfeatures for newly developed industries. These results can provide potentially\nimplications on characterizing inner dynamical structure of financial markets\nand making optimal investment tragedies.\n"
    },
    {
        "paper_id": 1605.02418,
        "authors": "Sujay Mukhoti, Pritam Ranjan",
        "title": "Mean-correction and Higher Order Moments for a Stochastic Volatility\n  Model with Correlated Errors",
        "comments": "15 pages; 5 figures, submitted to IJSP",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an efficient stock market, the log-returns and their time-dependent\nvariances are often jointly modelled by stochastic volatility models (SVMs).\nMany SVMs assume that errors in log-return and latent volatility process are\nuncorrelated, which is unrealistic. It turns out that if a non-zero correlation\nis included in the SVM (e.g., Shephard (2005)), then the expected log-return at\ntime t conditional on the past returns is non-zero, which is not a desirable\nfeature of an efficient stock market. In this paper, we propose a\nmean-correction for such an SVM for discrete-time returns with non-zero\ncorrelation. We also find closed form analytical expressions for higher moments\nof log-return and its lead-lag correlations with the volatility process. We\ncompare the performance of the proposed and classical SVMs on S&P 500 index\nreturns obtained from NYSE.\n"
    },
    {
        "paper_id": 1605.02472,
        "authors": "Guglielmo D'Amico",
        "title": "Generalized semi-Markovian dividend discount model: risk and return",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article presents a general discrete time dividend valuation model when\nthe dividend growth rate is a general continuous variable. The main assumption\nis that the dividend growth rate follows a discrete time semi-Markov chain with\nmeasurable space. The paper furnishes sufficient conditions that assure\nfiniteness of fundamental prices and risks and new equations that describe the\nfirst and second order price-dividend ratios. Approximation methods to solve\nequations are provided and some new results for semi-Markov reward processes\nwith Borel state space are established. The paper generalizes previous\ncontributions dealing with pricing firms on the basis of fundamentals.\n"
    },
    {
        "paper_id": 1605.02539,
        "authors": "Anna Aksamit, Zhaoxu Hou and Jan Ob\\l\\'oj",
        "title": "Robust framework for quantifying the value of information in pricing and\n  hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate asymmetry of information in the context of robust approach to\npricing and hedging of financial derivatives. We consider two agents, one who\nonly observes the stock prices and another with some additional information,\nand investigate when the pricing--hedging duality for the former extends to the\nlatter. We introduce a general framework to express the superhedging and market\nmodel prices for an informed agent. Our key insight is that an informed agent\ncan be seen as a regular agent who can restrict her attention to a certain\nsubset of possible paths. We use results of Hou & Ob\\l\\'oj on robust approach\nwith beliefs to establish the pricing--hedging duality for an informed agent.\nOur results cover number of scenarios, including information arriving before\ntrading starts, arriving after static position in European options is formed\nbut before dynamic trading starts or arriving at some point before the\nmaturity. For the latter we show that the superhedging value satisfies a\nsuitable dynamic programming principle, which is of independent interest.\n"
    },
    {
        "paper_id": 1605.02654,
        "authors": "Yves-Laurent Kom Samo, Alexander Vervuurt",
        "title": "Stochastic Portfolio Theory: A Machine Learning Perspective",
        "comments": "9 pages, UAI 2016 conference paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a novel application of Gaussian processes (GPs) to\nfinancial asset allocation. Our approach is deeply rooted in Stochastic\nPortfolio Theory (SPT), a stochastic analysis framework introduced by Robert\nFernholz that aims at flexibly analysing the performance of certain investment\nstrategies in stock markets relative to benchmark indices. In particular, SPT\nhas exhibited some investment strategies based on company sizes that, under\nrealistic assumptions, outperform benchmark indices with probability 1 over\ncertain time horizons. Galvanised by this result, we consider the inverse\nproblem that consists of learning (from historical data) an optimal investment\nstrategy based on any given set of trading characteristics, and using a\nuser-specified optimality criterion that may go beyond outperforming a\nbenchmark index. Although this inverse problem is of the utmost interest to\ninvestment management practitioners, it can hardly be tackled using the SPT\nframework. We show that our machine learning approach learns investment\nstrategies that considerably outperform existing SPT strategies in the US stock\nmarket.\n"
    },
    {
        "paper_id": 1605.03097,
        "authors": "Siyan Zhang, Anna L. Mazzucato, Victor Nistor",
        "title": "Heat Kernels, Solvable Lie Groups, and the Mean Reverting SABR\n  Stochastic Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use commutator techniques and calculations in solvable Lie groups to\ninvestigate certain evolution Partial Differential Equations (PDEs for short)\nthat arise in the study of stochastic volatility models for pricing contingent\nclaims on risky assets. In particular, by restricting to domains of bounded\nvolatility, we establish the existence of the semi-groups generated by the\nspatial part of the operators in these models, concentrating on those arising\nin the so-called \"SABR stochastic volatility model with mean reversion.\" The\nmain goal of this work is to approximate the solutions of the Cauchy problem\nfor the SABR PDE with mean reversion, a parabolic problem the generator of\nwhich is denoted by $L$. The fundamental solution for this problem is not known\nin closed form. We obtain an approximate solution by performing an expansion in\nthe so-called volvol or volatility of the volatility, which leads us to study a\ndegenerate elliptic operator $L_0$, corresponding the the zero-volvol case of\nthe SABR model with mean reversion, to which the classical results do not\napply. However, using Lie algebra techniques we are able to derive an exact\nformula for the solution operator of the PDE $\\partial_t u - L_0 u = 0$. We\nthen compare the semi-group generated by $L$--the existence of which does\nfollows from standard arguments--to that generated by $L_0$, thus establishing\na perturbation result that is useful for numerical methods for the SABR PDE\nwith mean reversion. In the process, we are led to study semigroups arising\nfrom both a strongly parabolic and a hyperbolic problem.\n"
    },
    {
        "paper_id": 1605.03133,
        "authors": "Angelica Sbardella, Emanuele Pugliese, and Luciano Pietronero",
        "title": "Economic Development and Inequality: a complex system analysis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0182774",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By borrowing methods from complex system analysis, in this paper we analyze\nthe features of the complex relationship that links the development and the\nindustrialization of a country to economic inequality. In order to do this, we\nidentify industrialization as a combination of a monetary index, the GDP per\ncapita, and a recently introduced measure of the complexity of an economy, the\nFitness. At first we explore these relations on a global scale over the time\nperiod 1990--2008 focusing on two different dimensions of inequality: the\ncapital share of income and a Theil measure of wage inequality. In both cases,\nthe movement of inequality follows a pattern similar to the one theorized by\nKuznets in the fifties. We then narrow down the object of study ad we\nconcentrate on wage inequality within the United States. By employing data on\nwages and employment on the approximately 3100 US counties for the time\ninterval 1990--2014, we generalize the Fitness-Complexity algorithm for\ncounties and NAICS sectors, and we investigate wage inequality between\nindustrial sectors within counties. At this scale, in the early nineties we\nrecover a behavior similar to the global one. While, in more recent years, we\nuncover a trend reversal: wage inequality monotonically increases as\nindustrialization levels grow. Hence at a county level, at net of the social\nand institutional factors that differ among countries, we not only observe an\nupturn in inequality but also a change in the structure of the relation between\nwage inequality and development.\n"
    },
    {
        "paper_id": 1605.03551,
        "authors": "Martin Gremm",
        "title": "Global Gauge Symmetries, Risk-Free Portfolios, and the Risk-Free Rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define risk-free portfolios using three gauge invariant differential\noperators that require such portfolios to be insensitive to price changes, to\nbe self-financing, and to produce a zero real return so there are no risk-free\nprofits. This definition identifies the risk-free rate as the return of an\ninfinitely diversified portfolio rather than as an arbitrary external\nparameter. The risk-free rate measures the rate of global price rescaling,\nwhich is a gauge symmetry of economies. We explore the properties of risk-free\nrates, rederive the Black Scholes equation with a new interpretation of the\nrisk-free rate parameter as a that background gauge field, and discuss gauge\ninvariant discounting of cash flows.\n"
    },
    {
        "paper_id": 1605.03559,
        "authors": "Ren\\'e Kempen and Stanislaus Maier-Paape",
        "title": "Survey on log-normally distributed market-technical trend data",
        "comments": "17 pages, 21 figures, 23 tables, Keywords: log-normal,\n  market-technical trend, MinMax-process, trend statistics, market analysis,\n  empirical distribution, quantitative finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this survey, a short introduction in the recent discovery of log-normally\ndistributed market-technical trend data will be given. The results of the\nstatistical evaluation of typical market-technical trend variables will be\npresented. It will be shown that the log-normal assumption fits better to\nempirical trend data than to daily returns of stock prices. This enables to\nmathematically evaluate trading systems depending on such variables. In this\nmanner, a basic approach to an anti cyclic trading system will be given as an\nexample.\n"
    },
    {
        "paper_id": 1605.03653,
        "authors": "Erhan Bayraktar and Alexander Munk",
        "title": "High-Roller Impact: A Large Generalized Game Model of Parimutuel\n  Wagering",
        "comments": "Final version. To appear in Market Microstructure and Liquidity. Key\n  words: Parimutuel wagering, Large generalized games, Nash equilibrium",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do large-scale participants in parimutuel wagering events affect the\nhouse and ordinary bettors? A standard narrative suggests that they may\ntemporarily benefit the former at the expense of the latter. To approach this\nproblem, we begin by developing a model based on the theory of large\ngeneralized games. Constrained only by their budgets, a continuum of diffuse\n(ordinary) players and a single atomic (large-scale) player simultaneously\nwager to maximize their expected profits according to their individual beliefs.\nOur main theoretical result gives necessary and sufficient conditions for the\nexistence and uniqueness of a pure-strategy Nash equilibrium. Using this\nframework, we analyze our question in concrete scenarios. First, we study a\nsituation in which both predicted effects are observed. Neither is always\nobserved in our remaining examples, suggesting the need for a more nuanced view\nof large-scale participants.\n"
    },
    {
        "paper_id": 1605.03683,
        "authors": "Takashi Kato",
        "title": "Optimality of VWAP Execution Strategies under General Shaped Market\n  Impact Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short note, we study an optimization problem of expected\nimplementation shortfall (IS) cost under general shaped market impact\nfunctions. In particular, we find that an optimal strategy is a VWAP (volume\nweighted average price) execution strategy when the market model is a\nBlack-Scholes type with stochastic clock and market trading volume is large.\n"
    },
    {
        "paper_id": 1605.04219,
        "authors": "Francisco Salas-Molina, Francisco J. Martin, Juan A.\n  Rodr\\'iguez-Aguilar, Joan Serr\\`a, Josep Ll. Arcos",
        "title": "Empowering cash managers to achieve cost savings by improving predictive\n  accuracy",
        "comments": "23 pages, 6 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cash management is concerned with optimizing the short-term funding\nrequirements of a company. To this end, different optimization strategies have\nbeen proposed to minimize costs using daily cash flow forecasts as the main\ninput to the models. However, the effect of the accuracy of such forecasts on\ncash management policies has not been studied. In this article, using two real\ndata sets from the textile industry, we show that predictive accuracy is highly\ncorrelated with cost savings when using daily forecasts in cash management\nmodels. A new method is proposed to help cash managers estimate if efforts in\nimproving predictive accuracy are proportionally rewarded by cost savings. Our\nresults imply the need for an analysis of potential cost savings derived from\nimproving predictive accuracy. From that, the search for better forecasting\nmodels is in place to improve cash management.\n"
    },
    {
        "paper_id": 1605.04385,
        "authors": "Patrick Beissner and Frank Riedel",
        "title": "Knight--Walras Equilibria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Knightian uncertainty leads naturally to nonlinear expectations. We introduce\na corresponding equilibrium concept with sublinear prices and establish their\nexistence. In general, such equilibria lead to Pareto inefficiency and coincide\nwith Arrow--Debreu equilibria only if the values of net trades are\nambiguity--free in the mean. Without aggregate uncertainty, inefficiencies\narise generically.\n  We introduce a constrained efficiency concept, uncertainty--neutral\nefficiency and show that Knight--Walras equilibrium allocations are efficient\nin this constrained sense. Arrow--Debreu equilibria turn out to be non--robust\nwith respect to the introduction of Knightian uncertainty.\n"
    },
    {
        "paper_id": 1605.04584,
        "authors": "Ewa Marciniak and Zbigniew Palmowski",
        "title": "On the Optimal Dividend Problem in the Dual Model with Surplus-Dependent\n  Premiums",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns the dual risk model, dual to the risk model for insurance\napplications, where premiums are surplus-dependent. In such a model premiums\nare regarded as costs, while claims refer to profits. We calculate the mean of\nthe cumulative discounted dividends paid until ruin, if the barrier strategy is\napplied. We formulate associated Hamilton-Jacobi-Bellman equation and identify\nsufficient conditions for a barrier strategy to be optimal. Some numerical\nexamples are provided when profits have exponential law.\n"
    },
    {
        "paper_id": 1605.046,
        "authors": "Tim Gebbie and Fayyaaz Loonat",
        "title": "Learning zero-cost portfolio selection with pattern matching",
        "comments": "28 pages, 21 figures",
        "journal-ref": "PLoS ONE 13(9): e0202788 (2018)",
        "doi": "10.1371/journal.pone.0202788",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider and extend the adversarial agent-based learning approach of\nGy{\\\"o}rfi {\\it et al} to the situation of zero-cost portfolio selection\nimplemented with a quadratic approximation derived from the mutual fund\nseparation theorems. The algorithm is applied to daily sampled sequential\nOpen-High-Low-Close data and sequential intraday 5-minute bar-data from the\nJohannesburg Stock Exchange (JSE). Statistical tests of the algorithms are\nconsidered. The algorithms are directly compared to standard NYSE test cases\nfrom prior literature. The learning algorithm is used to select parameters for\nagents (or experts) generated by pattern matching past dynamics using a simple\nnearest-neighbour search algorithm. It is shown that there is a speed advantage\nassociated with using an analytic solution of the mutual fund separation\ntheorems. It is argued that the expected loss in performance does not undermine\nthe potential application to intraday quantitative trading and that when\ntransactions costs and slippage are considered the strategies can still remain\nprofitable when unleveraged. The paper demonstrates that patterns in financial\ntime-series on the JSE can be systematically exploited in collective but that\nthis does not imply predictability of the individual asset time-series\nthemselves.\n"
    },
    {
        "paper_id": 1605.04938,
        "authors": "Massimiliano Zanin, David Papo, Miguel Romance, Regino Criado,\n  Santiago Moral",
        "title": "The topology of card transaction money flows",
        "comments": "Submitted to Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.06.091",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Money flow models are essential tools to understand different economical\nphenomena, like saving propensities and wealth distributions. In spite of their\nimportance, most of them are based on synthetic transaction networks with\nsimple topologies, e.g. random or scale-free ones, as the characterisation of\nreal networks is made difficult by the confidentiality and sensitivity of money\ntransaction data. Here we present an analysis of the topology created by real\ncredit card transactions from one of the biggest world banks, and show how\ndifferent distributions, e.g. number of transactions per card or amount, have\nnontrivial characteristics. We further describe a stochastic model to create\ntransactions data sets, feeding from the obtained distributions, which will\nallow researchers to create more realistic money flow models.\n"
    },
    {
        "paper_id": 1605.0494,
        "authors": "Khizar Qureshi",
        "title": "Value-at-Risk: The Effect of Autoregression in a Quantile Process",
        "comments": "Columbia Economics Review, November 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Value-at-Risk (VaR) is an institutional measure of risk favored by financial\nregulators. VaR may be interpreted as a quantile of future portfolio values\nconditional on the information available, where the most common quantile used\nis 95%. Here we demonstrate Conditional Autoregressive Value at Risk, first\nintroduced by Engle, Manganelli (2001). CAViaR suggests that negative/positive\nreturns are not i.i.d., and that there is significant autocorrelation. The\nmodel is tested using data from 1986- 1999 and 1999-2009 for GM, IBM, XOM, SPX,\nand then validated via the dynamic quantile test. Results suggest that the\ntails (upper/lower quantile) of a distribution of returns behave differently\nthan the core.\n"
    },
    {
        "paper_id": 1605.04941,
        "authors": "Khizar Qureshi, Cheng Su",
        "title": "Mortgages and Refinancing",
        "comments": "MIT Research Journal, May 2015",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In general, homeowners refinance in response to a decrease in interest rates,\nas their borrowing costs are lowered. However, it is worth investigating the\neffects of refinancing after taking the underlying costs into consideration.\nHere we develop a synthetic mortgage calculator that sufficiently accounts for\nsuch costs and the implications on new monthly payments. To confirm the\naccuracy of the calculator, we simulate the effects of refinancing over 15 and\n30 year periods. We then model the effects of refinancing as risk to the issuer\nof the mortgage, as there is negative duration associated with shifts in the\ninterest rate. Furthermore, we investigate the effects on the swap market as\nwell as the treasury bond market. We model stochastic interest rates using the\nVasicek model.\n"
    },
    {
        "paper_id": 1605.04943,
        "authors": "M.L. Bertotti, A.K. Chattopadhyay, G. Modanese",
        "title": "Stochastic Effects in a Discretized Kinetic Model of Economic Exchange",
        "comments": "14 pages, 3 figures",
        "journal-ref": "Physica A 471, pp. 724-732 (2017)",
        "doi": "10.1016/j.physa.2016.12.072",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Linear stochastic models and discretized kinetic theory are two complementary\nanalytical techniques used for the investigation of complex systems of economic\ninteractions. The former employ Langevin equations, with an emphasis on stock\ntrade; the latter is based on systems of ordinary differential equations and is\nbetter suited for the description of binary interactions, taxation and welfare\nredistribution. We propose a new framework which establishes a connection\nbetween the two approaches by introducing stochastic effects into the kinetic\nmodel based on Langevin and Fokker-Planck formalisms. Numerical simulations of\nthe resulting model indicate positive correlations between the Gini index and\nthe total wealth, that suggests a growing inequality with increasing income.\nFurther analysis shows a simultaneous decrease in inequality as social mobility\nincreases in presence of a conserved total wealth, in conformity with economic\ndata.\n"
    },
    {
        "paper_id": 1605.04945,
        "authors": "M A Szybisz and L Szybisz",
        "title": "Extended nonlinear feedback model for describing episodes of high\n  inflation",
        "comments": "16 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An extension of the nonlinear feedback (NLF) formalism to describe regimes of\nhyper- and high-inflation in economy is proposed in the present work. In the\nNLF model the consumer price index (CPI) exhibits a finite time singularity of\nthe type $1/(t_c -t)^{(1- \\beta)/\\beta}$, with $\\beta>0$, predicting a blow up\nof the economy at a critical time $t_c$. However, this model fails in\ndetermining $t_c$ in the case of weak hyperinflation regimes like, e.g., that\noccurred in Israel. To overcome this trouble, the NLF model is extended by\nintroducing a parameter $\\gamma$, which multiplies all therms with past growth\nrate index (GRI). In this novel approach the solution for CPI is also analytic\nbeing proportional to the Gaussian hypergeometric function\n$_2F_1(1/\\beta,1/\\beta,1+1/\\beta;z)$, where $z$ is a function of $\\beta$,\n$\\gamma$, and $t_c$. For $z \\to 1$ this hypergeometric function diverges\nleading to a finite time singularity, from which a value of $t_c$ can be\ndetermined. This singularity is also present in GRI. It is shown that the\ninterplay between parameters $\\beta$ and $\\gamma$ may produce phenomena of\nmultiple equilibria. An analysis of the severe hyperinflation occurred in\nHungary proves that the novel model is robust. When this model is used for\nexamining data of Israel a reasonable $t_c$ is got. High-inflation regimes in\nMexico and Iceland, which exhibit weaker inflations than that of Israel, are\nalso successfully described.\n"
    },
    {
        "paper_id": 1605.04948,
        "authors": "Jack Sarkissian",
        "title": "Quantum theory of securities price formation in financial markets",
        "comments": "Corrected minor typos and formatting",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a theory of securities price formation and dynamics based on\nquantum approach and without presuming any similarities with quantum mechanics.\nDisorder introduced by trading environment leads to probability distribution of\nreturns that is not a smooth curve, but a speckle-pattern fluctuating in both\nprice coordinate and time. This means that any given return can at times\nacquire a substantial probability of occurring while remaining low on average\nin time. Still, due to local character of order interaction during price\nformation the distribution width grows smoothly, has a minimum value at small\ntime scale and a square root behavior at large time scale. Examples of\ncalibration to market data, both intraday and daily, are provided.\n"
    },
    {
        "paper_id": 1605.04949,
        "authors": "Manuel Lafond",
        "title": "How brokers can optimally plot against traders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traders buy and sell financial instruments in hopes of making profit, and\nbrokers are responsible for the transaction. There are several hypotheses and\nconspiracy theories arguing that in some situations, brokers want their traders\nto lose money. For instance, a broker may want to protect the positions of a\nprivileged customer. Another example is that some brokers take positions\nopposite to their traders', in which case they make money whenever their\ntraders lose money. These are reasons for which brokers might manipulate prices\nin order to maximize the losses of their traders.\n  In this paper, our goal is to perform this shady task optimally -- or at\nleast to check whether this can actually be done algorithmically. Assuming\ntotal control over the price of an asset (ignoring the usual aspects of finance\nsuch as market conditions, external influence or stochasticity), we show how in\nquadratic time, given a set of trades specified by a stop-loss and a\ntake-profit price, a broker can find a maximum loss price movement. We also\nlook at an online trade model where broker and trader exchange turns, each\ntrying to make a profit. We show in which condition either side can make a\nprofit, and that the best option for the trader is to never trade.\n"
    },
    {
        "paper_id": 1605.04995,
        "authors": "Kazutoshi Yamazaki",
        "title": "Optimality of two-parameter strategies in stochastic control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we study a class of stochastic control problems where the\noptimal strategies are described by two parameters. These include a subset of\nsingular control, impulse control, and two-player stochastic games. The\nparameters are first chosen by the two continuous/smooth fit conditions, and\nthen the optimality of the corresponding strategy is shown by verification\narguments. Under the setting driven by a spectrally one-sided Levy process,\nthese procedures can be efficiently done thanks to the recent developments of\nscale functions. In this note, we illustrate these techniques using several\nexamples where the optimal strategy as well as the value function can be\nconcisely expressed via scale functions.\n"
    },
    {
        "paper_id": 1605.051,
        "authors": "Fr\\'ed\\'eric Vrins",
        "title": "Wrong-Way Risk Models: A Comparison of Analytical Exposures",
        "comments": "41 pages double space, many figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we compare static and dynamic (reduced form) approaches for\nmodeling wrong-way risk in the context of CVA. Although all these approaches\npotentially suffer from arbitrage problems, they are popular (respectively) in\nindustry and academia, mainly due to analytical tractability reasons. We\ncomplete the stochastic intensity models with another dynamic approach,\nconsisting in the straight modeling of the survival (Az\\'ema supermartingale)\nprocess using the $\\Phi$-martingale. Just like the other approaches, this\nmethod allows for automatic calibration to a given default probability curve.\nWe derive analytically the positive exposures $V^+_t$ \"conditional upon\ndefault\" associated to prototypical market price processes of FRA and IRS in\nall cases. We further discuss the link between the \"default\" condition and\nchange-of-measure techniques. The expectation of $V^+_t$ conditional upon\n$\\tau=t$ is equal to the unconditional expectation of $V^+_t\\zeta_t$. The\nprocess $\\zeta$ is explicitly derived in the dynamic approaches: it is proven\nto be positive and to have unit expectation. Unfortunately however, it fails to\nbe a martingale, so that Girsanov machinery cannot be used. Nevertheless, the\nexpectation of $V^+_t\\zeta_t$ can be computed explicitly, leading to analytical\nexpected positive exposure profiles in the considered examples.\n"
    },
    {
        "paper_id": 1605.05545,
        "authors": "Daniel Treisman",
        "title": "Elections in Russia, 1991-2008",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, I review the main trends in voting in national elections in\nRussia since 1991, discuss the evidence of manipulation or falsification by the\nauthorities, and use statistical techniques to examine the determinants of\nvoting trends.\n"
    },
    {
        "paper_id": 1605.05631,
        "authors": "Yonatan Berman, Ole Peters, Alexander Adamou",
        "title": "Far from equilibrium: Wealth reallocation in the United States",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Studies of wealth inequality often assume that an observed wealth\ndistribution reflects a system in equilibrium. This constraint is rarely tested\nempirically. We introduce a simple model that allows equilibrium but does not\nassume it. To geometric Brownian motion (GBM) we add reallocation: all\nindividuals contribute in proportion to their wealth and receive equal shares\nof the amount collected. We fit the reallocation rate parameter required for\nthe model to reproduce observed wealth inequality in the United States from\n1917 to 2012. We find that this rate was positive until the 1980s, after which\nit became negative and of increasing magnitude. With negative reallocation, the\nsystem cannot equilibrate. Even with the positive reallocation rates observed,\nequilibration is too slow to be practically relevant. Therefore, studies which\nassume equilibrium must be treated skeptically. By design they are unable to\ndetect the dramatic conditions found here when data are analysed without this\nconstraint.\n"
    },
    {
        "paper_id": 1605.05802,
        "authors": "Shaolin Ji and Xiaomin Shi",
        "title": "Recursive utility maximization under partial information",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper concerns the recursive utility maximization problem under partial\ninformation. We first transform our problem under partial information into the\none under full information. When the generator of the recursive utility is\nconcave, we adopt the variational formulation of the recursive utility which\nleads to a stochastic game problem and a characterization of the saddle point\nof the game is obtained. Then, we study the K-ignorance case and explicit\nsaddle points of several examples are obtained. At last, when the generator of\nthe recursive utility is smooth, we employ the terminal perturbation method to\ncharacterize the optimal terminal wealth.\n"
    },
    {
        "paper_id": 1605.05814,
        "authors": "Y. Bai, E. Hashorva, G. Ratovomirija, M. Tamraz",
        "title": "Some Mathematical Aspects of Price Optimisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Calculation of an optimal tariff is a principal challenge for pricing\nactuaries. In this contribution we are concerned with the renewal insurance\nbusiness discussing various mathematical aspects of calculation of an optimal\nrenewal tariff. Our motivation comes from two important actuarial tasks, namely\na) construction of an optimal renewal tariff subject to business and technical\nconstraints, and b) determination of an optimal allocation of certain premium\nloadings. We consider both continuous and discrete optimisation and then\npresent several algorithmic sub-optimal solutions. Additionally, we explore\nsome simulation techniques. Several illustrative examples show both the\ncomplexity and the importance of the optimisation approach.\n"
    },
    {
        "paper_id": 1605.05819,
        "authors": "Soumik Pal, Ting-Kam Leonard Wong",
        "title": "Exponentially concave functions and a new information geometry",
        "comments": "49 pages, 3 figures; revised. To appear in the Annals of Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A function is exponentially concave if its exponential is concave. We\nconsider exponentially concave functions on the unit simplex. In a previous\npaper we showed that gradient maps of exponentially concave functions provide\nsolutions to a Monge-Kantorovich optimal transport problem and give a better\ngradient approximation than those of ordinary concave functions. The\napproximation error, called L-divergence, is different from the usual Bregman\ndivergence. Using tools of information geometry and optimal transport, we show\nthat L-divergence induces a new information geometry on the simplex consisting\nof a Riemannian metric and a pair of dually coupled affine connections which\ndefines two kinds of geodesics. We show that the induced geometry is dually\nprojectively flat but not flat. Nevertheless, we prove an analogue of the\ncelebrated generalized Pythagorean theorem from classical information geometry.\nOn the other hand, we consider displacement interpolation under a Lagrangian\nintegral action that is consistent with the optimal transport problem and show\nthat the action minimizing curves are dual geodesics. The Pythagorean theorem\nis also shown to have an interesting application of determining the optimal\ntrading frequency in stochastic portfolio theory.\n"
    },
    {
        "paper_id": 1605.06301,
        "authors": "Philippe Briand (LAMA), Romuald Elie (LAMA), Ying Hu (IRMAR)",
        "title": "BSDEs with mean reflection",
        "comments": null,
        "journal-ref": "The Annals of Applied Probability 2018, Vol. 24, No. 1, 1129-1171",
        "doi": "10.1214/17-AAP1310",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a new type of BSDE, where the distribution of the\nY-component of the solution is required to satisfy an additional constraint,\nwritten in terms of the expectation of a loss function. This constraint is\nimposed at any deterministic time t and is typically weaker than the classical\npointwise one associated to reflected BSDEs. Focusing on solutions (Y, Z, K)\nwith deterministic K, we obtain the well-posedness of such equation, in the\npresence of a natural Skorokhod type condition. Such condition indeed ensures\nthe minimality of the enhanced solution, under an additional structural\ncondition on the driver. Our results extend to the more general framework where\nthe constraint is written in terms of a static risk measure on Y. In\nparticular, we provide an application to the super hedging of claims under\nrunning risk management constraint.\n"
    },
    {
        "paper_id": 1605.06429,
        "authors": "Sebastian Herrmann, Johannes Muhle-Karbe, Frank Thomas Seifried",
        "title": "Hedging with Small Uncertainty Aversion",
        "comments": "48 pages; forthcoming in 'Finance and Stochastics'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the pricing and hedging of derivative securities with uncertainty\nabout the volatility of the underlying asset. Rather than taking all models\nfrom a prespecified class equally seriously, we penalise less plausible ones\nbased on their \"distance\" to a reference local volatility model. In the limit\nfor small uncertainty aversion, this leads to explicit formulas for prices and\nhedging strategies in terms of the security's cash gamma.\n"
    },
    {
        "paper_id": 1605.06482,
        "authors": "Kenichiro McAlinn, Asahi Ushio, Teruo Nakatsuma",
        "title": "Volatility Forecasts Using Nonlinear Leverage Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The leverage effect-- the correlation between an asset's return and its\nvolatility-- has played a key role in forecasting and understanding volatility\nand risk. While it is a long standing consensus that leverage effects exist and\nimprove forecasts, empirical evidence paradoxically do not show that most\nindividual stocks exhibit this phenomena, mischaracterizing risk and therefore\nleading to poor predictive performance. We examine this paradox, with the goal\nto improve density forecasts, by relaxing the assumption of linearity in the\nleverage effect. Nonlinear generalizations of the leverage effect are proposed\nwithin the Bayesian stochastic volatility framework in order to capture\nflexible leverage structures, where small fluctuations in prices have a\ndifferent effect from large shocks. Efficient Bayesian sequential computation\nis developed and implemented to estimate this effect in a practical, on-line\nmanner. Examining 615 stocks that comprise the S\\&P500 and Nikkei 225, we find\nthat relaxing the linear assumption to our proposed nonlinear leverage effect\nfunction improves predictive performances for 89\\% of all stocks compared to\nthe conventional model assumption.\n"
    },
    {
        "paper_id": 1605.067,
        "authors": "Lisana B. Martinez, M. Belen Guercio, Aurelio F. Bariviera, Antonio\n  Terce\\~no",
        "title": "The impact of the financial crisis on the long-range memory of European\n  corporate bond and stock markets",
        "comments": null,
        "journal-ref": "Empirica, pp. 1-15, 2016",
        "doi": "10.1007/s10663-016-9340-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the presence of long memory in corporate bond and\nstock indices of six European Union countries from July 1998 to February 2015.\nWe compute the Hurst exponent by means of the DFA method and using a sliding\nwindow in order to measure long range dependence. We detect that Hurst\nexponents behave differently in the stock and bond markets, being smoother in\nthe stock indices than in the bond indices. We verify that the level of\ninformational efficiency is time-varying. Moreover we find an asymmetric impact\nof the 2008 financial crisis in the fixed income and the stock markets,\naffecting the former but not the latter. Similar results are obtained using the\nR/S method.\n"
    },
    {
        "paper_id": 1605.0684,
        "authors": "Takashi Shinzato",
        "title": "Asymptotic Eigenvalue Distribution of Wishart Matrices whose Components\n  are not Independently and Identically Distributed",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work, eigenvalue distributions defined by a random rectangular\nmatrix whose components are neither independently nor identically distributed\nare analyzed using replica analysis and belief propagation. In particular, we\nconsider the case in which the components are independently but not identically\ndistributed; for example, only the components in each row or in each column may\nbe {identically distributed}. We also consider the more general case in which\nthe components are correlated with one another. We use the replica approach\nwhile making only weak assumptions in order to determine the asymptotic\neigenvalue distribution and to derive an algorithm for doing so, based on\nbelief propagation. One of our findings supports the results obtained from\nFeynman diagrams. We present the results of several numerical experiments that\nvalidate our proposed methods.\n"
    },
    {
        "paper_id": 1605.06843,
        "authors": "Takashi Shinzato",
        "title": "Portfolio Optimization Problem with Non-identical Variances of Asset\n  Returns using Statistical Mechanical Informatics",
        "comments": "11 pages, 4 figures",
        "journal-ref": "Phys. Rev. E 94, 062102 (2016)",
        "doi": "10.1103/PhysRevE.94.062102",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The portfolio optimization problem in which the variances of the return rates\nof assets are not identical is analyzed in this paper using the methodology of\nstatistical mechanical informatics, specifically, replica analysis. We define\ntwo characteristic quantities of an optimal portfolio, namely, minimal\ninvestment risk and concentrated investment level, in order to solve the\nportfolio optimization problem and analytically determine their asymptotical\nbehaviors using replica analysis. Moreover, numerical experiments were\nperformed, and a comparison between the results of our simulation and those\nobtained via replica analysis validated our proposed method.\n"
    },
    {
        "paper_id": 1605.06845,
        "authors": "Takashi Shinzato",
        "title": "Minimal Investment Risk of Portfolio Optimization Problem with Budget\n  and Investment Concentration Constraints",
        "comments": "8 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa56a0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, the minimal investment risk for a portfolio\noptimization problem with imposed budget and investment concentration\nconstraints is considered using replica analysis. Since the minimal investment\nrisk is influenced by the investment concentration constraint (as well as the\nbudget constraint), it is intuitive that the minimal investment risk for the\nproblem with an investment concentration constraint be larger than that without\nthe constraint (that is, with only the budget constraint). Moreover, a\nnumerical experiment shows the effectiveness of our proposed analysis.\n"
    },
    {
        "paper_id": 1605.06849,
        "authors": "Xiaoqing Liang and Zbigniew Palmowski",
        "title": "A note on optimal expected utility of dividend payments with\n  proportional reinsurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of maximizing the expected discounted\nutility of dividend payments for an insurance company that controls risk\nexposure by purchasing proportional reinsurance. We assume the preference of\nthe insurer is of CRRA form. By solving the corresponding\nHamilton-Jacobi-Bellman equation, we identify the value function and the\ncorresponding optimal strategy. We also analyze the asymptotic behavior of the\nvalue function for large initial reserves. Finally, we provide some numerical\nexamples to illustrate the results and analyze the sensitivity of the\nparameters.\n"
    },
    {
        "paper_id": 1605.07099,
        "authors": "Damien Ackerer, Damir Filipovi\\'c, Sergio Pulido",
        "title": "The Jacobi Stochastic Volatility Model",
        "comments": "32 pages, 5 Figures, 1 Table",
        "journal-ref": "Finance and Stochastics, Volume 22, Issue 3, Pages 667-700, 2018",
        "doi": "10.1007/s00780-018-0364-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel stochastic volatility model where the squared volatility\nof the asset return follows a Jacobi process. It contains the Heston model as a\nlimit case. We show that the joint density of any finite sequence of log\nreturns admits a Gram-Charlier A expansion with closed-form coefficients. We\nderive closed-form series representations for option prices whose discounted\npayoffs are functions of the asset price trajectory at finitely many time\npoints. This includes European call, put, and digital options, forward start\noptions, and can be applied to discretely monitored Asian options. In a\nnumerical analysis we show that option prices can be accurately and efficiently\napproximated by truncating their series representations.\n"
    },
    {
        "paper_id": 1605.0723,
        "authors": "J. B. Heaton, N. G. Polson, J. H. Witte",
        "title": "Deep Portfolio Theory",
        "comments": "17 Pages, 3 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a deep portfolio theory. By building on Markowitz's classic\nrisk-return trade-off, we develop a self-contained four-step routine of encode,\ncalibrate, validate and verify to formulate an automated and general portfolio\nselection process. At the heart of our algorithm are deep hierarchical\ncompositions of portfolios constructed in the encoding step. The calibration\nstep then provides multivariate payouts in the form of deep hierarchical\nportfolios that are designed to target a variety of objective functions. The\nvalidate step trades-off the amount of regularization used in the encode and\ncalibrate steps. The verification step uses a cross validation approach to\ntrace out an ex post deep portfolio efficient frontier. We demonstrate all four\nsteps of our portfolio theory numerically.\n"
    },
    {
        "paper_id": 1605.07278,
        "authors": "Dhanya Jothimani, Ravi Shankar, Surendra S. Yadav",
        "title": "Discrete Wavelet Transform-Based Prediction of Stock Index: A Study on\n  National Stock Exchange Fifty Index",
        "comments": null,
        "journal-ref": "Journal of Financial Management and Analysis Vol. 28 Iss. 2 (2015)\n  35-49",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial Times Series such as stock price and exchange rates are, often,\nnon-linear and non-stationary. Use of decomposition models has been found to\nimprove the accuracy of predictive models. The paper proposes a hybrid approach\nintegrating the advantages of both decomposition model (namely, Maximal Overlap\nDiscrete Wavelet Transform (MODWT)) and machine learning models (ANN and SVR)\nto predict the National Stock Exchange Fifty Index. In first phase, the data is\ndecomposed into a smaller number of subseries using MODWT. In next phase, each\nsubseries is predicted using machine learning models (i.e., ANN and SVR). The\npredicted subseries are aggregated to obtain the final forecasts. In final\nstage, the effectiveness of the proposed approach is evaluated using error\nmeasures and statistical test. The proposed methods (MODWT-ANN and MODWT-SVR)\nare compared with ANN and SVR models and, it was observed that the return on\ninvestment obtained based on trading rules using predicted values of MODWT-SVR\nmodel was higher than that of Buy-and-hold strategy.\n"
    },
    {
        "paper_id": 1605.07419,
        "authors": "Damien Ackerer and Damir Filipovi\\'c",
        "title": "Linear Credit Risk Models",
        "comments": "forthcoming in Finance and Stochastics, 49 pages, 3 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel class of credit risk models in which the drift of the\nsurvival process of a firm is a linear function of the factors. The prices of\ndefaultable bonds and credit default swaps (CDS) are linear-rational in the\nfactors. The price of a CDS option can be uniformly approximated by polynomials\nin the factors. Multi-name models can produce simultaneous defaults, generate\npositively as well as negatively correlated default intensities, and\naccommodate stochastic interest rates. A calibration study illustrates the\nversatility of these models by fitting CDS spread time series. A numerical\nanalysis validates the efficiency of the option price approximation method.\n"
    },
    {
        "paper_id": 1605.075,
        "authors": "Christian Bender, Christian Gaertner, Nikolaus Schweizer",
        "title": "Pathwise Iteration for Backward SDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel numerical approach for a class of stochastic dynamic\nprograms which arise as discretizations of backward stochastic differential\nequations or semi-linear partial differential equations. Solving such dynamic\nprograms numerically requires the approximation of nested conditional\nexpectations, i.e., iterated integrals of previous approximations. Our approach\nallows us to compute and iteratively improve upper and lower bounds on the true\nsolution starting from an arbitrary and possibly crude input approximation. We\ndemonstrate the benefits of our approach in a high dimensional financial\napplication.\n"
    },
    {
        "paper_id": 1605.0768,
        "authors": "Hugo Cruz-Sanchez",
        "title": "Generalized Subjective Lexicographic Expected Utility Representation",
        "comments": "Originally available on November 18, 2014, in my website\n  (https://sites.google.com/site/hugocruzsanchez/)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide foundations for decisions in face of unlikely events by extending\nthe standard framework of Savage to include preferences indexed by a family of\nevents. We derive a subjective lexicographic expected utility representation\nwhich allows for infinitely many lexicographically ordered levels of events and\nfor event-dependent attitudes toward risk. Our model thus provides foundations\nfor models in finance that rely on different attitudes toward risk (e.g.\nSkiadas [9]) and for off-equilibrium reasonings in infinite dynamic games, thus\nextending and generalizing the analysis in Blume, Brandenburger and Dekel [3].\n"
    },
    {
        "paper_id": 1605.07884,
        "authors": "Emmanuel Lepinette and Ilya Molchanov",
        "title": "Risk Arbitrage and Hedging to Acceptability under Transaction Costs",
        "comments": "31 page. Accepted for publication in Finance and Stochastics",
        "journal-ref": "Finance and Stochastics, 2021, 25, 101-132",
        "doi": "10.1007/s00780-020-00434-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical discrete time model of proportional transaction costs relies on\nthe assumption that a feasible portfolio process has solvent increments at each\nstep. We extend this setting in two directions, allowing for convex transaction\ncosts and assuming that increments of the portfolio process belong to the sum\nof a solvency set and a family of multivariate acceptable positions, e.g. with\nrespect to a dynamic risk measure. We describe the sets of superhedging prices,\nformulate several no (risk) arbitrage conditions and explore connections\nbetween them. In the special case when multivariate positions are converted\ninto a single fixed asset, our framework turns into the no good deals setting.\nHowever, in general, the possibilities of assessing the risk with respect to\nany asset or a basket of the assets lead to a decrease of superhedging prices\nand the no arbitrage conditions become stronger. The mathematical technique\nrelies on results for unbounded and possibly non-closed random sets in\nEuclidean space.\n"
    },
    {
        "paper_id": 1605.07945,
        "authors": "Jiao Li",
        "title": "Trading VIX Futures under Mean Reversion with Regime Switching",
        "comments": "16 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1601.04210",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal VIX futures trading problems under a\nregime-switching model. We consider the VIX as mean reversion dynamics with\ndependence on the regime that switches among a finite number of states. For the\ntrading strategies, we analyze the timings and sequences of the investor's\nmarket participation, which leads to several corresponding coupled system of\nvariational inequalities. The numerical approach is developed to solve these\noptimal double stopping problems by using projected-successive-over-relaxation\n(PSOR) method with Crank-Nicolson scheme. We illustrate the optimal boundaries\nvia numerical examples of two-state Markov chain model. In particular, we\nexamine the impacts of transaction costs and regime-switching timings on the\nVIX futures trading strategies.\n"
    },
    {
        "paper_id": 1605.08025,
        "authors": "Siwat Nakmai",
        "title": "Foreign exchange risk premia: from traditional to state-space analyses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines foreign exchange risk premia from simple univariate\nregressions to the state-space method. The adjusted traditional regressions\nproperly figure out the existence and time-evolving property of the risk\npremia. Successively, the state-space estimations overall are quite rationally\ncompetent in examining the essence of time variability of the unobservable risk\npremia. To be more precise, the coefficients on the lagged estimated\ntime-series are significant and the disturbance combined from the observation\nand transition equations in the state-space system, rational and premium\nerrors, respectively, is statistically white noise. Such the two residuals are\ndiscovered to move oppositely with their covariance approaching zero suggested\nby the empirics. Besides, foreign exchange risk premia are projected and found\nsignificantly stationary at level and relatively volatile throughout time with\nsome clustering. This volatility is however not quite dominant in the\ndeviations of forward prediction errors.\n"
    },
    {
        "paper_id": 1605.08099,
        "authors": "Romuald Elie and Dylan Possama\\\"i",
        "title": "Contracting theory with competitive interacting agents",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a framework close to the one developed by Holmstr\\\"om and Milgrom [44], we\nstudy the optimal contracting scheme between a Principal and several Agents.\nEach hired Agent is in charge of one project, and can make efforts towards\nmanaging his own project, as well as impact (positively or negatively) the\nprojects of the other Agents. Considering economic Agents in competition with\nrelative performance concerns, we derive the optimal contracts in both first\nbest and moral hazard settings. The enhanced resolution methodology relies\nheavily on the connection between Nash equilibria and multidimensional\nquadratic BSDEs. The optimal contracts are linear and each agent is paid a\nfixed proportion of the terminal value of all the projects of the firm.\nBesides, each Agent receives his reservation utility, and those with high\ncompetitive appetence are assigned less volatile projects, and shall even\nreceive help from the other Agents. From the principal point of view, it is in\nthe firm interest in our model to strongly diversify the competitive appetence\nof the Agents.\n"
    },
    {
        "paper_id": 1605.08166,
        "authors": "Christian Mullon and Charles Mullon",
        "title": "A constraint-based framework to study rationality, competition and\n  cooperation in fisheries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a simplified framework to represent competition,\ncoordination and bargaining in fisheries when they operate under financial and\ntechnological constraints. Competition within constraints leads to a particular\ntype of mathematical game in which the strategy choice by one player changes\nstrategy set of the other. By studying the equilibria and bargaining space of\nthis game when players maximize either profit or fishing capacity, we highlight\nthat differences in financial constraints among players leads to a tougher\nplay, with a reduced bargaining space as the least constrained player can\nreadily exclude another from the competition. The exacerbating effects of\nconstraints on competition are even stronger when players maximize capacity. We\ndiscuss the significance of our results for global ocean governance in a\ncurrent context characterized by financialization and technological\ndevelopment. We suggest that in order to maximize the chances of fruitful\nnegociations and aim towards a fair sharing of sea resources, it would be\nhelpful to focus on leveling current differences in the constraints faced\nbetween competing fishing systems by supporting local financial systems and\ntechnological control, before implementing sophisticated economic tools.\n"
    },
    {
        "paper_id": 1605.08354,
        "authors": "Anirban Chakraborti, Dhruv Raina, and Kiran Sharma",
        "title": "Can an interdisciplinary field contribute to one of the parent\n  disciplines from which it emerged?",
        "comments": "9 pages, EPJ format. To appear in the European Physical Journal\n  Special Topics, entitled \"Can economics be a physical science?\" Eds. A.S.\n  Chakrabarti et al",
        "journal-ref": "The European Physical Journal Special Topics, 225(17), 3127-3135\n  (2016)",
        "doi": "10.1140/epjst/e2016-60115-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the light of contemporary discussions of inter and transdisciplinarity,\nthis paper approaches econophysics and sociophysics to seek a response to the\nquestion -- whether these interdisciplinary fields could contribute to physics\nand economics. Drawing upon the literature on history and philosophy of\nscience, the paper argues that the two way traffic between physics and\neconomics has a long history and this is likely to continue in the future.\n"
    },
    {
        "paper_id": 1605.08899,
        "authors": "Rafael A. Barrio, Tzipe Govezensky, \\'Elfego Ruiz-Guti\\'errez, Kimmo\n  K. Kaski",
        "title": "Modelling Trading Networks and the Role of Trust",
        "comments": "8 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.11.144",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple dynamical model for describing trading interactions\nbetween agents in a social network by considering only two dynamical variables,\nnamely money and goods or services, that are assumed conserved over the whole\ntime span of the agents' trading transactions. A key feature of the model is\nthat agent-to-agent transactions are governed by the price in units of money\nper goods, which is dynamically changing, and by a trust variable, which is\nrelated to the trading history of each agent. All agents are able to sell or\nbuy, and the decision to do either has to do with the level of trust the buyer\nhas in the seller, the price of the goods and the amount of money and goods at\nthe disposal of the buyer. Here we show the results of extensive numerical\ncalculations under various initial conditions in a random network of agents and\ncompare the results with the available related data. In most cases the\nagreement between the model results and real data turns out to be fairly good,\nwhich allow us to draw some general conclusions as how different trading\nstrategies could affect the distribution of wealth in different kinds of\nsocieties.\n"
    },
    {
        "paper_id": 1605.08908,
        "authors": "Nicol\\'o Musmeci, Tomaso Aste, Tiziana Di Matteo",
        "title": "What does past correlation structure tell us about the future? An answer\n  from network filtering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discovered that past changes in the market correlation structure are\nsignificantly related with future changes in the market volatility. By using\ncorrelation-based information filtering networks we device a new tool for\nforecasting the market volatility changes. In particular, we introduce a new\nmeasure, the \"correlation structure persistence\", that quantifies the rate of\nchange of the market dependence structure. This measure shows a deep interplay\nwith changes in volatility and we demonstrate it can anticipate market risk\nvariations. Notably, our method overcomes the curse of dimensionality that\nlimits the applicability of traditional econometric tools to portfolios made of\na large number of assets. We report on forecasting performances and statistical\nsignificance of this tool for two different equity datasets. We also identify\nan optimal region of parameters in terms of True Positive and False Positive\ntrade-off, through a ROC curve analysis. We find that our forecasting method is\nrobust and it outperforms predictors based on past volatility only. Moreover\nthe temporal analysis indicates that our method is able to adapt to abrupt\nchanges in the market, such as financial crises, more rapidly than methods\nbased on past volatility.\n"
    },
    {
        "paper_id": 1605.09112,
        "authors": "Marcel Nutz",
        "title": "A Mean Field Game of Optimal Stopping",
        "comments": "Forthcoming in 'SIAM Journal on Control and Optimization'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate a stochastic game of mean field type where the agents solve\noptimal stopping problems and interact through the proportion of players that\nhave already stopped. Working with a continuum of agents, typical equilibria\nbecome functions of the common noise that all agents are exposed to, whereas\nidiosyncratic randomness can be eliminated by an Exact Law of Large Numbers.\nUnder a structural monotonicity assumption, we can identify equilibria with\nsolutions of a simple equation involving the distribution function of the\nidiosyncratic noise. Solvable examples allow us to gain insight into the\nuniqueness of equilibria and the dynamics in the population.\n"
    },
    {
        "paper_id": 1605.09181,
        "authors": "Krzysztof Domino",
        "title": "The use of the multi-cumulant tensor analysis for the algorithmic\n  optimisation of investment portfolios",
        "comments": "19 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.10.042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cumulant analysis plays an important role in non Gaussian distributed\ndata analysis. The shares' prices returns are good example of such data. The\npurpose of this research is to develop the cumulant based algorithm and use it\nto determine eigenvectors that represent investment portfolios with low\nvariability. Such algorithm is based on the Alternating Least Square method and\ninvolves the simultaneous minimisation 2'nd -- 6'th cumulants of the\nmultidimensional random variable (percentage shares' returns of many\ncompanies). Then the algorithm was tested during the recent crash on the Warsaw\nStock Exchange. To determine incoming crash and provide enter and exit signal\nfor the investment strategy the Hurst exponent was calculated using the local\nDFA. It was shown that introduced algorithm is on average better that benchmark\nand other portfolio determination methods, but only within examination window\ndetermined by low values of the Hurst exponent. Remark that the algorithm of is\nbased on cumulant tensors up to the 6'th order calculated for a\nmultidimensional random variable, what is the novel idea. It can be expected\nthat the algorithm would be useful in the financial data analysis on the world\nwide scale as well as in the analysis of other types of non Gaussian\ndistributed data.\n"
    },
    {
        "paper_id": 1605.09484,
        "authors": "Man Chung Fung, Gareth W. Peters, Pavel V. Shevchenko",
        "title": "A unified approach to mortality modelling using state-space framework:\n  characterisation, identification, estimation and forecasting",
        "comments": "46 pages",
        "journal-ref": "Annals of Actuarial Science 11 (2), pp. 343-389, 2017",
        "doi": "10.1017/S1748499517000069",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores and develops alternative statistical representations and\nestimation approaches for dynamic mortality models. The framework we adopt is\nto reinterpret popular mortality models such as the Lee-Carter class of models\nin a general state-space modelling methodology, which allows modelling,\nestimation and forecasting of mortality under a unified framework. Furthermore,\nwe propose an alternative class of model identification constraints which is\nmore suited to statistical inference in filtering and parameter estimation\nsettings based on maximization of the marginalized likelihood or in Bayesian\ninference. We then develop a novel class of Bayesian state-space models which\nincorporate apriori beliefs about the mortality model characteristics as well\nas for more flexible and appropriate assumptions relating to heteroscedasticity\nthat present in observed mortality data. We show that multiple period and\ncohort effect can be cast under a state-space structure. To study long term\nmortality dynamics, we introduce stochastic volatility to the period effect.\nThe estimation of the resulting stochastic volatility model of mortality is\nperformed using a recent class of Monte Carlo procedure specifically designed\nfor state and parameter estimation in Bayesian state-space models, known as the\nclass of particle Markov chain Monte Carlo methods. We illustrate the framework\nwe have developed using Danish male mortality data, and show that incorporating\nheteroscedasticity and stochastic volatility markedly improves model fit\ndespite an increase of model complexity. Forecasting properties of the enhanced\nmodels are examined with long term and short term calibration periods on the\nreconstruction of life tables.\n"
    },
    {
        "paper_id": 1605.0972,
        "authors": "Roman Gayduk and Sergey Nadtochiy",
        "title": "Endogenous Formation of Limit Order Books: Dynamics Between Trades",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we present a continuous-time large-population game for modeling\nmarket microstructure betweentwo consecutive trades. The proposed modeling\nframework is inspired by our previous work [23]. In this framework, the Limit\nOrder Book (LOB) arises as an outcome of an equilibrium between multiple agents\nwho have different beliefs about the future demand for the asset. The agents'\nbeliefs may change according to the information they observe, triggering\nchanges in their behavior. We present an example illustrating how the proposed\nmodels can be used to quantify the consequences of changes in relevant\ninformation signals. If these signals, themselves, depend on the LOB, then, our\napproach allows one to model the \"indirect\" market impact (as opposed to the\n\"direct\" impact that a market order makes on the LOB, by eliminating certain\nlimit orders). On the mathematical side, we formulate the proposed modeling\nframework as a continuum-player control-stopping game. We manage to split the\nequilibrium problem into two parts. The first one is described by a\ntwo-dimensional system of Reflected Backward Stochastic Differential Equations\n(RBSDEs), whose solution components reflect against each other. The second one\nleads to an infinite-dimensional fixed-point problem for a discontinuous\nmapping. Both problems are non-standard, and we prove the existence of their\nsolutions in the paper.\n"
    },
    {
        "paper_id": 1606.00092,
        "authors": "Tatsushi Oka and Pierre Perron",
        "title": "Testing for Common Breaks in a Multiple Equations System",
        "comments": "44 pages, 2 tables and 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The issue addressed in this paper is that of testing for common breaks across\nor within equations of a multivariate system. Our framework is very general and\nallows integrated regressors and trends as well as stationary regressors. The\nnull hypothesis is that breaks in different parameters occur at common\nlocations and are separated by some positive fraction of the sample size unless\nthey occur across different equations. Under the alternative hypothesis, the\nbreak dates across parameters are not the same and also need not be separated\nby a positive fraction of the sample size whether within or across equations.\nThe test considered is the quasi-likelihood ratio test assuming normal errors,\nthough as usual the limit distribution of the test remains valid with\nnon-normal errors. Of independent interest, we provide results about the rate\nof convergence of the estimates when searching over all possible partitions\nsubject only to the requirement that each regime contains at least as many\nobservations as some positive fraction of the sample size, allowing break dates\nnot separated by a positive fraction of the sample size across equations.\nSimulations show that the test has good finite sample properties. We also\nprovide an application to issues related to level shifts and persistence for\nvarious measures of inflation to illustrate its usefulness.\n"
    },
    {
        "paper_id": 1606.00142,
        "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher",
        "title": "Model selection consistency from the perspective of generalization\n  ability and VC theory with an application to Lasso",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Model selection is difficult to analyse yet theoretically and empirically\nimportant, especially for high-dimensional data analysis. Recently the least\nabsolute shrinkage and selection operator (Lasso) has been applied in the\nstatistical and econometric literature. Consis- tency of Lasso has been\nestablished under various conditions, some of which are difficult to verify in\npractice. In this paper, we study model selection from the perspective of\ngeneralization ability, under the framework of structural risk minimization\n(SRM) and Vapnik-Chervonenkis (VC) theory. The approach emphasizes the balance\nbetween the in-sample and out-of-sample fit, which can be achieved by using\ncross-validation to select a penalty on model complexity. We show that an exact\nrelationship exists between the generalization ability of a model and model\nselection consistency. By implementing SRM and the VC inequality, we show that\nLasso is L2-consistent for model selection under assumptions similar to those\nimposed on OLS. Furthermore, we derive a probabilistic bound for the distance\nbetween the penalized extremum estimator and the extremum estimator without\npenalty, which is dominated by overfitting. We also propose a new measurement\nof overfitting, GR2, based on generalization ability, that converges to zero if\nmodel selection is consistent. Using simulations, we demonstrate that the\nproposed CV-Lasso algorithm performs well in terms of model selection and\noverfitting control.\n"
    },
    {
        "paper_id": 1606.00424,
        "authors": "Marco Pangallo, Jean Pierre Nadal, Annick Vignes",
        "title": "Residential income segregation: A behavioral model of the housing market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We represent the functioning of the housing market and study the relation\nbetween income segregation, income inequality and house prices by introducing a\nspatial Agent-Based Model (ABM). Differently from traditional models in urban\neconomics, we explicitly specify the behavior of buyers and sellers and the\nprice formation mechanism. Buyers who differ by income select among\nheterogeneous neighborhoods using a probabilistic model of residential choice;\nsellers employ an aspiration level heuristic to set their reservation offer\nprice; prices are determined through a continuous double auction. We first\nprovide an approximate analytical solution of the ABM, shedding light on the\nstructure of the model and on the effect of the parameters. We then simulate\nthe ABM and find that: (i) a more unequal income distribution lowers the prices\nglobally, but implies stronger segregation; (ii) a spike of the demand in one\npart of the city increases the prices all over the city; (iii) subsidies are\nmore efficient than taxes in fostering social mixing.\n"
    },
    {
        "paper_id": 1606.0053,
        "authors": "Jerome Detemple and Yerkin Kitapbayev",
        "title": "On American VIX options under the generalized 3/2 and 1/2 models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/mafi.12153",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the 3/2-model for VIX studied by Goard and Mazur\n(2013) and introduce the generalized 3/2 and 1/2 classes of volatility\nprocesses. Under these models, we study the pricing of European and American\nVIX options and, for the latter, we obtain an early exercise premium\nrepresentation using a free-boundary approach and local time-space calculus.\nThe optimal exercise boundary for the volatility is obtained as the unique\nsolution to an integral equation of Volterra type.\n  We also consider a model mixing these two classes and formulate the\ncorresponding optimal stopping problem in terms of the observed factor process.\nThe price of an American VIX call is then represented by an early exercise\npremium formula. We show the existence of a pair of optimal exercise boundaries\nfor the factor process and characterize them as the unique solution to a system\nof integral equations.\n"
    },
    {
        "paper_id": 1606.00631,
        "authors": "Beatrice Acciaio, Martin Larsson, Walter Schachermayer",
        "title": "The space of outcomes of semi-static trading strategies need not be\n  closed",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Semi-static trading strategies make frequent appearances in mathematical\nfinance, where dynamic trading in a liquid asset is combined with static\nbuy-and-hold positions in options on that asset. We show that the space of\noutcomes of such strategies can have very poor closure properties when all\nEuropean options for a fixed date $T$ are available for static trading. This\ncauses problems for optimal investment, and stands in sharp contrast to the\npurely dynamic case classically considered in mathematical finance.\n"
    },
    {
        "paper_id": 1606.01218,
        "authors": "Marcin W\\k{a}torek, Stanis{\\l}aw Dro\\.zd\\.z, Pawe{\\l} O\\'swi\\k{e}cimka",
        "title": "World Financial 2014-2016 Market Bubbles: Oil Negative - US Dollar\n  Positive",
        "comments": "presented at FENS2015 conference, to appear in Acta Physica Polonica\n  A. arXiv admin note: text overlap with arXiv:1003.5926 by other authors",
        "journal-ref": "M. W\\k{a}torek, S. Dro\\.zd\\.z and P. O\\'swi\\k{e}cimka, Acta\n  Physica Polonica A 129(5), 932-936. (2016)",
        "doi": "10.12693/APhysPolA.129.932",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the Log-Periodic Power Law (LPPL) methodology, with the universal\npreferred scaling factor $\\lambda \\approx 2$, the negative bubble on the oil\nmarket in 2014-2016 has been detected. Over the same period a positive bubble\non the so called commodity currencies expressed in terms of the US dollar\nappears to take place with the oscillation pattern which largely is mirror\nreflected relative to oil price oscillation pattern. This documents recent\nstrong anti-correlation between the dynamics of the oil price and of the USD. A\nrelated forecast made at the time of FENS 2015 conference (beginning of\nNovember) turned out to be quite satisfactory. These findings provide also\nfurther indication that such a log-periodically accelerating down-trend signals\ntermination of the corresponding decreases.\n"
    },
    {
        "paper_id": 1606.0127,
        "authors": "Chengyi Tu, Joel Carr, Samir Suweis",
        "title": "A data driven network approach to rank countries production diversity\n  and food specialization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0165941",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The easy access to large data sets has allowed for leveraging methodology in\nnetwork physics and complexity science to disentangle patterns and processes\ndirectly from the data, leading to key insights in the behavior of systems.\nHere we use to country specific food production data to study binary and\nweighted topological properties of the bipartite country-food production\nmatrix. This country-food production matrix can be: 1) transformed into overlap\nmatrices which embed information regarding shared production of products among\ncountries, and or shared countries for individual products, 2) identify subsets\nof countries which produce similar commodities or subsets of commodities shared\nby a given country allowing for visualization of correlations in large\nnetworks, and 3) used to rank country's fitness (the ability to produce a\ndiverse array of products weighted on the type of food commodities) and food\nspecialization (quantified on the number of countries producing that food\nproduct weighted on their fitness). Our results show that, on average,\ncountries with high fitness producing highly specialized food commodities also\nproduce low specialization goods, while nations with low fitness producing a\nsmall basket of diverse food products, typically produce low specialized food\ncommodities.\n"
    },
    {
        "paper_id": 1606.01343,
        "authors": "Xiao Lin",
        "title": "The Zero-Coupon Rate Model for Derivatives Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to present a dual-term structure model of interest\nrate derivatives in order to solve the two hardest problems in financial\nmodeling: the exact volatility calibration of the entire swaption matrix, and\nthe calculation of bucket vegas for structured products. The model takes a\nseries of long-term zero-coupon rates as basic state variables that are driven\ndirectly by one or more Brownian motion. The model volatility is assigned in a\nmatrix form with two terms. A numerical scheme for implementing the model has\nbeen developed in the paper. At the end, several examples have been given for\nthe model calibration, the structured products pricing and the calculation of\nbucket vegas.\n"
    },
    {
        "paper_id": 1606.01495,
        "authors": "Donovan Platt, Tim Gebbie",
        "title": "The Problem of Calibrating an Agent-Based Model of High-Frequency\n  Trading",
        "comments": "28 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based models, particularly those applied to financial markets,\ndemonstrate the ability to produce realistic, simulated system dynamics,\ncomparable to those observed in empirical investigations. Despite this, they\nremain fairly difficult to calibrate due to their tendency to be\ncomputationally expensive, even with recent advances in technology. For this\nreason, financial agent-based models are frequently validated by demonstrating\nan ability to reproduce well-known log return time series and central limit\norder book stylized facts, as opposed to being rigorously calibrated to\ntransaction data. We thus apply an established financial agent-based model\ncalibration framework to a simple model of high- and low-frequency trader\ninteraction and demonstrate possible inadequacies of a stylized fact-centric\napproach to model validation. We further argue for the centrality of\ncalibration to the validation of financial agent-based models and possible\npitfalls of current approaches to financial agent-based modeling.\n"
    },
    {
        "paper_id": 1606.02045,
        "authors": "Marcel Ausloos, Franck Jovanovic, and Christophe Schinckus",
        "title": "On the \"usual\" misunderstandings between econophysics and finance: some\n  clarifications on modelling approaches and efficient market hypothesis",
        "comments": "21 pages, 155 references; prepared for International Review of\n  Financial Analysis",
        "journal-ref": "International Review of Financial Analysis 47, 7-14 (2016)",
        "doi": "10.1016/j.irfa.2016.05",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In line with the recent research and debates about econophysics and financial\neconomics, this article discusses on usual misunderstandings between the two\ndisciplines in terms of modelling and basic hypotheses. In the literature\ndevoted to econophysics, the methodology used by financial economists is\nfrequently considered as a top-down approach (starting from a priori \"first\nprinciples\") while econophysicists rather present themselves as scholars\nworking with a (empirical data prone) bottom-up approach. Although this dualist\nperspective is very common in the econophysics literature, this paper claims\nthat the distinction is very confusing and does not permit to reveal the\nessence of the differences between finance and econophysics. The distinction\nbetween these two fields is mainly investigated here through the lens of the\nEfficient Market Hypothesis in order to show that, in substance, econophysics\nand financial economics tend to have a similar approach implying that the\nmisunderstanding between these two fields at the modelling level can therefore\nbe overstepped.\n"
    },
    {
        "paper_id": 1606.02748,
        "authors": "Olga Nicoara and David White",
        "title": "A Contextual Model Of The Secessionist Rebellion in Eastern Ukraine",
        "comments": "This is an applied game theory paper, including a lengthy discussion\n  of how game theory has been used in economics papers over the years. The\n  second author is a professor of mathematics and computer science, and hopes\n  this paper will be of interest to other theorists interested in doing more\n  applied work",
        "journal-ref": "2015 Association of Private Enterprise Education International\n  Conference",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the possible contextual factors that drove some\nindividuals to lead, and others to join the pro-secessionist rebellion in the\n2013-2014 conflict in Eastern Ukraine. We expand on the existing rational\nchoice literature on revolutionary participation and rebellious movements by\nbuilding a contextual choice model accounting for both cost-benefit and\nbehavioral considerations taken by Pro-Russian militants and rebels in the\nregion of Donbass. Our model generates predictions about the characteristics of\nthe socio-political-cultural context that are most likely to ignite and sustain\nhierarchical rebel movements similar to those in Ukraine.\n"
    },
    {
        "paper_id": 1606.02783,
        "authors": "Matteo Smerlak, Bapu Vaitla",
        "title": "A non-equilibrium formulation of food security resilience",
        "comments": "15 pages, 4 figures + appendix",
        "journal-ref": "R. Soc. open sci. 4: 160874 (2017)",
        "doi": "10.1098/rsos.160874",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Resilience, the ability to recover from adverse events (\"shocks\"), is of\nfundamental importance to food security. This is especially true in poor\ncountries, where basic needs are frequently threatened by economic,\nenvironmental, and health shocks. An empirically sound formalization of the\nconcept of food security resilience, however, is lacking. Here we introduce a\ngeneral framework for quantifying resilience based on a simple definition: a\nunit is resilient if $(a)$ its long-term food security trend is not\ndeteriorating and $(b)$ the effects of shocks on this trend do not persist over\ntime. Our approach can be applied to any food security variable for which\nhigh-frequency time-series data is available, can accommodate any unit of\nanalysis (e.g., individuals, households, countries), and is especially useful\nin rapidly changing contexts wherein standard equilibrium-based economic models\nare ineffective. We illustrate our method with an analysis of per capita\nkilocalorie availability for 161 countries between 1961 and 2011. We find that\nresilient countries are not necessarily those that are characterized by high\nlevels or less volatile fluctuations of kilocalorie intake. Accordingly, food\nsecurity policies and programs will need to be tailored not only to welfare\nlevels at any one time, but also to long-run welfare dynamics.\n"
    },
    {
        "paper_id": 1606.02871,
        "authors": "K. Kanjamapornkul and Richard Pin\\v{c}\\'ak and Erik Barto\\v{s}",
        "title": "The study of Thai stock market across the 2008 financial crisis",
        "comments": "18 pages, 11 figures, 1 table",
        "journal-ref": "Physica A462 (2016) 117-133",
        "doi": "10.1016/j.physa.2016.06.078",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cohomology theory for financial market can allow us to deform Kolmogorov\nspace of time series data over time period with the explicit definition of\neight market states in grand unified theory. The anti-de Sitter space induced\nfrom a coupling behavior field among traders in case of a financial market\ncrash acts like gravitational field in financial market spacetime. Under this\nhybrid mathematical superstructure, we redefine a behavior matrix by using\nPauli matrix and modified Wilson loop for time series data. We use it to detect\nthe 2008 financial market crash by using a degree of cohomology group of sphere\nover tensor field in correlation matrix over all possible dominated stocks\nunderlying Thai SET50 Index Futures. The empirical analysis of financial tensor\nnetwork was performed with the help of empirical mode decomposition and\nintrinsic time scale decomposition of correlation matrix and the calculation of\ncloseness centrality of planar graph.\n"
    },
    {
        "paper_id": 1606.03261,
        "authors": "Arnab Chatterjee, Asim Ghosh, and Bikas K Chakrabarti",
        "title": "Socio-economic inequality: Relationship between Gini and Kolkata indices",
        "comments": "7 pages, 4 figures + 7 pages appendix with 10 data tables, 1 figure;\n  accepted in Physica A",
        "journal-ref": "Physica A 466 (2017) 583-595",
        "doi": "10.1016/j.physa.2016.09.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Socio-economic inequality is characterized from data using various indices.\nThe Gini ($g$) index, giving the overall inequality is the most common one,\nwhile the recently introduced Kolkata ($k$) index gives a measure of $1-k$\nfraction of population who possess top $k$ fraction of wealth in the society.\nHere, we show the relationship between the two indices, using both empirical\ndata and analytical estimates. The significance of their relationship has been\ndiscussed.\n"
    },
    {
        "paper_id": 1606.03325,
        "authors": "Alexander Schied, Leo Speiser, and Iryna Voloshchenko",
        "title": "Model-free portfolio theory and its functional master formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use pathwise It\\^o calculus to prove two strictly pathwise versions of the\nmaster formula in Fernholz' stochastic portfolio theory. Our first version is\nset within the framework of F\\\"ollmer's pathwise It\\^o calculus and works for\nportfolios generated from functions that may depend on the current states of\nthe market portfolio and an additional path of finite variation. The second\nversion is formulated within the functional pathwise It\\^o calculus of Dupire\n(2009) and Cont \\& Fourni\\'e (2010) and allows for portfolio-generating\nfunctionals that may depend additionally on the entire path of the market\nportfolio. Our results are illustrated by several examples and shown to work on\nempirical market data.\n"
    },
    {
        "paper_id": 1606.03388,
        "authors": "Moustapha Pemy",
        "title": "Optimal Resource Extraction in Regime Switching L\\'evy Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the problem of optimally extracting nonrenewable natural\nresource in light of various financial and economic restrictions and\nconstraints. Taking into account the fact that the market values of the main\nnatural resources i.e. oil, natural gas, copper,...,etc, fluctuate randomly\nfollowing global and seasonal macroeconomic parameters, these values are\nmodeled using Markov switching L\\'evy processes. We formulate this problem as\nfinite-time horizon combined optimal stopping and optimal control problem. We\nprove that the value function is the unique viscosity solution of the\ncorresponding Hamilton-Jacobi-Bellman equations. Moreover, we prove the\nconvergence of a finite difference approximation of the value function.\nNumerical examples are presented to illustrate these results.\n"
    },
    {
        "paper_id": 1606.0359,
        "authors": "Mihaly Ormos and Dusan Timotity",
        "title": "Market Microstructure During Financial Crisis: Dynamics of Informed and\n  Heuristic-Driven Trading",
        "comments": "15 pages, 1 figure, 7 tables and 1 appendix, Finance Research\n  Letters, (2016)",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2016.06.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We implement a market microstructure model including informed, uninformed and\nheuristic-driven investors, which latter behave in line with loss-aversion and\nmental accounting. We show that the probability of informed trading (PIN)\nvaries significantly during 2008. In contrast, the probability of\nheuristic-driven trading (PH) remains constant both before and after the\ncollapse of Lehman Brothers. Cross-sectional analysis yields that, unlike PIN,\nPH is not sensitive to size and volume effects. We show that heuristic-driven\ntraders are universally present in all market segments and their presence is\nconstant over time. Furthermore, we find that heuristic-driven investors and\ninformed traders are disjoint sets.\n"
    },
    {
        "paper_id": 1606.03595,
        "authors": "Matt V. Leduc, Stefan Thurner",
        "title": "Incentivizing Resilience in Financial Networks",
        "comments": "38 pages, 9 figures",
        "journal-ref": "Journal of Economic Dynamics & Control 82 (2017) 44-66",
        "doi": "10.1016/j.jedc.2017.05.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When banks extend loans to each other, they generate a negative externality\nin the form of systemic risk. They create a network of interbank exposures by\nwhich they expose other banks to potential insolvency cascades. In this paper,\nwe show how a regulator can use information about the financial network to\ndevise a transaction-specific tax based on a network centrality measure that\ncaptures systemic importance. Since different transactions have different\nimpact on creating systemic risk, they are taxed differently. We call this tax\na Systemic Risk Tax (SRT). We use an equilibrium concept inspired by the\nmatching markets literature to show analytically that this SRT induces a unique\nequilibrium matching of lenders and borrowers that is systemic-risk efficient,\ni.e. it minimizes systemic risk given a certain transaction volume. On the\nother hand, we show that without this SRT multiple equilibrium matchings exist,\nwhich are generally inefficient. This allows the regulator to effectively\nstimulate a `rewiring' of the equilibrium interbank network so as to make it\nmore resilient to insolvency cascades, without sacrificing transaction volume.\nMoreover, we show that a standard financial transaction tax (e.g. a Tobin-like\ntax) has no impact on reshaping the equilibrium financial network because it\ntaxes all transactions indiscriminately. A Tobin-like tax is indeed shown to\nhave a limited effect on reducing systemic risk while it decreases transaction\nvolume.\n"
    },
    {
        "paper_id": 1606.03597,
        "authors": "Mihaly Ormos and Dusan Timotity",
        "title": "Unravelling the Asymmetric Volatility Puzzle: A Novel Explanation of\n  Volatility Through Anchoring",
        "comments": "26 pages, 7 figures and 5 tables, Economic Systems, 2016",
        "journal-ref": null,
        "doi": "10.1016/j.ecosys.2015.09.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses a novel explanation for asymmetric volatility based on\nthe anchoring behavioral pattern. Anchoring as a heuristic bias causes\ninvestors focusing on recent price changes and price levels, which two lead to\na belief in continuing trend and mean-reversion respectively. The empirical\nresults support our theoretical explanation through an analysis of large price\nfluctuations in the S&P 500 and the resulting effects on implied and realized\nvolatility. These results indicate that asymmetry (a negative relationship)\nbetween shocks and volatility in the subsequent period indeed exists. Moreover,\ncontrary to previous research, our empirical tests also suggest that implied\nvolatility is not simply an upward biased predictor of future deviation\ncompensating for the variance of the volatility but rather, due to investors\nsystematic anchoring to losses and gains in their volatility forecasts, it is a\nco-integrated yet asymmetric over/under estimated financial instrument. We also\nprovide results indicating that the medium-term implied volatility (measured by\nthe VIX Index) is an unbiased though inefficient estimation of realized\nvolatility, while in contrast, the short-term volatility (measured by the\nrecently introduced VXST Index representing the 9-day implied volatility) is\nalso unbiased and yet efficient.\n"
    },
    {
        "paper_id": 1606.03709,
        "authors": "Rene Carmona, Francois Delarue, and Daniel Lacker",
        "title": "Mean field games of timing and models for bank runs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of the paper is to introduce a set of problems which we call mean\nfield games of timing. We motivate the formulation by a dynamic model of bank\nrun in a continuous-time setting. We briefly review the economic and game\ntheoretic contributions at the root of our effort, and we develop a\nmathematical theory for continuous-time stochastic games where the strategic\ndecisions of the players are merely choices of times at which they leave the\ngame, and the interaction between the strategic players is of a mean field\nnature.\n"
    },
    {
        "paper_id": 1606.03899,
        "authors": "Damir Filipovi\\'c and Sander Willems",
        "title": "Exact Smooth Term-Structure Estimation",
        "comments": "Forthcoming in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a non-parametric method to estimate the discount curve from market\nquotes based on the Moore-Penrose pseudoinverse. The discount curve reproduces\nthe market quotes perfectly, has maximal smoothness, and is given in\nclosed-form. The method is easy to implement and requires only basic linear\nalgebra operations. We provide a full theoretical framework as well as several\npractical applications.\n"
    },
    {
        "paper_id": 1606.03901,
        "authors": "K. Kanjamapornkul and R. Pin\\v{c}\\'ak",
        "title": "Kolmogorov Space in Time Series Data",
        "comments": "22 pages, 20 figures",
        "journal-ref": null,
        "doi": "10.1002/mma.3875",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide the proof that the space of time series data is a Kolmogorov space\nwith $T_{0}$-separation axiom using the loop space of time series data. In our\napproach we define a cyclic coordinate of intrinsic time scale of time series\ndata after empirical mode decomposition. A spinor field of time series data\ncomes from the rotation of data around price and time axis by defining a new\nextradimension to time series data. We show that there exist hidden eight\ndimensions in Kolmogorov space for time series data. Our concept is realized as\nthe algorithm of empirical mode decomposition and intrinsic time scale\ndecomposition and it is subsequently used for preliminary analysis on the real\ntime series data.\n"
    },
    {
        "paper_id": 1606.04039,
        "authors": "Miles B. Gietzmann and Adam J. Ostaszewski",
        "title": "The Sound of Silence: equilibrium filtering and optimal censoring in\n  financial markets",
        "comments": "Shortened version to appear in: Adv. Appl. Prob. Spec. Vol. 48A\n  (2016)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the approach of standard filtering theory, we analyse\ninvestor-valuation of firms, when these are modelled as geometric-Brownian\nstate processes that are privately and partially observed, at random (Poisson)\ntimes, by agents. Tasked with disclosing forecast values, agents are able\npurposefully to withhold their observations; explicit filtering formulas are\nderived for downgrading the valuations in the absence of disclosures. The\nanalysis is conducted for both a solitary firm and m co-dependent firms.\n"
    },
    {
        "paper_id": 1606.04139,
        "authors": "Javier E. Contreras-Reyes",
        "title": "Credit allocation based on journal impact factor and coauthorship\n  contribution",
        "comments": "9 pages; 3 figures; 2 tables",
        "journal-ref": "Journal of Social and Administrative Sciences (2016), 3(2),\n  111-118",
        "doi": "10.1453/jsas.v3i2.809",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some research institutions demand researchers to distribute the incomes they\nearn from publishing papers to their researchers and/or co-authors. In this\nstudy, we deal with the Impact Factor-based ranking journal as a criteria for\nthe correct distribution of these incomes. We also include the Authorship\nCredit factor for distribution of the incomes among authors, using the\ngeometric progression of Cantor's theory and the Harmonic Credit Index.\nDepending on the ranking of the journal, the proposed model develops a proper\npublication credit allocation among all authors. Moreover, our tool can be\ndeployed in the evaluation of an institution for a funding program, as well as\ncalculating the amounts necessary to incentivize research among personnel.\n"
    },
    {
        "paper_id": 1606.04285,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "Solving Backward Stochastic Differential Equations with quadratic-growth\n  drivers by Connecting the Short-term Expansions",
        "comments": "Forthcoming in Stochastic Processes and their Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article proposes a new approximation scheme for quadratic-growth BSDEs\nin a Markovian setting by connecting a series of semi-analytic asymptotic\nexpansions applied to short-time intervals. Although there remains a condition\nwhich needs to be checked a posteriori, one can avoid altogether time-consuming\nMonte Carlo simulation and other numerical integrations for estimating\nconditional expectations at each space-time node. Numerical examples of\nquadratic-growth as well as Lipschitz BSDEs suggest that the scheme works well\neven for large quadratic coefficients, and a fortiori for large Lipschitz\nconstants.\n"
    },
    {
        "paper_id": 1606.0479,
        "authors": "M. Andrecut",
        "title": "Local Operators in Kinetic Wealth Distribution",
        "comments": "IJMPC, 11 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1142/S0129183116501321",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The statistical mechanics approach to wealth distribution is based on the\nconservative kinetic multi-agent model for money exchange, where the local\ninteraction rule between the agents is analogous to the elastic particle\nscattering process. Here, we discuss the role of a class of conservative local\noperators, and we show that, depending on the values of their parameters, they\ncan be used to generate all the relevant distributions. We also show\nnumerically that in order to generate the power-law tail an heterogeneous risk\naversion model is required. By changing the parameters of these operators one\ncan also fine tune the resulting distributions in order to provide support for\nthe emergence of a more egalitarian wealth distribution.\n"
    },
    {
        "paper_id": 1606.04796,
        "authors": "Giuseppe Toscani",
        "title": "Kinetic and mean field description of Gibrat's law",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.06.063",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and analyze a linear kinetic model that describes the evolution\nof the probability density of the number of firms in a society, in which the\nmicroscopic rate of change obeys to the so-called law of proportional effect\nproposed by Gibrat. Despite its apparent simplicity, the possible mean field\nlimits of the kinetic model are varied. In some cases, the asymptotic limit can\nbe described by a first-order partial differential equation. In other cases,\nthe mean field equation is a linear diffusion with a non constant diffusion\ncoefficient that models also the geometric Brownian motion and can be studied\nanalytically. In this case, it is shown that the large-time behavior of the\nsolution is represented, for a large class of initial data, by a lognormal\ndistribution with constant mean value and variance increasing exponentially in\ntime at a precise rate. The relationship between the kinetic and the diffusion\nmodels allow to introduce an easy-to- implement expression for computing the\nFourier transform of the lognormal distribution.\n"
    },
    {
        "paper_id": 1606.04816,
        "authors": "Nikolay L. Poliakov",
        "title": "Note on level r consensus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the hierarchy of level $r$ consensus partially collapses. In\nparticular, any profile $\\pi\\in \\mathcal{P}$ that exhibits consensus of level\n$(K-1)!$ around $\\succ_0$ in fact exhibits consensus of level $1$ around\n$\\succ_0$.\n"
    },
    {
        "paper_id": 1606.04872,
        "authors": "Nicol\\'o Musmeci, Vincenzo Nicosia, Tomaso Aste, Tiziana Di Matteo,\n  Vito Latora",
        "title": "The multiplex dependency structure of financial markets",
        "comments": "12 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose here a multiplex network approach to investigate simultaneously\ndifferent types of dependency in complex data sets. In particular, we consider\nmultiplex networks made of four layers corresponding respectively to linear,\nnon-linear, tail, and partial correlations among a set of financial time\nseries. We construct the sparse graph on each layer using a standard network\nfiltering procedure, and we then analyse the structural properties of the\nobtained multiplex networks. The study of the time evolution of the multiplex\nconstructed from financial data uncovers important changes in intrinsically\nmultiplex properties of the network, and such changes are associated with\nperiods of financial stress. We observe that some features are unique to the\nmultiplex structure and would not be visible otherwise by the separate analysis\nof the single-layer networks corresponding to each dependency measure.\n"
    },
    {
        "paper_id": 1606.05079,
        "authors": "Katia Colaneri, Zehra Eksi, R\\\"udiger Frey, Michaela Sz\\\"olgyenyi",
        "title": "Optimal Liquidation under Partial Information with Price Impact",
        "comments": null,
        "journal-ref": "Stochastic Processes and their Applications 2019",
        "doi": "10.1016/j.spa.2019.06.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal liquidation problem in a market model where the bid\nprice follows a geometric pure jump process whose local characteristics are\ndriven by an unobservable finite-state Markov chain and by the liquidation\nrate. This model is consistent with stylized facts of high frequency data such\nas the discrete nature of tick data and the clustering in the order flow. We\ninclude both temporary and permanent effects into our analysis. We use\nstochastic filtering to reduce the optimal liquidation problem to an equivalent\noptimization problem under complete information. This leads to a stochastic\ncontrol problem for piecewise deterministic Markov processes (PDMPs). We carry\nout a detailed mathematical analysis of this problem. In particular, we derive\nthe optimality equation for the value function, we characterize the value\nfunction as continuous viscosity solution of the associated dynamic programming\nequation, and we prove a novel comparison result. The paper concludes with\nnumerical results illustrating the impact of partial information and price\nimpact on the value function and on the optimal liquidation rate.\n"
    },
    {
        "paper_id": 1606.05164,
        "authors": "Paolo Barucca, Marco Bardoscia, Fabio Caccioli, Marco D'Errico,\n  Gabriele Visentin, Guido Caldarelli, Stefano Battiston",
        "title": "Network Valuation in Financial Systems",
        "comments": "23 pages, 2 figures",
        "journal-ref": "Mathematical Finance (2020)",
        "doi": "10.1111/mafi.12272",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a general model for the balance-sheet consistent valuation of\ninterbank claims within an interconnected financial system. Our model\nrepresents an extension of clearing models of interdependent liabilities to\naccount for the presence of uncertainty on banks' external assets. At the same\ntime, it also provides a natural extension of classic structural credit risk\nmodels to the case of an interconnected system. We characterize the existence\nand uniqueness of a valuation that maximises individual and total equity values\nfor all banks. We apply our model to the assessment of systemic risk, and in\nparticular for the case of stress-testing. Further, we provide a fixed-point\nalgorithm to carry out the network valuation and the conditions for its\nconvergence.\n"
    },
    {
        "paper_id": 1606.05488,
        "authors": "Shaolin Ji and Xiaomin Shi",
        "title": "Explicit solutions for continuous time mean-variance portfolio selection\n  with nonlinear wealth equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns the continuous time mean-variance portfolio selection\nproblem with a special nonlinear wealth equation. This nonlinear wealth\nequation has a nonsmooth coefficient and the dual method developed in [6] does\nnot work. We invoke the HJB equation of this problem and give an explicit\nviscosity solution of the HJB equation. Furthermore, via this explicit\nviscosity solution, we obtain explicitly the efficient portfolio strategy and\nefficient frontier for this problem. Finally, we show that our nonlinear wealth\nequation can cover three important cases.\n"
    },
    {
        "paper_id": 1606.05877,
        "authors": "Robert Fernholz",
        "title": "A new decomposition of portfolio return",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a functionally generated portfolio, there is a natural decomposition of\nthe relative log-return into the log-change in the generating function and a\ndrift process. In this note, this decomposition is extended to arbitrary stock\nportfolios by an application of Fisk-Stratonovich integration. With the\nextended methodology, the generating function is represented by a structural\nprocess, and the drift process is subsumed into a trading process that measures\nthe profit and loss to the portfolio from trading.\n"
    },
    {
        "paper_id": 1606.06003,
        "authors": "Marek Bundzel, Tomas Kasanicky, Richard Pincak",
        "title": "Using String Invariants for Prediction Searching for Optimal Parameters",
        "comments": "11 pages, 4 figures",
        "journal-ref": "Physica A, 444 (2016) 680-688",
        "doi": "10.1016/j.physa.2015.10.050",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have developed a novel prediction method based on string invariants. The\nmethod does not require learning but a small set of parameters must be set to\nachieve optimal performance. We have implemented an evolutionary algorithm for\nthe parametric optimization. We have tested the performance of the method on\nartificial and real world data and compared the performance to statistical\nmethods and to a number of artificial intelligence methods. We have used data\nand the results of a prediction competition as a benchmark. The results show\nthat the method performs well in single step prediction but the methods\nperformance for multiple step prediction needs to be improved. The method works\nwell for a wide range of parameters.\n"
    },
    {
        "paper_id": 1606.06051,
        "authors": "Kiran Sharma and Anirban Chakraborti",
        "title": "Physicists' approach to studying socio-economic inequalities: Can humans\n  be modelled as atoms?",
        "comments": "19 pages, 6 figures. Springer-Verlag format. Lecture note based on\n  the seminars delivered at International Christian University-Mitaka, Japan,\n  Indian Institute of Advanced Study (IIAS)-Shimla, India and Asian Development\n  Research Institute-Patna, India",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A brief overview of the models and data analyses of income, wealth,\nconsumption distributions by the physicists, are presented here. It has been\nfound empirically that the distributions of income and wealth possess fairly\nrobust features, like the bulk of both the income and wealth distributions seem\nto reasonably fit both the log-normal and Gamma distributions, while the tail\nof the distribution fits well to a power law (as first observed by sociologist\nPareto). We also present our recent studies of the unit-level expenditure on\nconsumption across multiple countries and multiple years, where it was found\nthat there exist invariant features of consumption distribution: the bulk is\nlog-normally distributed, followed by a power law tail at the limit. The\nmechanisms leading to such inequalities and invariant features for the\ndistributions of socio-economic variables are not well-understood. We also\npresent some simple models from physics and demonstrate how they can be used to\nexplain some of these findings and their consequences.\n"
    },
    {
        "paper_id": 1606.06111,
        "authors": "Abhijit Chakraborty, Soumya Easwaran and Sitabhra Sinha",
        "title": "Deviations from universality in the fluctuation behavior of a\n  heterogeneous complex system reveal intrinsic properties of components: The\n  case of the international currency market",
        "comments": "10 pages, 6 figures, final revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying behavior that is relatively invariant under different conditions\nis a challenging task in far-from-equilibrium complex systems. As an example of\nhow the existence of a semi-invariant signature can be masked by the\nheterogeneity in the properties of the components comprising such systems, we\nconsider the exchange rate dynamics in the international currency market. We\nshow that the exponents characterizing the heavy tails of fluctuation\ndistributions for different currencies systematically diverge from a putative\nuniversal form associated with the median value (~2) of the exponents. We\nrelate the degree of deviation of a particular currency from such an \"inverse\nsquare law\" to fundamental macroscopic properties of the corresponding economy,\nviz., measures of per capita production output and diversity of export\nproducts. We also show that in contrast to uncorrelated random walks exhibited\nby the exchange rate dynamics for currencies belonging to developed economies,\nthose of the less developed economies show characteristics of sub-diffusive\nprocesses which we relate to the anti-correlated nature of the corresponding\nfluctuations. Approaches similar to that presented here may help in identifying\ninvariant features obscured by the heterogeneous nature of components in other\ncomplex systems.\n"
    },
    {
        "paper_id": 1606.06143,
        "authors": "Gilles Pag\\`es (UPMC), Olivier Pironneau (LJLL), Guillaume Sall (LJLL)",
        "title": "Vibrato and automatic differentiation for high order derivatives and\n  sensitivities of financial options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the computation of second or higher order greeks of\nfinancial securities. It combines two methods, Vibrato and automatic\ndifferentiation and compares with other methods. We show that this combined\ntechnique is faster than standard finite difference, more stable than automatic\ndifferentiation of second order derivatives and more general than Malliavin\nCalculus. We present a generic framework to compute any greeks and present\nseveral applications on different types of financial contracts: European and\nAmerican options, multidimensional Basket Call and stochastic volatility models\nsuch as Heston's model. We give also an algorithm to compute derivatives for\nthe Longstaff-Schwartz Monte Carlo method for American options. We also extend\nautomatic differentiation for second order derivatives of options with\nnon-twice differentiable payoff. 1. Introduction. Due to BASEL III regulations,\nbanks are requested to evaluate the sensitivities of their portfolios every day\n(risk assessment). Some of these portfolios are huge and sensitivities are time\nconsuming to compute accurately. Faced with the problem of building a software\nfor this task and distrusting automatic differentiation for non-differentiable\nfunctions, we turned to an idea developed by Mike Giles called Vibrato. Vibrato\nat core is a differentiation of a combination of likelihood ratio method and\npathwise evaluation. In Giles [12], [13], it is shown that the computing time,\nstability and precision are enhanced compared with numerical differentiation of\nthe full Monte Carlo path. In many cases, double sensitivities, i.e. second\nderivatives with respect to parameters, are needed (e.g. gamma hedging). Finite\ndifference approximation of sensitivities is a very simple method but its\nprecision is hard to control because it relies on the appropriate choice of the\nincrement. Automatic differentiation of computer programs bypass the difficulty\nand its computing cost is similar to finite difference, if not cheaper. But in\nfinance the payoff is never twice differentiable and so generalized derivatives\nhave to be used requiring approximations of Dirac functions of which the\nprecision is also doubtful. The purpose of this paper is to investigate the\nfeasibility of Vibrato for second and higher derivatives. We will first compare\nVibrato applied twice with the analytic differentiation of Vibrato and show\nthat it is equivalent, as the second is easier we propose the best compromise\nfor second derivatives: Automatic Differentiation of Vibrato. In [8], Capriotti\nhas recently investigated the coupling of different mathematical methods --\nnamely pathwise and likelihood ratio methods -- with an Automatic differ\n"
    },
    {
        "paper_id": 1606.06578,
        "authors": "Byung-Geun Choi, Napat Rujeerapaiboon, Ruiwei Jiang",
        "title": "Multi-Period Portfolio Optimization: Translation of Autocorrelation Risk\n  to Excess Variance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Growth-optimal portfolios are guaranteed to accumulate higher wealth than any\nother investment strategy in the long run. However, they tend to be risky in\nthe short term. For serially uncorrelated markets, similar portfolios with more\nrobust guarantees have been recently proposed. This paper extends these robust\nportfolios by accommodating non-zero autocorrelations that may reflect\ninvestors' beliefs about market movements. Moreover, we prove that the risk\nincurred by such autocorrelations can be absorbed by modifying the covariance\nmatrix of asset returns.\n"
    },
    {
        "paper_id": 1606.0672,
        "authors": "Y. Charles Li, Hong Yang",
        "title": "A mathematical model of demand-supply dynamics with collectability and\n  saturation factors",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S021812741750016X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a mathematical model on the dynamics of demand and supply\nincorporating collectability and saturation factors. Our analysis shows that\nwhen the fluctuation of the determinants of demand and supply is strong enough,\nthere is chaos in the demand-supply dynamics. Our numerical simulation shows\nthat such a chaos is not an attractor (i.e. dynamics is not approaching the\nchaos), instead a periodic attractor (of period 3 under the Poincar\\'e period\nmap) exists near the chaos, and co-exists with another periodic attractor (of\nperiod 1 under the Poincar\\'e period map) near the market equilibrium. Outside\nthe basins of attraction of the two periodic attractors, the dynamics\napproaches infinity indicating market irrational exuberance or flash crash. The\nperiod 3 attractor represents the product's market cycle of growth and\nrecession, while period 1 attractor near the market equilibrium represents the\nregular fluctuation of the product's market. Thus our model captures more\nmarket phenomena besides Marshall's market equilibrium. When the fluctuation of\nthe determinants of demand and supply is strong enough, a three leaf danger\nzone exists where the basins of attraction of all attractors intertwine and\nfractal basin boundaries are formed. Small perturbations in the danger zone can\nlead to very different attractors. That is, small perturbations in the danger\nzone can cause the market to experience oscillation near market equilibrium,\nlarge growth and recession cycle, and irrational exuberance or flash crash.\n"
    },
    {
        "paper_id": 1606.06829,
        "authors": "Marco Bianchetti, Davide Galli, Camilla Ricci, Angelo Salvatori, Marco\n  Scaringi",
        "title": "Brexit or Bremain ? Evidence from bubble analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We applied the Johansen-Ledoit-Sornette (JLS) model to detect possible\nbubbles and crashes related to the Brexit/Bremain referendum scheduled for 23rd\nJune 2016. Our implementation includes an enhanced model calibration using\nGenetic Algorithms. We selected a few historical financial series sensitive to\nthe Brexit/Bremain scenario, representative of multiple asset classes. We found\nthat equity and currency asset classes show no bubble signals, while rates,\ncredit and real estate show super-exponential behaviour and instabilities\ntypical of bubble regime. Our study suggests that, under the JLS model, equity\nand currency markets do not expect crashes or sharp rises following the\nreferendum results. Instead, rates and credit markets consider the referendum a\nrisky event, expecting either a Bremain scenario or a Brexit scenario\nedulcorated by central banks intervention. In the case of real estate, a crash\nis expected, but its relationship with the referendum results is unclear.\n"
    },
    {
        "paper_id": 1606.06948,
        "authors": "Boliang Lin and Ruixi Lin",
        "title": "A New Currency of the Future: The Novel Commodity Money with Attenuation\n  Coefficient Based on the Logistics Cost of Anchor",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we reveal the attenuation mechanism of anchor of the commodity\nmoney from the perspective of logistics warehousing costs, and propose a novel\nDecayed Commodity Money (DCM) for the store of value across time and space.\nConsidering the logistics cost of commodity warehousing by the third financial\ninstitution such as London Metal Exchange, we can award the difference between\nthe original and the residual value of the anchor to the financial institution.\nThis type of currency has the characteristic of self-decaying value over time.\nTherefore DCM has the advantages of both the commodity money which has the\nfunction of preserving wealth and credit currency without the logistics cost.\nIn addition, DCM can also avoid the defects that precious metal money is\nhoarded by market and credit currency often leads to excessive liquidity. DCM\nis also different from virtual currency, such as bitcoin, which does not have a\ncorresponding commodity anchor. As a conclusion, DCM can provide a new way of\nstoring wealth for nations, corporations and individuals effectively.\n"
    },
    {
        "paper_id": 1606.07277,
        "authors": "Takashi Shinzato",
        "title": "Validation of the Replica Trick for Simple Models",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We discuss replica analytic continuation using several simple models in order\nto prove mathematically the validity of replica analysis, which is used in a\nwide range of fields related to large scale complex systems. While replica\nanalysis consists of two analytical techniques, the replica trick (or replica\nanalytic continuation) and the thermodynamical limit (and/or order parameter\nexpansion), we focus our study on replica analytic continuation, which is the\nmathematical basis of the replica trick. We apply replica analysis to solve a\nvariety of analytical models, and examine the properties of replica analytic\ncontinuation. Based on the positive results for these models we propose that\nreplica analytic continuation is a robust procedure in replica analysis.\n"
    },
    {
        "paper_id": 1606.07311,
        "authors": "Huy N. Chau and Mikl\\'os R\\'asonyi",
        "title": "Skorohod's representation theorem and optimal strategies for markets\n  with frictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the existence of optimal strategies for agents with cumulative\nprospect theory preferences who trade in a continuous-time illiquid market,\ntranscending known results which pertained only to risk-averse utility\nmaximizers. The arguments exploit an extension of Skorohod's representation\ntheorem for tight sequences of probability measures. This method is applicable\nin a number of similar optimization problems.\n"
    },
    {
        "paper_id": 1606.07381,
        "authors": "Jack Sarkissian",
        "title": "Spread, volatility, and volume relationship in financial markets and\n  market making profit optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the relationship between price spread, volatility and trading\nvolume. We find that spread forms as a result of interplay between order\nliquidity and order impact. When trading volume is small adding more liquidity\nhelps improve price accuracy and reduce spread, but after some point additional\nliquidity begins to deteriorate price. The model allows to connect the bid-ask\nspread and high-low bars to measurable microstructural parameters and express\ntheir dependence on trading volume, volatility and time horizon. Using the\nestablished relations, we address the operating spread optimization problem to\nmaximize market-making profit.\n"
    },
    {
        "paper_id": 1606.07684,
        "authors": "Tiziano Squartini, Assaf Almog, Guido Caldarelli, Iman van Lelyveld,\n  Diego Garlaschelli, Giulio Cimini",
        "title": "Enhanced capital-asset pricing model for the reconstruction of bipartite\n  financial networks",
        "comments": null,
        "journal-ref": "Phys. Rev. E 96, 032315 (2017)",
        "doi": "10.1103/PhysRevE.96.032315",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reconstructing patterns of interconnections from partial information is one\nof the most important issues in the statistical physics of complex networks. A\nparamount example is provided by financial networks. In fact, the spreading and\namplification of financial distress in capital markets is strongly affected by\nthe interconnections among financial institutions. Yet, while the aggregate\nbalance sheets of institutions are publicly disclosed, information on single\npositions is mostly confidential and, as such, unavailable. Standard approaches\nto reconstruct the network of financial interconnection produce unrealistically\ndense topologies, leading to a biased estimation of systemic risk. Moreover,\nreconstruction techniques are generally designed for monopartite networks of\nbilateral exposures between financial institutions, thus failing in reproducing\nbipartite networks of security holdings (\\eg, investment portfolios). Here we\npropose a reconstruction method based on constrained entropy maximization,\ntailored for bipartite financial networks. Such a procedure enhances the\ntraditional {\\em capital-asset pricing model} (CAPM) and allows to reproduce\nthe correct topology of the network. We test this ECAPM method on a dataset,\ncollected by the European Central Bank, of detailed security holdings of\nEuropean institutional sectors over a period of six years (2009-2015). Our\napproach outperforms the traditional CAPM and the recently proposed MECAPM both\nin reproducing the network topology and in estimating systemic risk due to\nfire-sales spillovers. In general, ECAPM can be applied to the whole class of\nweighted bipartite networks described by the fitness model.\n"
    },
    {
        "paper_id": 1606.07831,
        "authors": "Seyed Amir Hejazi, Kenneth R. Jackson",
        "title": "A Neural Network Approach to Efficient Valuation of Large Portfolios of\n  Variable Annuities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing and hedging the risks associated with Variable Annuity (VA) products\nrequire intraday valuation of key risk metrics for these products. The complex\nstructure of VA products and computational complexity of their accurate\nevaluation have compelled insurance companies to adopt Monte Carlo (MC)\nsimulations to value their large portfolios of VA products. Because the MC\nsimulations are computationally demanding, especially for intraday valuations,\ninsurance companies need more efficient valuation techniques. Recently, a\nframework based on traditional spatial interpolation techniques has been\nproposed that can significantly decrease the computational complexity of MC\nsimulation (Gan and Lin, 2015). However, traditional interpolation techniques\nrequire the definition of a distance function that can significantly impact\ntheir accuracy. Moreover, none of the traditional spatial interpolation\ntechniques provide all of the key properties of accuracy, efficiency, and\ngranularity (Hejazi et al., 2015). In this paper, we present a neural network\napproach for the spatial interpolation framework that affords an efficient way\nto find an effective distance function. The proposed approach is accurate,\nefficient, and provides an accurate granular view of the input portfolio. Our\nnumerical experiments illustrate the superiority of the performance of the\nproposed neural network approach compared to the traditional spatial\ninterpolation schemes.\n"
    },
    {
        "paper_id": 1606.08269,
        "authors": "Christof Henkel",
        "title": "An agent behavior based model for diffusion price processes with\n  application to phase transition and oscillations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an agent behavior based microscopic model for diffusion price\nprocesses. As such we provide a model not only containing a convenient\nframework for describing socio-economic behavior, but also a sophisticated link\nto price dynamics. We furthermore establish the circumstances under which the\ndynamics converge to diffusion processes in the large market limit. To\ndemonstrate the applicability of a separation of behavior and price process, we\nshow how herding behavior of market participants can lead to equilibria\ntransition and oscillations in diffusion price processes.\n"
    },
    {
        "paper_id": 1606.08381,
        "authors": "Sinem Kozp{\\i}nar, Murat Uzunca, B\\\"ulent Karas\\\"ozen",
        "title": "Pricing European and American Options under Heston Model using\n  Discontinuous Galerkin Finite Elements",
        "comments": null,
        "journal-ref": "Mathematics and Computers in Simulation, 177, 568-587 (2020)",
        "doi": "10.1016/j.matcom.2020.05.022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with pricing of European and American options, when the\nunderlying asset price follows Heston model, via the interior penalty\ndiscontinuous Galerkin finite element method (dGFEM). The advantages of dGFEM\nspace discretization with Rannacher smoothing as time integrator with nonsmooth\ninitial and boundary conditions are illustrated for European vanilla options,\ndigital call and American put options. The convection dominated Heston model\nfor vanishing volatility is efficiently solved utilizing the adaptive dGFEM.\nFor fast solution of the linear complementary problem of the American options,\na projected successive over relaxation (PSOR) method is developed with the norm\npreconditioned dGFEM. We show the efficiency and accuracy of dGFEM for option\npricing by conducting comparison analysis with other methods and numerical\nexperiments.\n"
    },
    {
        "paper_id": 1606.08562,
        "authors": "Abdullah Almaatouq",
        "title": "Complex Systems and a Computational Social Science Perspective on the\n  Labor Market",
        "comments": "PhD thesis, MIT",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Labor market institutions are central for modern economies, and their polices\ncan directly affect unemployment rates and economic growth. At the individual\nlevel, unemployment often has a detrimental impact on people's well-being and\nhealth. At the national level, high employment is one of the central goals of\nany economic policy, due to its close association with national prosperity. The\nmain goal of this thesis is to highlight the need for frameworks that take into\naccount the complex structure of labor market interactions. In particular, we\nexplore the benefits of leveraging tools from computational social science,\nnetwork science, and data-driven theories to measure the flow of opportunities\nand information in the context of the labor market. First, we investigate our\nkey hypothesis, which is that opportunity/information flow through weak ties,\nand this is a key determinant of the length of unemployment. We then extend the\nidea of opportunity/information flow to clusters of other economic activities,\nwhere we expect the flow within clusters of related activities to be higher\nthan within isolated activities. This captures the intuition that within\nrelated activities there are more \"capitals\" involved and that such activities\nrequire similar \"capabilities.\" Therefore, more extensive clusters of economic\nactivities should generate greater growth through exploiting the greater flow\nof opportunities and information. We quantify the opportunity/information flow\nusing a complexity measure of two economic activities (i.e. jobs and exports).\n"
    },
    {
        "paper_id": 1606.08679,
        "authors": "Istvan Varga-Haszonits, Fabio Caccioli, Imre Kondor",
        "title": "Replica approach to mean-variance portfolio optimization",
        "comments": "21 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa4f9c",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of mean-variance portfolio optimization for a generic\ncovariance matrix subject to the budget constraint and the constraint for the\nexpected return, with the application of the replica method borrowed from the\nstatistical physics of disordered systems. We find that the replica symmetry of\nthe solution does not need to be assumed, but emerges as the unique solution of\nthe optimization problem. We also check the stability of this solution and find\nthat the eigenvalues of the Hessian are positive for $r=N/T<1$, where $N$ is\nthe dimension of the portfolio and $T$ the length of the time series used to\nestimate the covariance matrix. At the critical point $r=1$ a phase transition\nis taking place. The out of sample estimation error blows up at this point as\n$1/(1-r)$, independently of the covariance matrix or the expected return,\ndisplaying the universality not only of the critical index, but also the\ncritical point. As a conspicuous illustration of the dangers of in-sample\nestimates, the optimal in-sample variance is found to vanish at the critical\npoint inversely proportional to the divergent estimation error.\n"
    },
    {
        "paper_id": 1606.08984,
        "authors": "Johan G. Andreasson, Pavel V. Shevchenko, Alex Novikov",
        "title": "Optimal Consumption, Investment and Housing with Means-tested Public\n  Pension in Retirement",
        "comments": "28 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop an expected utility model for the retirement\nbehavior in the decumulation phase of Australian retirees with sequential\nfamily status subject to consumption, housing, investment, bequest and\ngovernment provided means-tested Age Pension. We account for mortality risk and\nrisky investment assets, and introduce a health proxy to capture the decreasing\nlevel of consumption for older retirees. Then we find optimal housing at\nretirement, and optimal consumption and optimal risky asset allocation\ndepending on age and wealth. The model is solved numerically as a stochastic\ncontrol problem, and is calibrated using the maximum likelihood method on\nempirical data of consumption and housing from the Australian Bureau of\nStatistics 2009-2010 Survey. The model fits the characteristics of the data\nwell to explain the behavior of Australian retirees. The key findings are the\nfollowing: First, the optimal policy is highly sensitive to means-tested Age\nPension early in retirement but this sensitivity fades with age. Secondly, the\nallocation to risky assets shows a complex relationship with the means-tested\nAge Pension that disappears once minimum withdrawal rules are enforced. As a\ngeneral rule, when wealth decreases the proportion allocated to risky assets\nincreases, due to the Age Pension working as a buffer against investment\nlosses. Finally, couples can be more aggressive with risky allocations due to\ntheir longer life expectancy compared with singles.\n"
    },
    {
        "paper_id": 1606.09194,
        "authors": "Alessio Emanuele Biondo, Alessandro Pluchino, Andrea Rapisarda",
        "title": "A multilayer approach for price dynamics in financial markets",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new Self-Organized Criticality (SOC) model for simulating\nprice evolution in an artificial financial market, based on a multilayer\nnetwork of traders. The model also implements, in a quite realistic way with\nrespect to previous studies, the order book dy- namics, by considering two\nassets with variable fundamental prices. Fat tails in the probability\ndistributions of normalized returns are observed, together with other features\nof real financial markets.\n"
    },
    {
        "paper_id": 1607.00035,
        "authors": "Albina Danilova",
        "title": "Stock Market Insider Trading in Continuous Time with Imperfect Dynamic\n  Information",
        "comments": null,
        "journal-ref": "Stochastics: an international journal of probability and\n  stochastic processes, 82 (1). pp. 111-131, 2010",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the equilibrium pricing of asset shares in the presence of\ndynamic private information. The market consists of a risk-neutral informed\nagent who observes the firm value, noise traders, and competitive market makers\nwho set share prices using the total order flow as a noisy signal of the\ninsider's information. I provide a characterization of all optimal strategies,\nand prove existence of both Markovian and non Markovian equilibria by deriving\nclosed form solutions for the optimal order process of the informed trader and\nthe optimal pricing rule of the market maker. The consideration of non\nMarkovian equilibrium is relevant since the market maker might decide to\nre-weight past information after receiving a new signal. Also, I show that a)\nthere is a unique Markovian equilibrium price process which allows the insider\nto trade undetected, and that b) the presence of an insider increases the\nmarket informational efficiency, in particular for times close to dividend\npayment.\n"
    },
    {
        "paper_id": 1607.00077,
        "authors": "Benjamin Jourdain and Alexandre Zhou",
        "title": "Existence of a calibrated regime switching local volatility model and\n  new fake Brownian motions",
        "comments": "52 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By Gyongy's theorem, a local and stochastic volatility (LSV) model is\ncalibrated to the market prices of all European call options with positive\nmaturities and strikes if its local volatility function is equal to the ratio\nof the Dupire local volatility function over the root conditional mean square\nof the stochastic volatility factor given the spot value. This leads to a SDE\nnonlinear in the sense of McKean. Particle methods based on a kernel\napproximation of the conditional expectation, as presented by Guyon and\nHenry-Labord\\`ere (2011), provide an efficient calibration procedure even if\nsome calibration errors may appear when the range of the stochastic volatility\nfactor is very large. But so far, no global existence result is available for\nthe SDE nonlinear in the sense of McKean. In the particular case where the\nlocal volatility function is equal to the inverse of the root conditional mean\nsquare of the stochastic volatility factor multiplied by the spot value given\nthis value and the interest rate is zero, the solution to the SDE is a fake\nBrownian motion. When the stochastic volatility factor is a constant (over\ntime) random variable taking finitely many values and the range of its square\nis not too large, we prove existence to the associated Fokker-Planck equation.\nThanks to Figalli (2008), we then deduce existence of a new class of fake\nBrownian motions. We then extend these results to the special case of the LSV\nmodel called regime switching local volatility, where the stochastic volatility\nfactor is a jump process taking finitely many values and with jump intensities\ndepending on the spot level. Under the same condition on the range of its\nsquare, we prove existence to the associated Fokker-Planck PDE. Finally, we\ndeduce existence of the calibrated model by extending the results in Figalli\n(2008).\n"
    },
    {
        "paper_id": 1607.00448,
        "authors": "Jinghai Shao, Siming Li, Yong Li",
        "title": "Estimation and prediction of credit risk based on rating transition\n  systems",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk management is an important practice in the banking industry. In this\npaper we develop a new methodology to estimate and predict the probability of\ndefault (PD) based on the rating transition matrices, which relates the rating\ntransition matrices to the macroeconomic variables. Our method can overcome the\nshortcomings of the framework of Belkin et al. (1998), and is especially useful\nin predicting the PD and doing stress testing. Simulation is conducted at the\nend, which shows that our method can provide more accurate estimate than that\nobtained by the method of Belkin et al. (1998).\n"
    },
    {
        "paper_id": 1607.00454,
        "authors": "Saran Ahuja, George Papanicolaou, Weiluo Ren, Tzu-Wei Yang",
        "title": "Limit order trading with a mean reverting reference price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal control models for limit order trading often assume that the\nunderlying asset price is a Brownian motion since they deal with relatively\nshort time scales. The resulting optimal bid and ask limit order prices tend to\ntrack the underlying price as one might expect. This is indeed the case with\nthe model of Avellaneda and Stoikov (2008), which has been studied extensively.\nWe consider here this model under the condition when the underlying price is\nmean reverting. Our main result is that when time is far from the terminal, the\noptimal price for bid and ask limit orders is constant, which means that it\ndoes not track the underlying price. Numerical simulations confirm this\nbehavior. When the underlying price is mean reverting, then for times\nsufficiently far from terminal, it is more advantageous to focus on the mean\nprice and ignore fluctuations around it. Mean reversion suggests that limit\norders will be executed with some regularity, and this is why they are optimal.\nWe also explore intermediate time regimes where limit order prices are\ninfluenced by the inventory of outstanding orders. The duration of this\nintermediate regime depends on the liquidity of the market as measured by\nspecific parameters in the model.\n"
    },
    {
        "paper_id": 1607.00638,
        "authors": "Qinglong Zhou, Gaofeng Zong",
        "title": "Time-Inconsistent Stochastic Linear-quadratic Differential Game",
        "comments": "15 pages. arXiv admin note: text overlap with arXiv:1111.0818 by\n  other authors",
        "journal-ref": "Electronic Research Archive 2022",
        "doi": "10.3934/era.2022131",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general time-inconsistent stochastic linear-quadratic\ndifferential game. The time-inconsistency arises from the presence of quadratic\nterms of the expected state as well as state-dependent term in the objective\nfunctionals. We define an equilibrium strategy, which is different from the\nclassical one, and derived a sufficient conditions for equilibrium strategies\nvia a system of forward-backward stochastic differential equations. When the\nstate is one-dimensional and the coefficients are all deterministic, we find an\nexplicit equilibrium strategy. The uniqueness of such equilibrium strategy is\nalso given.\n"
    },
    {
        "paper_id": 1607.00721,
        "authors": "Shaolin Ji, Xiaomin Shi",
        "title": "Recursive utility optimization with concave coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns the recursive utility maximization problem. We assume\nthat the coefficients of the wealth equation and the recursive utility are\nconcave. Then some interesting and important cases with nonlinear and nonsmooth\ncoefficients satisfy our assumption. After given an equivalent backward\nformulation of our problem, we employ the Fenchel-Legendre transform and derive\nthe corresponding variational formulation. By the convex duality method, the\nprimal \"sup-inf\" problem is translated to a dual minimization problem and the\nsaddle point of our problem is derived. Finally, we obtain the optimal terminal\nwealth. To illustrate our results, three cases for investors with ambiguity\naversion are explicitly worked out under some special assumptions.\n"
    },
    {
        "paper_id": 1607.00756,
        "authors": "Giulio Mignola, Roberto Ugoccioni, Eric Cope",
        "title": "Comments on the BCBS proposal for a New Standardized Approach for\n  Operational Risk",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On March 4th 2016 the Basel Committee on Banking Supervision published a\nconsultative document where a new methodology, called the Standardized\nMeasurement Approach (SMA), is introduced for computing Operational Risk\nregulatory capital for banks. In this note, the behavior of the SMA is studied\nunder a variety of hypothetical and realistic conditions, showing that the\nsimplicity of the new approach is very costly on other aspects: we find that\nthe SMA does not respond appropriately to changes in the risk profile of a\nbank, nor is it capable of differentiating among the range of possible risk\nprofiles across banks; that SMA capital results generally appear to be more\nvariable across banks than the previous AMA option of fitting the loss data;\nthat the SMA can result in banks over- or under-insuring against operational\nrisks relative to previous AMA standards. Finally, we argue that the SMA is not\nonly retrograde in terms of its capability to measure risk, but perhaps more\nimportantly, it fails to create any link between management actions and capital\nrequirement.\n"
    },
    {
        "paper_id": 1607.0083,
        "authors": "Vladimir Vovk and Glenn Shafer",
        "title": "A probability-free and continuous-time explanation of the equity premium\n  and CAPM",
        "comments": "21 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper gives yet another definition of game-theoretic probability in the\ncontext of continuous-time idealized financial markets. Without making any\nprobabilistic assumptions (but assuming positive and continuous price paths),\nwe obtain a simple expression for the equity premium and derive a version of\nthe capital asset pricing model.\n"
    },
    {
        "paper_id": 1607.0111,
        "authors": "Andreas Eichler, Gunther Leobacher, Michaela Sz\\\"olgyenyi",
        "title": "Utility Indifference Pricing of Insurance Catastrophe Derivatives",
        "comments": null,
        "journal-ref": "European Actuarial Journal, 7:515-534, 2017",
        "doi": "10.1007/s13385-017-0154-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for an insurance loss index and the claims process of a\nsingle insurance company holding a fraction of the total number of contracts\nthat captures both ordinary losses and losses due to catastrophes. In this\nmodel we price a catastrophe derivative by the method of utility indifference\npricing. The associated stochastic optimization problem is treated by\ntechniques for piecewise deterministic Markov processes. A numerical study\nillustrates our results.\n"
    },
    {
        "paper_id": 1607.01207,
        "authors": "Nemat Safarov and Colin Atkinson",
        "title": "Natural gas-fired power plants valuation and optimisation under Levy\n  copulas and regime-switching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we analyse a stochastic control problem for the valuation of a\nnatural gas power station while taking into account operating characteristics.\nBoth electricity and gas spot price processes exhibit mean-reverting spikes and\nMarkov regime-switches. The Levy regime-switching model incorporates the\neffects of demand-supply fluctuations in energy markets and abrupt economic\ndisruptions or business cycles. We make use of skewed Levy copulas to model the\ndependence risk of electricity and gas jumps.\n  The corresponding HJB equation is the non-linear PIDE which is solved by an\nexplicit finite difference method. The numerical approach gives us both the\nvalue of the plant and its optimal operating strategy depending on the gas and\nelectricity prices, current temperature of the boiler and time. The surfaces of\ncontrol strategies and contract values are obtained by implementing the\nnumerical method for a particular example.\n"
    },
    {
        "paper_id": 1607.01248,
        "authors": "Joachim Kaldasch",
        "title": "Evolutionary Model of Stock Markets",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 415 (2014)\n  449-462",
        "doi": "10.1016/j.physa.2014.08.037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents an evolutionary economic model for the price evolution of\nstocks. Treating a stock market as a self-organized system governed by a fast\npurchase process and slow variations of demand and supply the model suggests\nthat the short term price distribution has the form a logistic (Laplace)\ndistribution. The long term return can be described by Laplace-Gaussian mixture\ndistributions. The long term mean price evolution is governed by a Walrus\nequation, which can be transformed into a replicator equation. This allows\nquantifying the evolutionary price competition between stocks. The theory\nsuggests that stock prices scaled by the price over all stocks can be used to\ninvestigate long-term trends in a Fisher-Pry plot. The price competition that\nfollows from the model is illustrated by examining the empirical long-term\nprice trends of two stocks.\n"
    },
    {
        "paper_id": 1607.01317,
        "authors": "Mauricio Contreras, Rely Pellicer and Marcelo Villena",
        "title": "Dynamic optimization and its relation to classical and quantum\n  constrained systems",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.02.075",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the structure of a simple dynamic optimization problem consisting of\none state and one control variable, from a physicist's point of view. By using\nan analogy to a physical model, we study this system in the classical and\nquantum frameworks. Classically, the dynamic optimization problem is equivalent\nto a classical mechanics constrained system, so we must use the Dirac method to\nanalyze it in a correct way. We find that there are two second-class\nconstraints in the model: one fix the momenta associated with the control\nvariables, and the other is a reminder of the optimal control law. The dynamic\nevolution of this constrained system is given by the Dirac's bracket of the\ncanonical variables with the Hamiltonian. This dynamic results to be identical\nto the unconstrained one given by the Pontryagin equations, which are the\ncorrect classical equations of motion for our physical optimization problem. In\nthe same Pontryagin scheme, by imposing a closed-loop $\\lambda$-strategy, the\noptimality condition for the action gives a consistency relation, which is\nassociated to the Hamilton-Jacobi-Bellman equation of the dynamic programming\nmethod. A similar result is achieved by quantizing the classical model. By\nsetting the wave function $\\Psi(x,t) = e^{iS(x,t)}$ in the quantum\nSchr\\\"odinger equation, a non-linear partial equation is obtained for the $S$\nfunction. For the right-hand side quantization, this is the\nHamilton-Jacobi-Bellman equation, when $S(x,t)$ is identified with the optimal\nvalue function. Thus, the Hamilton-Jacobi-Bellman equation in Bellman's maximum\nprinciple, can be interpreted as the quantum approach of the optimization\nproblem.\n"
    },
    {
        "paper_id": 1607.01519,
        "authors": "Umberto Cherubini, Fabio Gobbi, Sabrina Mulinacci and Silvia Romagnoli",
        "title": "Granger Independent Martingale Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new class of processes for the evaluation of multivariate\nequity derivatives. The proposed setting is well suited for the application of\nthe standard copula function theory to processes, rather than variables, and\neasily enables to enforce the martingale pricing requirement. The martingale\ncondition is imposed in a general multidimensional Markov setting to which we\nonly add the restriction of no-Granger-causality of the increments\n(Granger-independent increments). We call this class of processes GIMP (Granger\nIndependent Martingale Processes). The approach can also be extended to the\napplication of time change, under which the martingale restriction continues to\nhold. Moreover, we show that the class of GIMP processes is closed under time\nchanging: if a Granger independent process is used as a multivariate stochastic\nclock for the change of time of a GIMP process, the new process is also GIMP.\n"
    },
    {
        "paper_id": 1607.01619,
        "authors": "V.M. Belyaev",
        "title": "Swaption Prices in HJM model. Nonparametric fit",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Closed form formulas for swaption prices in HJM model are derived. These\nformulas are used for nonparametric fit of deterministic forward volatility. It\nis demonstrated that this formula and non-parametric fit works very well and\ncan be used to identify arbitrage opportunities\n"
    },
    {
        "paper_id": 1607.01751,
        "authors": "Sylwester Arabas and Ahmad Farhat",
        "title": "Derivative pricing as a transport problem: MPDATA solutions to\n  Black-Scholes-type equations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.cam.2019.05.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss in this note applications of the Multidimensional Positive\nDefinite Advection Transport Algorithm (MPDATA) to numerical solutions of\npartial differential equations arising from stochastic models in quantitative\nfinance. In particular, we develop a framework for solving Black-Scholes-type\nequations by first transforming them into advection-diffusion problems, and\nnumerically integrating using an iterative explicit finite-difference approach,\nin which the Fickian term is represented as an additional advective term. We\ndiscuss the correspondence between transport phenomena and financial models,\nuncovering the possibility of expressing the no-arbitrage principle as a\nconservation law. We depict second-order accuracy in time and space of the\nembraced numerical scheme. This is done in a convergence analysis comparing\nMPDATA numerical solutions with classic Black-Scholes analytical formulae for\nthe valuation of European options. We demonstrate in addition a way of applying\nMPDATA to solve the free boundary problem (leading to a linear complementarity\nproblem) for the valuation of American options. We finally comment on the\npotential the MPDATA framework has with respect to being applied in tandem with\nmore complex models typically used in quantitive finance.\n"
    },
    {
        "paper_id": 1607.01902,
        "authors": "Benjamin Avanzi, Jos\\'e-Luis P\\'erez, Bernard Wong, Kazutoshi Yamazaki",
        "title": "On optimal joint reflective and refractive dividend strategies in\n  spectrally positive L\\'evy models",
        "comments": "Forthcoming in Insurance: Mathematics and Economics",
        "journal-ref": "Insurance: Mathematics and Economics, Volume 72, January 2017,\n  Pages 148-162",
        "doi": "10.1016/j.insmatheco.2016.10.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The expected present value of dividends is one of the classical stability\ncriteria in actuarial risk theory. In this context, numerous papers considered\nthreshold (refractive) and barrier (reflective) dividend strategies. These were\nshown to be optimal in a number of different contexts for bounded and unbounded\npayout rates, respectively. In this paper, motivated by the behaviour of some\ndividend paying stock exchange companies, we determine the optimal dividend\nstrategy when both continuous (refractive) and lump sum (reflective) dividends\ncan be paid at any time, and if they are subject to different transaction\nrates. We consider the general family of spectrally positive L\\'evy processes.\nUsing scale functions, we obtain explicit formulas for the expected present\nvalue of dividends until ruin, with a penalty at ruin. We develop a\nverification lemma, and show that a two-layer (a,b) strategy is optimal. Such a\nstrategy pays continuous dividends when the surplus exceeds level a>0, and all\nof the excess over b>a as lump sum dividend payments. Results are illustrated.\n"
    },
    {
        "paper_id": 1607.01999,
        "authors": "Somwrita Sarkar and Sanjay Chawla",
        "title": "Inferring the contiguity matrix for spatial autoregressive analysis with\n  applications to house price prediction",
        "comments": "11 Pages, 6 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inference methods in traditional statistics, machine learning and data mining\nassume that data is generated from an independent and identically distributed\n(iid) process. Spatial data exhibits behavior for which the iid assumption must\nbe relaxed. For example, the standard approach in spatial regression is to\nassume the existence of a contiguity matrix which captures the spatial\nautoregressive properties of the data. However all spatial methods, till now,\nhave assumed that the contiguity matrix is given apriori or can be estimated by\nusing a spatial similarity function. In this paper we propose a convex\noptimization formulation to solve the spatial autoregressive regression (SAR)\nmodel in which both the contiguity matrix and the non-spatial regression\nparameters are unknown and inferred from the data. We solve the problem using\nthe alternating direction method of multipliers (ADMM) which provides a\nsolution which is both robust and efficient. While our approach is general we\nuse data from housing markets of Boston and Sydney to both guide the analysis\nand validate our results. A novel side effect of our approach is the automatic\ndiscovery of spatial clusters which translate to submarkets in the housing data\nsets.\n"
    },
    {
        "paper_id": 1607.02067,
        "authors": "Damir Filipovic and Yerkin Kitapbayev",
        "title": "On the American swaption in the linear-rational framework",
        "comments": "forthcoming in Quantitative Finance, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study American swaptions in the linear-rational (LR) term structure model\nintroduced in [5]. The American swaption pricing problem boils down to an\noptimal stopping problem that is analytically tractable. It reduces to a\nfree-boundary problem that we tackle by the local time-space calculus of [7].\nWe characterize the optimal stopping boundary as the unique solution to a\nnonlinear integral equation that can be readily solved numerically. We obtain\nthe arbitrage-free price of the American swaption and the optimal exercise\nstrategies in terms of swap rates for both fixed-rate payer and receiver swaps.\nFinally, we show that Bermudan swaptions can be efficiently priced as well.\n"
    },
    {
        "paper_id": 1607.02093,
        "authors": "Tamal Datta Chaudhuri, Indranil Ghosh",
        "title": "Artificial Neural Network and Time Series Modeling Based Approach to\n  Forecasting the Exchange Rate in a Multivariate Framework",
        "comments": null,
        "journal-ref": "Journal of Insurance and Financial Management, Vol. 1, Issue 5,\n  PP. 92-123, 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Any discussion on exchange rate movements and forecasting should include\nexplanatory variables from both the current account and the capital account of\nthe balance of payments. In this paper, we include such factors to forecast the\nvalue of the Indian rupee vis a vis the US Dollar. Further, factors reflecting\npolitical instability and lack of mechanism for enforcement of contracts that\ncan affect both direct foreign investment and also portfolio investment, have\nbeen incorporated. The explanatory variables chosen are the 3 month Rupee\nDollar futures exchange rate (FX4), NIFTY returns (NIFTYR), Dow Jones\nIndustrial Average returns (DJIAR), Hang Seng returns (HSR), DAX returns (DR),\ncrude oil price (COP), CBOE VIX (CV) and India VIX (IV). To forecast the\nexchange rate, we have used two different classes of frameworks namely,\nArtificial Neural Network (ANN) based models and Time Series Econometric\nmodels. Multilayer Feed Forward Neural Network (MLFFNN) and Nonlinear\nAutoregressive models with Exogenous Input (NARX) Neural Network are the\napproaches that we have used as ANN models. Generalized Autoregressive\nConditional Heteroskedastic (GARCH) and Exponential Generalized Autoregressive\nConditional Heteroskedastic (EGARCH) techniques are the ones that we have used\nas Time Series Econometric methods. Within our framework, our results indicate\nthat, although the two different approaches are quite efficient in forecasting\nthe exchange rate, MLFNN and NARX are the most efficient.\n"
    },
    {
        "paper_id": 1607.02289,
        "authors": "Wing Fung Chong, Ying Hu, Gechun Liang, Thaleia Zariphopoulou",
        "title": "An ergodic BSDE approach to forward entropic risk measures:\n  representation and large-maturity behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using elements from the theory of ergodic backward stochastic differential\nequations (BSDE), we study the behavior of forward entropic risk measures. We\nprovide their general representation results (via both BSDE and convex duality)\nand examine their behavior for risk positions of long maturities. We show that\nforward entropic risk measures converge to some constant exponentially fast. We\nalso compare them with their classical counterparts and derive a parity result.\n"
    },
    {
        "paper_id": 1607.02319,
        "authors": "Gareth W. Peters, Pavel V. Shevchenko, Bertrand Hassani and Ariane\n  Chapelle",
        "title": "Should the advanced measurement approach be replaced with the\n  standardized measurement approach for operational risk?",
        "comments": null,
        "journal-ref": "Journal of Operational Risk, Vol. 11, Issue 3, pp. 1-49, 2016",
        "doi": "10.21314/JOP.2016.177",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, Basel Committee for Banking Supervision proposed to replace all\napproaches, including Advanced Measurement Approach (AMA), for operational risk\ncapital with a simple formula referred to as the Standardised Measurement\nApproach (SMA). This paper discusses and studies the weaknesses and pitfalls of\nSMA such as instability, risk insensitivity, super-additivity and the implicit\nrelationship between SMA capital model and systemic risk in the banking sector.\nWe also discuss the issues with closely related operational risk\nCapital-at-Risk (OpCar) Basel Committee proposed model which is the precursor\nto the SMA. In conclusion, we advocate to maintain the AMA internal model\nframework and suggest as an alternative a number of standardization\nrecommendations that could be considered to unify internal modelling of\noperational risk. The findings and views presented in this paper have been\ndiscussed with and supported by many OpRisk practitioners and academics in\nAustralia, Europe, UK and USA, and recently at OpRisk Europe 2016 conference in\nLondon.\n"
    },
    {
        "paper_id": 1607.02349,
        "authors": "Marie Doumic (LJLL), Beno\\^it Perthame (LJLL), Edouard Ribes (IRSEM),\n  Delphine Salort (UPMC), Nathan Toubiana (LJLL)",
        "title": "Toward an integrated workforce planning framework using structured\n  equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Strategic Workforce Planning is a company process providing best in class,\neconomically sound, workforce management policies and goals. Despite the\nabundance of literature on the subject, this is a notorious challenge in terms\nof implementation. Reasons span from the youth of the field itself to broader\ndata integration concerns that arise from gathering information from financial,\nhuman resource and business excellence systems. This paper aims at setting the\nfirst stones to a simple yet robust quantitative framework for Strategic\nWorkforce Planning exercises. First a method based on structured equations is\ndetailed. It is then used to answer two main workforce related questions: how\nto optimally hire to keep labor costs flat? How to build an experience\nconstrained workforce at a minimal cost?\n"
    },
    {
        "paper_id": 1607.02378,
        "authors": "Fuad Aleskerov, Andrey Subochev",
        "title": "Matrix-vector representation of various solution concepts",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A unified matrix-vector representation is developed of such solution concepts\nas the core, the uncovered, the uncaptured, the minimal weakly stable, the\nminimal undominated, the minimal dominant and the untrapped sets. We also\npropose several new versions of solution sets.\n"
    },
    {
        "paper_id": 1607.0241,
        "authors": "Tung-Lam Dao, Trung-Tu Nguyen, Cyril Deremble, Yves Lemp\\'eri\\`ere,\n  Jean-Philippe Bouchaud, Marc Potters",
        "title": "Tail protection for long investors: Trend convexity at work",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The performance of trend following strategies can be ascribed to the\ndifference between long-term and short-term realized variance. We revisit this\ngeneral result and show that it holds for various definitions of trend\nstrategies. This explains the positive convexity of the aggregate performance\nof Commodity Trading Advisors (CTAs) which -- when adequately measured -- turns\nout to be much stronger than anticipated. We also highlight interesting\nconnections with so-called Risk Parity portfolios. Finally, we propose a new\nportfolio of strangle options that provides a pure exposure to the long-term\nvariance of the underlying, offering yet another viewpoint on the link between\ntrend and volatility.\n"
    },
    {
        "paper_id": 1607.02419,
        "authors": "Alexander Rubchinsky",
        "title": "Divisive-agglomerative algorithm and complexity of automatic\n  classification problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An algorithm of solution of the Automatic Classification (AC for brevity)\nproblem is set forth in the paper. In the AC problem, it is required to find\none or several artitions, starting with the given pattern matrix or\ndissimilarity, similarity matrix.\n"
    },
    {
        "paper_id": 1607.02421,
        "authors": "Andrey Subochev, Igor Zakhlebin",
        "title": "Alternative versions of the global competitive industrial performance\n  ranking constructed by methods from social choice theory",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Competitive Industrial Performance index (developed by experts of the\nUNIDO) is designed as a measure of national competitiveness. Index is an\naggregate of eight observable variables, representing different dimensions of\ncompetitive industrial performance.\n"
    },
    {
        "paper_id": 1607.02422,
        "authors": "Alexander Karminsky",
        "title": "Rating models: emerging market distinctions",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Basel II Accords have sparked increased interest in the development of\napproaches based on internal ratings systems and have initiated the elaboration\nof models for remote ratings forecasts based on external ones as part of Risk\nManagement and Early Warning Systems. This article evaluates the peculiarities\nof current ratings systems and addresses specific issues of development of\neconometrical rating models for emerging market companies.\n"
    },
    {
        "paper_id": 1607.02423,
        "authors": "Alexander Rubchinsky",
        "title": "Fair division with divisible and indivisible items",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the work the fair division problem for two participants in presence of\nboth divisible and indivisible items is considered. The set of all divisions is\nformally described; it is demonstrated that fair (in terms of Brams and Taylor)\ndivisions, unlikely the case where all the items are divisible, not always\nexist. The necessary and sufficient conditions of existence of proportional and\nequitable division were found.\n"
    },
    {
        "paper_id": 1607.0247,
        "authors": "Justin Sirignano, Apaar Sadhwani, and Kay Giesecke",
        "title": "Deep Learning for Mortgage Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a deep learning model of multi-period mortgage risk and use it to\nanalyze an unprecedented dataset of origination and monthly performance records\nfor over 120 million mortgages originated across the US between 1995 and 2014.\nOur estimators of term structures of conditional probabilities of prepayment,\nforeclosure and various states of delinquency incorporate the dynamics of a\nlarge number of loan-specific as well as macroeconomic variables down to the\nzip-code level. The estimators uncover the highly nonlinear nature of the\nrelationship between the variables and borrower behavior, especially\nprepayment. They also highlight the effects of local economic conditions on\nborrower behavior. State unemployment has the greatest explanatory power among\nall variables, offering strong evidence of the tight connection between housing\nfinance markets and the macroeconomy. The sensitivity of a borrower to changes\nin unemployment strongly depends upon current unemployment. It also\nsignificantly varies across the entire borrower population, which highlights\nthe interaction of unemployment and many other variables. These findings have\nimportant implications for mortgage-backed security investors, rating agencies,\nand housing finance policymakers.\n"
    },
    {
        "paper_id": 1607.02481,
        "authors": "Fabio Saracco, Mika J. Straka, Riccardo Di Clemente, Andrea Gabrielli,\n  Guido Caldarelli, Tiziano Squartini",
        "title": "Inferring monopartite projections of bipartite networks: an\n  entropy-based approach",
        "comments": "16 pages, 9 figures",
        "journal-ref": "New J. Phys. 19, 053022 (2017)",
        "doi": "10.1088/1367-2630/aa6b38",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bipartite networks are currently regarded as providing a major insight into\nthe organization of many real-world systems, unveiling the mechanisms driving\nthe interactions occurring between distinct groups of nodes. One of the most\nimportant issues encountered when modeling bipartite networks is devising a way\nto obtain a (monopartite) projection on the layer of interest, which preserves\nas much as possible the information encoded into the original bipartite\nstructure. In the present paper we propose an algorithm to obtain\nstatistically-validated projections of bipartite networks, according to which\nany two nodes sharing a statistically-significant number of neighbors are\nlinked. Since assessing the statistical significance of nodes similarity\nrequires a proper statistical benchmark, here we consider a set of four null\nmodels, defined within the exponential random graph framework. Our algorithm\noutputs a matrix of link-specific p-values, from which a validated projection\nis straightforwardly obtainable, upon running a multiple hypothesis testing\nprocedure. Finally, we test our method on an economic network (i.e. the\ncountries-products World Trade Web representation) and a social network (i.e.\nMovieLens, collecting the users' ratings of a list of movies). In both cases\nnon-trivial communities are detected: while projecting the World Trade Web on\nthe countries layer reveals modules of similarly-industrialized nations,\nprojecting it on the products layer allows communities characterized by an\nincreasing level of complexity to be detected; in the second case, projecting\nMovieLens on the films layer allows clusters of movies whose affinity cannot be\nfully accounted for by genre similarity to be individuated.\n"
    },
    {
        "paper_id": 1607.02688,
        "authors": "Luis A. Alcala",
        "title": "On the time consistency of collective preferences",
        "comments": "33 pages; changes in notation and major corrections",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A dynamic model of collective consumption and saving decisions made by a\nfinite number of agents with constant but different discount rates is\ndeveloped. Collective utility is a weighted sum of individual utilities with\ntime-varying utility weights. Under standard separability assumptions, it is\nshown that collective preferences may be nonstationary but still satisfy time\nconsistency. The assumption of time-varying weights is key to balance the need\nof the group for a changing distribution of consumption among its members over\ntime with their tolerance for consumption fluctuations.\n"
    },
    {
        "paper_id": 1607.02743,
        "authors": "Ying Jiao (SAF), Idris Kharroubi (CREST, CEREMADE)",
        "title": "Information uncertainty related to marked random times and optimal\n  investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal investment problem under default risk where related\ninformation such as loss or recovery at default is considered as an exogenous\nrandom mark added at default time. Two types of agents who have different\nlevels of information are considered. We first make precise the insider's\ninformation flow by using the theory of enlargement of filtrations and then\nobtain explicit logarithmic utility maximization results to compare optimal\nwealth for the insider and the ordinary agent. MSC: 60G20, 91G40, 93E20\n"
    },
    {
        "paper_id": 1607.03161,
        "authors": "Romulus Breban",
        "title": "A mathematical model for a gaming community",
        "comments": "4 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a large community of individuals who mix strongly and meet in\npairs to bet on a coin toss. We investigate the asset distribution of the\nplayers involved in this zero-sum repeated game. Our main result is that the\nasset distribution converges to the exponential distribution, irrespective of\nthe size of the bet, as long as players can never go bankrupt. Analytical\nresults suggests that the exponential distribution is a stable fixed point for\nthis zero-sum repreated game. This is confirmed in numerical experiments.\n"
    },
    {
        "paper_id": 1607.03205,
        "authors": "Taisei Kaizoji and Michiko Miyano",
        "title": "Stock Market Market Crash of 2008: an empirical study of the deviation\n  of share prices from company fundamentals",
        "comments": "11 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1080/13504851.2018.1486004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this study is to investigate quantitatively whether share prices\ndeviated from company fundamentals in the stock market crash of 2008. For this\npurpose, we use a large database containing the balance sheets and share prices\nof 7,796 worldwide companies for the period 2004 through 2013. We develop a\npanel regression model using three financial indicators--dividends per share,\ncash flow per share, and book value per share--as explanatory variables for\nshare price. We then estimate individual company fundamentals for each year by\nremoving the time fixed effects from the two-way fixed effects model, which we\nidentified as the best of the panel regression models. One merit of our model\nis that we are able to extract unobservable factors of company fundamentals by\nusing the individual fixed effects.\n  Based on these results, we analyze the market anomaly quantitatively using\nthe divergence rate--the rate of the deviation of share price from a company's\nfundamentals. We find that share prices on average were overvalued in the\nperiod from 2005 to 2007, and were undervalued significantly in 2008, when the\nglobal financial crisis occurred. Share prices were equivalent to the\nfundamentals on average in the subsequent period. Our empirical results clearly\ndemonstrate that the worldwide stock market fluctuated excessively in the time\nperiod before and just after the global financial crisis of 2008.\n"
    },
    {
        "paper_id": 1607.0343,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Birgit Rudloff",
        "title": "Dual representations for systemic risk measures",
        "comments": "36 pages",
        "journal-ref": "Mathematics and Financial Economics 14 (1), 139-174, (2020)",
        "doi": "10.1007/s11579-019-00249-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial crisis showed the importance of measuring, allocating and\nregulating systemic risk. Recently, the systemic risk measures that can be\ndecomposed into an aggregation function and a scalar measure of risk, received\na lot of attention. In this framework, capital allocations are added after\naggregation and can represent bailout costs. More recently, a framework has\nbeen introduced, where institutions are supplied with capital allocations\nbefore aggregation. This yields an interpretation that is particularly useful\nfor regulatory purposes. In each framework, the set of all feasible capital\nallocations leads to a multivariate risk measure. In this paper, we present\ndual representations for scalar systemic risk measures as well as for the\ncorresponding multivariate risk measures concerning capital allocations. Our\nresults cover both frameworks: aggregating after allocating and allocating\nafter aggregation. As examples, we consider the aggregation mechanisms of the\nEisenberg-Noe model as well as those of the resource allocation and network\nflow models.\n"
    },
    {
        "paper_id": 1607.03522,
        "authors": "Antonis Papapantoleon, Robert Wardenga",
        "title": "Continuous tenor extension of affine LIBOR models with multiple curves\n  and applications to XVA",
        "comments": "25 pages, 6 figures, revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the class of affine LIBOR models with multiple curves, which is\nan analytically tractable class of discrete tenor models that easily\naccommodates positive or negative interest rates and positive spreads. By\nintroducing an interpolating function, we extend the affine LIBOR models to a\ncontinuous tenor and derive expressions for the instantaneous forward rate and\nthe short rate. We show that the continuous tenor model is arbitrage-free, that\nthe analytical tractability is retained under the spot martingale measure, and\nthat under mild conditions an interpolating function can be found such that the\nextended model fits any initial forward curve. This allows us to compute value\nadjustments (i.e. XVAs) consistently, by solving the corresponding\n`pre-default' BSDE. As an application, we compute the price and value\nadjustments for a basis swap, and study the model risk associated to different\ninterpolating functions.\n"
    },
    {
        "paper_id": 1607.04047,
        "authors": "Jana Bielagk, Ulrich Horst and Santiago Moreno--Bromberg",
        "title": "A Principal-Agent Model of Trading Under Market Impact -Crossing\n  networks interacting with dealer markets-",
        "comments": "28 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a principal-agent model to analyze the structure of a book-driven\ndealer market when the dealer faces competition from a crossing network or dark\npool. The agents are privately informed about their types (e.g. their\nportfolios), which is something that the dealer must take into account when\nengaging his counterparties. Instead of trading with the dealer, the agents may\nchose to trade in a crossing network. We show that the presence of such a\nnetwork results in more types being serviced by the dealer and that, under\ncertain conditions and due to reduced adverse selection effects, the book's\nspread shrinks. We allow for the pricing on the dealer market to determine the\nstructure of the crossing network and show that the same conditions that lead\nto a reduction of the spread imply the existence of an equilibrium\nbook/crossing network pair.\n"
    },
    {
        "paper_id": 1607.041,
        "authors": "Hampus Engsner, Mathias Lindholm, Filip Lindskog",
        "title": "Insurance valuation: a computable multi-period cost-of-capital approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an approach to market-consistent multi-period valuation of\ninsurance liability cash flows based on a two-stage valuation procedure. First,\na portfolio of traded financial instrument aimed at replicating the liability\ncash flow is fixed. Then the residual cash flow is managed by repeated\none-period replication using only cash funds. The latter part takes capital\nrequirements and costs into account, as well as limited liability and risk\naverseness of capital providers. The cost-of-capital margin is the value of the\nresidual cash flow. We set up a general framework for the cost-of-capital\nmargin and relate it to dynamic risk measurement. Moreover, we present explicit\nformulas and properties of the cost-of-capital margin under further assumptions\non the model for the liability cash flow and on the conditional risk measures\nand utility functions. Finally, we highlight computational aspects of the\ncost-of-capital margin, and related quantities, in terms of an example from\nlife insurance.\n"
    },
    {
        "paper_id": 1607.04136,
        "authors": "Sandro Lera and Didier Sornette",
        "title": "Secular bipolar growth rate of the real US GDP per capita: implications\n  for understanding past and future economic growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a quantitative characterisation of the fluctuations of the\nannualized growth rate of the real US GDP per capita growth at many scales,\nusing a wavelet transform analysis of two data sets, quarterly data from 1947\nto 2015 and annual data from 1800 to 2010. Our main finding is that the\ndistribution of GDP growth rates can be well approximated by a bimodal function\nassociated to a series of switches between regimes of strong growth rate\n$\\rho_\\text{high}$ and regimes of low growth rate $\\rho_\\text{low}$. The\nsuccession of such two regimes compounds to produce a remarkably stable long\nterm average real annualized growth rate of 1.6\\% from 1800 to 2010 and\n$\\approx 2.0\\%$ since 1950, which is the result of a subtle compensation\nbetween the high and low growth regimes that alternate continuously. Thus, the\noverall growth dynamics of the US economy is punctuated, with phases of strong\ngrowth that are intrinsically unsustainable, followed by corrections or\nconsolidation until the next boom starts. We interpret these findings within\nthe theory of \"social bubbles\" and argue as a consequence that estimations of\nthe cost of the 2008 crisis may be misleading. We also interpret the absence of\nstrong recovery since 2008 as a protracted low growth regime $\\rho_\\text{low}$\nassociated with the exceptional nature of the preceding large growth regime.\n"
    },
    {
        "paper_id": 1607.04153,
        "authors": "Giorgio Ferrari",
        "title": "On the Optimal Management of Public Debt: a Singular Stochastic Control\n  Problem",
        "comments": "39 pages. A previous version of this work was circulating under the\n  title \"Controlling Public Debt without Forgetting Inflation\". In the current\n  version new results have been added, and exposition has been improved",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider the problem of a government that wants to reduce the debt-to-GDP\n(gross domestic product) ratio of a country. The government aims at choosing a\ndebt reduction policy which minimises the total expected cost of having debt,\nplus the total expected cost of interventions on the debt ratio. We model this\nproblem as a singular stochastic control problem over an infinite time-horizon.\nIn a general not necessarily Markovian framework, we first show by\nprobabilistic arguments that the optimal debt reduction policy can be expressed\nin terms of the optimal stopping rule of an auxiliary optimal stopping problem.\nWe then exploit such link to characterise the optimal control in a\ntwo-dimensional Markovian setting in which the state variables are the level of\nthe debt-to-GDP ratio and the current inflation rate of the country. The latter\nfollows uncontrolled Ornstein-Uhlenbeck dynamics and affects the growth rate of\nthe debt ratio. We show that it is optimal for the government to adopt a policy\nthat keeps the debt-to-GDP ratio under an inflation-dependent ceiling. This\ncurve is given in terms of the solution of a nonlinear integral equation\narising in the study of a fully two-dimensional optimal stopping problem.\n"
    },
    {
        "paper_id": 1607.04155,
        "authors": "Jean-Francois Mercure",
        "title": "Fashion, fads and the popularity of choices: micro-foundations for\n  diffusion consumer theory",
        "comments": "20 pages including appendix",
        "journal-ref": "Structural Change and Economic Dynamics, 2018",
        "doi": "10.1016/j.strueco.2018.06.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Knowledge acquisition by consumers is a key process in the diffusion of\ninnovations. However, in standard theories of the representative agent, agents\ndo not learn and innovations are adopted instantaneously. Here, we show that in\na discrete choice model where utility-maximising agents with heterogenous\npreferences learn about products through peers, their stock of knowledge on\nproducts becomes heterogenous, fads and fashions arise, and transitivity in\naggregate preferences is lost. Non-equilibrium path-dependent dynamics emerge,\nthe representative agent exhibits behavioural rules different than individual\nagents, and aggregate utility cannot be optimised. Instead, an evolutionary\ntheory of product innovation and diffusion emerges.\n"
    },
    {
        "paper_id": 1607.04214,
        "authors": "Antonis Papapantoleon and Dylan Possama\\\"i and Alexandros Saplaouras",
        "title": "Existence and uniqueness results for BSDEs with jumps: the whole nine\n  yards",
        "comments": "48 pages, final version, forthcoming in the Electronic Journal of\n  Probability",
        "journal-ref": null,
        "doi": "10.1214/18-EJP240",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to obtaining a wellposedness result for\nmultidimensional BSDEs with possibly unbounded random time horizon and driven\nby a general martingale in a filtration only assumed to satisfy the usual\nhypotheses, i.e. the filtration may be stochastically discontinuous. We show\nthat for stochastic Lipschitz generators and unbounded, possibly infinite, time\nhorizon, these equations admit a unique solution in appropriately weighted\nspaces. Our result allows in particular to obtain a wellposedness result for\nBSDEs driven by discrete--time approximations of general martingales.\n"
    },
    {
        "paper_id": 1607.04484,
        "authors": "Bent Flyvbjerg, Allison Stewart, Alexander Budzier",
        "title": "The Oxford Olympics Study 2016: Cost and Cost Overrun at the Games",
        "comments": "28 pp",
        "journal-ref": "Said Business School Working Papers (Oxford: University of\n  Oxford), july 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given that Olympic Games held over the past decade each have cost USD 8.9\nbillion on average, the size and financial risks of the Games warrant study.\nThe objectives of the Oxford Olympics study are to (1) establish the actual\noutturn costs of previous Olympic Games in a manner where cost can consistently\nbe compared across Games; (2) establish cost overruns for previous Games, i.e.,\nthe degree to which final outturn costs reflect projected budgets at the bid\nstage, again in a way that allows comparison across Games; (3) test whether the\nOlympic Games Knowledge Management Program has reduced cost risk for the Games,\nand, finally, (4) benchmark cost and cost overrun for the Rio 2016 Olympics\nagainst previous Games. The main contribution of the Oxford study is to\nestablish a phenomenology of cost and cost overrun at the Olympics, which\nallows consistent and systematic comparison across Games. This has not been\ndone before. The study concludes that for a city and nation to decide to stage\nthe Olympic Games is to decide to take on one of the most costly and\nfinancially most risky type of megaproject that exists, something that many\ncities and nations have learned to their peril.\n"
    },
    {
        "paper_id": 1607.04488,
        "authors": "Dirk Becherer and Klebert Kentia",
        "title": "Hedging under generalized good-deal bounds and model uncertainty",
        "comments": "30 pages, 2 figures, 1 table. Revised Version",
        "journal-ref": "Math Meth Oper Res (2017), 86/1, 171-214",
        "doi": "10.1007/s00186-017-0588-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a notion of good-deal hedging, that corresponds to good-deal\nvaluation for generalized good-deal constraints. Under model uncertainty about\nthe market prices of risk of hedging assets, a robust approach leads to a\nreduction or even elimination of a speculative component in good-deal hedging,\nwhich is shown to be equivalent to a global risk-minimization in the sense of\nF\\\"ollmer and Sondermann (1986) if uncertainty is sufficiently large.\nConstructive results on hedges and valuations are derived from backward\nstochastic differential equations, including new examples with explicit\nformulas.\n"
    },
    {
        "paper_id": 1607.04553,
        "authors": "Qing-Qing Yang, Wai-Ki Ching, Jia-Wen Gu, Tak-Kuen Siu",
        "title": "Generalized Optimal Liquidation Problems Across Multiple Trading Venues",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we generalize the Almgren-Chriss's market impact model to a\nmore realistic and flexible framework and employ it to derive and analyze some\naspects of optimal liquidation problem in a security market. We illustrate how\na trader's liquidation strategy alters when multiple venues and extra\ninformation are brought into the security market and detected by the trader.\nThis study gives some new insights into the relationship between liquidation\nstrategy and market liquidity, and provides a multi-scale approach to the\noptimal liquidation problem with randomly varying volatility.\n"
    },
    {
        "paper_id": 1607.04737,
        "authors": "Jianxi Su, Edward Furman",
        "title": "A form of multivariate Pareto distribution with applications to\n  financial risk measurement",
        "comments": "ASTIN Bulletin: The Journal of the International Actuarial\n  Association, 2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new multivariate distribution possessing arbitrarily parametrized and\npositively dependent univariate Pareto margins is introduced. Unlike the\nprobability law of Asimit et al. (2010) [Asimit, V., Furman, E. and Vernic, R.\n(2010) On a multivariate Pareto distribution. Insurance: Mathematics and\nEconomics 46(2), 308-316], the structure in this paper is absolutely continuous\nwith respect to the corresponding Lebesgue measure. The distribution is of\nimportance to actuaries through its connections to the popular frailty models,\nas well as because of the capacity to describe dependent heavy-tailed risks.\nThe genesis of the new distribution is linked to a number of existing\nprobability models, and useful characteristic results are proved. Expressions\nfor, e.g., the decumulative distribution and probability density functions,\n(joint) moments and regressions are developed. The distributions of minima and\nmaxima, as well as, some weighted risk measures are employed to exemplify\npossible applications of the distribution in insurance.\n"
    },
    {
        "paper_id": 1607.04739,
        "authors": "Jianxi Su, Edward Furman",
        "title": "Multiple risk factor dependence structures: Distributional properties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a class of dependence structures, that we call the Multiple Risk\nFactor (MRF) dependence structures. On the one hand, the new constructions\nextend the popular CreditRisk+ approach, and as such they formally describe\ndefault risk portfolios exposed to an arbitrary number of fatal risk factors\nwith conditionally exponential and dependent hitting (or occurrence) times. On\nthe other hand, the MRF structures can be seen as an encompassing family of\nmultivariate probability distributions with univariate margins distributed\nPareto of the 2nd kind, and in this role they can be used to model insurance\nrisk portfolios of dependent and heavy tailed risk components.\n"
    },
    {
        "paper_id": 1607.04883,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Statistical Industry Classification",
        "comments": "44 pages; trivial misprints corrected",
        "journal-ref": "Journal of Risk & Control 3(1) (2016) 17-65, Invited Editorial",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give complete algorithms and source code for constructing (multilevel)\nstatistical industry classifications, including methods for fixing the number\nof clusters at each level (and the number of levels). Under the hood there are\nclustering algorithms (e.g., k-means). However, what should we cluster?\nCorrelations? Returns? The answer turns out to be neither and our backtests\nsuggest that these details make a sizable difference. We also give an algorithm\nand source code for building \"hybrid\" industry classifications by improving\noff-the-shelf \"fundamental\" industry classifications by applying our\nstatistical industry classification methods to them. The presentation is\nintended to be pedagogical and geared toward practical applications in\nquantitative trading.\n"
    },
    {
        "paper_id": 1607.04968,
        "authors": "Zuzana Buckova, Beata Stehlikova, Daniel Sevcovic",
        "title": "Numerical and analytical methods for bond pricing in short rate\n  convergence models of interest rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this survey paper we discuss recent advances on short interest rate models\nwhich can be formulated in terms of a stochastic differential equation for the\ninstantaneous interest rate (also called short rate) or a system of such\nequations in case the short rate is assumed to depend also on other stochastic\nfactors. Our focus is on convergence models, which explain the evolution of\ninterest rate in connection with the adoption of Euro currency. Here, the\ndomestic short rate depends on a stochastic European short rate. In short rate\nmodels, the bond prices, which determine the term structure of interest rate,\nare obtained as solutions to partial differential equations. Analytical\nsolutions are available only in special cases; therefore we consider the\nquestion of obtaining their approximations. We use both analytical and\nnumerical methods to get an approximate solution to the partial differential\nequation for bond prices.\n"
    },
    {
        "paper_id": 1607.05235,
        "authors": "Yuke Li, Tianhao Wu, Nicholas Marshall, Stefan Steinerberger",
        "title": "Extracting Geography from Trade Data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.01.037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding international trade is a fundamental problem in economics --\none standard approach is via what is commonly called the \"gravity equation\",\nwhich predicts the total amount of trade $F_ij$ between two countries $i$ and\n$j$ as $$ F_{ij} = G \\frac{M_i M_j}{D_{ij}},$$ where $G$ is a constant, $M_i,\nM_j$ denote the \"economic mass\" (often simply the gross domestic product) and\n$D_{ij}$ the \"distance\" between countries $i$ and $j$, where \"distance\" is a\ncomplex notion that includes geographical, historical, linguistic and\nsociological components. We take the \\textit{inverse} route and ask ourselves\nto which extent it is possible to reconstruct meaningful information about\ncountries simply from knowing the bilateral trade volumes $F_{ij}$: indeed, we\nshow that a remarkable amount of geopolitical information can be extracted. The\nmain tool is a spectral decomposition of the Graph Laplacian as a tool to\nperform nonlinear dimensionality reduction. This may have further applications\nin economic analysis and provides a data-based approach to \"trade distance\".\n"
    },
    {
        "paper_id": 1607.05514,
        "authors": "Kiran Sharma, Shreyansh Shah, Anindya S. Chakrabarti and Anirban\n  Chakraborti",
        "title": "Sectoral co-movements in the Indian stock market: A mesoscopic network\n  analysis",
        "comments": "28 pages, 14 figures. To be submitted for the proceedings of the\n  JAFEE 20th International Conference on \"SOCIO-ECONOMIC SYSTEMS WITH ICT AND\n  NETWORKS\" held on 26-27 March, 2016 at the University of Tokyo, Japan",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we review several techniques to extract information from\nstock market data. We discuss recurrence analysis of time series, decomposition\nof aggregate correlation matrices to study co-movements in financial data,\nstock level partial correlations with market indices, multidimensional scaling\nand minimum spanning tree. We apply these techniques to daily return time\nseries from the Indian stock market. The analysis allows us to construct\nnetworks based on correlation matrices of individual stocks in one hand and on\nthe other, we discuss dynamics of market indices. Thus both micro level and\nmacro level dynamics can be analyzed using such tools. We use the\nmulti-dimensional scaling methods to visualize the sectoral structure of the\nstock market, and analyze the comovements among the sectoral stocks. Finally,\nwe construct a mesoscopic network based on sectoral indices. Minimum spanning\ntree technique is seen to be extremely useful in order to separate\ntechnologically related sectors and the mapping corresponds to actual\nproduction relationship to a reasonable extent.\n"
    },
    {
        "paper_id": 1607.05572,
        "authors": "Christian Bayer, Markus Siebenmorgen, Raul Tempone",
        "title": "Smoothing the payoff for efficient computation of Basket option prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of pricing basket options in a multivariate Black\nScholes or Variance Gamma model. From a numerical point of view, pricing such\noptions corresponds to moderate and high dimensional numerical integration\nproblems with non-smooth integrands. Due to this lack of regularity, higher\norder numerical integration techniques may not be directly available, requiring\nthe use of methods like Monte Carlo specifically designed to work for\nnon-regular problems. We propose to use the inherent smoothing property of the\ndensity of the underlying in the above models to mollify the payoff function by\nmeans of an exact conditional expectation. The resulting conditional\nexpectation is unbiased and yields a smooth integrand, which is amenable to the\nefficient use of adaptive sparse grid cubature. Numerical examples indicate\nthat the high-order method may perform orders of magnitude faster compared to\nMonte Carlo or Quasi Monte Carlo in dimensions up to 35.\n"
    },
    {
        "paper_id": 1607.05608,
        "authors": "Erik Barto\\v{s} and Richard Pin\\v{c}\\'ak",
        "title": "Identification of market trends with string and D2-brane maps",
        "comments": "10 pages, 8 figures, 3 tables",
        "journal-ref": "Physica A479 (2017) 57-70",
        "doi": "10.1016/j.physa.2017.03.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multi dimensional string objects are introduced as a new alternative for\nan application of string models for time series forecasting in trading on\nfinancial markets. The objects are represented by open string with 2-endpoints\nand D2-brane, which are continuous enhancement of 1-endpoint open string model.\nWe show how new object properties can change the statistics of the predictors,\nwhich makes them the candidates for modeling a wide range of time series\nsystems. String angular momentum is proposed as another tool to analyze the\nstability of currency rates except the historical volatility. To show the\nreliability of our approach with application of string models for time series\nforecasting we present the results of real demo simulations for four currency\nexchange pairs.\n"
    },
    {
        "paper_id": 1607.0566,
        "authors": "T. O. Benli",
        "title": "A Comparison of Nineteen Various Electricity Consumption Forecasting\n  Approaches and Practicing to Five Different Households in Turkey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The accuracy of the household electricity consumption forecast is vital in\ntaking better cost effective and energy efficient decisions. In order to design\naccurate, proper and efficient forecasting model, characteristics of the series\nhave to been analyzed. The source of time series data comes from Online\nEnerjisa System, the system of electrical energy provider in capital of Turkey,\nwhich consumers can reach their latest two year period electricity\nconsumptions; in our study the period was May 2014 to May 2016. Various\ntechniques had been applied in order to analyze the data; classical\ndecomposition models; standard typed and also with the centering moving average\nmethod, regression equations, exponential smoothing models and ARIMA models. In\nour study, nine teen different approaches; all of these have at least\ndiversified aspects of methodology, had been compared and the best model for\nforecasting were decided by considering the smallest values of MAPE, MAD and\nMSD. As a first step we took the time period May 2014 to May 2016 and found\npredicted value for June 2016 with the best forecasting model. After finding\nthe best forecasting model and fitted value for June 2016, than validating\nprocess had been taken place; we made comparisons to see how well the real\nvalue of June 2016 and forecasted value for that specific period matched.\nAfterwards we made electrical consumption forecast for the following 3 months;\nJune-September 2016 for each of five households individually.\n"
    },
    {
        "paper_id": 1607.05831,
        "authors": "Simon Clinet and Yoann Potiron",
        "title": "Statistical inference for the doubly stochastic self-exciting process",
        "comments": "47 pages, 4 figures, 4 tables. Under revision for Bernoulli Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and show the existence of a Hawkes self-exciting point process\nwith exponentially-decreasing kernel and where parameters are time-varying. The\nquantity of interest is defined as the integrated parameter\n$T^{-1}\\int_0^T\\theta_t^*dt$, where $\\theta_t^*$ is the time-varying parameter,\nand we consider the high-frequency asymptotics. To estimate it na\\\"ively, we\nchop the data into several blocks, compute the maximum likelihood estimator\n(MLE) on each block, and take the average of the local estimates. The\nasymptotic bias explodes asymptotically, thus we provide a non-na\\\"ive\nestimator which is constructed as the na\\\"ive one when applying a first-order\nbias reduction to the local MLE. We show the associated central limit theorem.\nMonte Carlo simulations show the importance of the bias correction and that the\nmethod performs well in finite sample, whereas the empirical study discusses\nthe implementation in practice and documents the stochastic behavior of the\nparameters.\n"
    },
    {
        "paper_id": 1607.06158,
        "authors": "Andrew Papanicolaou, Konstantinos Spiliopoulos",
        "title": "Dimension Reduction in Statistical Estimation of Partially Observed\n  Multiscale Processes",
        "comments": "SIAM Journal of Uncertainty Quantification, 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider partially observed multiscale diffusion models that are specified\nup to an unknown vector parameter. We establish for a very general class of\ntest functions that the filter of the original model converges to a filter of\nreduced dimension. Then, this result is used to justify statistical estimation\nfor the unknown parameters of interest based on the model of reduced dimension\nbut using the original available data. This allows to learn the unknown\nparameters of interest while working in lower dimensions, as opposed to working\nwith the original high dimensional system. Simulation studies support and\nillustrate the theoretical results.\n"
    },
    {
        "paper_id": 1607.06163,
        "authors": "David T. Frazier and Eric Renault",
        "title": "Indirect Inference With(Out) Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Indirect Inference (I-I) estimation of structural parameters $\\theta$\n{{requires matching observed and simulated statistics, which are most often\ngenerated using an auxiliary model that depends on instrumental parameters\n$\\beta$.}} {The estimators of the instrumental parameters will encapsulate} the\nstatistical information used for inference about the structural parameters. As\nsuch, artificially constraining these parameters may restrict the ability of\nthe auxiliary model to accurately replicate features in the structural data,\nwhich may lead to a range of issues, such as, a loss of identification.\nHowever, in certain situations the parameters $\\beta$ naturally come with a set\nof $q$ restrictions. Examples include settings where $\\beta$ must be estimated\nsubject to $q$ possibly strict inequality constraints $g(\\beta) > 0$, such as,\nwhen I-I is based on GARCH auxiliary models. In these settings we propose a\nnovel I-I approach that uses appropriately modified unconstrained auxiliary\nstatistics, which are simple to compute and always exists. We state the\nrelevant asymptotic theory for this I-I approach without constraints and show\nthat it can be reinterpreted as a standard implementation of I-I through a\nproperly modified binding function. Several examples that have featured in the\nliterature illustrate our approach.\n"
    },
    {
        "paper_id": 1607.06247,
        "authors": "Monika Novackova and Richard S.J. Tol",
        "title": "Effects of Sea Level Rise on Economy of the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report the first ex post study of the economic impact of sea level rise.\nWe apply two econometric approaches to estimate the past effects of sea level\nrise on the economy of the USA, viz. Barro type growth regressions adjusted for\nspatial patterns and a matching estimator. Unit of analysis is 3063 counties of\nthe USA. We fit growth regressions for 13 time periods and we estimated\nnumerous varieties and robustness tests for both growth regressions and\nmatching estimator. Although there is some evidence that sea level rise has a\npositive effect on economic growth, in most specifications the estimated\neffects are insignificant. We therefore conclude that there is no stable,\nsignificant effect of sea level rise on economic growth. This finding\ncontradicts previous ex ante studies.\n"
    },
    {
        "paper_id": 1607.06373,
        "authors": "Rene Carmona, Jean-Pierre Fouque, Seyyed Mostafa Mousavi, Li-Hsien Sun",
        "title": "Systemic Risk and Stochastic Games with Delay",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model of inter-bank lending and borrowing which takes into\naccount clearing debt obligations. The evolution of log-monetary reserves of\n$N$ banks is described by coupled diffusions driven by controls with delay in\ntheir drifts. Banks are minimizing their finite-horizon objective functions\nwhich take into account a quadratic cost for lending or borrowing and a linear\nincentive to borrow if the reserve is low or lend if the reserve is high\nrelative to the average capitalization of the system. As such, our problem is\nan $N$-player linear-quadratic stochastic differential game with delay. An\nopen-loop Nash equilibrium is obtained using a system of fully coupled forward\nand advanced backward stochastic differential equations. We then describe how\nthe delay affects liquidity and systemic risk characterized by a large number\nof defaults. We also derive a close-loop Nash equilibrium using an HJB\napproach.\n"
    },
    {
        "paper_id": 1607.06644,
        "authors": "Dirk Becherer and Martin B\\\"uttner and Klebert Kentia",
        "title": "On the monotone stability approach to BSDEs with jumps: Extensions,\n  concrete criteria and examples",
        "comments": "28 pages. Added DOI\n  https://link.springer.com/chapter/10.1007%2F978-3-030-22285-7_1 for final\n  publication, corrected typo (missing gamma) in example 4.15",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-22285-7_1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show a concise extension of the monotone stability approach to backward\nstochastic differential equations (BSDEs) that are jointly driven by a Brownian\nmotion and a random measure for jumps, which could be of infinite activity with\na non-deterministic and time inhomogeneous compensator. The BSDE generator\nfunction can be non convex and needs not to satisfy global Lipschitz conditions\nin the jump integrand. We contribute concrete criteria, that are easy to\nverify, for results on existence and uniqueness of bounded solutions to BSDEs\nwith jumps, and on comparison and a-priori $L^{\\infty}$-bounds. Several\nexamples and counter examples are discussed to shed light on the scope and\napplicability of different assumptions, and we provide an overview of major\napplications in finance and optimal control.\n"
    },
    {
        "paper_id": 1607.07099,
        "authors": "Jonathan Yu-Meng Li",
        "title": "Inverse Optimization of Convex Risk Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theory of convex risk functions has now been well established as the\nbasis for identifying the families of risk functions that should be used in\nrisk averse optimization problems. Despite its theoretical appeal, the\nimplementation of a convex risk function remains difficult, as there is little\nguidance regarding how a convex risk function should be chosen so that it also\nwell represents one's own risk preferences. In this paper, we address this\nissue through the lens of inverse optimization. Specifically, given solution\ndata from some (forward) risk-averse optimization problems we develop an\ninverse optimization framework that generates a risk function that renders the\nsolutions optimal for the forward problems. The framework incorporates the\nwell-known properties of convex risk functions, namely, monotonicity,\nconvexity, translation invariance, and law invariance, as the general\ninformation about candidate risk functions, and also the feedbacks from\nindividuals, which include an initial estimate of the risk function and\npairwise comparisons among random losses, as the more specific information. Our\nframework is particularly novel in that unlike classical inverse optimization,\nno parametric assumption is made about the risk function, i.e. it is\nnon-parametric. We show how the resulting inverse optimization problems can be\nreformulated as convex programs and are polynomially solvable if the\ncorresponding forward problems are polynomially solvable. We illustrate the\nimputed risk functions in a portfolio selection problem and demonstrate their\npractical value using real-life data.\n"
    },
    {
        "paper_id": 1607.07108,
        "authors": "Raj Kumari Bahl and Sotirios Sabanis",
        "title": "Model-Independent Price Bounds for Catastrophic Mortality Bonds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we are concerned with the valuation of Catastrophic Mortality\nBonds and, in particular, we examine the case of the Swiss Re Mortality Bond\n2003 as a primary example of this class of assets. This bond was the first\nCatastrophic Mortality Bond to be launched in the market and encapsulates the\nbehaviour of a well-defined mortality index to generate payoffs for\nbondholders. Pricing these type of bonds is a challenging task and no closed\nform solution exists in the literature. In our approach, we express the payoff\nof such a bond in terms of the payoff of an Asian put option and present a new\napproach to derive model-independent bounds exploiting comonotonic theory as\nillustrated in \\cite{prime1}, \\cite{2} and \\cite{Simon} for the pricing of\nAsian options. We carry out Monte Carlo simulations to estimate the bond price\nand illustrate the quality of the bounds.\n"
    },
    {
        "paper_id": 1607.07197,
        "authors": "Luciano Campi, Claude Martini",
        "title": "On the support of extremal martingale measures with given marginals: the\n  countable case",
        "comments": "14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the supports of extremal martingale measures with\npre-specified marginals in a two-period setting. First, we establish in full\ngenerality the equivalence between the extremality of a given measure $Q$ and\nthe denseness in $L^1(Q)$ of a suitable linear subspace, which can be seen in a\nfinancial context as the set of all semi-static trading strategies. Moreover,\nwhen the supports of both marginals are countable, we focus on the slightly\nstronger notion of weak exact predictable representation property (henceforth,\nWEP) and provide two combinatorial sufficient conditions, called \"2-link\nproperty\" and \"full erasability\", on how the points in the supports are linked\nto each other for granting extremality. When the support of the first marginal\nis a finite set, we give a necessary and sufficient condition for the WEP to\nhold in terms of the new concepts of $2$-net and deadlock. Finally, we study\nthe relation between cycles and extremality.\n"
    },
    {
        "paper_id": 1607.07398,
        "authors": "Andrea Saltelli, Mario Giampietro",
        "title": "The fallacy of evidence based policy",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.futures.2016.11.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of science for policy is at the core of a perfect storm generated by\nthe insurgence of several concurrent crises: of science, of trust, of\nsustainability. The modern positivistic model of science for policy, known as\nevidence based policy, is based on dramatic simplifications and compressions of\navailable perceptions of the state of affairs and possible explanations\n(hypocognition). This model can result in flawed prescriptions. The flaws\nbecome more evident when dealing with complex issues characterized by\nconcomitant uncertainties in the normative, descriptive and ethical domains. In\nthis situation evidence-based policy may concur to the fragility of the social\nsystem. Science plays an important role in reducing the feeling of\nvulnerability of humans by projecting a promise of protection against\nuncertainties. In many applications quantitative science is used to remove\nuncertainty by transforming it into probability, so that mathematical modelling\ncan play the ritual role of haruspices. This epistemic governance arrangement\nis today in crisis. The primacy of science to adjudicate political issues must\npass through an assessment of the level of maturity and effectiveness of the\nvarious disciplines deployed. The solution implies abandoning dreams of\nprediction, control and optimization obtained by relying on a limited set of\nsimplified narratives to define the problem and moving instead to an open\nexploration of a broader set of plausible and relevant stories. Evidence based\npolicy has to be replaced by robust policy, where robustness is tested with\nrespect to feasibility (compatibility with processes outside human control);\nviability (compatibility with processes under human control, in relation to\nboth the economic and technical dimensions), and desirability domain\n(compatibility with a plurality of normative considerations relevant to a\nplurality of actors).\n"
    },
    {
        "paper_id": 1607.0751,
        "authors": "Ricardo T. Fernholz and Christoffer Koch",
        "title": "The Rank Effect for Commodities",
        "comments": "25 pages, 10 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We uncover a large and significant low-minus-high rank effect for commodities\nacross two centuries. There is nothing anomalous about this anomaly, nor is it\nclear how it can be arbitraged away. Using nonparametric econometric methods,\nwe demonstrate that such a rank effect is a necessary consequence of a\nstationary relative asset price distribution. We confirm this prediction using\ndaily commodity futures prices and show that a portfolio consisting of\nlower-ranked, lower-priced commodities yields 23% higher annual returns than a\nportfolio consisting of higher-ranked, higher-priced commodities. These excess\nreturns have a Sharpe ratio nearly twice as high as the U.S. stock market yet\nare uncorrelated with market risk. In contrast to the extensive literature on\nasset pricing factors and anomalies, our results are structural and rely on\nminimal and realistic assumptions for the long-run behavior of relative asset\nprices.\n"
    },
    {
        "paper_id": 1607.07582,
        "authors": "Maria d'Errico, Alessandro Laio and Guido L. Chiarotti",
        "title": "Modelling the impact of financialization on agricultural commodity\n  markets",
        "comments": "30 pages, 1 table, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a stylized model of production and exchange in which long-term\ninvestors set their production decision over a horizon {\\tau} , the \"time to\nproduce\", and are liquidity constrained, while financial investors trade over a\nmuch shorter horizon {\\delta} (<< {\\tau} ) and are therefore more duly informed\non the exogenous shocks affecting the production output. The equilibrium\nsolution proves that: (i) long-term producers modify their production decisions\nto anticipate the impact of short-term investors allocations on prices; (ii)\nshort-term investments return a positive expected profit commensurate to the\ninformational advantage. While the presence of financial investors improves the\nefficiency of risk allocation in the short-term and reduces price volatility,\nthe model shows that the aggregate effect of commodity market financialization\nresults in rising the volatility of both farms' default risk and production\noutput.\n"
    },
    {
        "paper_id": 1607.07706,
        "authors": "Elena-Iulia Ap\\u{a}v\\u{a}loaie, Liviu Onoriu Marian, Elena Lucia Harpa",
        "title": "Online shopping key features analysis in Mures county",
        "comments": "4th RMEE Conference The Management Between Profit and Social\n  Responsibility, Todesco Publishing House, Cluj, Romania, 2014",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to get an overview of the online buyer profile, and\nalso some key aspects in the way the online shopping is conducted. In this\nproject we conducted a quantitative research, consisting of a questionnaire\nbased survey. For data processing and interpretation we used SPSS statistical\nsoftware and Excel. For data analysis, we used the descriptive statistics\nindicators, and a series of bi-varied analysis for testing some statistical\nassumptions. Viewed at first with skepticism by the Internet users in Romania,\nbecause of the many news about how dangerous the credit card payments are, the\nonline stores have gained much ground and trust in the recent years. Since the\nstudy was conducted mainly in the online environment, we can not talk about the\nrepresentativeness of the sample, only about a trend observed in the studied\npopulation. The study helps us understand the population reactions and\nattitudes regarding the online shopping. The study revealed some important\nissues regarding the online shopping in Mures county, issues that are described\nin detail in the content of this paper.\n"
    },
    {
        "paper_id": 1607.08214,
        "authors": "Jozef Barunik and Evzen Kocenda and Lukas Vacha",
        "title": "Asymmetric volatility connectedness on forex markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how bad and good volatility propagate through forex markets, i.e., we\nprovide evidence for asymmetric volatility connectedness on forex markets.\nUsing high-frequency, intra-day data of the most actively traded currencies\nover 2007 - 2015 we document the dominating asymmetries in spillovers that are\ndue to bad rather than good volatility. We also show that negative spillovers\nare chiefly tied to the dragging sovereign debt crisis in Europe while positive\nspillovers are correlated with the subprime crisis, different monetary policies\namong key world central banks, and developments on commodities markets. It\nseems that a combination of monetary and real-economy events is behind the net\npositive asymmetries in volatility spillovers, while fiscal factors are linked\nwith net negative spillovers.\n"
    },
    {
        "paper_id": 1607.08287,
        "authors": "Fei Fang, Yiwei Sun and Konstantinos Spiliopoulos",
        "title": "The effect of heterogeneity on flocking behavior and systemic risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to study organized flocking behavior and systemic\nrisk in heterogeneous mean-field interacting diffusions. We illustrate in a\nnumber of case studies the effect of heterogeneity in the behavior of systemic\nrisk in the system, i.e., the risk that several agents default simultaneously\nas a result of interconnections. We also investigate the effect of\nheterogeneity on the \"flocking behavior\" of different agents, i.e., when agents\nwith different dynamics end up following very similar paths and follow closely\nthe mean behavior of the system. Using Laplace asymptotics, we derive an\nasymptotic formula for the tail of the loss distribution as the number of\nagents grows to infinity. This characterizes the tail of the loss distribution\nand the effect of the heterogeneity of the network on the tail loss\nprobability.\n"
    },
    {
        "paper_id": 1608.00213,
        "authors": "S. Agarwal, D. Ghosh and A. S. Chakrabarti",
        "title": "Self-organization in a distributed coordination game through heuristic\n  rules",
        "comments": "11 pages, 12 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2016-70464-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a distributed coordination game played by a large\nnumber of agents with finite information sets, which characterizes emergence of\na single dominant attribute out of a large number of competitors. Formally, $N$\nagents play a coordination game repeatedly which has exactly $N$ Nash\nequilibria and all of the equilibria are equally preferred by the agents. The\nproblem is to select one equilibrium out of $N$ possible equilibria in the\nleast number of attempts. We propose a number of heuristic rules based on\nreinforcement learning to solve the coordination problem. We see that the\nagents self-organize into clusters with varying intensities depending on the\nheuristic rule applied although all clusters but one are transitory in most\ncases. Finally, we characterize a trade-off in terms of the time requirement to\nachieve a degree of stability in strategies and the efficiency of such a\nsolution.\n"
    },
    {
        "paper_id": 1608.0023,
        "authors": "S. Kuchuk-Iatsenko, Y. Mishura, Y. Munchak",
        "title": "Application of Malliavin calculus to exact and approximate option\n  pricing under stochastic volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article is devoted to models of financial markets with stochastic\nvolatility, which is defined by a functional of Ornstein-Uhlenbeck process or\nCox-Ingersoll-Ross process. We study the question of exact price of European\noption. The form of the density function of the random variable, which\nexpresses the average of the volatility over time to maturity is established\nusing Malliavin calculus.The result allows calculate the price of the option\nwith respect to minimum martingale measure when the Wiener process driving the\nevolution of asset price and the Wiener process, which defines volatility, are\nuncorrelated.\n"
    },
    {
        "paper_id": 1608.00275,
        "authors": "Ali Hosseiny, Mohammad Bahrami, Antonio Palestrini, Mauro Gallegati",
        "title": "Metastable Features of Economic Networks and Responses to Exogenous\n  Shocks",
        "comments": "13 pages, 10 figures, accepted for publication in PloS One",
        "journal-ref": "PloS one 11 (10), e0160363 (2016)",
        "doi": "10.1371/journal.pone.0160363",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been proved that network structure plays an important role in\naddressing a collective behaviour. In this paper we consider a network of firms\nand corporations and study its metastable features in an Ising based model. In\nour model, we observe that if in a recession the government imposes a demand\nshock to stimulate the network, metastable features shape its response.\nActually we find that there is a minimum bound where demand shocks with a size\nbelow it are unable to trigger the market out from recession. We then\ninvestigate the impact of network characteristics on this minimum bound. We\nsurprisingly observe that in a Watts-Strogatz network though the minimum bound\ndepends on the average of the degrees, when translated into the economics\nlanguage, such a bound is independent of the average degrees. This bound is\nabout $0.44 \\Delta$GDP, where $\\Delta$GDP is the gap of GDP between recession\nand expansion. We examine our suggestions for the cases of the United States\nand the European Union in the recent recession, and compare them with the\nimposed stimulations. While stimulation in the US has been above our threshold,\nin the EU it has been far below our threshold. Beside providing a minimum bound\nfor a successful stimulation, our study on the metastable features suggests\nthat in the time of crisis there is a \"golden time passage\" in which the\nminimum bound for successful stimulation can be much lower. So, our study\nstrongly suggests stimulations to be started within this time passage.\n"
    },
    {
        "paper_id": 1608.0028,
        "authors": "Jan Kuklinski and Panagiotis Papaioannou and Kevin Tyloo",
        "title": "Pricing Weakly Model Dependent Barrier Products",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We discuss the pricing methodology for Bonus Certificates and Barrier\nReverse-Convertible Structured Products. Pricing for a European barrier\ncondition is straightforward for products of both types and depends on an\nefficient interpolation of observed market option pricing. Pricing products We\ndiscuss the pricing methodology for Bonus Certificates and Barrier\nReverse-Convertible Structured Products. Pricing for a European barrier\ncondition is straightforward for products of both types and depends on an\nefficient interpolation of observed market option pricing. Pricing products\nwith an American barrier condition requires stochastic modelling. We show that\nfor typical market parameters, this stochastic pricing problem can be\nsystematically reduced to evaluating only one fairly simple stochastic\nparameter being the asymmetry of hitting the barrier. Eventually, pricing Bonus\nCertificates and Barrier Reverse Convertibles with an American barrier\ncondition, shows to be dependent on stochastic modelling only within a range of\n$\\pm\\frac{2}{3}$ of accuracy - e.g. within this accuracy limitation we can\nprice these products without stochastic modelling. We show that the remaining\nprice component is weakly dependent on the stochastic models. Combining these\ntogether, we prove to have established an almost model independent pricing\nprocedure for Bonus Certificates and Barrier Reverse-Convertible Structured\nProducts with American barrier conditions.\n"
    },
    {
        "paper_id": 1608.00756,
        "authors": "Julius Bonart, Fabrizio Lillo",
        "title": "A continuous and efficient fundamental price on the discrete order book\n  grid",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a model of liquidity provision in financial markets by\nadapting the Madhavan, Richardson, and Roomans (1997) price formation model to\nrealistic order books with quote discretization and liquidity rebates. We\npostulate that liquidity providers observe a fundamental price which is\ncontinuous, efficient, and can assume values outside the interval spanned by\nthe best quotes. We confirm the predictions of our price formation model with\nextensive empirical tests on large high-frequency datasets of 100 liquid Nasdaq\nstocks. Finally we use the model to propose an estimator of the fundamental\nprice based on the rebate adjusted volume imbalance at the best quotes and we\nempirically show that it outperforms other simpler estimators.\n"
    },
    {
        "paper_id": 1608.00768,
        "authors": "Huy N. Chau and Miklos Rasonyi",
        "title": "On optimal investment with processes of long or negative memory",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of utility maximization for investors with power\nutility functions. Building on the earlier work Larsen et al. (2016), we prove\nthat the value of the problem is a Frechet-differentiable function of the drift\nof the price process, provided that this drift lies in a suitable Banach space.\n  We then study optimal investment problems with non-Markovian driving\nprocesses. In such models there is no hope to get a formula for the achievable\nmaximal utility. Applying results of the first part of the paper we provide\nfirst order expansions for certain problems involving fractional Brownian\nmotion either in the drift or in the volatility. We also point out how\nasymptotic results can be derived for models with strong mean reversion.\n"
    },
    {
        "paper_id": 1608.00814,
        "authors": "Praveen Kolli, Mykhaylo Shkolnikov",
        "title": "SPDE limit of the global fluctuations in rank-based models",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider systems of diffusion processes (\"particles\") interacting through\ntheir ranks (also referred to as \"rank-based models\" in the mathematical\nfinance literature). We show that, as the number of particles becomes large,\nthe process of fluctuations of the empirical cumulative distribution functions\nconverges to the solution of a linear parabolic SPDE with additive noise. The\ncoefficients in the limiting SPDE are determined by the hydrodynamic limit of\nthe particle system which, in turn, can be described by the porous medium PDE.\nThe result opens the door to a thorough investigation of large equity markets\nand investment therein. In the course of the proof we also derive quantitative\npropagation of chaos estimates for the particle system.\n"
    },
    {
        "paper_id": 1608.00878,
        "authors": "Ross D. King",
        "title": "On the Use of Computer Programs as Money",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Money is a technology for promoting economic prosperity. Over history money\nhas become increasingly abstract, it used to be hardware, gold coins and the\nlike, now it is mostly software, data structures located in banks. Here I\npropose the logical conclusion of the abstraction of money: to use as money the\nmost general form of information - computer programs. The key advantage that\nusing programs for money (program-money) adds to the technology of money is\nagency. Program-money is active and thereby can fully participate in economics\nas economic agents. I describe the three basic technologies required to\nimplement program-money: computational languages/logics to unambiguously\ndescribe the actions and interactions of program-money; computational\ncryptography to ensure that only the correct actions and interactions are\nperformed; and a distributed computational environment in which the money can\nexecute. I demonstrate that most of the technology for program-money has\nalready been developed. The adoption of program-money transfers responsibility\nfrom human economic agents to money itself and has great potential economic\nadvantages over the current passive form of money. For example in\nmicroeconomics, adding agency to money will simplify the exchange of ownership,\nensure money is only used legally, automate the negotiation and forming of\ncontracts, etc. Similar advantages occur in macroeconomics, where for example\nthe control of the money supply could be transferred from central banks to\nmoney. It is also possible to envisage money that is not owned by any external\nhuman agent or corporation. One motivation for this is to force economic\nsystems to behave more rationally and/or more like a specific economic theory,\nthereby increasing the success of economic forecasting.\n"
    },
    {
        "paper_id": 1608.01103,
        "authors": "Susmita Bhaduri, Dipak Ghosh, Subhadeep Ghosh",
        "title": "Fluctuation of USA Gold Price - Revisited with Chaos-based Complex\n  Network Method",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give emphasis on the use of chaos-based rigorous nonlinear technique\ncalled Visibility Graph Analysis, to study one economic time series - gold\nprice of USA. This method can offer reliable results with fiinite data. This\npaper reports the result of such an analysis on the times series depicting the\nfluctuation of gold price of USA for the span of 25 years(1990 - 2013). This\nanalysis reveals that a quantitative parameter from the theory can explain\nsatisfactorily the real life nature of fluctuation of gold price of USA and\nhence building a strong database in terms of a quantitative parameter which can\neventually be used for forecasting purpose.\n"
    },
    {
        "paper_id": 1608.01133,
        "authors": "Pingjin Deng",
        "title": "The boundary non-Crossing probabilities for Slepian process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this contribution we derive an explicit formula for the boundary\nnon-crossing probabilities for Slepian processes associated with the piecewise\nlinear boundary function. This formula is used to develop an approximation\nformula to the boundary non-crossing probabilities for general continuous\nboundaries. The formulas we developed are easy to implement in calculation the\nboundary non-crossing probabilities.\n"
    },
    {
        "paper_id": 1608.01197,
        "authors": "Cornelis S.L. de Graaf and Drona Kandhai and Christoph Reisinger",
        "title": "Efficient exposure computation by risk factor decomposition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The focus of this paper is the efficient computation of counterparty credit\nrisk exposure on portfolio level. Here, the large number of risk factors rules\nout traditional PDE-based techniques and allows only a relatively small number\nof paths for nested Monte Carlo simulations, resulting in large variances of\nestimators in practice. We propose a novel approach based on Kolmogorov forward\nand backward PDEs, where we counter the high dimensionality by a generalisation\nof anchored-ANOVA decompositions. By computing only the most significant terms\nin the decomposition, the dimensionality is reduced effectively, such that a\nsignificant computational speed-up arises from the high accuracy of PDE schemes\nin low dimensions compared to Monte Carlo estimation. Moreover, we show how\nthis truncated decomposition can be used as control variate for the full\nhigh-dimensional model, such that any approximation errors can be corrected\nwhile a substantial variance reduction is achieved compared to the standard\nsimulation approach. We investigate the accuracy for a realistic portfolio of\nexchange options, interest rate and cross-currency swaps under a fully\ncalibrated ten-factor model.\n"
    },
    {
        "paper_id": 1608.01351,
        "authors": "Fuad Aleskerov, Victoria Oleynik",
        "title": "Multidimensional Polarization Index and its Application to an Analysis\n  of the Russian State Duma",
        "comments": "52 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multidimensional extension of the Aleskerov-Golubenko polarization index\nis developed. Several versions of the polarization index are proposed based on\ndifferent distance functions. Basic properties of the index are examined.\n"
    },
    {
        "paper_id": 1608.01365,
        "authors": "Jiyoung Kim, Satoshi Nakano, Kazuhiko Nishimura",
        "title": "Multifactor CES General Equilibrium: Models and Applications",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.econmod.2017.01.024",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sector specific multifactor CES elasticity of substitution and the\ncorresponding productivity growths are jointly measured by regressing the\ngrowths of factor-wise cost shares against the growths of factor prices. We use\nlinked input-output tables for Japan and the Republic of Korea as the data\nsource for factor price and cost shares in two temporally distant states. We\nthen construct a multi-sectoral general equilibrium model using the system of\nestimated CES unit cost functions, and evaluate the economy-wide propagation of\nan exogenous productivity stimuli, in terms of welfare. Further, we examine the\ndifferences between models based on a priori elasticity such as Leontief and\nCobb-Douglas.\n"
    },
    {
        "paper_id": 1608.01415,
        "authors": "Christoph Czichowsky, R\\'emi Peyre, Walter Schachermayer and Junjian\n  Yang",
        "title": "Shadow prices, fractional Brownian motion, and portfolio optimisation\n  under transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We continue the analysis of our previous paper (Czichowsky/Schachermayer/Yang\n2014) pertaining to the existence of a shadow price process for portfolio\noptimisation under proportional transaction costs. There, we established a\npositive answer for a continuous price process $S=(S_t)_{0\\leq t\\leq T}$\nsatisfying the condition $(NUPBR)$ of \"no unbounded profit with bounded risk\".\nThis condition requires that $S$ is a semimartingale and therefore is too\nrestrictive for applications to models driven by fractional Brownian motion. In\nthe present paper, we derive the same conclusion under the weaker condition\n$(TWC)$ of \"two way crossing\", which does not require $S$ to be a\nsemimartingale. Using a recent result of R.~Peyre, this allows us to show the\nexistence of a shadow price for exponential fractional Brownian motion and\n$all$ utility functions defined on the positive half-line having reasonable\nasymptotic elasticity. Prime examples of such utilities are logarithmic or\npower utility.\n"
    },
    {
        "paper_id": 1608.01535,
        "authors": "Satoshi Nakano, Kazuhiko Nishimura",
        "title": "Optimal Population in a Finite Horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A favorable population schedule for the entire potential human family is\nsought, under the overlapping generations framework, by treating population (or\nfertility) as a planning variable in a dynamical social welfare maximization\ncontext. The utilitarian and maximin social welfare functions are examined,\nwith zero future discounting, while infinity in the maximand is circumvented by\nintroducing the depletion of energy resources and its postponement through\ntechnological innovations. The model is formulated as a free-horizon dynamical\nplanning problem, solved via a non-linear optimizer. Under exploratory\nscenarios, we visualize the potential trade-offs between the two welfare\ncriteria.\n"
    },
    {
        "paper_id": 1608.01795,
        "authors": "Ulrich Horst and D\\\"orte Kreher",
        "title": "A diffusion approximation for limit order book models",
        "comments": "Titel changed, appendix added, presentation improved",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper derives a diffusion approximation for a sequence of discrete-time\none-sided limit order book models with non-linear state dependent order arrival\nand cancellation dynamics. The discrete time sequences are specified in terms\nof an $\\R_+$-valued best bid price process and an $L^2_{loc}$-valued volume\nprocess. It is shown that under suitable assumptions the sequence of\ninterpolated discrete time models is relatively compact in a localized sense\nand that any limit point satisfies a certain infinite dimensional SDE. Under\nadditional assumptions on the dependence structure we construct two classes of\nmodels, which fit in the general framework, such that the limiting SDE admits a\nunique solution and thus the discrete dynamics converge to a diffusion limit in\na localized sense.\n"
    },
    {
        "paper_id": 1608.01891,
        "authors": "Alexei Botchkarev",
        "title": "Toward Development of a New Health Economic Evaluation Definition",
        "comments": null,
        "journal-ref": "Journal of Economics Bibliography, 2016, (3)4, pp. 590-601",
        "doi": "10.1453/jeb.v3i4.1005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic evaluation is a dynamically advancing knowledge area of health\neconomics. It has been conceived to provide evidence for allocating scarce\nresources to gain the best value for money. The problem of efficiency of\ninvestments becomes even more crucial with advances in modern medicine and\npublic health which bring about both improved patient outcomes and higher\ncosts. Despite the abundance of literature on the economic evaluation concepts,\nsome key notions including the definition of the health economic evaluation\nremain open for discussion. Academic literature offers a large number and\ngrowing variety of economic evaluation definitions. It testifies to the fact\nthat existing definitions do not meet requirements of economists. The aim of\nthis study was to examine existing definitions and reveal their common\nfeatures.\n"
    },
    {
        "paper_id": 1608.01895,
        "authors": "Mikkel Bennedsen",
        "title": "Semiparametric inference on the fractal index of Gaussian and\n  conditionally Gaussian time series data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/07474938.2020.1721832",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a well-known estimator of the fractal index of a stochastic process.\nOur framework is very general and encompasses many models of interest; we show\nhow to extend the theory of the estimator to a large class of non-Gaussian\nprocesses. Particular focus is on clarity and ease of implementation of the\nestimator and the associated asymptotic results, making it easy for\npractitioners to apply the methods. We additionally show how measurement noise\nin the observations will bias the estimator, potentially resulting in the\npractitioner erroneously finding evidence of fractal characteristics in a time\nseries. We propose a new estimator which is robust to such noise and construct\na formal hypothesis test for the presence of noise in the observations.\nFinally, the methods are illustrated on two empirical data sets; one of\nturbulent velocity flows and one of financial prices.\n"
    },
    {
        "paper_id": 1608.019,
        "authors": "T. M. A. Fink, M. Reeves, R. Palma and R. S. Farr",
        "title": "Serendipity and strategy in rapid innovation",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1038/s41467-017-02042-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Innovation is to organizations what evolution is to organisms: it is how\norganisations adapt to changes in the environment and improve. Governments,\ninstitutions and firms that innovate are more likely to prosper and stand the\ntest of time; those that fail to do so fall behind their competitors and\nsuccumb to market and environmental change. Yet despite steady advances in our\nunderstanding of evolution, what drives innovation remains elusive. On the one\nhand, organizations invest heavily in systematic strategies to drive\ninnovation. On the other, historical analysis and individual experience suggest\nthat serendipity plays a significant role in the discovery process. To unify\nthese two perspectives, we analyzed the mathematics of innovation as a search\nprocess for viable designs across a universe of building blocks. We then tested\nour insights using historical data from language, gastronomy and technology. By\nmeasuring the number of makeable designs as we acquire more components, we\nobserved that the relative usefulness of different components is not fixed, but\ncross each other over time. When these crossovers are unanticipated, they\nappear to be the result of serendipity. But when we can predict crossovers\nahead of time, they offer an opportunity to strategically increase the growth\nof our product space. Thus we find that the serendipitous and strategic visions\nof innovation can be viewed as different manifestations of the same thing: the\nchanging importance of component building blocks over time.\n"
    },
    {
        "paper_id": 1608.02028,
        "authors": "Michael A. Kouritzin",
        "title": "Explicit Heston Solutions and Stochastic Approximation for\n  Path-dependent Option Pricing",
        "comments": "42 pages",
        "journal-ref": "Int. J. Theor. Appl. Finan., 21, 1850006 (2018) [45 pages]",
        "doi": "10.1142/S0219024918500061",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  New simulation approaches to evaluating path-dependent options without matrix\ninversion issues nor Euler bias are evaluated. They employ three main\ncontributions: Stochastic approximation replaces regression in the LSM\nalgorithm; Explicit weak solutions to stochastic differential equations are\ndeveloped and applied to Heston model simulation; and Importance sampling\nexpands these explicit solutions. The approach complements Heston (1993) and\nBroadie and Kaya (2006) by handling the case of path-dependence in the option's\nexecution strategy. Numeric comparison against standard Monte Carlo methods\ndemonstrate up to two orders of magnitude speed improvement. The general ideas\nwill extend beyond the important Heston setting.\n"
    },
    {
        "paper_id": 1608.02068,
        "authors": "Ngoc Huy Chau, Wolfgang Runggaldier and Peter Tankov",
        "title": "Arbitrage and utility maximization in market models with an insider",
        "comments": "Replaced with revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study arbitrage opportunities, market viability and utility maximization\nin market models with an insider. Assuming that an economic agent possesses\nfrom the beginning an additional information in the form of a random variable\nG, which only becomes known to the ordinary agents at date T, we give criteria\nfor the No Unbounded Profits with Bounded Risk property to hold, characterize\noptimal arbitrage strategies, and prove duality results for the utility\nmaximization problem faced by the insider. Examples of markets satisfying NUPBR\nyet admitting arbitrage opportunities are provided for both atomic and\ncontinuous random variables G.\n"
    },
    {
        "paper_id": 1608.02365,
        "authors": "Bernardi Mauro and Roy Cerqueti and Arsen Palestini",
        "title": "Allocation of risk capital in a cost cooperative game induced by a\n  modified Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The standard theory of coherent risk measures fails to consider individual\ninstitutions as part of a system which might itself experience instability and\nspread new sources of risk to the market participants. In compliance with an\napproach adopted by Shapley and Shubik (1969), this paper proposes a\ncooperative market game where agents and institutions play the same role can be\ndeveloped. We take into account a multiple institutions framework where some of\nthem jointly experience distress events in order to evaluate their individual\nand collective impact on the remaining institutions in the market. To carry out\nthis analysis, we define a new risk measure (SCoES), generalising the Expected\nShortfall of Acerbi (2002) and we characterise the riskiness profile as the\noutcome of a cost cooperative game played by institutions in distress (a\nsimilar approach was adopted by Denault 2001). Each institution's marginal\ncontribution to the spread of riskiness towards the safe institutions in then\nevaluated by calculating suitable solution concepts of the game such as the\nBanzhaf--Coleman and the Shapley--Shubik values.\n"
    },
    {
        "paper_id": 1608.02428,
        "authors": "Eugen Tarnow",
        "title": "The Opium for the Poor Is Opium. Medicare Providers in States with Low\n  Income Prescribe High Levels of Opiates",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.22219.18722",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The majority of Medicare opioid prescriptions originate with family practice\nand internal medicine providers.\n  I show that the average number of Medicare opium prescriptions by these\nproviders vary strongly by state and that 54% of the variance is accounted for\nby the state median household income. I also show that there is a very similar\nrelationship in opioid claims per capita and per Medicare recipient.\n  In all cases Alabama is the state with the most claims and Hawaii is the\nstate with the least claims.\n"
    },
    {
        "paper_id": 1608.02446,
        "authors": "Nuno Azevedo, Diogo Pinheiro, Stylianos Xanthopoulos, Athanasios\n  Yannacopoulos",
        "title": "Who would invest only in the risk-free asset?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the setup of continuous-time semimartingale financial markets, we show\nthat a multiprior Gilboa-Schmeidler minimax expected utility maximizer forms a\nportfolio consisting only of the riskless asset if and only if among the\ninvestor's priors there exists a probability measure under which all admissible\nwealth processes are supermartingales. Furthermore, we show that under a\ncertain attainability condition (which is always valid in finite or complete\nmarkets) this is also equivalent to the existence of an equivalent (local)\nmartingale measure among the investor's priors. As an example, we generalize a\nno betting result due to Dow and Werlang.\n"
    },
    {
        "paper_id": 1608.02523,
        "authors": "Ali Hosseiny and Mauro Gallegati",
        "title": "Role of Intensive and Extensive Variables in a Soup of Firms in Economy\n  to Address Long Run Prices and Aggregate Data",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 470, 51-59,\n  2017",
        "doi": "10.1016/j.physa.2016.11.130",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review the production function and the hypothesis of equilibrium in the\nneoclassical framework. We notify that in a soup of sectors in economy, while\ncapital and labor resemble extensive variables, wage and rate of return on\ncapital act as intensive variables. As a result, Baumol and Bowen's statement\nof equal wages is inevitable from the thermodynamics point of view. We try to\nsee how aggregation can be performed concerning the extensive variables in a\nsoup of firms. We provide a toy model to perform aggregation for production and\nthe labor income as extensive quantities in a neoclassical framework.\n"
    },
    {
        "paper_id": 1608.0255,
        "authors": "Camilo Hernandez and Mauricio Junca and Harold Moreno-Franco",
        "title": "A time of ruin constrained optimal dividend problem for spectrally\n  one-sided L\\'evy processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a longevity feature to the classical optimal dividend problem by\nadding a constraint on the time of ruin of the firm. We extend the results in\n\\cite{HJ15}, now in context of one-sided L\\'evy risk models. We consider de\nFinetti's problem in both scenarios with and without fix transaction costs,\ne.g. taxes. We also study the constrained analog to the so called Dual model.\nTo characterize the solution to the aforementioned models we introduce the dual\nproblem and show that the complementary slackness conditions are satisfied and\ntherefore there is no duality gap. As a consequence the optimal value function\ncan be obtained as the pointwise infimum of auxiliary value functions indexed\nby Lagrange multipliers. Finally, we illustrate our findings with a series of\nnumerical examples.\n"
    },
    {
        "paper_id": 1608.0269,
        "authors": "Maxim Bichuch, Agostino Capponi, Stephan Sturm",
        "title": "Arbitrage-Free XVA",
        "comments": "39 pages, 11 figures, 2 tables. This article subsumes the two\n  permanent working papers by the same authors: \"Arbitrage-Free Pricing of XVA\n  - Part I: Framework and Explicit Examples\", and \"Arbitrage-Free Pricing of\n  XVA - Part II: PDE Representation and Numerical Analysis\". These papers are\n  accessible at arXiv:1501.05893 and arXiv:1502.06106, respectively",
        "journal-ref": "Math. Finance 28:2 (2018), 582-620",
        "doi": "10.1111/mafi.12146",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a framework for computing the total valuation adjustment (XVA) of\na European claim accounting for funding costs, counterparty credit risk, and\ncollateralization. Based on no-arbitrage arguments, we derive backward\nstochastic differential equations (BSDEs) associated with the replicating\nportfolios of long and short positions in the claim. This leads to the\ndefinition of buyer's and seller's XVA, which in turn identify a no-arbitrage\ninterval. In the case that borrowing and lending rates coincide, we provide a\nfully explicit expression for the unique XVA, expressed as a percentage of the\nprice of the traded claim, and for the corresponding replication strategies. In\nthe general case of asymmetric funding, repo and collateral rates, we study the\nsemilinear partial differential equations (PDE) characterizing buyer's and\nseller's XVA and show the existence of a unique classical solution to it. To\nillustrate our results, we conduct a numerical study demonstrating how funding\ncosts, repo rates, and counterparty risk contribute to determine the total\nvaluation adjustment.\n"
    },
    {
        "paper_id": 1608.02706,
        "authors": "Vladimir Vovk",
        "title": "Another example of duality between game-theoretic and measure-theoretic\n  probability",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper makes a small step towards a non-stochastic version of\nsuperhedging duality relations in the case of one traded security with a\ncontinuous price path. Namely, we prove the coincidence of game-theoretic and\nmeasure-theoretic expectation for lower semicontinuous positive functionals. We\nconsider a new broad definition of game-theoretic probability, leaving the\nolder narrower definitions for future work.\n"
    },
    {
        "paper_id": 1608.0274,
        "authors": "Monica Billio, Roberto Casarin, Luca Rossini",
        "title": "Bayesian nonparametric sparse VAR models",
        "comments": "Forthcoming in \"Journal of Econometrics\" ---- Revised Version of the\n  paper \"Bayesian nonparametric Seemingly Unrelated Regression Models\" ----\n  Supplementary Material available on request",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High dimensional vector autoregressive (VAR) models require a large number of\nparameters to be estimated and may suffer of inferential problems. We propose a\nnew Bayesian nonparametric (BNP) Lasso prior (BNP-Lasso) for high-dimensional\nVAR models that can improve estimation efficiency and prediction accuracy. Our\nhierarchical prior overcomes overparametrization and overfitting issues by\nclustering the VAR coefficients into groups and by shrinking the coefficients\nof each group toward a common location. Clustering and shrinking effects\ninduced by the BNP-Lasso prior are well suited for the extraction of causal\nnetworks from time series, since they account for some stylized facts in\nreal-world networks, which are sparsity, communities structures and\nheterogeneity in the edges intensity. In order to fully capture the richness of\nthe data and to achieve a better understanding of financial and macroeconomic\nrisk, it is therefore crucial that the model used to extract network accounts\nfor these stylized facts.\n"
    },
    {
        "paper_id": 1608.03053,
        "authors": "Li-Ling Su, Xiong-Fei Jiang, Sai-Ping Li, Li-Xin Zhong, Fei Ren",
        "title": "Dynamic structure of stock communities: A comparative study between\n  stock returns and turnover rates",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1140/epjb/e2017-70625-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The detection of community structure in stock market is of theoretical and\npractical significance for the study of financial dynamics and portfolio risk\nestimation. We here study the community structures in Chinese stock markets\nfrom the aspects of both price returns and turnover rates, by using a\ncombination of the PMFG and infomap methods based on a distance matrix. We find\nthat a few of the largest communities are composed of certain specific industry\nor conceptional sectors and the correlation inside a sector is generally larger\nthan the correlation between different sectors. In comparison with returns, the\ncommunity structure for turnover rates is more complex and the sector effect is\nrelatively weaker. The financial dynamics is further studied by analyzing the\ncommunity structures over five sub-periods. Sectors like banks, real estate,\nhealth care and New Shanghai take turns to compose a few of the largest\ncommunities for both returns and turnover rates in different sub-periods.\nSeveral specific sectors appear in the communities with different rank orders\nfor the two time series even in the same sub-period. A comparison between the\nevolution of prices and turnover rates of stocks from these sectors is\nconducted to better understand their differences. We find that stock prices\nonly had large changes around some important events while turnover rates surged\nafter each of these events relevant to specific sectors, which may offer a\npossible explanation for the complexity of stock communities for turnover\nrates.\n"
    },
    {
        "paper_id": 1608.03058,
        "authors": "Fei Ren, Ya-Nan Lu, Sai-Ping Li, Xiong-Fei Jiang, Li-Xin Zhong, and\n  Tian Qiu",
        "title": "Dynamic portfolio strategy using clustering approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0169299",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of portfolio optimization is one of the most important issues in\nasset management. This paper proposes a new dynamic portfolio strategy based on\nthe time-varying structures of MST networks in Chinese stock markets, where the\nmarket condition is further considered when using the optimal portfolios for\ninvestment. A portfolio strategy comprises two stages: selecting the portfolios\nby choosing central and peripheral stocks in the selection horizon using five\ntopological parameters, i.e., degree, betweenness centrality, distance on\ndegree criterion, distance on correlation criterion and distance on distance\ncriterion, then using the portfolios for investment in the investment horizon.\nThe optimal portfolio is chosen by comparing central and peripheral portfolios\nunder different combinations of market conditions in the selection and\ninvestment horizons. Market conditions in our paper are identified by the\nratios of the number of trading days with rising index or the sum of the\namplitudes of the trading days with rising index to the total number of trading\ndays. We find that central portfolios outperform peripheral portfolios when the\nmarket is under a drawup condition, or when the market is stable or drawup in\nthe selection horizon and is under a stable condition in the investment\nhorizon. We also find that the peripheral portfolios gain more than central\nportfolios when the market is stable in the selection horizon and is drawdown\nin the investment horizon. Empirical tests are carried out based on the optimal\nportfolio strategy. Among all the possible optimal portfolio strategy based on\ndifferent parameters to select portfolios and different criteria to identify\nmarket conditions, $65\\%$ of our optimal portfolio strategies outperform the\nrandom strategy for the Shanghai A-Share market and the proportion is $70\\%$\nfor the Shenzhen A-Share market.\n"
    },
    {
        "paper_id": 1608.03237,
        "authors": "Andrew Lesniewski and Anja Richter",
        "title": "Managing counterparty credit risk via BSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a general dynamic replication approach to counterparty credit risk\nmodeling. This leads to a fundamental jump-process backward stochastic\ndifferential equation (BSDE) for the credit risk adjusted portfolio value. We\nthen reduce the fundamental BSDE to a continuous BSDE. Depending on the close\nout value convention, the reduced fundamental BSDE's solution can be\nrepresented explicitly or through an accurate approximate expression.\nFurthermore, we discuss practical aspects of the approach, important for the\nits industry applications: (i) efficient numerical methodology for solving a\nBSDE driven by a moderate number of Brownian motions, and (ii) factor reduction\nmethodology that allows one to approximately replace a portfolio driven by a\nlarge number of risk factors with a portfolio driven by a moderate number of\nrisk factors.\n"
    },
    {
        "paper_id": 1608.03352,
        "authors": "Deborshee Sen, Ajay Jasra and Yan Zhou",
        "title": "Some Contributions to Sequential Monte Carlo Methods for Option Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing options is an important problem in financial engineering. In many\nscenarios of practical interest, financial option prices associated to an\nunderlying asset reduces to computing an expectation w.r.t.~a diffusion\nprocess. In general, these expectations cannot be calculated analytically, and\none way to approximate these quantities is via the Monte Carlo method; Monte\nCarlo methods have been used to price options since at least the 1970's. It has\nbeen seen in Del Moral, P. \\& Shevchenko, P.V. (2014) `Valuation of barrier\noptions using Sequential Monte Carlo' and Jasra, A. \\& Del Moral, P. (2011)\n`Sequential Monte Carlo for option pricing' that Sequential Monte Carlo (SMC)\nmethods are a natural tool to apply in this context and can vastly improve over\nstandard Monte Carlo. In this article, in a similar spirit to Del Moral, P. \\&\nShevchenko, P.V. (2014) `Valuation of barrier options using sequential Monte\nCarlo' and Jasra, A. \\& Del Moral, P. (2011) `Sequential Monte Carlo for option\npricing' we show that one can achieve significant gains by using SMC methods by\nconstructing a sequence of artificial target densities over time. In\nparticular, we approximate the optimal importance sampling distribution in the\nSMC algorithm by using a sequence of weighting functions. This is demonstrated\non two examples, barrier options and target accrual redemption notes (TARN's).\nWe also provide a proof of unbiasedness of our SMC estimate.\n"
    },
    {
        "paper_id": 1608.03428,
        "authors": "Daniel Conus and Mackenzie Wildman",
        "title": "A Gaussian Markov alternative to fractional Brownian motion for pricing\n  financial derivatives",
        "comments": "28 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Replacing Black-Scholes' driving process, Brownian motion, with fractional\nBrownian motion allows for incorporation of a past dependency of stock prices\nbut faces a few major downfalls, including the occurrence of arbitrage when\nimplemented in the financial market. We present the development, testing, and\nimplementation of a simplified alternative to using fractional Brownian motion\nfor pricing derivatives. By relaxing the assumption of past independence of\nBrownian motion but retaining the Markovian property, we are developing a\ncompeting model that retains the mathematical simplicity of the standard\nBlack-Scholes model but also has the improved accuracy of allowing for past\ndependence. This is achieved by replacing Black-Scholes' underlying process,\nBrownian motion, with a particular Gaussian Markov process, proposed by\nVladimir Dobri\\'{c} and Francisco Ojeda.\n"
    },
    {
        "paper_id": 1608.03521,
        "authors": "Avinash Chand Yadav, Kaustubh Manchanda, and Ramakrishna Ramaswamy",
        "title": "Emergent organization in a model market",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the collective behavior of interacting agents in a simple model of\nmarket economics originally introduced by N{\\o}rrelykke and Bak. A general\ntheoretical framework for interacting traders on an arbitrary network is\npresented, with the interaction consisting of buying (namely, consumption) and\nselling (namely, production) of commodities. Extremal dynamics is introduced by\nhaving the agent with least profit in the market readjust prices, causing the\nmarket to self--organize. We study this model market on regular lattices in\ntwo--dimension as well as on random complex networks; in the critical state\nfluctuations in an activity signal exhibit properties that are characteristic\nof avalanches observed in models of self-organized criticality, and these can\nbe described by power--law distributions.\n"
    },
    {
        "paper_id": 1608.03636,
        "authors": "Atul Deshpande and B. Ross Barmish",
        "title": "A General Framework for Pairs Trading with a Control-Theoretic Point of\n  View",
        "comments": "Accepted at IEEE Multi Systems Conference, 2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pairs trading is a market-neutral strategy that exploits historical\ncorrelation between stocks to achieve statistical arbitrage. Existing\npairs-trading algorithms in the literature require rather restrictive\nassumptions on the underlying stochastic stock-price processes and the\nso-called spread function. In contrast to existing literature, we consider an\nalgorithm for pairs trading which requires less restrictive assumptions than\nheretofore considered. Since our point of view is control-theoretic in nature,\nthe analysis and results are straightforward to follow by a non-expert in\nfinance. To this end, we describe a general pairs-trading algorithm which\nallows the user to define a rather arbitrary spread function which is used in a\nfeedback context to modify the investment levels dynamically over time. When\nthis function, in combination with the price process, satisfies a certain\nmean-reversion condition, we deem the stocks to be a tradeable pair. For such a\ncase, we prove that our control-inspired trading algorithm results in positive\nexpected growth in account value. Finally, we describe tests of our algorithm\non historical trading data by fitting stock price pairs to a popular spread\nfunction used in literature. Simulation results from these tests demonstrate\nrobust growth while avoiding huge drawdowns.\n"
    },
    {
        "paper_id": 1608.03985,
        "authors": "Peter Richmond, Bertrand M. Roehner",
        "title": "Property bubble in Hong Kong: A predicted decade-long slump (2016-2025)",
        "comments": "21 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Between 2003 and 2015 the prices of apartments in Hong Kong (adjusted for\ninflation) increased by a factor of 3.8. This is much higher than in the United\nStates prior to the so-called subprime crisis of 2007. The analysis of this\nspeculative episode confirms the mechanism and regularities already highlighted\nby the present authors in similar episodes in other countries. Based on these\nregularities, it is possible to predict the price trajectory over the time\ninterval 2016-2025. It suggests that, unless appropriate relief is provided by\nthe mainland, Hong Kong will experience a decade-long slump. Possible\nimplications for its relations with Beijing are discussed at the end of the\npaper.\n"
    },
    {
        "paper_id": 1608.04506,
        "authors": "Bulcs\\'u S\\'andor, Ingve Simonsen, B\\'alint Zsolt Nagy and Zolt\\'an\n  N\\'eda",
        "title": "Time-scale effects on the gain-loss asymmetry in stock indices",
        "comments": "10 pages, 12 figures, 1 supplementary material",
        "journal-ref": "Phys. Rev. E 94, 022311 (2016)",
        "doi": "10.1103/PhysRevE.94.022311",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The gain-loss asymmetry, observed in the inverse statistics of stock indices\nis present for logarithmic return levels that are over $2\\%$, and it is the\nresult of the non-Pearson type auto-correlations in the index. These\nnon-Pearson type correlations can be viewed also as functionally dependent\ndaily volatilities, extending for a finite time interval. A generalized\ntime-window shuffling method is used to show the existence of such\nauto-correlations. Their characteristic time-scale proves to be smaller (less\nthan $25$ trading days) than what was previously believed. It is also found\nthat this characteristic time-scale has decreased with the appearance of\nprogram trading in the stock market transactions. Connections with the leverage\neffect are also established.\n"
    },
    {
        "paper_id": 1608.04522,
        "authors": "Takashi Shinzato",
        "title": "Maximizing and Minimizing Investment Concentration with Constraints of\n  Budget and Investment Risk",
        "comments": "6 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, as a first step in examining the properties of a feasible\nportfolio subset that is characterized by budget and risk constraints, we\nassess the maximum and minimum of the investment concentration using replica\nanalysis. To do this, we apply an analytical approach of statistical mechanics.\nWe note that the optimization problem considered in this paper is the dual\nproblem of the portfolio optimization problem discussed in the literature, and\nwe verify that these optimal solutions are also dual. We also present numerical\nexperiments, in which we use the method of steepest descent that is based on\nLagrange's method of undetermined multipliers, and we compare the numerical\nresults to those obtained by replica analysis in order to assess the\neffectiveness of our proposed approach.\n"
    },
    {
        "paper_id": 1608.04537,
        "authors": "Luis H. R. Alvarez E. and Paavo Salminen",
        "title": "Timing in the Presence of Directional Predictability: Optimal Stopping\n  of Skew Brownian Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a class of optimal stopping problems arising in, for example,\nstudies considering the timing of an irreversible investment when the\nunderlying follows a skew Brownian motion. Our results indicate that the local\ndirectional predictability modeled by the presence of a skew point for the\nunderlying has a nontrivial and somewhat surprising impact on the timing\nincentives of the decision maker. We prove that waiting is always optimal at\nthe skew point for a large class of exercise payoffs. An interesting\nconsequence of this finding, which is in sharp contrast with studies relying on\nordinary Brownian motion, is that the exercise region for the problem can\nbecome unconnected even when the payoff is linear. We also establish that\nhigher skewness increases the incentives to wait and postpones the optimal\ntiming of an investment opportunity. Our general results are explicitly\nillustrated for a piecewise linear payoff.\n"
    },
    {
        "paper_id": 1608.04556,
        "authors": "Jan Lorenz and Christoph Brauer and Dirk A. Lorenz",
        "title": "Rank-optimal weighting or \"How to be best in the OECD Better Life\n  Index?\"",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1007/s11205-016-1416-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method of rank-optimal weighting which can be used to explore\nthe best possible position of a subject in a ranking based on a composite\nindicator by means of a mathematical optimization problem. As an example, we\nexplore the dataset of the OECD Better Life Index and compute for each country\na weight vector which brings it as far up in the ranking as possible with the\ngreatest advance of the immediate rivals. The method is able to answer the\nquestion \"What is the best possible rank a country can achieve with a given set\nof weighted indicators?\" Typically, weights in composite indicators are\njustified normatively and not empirically. Our approach helps to give bounds on\nwhat is achievable by such normative judgments from a purely output-oriented\nand strongly competitive perspective. The method can serve as a basis for exact\nbounds in sensitivity analysis focused on ranking positions.\n  In the OECD Better Life Index data we find that 19 out the 36 countries in\nthe OECD Better Life Index 2014 can be brought to the top of the ranking by\nspecific weights. We give a table of weights for each country which brings it\nto its highest possible position. Many countries achieve their best rank by\nfocusing on their strong dimensions and setting the weights of many others to\nzero. Although setting dimensions to zero is possible in the OECD's online\ntool, this contradicts the idea of better life being multidimensional in\nessence. We discuss modifications of the optimization problem which could take\nthis into account, e.g. by allowing only a minimal weight of one.\n  Methods to find rank-optimal weights can be useful for various\nmultidimensional datasets like the ones used to rank universities or employers.\n"
    },
    {
        "paper_id": 1608.04621,
        "authors": "Adrien Genin and Peter Tankov",
        "title": "Optimal importance sampling for L\\'evy Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop generic and efficient importance sampling estimators for Monte\nCarlo evaluation of prices of single- and multi-asset European and\npath-dependent options in asset price models driven by L\\'evy processes,\nextending earlier works which focused on the Black-Scholes and continuous\nstochastic volatility models. Using recent results from the theory of large\ndeviations on the path space for processes with independent increments, we\ncompute an explicit asymptotic approximation for the variance of the pay-off\nunder an Esscher-style change of measure. Minimizing this asymptotic variance\nusing convex duality, we then obtain an easy to compite asymptotically\nefficient importance sampling estimator of the option price. Numerical tests\nfor European baskets and for Asian options in the variance gamma model show\nconsistent variance reduction with a very small computational overhead.\n"
    },
    {
        "paper_id": 1608.04683,
        "authors": "Alessandro Gnoatto, Martino Grasselli, Eckhard Platen",
        "title": "A Penny Saved is a Penny Earned: Less Expensive Zero Coupon Bonds",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show how to hedge a zero coupon bond with a smaller amount\nof initial capital than required by the classical risk neutral paradigm, whose\n(trivial) hedging strategy does not suggest to invest in the risky assets. Long\ndated zero coupon bonds we derive, invest first primarily in risky securities\nand when approaching more and more the maturity date they increase also more\nand more the fraction invested in fixed income. The conventional wisdom of\nfinancial planners suggesting investor to invest in risky securities when they\nare young and mostly in fixed income when they approach retirement, is here\nmade rigorous. The paper provides a strong warning for life insurers, pension\nfund managers and long term investors to take the possibility of less expensive\nproducts seriously to avoid the adverse consequences of the low interest rate\nregimes that many developed economies face.\n"
    },
    {
        "paper_id": 1608.04832,
        "authors": "Victor M. Yakovenko",
        "title": "Monetary economics from econophysics perspective",
        "comments": "23 pages, 1 figure",
        "journal-ref": "Eur. Phys. J. Spec. Top. 225, 3313 (2016)",
        "doi": "10.1140/epjst/e2016-60213-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is an invited article for the Discussion and Debate special issue of The\nEuropean Physical Journal Special Topics on the subject \"Can Economics Be a\nPhysical Science?\" The first part of the paper traces the personal path of the\nauthor from theoretical physics to economics. It briefly summarizes\napplications of statistical physics to monetary transactions in an ensemble of\neconomic agents. It shows how a highly unequal probability distribution of\nmoney emerges due to irreversible increase of entropy in the system. The second\npart examines deep conceptual and controversial issues and fallacies in\nmonetary economics from econophysics perspective. These issues include the\nnature of money, conservation (or not) of money, distinctions between money vs.\nwealth and money vs. debt, creation of money by the state and debt by the\nbanks, the origins of monetary crises and capitalist profit. Presentation uses\nplain language understandable to laypeople and may be of interest to both\nspecialists and general public.\n"
    },
    {
        "paper_id": 1608.05002,
        "authors": "Drew Fudenberg, Kevin He, and Lorens Imhof",
        "title": "Bayesian Posteriors For Arbitrarily Rare Events",
        "comments": null,
        "journal-ref": "Proceedings of the National Academy of Sciences 114(19):4925-4929,\n  May 2017",
        "doi": "10.1073/pnas.1618780114",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how much data a Bayesian observer needs to correctly infer the\nrelative likelihoods of two events when both events are arbitrarily rare. Each\nperiod, either a blue die or a red die is tossed. The two dice land on side $1$\nwith unknown probabilities $p_1$ and $q_1$, which can be arbitrarily low. Given\na data-generating process where $p_1\\ge c q_1$, we are interested in how much\ndata is required to guarantee that with high probability the observer's\nBayesian posterior mean for $p_1$ exceeds $(1-\\delta)c$ times that for $q_1$.\nIf the prior densities for the two dice are positive on the interior of the\nparameter space and behave like power functions at the boundary, then for every\n$\\epsilon>0,$ there exists a finite $N$ so that the observer obtains such an\ninference after $n$ periods with probability at least $1-\\epsilon$ whenever\n$np_1\\ge N$. The condition on $n$ and $p_1$ is the best possible. The result\ncan fail if one of the prior densities converges to zero exponentially fast at\nthe boundary.\n"
    },
    {
        "paper_id": 1608.05024,
        "authors": "Gilles Boevi Koumou",
        "title": "Risk reduction and Diversification within Markowitz's Mean-Variance\n  Model: Theoretical Revisit",
        "comments": "16 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The conventional wisdom of mean-variance (MV) portfolio theory asserts that\nthe nature of the relationship between risk and diversification is a decreasing\nasymptotic function, with the asymptote approximating the level of portfolio\nsystematic risk or undiversifiable risk. This literature assumes that investors\nhold an equally-weighted or a MV portfolio and quantify portfolio\ndiversification using portfolio size. However, the equally-weighted portfolio\nand portfolio size are MV optimal if and only if asset returns distribution is\nexchangeable or investors have no useful information about asset expected\nreturn and risk. Moreover, the whole of literature, absolutely all of it,\nfocuses only on risky assets, ignoring the role of the risk free asset in the\nefficient diversification. Therefore, it becomes interesting and important to\nanswer this question: how valid is this conventional wisdom when investors have\nfull information about asset expected return and risk and asset returns\ndistribution is not exchangeable in both the case where the risk free rate is\navailable or not? Unfortunately, this question have never been addressed in the\ncurrent literature. This paper fills the gap.\n"
    },
    {
        "paper_id": 1608.05038,
        "authors": "Michael Y. Levy",
        "title": "Electoral Stability and Rigidity",
        "comments": "34 pages (body = 28, appendix = 6), 3 Figures, 3 tables, includes 2\n  apendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some argue that political stability is best served through a two-party\nsystem. This study refutes this. The author mathematically defines the\nstability and rigidity of electoral systems comprised of any quantity of\nelectors and parties. In fact, stability is a function of the quantity of\nelectors - i.e., the number of occupied seats at the table. As the number of\nelectors increases, the properties of an electorate are increasingly well\nresolved, and well described by those of an electorate that is least excessive\n-- that is to say an electorate that is closest to equilibrium. Further,\nelectoral rigidity is a function of the quantity of parties and their\nprobabilities of representation. An absolutely rigid system admits no\nfluctuations -- whatever happens to one elector will happen to all electors. As\nthe quantity of parties increases so does the number of party lines, and with\nit the quantity of alternatives with which to respond to an external stimulus.\nRigidity is significant in a social system that places high value on party\nloyalty. In conclusion, (i) electoral stability is best served by increasing\nthe quantity of electors; (ii) electoral rigidity is best served by decreasing\nthe quantity of parties, and by increasing the representation of some parties\nat the expense of others; and (iii) the less stable a branch of government, the\nmore concern is placed on those who would hold those offices for the people.\n"
    },
    {
        "paper_id": 1608.0506,
        "authors": "Anatoliy Swishchuk, Katharina Cera, Julia Schmidt, Tyler Hofmeister",
        "title": "General Semi-Markov Model for Limit Order Books: Theory, Implementation\n  and Numerics",
        "comments": "20 pages, 10 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper considers a general semi-Markov model for Limit Order Books with\ntwo states, which incorporates price changes that are not fixed to one tick.\nFurthermore, we introduce an even more general case of the semi-Markov model\nfor LimitOrder Books that incorporates an arbitrary number of states for the\nprice changes. For both cases the justifications, diffusion limits,\nimplementations and numerical results are presented for different Limit Order\nBook data: Apple, Amazon, Google, Microsoft, Intel on 2012/06/21 and Cisco,\nFacebook, Intel, Liberty Global, Liberty Interactive, Microsoft, Vodafone from\n2014/11/03 to 2014/11/07.\n"
    },
    {
        "paper_id": 1608.05145,
        "authors": "Andrey Itkin, Alexander Lipton",
        "title": "Filling the gaps smoothly",
        "comments": "37 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The calibration of a local volatility models to a given set of option prices\nis a classical problem of mathematical finance. It was considered in multiple\npapers where various solutions were proposed. In this paper an extension of the\napproach proposed in LiptonSepp2011 is developed by i) replacing a piecewise\nconstant local variance construction with a piecewise linear one, and ii)\nallowing non-zero interest rates and dividend yields. Our approach remains\nanalytically tractable; it combines the Laplace transform in time with an\nanalytical solution of the resulting spatial equations in terms of Kummer's\ndegenerate hypergeometric functions.\n"
    },
    {
        "paper_id": 1608.05378,
        "authors": "V. G. Filev, P. Neykov, G. S. Vasilev",
        "title": "A Semi-Analytic Approach To Valuing Auto-Callable Accrual Notes",
        "comments": "17 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a semi-analytic approach to the valuation of auto-callable\nstructures with accrual features subject to barrier conditions. Our approach is\nbased on recent studies of multi-assed binaries, present in the literature. We\nextend these studies to the case of time-dependent parameters. We compare\nnumerically the semi-analytic approach and the day to day Monte Carlo approach\nand conclude that the semi-analytic approach is more advantageous for high\nprecision valuation.\n"
    },
    {
        "paper_id": 1608.05498,
        "authors": "Natalia Nolde and Johanna F. Ziegel",
        "title": "Elicitability and backtesting: Perspectives for banking regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conditional forecasts of risk measures play an important role in internal\nrisk management of financial institutions as well as in regulatory capital\ncalculations. In order to assess forecasting performance of a risk measurement\nprocedure, risk measure forecasts are compared to the realized financial losses\nover a period of time and a statistical test of correctness of the procedure is\nconducted. This process is known as backtesting. Such traditional backtests are\nconcerned with assessing some optimality property of a set of risk measure\nestimates. However, they are not suited to compare different risk estimation\nprocedures. We investigate the proposal of comparative backtests, which are\nbetter suited for method comparisons on the basis of forecasting accuracy, but\nnecessitate an elicitable risk measure. We argue that supplementing traditional\nbacktests with comparative backtests will enhance the existing trading book\nregulatory framework for banks by providing the correct incentive for accuracy\nof risk measure forecasts. In addition, the comparative backtesting framework\ncould be used by banks internally as well as by researchers to guide selection\nof forecasting methods. The discussion focuses on three risk measures,\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\nsimulation study and data analysis.\n"
    },
    {
        "paper_id": 1608.05585,
        "authors": "Stefan Gerhold and I. Cetin G\\\"ul\\\"um",
        "title": "Consistency of option prices under bid-ask spreads",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a finite set of European call option prices on a single underlying, we\nwant to know when there is a market model which is consistent with these\nprices. In contrast to previous studies, we allow models where the underlying\ntrades at a bid-ask spread. The main question then is how large (in terms of a\ndeterministic bound) this spread must be to explain the given prices. We fully\nsolve this problem in the case of a single maturity, and give several partial\nresults for multiple maturities. For the latter, our main mathematical tool is\na recent result on approximation by peacocks [S. Gerhold, I.C. G\\\"ul\\\"uum,\narXiv:1512.06640].\n"
    },
    {
        "paper_id": 1608.05597,
        "authors": "Richard S.J. Tol",
        "title": "The structure of the climate debate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  First-best climate policy is a uniform carbon tax which gradually rises over\ntime. Civil servants have complicated climate policy to expand bureaucracies,\npoliticians to create rents. Environmentalists have exaggerated climate change\nto gain influence, other activists have joined the climate bandwagon. Opponents\nto climate policy have attacked the weaknesses in climate research. The climate\ndebate is convoluted and polarized as a result, and climate policy complex.\nClimate policy should become easier and more rational as the Paris Agreement\nhas shifted climate policy back towards national governments. Changing\npolitical priorities, austerity, and a maturing bureaucracy should lead to a\nmore constructive climate debate.\n"
    },
    {
        "paper_id": 1608.0565,
        "authors": "Amit K Chattopadhyay, T Krishna Kumar and Sushanta K Mallick",
        "title": "Poverty Index With Time Varying Consumption and Income Distributions",
        "comments": "10 (2 columned) pages, 8 figures",
        "journal-ref": "Phys. Rev. E 95, 032109 (2017)",
        "doi": "10.1103/PhysRevE.95.032109",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a recent work (Chattopadhyay, A. K. et al, Europhys. Lett. {\\bf 91},\n58003, 2010) based on food consumption statistics, we showed how a stochastic\nagent based model could represent the time variation of the income distribution\nstatistics in a developing economy, thereby defining an alternative\n\\enquote{poverty index} (PI) that largely agreed with poverty gap index data.\nThis PI used two variables, the probability density function of the income\nstatistics and a consumption deprivation (CD) function, representing the\nshortfall in the minimum consumption needed for survival. Since the time\ndependence of the CD function was introduced there through data extrapolation\nonly and not through an endogenous time dependent series, this model left\nunexplained how the minimum consumption needed for survival varies with time.\nThe present article overcomes these limitations and arrives at a new unified\ntheoretical structure through time varying consumption and income distributions\nwhere trade is only allowed when the income exceeds consumption deprivation\n(CD). Our results reveal that such CD-dynamics reduces the threshold level of\nconsumption of basic necessities, suggesting a possible dietary transition in\nterms of lower saturation level of food-grain consumption. The new poverty\nindex conforms to recently observed trends more closely than conventional\nmeasures of poverty and allows probabilistic prediction of PI for future times.\n"
    },
    {
        "paper_id": 1608.05814,
        "authors": "Zdzislaw Brzezniak and Tayfun Kok",
        "title": "Stochastic Evolution Equations in Banach Spaces and Applications to\n  Heath-Jarrow-Morton-Musiela Equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the stochastic evolution equation (1.1) in\nmartingale-type 2 Banach spaces (with the linear part of the drift being only a\ngenerator of a C0-semigroup). We prove the existence and the uniqueness of\nsolutions to this equation. We apply the abstract results to the\nHeath-Jarrow-Morton-Musiela (HJMM) equation (6.3). In particular, we prove the\nexistence and the uniqueness of solutions to the latter equation in the\nweighted Lebesgue and Sobolev spaces respectively. We also find a sufficient\ncondition for the existence and the uniqueness of an invariant measure for the\nMarkov semigroup associated to equation (6.3) in the weighted Lebesgue spaces.\n"
    },
    {
        "paper_id": 1608.05851,
        "authors": "Bruce M. Boghosian and Adrian Devitt-Lee and Hongyan Wang",
        "title": "The Growth of Oligarchy in a Yard-Sale Model of Asset Exchange: A\n  Logistic Equation for Wealth Condensation",
        "comments": "15 pages, 2 figures",
        "journal-ref": "Proceedings of the 1st International Conference on Complex\n  Information Systems, ISBN 978-989-758-181-6, pages 187-193 (2016)",
        "doi": "10.5220/0005956501870193",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The addition of wealth-attained advantage (WAA) to the Yard-Sale Model (YSM)\nof asset exchange has been demonstrated to induce wealth condensation. In a\nmodel of WAA for which the bias is a continuous function of the wealth\ndifference of the transacting agents, the condensation was shown to arise from\na second-order phase transition to a coexistence regime. In this paper, we\npresent the first analytic time-dependent results for this model, by showing\nthat the condensed wealth obeys a logistic equation in time.\n"
    },
    {
        "paper_id": 1608.059,
        "authors": "Sergey Lototsky and Henry Schellhorn and Ran Zhao",
        "title": "A String Model of Liquidity in Financial Markets",
        "comments": "arXiv admin note: text overlap with arXiv:1206.4804",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a dynamic market model of liquidity where unmatched buy and sell\nlimit orders are stored in order books. The resulting net demand surface\nconstitutes the sole input to the model. We prove that generically there is no\narbitrage in the model when the driving noise is a stochastic string. Under the\nequivalent martingale measure, the clearing price is a martingale, and options\ncan be priced under the no-arbitrage hypothesis. We consider several\nparameterized versions of the model, and show some advantages of specifying the\ndemand curve as quantity as a function of price (as opposed to price as a\nfunction of quantity). We calibrate our model to real order book data, compute\noption prices by Monte Carlo simulation, and compare the results to observed\ndata.\n"
    },
    {
        "paper_id": 1608.06045,
        "authors": "Yuki Shigeta",
        "title": "Optimal Switching under Ambiguity and Its Applications in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study optimal switching problems under ambiguity. To\ncharacterize the optimal switching under ambiguity in the finite horizon, we\nuse multidimensional reflected backward stochastic differential equations\n(multidimensional RBSDEs) and show that a value function of the optimal\nswitching under ambiguity coincides with a solutions to multidimensional RBSDEs\nwith allowing negative switching costs. Furthermore, we naturally extend the\nfinite horizon problem to the infinite horizon problem. In some applications,\nwe show that ambiguity affects an optimal switching strategy with the different\nway to a usual switching problem without ambiguity.\n"
    },
    {
        "paper_id": 1608.06076,
        "authors": "F. Clementi, M. Gallegati",
        "title": "New economic windows on income and wealth: The k-generalized family of\n  distributions",
        "comments": "LaTeX2e; 14 pages with 3 figures",
        "journal-ref": "Journal of Social and Economic Statistics, Vol: 6, Issue: 1,\n  Summer 2017, pp: 1-15",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the last decades, the distribution of income and wealth has been\ndeteriorating in many countries, leading to increased inequalities within and\nbetween societies. This tendency has revived the interest in the subject\ngreatly, yet it still receives very little attention within the realm of\nmainstream economic thinking. One reason for this is that the basic paradigm of\n\"standard economics\", the representative-agent General Equilibrium framework,\nis badly equipped to cope with distributional issues. Here we argue that when\nthe economy is treated as a complex system composed of many heterogeneous\ninteracting agents who give rise to emergent phenomena, to address the main\nstylized facts of income/wealth distribution requires leaving the toolbox of\nmainstream economics in favour of alternative approaches. The \"k-generalized\"\nfamily of income/wealth distributions, building on the categories of\ncomplexity, is an example of how advances in the field can be achieved within\nnew interdisciplinary research contexts.\n"
    },
    {
        "paper_id": 1608.06121,
        "authors": "E. Robert Fernholz, Ioannis Karatzas, Johannes Ruf",
        "title": "Volatility and Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The capitalization-weighted total relative variation $\\sum_{i=1}^d\n\\int_0^\\cdot \\mu_i (t) \\mathrm{d} \\langle \\log \\mu_i \\rangle (t)$ in an equity\nmarket consisting of a fixed number $d$ of assets with capitalization weights\n$\\mu_i (\\cdot)$ is an observable and nondecreasing function of time. If this\nobservable of the market is not just nondecreasing, but actually grows at a\nrate which is bounded away from zero, then strong arbitrage can be constructed\nrelative to the market over sufficiently long time horizons. It has been an\nopen issue for more than ten years, whether such strong outperformance of the\nmarket is possible also over arbitrary time horizons under the stated\ncondition. We show that this is not possible in general, thus settling this\nlong-open question. We also show that, under appropriate additional conditions,\noutperformance over any time horizon indeed becomes possible, and exhibit\ninvestment strategies that effect it.\n"
    },
    {
        "paper_id": 1608.06376,
        "authors": "Dorje C. Brody, Lane P. Hughston, and David M. Meier",
        "title": "L\\'evy-Vasicek Models and the Long-Bond Return Process",
        "comments": "23 pages",
        "journal-ref": "International Journal of Theoretical and Applied Finance, Vol. 21,\n  1850026 (2018)",
        "doi": "10.1142/S0219024918500267",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical derivation of the well-known Vasicek model for interest rates\nis reformulated in terms of the associated pricing kernel. An advantage of the\npricing kernel method is that it allows one to generalize the construction to\nthe L\\'evy-Vasicek case, avoiding issues of market incompleteness. In the\nL\\'evy-Vasicek model the short rate is taken in the real-world measure to be a\nmean-reverting process with a general one-dimensional L\\'evy driver admitting\nexponential moments. Expressions are obtained for the L\\'evy-Vasicek bond\nprices and interest rates, along with a formula for the return on a unit\ninvestment in the long bond, defined by $L_t = \\lim_{T \\rightarrow \\infty}\nP_{tT} / P_{0T}$, where $P_{tT}$ is the price at time $t$ of a $T$-maturity\ndiscount bond. We show that the pricing kernel of a L\\'evy-Vasicek model is\nuniformly integrable if and only if the long rate of interest is strictly\npositive.\n"
    },
    {
        "paper_id": 1608.06416,
        "authors": "Elnura Irmatova",
        "title": "RELARM: A rating model based on relative PCA attributes and k-means\n  clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following widely used in visual recognition concept of relative attributes,\nthe article establishes definition of the relative PCA attributes for a class\nof objects defined by vectors of their parameters. A new rating model (RELARM)\nis built using relative PCA attribute ranking functions for rating object\ndescription and k-means clustering algorithm. Rating assignment of each rating\nobject to a rating category is derived as a result of cluster centers\nprojection on the specially selected rating vector. Empirical study has shown a\nhigh level of approximation to the existing S & P, Moody's and Fitch ratings.\n"
    },
    {
        "paper_id": 1608.06781,
        "authors": "Ladislav Kristoufek",
        "title": "Fractal approach towards power-law coherency to measure\n  cross-correlations between time series",
        "comments": "16 pages, 3 tables, 1 figure",
        "journal-ref": "Communications in Nonlinear Science and Numerical Simulation,\n  Volume 50, September 2017, Pages 193-200",
        "doi": "10.1016/j.cnsns.2017.02.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on power-law coherency as an alternative approach towards studying\npower-law cross-correlations between simultaneously recorded time series. To be\nable to study empirical data, we introduce three estimators of the power-law\ncoherency parameter $H_{\\rho}$ based on popular techniques usually utilized for\nstudying power-law cross-correlations -- detrended cross-correlation analysis\n(DCCA), detrending moving-average cross-correlation analysis (DMCA) and height\ncross-correlation analysis (HXA). In the finite sample properties study, we\nfocus on the bias, variance and mean squared error of the estimators. We find\nthat the DMCA-based method is the safest choice among the three. The HXA method\nis reasonable for long time series with at least $10^4$ observations, which can\nbe easily attainable in some disciplines but problematic in others. The\nDCCA-based method does not provide favorable properties which even deteriorate\nwith an increasing time series length. The paper opens a new venue towards\nstudying cross-correlations between time series.\n"
    },
    {
        "paper_id": 1608.06959,
        "authors": "Luis Alcala, Fernando Tohme, Carlos Dabus",
        "title": "Strategic Growth with Recursive Preferences: Decreasing Marginal\n  Impatience",
        "comments": "55 pages, 14 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the interaction between strategy, heterogeneity and growth in a\ntwo-agent model of capital accumulation. Preferences are represented by\nrecursive utility functions with decreasing marginal impatience. The stationary\nequilibria of this dynamic game are analyzed under two alternative information\nstructures: one in which agents precommit to future actions, and another one\nwhere agents use Markovian strategies. In both cases, we develop sufficient\nconditions to prove the existence of equilibria and characterize their\nstability properties. The precommitment case is characterized by monotone\nconvergence, but Markovian equilibria may exhibit nonmonotonic paths, even in\nthe long-run.\n"
    },
    {
        "paper_id": 1608.07158,
        "authors": "Antoine Jacquier and Fangwei Shi",
        "title": "The randomised Heston model",
        "comments": "35 pages, 25 figures. Link with model uncertainty added, Some\n  higher-order terms as well",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a randomised version of the Heston model-a widely used stochastic\nvolatility model in mathematical finance-assuming that the starting point of\nthe variance process is a random variable. In such a system, we study the\nsmall-and large-time behaviours of the implied volatility, and show that the\nproposed randomisation generates a short-maturity smile much steeper (`with\nexplosion') than in the standard Heston model, thereby palliating the\ndeficiency of classical stochastic volatility models in short time. We\nprecisely quantify the speed of explosion of the smile for short maturities in\nterms of the right tail of the initial distribution, and in particular show\nthat an explosion rate of~$t^\\gamma$ ($\\gamma\\in[0,1/2]$) for the squared\nimplied volatility--as observed on market data--can be obtained by a suitable\nchoice of randomisation. The proofs are based on large deviations techniques\nand the theory of regular variations.\n"
    },
    {
        "paper_id": 1608.07193,
        "authors": "Heejoon Han",
        "title": "Quantile Dependence between Stock Markets and its Application in\n  Volatility Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines quantile dependence between international stock markets\nand evaluates its use for improving volatility forecasting. First, we analyze\nquantile dependence and directional predictability between the US stock market\nand stock markets in the UK, Germany, France and Japan. We use the\ncross-quantilogram, which is a correlation statistic of quantile hit processes.\nThe detailed dependence between stock markets depends on specific quantile\nranges and this dependence is generally asymmetric; the negative spillover\neffect is stronger than the positive spillover effect and there exists strong\ndirectional predictability from the US market to the UK, Germany, France and\nJapan markets. Second, we consider a simple quantile-augmented volatility model\nthat accommodates the quantile dependence and directional predictability\nbetween the US market and these other markets. The quantile-augmented\nvolatility model provides superior in-sample and out-of-sample volatility\nforecasts.\n"
    },
    {
        "paper_id": 1608.07226,
        "authors": "Claudia Ceci, Katia Colaneri, Alessandra Cretarola",
        "title": "Unit-linked life insurance policies: optimal hedging in partially\n  observable market models",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2017.07.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the hedging problem of a unit-linked life\ninsurance contract via the local risk-minimization approach, when the insurer\nhas a restricted information on the market. In particular, we consider an\nendowment insurance contract, that is a combination of a term insurance policy\nand a pure endowment, whose final value depends on the trend of a stock market\nwhere the premia the policyholder pays are invested. We assume that the stock\nprice process dynamics depends on an exogenous unobservable stochastic factor\nthat also influences the mortality rate of the policyholder. To allow for\nmutual dependence between the financial and the insurance markets, we use the\nprogressive enlargement of filtration approach. We characterize the optimal\nhedging strategy in terms of the integrand in the Galtchouk-Kunita-Watanabe\ndecomposition of the insurance claim with respect to the minimal martingale\nmeasure and the available information flow. We provide an explicit formula by\nmeans of predictable projection of the corresponding hedging strategy under\nfull information with respect to the natural filtration of the risky asset\nprice and the minimal martingale measure. Finally, we discuss applications in a\nMarkovian setting via filtering.\n"
    },
    {
        "paper_id": 1608.07694,
        "authors": "Mansooreh Kazemilari, Maman Abdurachman Djauhari and Zuhaimy Ismail",
        "title": "Foreign Exchange Market Performance: Evidence from Bivariate Time Series\n  Approach",
        "comments": "13 pages, 1 figure and 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are many studies dealing with the analysis of similarity among\ncurrencies in foreign exchange market by using network analysis approach. In\nthose studies, each currency is represented by a univariate time series of\nexchange rate return. This is the standard practice to analyze the underlying\ninformation in the foreign exchange market. In this paper, Escoufier's RV\ncoefficient is applied to measure the similarity among currencies where each of\nthem is represented by bivariate time series. Based on that coefficient, we\nanalyze the topological structure of the currencies. An example of FOREX\nanalysis will be presented and discussed to illustrate the advantages of RV\ncoefficient.\n"
    },
    {
        "paper_id": 1608.07752,
        "authors": "Sandhya Devi",
        "title": "Financial Market Dynamics: Superdiffusive or not?",
        "comments": "The paper has gone through considerable revisions after referees'\n  comments. The major changes are, The old introduction has been split into\n  Section 1: Introduction and Section 2: A brief review of the Theory. The\n  Results section now has 4 subsections. Section 5 (Conclusions) is reworded\n  but the conclusions remain the same. Some additional references have been\n  added. 29 pages. 11 Figures",
        "journal-ref": "J. Stat. Mech. (2017) 083207",
        "doi": "10.1088/1742-5468/aa8199",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The behavior of stock market returns over a period of 1-60 days has been\ninvestigated for S&P 500 and Nasdaq within the framework of nonextensive\nTsallis statistics. Even for such long terms, the distributions of the returns\nare non-Gaussian. They have fat tails indicating that the stock returns do not\nfollow a random walk model. In this work, a good fit to a Tsallis q-Gaussian\ndistribution is obtained for the distributions of all the returns using the\nmethod of Maximum Likelihood Estimate. For all the regions of data considered,\nthe values of the scaling parameter q, estimated from one day returns, lie in\nthe range 1.4 to 1.65. The estimated inverse mean square deviations (beta) show\na power law behavior in time with exponent values between -0.91 and -1.1\nindicating normal to mildly subdiffusive behavior. Quite often, the dynamics of\nmarket return distributions is modelled by a Fokker-Plank (FP) equation either\nwith a linear drift and a nonlinear diffusion term or with just a nonlinear\ndiffusion term. Both of these cases support a q-Gaussian distribution as a\nsolution. The distributions obtained from current estimated parameters are\ncompared with the solutions of the FP equations. For negligible drift term, the\ninverse mean square deviations (betaFP) from the FP model follow a power law\nwith exponent values between -1.25 and -1.48 indicating superdiffusion. When\nthe drift term is non-negligible, the corresponding betaFP do not follow a\npower law and become stationary after certain characteristic times that depend\non the values of the drift parameter and q. Neither of these behaviors is\nsupported by the results of the empirical fit.\n"
    },
    {
        "paper_id": 1608.07796,
        "authors": "Neeraj and Prasanta K. Panigrahi",
        "title": "Causality and Correlations between BSE and NYSE indexes: A Janus Faced\n  Relationship",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the multi-scale temporal correlations and causality connections\nbetween the New York Stock Exchange (NYSE) and Bombay Stock Exchange (BSE)\nmonthly average closing price indexes for a period of 300 months, encompassing\nthe time period of the liberalisation of the Indian economy and its gradual\nglobal exposure. In multi-scale analysis; clearly identifiable 1, 2 and 3 year\nnon-stationary periodic modulations in NYSE and BSE have been observed, with\nNYSE commensurating changes in BSE at 3 years scale. Interestingly, at one year\ntime scale, the two exchanges are phase locked only during the turbulent times,\nwhile at the scale of three year, in-phase nature is observed for a much longer\ntime frame. The two year time period, having characteristics of both one and\nthree year variations, acts as the transition regime. The normalised NYSE's\nstock value is found to Granger cause those of BSE, with a time lag of 9\nmonths. Surprisingly, observed Granger causality of high frequency variations\nreveals BSE behaviour getting reflected in the NYSE index fluctuations, after a\nsmaller time lag. This Janus faced relationship, shows that smaller stock\nexchanges may provide a natural setting for simulating market fluctuations of\nmuch bigger exchanges. This possibly arises due to the fact that high frequency\nfluctuations form an universal part of the financial time series, and are\nexpected to exhibit similar characteristics in open market economies.\n"
    },
    {
        "paper_id": 1608.07831,
        "authors": "Gabriele Visentin, Stefano Battiston, Marco D'Errico",
        "title": "Rethinking Financial Contagion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How, and to what extent, does an interconnected financial system endogenously\namplify external shocks? This paper attempts to reconcile some apparently\ndifferent views emerged after the 2008 crisis regarding the nature and the\nrelevance of contagion in financial networks. We develop a common framework\nencompassing several network contagion models and show that, regardless of the\nshock distribution and the network topology, precise ordering relationships on\nthe level of aggregate systemic losses hold among models. We argue that the\nextent of contagion crucially depends on the amount of information that each\nmodel assumes to be available to market players. Under no uncertainty about the\nnetwork structure and values of external assets, the well-known Eisenberg and\nNoe (2001) model applies, which delivers the lowest level of contagion. This is\ndue to a property of loss conservation: aggregate losses after contagion are\nequal to the losses incurred by those institutions initially hit by a shock.\nThis property implies that many contagion analyses rule out by construction any\nloss amplification, treating de facto an interconnected system as a single\naggregate entity, where losses are simply mutualised. Under higher levels of\nuncertainty, as captured for instance by the DebtRank model, losses become\nnon-conservative and get compounded through the network. This has important\npolicy implications: by reducing the levels of uncertainty in times of distress\n(e.g. by obtaining specific data on the network) policymakers would be able to\nmove towards more conservative scenarios. Empirically, we compare the magnitude\nof contagion across models on a sample of the largest European banks during the\nyears 2006-2016. In particular, we analyse contagion effects as a function of\nthe size of the shock and the type of external assets shocked.\n"
    },
    {
        "paper_id": 1608.07863,
        "authors": "Jos\\'e E. Figueroa-L\\'opez, Ruoting Gong, Matthew Lorig",
        "title": "Short-Time Expansions for Call Options on Leveraged ETFs Under\n  Exponential L\\'evy models With Local Volatility",
        "comments": "29 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we consider the small-time asymptotics of options on a\n\\emph{Leveraged Exchange-Traded Fund} (LETF) when the underlying Exchange\nTraded Fund (ETF) exhibits both local volatility and jumps of either finite or\ninfinite activity. Our main results are closed-form expressions for the leading\norder terms of off-the-money European call and put LETF option prices, near\nexpiration, with explicit error bounds. We show that the price of an\nout-of-the-money European call on a LETF with positive (negative) leverage is\nasymptotically equivalent, in short-time, to the price of an out-of-the-money\nEuropean call (put) on the underlying ETF, but with modified spot and strike\nprices. Similar relationships hold for other off-the-money European options. In\nparticular, our results suggest a method to hedge off-the-money LETF options\nnear expiration using options on the underlying ETF. Finally, a second order\nexpansion for the corresponding implied volatility is also derived and\nillustrated numerically.\n"
    },
    {
        "paper_id": 1608.07901,
        "authors": "Matthew O. Jackson and Brian W. Rogers and Yves Zenou",
        "title": "Networks: An Economic Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss social network analysis from the perspective of economics. We\norganize the presentaion around the theme of externalities: the effects that\none's behavior has on others' well-being. Externalities underlie the\ninterdependencies that make networks interesting. We discuss network formation,\nas well as interactions between peoples' behaviors within a given network, and\nthe implications in a variety of settings. Finally, we highlight some empirical\nchallenges inherent in the statistical analysis of network-based data.\n"
    },
    {
        "paper_id": 1608.0821,
        "authors": "Deepak Malghan and Hema Swaminathan",
        "title": "What is the Contribution of Intra-household Inequality to Overall Income\n  Inequality? Evidence from Global Data, 1973-2013",
        "comments": "Submitted to LIS Papers",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intra-household inequality continues to remain a neglected corner despite\nrenewed focus on income and wealth inequality. Using the LIS micro data, we\npresent evidence that this neglect is equivalent to ignoring up to a third of\ntotal inequality. For a wide range of countries and over four decades, we show\nthat at least 30 per cent of total inequality is attributable to inequality\nwithin the household. Using a simple normative measure of inequality, we\ncomment on the welfare implications of these trends.\n"
    },
    {
        "paper_id": 1608.08268,
        "authors": "Bahman Angoshtari",
        "title": "On the Market-Neutrality of Optimal Pairs-Trading Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal investment in a market with two\ncointegrated stocks and an agent with CRRA utility. We extend the findings of\nLiu and Timmermann [The Review of Financial Studies, 26(4):1048-1086, 2013] by\npaying special attention to when/if the associated stochastic control problem\nis well-posed and providing a verification result. Our new findings lead to a\nsharp well-posedness condition which is, surprisingly, also the necessary and\nsufficient condition for the optimal investment to be market-neutral (i.e.\nhaving offsetting long/short positions in the stocks). Hence, we provide a\ntheoretical justification for market-neutral pairs-trading which, despite\nhaving a strong practical relevance, has been lacking a theoretical ground.\n"
    },
    {
        "paper_id": 1608.08283,
        "authors": "Giuseppe Carlo Calafiore, Leonardo Massai",
        "title": "Risk measures and Margining control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This document constitutes the final report of the contractual activity\nbetween Directa SIM and Dipartimento di Automatica e Informatica, Politecnico\ndi Torino, on the research topic titled \"quantificazione del rischio di un\nportafoglio di strumenti finanziari per trading online su device fissi e\nmobili.\"\n"
    },
    {
        "paper_id": 1608.08468,
        "authors": "Gregor Kastner",
        "title": "Sparse Bayesian time-varying covariance estimation in many dimensions",
        "comments": null,
        "journal-ref": "Journal of Econometrics 210(1), 98-115 (2019)",
        "doi": "10.1016/j.jeconom.2018.11.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks.\n"
    },
    {
        "paper_id": 1608.0849,
        "authors": "Liurui Deng and Traian A. Pirvu",
        "title": "Multi-period investment strategies under Cumulative Prospect Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, inspired by Shi, et al. we investigate the optimal portfolio\nselection with one risk-free asset and one risky asset in a multiple period\nsetting under cumulative prospect theory (CPT). Compared with their study, our\nnovelty is that we consider a stochastic benchmark, and portfolio constraints.\nWe test the sensitivity of the optimal CPT-investment strategies to different\nmodel parameters by performing a numerical analysis.\n"
    },
    {
        "paper_id": 1608.08582,
        "authors": "Benjamin Vandermarliere, Jan Ryckebusch, Koen Schoors, Peter Cauwels,\n  Didier Sornette",
        "title": "Discrete hierarchy of sizes and performances in the exchange-traded fund\n  universe",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.11.084",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using detailed statistical analyses of the size distribution of a universe of\nequity exchange-traded funds (ETFs), we discover a discrete hierarchy of sizes,\nwhich imprints a log-periodic structure on the probability distribution of ETF\nsizes that dominates the details of the asymptotic tail. This allows us to\npropose a classification of the studied universe of ETFs into seven size layers\napproximately organized according to a multiplicative ratio of 3.5 in their\ntotal market capitalization. Introducing a similarity metric generalising the\nHerfindhal index, we find that the largest ETFs exhibit a significantly\nstronger intra-layer and inter-layer similarity compared with the smaller ETFs.\nComparing the performance across the seven discerned ETF size layers, we find\nan inverse size effect, namely large ETFs perform significantly better than the\nsmall ones both in 2014 and 2015.\n"
    },
    {
        "paper_id": 1609.00232,
        "authors": "Maarten Wyns and Karel in 't Hout",
        "title": "An adjoint method for the exact calibration of Stochastic Local\n  Volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the exact calibration of semidiscretized stochastic\nlocal volatility (SLV) models to their underlying semidiscretized local\nvolatility (LV) models. Under an SLV model, it is common to approximate the\nfair value of European-style options by semidiscretizing the backward\nKolmogorov equation using finite differences. In the present paper we introduce\nan adjoint semidiscretization of the corresponding forward Kolmogorov equation.\nThis adjoint semidiscretization is used to obtain an expression for the\nleverage function in the pertinent SLV model such that the approximated fair\nvalues defined by the LV and SLV models are identical for non-path-dependent\nEuropean-style options. In order to employ this expression, a large non-linear\nsystem of ODEs needs to be solved. The actual numerical calibration is\nperformed by combining ADI time stepping with an inner iteration to handle the\nnon-linearity. Ample numerical experiments are presented that illustrate the\neffectiveness of the calibration procedure.\n"
    },
    {
        "paper_id": 1609.00415,
        "authors": "Atif Ansar, Bent Flyvbjerg, Alexander Budzier, and Daniel Lunn",
        "title": "Does Infrastructure Investment Lead to Economic Growth or Economic\n  Fragility? Evidence from China",
        "comments": null,
        "journal-ref": "Oxford Review of Economic Policy, vol 32, 2016",
        "doi": "10.1093/oxrep/grw022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The prevalent view in the economics literature is that a high level of\ninfrastructure investment is a precursor to economic growth. China is\nespecially held up as a model to emulate. Based on the largest dataset of its\nkind, this paper punctures the twin myths that, first, infrastructure creates\neconomic value, and, second, China has a distinct advantage in its delivery.\nFar from being an engine of economic growth, the typical infrastructure\ninvestment fails to deliver a positive risk adjusted return. Moreover, China's\ntrack record in delivering infrastructure is no better than that of rich\ndemocracies. Where investments are debt-financed, overinvesting in unproductive\nprojects results in the buildup of debt, monetary expansion, instability in\nfinancial markets, and economic fragility, exactly as we see in China today. We\nconclude that poorly managed infrastructure investments are a main explanation\nof surfacing economic and financial problems in China. We predict that, unless\nChina shifts to a lower level of higher-quality infrastructure investments, the\ncountry is headed for an infrastructure-led national financial and economic\ncrisis, which is likely also to be a crisis for the international economy.\nChina's infrastructure investment model is not one to follow for other\ncountries but one to avoid.\n"
    },
    {
        "paper_id": 1609.00554,
        "authors": "Wioletta Szeligowska, Marek Kaluszka",
        "title": "On Jensen's inequality for generalized Choquet integral with an\n  application to risk aversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper we give necessary and sufficient conditions for the Jensen\ninequality to hold for the generalized Choquet integral with respect to a pair\nof capacities. Next, we apply obtained result to the theory of risk aversion by\nproviding the assumptions on utility function and capacities under which an\nagent is risk averse. Moreover, we show that the Arrow-Pratt theorem can be\ngeneralized to cover the case, where the expectation is replaced by the\ngeneralized Choquet integral.\n"
    },
    {
        "paper_id": 1609.00599,
        "authors": "Elias Strehle",
        "title": "Optimal Execution in a Multiplayer Model of Transient Price Impact",
        "comments": "17 pages, 5 figures",
        "journal-ref": "Market Microstructure and Liquidity, 3(03n04), p.1850007",
        "doi": "10.1142/S2382626618500077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading algorithms that execute large orders are susceptible to exploitation\nby order anticipation strategies. This paper studies the influence of order\nanticipation strategies in a multi-investor model of optimal execution under\ntransient price impact. Existence and uniqueness of a Nash equilibrium is\nestablished under the assumption that trading incurs quadratic transaction\ncosts. A closed-form representation of the Nash equilibrium is derived for\nexponential decay kernels. With this representation, it is shown that while\norder anticipation strategies raise the execution costs of a large order\nsignificantly, they typically do not cause price overshooting in the sense of\nBrunnermeier and Pedersen.\n"
    },
    {
        "paper_id": 1609.00702,
        "authors": "Mourad Lazgham",
        "title": "Numerical solution of a semilinear parabolic degenerate\n  Hamilton-Jacobi-Bellman equation with singularity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a semilinear parabolic degenerated Hamilton-Jacobi-Bellman (HJB)\nequation with singularity which is related to a stochastic control problem with\nfuel constraint. The fuel constraint translates into a singular initial\ncondition for the HJB equation. We first propose a transformation based on a\nchange of variables that gives rise to an equivalent HJB equation with\nnonsingular initial condition but irregular coefficients. We then construct\nexplicit and implicit numerical schemes for solving the transformed HJB\nequation and prove their convergences by establishing an extension to the\nresult of Barles and Souganidis (1991).\n"
    },
    {
        "paper_id": 1609.00819,
        "authors": "Chris Kenyon and Andrew Green",
        "title": "Option-Based Pricing of Wrong Way Risk for CVA",
        "comments": "Significant errors existed in this paper. I have a different approach\n  in a more recent paper that is simpler and better",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The two main issues for managing wrong way risk (WWR) for the credit\nvaluation adjustment (CVA, i.e. WW-CVA) are calibration and hedging. Hence we\nstart from a novel model-free worst-case approach based on static hedging of\ncounterparty exposure with liquid options. We say \"start from\" because we\ndemonstrate that a naive worst-case approach contains hidden unrealistic\nassumptions on the variance of the hazard rate (i.e. that it is infinite). We\ncorrect this by making it an explicit (finite) parameter and present an\nefficient method for solving the parametrized model optimizing the hedges. We\nalso prove that WW-CVA is theoretically, but not practically, unbounded. The\noption-based hedges serve to significantly reduce (typically halve) practical\nWW-CVA. Thus we propose a realistic and practical option-based worst case CVA.\n"
    },
    {
        "paper_id": 1609.00869,
        "authors": "Antoine Emil Zambelli",
        "title": "Determining Optimal Stop-Loss Thresholds via Bayesian Analysis of\n  Drawdown Distributions",
        "comments": "6 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stop-loss rules are often studied in the financial literature, but the\nstop-loss levels are seldom constructed systematically. In many papers, and\nindeed in practice as well, the level of the stops is too often set\narbitrarily. Guided by the overarching goal in finance to maximize expected\nreturns given available information, we propose a natural method by which to\nsystematically select the stop-loss threshold by analyzing the distribution of\nmaximum drawdowns. We present results for an hourly trading strategy with two\nvariations on the construction.\n"
    },
    {
        "paper_id": 1609.00926,
        "authors": "Asmerilda Hitaj, Friedrich Hubalek, Lorenzo Mercuri and Edit Rroji",
        "title": "Multivariate Mixed Tempered Stable Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multivariate version of the Mixed Tempered Stable is proposed. It is a\ngeneralization of the Normal Variance Mean Mixtures. Characteristics of this\nnew distribution and its capacity in fitting tails and capturing dependence\nstructure between components are investigated. We discuss a random number\ngenerating procedure and introduce an estimation methodology based on the\nminimization of a distance between empirical and theoretical characteristic\nfunctions. Asymptotic tail behavior of the univariate Mixed Tempered Stable is\nexploited in the estimation procedure in order to obtain a better model\nfitting. Advantages of the multivariate Mixed Tempered Stable distribution are\ndiscussed and illustrated via simulation study.\n"
    },
    {
        "paper_id": 1609.00987,
        "authors": "Jean-Philippe Aguilar, Cyril Coste, Jan Korbel",
        "title": "Non-Gaussian analytic option pricing: a closed formula for the\n  L\\'evy-stable model",
        "comments": "v2-2 (nov 17): theoretical details and other numerical tests added",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2828673",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish an explicit pricing formula for the class of L\\'evy-stable\nmodels with maximal negative asymmetry (Log-L\\'evy model with finite moments\nand stability parameter $1<\\alpha\\leq 2$) in the form of rapidly converging\nseries. The series is obtained with help of Mellin transform and the residue\ntheory in $\\mathbb{C}^2$. The resulting formula enables the straightforward\nevaluation of an European option with arbitrary accuracy without the use of\nnumerical techniques. The formula can be used by any practitioner, even if not\nfamiliar with the underlying mathematical techniques. We test the efficiency of\nthe formula, and compare it with numerical methods.\n"
    },
    {
        "paper_id": 1609.01274,
        "authors": "Ravi Kashyap",
        "title": "Options as Silver Bullets: Valuation of Term Loans, Inventory\n  Management, Emissions Trading and Insurance Risk Mitigation using Option\n  Theory",
        "comments": null,
        "journal-ref": "Annals of Operations Research, (2022), S.I.: Business Analytics\n  and Operations Research, 001-041",
        "doi": "10.1007/s10479-022-04610-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models to price long term loans in the securities lending business are\ndeveloped. These longer horizon deals can be viewed as contracts with\noptionality embedded in them. This insight leads to the usage of established\nmethods from derivatives theory to price such contracts. Numerical simulations\nare used to demonstrate the practical applicability of these models. The\ntechniques advanced here can lead to greater synergies between the management\nof derivative and delta-one trading desks, perhaps even being able to combine\ncertain aspects of the day to day operations of these seemingly disparate\nentities. These models are part of one of the least explored, yet profit laden,\nareas of modern investment management.\n  A heuristic is developed to mitigate any loss of information, which might set\nin when parameters are estimated first and then the valuations are performed,\nby directly calculating valuations using the historical time series. This\napproach to valuations can lead to reduced models errors, robust estimation\nsystems, greater financial stability and economic strength. An illustration is\nprovided regarding how the methodologies developed here could be useful for\ninventory management, emissions trading and insurance risk mitigation. All\nthese techniques could have applications for dealing with other financial\ninstruments, non-financial commodities and many forms of uncertainty.\n"
    },
    {
        "paper_id": 1609.01621,
        "authors": "David Criens",
        "title": "Deterministic Criteria for the Absence and Existence of Arbitrage in\n  Multi-Dimensional Diffusion Markets",
        "comments": "Forthcoming in \"International Journal of Theoretical and Applied\n  Finance\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive deterministic criteria for the existence and non-existence of\nequivalent (local) martingale measures for financial markets driven by\nmulti-dimensional time-inhomogeneous diffusions. Our conditions can be used to\nconstruct financial markets in which the \\emph{no unbounded profit with bounded\nrisk} condition holds, while the classical \\emph{no free lunch with vanishing\nrisk} condition fails.\n"
    },
    {
        "paper_id": 1609.01655,
        "authors": "Tiziano De Angelis and Erik Ekstr\\\"om",
        "title": "The dividend problem with a finite horizon",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterise the value function of the optimal dividend problem with a\nfinite time horizon as the unique classical solution of a suitable\nHamilton-Jacobi-Bellman equation. The optimal dividend strategy is realised by\na Skorokhod reflection of the fund's value at a time-dependent optimal\nboundary. Our results are obtained by establishing for the first time a new\nconnection between singular control problems with an absorbing boundary and\noptimal stopping problems on a diffusion reflected at $0$ and created at a rate\nproportional to its local time.\n"
    },
    {
        "paper_id": 1609.019,
        "authors": "Claudiu Albulescu (CRIEF), Dominique P\\'epin (CRIEF)",
        "title": "The loss of interest for the euro in Romania",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize a money demand micro-founded model to explain Romanians' recent\nloss of interest for the euro. We show that the reason behind this loss of\ninterest is a severe decline in the relative degree of the euro liquidity\nagainst that of the Romanian leu.\n"
    },
    {
        "paper_id": 1609.02108,
        "authors": "Omar El Euch and Mathieu Rosenbaum",
        "title": "The characteristic function of rough Heston models",
        "comments": "35 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been recently shown that rough volatility models, where the volatility\nis driven by a fractional Brownian motion with small Hurst parameter, provide\nvery relevant dynamics in order to reproduce the behavior of both historical\nand implied volatilities. However, due to the non-Markovian nature of the\nfractional Brownian motion, they raise new issues when it comes to derivatives\npricing. Using an original link between nearly unstable Hawkes processes and\nfractional volatility models, we compute the characteristic function of the\nlog-price in rough Heston models. In the classical Heston model, the\ncharacteristic function is expressed in terms of the solution of a Riccati\nequation. Here we show that rough Heston models exhibit quite a similar\nstructure, the Riccati equation being replaced by a fractional Riccati\nequation.\n"
    },
    {
        "paper_id": 1609.02334,
        "authors": "Claudiu Tiberiu Albulescu (UPT), Daniel Goyeau (CRIEF)",
        "title": "The interaction between trade and FDI: the CEE countries experience",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inside the EU, the commercial integration of the CEE countries has gained\nremarkable momentum before the crisis appearance, but it has slightly slowed\ndown afterwards. Consequently, the interest in identifying the factors\nsupporting the commercial integration process is high. Recent findings in the\nnew trade theory suggest that FDI influence the trade intensity but the studies\napproaching this relationship for the CEE countries present mixed evidence, and\ninvestigate the commercial integration of CEE countries with the old EU\nmembers. Against this background, the purpose of this paper is to assess the\nCEE countries' intra-integration, focusing on the Czech Republic, Hungary,\nPoland and the Slovak Republic. For each country we employ a panel\ngravitational model for the bilateral trade and FDI, considering its\ninteractions with the other three countries in the sample on the one hand, and\nwith the three EU main commercial partners on the other hand. We investigate\ndifferent facets of the trade -- FDI nexus, resorting to a fixed effects model,\na random effects model, as well as to an instrumental variable estimator, over\nthe period 2000-2013. Our results suggest that outward FDI sustains the CEE\ncountries' commercial integration, while inward FDI has no significant effect.\nIn all the cases a complementarity effect between trade and FDI is documented,\nwhich is stronger for the CEE countries' historical trade partners.\nConsequently, these findings show that CEE countries' policymakers are\ninterested in encouraging the outward FDI toward their neighbour countries in\norder to increase the commercial integration.\n"
    },
    {
        "paper_id": 1609.02349,
        "authors": "Rafa{\\l} M. {\\L}ochowski, Nicolas Perkowski, David J. Pr\\\"omel",
        "title": "A superhedging approach to stochastic integration",
        "comments": "25 pages",
        "journal-ref": "Stoch. Process. Appl., Vol. 128, No. 12, p. 4078-4103, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Vovk's outer measure, which corresponds to a minimal superhedging\nprice, the existence of quadratic variation is shown for \"typical price paths\"\nin the space of c\\`adl\\`ag functions possessing a mild restriction on the jumps\ndirected downwards. In particular, this result includes the existence of\nquadratic variation of \"typical price paths\" in the space of non-negative\nc\\`adl\\`ag paths and implies the existence of quadratic variation in the sense\nof F\\\"ollmer quasi surely under all martingale measures. Based on the robust\nexistence of the quadratic variation, a model-free It\\^o integration is\ndeveloped.\n"
    },
    {
        "paper_id": 1609.02354,
        "authors": "David Ardia, Kris Boudt, Leopoldo Catania",
        "title": "Generalized Autoregressive Score Models in R: The GAS Package",
        "comments": null,
        "journal-ref": null,
        "doi": "10.18637/jss.v088.i06",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the R package GAS for the analysis of time series under\nthe Generalized Autoregressive Score (GAS) framework of Creal et al. (2013) and\nHarvey (2013). The distinctive feature of the GAS approach is the use of the\nscore function as the driver of time-variation in the parameters of nonlinear\nmodels. The GAS package provides functions to simulate univariate and\nmultivariate GAS processes, estimate the GAS parameters and to make time series\nforecasts. We illustrate the use of the GAS package with a detailed case study\non estimating the time-varying conditional densities of a set of financial\nassets.\n"
    },
    {
        "paper_id": 1609.02369,
        "authors": "Nassim Nicholas Taleb",
        "title": "Stochastic Tail Exponent For Asymmetric Power Laws",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine random variables in the power law/regularly varying class with\nstochastic tail exponent, the exponent $\\alpha$ having its own distribution. We\nshow the effect of stochasticity of $\\alpha$ on the expectation and higher\nmoments of the random variable. For instance, the moments of a right-tailed or\nright-asymmetric variable, when finite, increase with the variance of $\\alpha$;\nthose of a left-asymmetric one decreases. The same applies to conditional\nshortfall (CVar), or mean-excess functions. We prove the general case and\nexamine the specific situation of lognormally distributed $\\alpha \\in\n[b,\\infty), b>1$. The stochasticity of the exponent induces a significant bias\nin the estimation of the mean and higher moments in the presence of data\nuncertainty. This has consequences on sampling error as uncertainty about\n$\\alpha$ translates into a higher expected mean. The bias is conserved under\nsummation, even upon large enough a number of summands to warrant convergence\nto the stable distribution. We establish inequalities related to the asymmetry.\nWe also consider the situation of capped power laws (i.e. with compact\nsupport), and apply it to the study of violence by Cirillo and Taleb (2016). We\nshow that uncertainty concerning the historical data increases the true mean.\n"
    },
    {
        "paper_id": 1609.02395,
        "authors": "Michael Benzaquen, Iacopo Mastromatteo, Zoltan Eisler and\n  Jean-Philippe Bouchaud",
        "title": "Dissecting cross-impact on stock markets: An empirical analysis",
        "comments": "22 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa53f7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The vast majority of market impact studies assess each product individually,\nand the interactions between the different order flows are disregarded. This\nstrong approximation may lead to an underestimation of trading costs and\npossible contagion effects. Transactions in fact mediate a significant part of\nthe correlation between different instruments. In turn, liquidity shares the\nsectorial structure of market correlations, which can be encoded as a set of\neigenvalues and eigenvectors. We introduce a multivariate linear propagator\nmodel that successfully describes such a structure, and accounts for a\nsignificant fraction of the covariance of stock returns. We dissect the various\ndynamical mechanisms that contribute to the joint dynamics of assets. We also\ndefine two simplified models with substantially less parameters in order to\nreduce overfitting, and show that they have superior out-of-sample performance.\n"
    },
    {
        "paper_id": 1609.02774,
        "authors": "Arturo Erdely",
        "title": "Value at risk and the diversification dogma",
        "comments": "10 pages, 1 figure, in English and Spanish",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The so-called risk diversification principle is analyzed, showing that its\nconvenience depends on individual characteristics of the risks involved and the\ndependence relationship among them.\n  -----\n  Se analiza el principio de diversificaci\\'on de riesgos y se demuestra que no\nsiempre resulta mejor que no diversificar, pues esto depende de\ncaracter\\'isticas individuales de los riesgos involucrados, as\\'i como de la\nrelaci\\'on de dependencia entre los mismos.\n"
    },
    {
        "paper_id": 1609.02867,
        "authors": "Marcel Nutz, Florian Stebegg",
        "title": "Canonical Supermartingale Couplings",
        "comments": "47 pages, 5 figures; forthcoming in 'Annals of Probability'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two probability distributions $\\mu$ and $\\nu$ in second stochastic order can\nbe coupled by a supermartingale, and in fact by many. Is there a canonical\nchoice? We construct and investigate two couplings which arise as optimizers\nfor constrained Monge-Kantorovich optimal transport problems where only\nsupermartingales are allowed as transports. Much like the Hoeffding-Fr\\'echet\ncoupling of classical transport and its symmetric counterpart, the antitone\ncoupling, these can be characterized by order-theoretic minimality properties,\nas simultaneous optimal transports for certain classes of reward (or cost)\nfunctions, and through no-crossing conditions on their supports; however, our\ntwo couplings have asymmetric geometries. Remarkably, supermartingale optimal\ntransport decomposes into classical and martingale transport in several ways.\n"
    },
    {
        "paper_id": 1609.03029,
        "authors": "Arianna Agosto, Alessandra Mainini, Enrico Moretto",
        "title": "Covariance of random stock prices in the Stochastic Dividend Discount\n  Model",
        "comments": "Version #2",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dividend discount models have been developed in a deterministic setting. Some\nauthors (Hurley and Johnson, 1994 and 1998; Yao, 1997) have introduced\nrandomness in terms of stochastic growth rates, delivering closed-form\nexpressions for the expected value of stock prices. This paper extends such\nprevious results by determining a formula for the covariance between random\nstock prices when the dividends' rates of growth are correlated. The formula is\neventually applied to real market data.\n"
    },
    {
        "paper_id": 1609.03223,
        "authors": "Bruce Knuteson",
        "title": "The Solution to Science's Replication Crisis",
        "comments": "2 pages (main text) + 5 pages (references)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The solution to science's replication crisis is a new ecosystem in which\nscientists sell what they learn from their research. In each pairwise\ntransaction, the information seller makes (loses) money if he turns out to be\ncorrect (incorrect). Responsibility for the determination of correctness is\ndelegated, with appropriate incentives, to the information purchaser. Each\ntransaction is brokered by a central exchange, which holds money from the\nanonymous information buyer and anonymous information seller in escrow, and\nwhich enforces a set of incentives facilitating the transfer of useful, bluntly\nhonest information from the seller to the buyer. This new ecosystem, capitalist\nscience, directly addresses socialist science's replication crisis by\nexplicitly rewarding accuracy and penalizing inaccuracy.\n"
    },
    {
        "paper_id": 1609.03344,
        "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher",
        "title": "Finite-sample and asymptotic analysis of generalization ability with an\n  application to penalized regression",
        "comments": "The theoretical generalization and extension of arXiv:1606.00142",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the performance of extremum estimators from the\nperspective of generalization ability (GA): the ability of a model to predict\noutcomes in new samples from the same population. By adapting the classical\nconcentration inequalities, we derive upper bounds on the empirical\nout-of-sample prediction errors as a function of the in-sample errors,\nin-sample data size, heaviness in the tails of the error distribution, and\nmodel complexity. We show that the error bounds may be used for tuning key\nestimation hyper-parameters, such as the number of folds $K$ in\ncross-validation. We also show how $K$ affects the bias-variance trade-off for\ncross-validation. We demonstrate that the $\\mathcal{L}_2$-norm difference\nbetween penalized and the corresponding un-penalized regression estimates is\ndirectly explained by the GA of the estimates and the GA of empirical moment\nconditions. Lastly, we prove that all penalized regression estimates are\n$L_2$-consistent for both the $n \\geqslant p$ and the $n < p$ cases.\nSimulations are used to demonstrate key results.\n  Keywords: generalization ability, upper bound of generalization error,\npenalized regression, cross-validation, bias-variance trade-off,\n$\\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,\nhigh-dimensional data.\n"
    },
    {
        "paper_id": 1609.03471,
        "authors": "Joachim R. Groeger",
        "title": "The Informational Content of the Limit Order Book: An Empirical Study of\n  Prediction Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper I empirically investigate prediction markets for binary\noptions. Advocates of prediction markets have suggested that asset prices are\nconsistent estimators of the \"true\" probability of a state of the world being\nrealized. I test whether the market reaches a \"consensus.\" I find little\nevidence for convergence in beliefs. I then determine whether an econometrician\nusing data beyond execution prices can leverage this data to estimate the\nconsensus belief. I use an incomplete specification of equilibrium outcomes to\nderive bounds on beliefs from order submission decisions. Interval estimates of\nmean beliefs cannot exclude aggregate beliefs equal to 0.5.\n"
    },
    {
        "paper_id": 1609.03996,
        "authors": "Bernardo Alves Furtado, Isaque Daniel Rocha Eberhardt, Alexandre Messa",
        "title": "SEAL's operating manual: a Spatially-bounded Economic Agent-based Lab",
        "comments": "18 pages, 1 figure, Research Report",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This text reports in detail how SEAL, a modeling framework for the economy\nbased on individual agents and firms, works. Thus, it aims to be an usage\nmanual for those wishing to use SEAL or SEAL's results. As a reference work,\ntheoretical and research studies are only cited. SEAL is thought as a Lab that\nenables the simulation of the economy with spatially bounded\nmicroeconomic-based computational agents. Part of the novelty of SEAL comes\nfrom the possibility of simulating the economy in space and the instantiation\nof different public offices, i.e. government institutions, with embedded\nmarkets and actual data. SEAL is designed for Public Policy analysis,\nspecifically those related to Public Finance, Taxes and Real Estate.\n"
    },
    {
        "paper_id": 1609.04065,
        "authors": "Jonathan Yu-Meng Li",
        "title": "Closed-form solutions for worst-case law invariant risk measures with\n  application to robust portfolio optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Worst-case risk measures refer to the calculation of the largest value for\nrisk measures when only partial information of the underlying distribution is\navailable. For the popular risk measures such as Value-at-Risk (VaR) and\nConditional Value-at-Risk (CVaR), it is now known that their worst-case\ncounterparts can be evaluated in closed form when only the first two moments\nare known for the underlying distribution. These results are remarkable since\nthey not only simplify the use of worst-case risk measures but also provide\ngreat insight into the connection between the worst-case risk measures and\nexisting risk measures. We show in this paper that somewhat surprisingly\nsimilar closed-form solutions also exist for the general class of law invariant\ncoherent risk measures, which consists of spectral risk measures as special\ncases that are arguably the most important extensions of CVaR. We shed light on\nthe one-to-one correspondence between a worst-case law invariant risk measure\nand a worst-case CVaR (and a worst-case VaR), which enables one to carry over\nthe development of worst-case VaR in the context of portfolio optimization to\nthe worst-case law invariant risk measures immediately.\n"
    },
    {
        "paper_id": 1609.04199,
        "authors": "Lucio Maria Calcagnile, Fulvio Corsi, Stefano Marmi",
        "title": "Entropy and efficiency of the ETF market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the relative information efficiency of financial markets by\nmeasuring the entropy of the time series of high frequency data. Our tool to\nmeasure efficiency is the Shannon entropy, applied to 2-symbol and 3-symbol\ndiscretisations of the data. Analysing 1-minute and 5-minute price time series\nof 55 Exchange Traded Funds traded at the New York Stock Exchange, we develop a\nmethodology to isolate true inefficiencies from other sources of regularities,\nsuch as the intraday pattern, the volatility clustering and the microstructure\neffects. The first two are modelled as multiplicative factors, while the\nmicrostructure is modelled as an ARMA noise process. Following an analytical\nand empirical combined approach, we find a strong relationship between low\nentropy and high relative tick size and that volatility is responsible for the\nlargest amount of regularity, averaging 62% of the total regularity against 18%\nof the intraday pattern regularity and 20% of the microstructure.\n"
    },
    {
        "paper_id": 1609.04529,
        "authors": "Pingjin Deng",
        "title": "The joint distributions of running maximum of a Slepian processes",
        "comments": "11 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider the Slepian process $S$ defined by $ S(t)=B(t+1)-B(t),t\\in [0,1]$\nwith $B(t),t\\in \\R$ a standard Brownian motion.In this contribution we analyze\nthe joint distribution between the maximum $m_{s}=\\max_{0\\leq u\\leq s}S(u)$\ncertain and the maximum $M_t=\\max_{0\\leq u\\leq t}S(u)$ for $0< s < t$ fixed.\nExplicit integral expression are obtained for the distribution function of the\npartial maximum $m_{s}$ and the joint distribution function between $m_{s}$ and\n$M_t$. We also use our results to determine the moments of $m_{s}$.\n"
    },
    {
        "paper_id": 1609.0462,
        "authors": "Zoltan Eisler, Jean-Philippe Bouchaud",
        "title": "Price impact without order book: A study of the OTC credit index market",
        "comments": "15 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a study of price impact in the over-the-counter credit index\nmarket, where no limit order book is used. Contracts are traded via dealers,\nthat compete for the orders of clients. Despite this distinct microstructure,\nwe successfully apply the propagator technique to estimate the price impact of\nindividual transactions. Because orders are typically split less than in\nmultilateral markets, impact is observed to be mainly permanent, in line with\ntheoretical expectations. A simple method is presented to correct for errors in\nour classification of trades between buying and selling. We find a very\nsignificant, temporary increase in order flow correlations during late 2015 and\nearly 2016, which we attribute to increased order splitting or herding among\ninvestors. We also find indications that orders advertised to less dealers may\nhave lower price impact. Quantitative results are compatible with earlier\nfindings in other more classical markets, further supporting the argument that\nprice impact is a universal phenomenon, to a large degree independent of market\nmicrostructure.\n"
    },
    {
        "paper_id": 1609.04629,
        "authors": "Sheen S. Levine and Edward J. Zajac",
        "title": "Institutionalization in Efficient Markets: The Case of Price Bubbles",
        "comments": "Academy of Management Best Paper Proceedings (2008)",
        "journal-ref": null,
        "doi": "10.5465/AMBPP.2008.33649949",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We seek to deepen understanding of the micro-foundations of\ninstitutionalization while contributing to a sociological theory of markets by\ninvestigating the puzzle of price bubbles in financial markets. We find that\nsuch markets, despite textbook conditions of high efficiency -- perfect\ninformation, atomistic agents, no uncertainty -- quickly develop patterns\nconsistent with institutionalization processes.\n"
    },
    {
        "paper_id": 1609.0464,
        "authors": "Damien Challet, R\\'emy Chicheportiche, Mehdi Lallouache and Serge\n  Kassibrakis",
        "title": "Statistically validated lead-lag networks and inventory prediction in\n  the foreign exchange market",
        "comments": "33 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a method to infer lead-lag networks of agents' actions in\ncomplex systems. These networks open the way to both microscopic and\nmacroscopic states prediction in such systems. We apply this method to\ntrader-resolved data in the foreign exchange market. We show that these\nnetworks are remarkably persistent, which explains why and how order flow\nprediction is possible from trader-resolved data. In addition, if traders'\nactions depend on past prices, the evolution of the average price paid by\ntraders may also be predictable. Using random forests, we verify that the\npredictability of both the sign of order flow and the direction of average\ntransaction price is strong for retail investors at an hourly time scale, which\nis of great relevance to brokers and order matching engines. Finally, we argue\nthat the existence of trader lead-lag networks explains in a self-referential\nway why a given trader becomes active, which is in line with the fact that most\ntrading activity has an endogenous origin.\n"
    },
    {
        "paper_id": 1609.0489,
        "authors": "Shanshan Wang and Thomas Guhr",
        "title": "Microscopic Understanding of Cross-Responses between Stocks: a\n  Two-Component Price Impact Model",
        "comments": null,
        "journal-ref": "Market Microstructure and Liquidity (2017) 3:1850009",
        "doi": "10.1142/S2382626618500090",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a price impact model between stocks in a correlated market. For\nthe price change of a given stock induced by the short-run liquidity of this\nstock itself and of the information about other stocks, we introduce a self-\nand a cross-impact function of the time lag. We model the average\ncross-response functions for individual stocks employing the impact functions\nof the time lag, the impact functions of traded volumes and the trade-sign\ncorrelators. To quantify the self- and cross-impacts, we propose a construction\nto fix the parameters in the impact functions. These parameters are further\ncorroborated by a diffusion function that measures the correlated motion of\nprices from different stocks. This construction is mainly ad hoc and\nalternative ones are not excluded. It turns out that both the sign cross- and\nself-correlators are connected with the cross-responses. The self- and\ncross-impact functions are indispensable to compensate amplification effects\nwhich are due to the sign correlators integrated over time. We further quantify\nand interpret the price impacts of time lag in terms of temporary and permanent\ncomponents. To support our model, we also analyze empirical data, in particular\nthe memory properties of the sign self- and average cross-correlators. The\nrelation between the average cross-responses and the traded volumes which are\nsmaller than their average is of power-law form.\n"
    },
    {
        "paper_id": 1609.04907,
        "authors": "Tanmay S. Patankar",
        "title": "Asset Pricing in a Semi-Markov Modulated Market with Time-dependent\n  Volatility",
        "comments": "78 pages, 2 figures. MS thesis. arXiv admin note: substantial text\n  overlap with arXiv:1408.5266, arXiv:1506.01467 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This project attempts to address the problem of asset pricing in a financial\nmarket, where the interest rates and volatilities exhibit regime switching.\nThis is an extension of the Black-Scholes model. Studies of Markov-modulated\nregime switching models have been well-documented. This project extends that\nnotion to a class of semi-Markov processes known as age-dependent processes. We\nalso allow for time-dependence in volatility within regimes. We show that the\nproblem of option pricing in such a market is equivalent to solving a certain\nintegral equation.\n"
    },
    {
        "paper_id": 1609.04944,
        "authors": "Alan Roncoroni, Matus Medo",
        "title": "Spatial firm competition in two dimensions with linear transportation\n  costs: simulations and analytical results",
        "comments": "7 pages, 4 figures",
        "journal-ref": "European Physical Journal B 89, 270 (2016)",
        "doi": "10.1140/epjb/e2016-70148-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models of spatial firm competition assume that customers are distributed in\nspace and transportation costs are associated with their purchases of products\nfrom a small number of firms that are also placed at definite locations. It has\nbeen long known that the competition equilibrium is not guaranteed to exist if\nthe most straightforward linear transportation costs are assumed. We show by\nsimulations and also analytically that if periodic boundary conditions in two\ndimensions are assumed, the equilibrium exists for a pair of firms at any\ndistance. When a larger number of firms is considered, we find that their total\nequilibrium profit is inversely proportional to the square root of the number\nof firms. We end with a numerical investigation of the system's behavior for a\ngeneral transportation cost exponent.\n"
    },
    {
        "paper_id": 1609.04956,
        "authors": "Michele Caraglio, Fulvio Baldovin and Attilio L. Stella",
        "title": "Export dynamics as an optimal growth problem in the network of global\n  economy",
        "comments": "11 pages, 8 figures",
        "journal-ref": "Scientific Reports 6, 31461 (2016)",
        "doi": "10.1038/srep31461",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze export data aggregated at world global level of 219 classes of\nproducts over a period of 39 years. Our main goal is to set up a dynamical\nmodel to identify and quantify plausible mechanisms by which the evolutions of\nthe various exports affect each other. This is pursued through a stochastic\ndifferential description, partly inspired by approaches used in population\ndynamics or directed polymers in random media. We outline a complex network of\ntransfer rates which describes how resources are shifted between different\nproduct classes, and determines how casual favorable conditions for one export\ncan spread to the other ones. A calibration procedure allows to fit four free\nmodel-parameters such that the dynamical evolution becomes consistent with the\naverage growth, the fluctuations, and the ranking of the export values observed\nin real data. Growth crucially depends on the balance between maintaining and\nshifting resources to different exports, like in an explore-exploit problem.\nRemarkably, the calibrated parameters warrant a close-to-maximum growth rate\nunder the transient conditions realized in the period covered by data, implying\nan optimal self organization of the global export. According to the model,\nmajor structural changes in the global economy take tens of years.\n"
    },
    {
        "paper_id": 1609.05055,
        "authors": "Alexander Smirnov",
        "title": "A Simple Model of Credit Expansion",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The proposed model is aimed to reveal important patterns in the behavior of a\nsimplified financial system. The patterns could be detected as regular cycles\nconsisting of debt bubbles and crises. Financial cycles have a well defined\nstructure and form periodic sequences along the axis of credit expansion while\nretaining stochastic nature in terms of time.\n"
    },
    {
        "paper_id": 1609.05056,
        "authors": "Henry Penikas",
        "title": "Copula-Based Univariate Time Series Structural Shift Identification Test",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An approach is proposed to determine structural shift in time-series assuming\nnon-linear dependence of lagged values of dependent variable. Copulas are used\nto model non-linear dependence of time series components.\n"
    },
    {
        "paper_id": 1609.05177,
        "authors": "El Euch Omar, Fukasawa Masaaki and Rosenbaum Mathieu",
        "title": "The microstructural foundations of leverage effect and rough volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that typical behaviors of market participants at the high frequency\nscale generate leverage effect and rough volatility. To do so, we build a\nsimple microscopic model for the price of an asset based on Hawkes processes.\nWe encode in this model some of the main features of market microstructure in\nthe context of high frequency trading: high degree of endogeneity of market,\nno-arbitrage property, buying/selling asymmetry and presence of metaorders. We\nprove that when the first three of these stylized facts are considered within\nthe framework of our microscopic model, it behaves in the long run as a Heston\nstochastic volatility model, where leverage effect is generated. Adding the\nlast property enables us to obtain a rough Heston model in the limit,\nexhibiting both leverage effect and rough volatility. Hence we show that at\nleast part of the foundations of leverage effect and rough volatility can be\nfound in the microstructure of the asset.\n"
    },
    {
        "paper_id": 1609.052,
        "authors": "Weifan Zhang, Rebecca Liu, Chris Chatwin",
        "title": "Chinese Medical Device Market and The Investment Vector",
        "comments": "34 pages, 11 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China has attracted increasing amounts of foreign investment since it opened\nits doors to the world and whilst many analysts have focused on foreign\ninvestment in popular areas, little has been written about medical device\ninvestment. The purpose of this article is to analyze the status of the Chinese\nmedical device market from the perspective of the healthcare industry and its\nimportant market drivers; the study reveals that the medical device market has\nsignificant growth potential. This article aims to identify and assess the\nprofitable sectors of medical device technologies as a guide for international\ncompanies and investors.\n"
    },
    {
        "paper_id": 1609.05286,
        "authors": "Christof Henkel",
        "title": "From quantum mechanics to finance: Microfoundations for jumps, spikes\n  and high volatility phases in diffusion price processes",
        "comments": "arXiv admin note: text overlap with arXiv:1606.08269",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.11.125",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an agent behavior based microscopic model that induces jumps,\nspikes and high volatility phases in the price process of a traded asset. We\ntransfer dynamics of thermally activated jumps of an unexcited/ excited two\nstate system discussed in the context of quantum mechanics to agent\nsocio-economic behavior and provide microfoundations. After we link the\nendogenous agent behavior to price dynamics we establish the circumstances\nunder which the dynamics converge to an It\\^o-diffusion price processes in the\nlarge market limit.\n"
    },
    {
        "paper_id": 1609.05394,
        "authors": "Barack Wamkaya Wanjawa",
        "title": "Predicting Future Shanghai Stock Market Price using ANN in the Period\n  21-Sep-2016 to 11-Oct-2016",
        "comments": "10 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the prices of stocks at any stock market remains a quest for many\ninvestors and researchers. Those who trade at the stock market tend to use\ntechnical, fundamental or time series analysis in their predictions. These\nmethods usually guide on trends and not the exact likely prices. It is for this\nreason that Artificial Intelligence systems, such as Artificial Neural Network,\nthat is feedforward multi-layer perceptron with error backpropagation, can be\nused for such predictions. A difficulty in neural network application is the\ndetermination of suitable network parameters. A previous research by the author\nalready determined the network parameters as 5:21:21:1 with 80% training data\nor 4-year of training data as a good enough model for stock prediction. This\nmodel has been put to the test in predicting selected Shanghai Stock Exchange\nstocks in the future period of 21-Sep-2016 to 11-Oct-2016, about one week after\nthe publication of these predictions. The research aims at confirming that\nsimple neural network systems can be quite powerful in typical stock market\npredictions.\n"
    },
    {
        "paper_id": 1609.05475,
        "authors": "Takashi Shinzato",
        "title": "Replica Analysis for the Duality of the Portfolio Optimization Problem",
        "comments": "6 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.94.052307",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, the primal-dual problem consisting of the investment\nrisk minimization problem and the expected return maximization problem in the\nmean-variance model is discussed using replica analysis. As a natural extension\nof the investment risk minimization problem under only a budget constraint that\nwe analyzed in a previous study, we herein consider a primal-dual problem in\nwhich the investment risk minimization problem with budget and expected return\nconstraints is regarded as the primal problem, and the expected return\nmaximization problem with budget and investment risk constraints is regarded as\nthe dual problem. With respect to these optimal problems, we analyze a quenched\ndisordered system involving both of these optimization problems using the\napproach developed in statistical mechanical informatics, and confirm that both\noptimal portfolios can possess the primal-dual structure. Finally, the results\nof numerical simulations are shown to validate the effectiveness of the\nproposed method.\n"
    },
    {
        "paper_id": 1609.05523,
        "authors": "Damiano Brigo, Clement Piat",
        "title": "Static vs adapted optimal execution strategies in two benchmark trading\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal solutions to the trade execution problem in the two\ndifferent classes of i) fully adapted or adaptive and ii) deterministic or\nstatic strategies, comparing them. We do this in two different benchmark\nmodels. The first model is a discrete time framework with an information flow\nprocess, dealing with both permanent and temporary impact, minimizing the\nexpected cost of the trade. The second model is a continuous time framework\nwhere the objective function is the sum of the expected cost and a value at\nrisk (or expected shortfall) type risk criterion. Optimal adapted solutions are\nknown in both frameworks from the original works of Bertsimas and Lo (1998) and\nGatheral and Schied (2011). In this paper we derive the optimal static\nstrategies for both benchmark models and we study quantitatively the\nimprovement in optimality when moving from static strategies to fully adapted\nones. We conclude that, in the benchmark models we study, the difference is not\nrelevant, except for extreme unrealistic cases for the model or impact\nparameters. This indirectly confirms that in the similar framework of Almgren\nand Chriss (2000) one is fine deriving a static optimal solution, as done by\nthose authors, as opposed to a fully adapted one, since the static solution\nhappens to be tractable and known in closed form.\n"
    },
    {
        "paper_id": 1609.05832,
        "authors": "Julien Guyon, Romain Menegaux, Marcel Nutz",
        "title": "Bounds for VIX Futures given S&P 500 Smiles",
        "comments": "22 pages; to appear in 'Finance&Stochastics'",
        "journal-ref": null,
        "doi": "10.1007/s00780-017-0334-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive sharp bounds for the prices of VIX futures using the full\ninformation of S&P 500 smiles. To that end, we formulate the model-free\nsub/superreplication of the VIX by trading in the S&P 500 and its vanilla\noptions as well as the forward-starting log-contracts. A dual problem of\nminimizing/maximizing certain risk-neutral expectations is introduced and shown\nto yield the same value.\n  The classical bounds for VIX futures given the smiles only use a calendar\nspread of log-contracts on the S&P 500. We analyze for which smiles the\nclassical bounds are sharp and how they can be improved when they are not. In\nparticular, we introduce a family of functionally generated portfolios which\noften improves the classical bounds while still being tractable; more\nprecisely, determined by a single concave/convex function on the line.\nNumerical experiments on market data and SABR smiles show that the classical\nlower bound can be improved dramatically, whereas the upper bound is often\nclose to optimal.\n"
    },
    {
        "paper_id": 1609.05865,
        "authors": "Matyas Barczy, Mohamed Ben Alaya, Ahmed Kebaier, Gyula Pap",
        "title": "Asymptotic properties of maximum likelihood estimator for the growth\n  rate for a jump-type CIR process based on continuous time observations",
        "comments": "36 pages. In Appendices we recall some notions and statements from\n  arXiv:1509.08869",
        "journal-ref": "Stochastic Processes and their Applications 128 (4), (2018),\n  1135-1164",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a jump-type Cox--Ingersoll--Ross (CIR) process driven by a\nstandard Wiener process and a subordinator, and we study asymptotic properties\nof the maximum likelihood estimator (MLE) for its growth rate. We distinguish\nthree cases: subcritical, critical and supercritical. In the subcritical case\nwe prove weak consistency and asymptotic normality, and, under an additional\nmoment assumption, strong consistency as well. In the supercritical case, we\nprove strong consistency and mixed normal (but non-normal) asymptotic behavior,\nwhile in the critical case, weak consistency and non-standard asymptotic\nbehavior are described. We specialize our results to so-called basic affine\njump-diffusions as well. Concerning the asymptotic behavior of the MLE in the\nsupercritical case, we derive a stochastic representation of the limiting mixed\nnormal distribution, where the almost sure limit of an appropriately scaled\njump-type supercritical CIR process comes into play. This is a new phenomenon,\ncompared to the critical case, where a diffusion-type critical CIR process\nplays a role.\n"
    },
    {
        "paper_id": 1609.05939,
        "authors": "Nima Dehmamy, Sergey Buldyrev, Shlomo Havlin, Harry Eugene Stanley,\n  Irena Vodenska",
        "title": "Crises and Physical Phases of a Bipartite Market Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the linear response of a market network to shocks based on the\nbipartite market model we introduced in an earlier paper, which we claimed to\nbe able to identify the time-line of the 2009-2011 Eurozone crisis correctly.\nWe show that this model has three distinct phases that can broadly be\ncategorized as \"stable\" and \"unstable\". Based on the interpretation of our\nbehavioral parameters, the stable phase describes periods where investors and\ntraders have confidence in the market (e.g. predict that the market rebounds\nfrom a loss). We show that the unstable phase happens when there is a lack of\nconfidence and seems to describe \"boom-bust\" periods in which changes in prices\nare exponential. We analytically derive these phases and where the phase\ntransition happens using a mean field approximation of the model. We show that\nthe condition for stability is $\\alpha \\beta <1$ with $\\alpha$ being the\ninverse of the \"price elasticity\" and $\\beta$ the \"income elasticity of\ndemand\", which measures how rash the investors make decisions. We also show\nthat in the mean-field limit this model reduces to the Langevin model by\nBouchaud et al. for price returns.\n"
    },
    {
        "paper_id": 1609.06545,
        "authors": "Samuel N. Cohen",
        "title": "Data-driven nonlinear expectations for statistical uncertainty in\n  decisions",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1214/17-EJS1278",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In stochastic decision problems, one often wants to estimate the underlying\nprobability measure statistically, and then to use this estimate as a basis for\ndecisions. We shall consider how the uncertainty in this estimation can be\nexplicitly and consistently incorporated in the valuation of decisions, using\nthe theory of nonlinear expectations.\n"
    },
    {
        "paper_id": 1609.07051,
        "authors": "Matthias Raddant and Friedrich Wagner",
        "title": "Multivariate Garch with dynamic beta",
        "comments": "revised version. M. The European Journal of Finance (2021)",
        "journal-ref": "The European Journal of Finance (2021)",
        "doi": "10.1080/1351847X.2021.1882523",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a solution for the problems related to the application of\nmultivariate GARCH models to markets with a large number of stocks by\nrestricting the form of the conditional covariance matrix. The model is a\nfactor model and uses only six free GARCH parameters. One factor can be\ninterpreted as the market component, the remaining factors are equal. This\nallow the analytical calculation of the inverse covariance matrix. The\ntime-dependence of the factors enables the determination of dynamical beta\ncoefficients. We compare the results from our model with the results of other\nGARCH models for the daily returns from the S\\&P500 market and find that they\nare competitive. As applications we use the daily values of beta coefficients\nto confirm a transition of the market in 2006. Furthermore we discuss the\nrelationship of our model with the leverage effect.\n"
    },
    {
        "paper_id": 1609.07472,
        "authors": "Yongxin Yang, Yu Zheng, Timothy M. Hospedales",
        "title": "Gated Neural Networks for Option Pricing: Rationality by Design",
        "comments": "Accepted to AAAI 2017. 7 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a neural network approach to price EU call options that\nsignificantly outperforms some existing pricing models and comes with\nguarantees that its predictions are economically reasonable. To achieve this,\nwe introduce a class of gated neural networks that automatically learn to\ndivide-and-conquer the problem space for robust and accurate pricing. We then\nderive instantiations of these networks that are 'rational by design' in terms\nof naturally encoding a valid call option surface that enforces no arbitrage\nprinciples. This integration of human insight within data-driven learning\nprovides significantly better generalisation in pricing performance due to the\nencoded inductive bias in the learning, guarantees sanity in the model's\npredictions, and provides econometrically useful byproduct such as risk neutral\ndensity.\n"
    },
    {
        "paper_id": 1609.07558,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Discrete Sums of Geometric Brownian Motions, Annuities and Asian Options",
        "comments": "38 pages, 3 figures",
        "journal-ref": "Insurance: Mathematics and Economics 2016, Volume 70, 19-37",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The discrete sum of geometric Brownian motions plays an important role in\nmodeling stochastic annuities in insurance. It also plays a pivotal role in the\npricing of Asian options in mathematical finance. In this paper, we study the\nprobability distributions of the infinite sum of geometric Brownian motions,\nthe sum of geometric Brownian motions with geometric stopping time, and the\nfinite sum of the geometric Brownian motions. These results are extended to the\ndiscrete sum of the exponential L\\'evy process. We derive tail asymptotics and\ncompute numerically the asymptotic distribution function. We compare the\nresults against the known results for the continuous time integral of the\ngeometric Brownian motion up to an exponentially distributed time. The results\nare illustrated with numerical examples for life annuities with discrete\npayments, and Asian options.\n"
    },
    {
        "paper_id": 1609.07559,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Short Maturity Asian Options in Local Volatility Models",
        "comments": "42 pages, 2 figures",
        "journal-ref": "SIAM J. Finan. Math., 7(1), 947-992 (2016)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a rigorous study of the short maturity asymptotics for Asian\noptions with continuous-time averaging, under the assumption that the\nunderlying asset follows a local volatility model. The asymptotics for\nout-of-the-money, in-the-money, and at-the-money cases are derived, considering\nboth fixed strike and floating strike Asian options. The asymptotics for the\nout-of-the-money case involves a non-trivial variational problem which is\nsolved completely. We present an analytical approximation for Asian options\nprices, and demonstrate good numerical agreement of the asymptotic results with\nthe results of Monte Carlo simulations and benchmark test cases in the\nBlack-Scholes model for option parameters relevant in practical applications.\n"
    },
    {
        "paper_id": 1609.07897,
        "authors": "Hannes Hoffmann, Thilo Meyer-Brandis, Gregor Svindland",
        "title": "Risk-Consistent Conditional Systemic Risk Measures",
        "comments": null,
        "journal-ref": "Stochastic Processes and their Applications, Vol. 126, No. 7, pp.\n  2014-2037",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We axiomatically introduce risk-consistent conditional systemic risk measures\ndefined on multidimensional risks. This class consists of those conditional\nsystemic risk measures which can be decomposed into a state-wise conditional\naggregation and a univariate conditional risk measure. Our studies extend known\nresults for unconditional risk measures on finite state spaces. We argue in\nfavor of a conditional framework on general probability spaces for assessing\nsystemic risk. Mathematically, the problem reduces to selecting a realization\nof a random field with suitable properties. Moreover, our approach covers many\nprominent examples of systemic risk measures from the literature and used in\npractice.\n"
    },
    {
        "paper_id": 1609.07903,
        "authors": "Hannes Hoffmann, Thilo Meyer-Brandis, Gregor Svindland",
        "title": "Strongly Consistent Multivariate Conditional Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider families of strongly consistent multivariate conditional risk\nmeasures. We show that under strong consistency these families admit a\ndecomposition into a conditional aggregation function and a univariate\nconditional risk measure as introduced Hoffmann et al. (2016). Further, in\nanalogy to the univariate case in F\\\"ollmer (2014), we prove that under\nlaw-invariance strong consistency implies that multivariate conditional risk\nmeasures are necessarily multivariate conditional certainty equivalents.\n"
    },
    {
        "paper_id": 1609.0852,
        "authors": "Fan Cai, Nhien-An Le-Khac, Tahar Kechadi",
        "title": "Clustering Approaches for Financial Data Analysis: a Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays, financial data analysis is becoming increasingly important in the\nbusiness market. As companies collect more and more data from daily operations,\nthey expect to extract useful knowledge from existing collected data to help\nmake reasonable decisions for new customer requests, e.g. user credit category,\nconfidence of expected return, etc. Banking and financial institutes have\napplied different data mining techniques to enhance their business performance.\nAmong these techniques, clustering has been considered as a significant method\nto capture the natural structure of data. However, there are not many studies\non clustering approaches for financial data analysis. In this paper, we\nevaluate different clustering algorithms for analysing different financial\ndatasets varied from time series to transactions. We also discuss the\nadvantages and disadvantages of each method to enhance the understanding of\ninner structure of financial datasets as well as the capability of each\nclustering method in this context.\n"
    },
    {
        "paper_id": 1609.08746,
        "authors": "V. Sasidevan, Appilineni Kushal and Sitabhra Sinha",
        "title": "When Big Data Fails! Relative success of adaptive agents using\n  coarse-grained information to compete for limited resources",
        "comments": "6 pages, 4 figures + 2 pages supplementary information",
        "journal-ref": "Phys. Rev. E 98, 020301 (2018)",
        "doi": "10.1103/PhysRevE.98.020301",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent trend for acquiring big data assumes that possessing\nquantitatively more and qualitatively finer data necessarily provides an\nadvantage that may be critical in competitive situations. Using a model complex\nadaptive system where agents compete for a limited resource using information\ncoarse-grained to different levels, we show that agents having access to more\nand better data can perform worse than others in certain situations. The\nrelation between information asymmetry and individual payoffs is seen to be\ncomplex, depending on the composition of the population of competing agents.\n"
    },
    {
        "paper_id": 1609.08978,
        "authors": "Bertram D\\\"uring, Nicos Georgiou, Enrico Scalas",
        "title": "A stylized model for wealth distribution",
        "comments": "16 pages, this version corrects typos and an error while calculating\n  the invariant distribution of the discrete space-time chain, and the proof is\n  simplified",
        "journal-ref": "Economic Foundations for Social Complexity Science, Y.Aruka, A.\n  Kirman (eds.), pp. 135-157, Evolutionary Economics and Social Complexity\n  Science 9, Springer, Singapore, 2017",
        "doi": "10.1007/978-981-10-5705-2_7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent book by T. Piketty (Capital in the Twenty-First Century) promoted\nthe important issue of wealth inequality. In the last twenty years, physicists\nand mathematicians developed models to derive the wealth distribution using\ndiscrete and continuous stochastic processes (random exchange models) as well\nas related Boltzmann-type kinetic equations. In this literature, the usual\nconcept of equilibrium in Economics is either replaced or completed by\nstatistical equilibrium.\n  In order to illustrate this activity with a concrete example, we present a\nstylised random exchange model for the distribution of wealth. We first discuss\na fully discrete version (a Markov chain with finite state space). We then\nstudy its discrete-time continuous-state-space version and we prove the\nexistence of the equilibrium distribution. Finally, we discuss the connection\nof these models with Boltzmann-like kinetic equations for the marginal\ndistribution of wealth. This paper shows in practice how it is possible to\nstart from a finitary description and connect it to continuous models following\nBoltzmann's original research program.\n"
    },
    {
        "paper_id": 1609.09205,
        "authors": "Laurence Carassus, Romain Blanchard",
        "title": "Robust Optimal Investment in Discrete Time for Unbounded Utility\n  Function",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the problem of maximizing expected terminal utility\nin a discrete-time financial market model with a finite horizon under\nnon-dominated model uncertainty. We use a dynamic programming framework\ntogether with measurable selection arguments to prove that under mild\nintegrability conditions, an optimal portfolio exists for an unbounded utility\nfunction defined on the half-real line.\n"
    },
    {
        "paper_id": 1609.09571,
        "authors": "Gaurav Paruthi (University of Michigan), Enrique Frias-Martinez\n  (Telefonica Research), Vanessa Frias-Martinez (University of Maryland)",
        "title": "The Role of Rating and Loan Characteristics in Online Microfunding\n  Behaviors",
        "comments": "Presented at the Data For Good Exchange 2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an in-depth study of lending behaviors in Kiva using a mix of\nquantitative and large-scale data mining techniques. Kiva is a non-profit\norganization that offers an online platform to connect lenders with borrowers.\nTheir site, kiva.org, allows citizens to microlend small amounts of money to\nentrepreneurs (borrowers) from different countries. The borrowers are always\naffiliated with a Field Partner (FP) which can be a microfinance institution\n(MFI) or other type of local organization that has partnered with Kiva. Field\npartners give loans to selected businesses based on their local knowledge\nregarding the country, the business sector including agriculture, health or\nmanufacture among others, and the borrower.Our objective is to understand the\nrelationship between lending activity and various features offered by the\nonline platform. Specifically, we focus on two research questions: (i) the role\nthat MFI ratings play in driving lending activity and (ii) the role that\nvarious loan features have in the lending behavior. The first question analyzes\nwhether there exists a relationship between the MFI ratings - that lenders can\nexplore online - and their lending volumes. The second research question\nattempts to understand if certain loan features - available online at Kiva -\nsuch as the type of small business, the gender of the borrower, or the loan's\ncountry information might affect the way lenders lend.\n"
    },
    {
        "paper_id": 1609.09601,
        "authors": "Giancarlo Salirrosas Mart\\'inez",
        "title": "Biased Roulette Wheel: A Quantitative Trading Strategy Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this research paper it is to present a new approach in the\nframework of a biased roulette wheel. It is used the approach of a quantitative\ntrading strategy, commonly used in quantitative finance, in order to assess the\nprofitability of the strategy in the short term. The tools of backtesting and\nwalk-forward optimization were used to achieve such task. The data has been\ngenerated from a real European roulette wheel from an on-line casino based in\nRiga, Latvia. It has been recorded 10,980 spins and sent to the computer\nthrough a voice-to-text software for further numerical analysis in R. It has\nbeen observed that the probabilities of occurrence of the numbers at the\nroulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown\nthat a flat betting system against Kelly Criterion was more profitable in the\nshort term.\n"
    },
    {
        "paper_id": 1610.00256,
        "authors": "Andrew Green and Chris Kenyon",
        "title": "XVA at the Exercise Boundary",
        "comments": "15 pages, 8 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  XVA is a material component of a trade valuation and hence it must impact the\ndecision to exercise options within a given netting set. This is true for both\nunsecured trades and secured / cleared trades where KVA and MVA play a material\nrole even if CVA and FVA do not. However, this effect has frequently been\nignored in XVA models and indeed in exercise decisions made by option owners.\nThis paper describes how XVA impacts the exercise decision and how this can be\nreadily evaluated using regression techniques (Longstaff and Schwartz 2001).\nThe paper then assesses the materiality of the impact of XVA at the exercise\nboundary on swaption examples.\n"
    },
    {
        "paper_id": 1610.00259,
        "authors": "Rui Menezes and Sonia Bentes",
        "title": "Hysteresis and Duration Dependence of Financial Crises in the US:\n  Evidence from 1871-2016",
        "comments": "59 pages, 10 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study analyses the duration dependence of events that trigger volatility\npersistence in stock markets. Such events, in our context, are monthly spells\nof contiguous price decline or negative returns for the S&P500 stock market\nindex over the last 145 years. Factors known to affect the duration of these\nspells are the magnitude or intensity of the price decline, long-term interest\nrates and economic recessions, among others. The result of interest is the\nconditional probability of ending a spell of consecutive months over which\nstock market returns remain negative. In this study, we rely on continuous time\nsurvival models in order to investigate this question. Several specifications\nwere attempted, some of which under the proportional hazards assumption and\nothers under the accelerated failure time assumption. The best fit of the\nvarious models endeavored was obtained for the log-normal distribution. This\ndistribution yields a non-monotonic hazard function that increases up to a\nmaximum and then decreases. The peak is achieved 2-3 months after the spells\nonset with a hazard of around 0.9 or higher; this hazard then decays\nasymptotically to zero. Spells duration increase during recessions, when\ninterest rate rises and when price declines are more intense. The main\nconclusion is that short spells of negative returns appear to be mainly\nfrictional while long spells become structural and trigger hysteresis effects\nafter an initial period of adjustment. Although in line with our expectations,\nthese results may be of some importance for policy-makers.\n"
    },
    {
        "paper_id": 1610.00261,
        "authors": "Charles-Albert Lehalle and Othmane Mounjid",
        "title": "Limit Order Strategic Placement with Adverse Selection Risk and the Role\n  of Latency",
        "comments": "30 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is split in three parts: first we use labelled trade data to\nexhibit how market participants accept or not transactions via limit orders as\na function of liquidity imbalance; then we develop a theoretical stochastic\ncontrol framework to provide details on how one can exploit his knowledge on\nliquidity imbalance to control a limit order. We emphasis the exposure to\nadverse selection, of paramount importance for limit orders. For a participant\nbuying using a limit order: if the price has chances to go down the probability\nto be filled is high but it is better to wait a little more before the trade to\nobtain a better price. In a third part we show how the added value of\nexploiting a knowledge on liquidity imbalance is eroded by latency: being able\nto predict future liquidity consuming flows is of less use if you have not\nenough time to cancel and reinsert your limit orders. There is thus a rational\nfor market makers to be as fast as possible as a protection to adverse\nselection. Thanks to our optimal framework we can measure the added value of\nlatency to limit orders placement.\n  To authors' knowledge this paper is the first to make the connection between\nempirical evidences, a stochastic framework for limit orders including adverse\nselection, and the cost of latency. Our work is a first stone to shed light on\nthe roles of latency and adverse selection for limit order placement, within an\naccurate stochastic control framework.\n"
    },
    {
        "paper_id": 1610.00274,
        "authors": "Orazio Angelini, Matthieu Cristelli, Andrea Zaccaria, Luciano\n  Pietronero",
        "title": "The complex dynamics of products and its asymptotic properties",
        "comments": "20 pages, 5 figures, supporting information. This paper was published\n  on PLOS One on May 17, 2017",
        "journal-ref": "Angelini O, Cristelli M, Zaccaria A, Pietronero L (2017) The\n  complex dynamics of products and its asymptotic properties. PLOS ONE 12(5):\n  e0177360",
        "doi": "10.1371/journal.pone.0177360",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse global export data within the Economic Complexity framework. We\ncouple the new economic dimension Complexity, which captures how sophisticated\nproducts are, with an index called logPRODY, a measure of the income of the\nrespective exporters. Products' aggregate motion is treated as a 2-dimensional\ndynamical system in the Complexity-logPRODY plane. We find that this motion can\nbe explained by a quantitative model involving the competition on the markets,\nthat can be mapped as a scalar field on the Complexity-logPRODY plane and acts\nin a way akin to a potential. This explains the movement of products towards\nareas of the plane in which the competition is higher. We analyse market\ncomposition in more detail, finding that for most products it tends, over time,\nto a characteristic configuration, which depends on the Complexity of the\nproducts. This market configuration, which we called asymptotic, is\ncharacterized by higher levels of competition.\n"
    },
    {
        "paper_id": 1610.00312,
        "authors": "Oliver Pfante and Nils Bertschinger",
        "title": "Volatility Inference and Return Dependencies in Stochastic Volatility\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic volatility models describe stock returns $r_t$ as driven by an\nunobserved process capturing the random dynamics of volatility $v_t$. The\npresent paper quantifies how much information about volatility $v_t$ and future\nstock returns can be inferred from past returns in stochastic volatility models\nin terms of Shannon's mutual information.\n"
    },
    {
        "paper_id": 1610.00332,
        "authors": "Mikkel Bennedsen, Asger Lunde, Mikko S. Pakkanen",
        "title": "Decoupling the short- and long-term behavior of stochastic volatility",
        "comments": "48 pages, 7 figures, v3: major revision with increased emphasis on\n  noise-robust estimation, to appear in Journal of Financial Econometrics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new class of continuous-time models of the stochastic\nvolatility of asset prices. The models can simultaneously incorporate roughness\nand slowly decaying autocorrelations, including proper long memory, which are\ntwo stylized facts often found in volatility data. Our prime model is based on\nthe so-called Brownian semistationary process and we derive a number of\ntheoretical properties of this process, relevant to volatility modeling.\nApplying the models to realized volatility measures covering a vast panel of\nassets, we find evidence consistent with the hypothesis that time series of\nrealized measures of volatility are both rough and very persistent. Lastly, we\nillustrate the utility of the models in an extensive forecasting study; we find\nthat the models proposed in this paper outperform a wide array of benchmarks\nconsiderably, indicating that it pays off to exploit both roughness and\npersistence in volatility forecasting.\n"
    },
    {
        "paper_id": 1610.00395,
        "authors": "T. R. Hurd, Quentin H. Shao, Tuan Tran",
        "title": "Optimal Portfolios of Illiquid Assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the investment behaviour of a large unregulated\nfinancial institution (FI) with CARA risk preferences. It shows how the FI\noptimizes its trading to account for market illiquidity using an extension of\nthe Almgren-Chriss market impact model of multiple risky assets. This expected\nutility optimization problem over the set of adapted strategies turns out to\nhave the same solutions as a mean-variance optimization over deterministic\ntrading strategies. That means the optimal adapted trading strategy is both\ndeterministic and time-consistent. It is also found to have an explicit closed\nform that clearly displays interesting properties. For example, the classic\nconstant Merton portfolio strategy, a particular solution of the frictionless\nlimit of the problem, behaves like an attractor in the space of more general\nsolutions. The main effect of temporary market impact is to slow down the speed\nof convergence to this constant Merton portfolio. The effect of permanent\nmarket impact is to incentivize the FI to buy additional risky assets near the\nend of the period. This property, that we name the Ponzi property, is related\nto the creation and bursting of bubbles in the market. The proposed model can\nbe used as a stylized dynamic model of a typical FI in the study of the asset\nfire sale channel relevant to understanding systemic risk and financial\nstability.\n"
    },
    {
        "paper_id": 1610.00577,
        "authors": "Runhuan Feng, Alexey Kuznetsov, Fenghao Yang",
        "title": "Exponential functionals of Levy processes and variable annuity\n  guaranteed benefits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exponential functionals of Brownian motion have been extensively studied in\nfinancial and insurance mathematics due to their broad applications, for\nexample, in the pricing of Asian options. The Black-Scholes model is appealing\nbecause of mathematical tractability, yet empirical evidence shows that\ngeometric Brownian motion does not adequately capture features of market equity\nreturns. One popular alternative for modeling equity returns consists in\nreplacing the geometric Brownian motion by an exponential of a Levy process. In\nthis paper we use this latter model to study variable annuity guaranteed\nbenefits and to compute explicitly the distribution of certain exponential\nfunctionals.\n"
    },
    {
        "paper_id": 1610.00778,
        "authors": "Likuan Qin and Vadim Linetsky",
        "title": "Long-Term Factorization of Affine Pricing Kernels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper constructs and studies the long-term factorization of affine\npricing kernels into discounting at the rate of return on the long bond and the\nmartingale component that accomplishes the change of probability measure to the\nlong forward measure. The principal eigenfunction of the affine pricing kernel\ngermane to the long-term factorization is an exponential-affine function of the\nstate vector with the coefficient vector identified with the fixed point of the\nRiccati ODE. The long bond volatility and the volatility of the martingale\ncomponent are explicitly identified in terms of this fixed point. A range of\nexamples from the asset pricing literature is provided to illustrate the\ntheory.\n"
    },
    {
        "paper_id": 1610.00795,
        "authors": "Daniele Petrone and Vito Latora",
        "title": "A dynamic approach merging network theory and credit risk techniques to\n  assess systemic risk in financial networks",
        "comments": "8 pages, 5 figures, 1 table",
        "journal-ref": "Scientific Reports 8 (2018) 5561",
        "doi": "10.1038/s41598-018-23689-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The interconnectedness of financial institutions affects instability and\ncredit crises. To quantify systemic risk we introduce here the PD model, a\ndynamic model that combines credit risk techniques with a contagion mechanism\non the network of exposures among banks. A potential loss distribution is\nobtained through a multi-period Monte Carlo simulation that considers the\nprobability of default (PD) of the banks and their tendency of defaulting in\nthe same time interval. A contagion process increases the PD of banks exposed\ntoward distressed counterparties. The systemic risk is measured by statistics\nof the loss distribution, while the contribution of each node is quantified by\nthe new measures PDRank and PDImpact. We illustrate how the model works on the\nnetwork of the European Global Systemically Important Banks. For a certain\nrange of the banks' capital and of their assets volatility, our results reveal\nthe emergence of a strong contagion regime where lower default correlation\nbetween banks corresponds to higher losses. This is the opposite of the\ndiversification benefits postulated by standard credit risk models used by\nbanks and regulators who could therefore underestimate the capital needed to\novercome a period of crisis, thereby contributing to the financial system\ninstability.\n"
    },
    {
        "paper_id": 1610.00818,
        "authors": "Likuan Qin and Vadim Linetsky",
        "title": "The Long Bond, Long Forward Measure and Long-Term Factorization in\n  Heath-Jarrow-Morton Models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1411.3078",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proves existence of the long bond, long forward measure and\nlong-term factorization of the stochastic discount factor (SDF) of Alvarez and\nJermann (2005) and Hansen and Scheinkman (2009) in Heath-Jarrow-Morton (HJM)\nmodels in the function space framework of Filipovic (2001). A sufficient\ncondition on the weight in the Hilbert space of forward rate volatility curves\nis given that ensures existence of the long bond volatility process, the long\nbond process and the long-term factorization of the SDF into discounting at the\nrate of return on the long bond and a martingale component defining the long\nforward measure, the long-term limit of T-forward measures.\n"
    },
    {
        "paper_id": 1610.00937,
        "authors": "Juan F. Monge, Mercedes Landete and Jos\\'e L. Ruiz",
        "title": "Sharpe portfolio using a cross-efficiency evaluation",
        "comments": null,
        "journal-ref": "Data Science and Productivity Analytics. International Series in\n  Operations Research & Management Science, 2020",
        "doi": "10.1007/978-3-030-43384-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Sharpe ratio is a way to compare the excess returns (over the risk free\nasset) of portfolios for each unit of volatility that is generated by a\nportfolio. In this paper we introduce a robust Sharpe ratio portfolio under the\nassumption that the risk free asset is unknown. We propose a robust portfolio\nthat maximizes the Sharpe ratio when the risk free asset is unknown, but is\nwithin a given interval. To compute the best Sharpe ratio portfolio all the\nSharpe ratios for any risk free asset are considered and compared by using the\nso-called cross-efficiency evaluation. An explicit expression of the\nCross-Eficiency Sharpe ratio portfolio is presented when short selling is\nallowed.\n"
    },
    {
        "paper_id": 1610.00955,
        "authors": "Matheus Grasselli, Adrien Nguyen-Huu (LAMETA, CREST)",
        "title": "Inventory growth cycles with debt-financed investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a continuous-time stock-flow consistent model for inventory\ndynamics in an economy with firms, banks, and households. On the supply side,\nfirms decide on production based on adaptive expectations for sales demand and\na desired level of inventories. On the demand side, investment is determined as\na function of utilization and profitability and can be financed by debt,\nwhereas consumption is independently determined as a function of income and\nwealth. Prices adjust sluggishly to both changes in labour costs and inventory.\nDisequilibrium between expected sales and demand is absorbed by unplanned\nchanges in inventory. This results in a five-dimensional dynamical system for\nwage share, employment rate, private debt ratio, expected sales, and capacity\nutilization. We analyze two limiting cases: the long-run dynamics provides a\nversion of the Keen model with effective demand and varying inventories,\nwhereas the short-run dynamics gives rise to behaviour that we interpret as\nKitchin cycles.\n"
    },
    {
        "paper_id": 1610.00999,
        "authors": "Daniel Bartl",
        "title": "Exponential utility maximization under model uncertainty for unbounded\n  endowments",
        "comments": null,
        "journal-ref": "Annals of Applied Probability, 29(1), 577-612, 2019",
        "doi": "10.1214/18-AAP1428",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the robust exponential utility maximization problem in discrete\ntime: An investor maximizes the worst case expected exponential utility with\nrespect to a family of nondominated probabilistic models of her endowment by\ndynamically investing in a financial market, and statically in available\noptions. We show that, for any measurable random endowment (regardless of\nwhether the problem is finite or not) an optimal strategy exists, a dual\nrepresentation in terms of (calibrated) martingale measures holds true, and\nthat the problem satisfies the dynamic programming principle (in case of no\noptions). Further it is shown that the value of the utility maximization\nproblem converges to the robust superhedging price as the risk aversion\nparameter gets large, and examples of nondominated probabilistic models are\ndiscussed.\n"
    },
    {
        "paper_id": 1610.01149,
        "authors": "Qing Cai, Hai-Chuan Xu and Wei-Xing Zhou (ECUST)",
        "title": "Taylor's Law of temporal fluctuation scaling in stock illiquidity",
        "comments": "14 Latex pages including 4 figures and 5 tables",
        "journal-ref": "Fluctuation and Noise Letters 15 (4), 1650029 (2016)",
        "doi": "10.1142/S0219477516500292",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Taylor's law of temporal fluctuation scaling, variance $\\sim$ $a($mean$)^b$,\nis ubiquitous in natural and social sciences. We report for the first time\nconvincing evidence of a solid temporal fluctuation scaling law in stock\nilliquidity by investigating the mean-variance relationship of the\nhigh-frequency illiquidity of almost all stocks traded on the Shanghai Stock\nExchange (SHSE) and the Shenzhen Stock Exchange (SZSE) during the period from\n1999 to 2011. Taylor's law holds for A-share markets (SZSE Main Board, SZSE\nSmall & Mediate Enterprise Board, SZSE Second Board, and SHSE Main Board) and\nB-share markets (SZSE B-share and SHSE B-share). We find that the scaling\nexponent $b$ is greater than 2 for the A-share markets and less than 2 for the\nB-share markets. We further unveil that Taylor's law holds for stocks in 17\nindustry categories, in 28 industrial sectors and in 31 provinces and\ndirect-controlled municipalities with the majority of scaling exponents\n$b\\in(2,3)$. We also investigate the $\\Delta{t}$-min illiquidity and find that\nthe scaling exponent $b(\\Delta{t})$ increases logarithmically for small\n$\\Delta{t}$ values and decreases fast to a stable level.\n"
    },
    {
        "paper_id": 1610.01227,
        "authors": "Christopher W. Miller",
        "title": "A Duality Result for Robust Optimization with Expectation Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper demonstrates a practical method for computing the solution of an\nexpectation-constrained robust maximization problem with immediate applications\nto model-free no-arbitrage bounds and super-replication values for many\nfinancial derivatives. While the previous literature has connected\nsuper-replication values to a convex minimization problem whose objective\nfunction is related to a sequence of iterated concave envelopes, we show how\nthis whole process can be encoded in a single convex minimization problem. The\nnatural finite-dimensional approximation of this minimization problem results\nin an easily-implementable sparse linear program. We highlight this technique\nby obtaining no-arbitrage bounds on the prices of forward-starting options,\ncontinuously-monitored variance swaps, and discretely-monitored gamma swaps,\neach subject to observed bid-ask spreads of finitely-many vanilla options.\n"
    },
    {
        "paper_id": 1610.0127,
        "authors": "Joao Pedro Jerico and Renato Vicente",
        "title": "Information inefficiency in a random linear economy model",
        "comments": "5 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the effects of introducing information inefficiency in a model for a\nrandom linear economy with a representative consumer. This is done by\nconsidering statistical, instead of classical, economic general equilibria.\nEmploying two different approaches we show that inefficiency increases the\nconsumption set of a consumer but decreases her expected utility. In this\nscenario economic activity grows while welfare shrinks, that is the opposite of\nthe behavior obtained by considering a rational consumer.\n"
    },
    {
        "paper_id": 1610.01338,
        "authors": "Ananjan Bhattacharyya and Abhijeet Chandra",
        "title": "The Cross-section of Expected Returns on Penny Stocks: Are Low-hanging\n  Fruits Not-so Sweet?",
        "comments": "20 pages, 2 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study the determinants of expected returns on the listed\npenny stocks from two perspectives. Traditionally financial economics\nliterature has been devoted to study the macro and micro determinants of\nexpected returns on stocks (Subrahmanyam, 2010). Very few research has been\ncarried out on penny stocks (Liu, Rhee, & Zhang, 2011; Nofsinger & Verma,\n2014). Our study is an effort to contribute more empirical evidence on penny\nstocks in the emerging market context. We see the return dynamics of penny\nstocks from corporate governance perspective. Issues such as shareholding\npatters are considered to be of much significance when it comes to understand\nthe price movements. Using cross-sectional data on 167 penny stocks listed in\nthe National Stock Exchange of India, we show that (i) Returns of portfolio of\nlower market-cap penny stocks are significantly different(higher) than that of\nhigher market-cap penny stocks. (ii)Returns of portfolio lower P/E stocks are\nsignificantly different (higher) than that of higher P/E stocks. Similarly,\nreturns of portfolio of lower P/B stocks are significantly different (higher)\nthan that of higher P/B stocks, and returns of portfolio of lower priced penny\nstocks are significantly different(higher) than that of higher priced penny\nstocks. (iii) Trading volume differences due to alphabetism are insignificant.\n(iv)Differences in returns of portfolios based on beta and shareholding\npatterns are insignificant.\n"
    },
    {
        "paper_id": 1610.0145,
        "authors": "Xin Liu",
        "title": "Asset Pricing with Random Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes to model asset price dynamics with a mixture of diffusion\nprocesses where the instantaneous volatility of the underlying diffusion\nprocess contains a random vector. The marginal probability distributions of the\nproposed process can match exactly the risk-neutral distributions implied by\nboth spot vanilla options and forward start options. We can also derive the\nexplicit pricing formula for derivatives that have a closed-form solution under\nGeneralized Geometric Brownian Motion.\n"
    },
    {
        "paper_id": 1610.01645,
        "authors": "David R Walwyn",
        "title": "Administration Costs in the Management of Research Funds; A Case Study\n  of a Public Fund for the Promotion of Industrial Innovation",
        "comments": "16 pages, 5000 words, 2 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research funding agencies routinely use a proportion of their total revenues\nto support internal administration and marketing costs. The ratio of\nadministration to total costs, referred to as the administration ratio, is\nhighly variable and within any single fund depends on many factors including\nthe number and average size of projects and the overall efficiency of the\nfunding agency. In this study, the standard agency activities have been\nidentified and used to develop a model of administration costs against expected\noutcomes. In particular, the model has been designed to estimate the optimum\nportfolio success rate and administration ratio as a function of a range of key\ninput variables including the project size, the complexity of proposal\nevaluation and project management, the risk tolerance of the sponsor and the\ntargeted research domain.\n"
    },
    {
        "paper_id": 1610.01937,
        "authors": "Caroline Hillairet, Cody Hyndman, Ying Jiao, Renjie Wang",
        "title": "Trading against disorderly liquidation of a large position under\n  asymmetric information and market impact",
        "comments": "33 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider trading against a hedge fund or large trader that must liquidate\na large position in a risky asset if the market price of the asset crosses a\ncertain threshold. Liquidation occurs in a disorderly manner and negatively\nimpacts the market price of the asset. We consider the perspective of small\ninvestors whose trades do not induce market impact and who possess different\nlevels of information about the liquidation trigger mechanism and the market\nimpact. We classify these market participants into three types: fully informed,\npartially informed and uninformed investors. We consider the portfolio\noptimization problems and compare the optimal trading and wealth processes for\nthe three classes of investors theoretically and by numerical illustrations.\n"
    },
    {
        "paper_id": 1610.01946,
        "authors": "Seyed Amir Hejazi, Kenneth R. Jackson",
        "title": "Efficient Valuation of SCR via a Neural Network Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As part of the new regulatory framework of Solvency II, introduced by the\nEuropean Union, insurance companies are required to monitor their solvency by\ncomputing a key risk metric called the Solvency Capital Requirement (SCR). The\nofficial description of the SCR is not rigorous and has lead researchers to\ndevelop their own mathematical frameworks for calculation of the SCR. These\nframeworks are complex and are difficult to implement. Recently, Bauer et al.\nsuggested a nested Monte Carlo (MC) simulation framework to calculate the SCR.\nBut the proposed MC framework is computationally expensive even for a simple\ninsurance product. In this paper, we propose incorporating a neural network\napproach into the nested simulation framework to significantly reduce the\ncomputational complexity in the calculation. We study the performance of our\nneural network approach in estimating the SCR for a large portfolio of an\nimportant class of insurance products called Variable Annuities (VAs). Our\nexperiments show that the proposed neural network approach is both efficient\nand accurate.\n"
    },
    {
        "paper_id": 1610.02126,
        "authors": "Jianxi Su and Edward Furman",
        "title": "Multiple risk factor dependence structures: Copulas and related\n  properties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copulas have become an important tool in the modern best practice Enterprise\nRisk Management, often supplanting other approaches to modelling stochastic\ndependence. However, choosing the `right' copula is not an easy task, and the\ntemptation to prefer a tractable rather than a meaningful candidate from the\nencompassing copulas toolbox is strong. The ubiquitous applications of the\nGaussian copula is just one illuminating example.\n  Speaking generally, a `good' copula should conform to the problem at hand,\nallow for asymmetry in the domain of definition and exhibit some extent of tail\ndependence. In this paper we introduce and study a new class of Multiple Risk\nFactor (MRF) copula functions, which we show are exactly such. Namely, the MRF\ncopulas (1) arise from a number of meaningful default risk specification with\nstochastic default barriers, (2) are in general non-exchangeable and (3)\npossess a variety of tail dependences. That being said, the MRF copulas turn\nout to be surprisingly tractable analytically.\n"
    },
    {
        "paper_id": 1610.02456,
        "authors": "Zura Kakushadze",
        "title": "Volatility Smile as Relativistic Effect",
        "comments": "32 pages; a trivial typo corrected in footnote 1 on the title page,\n  no other changes; to appear in Physica A",
        "journal-ref": "Physica A 475 (2017) 59-76",
        "doi": "10.1016/j.physa.2017.02.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit formula for the probability distribution based on a\nrelativistic extension of Brownian motion. The distribution 1) is properly\nnormalized and 2) obeys the tower law (semigroup property), so we can construct\nmartingales and self-financing hedging strategies and price claims (options).\nThis model is a 1-constant-parameter extension of the Black-Scholes-Merton\nmodel. The new parameter is the analog of the speed of light in Special\nRelativity. However, in the financial context there is no \"speed limit\" and the\nnew parameter has the meaning of a characteristic diffusion speed at which\nrelativistic effects become important and lead to a much softer asymptotic\nbehavior, i.e., fat tails, giving rise to volatility smiles. We argue that a\nnonlocal stochastic description of such (Levy) processes is inadequate and\ndiscuss a local description from physics. The presentation is intended to be\npedagogical.\n"
    },
    {
        "paper_id": 1610.02863,
        "authors": "F Blasques, P Gorgi, S Koopman (CREATES), O Wintenberger (University\n  of Copenhagen, LSTA)",
        "title": "Feasible Invertibility Conditions for Maximum Likelihood Estimation for\n  Observation-Driven Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Invertibility conditions for observation-driven time series models often fail\nto be guaranteed in empirical applications. As a result, the asymptotic theory\nof maximum likelihood and quasi-maximum likelihood estimators may be\ncompromised. We derive considerably weaker conditions that can be used in\npractice to ensure the consistency of the maximum likelihood estimator for a\nwide class of observation-driven time series models. Our consistency results\nhold for both correctly specified and misspecified models. The practical\nrelevance of the theory is highlighted in a set of empirical examples. We\nfurther obtain an asymptotic test and confidence bounds for the unfeasible \"\ntrue \" invertibility region of the parameter space.\n"
    },
    {
        "paper_id": 1610.0294,
        "authors": "Ibrahim Ekren and H. Mete Soner",
        "title": "Constrained Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s00205-017-1178-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical duality theory of Kantorovich and Kellerer for the classical\noptimal transport is generalized to an abstract framework and a\ncharacterization of the dual elements is provided. This abstract generalization\nis set in a Banach lattice $\\cal{X}$ with a order unit. The primal problem is\ngiven as the supremum over a convex subset of the positive unit sphere of the\ntopological dual of $\\cal{X}$ and the dual problem is defined on the bi-dual of\n$\\cal{X}$. These results are then applied to several extensions of the\nclassical optimal transport.\n"
    },
    {
        "paper_id": 1610.0305,
        "authors": "Damien Ackerer and Thibault Vatter",
        "title": "Dependent Defaults and Losses with Factor Copula Models",
        "comments": "29 pages, 11 figures, 3 tables",
        "journal-ref": "Dependence Modeling, Volume 5, Issue 1, Pages 375-399, 2017",
        "doi": "10.1515/demo-2017-0022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a class of flexible and tractable static factor models for the\nterm structure of joint default probabilities, the factor copula models. These\nhigh-dimensional models remain parsimonious with pair-copula constructions, and\nnest many standard models as special cases. The loss distribution of a\nportfolio of contingent claims can be exactly and efficiently computed when\nindividual losses are discretely supported on a finite grid. Numerical examples\nstudy the key features affecting the loss distribution and multi-name credit\nderivatives prices. An empirical exercise illustrates the flexibility of our\napproach by fitting credit index tranche prices.\n"
    },
    {
        "paper_id": 1610.03086,
        "authors": "Julien Hok and Tat Lung Chan",
        "title": "Option pricing with Legendre polynomials",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we develop an option pricing method based on Legendre series expansion\nof the density function. The key insight, relying on the close relation of the\ncharacteristic function with the series coefficients, allows to recover the\ndensity function rapidly and accurately. Based on this representation for the\ndensity function, approximations formulas for pricing European type options are\nderived. To obtain highly accurate result for European call option, the\nimplementation involves integrating high degree Legendre polynomials against\nexponential function. Some numerical instabilities arise because of serious\nsubtractive cancellations in its formulation (96) in proposition 7.1. To\novercome this difficulty, we rewrite this quantity as solution of a\nsecond-order linear difference equation and solve it using a robust and stable\nalgorithm from Olver. Derivation of the pricing method has been accompanied by\nan error analysis. Errors bounds have been derived and the study relies more on\nsmoothness properties which are not provided by the payoff? functions, but\nrather by the density function of the underlying stochastic models. This is\nparticularly relevant for options pricing where the payoff of the contract are\ngenerally not smooth functions. The numerical experiments on a class of models\nwidely used in quantitative finance show exponential convergence.\n"
    },
    {
        "paper_id": 1610.0323,
        "authors": "R\\'uben Sousa, Ana Bela Cruzeiro, Manuel Guerra",
        "title": "Barrier Option Pricing under the 2-Hypergeometric Stochastic Volatility\n  Model",
        "comments": "22 pages. Accepted for publication in Journal of Computational and\n  Applied Mathematics",
        "journal-ref": null,
        "doi": "10.1016/j.cam.2017.06.034",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the pricing of financial options under the 2-hypergeometric\nstochastic volatility model. This is an analytically tractable model that\nreproduces the volatility smile and skew effects observed in empirical market\ndata.\n  Using a regular perturbation method from asymptotic analysis of partial\ndifferential equations, we derive an explicit and easily computable approximate\nformula for the pricing of barrier options under the 2-hypergeometric\nstochastic volatility model. The asymptotic convergence of the method is proved\nunder appropriate regularity conditions, and a multi-stage method for improving\nthe quality of the approximation is discussed. Numerical examples are also\nprovided.\n"
    },
    {
        "paper_id": 1610.03259,
        "authors": "Giuseppe Brandi, Riccardo Di Clemente, Giulio Cimini",
        "title": "Epidemics of Liquidity Shortages in Interbank Markets",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 507, pp\n  255-267, (2018)",
        "doi": "10.1016/j.physa.2018.05.104",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial contagion from liquidity shocks has being recently ascribed as a\nprominent driver of systemic risk in interbank lending markets. Building on\nstandard compartment models used in epidemics, in this work we develop an EDB\n(Exposed-Distressed-Bankrupted) model for the dynamics of liquidity shocks\nreverberation between banks, and validate it on electronic market for interbank\ndeposits data. We show that the interbank network was highly susceptible to\nliquidity contagion at the beginning of the 2007/2008 global financial crisis,\nand that the subsequent micro-prudential and liquidity hoarding policies\nadopted by banks increased the network resilience to systemic risk---yet with\nthe undesired side effect of drying out liquidity from the market. We finally\nshow that the individual riskiness of a bank is better captured by its network\ncentrality than by its participation to the market, along with the currently\ndebated concept of \"too interconnected to fail\".\n"
    },
    {
        "paper_id": 1610.03718,
        "authors": "J.D. Opdyke",
        "title": "Fast, Accurate, Straightforward Extreme Quantiles of Compound Loss\n  Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an easily implemented, fast, and accurate method for approximating\nextreme quantiles of compound loss distributions (frequency+severity) as are\ncommonly used in insurance and operational risk capital models. The\nInterpolated Single Loss Approximation (ISLA) of Opdyke (2014) is based on the\nwidely used Single Loss Approximation (SLA) of Degen (2010) and maintains two\nimportant advantages over its competitors: first, ISLA correctly accounts for a\ndiscontinuity in SLA that otherwise can systematically and notably bias the\nquantile (capital) approximation under conditions of both finite and infinite\nmean. Secondly, because it is based on a closed-form approximation, ISLA\nmaintains the notable speed advantages of SLA over other methods requiring\nalgorithmic looping (e.g. fast Fourier transform or Panjer recursion). Speed is\nimportant when simulating many quantile (capital) estimates, as is so often\nrequired in practice, and essential when simulations of simulations are needed\n(e.g. some power studies). The modified ISLA (MISLA) presented herein increases\nthe range of application across the severity distributions most commonly used\nin these settings, and it is tested against extensive Monte Carlo simulation\n(one billion years' worth of losses) and the best competing method (the\nperturbative expansion (PE2) of Hernandez et al., 2014) using twelve\nheavy-tailed severity distributions, some of which are truncated. MISLA is\nshown to be comparable to PE2 in terms of both speed and accuracy, and it is\narguably more straightforward to implement for the majority of Advanced\nMeasurement Approaches (AMA) banks that are already using SLA (and failing to\ntake into account its biasing discontinuity).\n"
    },
    {
        "paper_id": 1610.03769,
        "authors": "Zura Kakushadze",
        "title": "On Origins of Bubbles",
        "comments": "26 pages; a trivial typo corrected, no other changes",
        "journal-ref": "Journal of Risk & Control 4(1) (2017) 1-30",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss - in what is intended to be a pedagogical fashion - a criterion,\nwhich is a lower bound on a certain ratio, for when a stock (or a similar\ninstrument) is not a good investment in the long term, which can happen even if\nthe expected return is positive. The root cause is that prices are positive and\nhave skewed, long-tailed distributions, which coupled with volatility results\nin a long-run asymmetry. This relates to bubbles in stock prices, which we\ndiscuss using a simple binomial tree model, without resorting to the stochastic\ncalculus machinery. We illustrate empirical properties of the aforesaid ratio.\nLog of market cap and sectors appear to be relevant explanatory variables for\nthis ratio, while price-to-book ratio (or its log) is not. We also discuss a\nshort-term effect of volatility, to wit, the analog of Heisenberg's uncertainty\nprinciple in finance and a simple derivation thereof using a binary tree.\n"
    },
    {
        "paper_id": 1610.03936,
        "authors": "Thomas R. Hurd, James P. Gleeson, Sergey Melnik",
        "title": "A framework for analyzing contagion in assortative banking networks",
        "comments": "10 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1110.4312",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0170579",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a probabilistic framework that represents stylized banking\nnetworks with the aim of predicting the size of contagion events. Most previous\nwork on random financial networks assumes independent connections between\nbanks, whereas our framework explicitly allows for (dis)assortative edge\nprobabilities (e.g., a tendency for small banks to link to large banks). We\nanalyze default cascades triggered by shocking the network and find that the\ncascade can be understood as an explicit iterated mapping on a set of edge\nprobabilities that converges to a fixed point. We derive a cascade condition\nthat characterizes whether or not an infinitesimal shock to the network can\ngrow to a finite size cascade, in analogy to the basic reproduction number\n$R_0$ in epidemic modelling. The cascade condition provides an easily computed\nmeasure of the systemic risk inherent in a given banking network topology.\nUsing the percolation theory for random networks we also derive an analytic\nformula for the frequency of global cascades. Although the analytical methods\nare derived for infinite networks, we demonstrate using Monte Carlo simulations\nthe applicability of the results to finite-sized networks. We show that\nedge-assortativity, the propensity of nodes to connect to similar nodes, can\nhave a strong effect on the level of systemic risk as measured by the cascade\ncondition. However, the effect of assortativity on systemic risk is subtle, and\nwe propose a simple graph theoretic quantity, which we call the\ngraph-assortativity coefficient, that can be used to assess systemic risk.\n"
    },
    {
        "paper_id": 1610.03958,
        "authors": "Albert Altarovici and Max Reppen and H. Mete Soner",
        "title": "Optimal Consumption and Investment with Fixed and Proportional\n  Transaction Costs",
        "comments": "47 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical optimal investment and consumption problem with infinite\nhorizon is studied in the presence of transaction costs. Both proportional and\nfixed costs as well as general utility functions are considered. Weak dynamic\nprogramming is proved in the general setting and a comparison result for\npossibly discontinuous viscosity solutions of the dynamic programming equation\nis provided. Detailed numerical experiments illustrate several properties of\nthe optimal investment strategies.\n"
    },
    {
        "paper_id": 1610.04051,
        "authors": "N. Serhan Aydin",
        "title": "Time value of extra information against its timely value",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an interactive market setup with sequential auctions where\nagents receive variegated signals with a known deadline. The effects of\ndifferential information and mutual learning on the allocation of overall\nprofit \\& loss (P\\&L) and the pace of price discovery are analysed. We\ncharacterise the signal-based expected P\\&L of agents based on explicit\nformulae for the directional quality of the trading signal, and study the\noptimal trading pattern using dynamic programming and provided that there is a\ncommon anticipation by agents of gains from trade. We find evidence in favour\nof exploiting new information whenever it arrives, and market efficiency. Brief\nextensions of the problem to risk-adjusted gains as well as risk-averse agents\nare provided. We then introduce the `information-adjusted risk premium' and\nrecover the signal-based equilibrium price as the weighted average of the\nsignal-based individual prices with respect to the risk-aversion levels.\n"
    },
    {
        "paper_id": 1610.04085,
        "authors": "Marco Maggis, Thilo Meyer-Brandis and Gregor Svindland",
        "title": "The Fatou Closedness under Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a characterization in terms of Fatou closedness for weakly closed\nmonotone convex sets in the space of $\\mathcal{P}$-quasisure bounded random\nvariables, where $\\mathcal{P}$ is a (possibly non-dominated) class of\nprobability measures. Applications of our results lie within robust versions\nthe Fundamental Theorem of Asset Pricing or dual representation of convex risk\nmeasures.\n"
    },
    {
        "paper_id": 1610.04334,
        "authors": "Mikio Ito, Akihiko Noda, Tatsuma Wada",
        "title": "Time-Varying Comovement of Foreign Exchange Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A time-varying cointegration model for foreign exchange rates is presented.\nUnlike previous studies, we allow the loading matrix in the vector error\ncorrection (VEC) model to be varying over time. Because the loading matrix in\nthe VEC model is associated with the speed at which deviations from the\nlong-run relationship disappear, we propose a new degree of market comovement\\\nbased on the time-varying loading matrix to measure the strength or robustness\nof the long-run relationship over time. Since exchange rates are determined by\nmacrovariables, cointegration among exchange rates implies those macroeconomic\nvariables share common stochastic trends. Therefore, the proposed degree\nmeasures the degree of market comovement. Our main finding is that the market\ncomovement has become stronger over the past quarter century, but the rate at\nwhich market comovement strengthens is decreasing with two major turning\npoints: one in 1995 and the other one in 2008.\n"
    },
    {
        "paper_id": 1610.04458,
        "authors": "Zongjun Tan and Peter Tankov",
        "title": "Optimal trading policies for wind energy producer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal trading policies for a wind energy producer who aims to\nsell the future production in the open forward, spot, intraday and adjustment\nmarkets, and who has access to imperfect dynamically updated forecasts of the\nfuture production. We construct a stochastic model for the forecast evolution\nand determine the optimal trading policies which are updated dynamically as new\nforecast information becomes available. Our results allow to quantify the\nexpected future gain of the wind producer and to determine the economic value\nof the forecasts.\n"
    },
    {
        "paper_id": 1610.0476,
        "authors": "Oliver Pfante and Nils Bertschinger",
        "title": "Uncertainty Estimates in the Heston Model via Fisher Information",
        "comments": "29 pages, 11 figures. The text overlap of the first version with\n  arXiv:1609.02108 by other authors has been removed. It occurred in the\n  introduction (page 2 line 22-37) of the first version. A part of the\n  introduction of arXiv:1609.02108 was incidentally adopted. The issue is\n  removed in the second version: the introduction is independent and\n  arXiv:1609.02108 is cited properly",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the information content of European option prices about volatility\nin terms of the Fisher information matrix. We assume that observed option\nprices are centred on the theoretical price provided by Heston's model\ndisturbed by additive Gaussian noise. We fit the likelihood function on the\ncomponents of the VIX, i.e., near- and next-term put and call options on the\nS&P 500 with more than 23 days and less than 37 days to expiration and\nnon-vanishing bid, and compute their Fisher information matrices from the\nGreeks in the Heston model. We find that option prices allow reliable estimates\nof volatility with negligible uncertainty as long as volatility is large\nenough. Interestingly, if volatility drops below a critical value, inferences\nfrom option prices become impossible because Vega, the derivative of a European\noption w.r.t. volatility, nearly vanishes.\n"
    },
    {
        "paper_id": 1610.05018,
        "authors": "Kristoffer Lindensj\\\"o",
        "title": "An explicit formula for optimal portfolios in complete Wiener driven\n  markets: a functional It\\^o calculus approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a standard optimal investment problem in a complete financial\nmarket driven by a Wiener process and derive an explicit formula for the\noptimal portfolio process in terms of the vertical derivative from functional\nIt^o calculus. An advantage with this approach compared to the Malliavin\ncalculus approach is that it relies only on an integrability condition.\n"
    },
    {
        "paper_id": 1610.05171,
        "authors": "Jian-Xin Wu and Ling-Yun He",
        "title": "Urban-rural gap and poverty traps in China: A prefecture level analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Urban-rural gap and regional inequality are long standing problems in China\nand result in considerable number of studies. This paper examines the dynamic\nbehaviors of incomes for both urban and rural areas with a prefectural data\nset. The analysis is conducted by using a distribution dynamics approach, which\nhave advantages in examination on persistence, polarization and convergence\nclubs. The results show that persistence and immobility are the dominant\ncharacteristics in the income distribution dynamics. The prefectural urban and\nrural areas converge into their own steady states differentiated in income\nlevels. This pattern of urban-rural gap also exists in three regional groups,\nnamely the eastern, central and western regions. Examination on the dynamics of\nthe poorest areas shows that geographical poverty traps exist in both urban and\nrural prefectural areas. Our results indicate that more policy interventions\nare required to narrow down the urban-rural gap and to eliminate the poverty\ntraps in China.\n"
    },
    {
        "paper_id": 1610.05383,
        "authors": "Marcello Rambaldi, Vladimir Filimonov, Fabrizio Lillo",
        "title": "Detection of intensity bursts using Hawkes processes: an application to\n  high frequency financial data",
        "comments": null,
        "journal-ref": "Phys. Rev. E 97, 032318 (2018)",
        "doi": "10.1103/PhysRevE.97.032318",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a stationary point process, an intensity burst is defined as a short\ntime period during which the number of counts is larger than the typical count\nrate. It might signal a local non-stationarity or the presence of an external\nperturbation to the system. In this paper we propose a novel procedure for the\ndetection of intensity bursts within the Hawkes process framework. By using a\nmodel selection scheme we show that our procedure can be used to detect\nintensity bursts when both their occurrence time and their total number is\nunknown. Moreover, the initial time of the burst can be determined with a\nprecision given by the typical inter-event time. We apply our methodology to\nthe mid-price change in FX markets showing that these bursts are frequent and\nthat only a relatively small fraction is associated to news arrival. We show\nlead-lag relations in intensity burst occurrence across different FX rates and\nwe discuss their relation with price jumps.\n"
    },
    {
        "paper_id": 1610.05448,
        "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher",
        "title": "Generalization error minimization: a new approach to model evaluation\n  and selection with an application to penalized regression",
        "comments": "The theoretical generalization and extension of arXiv:1606.00142 and\n  arXiv:1609.03344",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study model evaluation and model selection from the perspective of\ngeneralization ability (GA): the ability of a model to predict outcomes in new\nsamples from the same population. We believe that GA is one way formally to\naddress concerns about the external validity of a model. The GA of a model\nestimated on a sample can be measured by its empirical out-of-sample errors,\ncalled the generalization errors (GE). We derive upper bounds for the GE, which\ndepend on sample sizes, model complexity and the distribution of the loss\nfunction. The upper bounds can be used to evaluate the GA of a model, ex ante.\nWe propose using generalization error minimization (GEM) as a framework for\nmodel selection. Using GEM, we are able to unify a big class of penalized\nregression estimators, including lasso, ridge and bridge, under the same set of\nassumptions. We establish finite-sample and asymptotic properties (including\n$\\mathcal{L}_2$-consistency) of the GEM estimator for both the $n \\geqslant p$\nand the $n < p$ cases. We also derive the $\\mathcal{L}_2$-distance between the\npenalized and corresponding unpenalized regression estimates. In practice, GEM\ncan be implemented by validation or cross-validation. We show that the GE\nbounds can be used for selecting the optimal number of folds in $K$-fold\ncross-validation. We propose a variant of $R^2$, the $GR^2$, as a measure of\nGA, which considers both both in-sample and out-of-sample goodness of fit.\nSimulations are used to demonstrate our key results.\n"
    },
    {
        "paper_id": 1610.05494,
        "authors": "Tiziano Squartini, Giulio Cimini, Andrea Gabrielli, Diego Garlaschelli",
        "title": "Network reconstruction via density sampling",
        "comments": "8 pages, 1 figure, 4 tables",
        "journal-ref": "App. Netw. Sci. 2 (3) (2017)",
        "doi": "10.1007/s41109-017-0021-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reconstructing weighted networks from partial information is necessary in\nmany important circumstances, e.g. for a correct estimation of systemic risk.\nIt has been shown that, in order to achieve an accurate reconstruction, it is\ncrucial to reliably replicate the empirical degree sequence, which is however\nunknown in many realistic situations. More recently, it has been found that the\nknowledge of the degree sequence can be replaced by the knowledge of the\nstrength sequence, which is typically accessible, complemented by that of the\ntotal number of links, thus considerably relaxing the observational\nrequirements. Here we further relax these requirements and devise a procedure\nvalid when even the the total number of links is unavailable. We assume that,\napart from the heterogeneity induced by the degree sequence itself, the network\nis homogeneous, so that its (global) link density can be estimated by sampling\nsubsets of nodes with representative density. We show that the best way of\nsampling nodes is the random selection scheme, any other procedure being biased\ntowards unrealistically large, or small, link densities. We then introduce our\ncore technique for reconstructing both the topology and the link weights of the\nunknown network in detail. When tested on real economic and financial data\nsets, our method achieves a remarkable accuracy and is very robust with respect\nto the sampled subsets, thus representing a reliable practical tool whenever\nthe available topological information is restricted to small portions of nodes.\n"
    },
    {
        "paper_id": 1610.05583,
        "authors": "Gesine A. Steudle, Saini Yang, Carlo C. Jaeger",
        "title": "Price Dynamics Via Expectations, and the Role of Money Therein",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Beyond its obvious macro-economic relevance, fiat money has important\nmicro-economic implications. They matter for addressing No. 8 in Smale's\n\"Mathematical Problems for the Next Century\": extend the mathematical model of\ngeneral equilibrium theory to include price adjustments. In the canonical\nArrow-Debreu framework, equilibrium prices are set by a fictitious auctioneer.\nRemoving that fiction raises the question of how prices are set and adjusted by\ndecentralised actors with incomplete information. We investigate this question\nthrough a very basic model where a unique factor of production, labour,\nproduces a single consumption good, called jelly for brevity. The point of the\nmodel is to study a price dynamics based on the firm's expectations about jelly\ndemand and labour supply. The system tends towards economic equilibrium,\nhowever, depending on the initial conditions it might not get there. In\ndifferent model versions, different kinds of money are introduced. Compared to\nthe case of no money, the introduction of money as a store of value facilitates\nthe system reaching economic equilibrium. If money is introduced as a third\ncommodity, i.e. there is also a demand for money, the system dynamics in\ngeneral becomes more complex.\n"
    },
    {
        "paper_id": 1610.05697,
        "authors": "Loretta Mastroeni and Pierluigi Vellucci",
        "title": "\"Butterfly Effect\" vs Chaos in Energy Futures Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we test for the sensitive dependence on initial conditions (the\nso called \"butterfly effect\") of energy futures time series (heating oil,\nnatural gas), and thus the determinism of those series. This paper is\ndistinguished from previous studies in the following points: first, we reread\nexistent works in the literature on energy markets, enlightening the role of\n\\emph{butterfly effect} in chaos definition (introduced by Devaney), using this\ndefinition to prevent us from misleading results about ostensible chaoticity of\nthe price series. Second, we test for the time series for sensitive dependence\non initial conditions, introducing a coefficient that describes the determinism\nrate of the series and that represents its reliability level (in percentage).\nThe introduction of this reliability level is motivated by the fact that time\nseries generated from stochastic systems also might show sensitive dependence\non initial conditions. According to this perspective, the maximum reliability\nlevel obtained here is too low to be able to ensure that there is strong\nevidence of sensitive The maximum reliability level obtained here was been\n$\\simeq 56\\% $, too low to ensure strong evidence of sensitive dependence on\ninitial conditions.\n"
    },
    {
        "paper_id": 1610.05703,
        "authors": "A. Belenky, L. Egorova",
        "title": "Two approaches to modeling the interaction of small and medium\n  price-taking traders with a stock exchange by mathematical programming\n  techniques",
        "comments": "88 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents two new approaches to modeling the interaction of small\nand medium pricetaking traders with a stock exchange. In the framework of these\napproaches, the traders can form and manage their portfolios of financial\ninstruments traded on a stock exchange with the use of linear, integer, and\nmixed programming techniques. Unlike previous authors publications on the\nsubject, besides standard securities, the present publication considers\nderivative financial instruments such as futures and options contracts.\n"
    },
    {
        "paper_id": 1610.05728,
        "authors": "Weston Barger, Matthew Lorig",
        "title": "Approximate pricing of European and Barrier claims in a local-stochastic\n  volatility setting",
        "comments": "25 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive asymptotic expansions for the prices of a variety of European and\nbarrier-style claims in a general local-stochastic volatility setting. Our\nmethod combines Taylor series expansions of the diffusion coefficients with an\nexpansion in the correlation parameter between the underlying asset and\nvolatility process. Rigorous accuracy results are provided for European-style\nclaims. For barrier-style claims, we include several numerical examples to\nillustrate the accuracy and versatility of our approximations.\n"
    },
    {
        "paper_id": 1610.05892,
        "authors": "F. Aleskerov, N. Meshcheryakova, S. Shvydun",
        "title": "Centrality measures in networks based on nodes attributes, long-range\n  interactions and group influence",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method for assessing agents influence in network structures,\nwhich takes into consideration nodes attributes, individual and group\ninfluences of nodes, and the intensity of interactions. This approach helps us\nto identify both explicit and hidden central elements which cannot be detected\nby classical centrality measures or other indices.\n"
    },
    {
        "paper_id": 1610.06805,
        "authors": "Amine Ismail (LPMA), Huy\\^en Pham (LPMA, CREST)",
        "title": "Robust Markowitz mean-variance portfolio selection under ambiguous\n  covariance matrix *",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a robust continuous-time Markowitz portfolio selection\npro\\-blem where the model uncertainty carries on the covariance matrix of\nmultiple risky assets. This problem is formulated into a min-max mean-variance\nproblem over a set of non-dominated probability measures that is solved by a\nMcKean-Vlasov dynamic programming approach, which allows us to characterize the\nsolution in terms of a Bellman-Isaacs equation in the Wasserstein space of\nprobability measures. We provide explicit solutions for the optimal robust\nportfolio strategies and illustrate our results in the case of uncertain\nvolatilities and ambiguous correlation between two risky assets. We then derive\nthe robust efficient frontier in closed-form, and obtain a lower bound for the\nSharpe ratio of any robust efficient portfolio strategy. Finally, we compare\nthe performance of Sharpe ratios for a robust investor and for an investor with\na misspecified model. MSC Classification: 91G10, 91G80, 60H30\n"
    },
    {
        "paper_id": 1610.07028,
        "authors": "Petr Jizba and Jan Korbel",
        "title": "Techniques for multifractal spectrum estimation in financial time series",
        "comments": "7 pages",
        "journal-ref": "Int J Des Nat Ecodyn 10(3), 2015, 261-266",
        "doi": "10.2495/DNE-V10-N3-261-266",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Multifractal analysis is one of the important approaches that enables us to\nmeasure the complexity of various data via the scaling properties. We compare\nthe most common techniques used for multifractal exponents estimation from both\ntheoretical and practical point of view. Particularly, we discuss the methods\nbased on estimation of R\\'enyi entropy, which provide a powerful tool\nespecially in presence of heavy-tailed data. To put some flesh on bare bones,\nall methods are compared on various real financial datasets, including daily\nand high-frequency data.\n"
    },
    {
        "paper_id": 1610.07131,
        "authors": "Pingjin Deng",
        "title": "Asymptotic of Non-Crossings probability of Additive Wiener Fields",
        "comments": "12 pages. arXiv admin note: text overlap with arXiv:1402.2620 by\n  other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $W_i=\\{W_i(t_i), t_i\\in \\R_+\\}, i=1,2,\\ldots,d$ are independent Wiener\nprocesses. $W=\\{W(\\mathbf{t}),t\\in \\R_+^d\\}$ be the additive Wiener field\ndefine as the sum of $W_i$. For any trend $f$ in $\\kHC$ (the reproducing kernel\nHilbert Space of $W$), we derive upper and lower bounds for the boundary\nnon-crossing probability $$P_f=P\\{\\sum_{i=1}^{d}W_i(t_i) +f(\\mathbf{t})\\leq\nu(\\mathbf{t}), \\mathbf{t}\\in\\R_+^d\\},$$ where $u: \\R_+^d\\rightarrow \\R_+$ is a\nmeasurable function. Furthermore, for large trend functions $\\gamma f>0$, we\nshow that the asymptotically relation $\\ln P_{\\gamma f}\\sim \\ln P_{\\gamma\n\\underline{f}}$ as $\\gamma \\to \\IF$, where $\\underline{f}$ is the projection of\n$f$ on some closed convex subset of $\\kHC$.\n"
    },
    {
        "paper_id": 1610.07287,
        "authors": "Shu-Peng Chen and Ling-Yun He",
        "title": "The asset price bubbles in emerging financial markets: a new statistical\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The bubble is a controversial and important issue. Many methods which based\non the rational expectation have been proposed to detect the bubble. However,\nfor some developing countries, epically China, the asset markets are so young\nthat for many companies, there are no dividends and fundamental value, making\nit difficult (if not impossible) to measure the bubbles by existing methods.\nTherefore, we proposed a simple but effective statistical method and three\nstatistics (that is, C, U, V) to capture and quantify asset price bubbles,\nespecially in immature emerging markets. To present a clear example of the\napplication of this method to real world problems, we also applied our method\nto re-examine empirically the asset price bubble in some stock markets. Our\nmain contributions to current literature are as follows: firstly, this method\ndoes not rely on fundamental value, the discount rates and dividends, therefore\nis applicable to the immature markets without the sufficient data of such\nkinds, secondly, this new method allows us to examine different influences\n(herding behavior, abnormal fluctuation and composite influence) of bubble. Our\nnew statistical approach is, to the best of our knowledge, the only robust way\nin existing literature to to quantify the asset price bubble especially in\nemerging markets.\n"
    },
    {
        "paper_id": 1610.07292,
        "authors": "Ling-Yun He and Xing-Chun Wen",
        "title": "Population growth, interest rate, and housing tax in the transitional\n  China",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.11.057",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper combines and develops the models in Lastrapes (2002) and Mankiw &\nWeil (1989), which enables us to analyze the effects of interest rate and\npopulation growth shocks on housing price in one integrated framework. Based on\nthis model, we carry out policy simulations to examine whether the housing\n(stock or flow) tax reduces the housing price fluctuations caused by interest\nrate or population growth shocks. Simulation results imply that the choice of\nhousing tax tools depends on the kind of shock that housing market faces. In\nthe situation where the housing price volatility is caused by the population\ngrowth shock, the flow tax can reduce the volatility of housing price while the\nstock tax makes no difference to it. If the shock is resulting from the\ninterest rate, the policy maker should not impose any kind of the housing\ntaxes. Furthermore, the effect of one kind of the housing tax can be\nstrengthened by that of the other type of housing tax.\n"
    },
    {
        "paper_id": 1610.07694,
        "authors": "Rongju Zhang, Nicolas Langren\\'e, Yu Tian, Zili Zhu, Fima Klebaner,\n  Kais Hamza",
        "title": "Dynamic portfolio optimization with liquidity cost and market impact: a\n  simulation-and-regression approach",
        "comments": "25 pages, 4 figures",
        "journal-ref": "Quantitative Finance 19(3) 519-532 (2019)",
        "doi": "10.1080/14697688.2018.1524155",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simulation-and-regression method for solving dynamic portfolio\nallocation problems in the presence of general transaction costs, liquidity\ncosts and market impacts. This method extends the classical least squares Monte\nCarlo algorithm to incorporate switching costs, corresponding to transaction\ncosts and transient liquidity costs, as well as multiple endogenous state\nvariables, namely the portfolio value and the asset prices subject to permanent\nmarket impacts. To do so, we improve the accuracy of the control randomization\napproach in the case of discrete controls, and propose a global iteration\nprocedure to further improve the allocation estimates. We validate our\nnumerical method by solving a realistic cash-and-stock portfolio with a\npower-law liquidity model. We quantify the certainty equivalent losses\nassociated with ignoring liquidity effects, and illustrate how our dynamic\nallocation protects the investor's capital under illiquid market conditions.\nLastly, we analyze, under different liquidity conditions, the sensitivities of\ncertainty equivalent returns and optimal allocations with respect to trading\nvolume, stock price volatility, initial investment amount, risk-aversion level\nand investment horizon.\n"
    },
    {
        "paper_id": 1610.08104,
        "authors": "Jo\\\"el Bun, Jean-Philippe Bouchaud and Marc Potters",
        "title": "Cleaning large correlation matrices: tools from random matrix theory",
        "comments": "165 pages, article submitted to Physics Reports",
        "journal-ref": null,
        "doi": "10.1016/j.physrep.2016.10.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This review covers recent results concerning the estimation of large\ncovariance matrices using tools from Random Matrix Theory (RMT). We introduce\nseveral RMT methods and analytical techniques, such as the Replica formalism\nand Free Probability, with an emphasis on the Marchenko-Pastur equation that\nprovides information on the resolvent of multiplicatively corrupted noisy\nmatrices. Special care is devoted to the statistics of the eigenvectors of the\nempirical correlation matrix, which turn out to be crucial for many\napplications. We show in particular how these results can be used to build\nconsistent \"Rotationally Invariant\" estimators (RIE) for large correlation\nmatrices when there is no prior on the structure of the underlying process. The\nlast part of this review is dedicated to some real-world applications within\nfinancial markets as a case in point. We establish empirically the efficacy of\nthe RIE framework, which is found to be superior in this case to all previously\nproposed methods. The case of additively (rather than multiplicatively)\ncorrupted noisy matrices is also dealt with in a special Appendix. Several open\nproblems and interesting technical developments are discussed throughout the\npaper.\n"
    },
    {
        "paper_id": 1610.08143,
        "authors": "Tim Leung and Zheng Wang",
        "title": "Optimal Risk-Averse Timing of an Asset Sale: Trending vs Mean-Reverting\n  Price Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal risk-averse timing to sell a risky asset. The\ninvestor's risk preference is described by the exponential, power, or log\nutility. Two stochastic models are considered for the asset price -- the\ngeometric Brownian motion and exponential Ornstein-Uhlenbeck models -- to\naccount for, respectively, the trending and mean-reverting price dynamics. In\nall cases, we derive the optimal thresholds and certainty equivalents to sell\nthe asset, and compare them across models and utilities, with emphasis on their\ndependence on asset price, risk aversion, and quantity. We find that the timing\noption may render the investor's value function and certainty equivalent\nnon-concave in price. Numerical results are provided to illustrate the\ninvestor's strategies and the premium associated with optimally timing to sell.\n"
    },
    {
        "paper_id": 1610.0823,
        "authors": "Zhi-Qiang Jiang (ECUST, BU), Gang-Jin Wang (HNU, BU), Askery Canabarro\n  (BU, UFA), Boris Podobnik (ZSEM), Chi Xie (HNU), H. Eugene Stanley (BU),\n  Wei-Xing Zhou (ECUST)",
        "title": "Short term prediction of extreme returns based on the recurrence\n  interval analysis",
        "comments": "18 pages, 5 figues, 3 tables",
        "journal-ref": "Quantitative Finance 18 (3), 353-370 (2018)",
        "doi": "10.1080/14697688.2017.1373843",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Being able to predict the occurrence of extreme returns is important in\nfinancial risk management. Using the distribution of recurrence intervals---the\nwaiting time between consecutive extremes---we show that these extreme returns\nare predictable on the short term. Examining a range of different types of\nreturns and thresholds we find that recurrence intervals follow a\n$q$-exponential distribution, which we then use to theoretically derive the\nhazard probability $W(\\Delta t |t)$. Maximizing the usefulness of extreme\nforecasts to define an optimized hazard threshold, we indicates a financial\nextreme occurring within the next day when the hazard probability is greater\nthan the optimized threshold. Both in-sample tests and out-of-sample\npredictions indicate that these forecasts are more accurate than a benchmark\nthat ignores the predictive signals. This recurrence interval finding deepens\nour understanding of reoccurring extreme returns and can be applied to forecast\nextremes in risk management.\n"
    },
    {
        "paper_id": 1610.08414,
        "authors": "Peter B. Lerner",
        "title": "The Fellowship of LIBOR: A Study of Spurious Interbank Correlations by\n  the Method of Wigner-Ville Function",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The manipulation of LIBOR by a group of banks became one of the major blows\nto the remaining confidence in financial industry. Yet, despite an enormous\namount of popular literature on the subject, rigorous time-series studies are\nfew. In my paper, I discuss the following hypothesis. Namely, if we should\nassume for a statistical null, the quotes, which were submitted by the member\nbanks were true, the deviations from the LIBOR should have been entirely random\nbecause they were determined by idiosyncratic conditions by the member banks.\nThis hypothesis can be statistically verified. Serial correlations of the\nrates, which cannot be explained by the differences in credit qualities of the\nmember banks or the domicile Governments, were subjected to correlation tests.\nA new econometric method--the analysis of the Wigner-Ville function borrowed\nfrom quantum mechanics and signal processing--is used and explained for the\nstatistical interpretation of regression residuals.\n"
    },
    {
        "paper_id": 1610.08415,
        "authors": "T.O. Benli",
        "title": "A Comparison of Various Electricity Tariff Price Forecasting Techniques\n  in Turkey and Identifying the Impact of Time Series Periods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is very vital for suppliers and distributors to predict the deregulated\nelectricity prices for creating their bidding strategies in the competitive\nmarket area. Pre requirement of succeeding in this field, accurate and suitable\nelectricity tariff price forecasting tools are needed. In the presence of\neffective forecasting tools, taking the decisions of production, merchandising,\nmaintenance and investment with the aim of maximizing the profits and benefits\ncan be successively and effectively done. According to the electricity demand,\nthere are four various electricity tariffs pricing in Turkey; monochromic, day,\npeak and night. The objective is find the best suitable tool for predicting the\nfour pricing periods of electricity and produce short term forecasts (one year\nahead-monthly). Our approach based on finding the best model, which ensures the\nsmallest forecasting error measurements of: MAPE, MAD and MSD. We conduct a\ncomparison of various forecasting approaches in total accounts for nine teen,\nat least all of those have different aspects of methodology. Our beginning step\nwas doing forecasts for the year 2015. We validated and analyzed the\nperformance of our best model and made comparisons to see how well the\nhistorical values of 2015 and forecasted data for that specific period matched.\nResults show that given the time-series data, the recommended models provided\ngood forecasts. Second part of practice, we also include the year 2015, and\ncompute all the models with the time series of January 2011 to December 2015.\nAgain by choosing the best appropriate forecasting model, we conducted the\nforecast process and also analyze the impact of enhancing of time series\nperiods (January 2007 to December 2015) to model that we used for forecasting\nprocess.\n"
    },
    {
        "paper_id": 1610.08416,
        "authors": "Jaroslaw Kwapien, Pawel Oswiecimka, Marcin Forczek, Stanislaw Drozdz",
        "title": "Minimum spanning tree filtering of correlations for varying time scales\n  and size of fluctuations",
        "comments": null,
        "journal-ref": "Phys. Rev. E 95, 052313 (2017)",
        "doi": "10.1103/PhysRevE.95.052313",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on a recently proposed $q$-dependent detrended cross-correlation\ncoefficient $\\rho_q$, we generalize the concept of minimum spanning tree (MST)\nby introducing a family of $q$-dependent minimum spanning trees ($q$MST) that\nare selective to cross-correlations between different fluctuation amplitudes\nand different time scales. They inherit this ability directly from the\ncoefficients $\\rho_q$ that are processed here to construct a distance matrix.\nConventional MST with detrending corresponds in this context to $q=2$. We apply\nthe $q$MSTs to sample empirical data from the stock market and discuss the\nresults. We show that the $q$MST graphs can complement $\\rho_q$ in\ndisentangling correlations that cannot be observed by the MST graphs based on\n$\\rho_{\\rm DCCA}$ and, therefore, they can be useful in many areas where the\nmultivariate cross-correlations are of interest. We apply our method to data\nfrom the stock market and obtain more information about correlation structure\nof the data than by using $q=2$ only. We show that two sets of signals that\ndiffer from each other statistically can give comparable trees for $q=2$, while\nonly by using the trees for $q \\ne 2$ we become able to distinguish between\nthese sets. We also show that a family of $q$MSTs for a range of $q$ express\nthe diversity of correlations in a manner resembling the multifractal analysis,\nwhere one computes a spectrum of the generalized fractal dimensions, the\ngeneralized Hurst exponents, or the multifractal singularity spectra: the more\ndiverse the correlations are, the more variable the tree topology is for\ndifferent $q$s. Our analysis exhibits that the stocks belonging to the same or\nsimilar industrial sectors are correlated via the fluctuations of moderate\namplitudes, while the largest fluctuations often happen to synchronize in those\nstocks that do not necessarily belong to the same industry.\n"
    },
    {
        "paper_id": 1610.08558,
        "authors": "Ankush Agarwal, Ronnie Sircar",
        "title": "Portfolio Benchmarking under Drawdown Constraint and Stochastic Sharpe\n  Ratio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor who seeks to maximize her expected utility derived\nfrom her terminal wealth relative to the maximum performance achieved over a\nfixed time horizon, and under a portfolio drawdown constraint, in a market with\nlocal stochastic volatility (LSV). In the absence of closed-form formulas for\nthe value function and optimal portfolio strategy, we obtain approximations for\nthese quantities through the use of a coefficient expansion technique and\nnonlinear transformations. We utilize regularity properties of the risk\ntolerance function to numerically compute the estimates for our approximations.\nIn order to achieve similar value functions, we illustrate that, compared to a\nconstant volatility model, the investor must deploy a quite different portfolio\nstrategy which depends on the current level of volatility in the stochastic\nvolatility model.\n"
    },
    {
        "paper_id": 1610.08644,
        "authors": "Oliver Janke",
        "title": "Utility Maximization and Indifference Value under Risk and Information\n  Constraints for a Market with a Change Point",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider an optimization problem of expected utility\nmaximization of continuous-time trading in a financial market. This trading is\nconstrained by a benchmark for a utility-based shortfall risk measure. The\nmarket consists of one asset whose price process is modeled by a Geometric\nBrownian motion where the market parameters change at a random time. The\ninformation flow is modeled by initially and progressively enlarged filtrations\nwhich represent the knowledge about the price process, the Brownian motion and\nthe random time. We solve the maximization problem and give the optimal\nterminal wealth depending on these different filtrations for general utility\nfunctions by using martingale representation results for the corresponding\nfiltration. Moreover, for a special utility function and risk measure we\ncalculate the utility indifference value which measures the gain of further\ninformation for the investor.\n"
    },
    {
        "paper_id": 1610.08676,
        "authors": "F. Clementi, M. Gallegati, G. Kaniadakis, S. Landini",
        "title": "$\\kappa$-generalized models of income and wealth distributions: A survey",
        "comments": "LaTeX2e; 22 pages with 7 figures",
        "journal-ref": "The European Physical Journal Special Topics, Vol: 225, Issue: 10,\n  October 2016, pp: 1959-1984",
        "doi": "10.1140/epjst/e2016-60014-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper provides a survey of results related to the \"$\\kappa$-generalized\ndistribution\", a statistical model for the size distribution of income and\nwealth. Topics include, among others, discussion of basic analytical\nproperties, interrelations with other statistical distributions as well as\naspects that are of special interest in the income distribution field, such as\nthe Gini index and the Lorenz curve. An extension of the basic model that is\nmost able to accommodate the special features of wealth data is also reviewed.\nThe survey of empirical applications given in this paper shows the\n$\\kappa$-generalized models of income and wealth to be in excellent agreement\nwith the observed data in many cases.\n"
    },
    {
        "paper_id": 1610.08732,
        "authors": "P. Salminen, L. Vostrikova",
        "title": "On exponential functionals of processes with independent increments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the exponential functionals of the processes $X$ with\nindependent increments , namely $$I_t= \\int _0^t\\exp(-X_s)ds, _,\\,\\, t\\geq 0,$$\nand also $$I_{\\infty}= \\int _0^{\\infty}\\exp(-X_s)ds.$$ When $X$ is a\nsemi-martingale with absolutely continuous characteristics, we derive recurrent\nintegral equations for Mellin transform ${\\bf E}( I_t^{\\alpha})$,\n$\\alpha\\in\\mathbb{R}$, of the integral functional $I_t$. Then we apply these\nrecurrent formulas to calculate the moments. We present also the corresponding\nresults for the exponential functionals of Levy processes, which hold under\nless restrictive conditions then in the paper of Bertoin, Yor (2005). In\nparticular, we obtain an explicit formula for the moments of $I_t$ and\n$I_{\\infty}$, and we precise the exact number of finite moments of\n$I_{\\infty}$.\n"
    },
    {
        "paper_id": 1610.08767,
        "authors": "Shiyu Han, Lan Wu and Yuan Cheng",
        "title": "Equity Market Impact Modeling: an Empirical Analysis for Chinese Market",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market impact has become a subject of increasing concern among academics and\nindustry experts. We put forward a price impact model which considers the\nheteroscedasticity of price in the time dimension and dependency between\npermanent impact and temporary impact. We discuss and derive the extremum of\nthe expectation of permanent impact and realized impact by constructing several\nspecial trading trajectories. Given our use of a large trade and quote tick\nrecords of 17,213,238,343 compiled from the Chinese stock market, the model\nassessment ultimately suggest that our model is better than Almgren's model.\nInterestingly, the result of random effect analysis indicates the parameter\n$\\alpha$, which is the exponent of the impact function, is a constant with a\nvalue of around 0.7 across all stocks. Our model and empirical result would\ngive academia some insight of mechanism of Chinese market, and can be applied\nto algorithm trading.\n"
    },
    {
        "paper_id": 1610.08782,
        "authors": "W. Farkas, A. Smirnow",
        "title": "Intrinsic risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monetary risk measures are usually interpreted as the smallest amount of\nexternal capital that must be added to a financial position to make it\nacceptable. We propose a new concept: intrinsic risk measures and argue that\nthis approach provides a direct path from unacceptable positions towards the\nacceptance set. Intrinsic risk measures use only internal resources and return\nthe smallest percentage of the currently held financial position which has to\nbe sold and reinvested into an eligible asset such that the resulting position\nbecomes acceptable. While avoiding the problem of infinite values, intrinsic\nrisk measures allow a free choice of the eligible asset and they preserve\ndesired properties such as monotonicity and quasi-convexity. A dual\nrepresentation on convex acceptance sets is derived and the link of intrinsic\nrisk measures to their monetary counterparts on cones is detailed.\n"
    },
    {
        "paper_id": 1610.08806,
        "authors": "Niushan Gao, Denny H. Leung, Foivos Xanthos",
        "title": "Closedness of convex sets in Orlicz spaces with applications to dual\n  representation of risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $(\\Phi,\\Psi)$ be a conjugate pair of Orlicz functions. A set in the\nOrlicz space $L^\\Phi$ is said to be order closed if it is closed with respect\nto dominated convergence of sequences of functions. A well known problem\narising from the theory of risk measures in financial mathematics asks whether\norder closedness of a convex set in $L^\\Phi$ characterizes closedness with\nrespect to the topology $\\sigma(L^\\Phi,L^\\Psi)$. (See [26, p.3585].) In this\npaper, we show that for a norm bounded convex set in $L^\\Phi$, order closedness\nand $\\sigma(L^\\Phi,L^\\Psi)$-closedness are indeed equivalent. In general,\nhowever, coincidence of order closedness and $\\sigma(L^\\Phi,L^\\Psi)$-closedness\nof convex sets in $L^\\Phi$ is equivalent to the validity of the Krein-Smulian\nTheorem for the topology $\\sigma(L^\\Phi,L^\\Psi)$; that is, a convex set is\n$\\sigma(L^\\Phi,L^\\Psi)$-closed if and only if it is closed with respect to the\nbounded-$\\sigma(L^\\Phi,L^\\Psi)$ topology. As a result, we show that order\nclosedness and $\\sigma(L^\\Phi,L^\\Psi)$-closedness of convex sets in $L^\\Phi$\nare equivalent if and only if either $\\Phi$ or $\\Psi$ satisfies the\n$\\Delta_2$-condition. Using this, we prove the surprising result that: \\emph{If\n(and only if) $\\Phi$ and $\\Psi$ both fail the $\\Delta_2$-condition, then there\nexists a coherent risk measure on $L^\\Phi$ that has the Fatou property but\nfails the Fenchel-Moreau dual representation with respect to the dual pair\n$(L^\\Phi, L^\\Psi)$}. A similar analysis is carried out for the dual pair of\nOrlicz hearts $(H^\\Phi,H^\\Psi)$.\n"
    },
    {
        "paper_id": 1610.08818,
        "authors": "Raphael Benichou, Yves Lemp\\'eri\\`ere, Emmanuel S\\'eri\\'e, Julien\n  Kockelkoren, Philip Seager, Jean-Philippe Bouchaud and Marc Potters",
        "title": "Agnostic Risk Parity: Taming Known and Unknown-Unknowns",
        "comments": "12 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markowitz' celebrated optimal portfolio theory generally fails to deliver\nout-of-sample diversification. In this note, we propose a new portfolio\nconstruction strategy based on symmetry arguments only, leading to \"Eigenrisk\nParity\" portfolios that achieve equal realized risk on all the principal\ncomponents of the covariance matrix. This holds true for any other definition\nof uncorrelated factors. We then specialize our general formula to the most\nagnostic case where the indicators of future returns are assumed to be\nuncorrelated and of equal variance. This \"Agnostic Risk Parity\" (AGP) portfolio\nminimizes unknown-unknown risks generated by over-optimistic hedging of the\ndifferent bets. AGP is shown to fare quite well when applied to standard\ntechnical strategies such as trend following.\n"
    },
    {
        "paper_id": 1610.08878,
        "authors": "Martin Forde and Hongzhong Zhang",
        "title": "Asymptotics for rough stochastic volatility models",
        "comments": "The argument for the case of unbounded volatility was incorrect\n  because Prob(Lambda_H(eps^H B^H)>c) = 1 i.e. the probability that the rate\n  function of the realized re-scaled fBM path is infinite is 1",
        "journal-ref": null,
        "doi": "10.1137/15M1009330",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the large deviation principle (LDP) for a re-scaled fractional Brownian\nmotion $B^H_t$ where the rate function is defined via the reproducing kernel\nHilbert space, we compute small-time asymptotics for a correlated fractional\nstochastic volatility model of the form $dS_t=S_t\\sigma(Y_t) (\\bar{\\rho} dW_t\n+\\rho dB_t), \\,dY_t=dB^H_t$ where $\\sigma$ is $\\alpha$-H\\\"{o}lder continuous\nfor some $\\alpha\\in(0,1]$; in particular, we show that $t^{H-\\frac{1}{2}} \\log\nS_t $ satisfies the LDP as $t\\to0$ and the model has a well-defined implied\nvolatility smile as $t \\to 0$, when the log-moneyness $k(t)=x\nt^{\\frac{1}{2}-H}$. Thus the smile steepens to infinity or flattens to zero\ndepending on whether $H\\in(0,\\frac{1}{2})$ or $H\\in(\\frac{1}{2},1)$. We also\ncompute large-time asymptotics for a fractional local-stochastic volatility\nmodel of the form: $dS_t= S_t^{\\beta} |Y_t|^p dW_t,dY_t=dB^H_t$, and we\ngeneralize two identities in Matsumoto&Yor05 to show that $\\frac{1}{t^{2H}}\\log\n\\frac{1}{t}\\int_0^t e^{2 B^H_s} ds$ and $\\frac{1}{t^{2H}}(\\log \\int_0^t\ne^{2(\\mu s+B^H_s)} ds-2 \\mu t)$ converge in law to $ 2\\mathrm{max}_{0 \\le s \\le\n1} B^H_{s}$ and $2B_1$ respectively for $H \\in (0,\\frac{1}{2})$ and $\\mu>0$ as\n$t \\to \\infty$.\n"
    },
    {
        "paper_id": 1610.08918,
        "authors": "Maciej Jagielski, Kordian Czy\\.zewski, Ryszard Kutner, H. Eugene\n  Stanley",
        "title": "Income and wealth distribution of the richest Norwegian individuals: An\n  inequality analysis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.01.077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the empirical data from the Norwegian tax office, we analyse the wealth\nand income of the richest individuals in Norway during the period 2010--2013.\nWe find that both annual income and wealth level of the richest individuals are\ndescribable using the Pareto law. We find that the robust mean Pareto exponent\nover the four-year period to be $\\approx 2.3$ for income and $\\approx 1.5$ for\nwealth.\n"
    },
    {
        "paper_id": 1610.08921,
        "authors": "Maciej Jagielski, Ryszard Kutner, Didier Sornette",
        "title": "Theory of earthquakes interevent times applied to financial markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.115",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the probability density function (PDF) of waiting times between\nfinancial loss exceedances. The empirical PDFs are fitted with the self-excited\nHawkes conditional Poisson process with a long power law memory kernel. The\nHawkes process is the simplest extension of the Poisson process that takes into\naccount how past events influence the occurrence of future events. By analyzing\nthe empirical data for 15 different financial assets, we show that the\nformalism of the Hawkes process used for earthquakes can successfully model the\nPDF of interevent times between successive market losses.\n"
    },
    {
        "paper_id": 1610.09085,
        "authors": "Takuji Arai and Yuto Imai",
        "title": "On the difference between locally risk-minimizing and delta hedging\n  strategies for exponential L\\'evy models",
        "comments": "11 pages and 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the difference between locally risk-minimizing and delta hedging\nstrategies for exponential L\\'evy models, where delta hedging strategies in\nthis paper are defined under the minimal martingale measure. We give firstly\nmodel-independent upper estimations for the difference. In addition we show\nnumerical examples for two typical exponential L\\'evy models: Merton models and\nvariance gamma models.\n"
    },
    {
        "paper_id": 1610.09124,
        "authors": "Beatrice Acciaio, Alexander M.G. Cox, Martin Huesmann",
        "title": "Model-independent pricing with insider information: a Skorokhod\n  embedding approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the pricing and hedging of a financial derivative\nfor an insider trader, in a model-independent setting. In particular, we\nsuppose that the insider wants to act in a way which is independent of any\nmodelling assumptions, but that she observes market information in the form of\nthe prices of vanilla call options on the asset. We also assume that both the\ninsider's information, which takes the form of a set of impossible paths, and\nthe payoff of the derivative are time-invariant. This setup allows us to adapt\nrecent work of Beiglboeck, Cox and Huesmann (2016) to prove duality results and\na monotonicity principle, which enables us to determine geometric properties of\nthe optimal models. Moreover, we show that this setup is powerful, in that we\nare able to find analytic and numerical solutions to certain pricing and\nhedging problems.\n"
    },
    {
        "paper_id": 1610.0923,
        "authors": "Ariel Neufeld, Mario Sikic",
        "title": "Robust Utility Maximization in Discrete-Time Markets with Friction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust stochastic optimization problem in the quasi-sure setting\nin discrete-time. We show that under a lineality-type condition the problem\nadmits a maximizer. This condition is implied by the no-arbitrage condition in\nmodels of financial markets. As a corollary, we obtain existence of an utility\nmaximizer in the frictionless market model, markets with proportional\ntransaction costs and also more general convex costs, like in the case of\nmarket impact.\n"
    },
    {
        "paper_id": 1610.09234,
        "authors": "Peter Bank and Yan Dolinsky",
        "title": "Super-Replication with Fixed Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study super--replication of contingent claims in markets with fixed\ntransaction costs. This can be viewed as a stochastic impulse control problem\nwith a terminal state constraint. The first result in this paper reveals that\nin reasonable continuous time financial market models the super--replication\nprice is prohibitively costly and leads to trivial buy--and--hold strategies.\nOur second result derives nontrivial scaling limits of super--replication\nprices for binomial models with small fixed costs.\n"
    },
    {
        "paper_id": 1610.09292,
        "authors": "Taras Bodnar, Ostap Okhrin, Nestor Parolya",
        "title": "Optimal Shrinkage Estimator for High-Dimensional Mean Vector",
        "comments": "20 pages, UPDATE2: revised version of the manuscript accepted for\n  publication in Journal of Multivariate Analysis",
        "journal-ref": null,
        "doi": "10.1016/j.jmva.2018.07.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive the optimal linear shrinkage estimator for the\nhigh-dimensional mean vector using random matrix theory. The results are\nobtained under the assumption that both the dimension $p$ and the sample size\n$n$ tend to infinity in such a way that $p/n \\to c\\in(0,\\infty)$. Under weak\nconditions imposed on the underlying data generating mechanism, we find the\nasymptotic equivalents to the optimal shrinkage intensities and estimate them\nconsistently. The proposed nonparametric estimator for the high-dimensional\nmean vector has a simple structure and is proven to minimize asymptotically,\nwith probability $1$, the quadratic loss when $c\\in(0,1)$. When $c\\in(1,\n\\infty)$ we modify the estimator by using a feasible estimator for the\nprecision covariance matrix. To this end, an exhaustive simulation study and an\napplication to real data are provided where the proposed estimator is compared\nwith known benchmarks from the literature. It turns out that the existing\nestimators of the mean vector, including the new proposal, converge to the\nsample mean vector when the true mean vector has an unbounded Euclidean norm.\n"
    },
    {
        "paper_id": 1610.09306,
        "authors": "Michael R. Tehranchi",
        "title": "Calls, zonoids, peacocks and log-concavity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main results are two characterisations of log-concave densities in terms\nof the collection of lift zonoids corresponding to a peacock. These notions are\nrecalled and connected to arbitrage-free asset pricing in financial\nmathematics.\n"
    },
    {
        "paper_id": 1610.09384,
        "authors": "M.A. Milevsky and T.S. Salisbury",
        "title": "Equitable retirement income tontines: Mixing cohorts without\n  discriminating",
        "comments": null,
        "journal-ref": "Astin Bulletin 46 (2016), 571-604",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is growing interest in the design of pension annuities that insure\nagainst idiosyncratic longevity risk while pooling and sharing systematic risk.\nThis is partially motivated by the desire to reduce capital and reserve\nrequirements while retaining the value of mortality credits; see for example\nPiggott, Valdez and Detzel (2005) or Donnelly, Guillen and Nielsen (2014). In\nthis paper we generalize the natural retirement income tontine introduced by\nMilevsky and Salisbury (2015) by combining heterogeneous cohorts into one pool.\nWe engineer this scheme by allocating tontine shares at either a premium or a\ndiscount to par based on both the age of the investor and the amount they\ninvest. For example, a 55 year-old allocating $\\$10,000$ to the tontine might\nbe told to pay $\\$$200 per share and receive 50 shares, while a 75 year-old\nallocating $\\$8,000$ might pay $\\$$40 per share and receive 200 shares. They\nwould all be mixed together into the same tontine pool and each tontine share\nwould have equal income rights. The current paper addresses existence and\nuniqueness issues and discusses the conditions under which this scheme can be\nconstructed equitably -- which is distinct from fairly -- even though it isn't\noptimal for any cohort. As such, this also gives us the opportunity to compare\nand contrast various pooling schemes that have been proposed in the literature\nand to differentiate between arrangements that are socially equitable, vs.\nactuarially fair vs. economically optimal.\n"
    },
    {
        "paper_id": 1610.09403,
        "authors": "Kevin Guo and Tim Leung",
        "title": "Understanding the Non-Convergence of Agricultural Futures via Stochastic\n  Storage Costs and Timing Options",
        "comments": "24 pages, Journal of Commodity Markets (forthcoming 2017)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the market phenomenon of non-convergence between futures\nand spot prices in the grains market. We postulate that the positive basis\nobserved at maturity stems from the futures holder's timing options to exercise\nthe shipping certificate delivery item and subsequently liquidate the physical\ngrain. In our proposed approach, we incorporate stochastic spot price and\nstorage cost, and solve an optimal double stopping problem to give the optimal\nstrategies to exercise and liquidate the grain. Our new models for stochastic\nstorage rates lead to explicit no-arbitrage prices for the shipping certificate\nand associated futures contract. We calibrate our models to empirical futures\ndata during the periods of observed non-convergence, and illustrate the premium\ngenerated by the shipping certificate.\n"
    },
    {
        "paper_id": 1610.09404,
        "authors": "Kevin Guo and Tim Leung",
        "title": "Understanding the Tracking Errors of Commodity Leveraged ETFs",
        "comments": null,
        "journal-ref": "Commodities, Energy & Environmental Finance, Fields Institute\n  Communications, R. Aid et al. Editors, pp.39-63, Springer, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Commodity exchange-traded funds (ETFs) are a significant part of the rapidly\ngrowing ETF market. They have become popular in recent years as they provide\ninvestors access to a great variety of commodities, ranging from precious\nmetals to building materials, and from oil and gas to agricultural products. In\nthis article, we analyze the tracking performance of commodity leveraged ETFs\nand discuss the associated trading strategies. It is known that leveraged ETF\nreturns typically deviate from their tracking target over longer holding\nhorizons due to the so-called volatility decay. This motivates us to construct\na benchmark process that accounts for the volatility decay, and use it to\nexamine the tracking performance of commodity leveraged ETFs. From empirical\ndata, we find that many commodity leveraged ETFs underperform significantly\nagainst the benchmark, and we quantify such a discrepancy via the novel idea of\n\\emph{realized effective fee}. Finally, we consider a number of trading\nstrategies and examine their performance by backtesting with historical price\ndata.\n"
    },
    {
        "paper_id": 1610.09519,
        "authors": "Zhi-Qiang Jiang (ECUST, BU), Xing-Lu Gao (ECUST), Wei-Xing Zhou\n  (ECUST), H. Eugene Stanley (BU)",
        "title": "Multifractal cross wavelet analysis",
        "comments": null,
        "journal-ref": "Fractals 25 (6), 1750054 (2017)",
        "doi": "10.1142/S0218348X17500542",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex systems are composed of mutually interacting components and the\noutput values of these components are usually long-range cross-correlated. We\npropose a method to characterize the joint multifractal nature of such\nlong-range cross correlations based on wavelet analysis, termed multifractal\ncross wavelet analysis (MFXWT). We assess the performance of the MFXWT method\nby performing extensive numerical experiments on the dual binomial measures\nwith multifractal cross correlations and the bivariate fractional Brownian\nmotions (bFBMs) with monofractal cross correlations. For binomial multifractal\nmeasures, the empirical joint multifractality of MFXWT is found to be in\napproximate agreement with the theoretical formula. For bFBMs, MFXWT may\nprovide spurious multifractality because of the wide spanning range of the\nmultifractal spectrum. We also apply the MFXWT method to stock market indexes\nand uncover intriguing joint multifractal nature in pairs of index returns and\nvolatilities.\n"
    },
    {
        "paper_id": 1610.09542,
        "authors": "Nils Detering, Thilo Meyer-Brandis, Konstantinos Panagiotou and Daniel\n  Ritter",
        "title": "Managing Default Contagion in Inhomogeneous Financial Networks",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to quantify and manage systemic risk caused by\ndefault contagion in the interbank market. We model the market as a random\ndirected network, where the vertices represent financial institutions and the\nweighted edges monetary exposures between them. Our model captures the strong\ndegree of heterogeneity observed in empirical data and the parameters can\neasily be fitted to real data sets. One of our main results allows us to\ndetermine the impact of local shocks, where initially some banks default, to\nthe entire system and the wider economy. Here the impact is measured by some\nindex of total systemic importance of all eventually defaulted institutions. As\na central application, we characterize resilient and non-resilient cases. In\nparticular, for the prominent case where the network has a degree sequence\nwithout second moment, we show that a small number of initially defaulted banks\ncan trigger a substantial default cascade. Our results complement and extend\nsignificantly earlier findings derived in the configuration model where the\nexistence of a second moment of the degree distribution is assumed. As a second\nmain contribution, paralleling regulatory discussions, we determine minimal\ncapital requirements for financial institutions sufficient to make the network\nresilient to small shocks. An appealing feature of these capital requirements\nis that they can be determined locally by each institution without knowing the\ncomplete network structure as they basically only depend on the institution's\nexposures to its counterparties.\n"
    },
    {
        "paper_id": 1610.09622,
        "authors": "Karel in 't Hout, Radoslav Valkov",
        "title": "Numerical study of splitting methods for American option valuation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the numerical approximation of American-style option\nvalues governed by partial differential complementarity problems. For a variety\nof one- and two-asset American options we investigate by ample numerical\nexperiments the temporal convergence behaviour of three modern splitting\nmethods: the explicit payoff approach, the Ikonen-Toivanen approach and the\nPeaceman-Rachford method. In addition, the temporal accuracy of these splitting\nmethods is compared to that of the penalty approach.\n"
    },
    {
        "paper_id": 1610.09714,
        "authors": "Teh Raihana Nazirah Roslan, Wenjun Zhang, Jiling Cao",
        "title": "Pricing variance swaps with stochastic volatility and stochastic\n  interest rate under full correlation structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the case of pricing discretely-sampled variance swaps\nunder the class of equity-interest rate hybridization. Our modeling framework\nconsists of the equity which follows the dynamics of the Heston stochastic\nvolatility model, and the stochastic interest rate is driven by the\nCox-Ingersoll-Ross (CIR) process with full correlation structure imposed among\nthe state variables. This full correlation structure possess the limitation to\nhave fully analytical pricing formula for hybrid models of variance swaps, due\nto the non-affinity property embedded in the model itself. We address this\nissue by obtaining an efficient semi-closed form pricing formula of variance\nswaps for an approximation of the hybrid model via the derivation of\ncharacteristic functions. Subsequently, we implement numerical experiments to\nevaluate the accuracy of our pricing formula. Our findings confirmed that the\nimpact of the correlation between the underlying and the interest rate is\nsignificant for pricing discretely-sampled variance swaps.\n"
    },
    {
        "paper_id": 1610.09734,
        "authors": "Thibaut Lux, Antonis Papapantoleon",
        "title": "Model-free bounds on Value-at-Risk using extreme value information and\n  statistical distances",
        "comments": "22 pages, revised version with new title",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive bounds on the distribution function, therefore also on the\nValue-at-Risk, of $\\varphi(\\mathbf X)$ where $\\varphi$ is an aggregation\nfunction and $\\mathbf X = (X_1,\\dots,X_d)$ is a random vector with known\nmarginal distributions and partially known dependence structure. More\nspecifically, we analyze three types of available information on the dependence\nstructure: First, we consider the case where extreme value information, such as\nthe distributions of partial minima and maxima of $\\mathbf X$, is available. In\norder to include this information in the computation of Value-at-Risk bounds,\nwe utilize a reduction principle that relates this problem to an optimization\nproblem over a standard Fr\\'echet class, which can then be solved by means of\nthe rearrangement algorithm or using analytical results. Second, we assume that\nthe copula of $\\mathbf X$ is known on a subset of its domain, and finally we\nconsider the case where the copula of $\\mathbf X$ lies in the vicinity of a\nreference copula as measured by a statistical distance. In order to derive\nValue-at-Risk bounds in the latter situations, we first improve the\nFr\\'echet--Hoeffding bounds on copulas so as to include this additional\ninformation on the dependence structure. Then, we translate the improved\nFr\\'echet--Hoeffding bounds to bounds on the Value-at-Risk using the so-called\nimproved standard bounds. In numerical examples we illustrate that the\nadditional information typically leads to a significant improvement of the\nbounds compared to the marginals-only case.\n"
    },
    {
        "paper_id": 1610.09812,
        "authors": "Zhongxing Wang, Yan Yan and Xiaosong Chen",
        "title": "Long-range Correlation and Market Segmentation in Bond Market",
        "comments": "19 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.066",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper looks into the analysis of the long-range auto-correlations and\ncross-correlations in bond market. Based on Detrended Moving Average (DMA)\nmethod, empirical results present a clear evidence of long-range persistence\nthat exists in one year scale. The degree of long-range correlation related to\nmaturities has an upward tendency with a peak in short term. These findings\nconfirm the expectations of fractal market hypothesis (FMH). Furthermore, we\nhave developed a method based on a complex network to study the long-range\ncross-correlation structure and apply it to our data, and found a clear pattern\nof market segmentation in the long run. We also detected the nature of\nlong-range correlation in the sub-period 2007 to 2012 and 2011 to 2016. The\nresult from our research shows that long-range auto-correlations are decreasing\nin the recent years while long-range cross-correlations are strengthening.\n"
    },
    {
        "paper_id": 1610.09875,
        "authors": "Eckhard Platen and David Taylor",
        "title": "Loading Pricing of Catastrophe Bonds and Other Long-Dated,\n  Insurance-Type Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Catastrophe risk is a major threat faced by individuals, companies, and\nentire economies. Catastrophe (CAT) bonds have emerged as a method to offset\nthis risk and a corresponding literature has developed that attempts to provide\na market-consistent pricing methodology for these and other long-dated,\ninsurance-type contracts. This paper aims to unify and generalize several of\nthe widely-used pricing approaches for long-dated contracts with a focus on\nstylized CAT bonds and market-consistent valuation. It proposes a loading\npricing concept that combines the theoretically possible minimal price of a\ncontract with its formally obtained risk neutral price, without creating\neconomically meaningful arbitrage. A loading degree controls how much influence\nthe formally obtained risk neutral price has on the market price. A key finding\nis that this loading degree has to be constant for a minimally fluctuating\ncontract, and is an important, measurable characteristic for prices of\nlong-dated contracts. Loading pricing allows long-dated, insurance-type\ncontracts to be priced less expensively and with higher return on investment\nthan under classical pricing approaches. Loading pricing enables insurance\ncompanies to accumulate systematically reserves needed to manage its risk of\nruin in a market consistent manner.\n"
    },
    {
        "paper_id": 1610.09904,
        "authors": "Pierre Cardaliaguet (CEREMADE), Charles-Albert Lehalle",
        "title": "Mean Field Game of Controls and An Application To Trade Crowding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we formulate the now classical problem of optimal liquidation\n(or optimal trading) inside a Mean Field Game (MFG). This is a noticeable\nchange since usually mathematical frameworks focus on one large trader in front\nof a \"background noise\" (or \"mean field\"). In standard frameworks, the\ninteractions between the large trader and the price are a temporary and a\npermanent market impact terms, the latter influencing the public price. In this\npaper the trader faces the uncertainty of fair price changes too but not only.\nHe has to deal with price changes generated by other similar market\nparticipants, impacting the prices permanently too, and acting strategically.\nOur MFG formulation of this problem belongs to the class of \"extended MFG\", we\nhence provide generic results to address these \"MFG of controls\", before\nsolving the one generated by the cost function of optimal trading. We provide a\nclosed form formula of its solution, and address the case of \"heterogenous\npreferences\" (when each participant has a different risk aversion). Last but\nnot least we give conditions under which participants do not need to\ninstantaneously know the state of the whole system, but can \"learn\" it day\nafter day, observing others' behaviors.\n"
    },
    {
        "paper_id": 1610.10029,
        "authors": "Bernhard K. Meister",
        "title": "Meta-CTA Trading Strategies based on the Kelly Criterion",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The influence of Commodity Trading Advisors (CTA) on the price process is\nexplored with the help of a simple model. CTA managers are taken to be Kelly\noptimisers, which invest a fixed proportion of their assets in the risky asset\nand the remainder in a riskless asset. This requires regular adjustment of the\nportfolio weights as prices evolve. The CTA trading activity impacts the price\nchange in the form of a power law. These two rules governing investment ratios\nand price impact are combined and lead through updating at fixed time intervals\nto a deterministic price dynamic. For different choices of the model parameters\none gets qualitatively different dynamics. The result can be expressed as a\nphase diagram. Meta-CTA strategies can be devised to exploit the predictability\ninherent in the model dynamics by avoiding critical areas of the phase diagram\nor by taking a contrarian position at an opportune time.\n"
    },
    {
        "paper_id": 1610.10078,
        "authors": "Moshe A. Milevsky and Thomas S. Salisbury",
        "title": "Optimal retirement income tontines",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1307.2824",
        "journal-ref": "Insurance: Mathematics and Economics 64 (2015), pp. 91-105",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tontines were once a popular type of mortality-linked investment pool. They\npromised enormous rewards to the last survivors at the expense of those died\nearly. And, while this design appealed to the gambling instinc}, it is a\nsuboptimal way to generate retirement income. Indeed, actuarially-fair life\nannuities making constant payments -- where the insurance company is exposed to\nlongevity risk -- induce greater lifetime utility. However, tontines do not\nhave to be structured the historical way, i.e. with a constant cash flow shared\namongst a shrinking group of survivors. Moreover, insurance companies do not\nsell actuarially-fair life annuities, in part due to aggregate longevity risk.\n  We derive the tontine structure that maximizes lifetime utility. Technically\nspeaking we solve the Euler-Lagrange equation and examine its sensitivity to\n(i.) the size of the tontine pool $n$, and (ii.) individual longevity risk\naversion $\\gamma$. We examine how the optimal tontine varies with $\\gamma$ and\n$n$, and prove some qualitative theorems about the optimal payout.\nInterestingly, Lorenzo de Tonti's original structure is optimal in the limit as\nlongevity risk aversion $\\gamma \\to \\infty$. We define the natural tontine as\nthe function for which the payout declines in exact proportion to the survival\nprobabilities, which we show is near-optimal for all $\\gamma$ and $n$. We\nconclude by comparing the utility of optimal tontines to the utility of loaded\nlife annuities under reasonable demographic and economic conditions and find\nthat the life annuity's advantage over the optimal tontine is minimal.\n  In sum, this paper's contribution is to (i.) rekindle a discussion about a\nretirement income product that has been long neglected, and (ii.) leverage\neconomic theory as well as tools from mathematical finance to design the next\ngeneration of tontine annuities.\n"
    },
    {
        "paper_id": 1611.00156,
        "authors": "Zichong Li and Pengyu Huang",
        "title": "Globalization Process in Emerging Capital Markets -- Lessons and\n  Implications to China",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since 2002 when China first introduced QFII (Qualified Foreign Institutional\nInvestors) system, QFII has been developing in China for 14 years, during when\nRQFII, Shanghai-Hongkong Stock Connect Program, Shanghai-London Stock Connect\nProgram furthur broadened the avenue for foreign capital to invest in Chinese\nSecurity Market. As FTA (Free Trade Area) Financial Reform Program emerged, RMB\n(CNY) Capital Project is likely to make the currency exchangeable. With the\nsuccess in QFII, RQFII and Shanghai-Hongkong Stock Connect Program, China's\nlong term advantage in interest rate, and the relatively low stock index value\nafter the recent stock market crashes in mid 2015 and early 2016, foreign\ncapitals' demand for Chinese market to loosen its restrictions continually\nincreases. This article picks the three most representative emerging capital\nmarkets in the world, namely Taiwan, Korea and India, by comparing and\nanalyzing their paths of globalization, attempts to shed light on China's next\nsteps regarding globalization.\n"
    },
    {
        "paper_id": 1611.00316,
        "authors": "Bertram D\\\"uring, Christof Heuer",
        "title": "Essentially high-order compact schemes with application to stochastic\n  volatility models on non-uniform grids",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present high-order compact schemes for a linear second-order parabolic\npartial differential equation (PDE) with mixed second-order derivative terms in\ntwo spatial dimensions. The schemes are applied to option pricing PDE for a\nfamily of stochastic volatility models. We use a non-uniform grid with more\ngrid-points around the strike price. The schemes are fourth-order accurate in\nspace and second-order accurate in time for vanishing correlation. In our\nnumerical convergence study we achieve fourth-order accuracy also for non-zero\ncorrelation. A combination of Crank-Nicolson and BDF-4 discretisation is\napplied in time. Numerical examples confirm that a standard, second-order\nfinite difference scheme is significantly outperformed.\n"
    },
    {
        "paper_id": 1611.00389,
        "authors": "Nicola Cantarutti, Jo\\~ao Guerra, Manuel Guerra, Maria do Ros\\'ario\n  Grossinho",
        "title": "Option pricing in exponential L\\'evy models with transaction costs",
        "comments": null,
        "journal-ref": "Journal of Computational Finance, 23(5), 2020, 1-32",
        "doi": "10.21314/JCF.2020.384",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an approach for pricing European call options in presence of\nproportional transaction costs, when the stock price follows a general\nexponential L\\'{e}vy process. The model is a generalization of the celebrated\nwork of Davis, Panas and Zariphopoulou (1993), where the value of the option is\ndefined as the utility indifference price. This approach requires the solution\nof two stochastic singular control problems in finite horizon, satisfying the\nsame Hamilton-Jacobi-Bellman equation, with different terminal conditions. We\nintroduce a general formulation for these portfolio selection problems, and\nthen we focus on the special case in which the probability of default is\nignored. We solve numerically the optimization problems using the Markov chain\napproximation method and show results for diffusion, Merton and Variance Gamma\nprocesses. Option prices are computed for both the writer and the buyer.\n"
    },
    {
        "paper_id": 1611.00464,
        "authors": "Ivan Guo, Gregoire Loeper",
        "title": "Pricing Bounds for VIX Derivatives via Least Squares Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Derivatives on the Chicago Board Options Exchange volatility index (VIX) have\ngained significant popularity over the last decade. The pricing of VIX\nderivatives involves evaluating the square root of the expected realised\nvariance which cannot be computed by direct Monte Carlo methods. Least squares\nMonte Carlo methods can be used but the sign of the error is difficult to\ndetermine. In this paper, we propose new model independent upper and lower\npricing bounds for VIX derivatives. In particular, we first present a general\nstochastic duality result on payoffs involving concave functions. This is then\napplied to VIX derivatives along with minor adjustments to handle issues caused\nby the square root function. The upper bound involves the evaluation of a\nvariance swap, while the lower bound involves estimating a martingale increment\ncorresponding to its hedging portfolio. Both can be achieved simultaneously\nusing a single linear least square regression. Numerical results show that the\nmethod works very well for VIX futures, calls and puts under a wide range of\nparameter choices.\n"
    },
    {
        "paper_id": 1611.00723,
        "authors": "Arnab Chatterjee, Asim Ghosh, and Bikas K Chakrabarti",
        "title": "Socio-economic inequality and prospects of institutional Econophysics",
        "comments": "To appear in \"Economic Foundations for Social Complexity Science:\n  Theory, Sentiments, and Empirical Laws\", Eds. Alan Kirman & Yuji Aruka,\n  Springer Nature (2017). arXiv admin note: substantial text overlap with\n  arXiv:1606.03261",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Socio-economic inequality is measured using various indices. The Gini ($g$)\nindex, giving the overall inequality is the most commonly used, while the\nrecently introduced Kolkata ($k$) index gives a measure of $1-k$ fraction of\npopulation who possess top $k$ fraction of wealth in the society. This article\nreviews the character of such inequalities, as seen from a variety of data\nsources, the apparent relationship between the two indices, and what toy models\ntell us. These socio-economic inequalities are also investigated in the context\nof man-made social conflicts or wars, as well as in natural disasters. Finally,\nwe forward a proposal for an international institution with sufficient fund for\nvisitors, where natural and social scientists from various institutions of the\nworld can come to discuss, debate and formulate further developments.\n"
    },
    {
        "paper_id": 1611.00885,
        "authors": "Maria do Rosario Grossinho, Yaser Kord Faghan, Daniel Sevcovic",
        "title": "Pricing Perpetual Put Options by the Black-Scholes Equation with a\n  Nonlinear Volatility Function",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate qualitative and quantitative behavior of a solution of the\nmathematical model for pricing American style of perpetual put options. We\nassume the option price is a solution to the stationary generalized\nBlack-Scholes equation in which the volatility function may depend on the\nsecond derivative of the option price itself. We prove existence and uniqueness\nof a solution to the free boundary problem. We derive a single implicit\nequation for the free boundary position and the closed form formula for the\noption price. It is a generalization of the well-known explicit closed form\nsolution derived by Merton for the case of a constant volatility. We also\npresent results of numerical computations of the free boundary position, option\nprice and their dependence on model parameters.\n"
    },
    {
        "paper_id": 1611.00897,
        "authors": "Zhi-Qiang Jiang (ECUST, BU), Yan-Hong Yang (ECUST, BU), Gang-Jin Wang\n  (HNU, BU), and Wei-Xing Zhou (ECUST)",
        "title": "Joint multifractal analysis based on wavelet leaders",
        "comments": "11 pages and 5 figures. arXiv admin note: text overlap with\n  arXiv:1610.09519",
        "journal-ref": "Frontiers of Physics 12 (6), 128907 (2017)",
        "doi": "10.1007/s11467-017-0674-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mutually interacting components form complex systems and the outputs of these\ncomponents are usually long-range cross-correlated. Using wavelet leaders, we\npropose a method of characterizing the joint multifractal nature of these\nlong-range cross correlations, a method we call joint multifractal analysis\nbased on wavelet leaders (MF-X-WL). We test the validity of the MF-X-WL method\nby performing extensive numerical experiments on the dual binomial measures\nwith multifractal cross correlations and the bivariate fractional Brownian\nmotions (bFBMs) with monofractal cross correlations. Both experiments indicate\nthat MF-X-WL is capable to detect the cross correlations in synthetic data with\nacceptable estimating errors. We also apply the MF-X-WL method to the pairs of\nseries from financial markets (returns and volatilities) and online worlds\n(online numbers of different genders and different societies) and find an\nintriguing joint multifractal behavior.\n"
    },
    {
        "paper_id": 1611.0097,
        "authors": "Michael Rolfes and Alex \"Sandy\" Pentland",
        "title": "Working Paper on Organizational Dynamics within Corporate Venture\n  Capital Firms",
        "comments": "11 pages, 3 figures, full text of piece highlighted in HBR (November\n  2016)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corporate venture capital is in the midst of a renaissance. The end of 2015\nmarked all-time highs both in the number of corporate firms participating in VC\ndeals and in the amount of capital being deployed by corporate VCs. This paper\nexplores, rather than defines, how these firms find success in the wake of this\nsudden influx of corporate investors. A series of interviews was conducted in\norder to capture the direct and indirect objectives, philosophies, and modes of\noperation within some of these corporate VC organizations. During the course of\nthis exploration, numerous operational coherency issues were discovered. Many\nfirms were implicitly incentivizing conflicting and inconsistent behavior among\ntheir investment team. Perhaps most surprising, the worst offenders were the\nmore mature corporate VCs who have been in the game for some time. As will be\ndiscussed, fundamental evidence suggests that this misalignment is due to lack\nof attention and commitment at the executive level as corporate strategy\nevolves.\n"
    },
    {
        "paper_id": 1611.00997,
        "authors": "M. Abeille, E. Serie, A. Lazaric and X. Brokmann",
        "title": "LQG for portfolio optimization",
        "comments": "20 pages, 6 figures, submitted to Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a generic solver for dynamic portfolio allocation problems when\nthe market exhibits return predictability, price impact and partial\nobservability. We assume that the price modeling can be encoded into a linear\nstate-space and we demonstrate how the problem then falls into the LQG\nframework. We derive the optimal control policy and introduce analytical tools\nthat preserve the intelligibility of the solution. Furthermore, we link the\nexistence and uniqueness of the optimal controller to a dynamical non-arbitrage\ncriterion. Finally, we illustrate our method using a synthetic portfolio\nallocation problem.\n"
    },
    {
        "paper_id": 1611.0128,
        "authors": "S\\\"oren Christensen, Albrecht Irle, Andreas Ludwig",
        "title": "Optimal portfolio selection under vanishing fixed transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, asymptotic results in a long-term growth rate portfolio\noptimization model under both fixed and proportional transaction costs are\nobtained. More precisely, the convergence of the model when the fixed costs\ntend to zero is investigated. A suitable limit model with purely proportional\ncosts is introduced and an optimal strategy is shown to consist of keeping the\nrisky fraction process in a unique interval $[A,B]\\subseteq\\,]0,1[$ with\nminimal effort. Furthermore, the convergence of optimal boundaries, asymptotic\ngrowth rates, and optimal risky fraction processes is rigorously proved. The\nresults are based on an in-depth analysis of the convergence of the solutions\nto the corresponding HJB-equations.\n"
    },
    {
        "paper_id": 1611.01285,
        "authors": "Enrico G. De Giorgi and Ola Mahmoud",
        "title": "Naive Diversification Preferences and their Representation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A widely applied diversification paradigm is the naive diversification choice\nheuristic. It stipulates that an economic agent allocates equal decision\nweights to given choice alternatives independent of their individual\ncharacteristics. This article provides mathematically and economically sound\nchoice theoretic foundations for the naive approach to diversification. We\naxiomatize naive diversification by defining it as a preference for equality\nover inequality and derive its relationship to the classical diversification\nparadigm. In particular, we show that (i) the notion of permutation invariance\nlies at the core of naive diversification and that an economic agent is a naive\ndiversifier if and only if his preferences are convex and permutation\ninvariant; (ii) Schur-concave utility functions capture the idea of being\ninequality averse on top of being risk averse; and (iii) the transformations,\nwhich rebalance unequal decision weights to equality, are characterized in\nterms of their implied turnover.\n"
    },
    {
        "paper_id": 1611.01379,
        "authors": "Bertram D\\\"uring, Christian Hendricks, James Miles",
        "title": "Sparse grid high-order ADI scheme for option pricing in stochastic\n  volatility models",
        "comments": "17 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1512.02529",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a sparse grid high-order alternating direction implicit (ADI)\nscheme for option pricing in stochastic volatility models. The scheme is\nsecond-order in time and fourth-order in space. Numerical experiments confirm\nthe computational efficiency gains achieved by the sparse grid combination\ntechnique.\n"
    },
    {
        "paper_id": 1611.01381,
        "authors": "Omar A. Guerrero and Ulrich Matter",
        "title": "Revealing the Anatomy of Vote Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cooperation in the form of vote trading, also known as logrolling, is central\nfor law-making processes, shaping the development of democratic societies.\nEmpirical evidence of logrolling is scarce and limited to highly specific\nsituations because existing methods are not easily applicable to broader\ncontexts. We have developed a general and scalable methodology for revealing a\nnetwork of vote traders, allowing us to measure logrolling on a large scale.\nAnalysis on more than 9 million votes spanning 40 years in the U.S. Congress\nreveals a higher logrolling prevalence in the Senate and an overall decreasing\ntrend over recent congresses, coincidental with high levels of political\npolarization. Our method is applicable in multiple contexts, shedding light on\nmany aspects of logrolling and opening new doors in the study of hidden\ncooperation.\n"
    },
    {
        "paper_id": 1611.0144,
        "authors": "Francesca Biagini, Andrea Mazzon, Thilo Meyer-Brandis",
        "title": "Liquidity induced asset bubbles via flows of ELMMs",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a constructive model for asset price bubbles, where the market\nprice $W$ is endogenously determined by the trading activity on the market and\nthe fundamental price $W^F$ is exogenously given, as in the work of Jarrow,\nProtter and Roch (2012). To justify $W^F$ from a fundamental point of view, we\nembed this constructive approach in the martingale theory of bubbles, see\nJarrow, Protter and Shimbo (2010) and Biagini, F\\\"ollmer and Nedelcu (2014), by\nshowing the existence of a flow of equivalent martingale measures for $W$,\nunder which $W^F$ equals the expectation of the discounted future cash flow. As\nan application, we study bubble formation and evolution in a financial network.\n"
    },
    {
        "paper_id": 1611.01463,
        "authors": "Nonthachote Chatsanga, Andrew J. Parkes",
        "title": "International Portfolio Optimisation with Integrated Currency Overlay\n  Costs and Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eswa.2017.04.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimisation typically aims to provide an optimal allocation that\nminimises risk, at a given return target, by diversifying over different\ninvestments. However, the potential scope of such risk diversification can be\nlimited if investments are concentrated in only one country, or more\nspecifically one currency. Multi-currency portfolio is an alternative to\nachieve higher returns and more diversified portfolios but it requires a\ncareful management of the entailed risks from changes in exchange rates.\n  The deviation between asset and currency exposures in a portfolio is defined\nas the \"currency overlay\". This paper addresses risk mitigation by allowing\ncurrency overlay and asset allocation be optimised together. We propose a model\nof the international portfolio optimisation problem in which the currency\noverlay is constructed by holding foreign exchange rate forward contracts.\nCrucially, the cost of carry, transaction costs, and margin requirement of\nforward contracts are also taken into account in portfolio return calculation.\nThis novel extension of previous overlay models improves the accuracy of risk\nand return calculation of portfolios; furthermore, our experimental results\nshow that inclusion of such costs significantly changes the optimal decisions.\nEffects of constraints imposed to reduce transaction costs associated are\nexamined and the empirical results show that risk-return compensation of\nportfolios varies significantly with different return targets.\n"
    },
    {
        "paper_id": 1611.01471,
        "authors": "Evgeny Ivanko",
        "title": "A fair monetization model to reconcile authors and consumers of\n  intellectual property",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this small article one compromise monetization strategy is proposed, which\nhopefully may lead to a more satisfactory coexistence of IP manufacturers and\nconsumers. The motto is \"fair exchange\": you use our IP-product, we use your\nproduct (in form of money); when you do not need our product any more, we\nchange back.\n"
    },
    {
        "paper_id": 1611.01524,
        "authors": "Vasyl Golosnoy and Nestor Parolya",
        "title": "`To Have What They are Having': Portfolio Choice for Mimicking\n  Mean-Variance Savers",
        "comments": "18 pages, 2 figures",
        "journal-ref": "Quantitative Finance, 17:11, 1645-1653, 2017",
        "doi": "10.1080/14697688.2017.1301674",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a group of mean-variance investors with mimicking desire such\nthat each investor is willing to penalize deviations of his portfolio\ncomposition from compositions of other group members. Penalizing norm\nconstraints are already applied for statistical improvement of Markowitz\nportfolio procedure in order to cope with estimation risk. We relate these\npenalties to individuals' wish of social learning and introduce a mutual fund\n(investment club) aggregating group member preferences unknown for individual\nsavers. We derive the explicit analytical solution for the fund's optimal\nportfolio weights and show advantages to invest in such a fund for individuals\nwilling to mimic.\n"
    },
    {
        "paper_id": 1611.01531,
        "authors": "Zhenhua Pei, Baokui Wang and Jinming Du",
        "title": "Effects of income redistribution on the evolution of cooperation in\n  spatial public goods games",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1088/1367-2630/aa5666",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Income redistribution is the transfer of income from some individuals to\nothers directly or indirectly by means of social mechanisms, such as taxation,\npublic services and so on. Employing a spatial public goods game, we study the\ninfluence of income redistribution on the evolution of cooperation. Two kinds\nof evolutionary models are constructed, which describe local and global\nredistribution of income respectively. In the local model, players have to pay\npart of their income after each PGG and the accumulated income is redistributed\nto the members. While in the global model, all the players pay part of their\nincome after engaging in all the local PGGs, which are centered on himself and\nhis nearest neighbours, and the accumulated income is redistributed to the\nwhole population. We show that the cooperation prospers significantly with\nincreasing income expenditure proportion in the local redistribution of income,\nwhile in the global model the situation is opposite. Furthermore, the\ncooperation drops dramatically from the maximum curvature point of income\nexpenditure proportion. In particular, the intermediate critical points are\nclosely related to the renormalized enhancement factors.\n"
    },
    {
        "paper_id": 1611.01767,
        "authors": "Steven Kou, Xianhua Peng, Xingbo Xu",
        "title": "EM Algorithm and Stochastic Control in Economics",
        "comments": "46 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generalising the idea of the classical EM algorithm that is widely used for\ncomputing maximum likelihood estimates, we propose an EM-Control (EM-C)\nalgorithm for solving multi-period finite time horizon stochastic control\nproblems. The new algorithm sequentially updates the control policies in each\ntime period using Monte Carlo simulation in a forward-backward manner; in other\nwords, the algorithm goes forward in simulation and backward in optimization in\neach iteration. Similar to the EM algorithm, the EM-C algorithm has the\nmonotonicity of performance improvement in each iteration, leading to good\nconvergence properties. We demonstrate the effectiveness of the algorithm by\nsolving stochastic control problems in the monopoly pricing of perishable\nassets and in the study of real business cycle.\n"
    },
    {
        "paper_id": 1611.01771,
        "authors": "Wolfgang Kuhle",
        "title": "An Equilibrium Model with Computationally Constrained Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a large economy in which firms cannot compute exact solutions to the\nnon-linear equations that characterize the equilibrium price at which they can\nsell future output. Instead, firms use polynomial expansions to approximate\nprices. The precision with which they can compute prices is endogenous and\ndepends on the overall level of supply. At the same time, firms' individual\nsupplies, and thus aggregate supply, depend on the precision with which they\napproximate prices. This interrelation between supply and price forecast\ninduces multiple equilibria, with inefficiently low output, in economies that\notherwise have a unique, efficient equilibrium. Moreover, exogenous parameter\nchanges, which would increase output were there no computational frictions, can\ndiminish agents' ability to approximate future prices, and reduce output. Our\nmodel therefore accommodates the intuition that interventions, such as\nunprecedented quantitative easing, can put agents into \"uncharted territory\".\n"
    },
    {
        "paper_id": 1611.01958,
        "authors": "Taras Bodnar, Yarema Okhrin and Nestor Parolya",
        "title": "Optimal shrinkage-based portfolio selection in high dimensions",
        "comments": "45 pages, UPDATE3: revised version of the manuscript accepted by\n  Journal of Business and Economic Statistics. substantially revised:\n  Ledoit-Wolf and Kan-Zhou estimators were added, conditions weakened, proofs\n  revised, discussion on the Moore-Penrose approximation included, mistake in\n  the shrinkage formula for c>1 corrected (big boost in performance as a\n  result)",
        "journal-ref": "Journal of Business & Economic Statistics, 2022",
        "doi": "10.1080/07350015.2021.2004897",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we estimate the mean-variance portfolio in the high-dimensional\ncase using the recent results from the theory of random matrices. We construct\na linear shrinkage estimator which is distribution-free and is optimal in the\nsense of maximizing with probability $1$ the asymptotic out-of-sample expected\nutility, i.e., mean-variance objective function for different values of risk\naversion coefficient which in particular leads to the maximization of the\nout-of-sample expected utility and to the minimization of the out-of-sample\nvariance.\n  One of the main features of our estimator is the inclusion of the estimation\nrisk related to the sample mean vector into the high-dimensional portfolio\noptimization. The asymptotic properties of the new estimator are investigated\nwhen the number of assets $p$ and the sample size $n$ tend simultaneously to\ninfinity such that $p/n \\rightarrow c\\in (0,+\\infty)$. The results are obtained\nunder weak assumptions imposed on the distribution of the asset returns, namely\nthe existence of the $4+\\varepsilon$ moments is only required.\n  Thereafter we perform numerical and empirical studies where the small- and\nlarge-sample behavior of the derived estimator is investigated. The suggested\nestimator shows significant improvements over the existent approaches including\nthe nonlinear shrinkage estimator and the three-fund portfolio rule, especially\nwhen the portfolio dimension is larger than the sample size. Moreover, it is\nrobust to deviations from normality.\n"
    },
    {
        "paper_id": 1611.02026,
        "authors": "Milan Kumar Das, Anindya Goswami and Tanmay S. Patankar",
        "title": "Pricing Derivatives in a Regime Switching Market with Time Inhomogeneous\n  Volatility",
        "comments": "24 pages",
        "journal-ref": "Stoch. Anal. Appl. 36(2018), no. 4, 700-725",
        "doi": "10.1080/07362994.2018.1448996",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies pricing derivatives in an age-dependent semi-Markov\nmodulated market. We consider a financial market where the asset price dynamics\nfollow a regime switching geometric Brownian motion model in which the\ncoefficients depend on finitely many age-dependent semi-Markov processes. We\nfurther allow the volatility coefficient to depend on time explicitly. Under\nthese market assumptions, we study locally risk minimizing pricing of a class\nof European options. It is shown that the price function can be obtained by\nsolving a non-local B-S-M type PDE. We establish existence and uniqueness of a\nclassical solution of the Cauchy problem. We also find another characterization\nof price function via a system of Volterra integral equation of second kind.\nThis alternative representation leads to computationally efficient methods for\nfinding price and hedging. Finally, we analyze the PDE to establish continuous\ndependence of the solution on the instantaneous transition rates of semi-Markov\nprocesses. An explicit expression of quadratic residual risk is also obtained.\n"
    },
    {
        "paper_id": 1611.0227,
        "authors": "Michal Fabinger and E. Glen Weyl",
        "title": "Functional Forms for Tractable Economic Models and the Cost Structure of\n  International Trade",
        "comments": "99 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present functional forms allowing a broader range of analytic solutions to\ncommon economic equilibrium problems. These can increase the realism of\npen-and-paper solutions or speed large-scale numerical solutions as\ncomputational subroutines. We use the latter approach to build a tractable\nheterogeneous firm model of international trade accommodating economies of\nscale in export and diseconomies of scale in production, providing a natural,\nunified solution to several puzzles concerning trade costs. We briefly\nhighlight applications in a range of other fields. Our method of generating\nanalytic solutions is a discrete approximation to a logarithmically modified\nLaplace transform of equilibrium conditions.\n"
    },
    {
        "paper_id": 1611.02547,
        "authors": "Moustapha Pemy",
        "title": "Optimal Extraction and Taxation of Strategic Natural Resources: A\n  Differential Game Approach",
        "comments": "arXiv admin note: text overlap with arXiv:1606.03388. substantial\n  text overlap with arXiv:1611.01492",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal extraction and taxation of nonrenewable\nnatural resources. It is well known that the market values of the main\nstrategic resources such as oil, natural gas, uranium, copper,..., etc,\nfluctuate randomly following global and seasonal macroeconomic parameters,\nthese values are modeled using Markov switching L\\'evy processes. We formulate\nthis problem as a differential game. The two players of this differential game\nare the mining company whose aim is to maximize the revenues generated from its\nextracting activities and the government agency in charge of regulating and\ntaxing natural resources. We prove the existence of a Nash equilibrium. The\ncorresponding Hamilton Jacobi Isaacs equations are completely solved and the\nvalue functions as well as the optimal extraction and taxation rates are\nderived in closed-form. A Numerical example is presented to illustrate our\nfindings.\n"
    },
    {
        "paper_id": 1611.02549,
        "authors": "Jacopo Rocchi, Enoch Yan Lok Tsui, David Saad",
        "title": "Emerging interdependence between stock values during financial crashes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0176764",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To identify emerging interdependencies between traded stocks we investigate\nthe behavior of the stocks of FTSE 100 companies in the period 2000-2015, by\nlooking at daily stock values. Exploiting the power of information theoretical\nmeasures to extract direct influences between multiple time series, we compute\nthe information flow across stock values to identify several different regimes.\nWhile small information flows is detected in most of the period, a dramatically\ndifferent situation occurs in the proximity of global financial crises, where\nstock values exhibit strong and substantial interdependence for a prolonged\nperiod. This behavior is consistent with what one would generally expect from a\ncomplex system near criticality in physical systems, showing the long lasting\neffects of crashes on stock markets.\n"
    },
    {
        "paper_id": 1611.02556,
        "authors": "Murwan H. M. A. Siddig",
        "title": "Application of the Generalized Linear Models in Actuarial Framework",
        "comments": "5 pages, 7th European Business Research Conference held in Rome,\n  Italy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to review the methodology behind the generalized linear\nmodels which are used in analyzing the actuarial situations instead of the\nordinary multiple linear regression. We introduce how to assess the adequacy of\nthe model which includes comparing nested models using the deviance and the\nscaled deviance. The Akiake information criterion is proposed as a\ncomprehensive tool for selecting the adequate model. We model a simple\nautomobile portfolio using the generalized linear models, and use the best\nchosen model to predict the number of claims made by the policyholders in the\nportfolio.\n"
    },
    {
        "paper_id": 1611.0276,
        "authors": "Davide Fiaschi, Imre Kondor, Matteo Marsili, Valerio Volpati",
        "title": "The missing assets and the size of Shadow Banking: an update",
        "comments": "3 pages, 1 figure and 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a recent paper, using data from Forbes Global 2000, we have observed that\nthe upper tail of the firm size distribution (by assets) falls off much faster\nthan a Pareto distribution. The missing mass was suggested as an indicator of\nthe size of the Shadow Banking (SB) sector. This short note provides the latest\nfigures of the missing assets for 2013, 2014 and 2015. In 2013 and 2014 the\ndynamics of the missing assets continued being strongly correlated with\nestimates of the size of the SB sector of the Financial Stability Board. In\n2015 we find a sharp decrease in the size of missing assets, suggesting that\nthe SB sector is deflating.\n"
    },
    {
        "paper_id": 1611.02877,
        "authors": "Damiano Brigo and Fr\\'ed\\'eric Vrins",
        "title": "Disentangling wrong-way risk: pricing CVA via change of measures and\n  drift adjustment",
        "comments": "29 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A key driver of Credit Value Adjustment (CVA) is the possible dependency\nbetween exposure and counterparty credit risk, known as Wrong-Way Risk (WWR).\nAt this time, addressing WWR in a both sound and tractable way remains\nchallenging: arbitrage-free setups have been proposed by academic research\nthrough dynamic models but are computationally intensive and hard to use in\npractice. Tractable alternatives based on resampling techniques have been\nproposed by the industry, but they lack mathematical foundations. This probably\nexplains why WWR is not explicitly handled in the Basel III regulatory\nframework in spite of its acknowledged importance. The purpose of this paper is\nto propose a new method consisting of an appealing compromise: we start from a\nstochastic intensity approach and end up with a pricing problem where WWR does\nnot enter the picture explicitly. This result is achieved thanks to a set of\nchanges of measure: the WWR effect is now embedded in the drift of the\nexposure, and this adjustment can be approximated by a deterministic function\nwithout affecting the level of accuracy typically required for CVA figures. The\nperformances of our approach are illustrated through an extensive comparison of\nExpected Positive Exposure (EPE) profiles and CVA figures produced either by\n(i) the standard method relying on a full bivariate Monte Carlo framework and\n(ii) our drift-adjustment approximation. Given the uncertainty inherent to CVA,\nthe proposed method is believed to provide a promising way to handle WWR in a\nsound and tractable way.\n"
    },
    {
        "paper_id": 1611.02952,
        "authors": "Matteo Ludovico Bedini, Rainer Buckdahn, Hans-J\\\"urgen Engelbert",
        "title": "Unexpected Default in an Information Based Model",
        "comments": "22 pages, submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides sufficient conditions for the time of bankruptcy (of a\ncompany or a state) for being a totally inaccessible stopping time and provides\nthe explicit computation of its compensator in a framework where the flow of\nmarket information on the default is modelled explicitly with a Brownian bridge\nbetween 0 and 0 on a random time interval.\n"
    },
    {
        "paper_id": 1611.02961,
        "authors": "Maarten Wyns and Jacques Du Toit",
        "title": "A Finite Volume - Alternating Direction Implicit Approach for the\n  Calibration of Stochastic Local Volatility Models",
        "comments": "29 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Calibration of stochastic local volatility (SLV) models to their underlying\nlocal volatility model is often performed by numerically solving a\ntwo-dimensional non-linear forward Kolmogorov equation. We propose a novel\nfinite volume (FV) discretization in the numerical solution of general 1D and\n2D forward Kolmogorov equations. The FV method does not require a\ntransformation of the PDE. This constitutes a main advantage in the calibration\nof SLV models as the pertinent PDE coefficients are often nonsmooth. Moreover,\nthe FV discretization has the crucial property that the total numerical mass is\nconserved. Applying the FV discretization in the calibration of SLV models\nyields a non-linear system of ODEs. Numerical time stepping is performed by the\nHundsdorfer-Verwer ADI scheme to increase the computational efficiency. The\nnon-linearity in the system of ODEs is handled by introducing an inner\niteration. Ample numerical experiments are presented that illustrate the\neffectiveness of the calibration procedure.\n"
    },
    {
        "paper_id": 1611.0311,
        "authors": "Tim Leung and Jamie Kang",
        "title": "Asynchronous ADRs: Overnight vs Intraday Returns and Trading Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  American Depositary Receipts (ADRs) are exchange-traded certificates that\nrep- resent shares of non-U.S. company securities. They are major financial\ninstruments for investing in foreign companies. Focusing on Asian ADRs in the\ncontext of asyn- chronous markets, we present methodologies and results of\nempirical analysis of their returns. In particular, we dissect their returns\ninto intraday and overnight com- ponents with respect to the U.S. market hours.\nThe return difference between the S&P500 index, traded through the SPDR S&P500\nETF (SPY), and each ADR is found to be a mean-reverting time series, and is\nfitted to an Ornstein-Uhlenbeck process via maximum-likelihood estimation\n(MLE). Our empirical observations also lead us to develop and backtest pairs\ntrading strategies to exploit the mean-reverting ADR-SPY spreads. We find\nconsistent positive payoffs when long position in ADR and short position in SPY\nare simultaneously executed at selected entry and exit levels.\n"
    },
    {
        "paper_id": 1611.03239,
        "authors": "Jean-Philippe Aguilar, Cyril Coste, Hagen Kleinert, Jan Korbel",
        "title": "Distributional Mellin calculus in $\\mathbb{C}^n$, with applications to\n  option pricing",
        "comments": "v1-2: more details on the American option part, some references\n  added, 23 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss several aspects of Mellin transform, including distributional\nMellin transform and inversion of multiple Mellin-Barnes integrals in\n$\\mathbb{C}^n$ and its connection to residue expansion or evaluation of Laplace\nintegrals. These mathematical concepts are demonstrated on several\noption-pricing models. This includes European option models such as\nBlack-Scholes or fractional-diffusion models, as well as evaluation of\nquantities related to the optimal exercise price of American options.\n"
    },
    {
        "paper_id": 1611.03435,
        "authors": "Paulwin Graewe, Ulrich Horst",
        "title": "Optimal Trade Execution with Instantaneous Price Impact and Stochastic\n  Resilience",
        "comments": null,
        "journal-ref": "SIAM J. Control Optim. 55 (2017) 3707-3725",
        "doi": "10.1137/16M1105463",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal execution problem in illiquid markets with both\ninstantaneous and persistent price impact and stochastic resilience when only\nabsolutely continuous trading strategies are admissible. In our model the value\nfunction can be described by a three-dimensional system of backward stochastic\ndifferential equations (BSDE) with a singular terminal condition in one\ncomponent. We prove existence and uniqueness of a solution to the BSDE system\nand characterize both the value function and the optimal strategy in terms of\nthe unique solution to the BSDE system. Our existence proof is based on an\nasymptotic expansion of the BSDE system at the terminal time that allows us to\nexpress the system in terms of a equivalent system with finite terminal value\nbut singular driver.\n"
    },
    {
        "paper_id": 1611.0374,
        "authors": "Domingo A. Tarzia",
        "title": "Properties of the financial break-even point in a simple investment\n  project as a function of the discount rate",
        "comments": "18 pages, 3 figures, 1 table",
        "journal-ref": "Journal of Economics and Financial Studies, Vol. 4 No. 2 (2016),\n  pp 31-45",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a simple investment project with the following parameters: I>0:\nInitial investment which is amortizable in n years; n: Number of years the\ninvestment allows production with constant output per year; A>0: Annual\namortization (A=I/n); Q>0: Quantity of products sold per year; Cv>0: Variable\ncost per unit; p>0: Price of the product with p>Cv; Cf>0: Annual fixed costs;\nte: Tax of earnings; r: Annual discount rate. We also assume inflation is\nnegligible. We derive a closed expression of the financial break-even point Qf\n(i.e. the value of Q for which the net present value (NPV) is zero) as a\nfunction of the parameters I, n, Cv, Cf, te, r, p. We study the behavior of Qf\nas a function of the discount rate r and we prove that: (i) For r negligible Qf\nequals the accounting break-even point Qc (i.e. the earnings before taxes (EBT)\nis null) ; (ii) When r is large the graph of the function Qf=Qf(r) has an\nasymptotic straight line with positive slope. Moreover, Qf(r) is an strictly\nincreasing and convex function of the variable r; (iii) From a sensitivity\nanalysis we conclude that, while the influence of p and Cv on Qf is strong, the\ninfluence of Cf on Qf is weak.\n"
    },
    {
        "paper_id": 1611.03782,
        "authors": "Giulia Poce, Giulio Cimini, Andrea Gabrielli, Andrea Zaccaria,\n  Giuditta Baldacci, Marco Polito, Mariangela Rizzo, Silvia Sabatini",
        "title": "What do central counterparties default funds really cover? A\n  network-based stress test answer",
        "comments": null,
        "journal-ref": "Journal of Network Theory in Finance 4(4), 43-57 (2018)",
        "doi": "10.21314/JNTF.2018.047",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the last years, increasing efforts have been put into the development of\neffective stress tests to quantify the resilience of financial institutions.\nHere we propose a stress test methodology for central counterparties based on a\nnetwork characterization of clearing members, whose links correspond to direct\ncredits and debits. This network constitutes the ground for the propagation of\nfinancial distress: equity losses caused by an initial shock with both\nexogenous and endogenous components reverberate within the network and are\namplified through credit and liquidity contagion channels. At the end of the\ndynamics, we determine the vulnerability of each clearing member, which\nrepresents its potential equity loss. We apply the proposed framework to the\nFixed Income asset class of CC&G, the central counterparty operating in Italy\nwhose main cleared securities are Italian Government Bonds. We consider two\ndifferent scenarios: a distributed, plausible initial shock, as well as a shock\ncorresponding to the cover 2 regulatory requirement (the simultaneous default\nof the two most exposed clearing members). Although the two situations lead to\nsimilar results after an unlimited reverberation of shocks on the network, the\ndistress propagation is much more hasty in the latter case, with a large number\nof additional defaults triggered at early stages of the dynamics. Our results\nthus show that setting a default fund to cover insolvencies only on a cover 2\nbasis may not be adequate for taming systemic events, and only very\nconservative default funds, such as CC&G's one, can face total losses due to\nthe shock propagation. Overall, our network-based stress test represents a\nrefined tool for calibrating default fund amounts.\n"
    },
    {
        "paper_id": 1611.0409,
        "authors": "Huai-Long Shi, Zhi-Qiang Jiang and Wei-Xing Zhou (ECUST)",
        "title": "Time-varying return predictability in the Chinese stock market",
        "comments": "11 Latex pages including 2 figures and 1 table",
        "journal-ref": "Reports in Advances of Physical Sciences 1 (1), 1740002 (2017)",
        "doi": "10.1142/S2424942417400023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China's stock market is the largest emerging market all over the world. It is\nwidely accepted that the Chinese stock market is far from efficiency and it\npossesses possible linear and nonlinear dependence. We study the predictability\nof returns in the Chinese stock market by employing the wild bootstrap\nautomatic variance ratio test and the generalized spectral test. We find that\nthe return predictability vary over time and significant return predictability\nis observed around market turmoils. Our findings are consistent with the\nAdaptive Markets Hypothesis and have practical implications for market\nparticipants.\n"
    },
    {
        "paper_id": 1611.04091,
        "authors": "Hai-Chuan Xu, Zhi-Qiang Jiang and Wei-Xing Zhou (ECUST)",
        "title": "Immediate price impact of a stock and its warrant: Power-law or\n  logarithmic model?",
        "comments": "11 Latex pages including 4 figures and 4 tables. accepted by\n  International Journal of Modern Physics B",
        "journal-ref": "International Journal of Modern Physics B 31 (8), 1750048 (2017)",
        "doi": "10.1142/S0217979217500485",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the order flow data of a stock and its warrant, the immediate price\nimpacts of market orders are estimated by two competitive models, the power-law\nmodel (PL model) and the logarithmic model (LG model). We find that the PL\nmodel is overwhelmingly superior to the LG model, regarding the robustness of\nthe estimated parameters and the accuracy of out-of-sample forecasting. We also\nfind that the price impacts of ask and bid orders are consistent with each\nother for filled trades, since significant positive correlations are observed\nbetween the model parameters of both types of orders. Our findings may provide\nvaluable insights for optimal trade execution.\n"
    },
    {
        "paper_id": 1611.04311,
        "authors": "Matteo Serri, Guido Caldarelli, Giulio Cimini",
        "title": "How the interbank market becomes systemically dangerous: an agent-based\n  network model of financial distress propagation",
        "comments": null,
        "journal-ref": "Journal of Network Theory in Finance 3(1), 1-18 (2017)",
        "doi": "10.21314/JNTF.2017.025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assessing the stability of economic systems is a fundamental research focus\nin economics, that has become increasingly interdisciplinary in the currently\ntroubled economic situation. In particular, much attention has been devoted to\nthe interbank lending market as an important diffusion channel for financial\ndistress during the recent crisis. In this work we study the stability of the\ninterbank market to exogenous shocks using an agent-based network framework.\nOur model encompasses several ingredients that have been recognized in the\nliterature as pro-cyclical triggers of financial distress in the banking\nsystem: credit and liquidity shocks through bilateral exposures, liquidity\nhoarding due to counterparty creditworthiness deterioration, target leveraging\npolicies and fire-sales spillovers. But we exclude the possibility of central\nauthorities intervention. We implement this framework on a dataset of 183\nEuropean banks that were publicly traded between 2004 and 2013. We document the\nextreme fragility of the interbank lending market up to 2008, when a systemic\ncrisis leads to total depletion of market equity with an increasing speed of\nmarket collapse. After the crisis instead the system is more resilient to\nsystemic events in terms of residual market equity. However, the speed at which\nthe crisis breaks out reaches a new maximum in 2011, and never goes back to\nvalues observed before 2007. Our analysis points to the key role of the crisis\noutbreak speed, which sets the maximum delay for central authorities\nintervention to be effective.\n"
    },
    {
        "paper_id": 1611.0432,
        "authors": "Jean-Philippe Aguilar, Cyril Coste, Hagen Kleinert, Jan Korbel",
        "title": "Regularization and analytic option pricing under $\\alpha$-stable\n  distribution of arbitrary asymmetry",
        "comments": "V1, with updated references and authorship",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a non-Gaussian option pricing model, into which the underlying\nlog-price is assumed to be driven by an $\\alpha$-stable distribution. We remove\nthe a priori divergence of the model by introducing a Mellin regularization for\nthe L\\'evy propagator. Using distributional and $\\mathbb{C}^n$ tools, we derive\nan analytic closed formula for the option price, valid for any stability\n$\\alpha\\in]1,2]$ and any asymmetry. This formula is very efficient and recovers\nprevious cases (Black-Scholes, Carr-Wu); we calibrate the formula on market\ndatas, make numerical tests, and discuss its many interesting properties.\n"
    },
    {
        "paper_id": 1611.04494,
        "authors": "Bahman Angoshtari, Thaleia Zariphopoulou, Xun Yu Zhou",
        "title": "Predictable Forward Performance Processes: The Binomial Case",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new class of forward performance processes that are endogenous\nand predictable with regards to an underlying market information set and,\nfurthermore, are updated at discrete times. We analyze in detail a binomial\nmodel whose parameters are random and updated dynamically as the market\nevolves. We show that the key step in the construction of the associated\npredictable forward performance process is to solve a single-period inverse\ninvestment problem, namely, to determine, period-by-period and conditionally on\nthe current market information, the end-time utility function from a given\ninitial-time value function. We reduce this inverse problem to solving a\nfunctional equation and establish conditions for the existence and uniqueness\nof its solutions in the class of inverse marginal functions.\n"
    },
    {
        "paper_id": 1611.04851,
        "authors": "Marie Kratz, Yen H. Lok, Alexander J McNeil",
        "title": "Multinomial VaR Backtests: A simple implicit approach to backtesting\n  expected shortfall",
        "comments": "30 pages; 7 tables; 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under the Fundamental Review of the Trading Book (FRTB) capital charges for\nthe trading book are based on the coherent expected shortfall (ES) risk\nmeasure, which show greater sensitivity to tail risk. In this paper it is\nargued that backtesting of expected shortfall - or the trading book model from\nwhich it is calculated - can be based on a simultaneous multinomial test of\nvalue-at-risk (VaR) exceptions at different levels, an idea supported by an\napproximation of ES in terms of multiple quantiles of a distribution proposed\nin Emmer et al. (2015). By comparing Pearson, Nass and likelihood-ratio tests\n(LRTs) for different numbers of VaR levels $N$ it is shown in a series of\nsimulation experiments that multinomial tests with $N\\geq 4$ are much more\npowerful at detecting misspecifications of trading book loss models than\nstandard binomial exception tests corresponding to the case $N=1$. Each test\nhas its merits: Pearson offers simplicity; Nass is robust in its size\nproperties to the choice of $N$; the LRT is very powerful though slightly\nover-sized in small samples and more computationally burdensome. A\ntraffic-light system for trading book models based on the multinomial test is\nproposed and the recommended procedure is applied to a real-data example\nspanning the 2008 financial crisis.\n"
    },
    {
        "paper_id": 1611.04877,
        "authors": "Xavier Warin",
        "title": "The Asset Liability Management problem of a nuclear operator : a\n  numerical stochastic optimization approach",
        "comments": "24 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We numerically study an Asset Liability Management problem linked to the\ndecommissioning of French nuclear power plants. We link the risk aversion of\npractitioners to an optimization problem. Using different price models we show\nthat the optimal solution is linked to a de-risking management strategy similar\nto a concave strategy and we propose an effective heuristic to simulate the\nunderlying optimal strategy. Besides we show that the strategy is stable with\nrespect to the main parameters involved in the liability problem.\n"
    },
    {
        "paper_id": 1611.04941,
        "authors": "Francisco Salas-Molina, Juan A. Rodr\\'iguez-Aguilar, Joan Serr\\`a,\n  Montserrat Guillen, Francisco J. Martin",
        "title": "Empirical analysis of daily cash flow time series and its implications\n  for forecasting",
        "comments": "20 pages, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cash managers make daily decisions based on predicted monetary inflows from\ndebtors and outflows to creditors. Usual assumptions on the statistical\nproperties of daily net cash flow include normality, absence of correlation and\nstationarity. We provide a comprehensive study based on a real-world cash flow\ndata set from small and medium companies, which is the most common type of\ncompanies in Europe. We also propose a new cross-validated test for time-series\nnon-linearity showing that: (i) the usual assumption of normality, absence of\ncorrelation and stationarity hardly appear; (ii) non-linearity is often\nrelevant for forecasting; and (iii) typical data transformations have little\nimpact on linearity and normality. Our results provide a forecasting strategy\nfor cash flow management which performs better than classical methods. This\nevidence may lead to consider a more data-driven approach such as time-series\nforecasting in an attempt to provide cash managers with expert systems in cash\nmanagement.\n"
    },
    {
        "paper_id": 1611.05194,
        "authors": "Kensuke Ishitani",
        "title": "Computation of first-order Greeks for barrier options using chain rules\n  for Wiener path integrals",
        "comments": "This paper has been withdrawn by the author due to a error in the\n  assumptions of main results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a new methodology to compute first-order Greeks for\nbarrier options under the framework of path-dependent payoff functions with\nEuropean, Lookback, or Asian type and with time-dependent trigger levels. In\nparticular, we develop chain rules for Wiener path integrals between two curves\nthat arise in the computation of first-order Greeks for barrier options. We\nalso illustrate the effectiveness of our method through numerical examples.\n"
    },
    {
        "paper_id": 1611.0528,
        "authors": "Taisei Kaizoji",
        "title": "Toward Economics as a New Complex System",
        "comments": "9 pages",
        "journal-ref": "Eur. Phyys. J. Special Topics 225, 3225-3230 (2016)",
        "doi": "10.1140/epjst/e2016-60161-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2015 Nobel Prize in Economic Sciences was awarded to Eugene Fama, Lars\nPeter Hansen and Robert Shiller for their contributions to the empirical\nanalysis of asset prices. Eugene Fama [1] is an advocate of the efficient\nmarket hypothesis. The efficient market hypothesis assumes that asset price is\ndetermined by using all available information and only reacts to new\ninformation not incorporated into the fundamentals. Thus, the movement of stock\nprices is unpredictable. Robert Shiller [2] has been studying the existence of\nirrational bubbles, which are defined as the long term deviations of asset\nprice from the fundamentals. This drives us to the unsettled question of how\nthe market actually works.\n  In this paper, I look back at the development of economics and consider the\ndirection in which we should move in order to truly understand the workings of\nan economic society.\n"
    },
    {
        "paper_id": 1611.05288,
        "authors": "Kathia Pinz\\'on",
        "title": "Analysis of Price and Income Elasticities of Energy Demand in Ecuador: A\n  Dynamic OLS Approach",
        "comments": "33 pages, 4 figures, 8 tables",
        "journal-ref": "Pinz\\'on, K. (2017)",
        "doi": "10.1016/j.eap.2017.09.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy consumption in Ecuador has increased significantly during the last\ndecades, affecting negatively the financial position of the country since large\nenergy consumption subsidies are provided in its internal market and Ecuador is\nmostly a crude oil exporter and oil derivatives importer country. This research\nseeks to state the long run price and income elasticities of energy demand in\nEcuador, by analyzing information spanning the period from 1970 to 2015. A\ncointegration analysis and an estimation by using a Dynamic Ordinary Least\nSquares approach considering structural breaks is carried out. Results obtained\nare robust and suggest that in the long run energy demand in Ecuador is highly\nincome elastic, has no relationship with its price and has an almost unitary\nbut inverse relationship with the industrial production level. Conclusions and\neconomic policy suggestions are also provided.\n"
    },
    {
        "paper_id": 1611.05518,
        "authors": "Sergey Nadtochiy and Jan Obloj",
        "title": "Robust Trading of Implied Skew",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a method for constructing a (static) portfolio of\nco-maturing European options whose price sign is determined by the skewness\nlevel of the associated implied volatility. This property holds regardless of\nthe validity of a specific model - i.e. the method is robust. The strategy is\ngiven explicitly and depends only on beliefs about the future values of implied\nskewness, which is an observable market indicator. As such, our method allows\nto use the existing statistical tools to formulate the beliefs, providing a\npractical interpretation of the more abstract mathematical setting, in which\nthe belies are understood as a family of probability measures. One of the\napplications of our results is a method for trading views on the future changes\nin implied skew, largely independently of other market factors. Another\napplication provides a concrete improvement of the existing model-independent\nsuper- and sub- replication strategies for barrier options, which exploits a\ngiven set of beliefs on the implied skew. Our theoretical results are tested\nempirically, using the historical prices of SP500 options.\n"
    },
    {
        "paper_id": 1611.05571,
        "authors": "Joongyeub Yeo, George Papanicolaou",
        "title": "Random matrix approach to estimation of high-dimensional factor models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In dealing with high-dimensional data sets, factor models are often useful\nfor dimension reduction. The estimation of factor models has been actively\nstudied in various fields. In the first part of this paper, we present a new\napproach to estimate high-dimensional factor models, using the empirical\nspectral density of residuals. The spectrum of covariance matrices from\nfinancial data typically exhibits two characteristic aspects: a few spikes and\nbulk. The former represent factors that mainly drive the features and the\nlatter arises from idiosyncratic noise. Motivated by these two aspects, we\nconsider a minimum distance between two spectrums; one from a covariance\nstructure model and the other from real residuals of financial data that are\nobtained by subtracting principal components. Our method simultaneously\nprovides estimators of the number of factors and information about correlation\nstructures in residuals. Using free random variable techniques, the proposed\nalgorithm can be implemented and controlled effectively. Monte Carlo\nsimulations confirm that our method is robust to noise or the presence of weak\nfactors. Furthermore, the application to financial time-series shows that our\nestimators capture essential aspects of market dynamics.\n"
    },
    {
        "paper_id": 1611.05688,
        "authors": "John J. Horton",
        "title": "The Tragedy of Your Upstairs Neighbors: Is the Airbnb Negative\n  Externality Internalized?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A commonly expressed concern about the rise of the peer-to-peer rental market\nAirbnb is that hosts---those renting out their properties---impose costs on\ntheir unwitting neighbors. I consider the question of whether apartment\nbuilding owners will, in a competitive rental market, set a building-specific\nAirbnb hosting policy that is socially efficient. I find that if tenants can\nsort across apartments based on the owners policy then the equilibrium fraction\nof buildings allowing Airbnb listing would be socially efficient.\n"
    },
    {
        "paper_id": 1611.0569,
        "authors": "Javiera Barrera and Eduardo Moreno and Sebastian Varas",
        "title": "A decomposition algorithm for computing income taxes with pass-through\n  entities and its application to the Chilean case",
        "comments": null,
        "journal-ref": "Ann Oper Res 286, 545-557 (2020)",
        "doi": "10.1007/s10479-017-2707-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Income tax systems with pass-through entities transfer a firm's incomes to\nthe shareholders, which are taxed individually. In 2014, a Chilean tax reform\nintroduced this type of entity and changed to an accrual basis that distributes\nincomes (but not losses) to shareholders. A crucial step for the Chilean\ntaxation authority is to compute the final income of each individual, given the\ncomplex network of corporations and companies, usually including cycles between\nthem. In this paper, we show the mathematical conceptualization and the\nsolution to the problem, proving that there is only one way to distribute\nincomes to taxpayers. Using the theory of absorbing Markov chains, we define a\nmathematical model for computing the taxable incomes of each taxpayer, and we\npropose a decomposition algorithm for this problem. This allows us to compute\nthe solution accurately and with the efficient use of computational resources.\nFinally, we present some characteristics of the Chilean taxpayers' network and\ncomputational results of the algorithm using this network.\n"
    },
    {
        "paper_id": 1611.0601,
        "authors": "David Ardia, Kris Boudt and Leopoldo Catania",
        "title": "Value-at-Risk Prediction in R with the GAS Package",
        "comments": "10 pages 1 fig 2 tab",
        "journal-ref": null,
        "doi": "10.32614/RJ-2018-064",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  GAS models have been recently proposed in time-series econometrics as\nvaluable tools for signal extraction and prediction. This paper details how\nfinancial risk managers can use GAS models for Value-at-Risk (VaR) prediction\nusing the novel GAS package for R. Details and code snippets for prediction,\ncomparison and backtesting with GAS models are presented. An empirical\napplication considering Dow Jones Index constituents investigates the VaR\nforecasting performance of GAS models.\n"
    },
    {
        "paper_id": 1611.06098,
        "authors": "Ki Wai Chau and Cornelis W. Oosterlee",
        "title": "On the wavelets-based SWIFT method for backward stochastic differential\n  equations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/imanum/drx022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a numerical algorithm for backward stochastic differential\nequations based on time discretization and trigonometric wavelets. This method\ncombines the effectiveness of Fourier-based methods and the simplicity of a\nwavelet-based formula, resulting in an algorithm that is both accurate and easy\nto implement. Furthermore, we mitigate the problem of errors near the\ncomputation boundaries by means of an antireflective boundary technique, giving\nan improved approximation. We test our algorithm with different numerical\nexperiments.\n"
    },
    {
        "paper_id": 1611.06181,
        "authors": "Olena Burkovska, Maximilian Ga{\\ss}, Kathrin Glau, Mirco Mahlstedt,\n  Wim Schoutens and Barbara Wohlmuth",
        "title": "Calibration to American Options: Numerical Investigation of the\n  de-Americanization",
        "comments": "Key words: American options, calibration, binomial tree model, CEV\n  model, Heston model, L\\'evy models, model reduction, variational inequalities",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  American options are the reference instruments for the model calibration of a\nlarge and important class of single stocks. For this task, a fast and accurate\npricing algorithm is indispensable. The literature mainly discusses pricing\nmethods for American options that are based on Monte Carlo, tree and partial\ndifferential equation methods. We present an alternative approach that has\nbecome popular under the name de-Americanization in the financial industry. The\nmethod is easy to implement and enjoys fast run-times. Since it is based on ad\nhoc simplifications, however, theoretical results guaranteeing reliability are\nnot available. To quantify the resulting methodological risk, we empirically\ntest the performance of the de-Americanization method for calibration. We\nclassify the scenarios in which de-Americanization performs very well. However,\nwe also identify the cases where de-Americanization oversimplifies and can\nresult in large errors.\n"
    },
    {
        "paper_id": 1611.06218,
        "authors": "Freddy Delbaen, Keita Owari",
        "title": "Convex functions on dual Orlicz spaces",
        "comments": "12 pages; added a new characterisation of the $\\Delta_2$-Orlicz\n  spaces as well as a few minor changes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the dual $L_{\\Phi^*}$ of a $\\Delta_2$-Orlicz space $L_\\Phi$, that we call\na dual Orlicz space, we show that a proper (resp. finite) convex function is\nlower semicontinuous (resp. continuous) for the Mackey topology\n$\\tau(L_{\\Phi^*},L_\\Phi)$ if and only if on each order interval\n$[-\\zeta,\\zeta]=\\{\\xi: -\\zeta\\leq \\xi\\leq\\zeta\\}$ ($\\zeta\\in L_{\\Phi^*}$), it\nis lower semicontinuous (resp. continuous) for the topology of convergence in\nprobability. For this purpose, we provide the following Koml\\'os type result:\nevery norm bounded sequence $(\\xi_n)_n$ in $L_{\\Phi^*}$ admits a sequence of\nforward convex combinations $\\bar\\xi_n\\in\\mathrm{conv}(\\xi_n,\\xi_{n+1},...)$\nsuch that $\\sup_n|\\bar\\xi_n|\\in L_{\\Phi^*}$ and $\\bar\\xi_n$ converges a.s.\n"
    },
    {
        "paper_id": 1611.06344,
        "authors": "Denis Belomestny, Stefan H\\\"afner and Mikhail Urusov",
        "title": "Regression-based complexity reduction of the nested Monte Carlo methods",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1137/17M114577X",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a novel dual regression-based approach for pricing\nAmerican options. This approach reduces the complexity of the nested Monte\nCarlo method and has especially simple form for time discretised diffusion\nprocesses. We analyse the complexity of the proposed approach both in the case\nof fixed and increasing number of exercise dates. The method is illustrated by\nseveral numerical examples.\n"
    },
    {
        "paper_id": 1611.06407,
        "authors": "Vygintas Gontis",
        "title": "Interplay between endogenous and exogenous fluctuations in financial\n  markets",
        "comments": "20 pages, 8 figures",
        "journal-ref": "Acta Physica Polonica A 129 (5), 2016, psl. 1023-1031",
        "doi": "10.12693/APhysPolA.129.1023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address microscopic, agent based, and macroscopic, stochastic, modeling of\nthe financial markets combining it with the exogenous noise. The interplay\nbetween the endogenous dynamics of agents and the exogenous noise is the\nprimary mechanism responsible for the observed long-range dependence and\nstatistical properties of high volatility return intervals. By exogenous noise\nwe mean information flow or/and order flow fluctuations. Numerical results\nbased on the proposed model reveal that the exogenous fluctuations have to be\nconsidered as indispensable part of comprehensive modeling of the financial\nmarkets.\n"
    },
    {
        "paper_id": 1611.06452,
        "authors": "Olena Burkovska, Kathrin Glau, Mirco Mahlstedt, Barbara Wohlmuth",
        "title": "Model reduction for calibration of American options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  American put options are among the most frequently traded single stock\noptions, and their calibration is computationally challenging since no\nclosed-form expression is available. Due to the higher flexibility in\ncomparison to European options, the mathematical model involves additional\nconstraints, and a variational inequality is obtained. We use the Heston\nstochastic volatility model to describe the price of a single stock option. In\norder to speed up the calibration process, we apply two model reduction\nstrategies. Firstly, a reduced basis method (RBM) is used to define a suitable\nlow-dimensional basis for the numerical approximation of the\nparameter-dependent partial differential equation ($\\mu$PDE) model. By doing so\nthe computational complexity for solving the $\\mu$PDE is drastically reduced,\nand applications of standard minimization algorithms for the calibration are\nsignificantly faster than working with a high-dimensional finite element basis.\nSecondly, so-called de-Americanization strategies are applied. Here, the main\nidea is to reformulate the calibration problem for American options as a\nproblem for European options and to exploit closed-form solutions. Both\nreduction techniques are systematically compared and tested for both synthetic\nand market data sets.\n"
    },
    {
        "paper_id": 1611.06666,
        "authors": "Wen-Jie Xie (ECUST), Ming-Xia Li (ECUST), Hai-Chuan Xu (ECUST), Wei\n  Chen (SZSE), Wei-Xing Zhou (ECUST) and H. E. Stanley (BU)",
        "title": "Quantifying immediate price impact of trades based on the $k$-shell\n  decomposition of stock trading networks",
        "comments": "6 pages including 3 figures and 1 table",
        "journal-ref": "EPL,116 (2016) 28006",
        "doi": "10.1209/0295-5075/116/28006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traders in a stock market exchange stock shares and form a stock trading\nnetwork. Trades at different positions of the stock trading network may contain\ndifferent information. We construct stock trading networks based on the limit\norder book data and classify traders into $k$ classes using the $k$-shell\ndecomposition method. We investigate the influences of trading behaviors on the\nprice impact by comparing a closed national market (A-shares) with an\ninternational market (B-shares), individuals and institutions, partially filled\nand filled trades, buyer-initiated and seller-initiated trades, and trades at\ndifferent positions of a trading network. Institutional traders professionally\nuse some trading strategies to reduce the price impact and individuals at the\nsame positions in the trading network have a higher price impact than\ninstitutions. We also find that trades in the core have higher price impacts\nthan those in the peripheral shell.\n"
    },
    {
        "paper_id": 1611.06672,
        "authors": "Li-Hsien Sun",
        "title": "Systemic Risk and Interbank Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple model of the banking system incorporating a game feature\nwhere the evolution of monetary reserve is modeled as a system of coupled\nFeller diffusions. The Markov Nash equilibrium generated through minimizing the\nlinear quadratic cost subject to Cox-Ingersoll-Ross type processes creates\nliquidity and deposit rate. The adding liquidity leads to a flocking effect but\nthe deposit rate diminishes the growth rate of the total monetary reserve\ncausing a large number of bank defaults. In addition, the corresponding Mean\nField Game and the infinite time horizon stochastic game with the discount\nfactor are also discussed.\n"
    },
    {
        "paper_id": 1611.06698,
        "authors": "Tam\\'as Bir\\'o and Zolt\\'an N\\'eda",
        "title": "Dynamical Stationarity as a Result of Sustained Random Growth",
        "comments": "7 pages, 2 Figures, PRE style",
        "journal-ref": "Phys. Rev. E 95, 032130 (2017)",
        "doi": "10.1103/PhysRevE.95.032130",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In sustained growth with random dynamics stationary distributions can exist\nwithout detailed balance. This suggests thermodynamical behavior in fast\ngrowing complex systems. In order to model such phenomena we apply both a\ndiscrete and a continuous master equation. The derivation of elementary rates\nfrom known stationary distributions is a generalization of the\nfluctuation--dissipation theorem. Entropic distance evolution is given for such\nsystems. We reconstruct distributions obtained for growing networks, particle\nproduction, scientific citations and income distribution.\n"
    },
    {
        "paper_id": 1611.07432,
        "authors": "Loretta Mastroeni and Pierluigi Vellucci",
        "title": "\"Chaos\" in energy and commodity markets: a controversial matter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test whether the futures prices of some commodity and energy markets are\ndetermined by stochastic rules or exhibit nonlinear deterministic endogenous\nfluctuations. As for the methodologies, we use the maximal Lyapunov exponents\n(MLE) and a determinism test, both based on the reconstruction of the phase\nspace. In particular, employing a recent methodology, we estimate a coefficient\n$\\kappa$ that describes the determinism rate of the analyzed time series. We\nfind that the underlying system for futures prices shows a reliability level\n$\\kappa$ near to $1$ while the MLE is positive for all commodity futures\nseries. Thus, the empirical evidence suggests that commodity and energy futures\nprices are the measured footprint of a nonlinear deterministic, rather than a\nstochastic, system.\n"
    },
    {
        "paper_id": 1611.07741,
        "authors": "John Armstrong",
        "title": "The Markowitz Category",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": "10.1137/17M1155727",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an algebraic definition of a Markowitz market and classify markets up\nto isomorphism. Given this classification, the theory of portfolio optimization\nin Markowitz markets without short selling constraints becomes trivial.\nConversely, this classification shows that, up to isomorphism, there is little\nthat can be said about a Markowitz market that is not already detected by the\ntheory of portfolio optimization. In particular, if one seeks to develop a\nsimplified low-dimensional model of a large financial market using\nmean--variance analysis alone, the resulting model can be at most\ntwo-dimensional.\n"
    },
    {
        "paper_id": 1611.07843,
        "authors": "Alexis Bismuth, Olivier Gu\\'eant, Jiang Pu",
        "title": "Portfolio choice, portfolio liquidation, and portfolio transition under\n  drift uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents several models addressing optimal portfolio choice,\noptimal portfolio liquidation, and optimal portfolio transition issues, in\nwhich the expected returns of risky assets are unknown. Our approach is based\non a coupling between Bayesian learning and dynamic programming techniques that\nleads to partial differential equations. It enables to recover the well-known\nresults of Karatzas and Zhao in a framework \\`a la Merton, but also to deal\nwith cases where martingale methods are no longer available. In particular, we\naddress optimal portfolio choice, portfolio liquidation, and portfolio\ntransition problems in a framework \\`a la Almgren-Chriss, and we build\ntherefore a model in which the agent takes into account in his decision process\nboth the liquidity of assets and the uncertainty with respect to their expected\nreturn.\n"
    },
    {
        "paper_id": 1611.08088,
        "authors": "Tetsuya Takaishi",
        "title": "Multiple Time Series Ising Model for Financial Market Simulations",
        "comments": "4 pages, 4 figures",
        "journal-ref": "Journal of Physics: Conference Series 574 (2015) 012149",
        "doi": "10.1088/1742-6596/574/1/012149",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we propose an Ising model which simulates multiple financial\ntime series. Our model introduces the interaction which couples to spins of\nother systems. Simulations from our model show that time series exhibit the\nvolatility clustering that is often observed in the real financial markets.\nFurthermore we also find non-zero cross correlations between the volatilities\nfrom our model. Thus our model can simulate stock markets where volatilities of\nstocks are mutually correlated.\n"
    },
    {
        "paper_id": 1611.0833,
        "authors": "Johan G. Andreasson and Pavel V. Shevchenko",
        "title": "The 2015-2017 policy changes to the means-tests of Australian Age\n  Pension: implication to decisions in retirement",
        "comments": "18 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Australian Government uses the means-test as a way of managing the\npension budget. Changes in Age Pension policy impose difficulties in retirement\nmodelling due to policy risk, but any major changes tend to be `grandfathered'\nmeaning that current retirees are exempt from the new changes. In 2015, two\nimportant changes were made in regards to allocated pension accounts -- the\nincome means-test is now based on deemed income rather than account\nwithdrawals, and the income-test deduction no longer applies. We examine the\nimplications of the new changes in regards to optimal decisions for\nconsumption, investment, and housing. We account for regulatory minimum\nwithdrawal rules that are imposed by regulations on allocated pension accounts,\nas well as the 2017 asset-test rebalancing. The new policy changes are modelled\nin a utility maximizing lifecycle model and solved as an optimal stochastic\ncontrol problem. We find that the new rules decrease the benefits from planning\nthe consumption in relation to the means-test, while the housing allocation\nincreases slightly in order to receive additional Age Pension. The difference\nin optimal drawdown between the old and new policy are only noticeable early in\nretirement until regulatory minimum withdrawal rates are enforced. However, the\namount of extra Age Pension received for many households is now significantly\ndifferent due to the new deeming income rules, which benefit slightly wealthier\nhouseholds who previously would receive no Age Pension due to the income-test\nand minimum withdrawals.\n"
    },
    {
        "paper_id": 1611.08393,
        "authors": "Ziping Zhao and Daniel P. Palomar",
        "title": "Mean-Reverting Portfolio Design via Majorization-Minimization Method",
        "comments": "5 pages, 2 figures, to appear in Proc. of the 50th Asilomar\n  Conference on Signals, Systems and Computers",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the mean-reverting portfolio design problem arising from\nstatistical arbitrage in the financial markets. The problem is formulated by\noptimizing a criterion characterizing the mean-reversion strength of the\nportfolio and taking into consideration the variance of the portfolio and an\ninvestment budget constraint at the same time. An efficient algorithm based on\nthe majorization-minimization (MM) method is proposed to solve the problem.\nNumerical results show that our proposed mean-reverting portfolio design method\ncan significantly outperform every underlying single spread and the benchmark\nmethod in the literature.\n"
    },
    {
        "paper_id": 1611.0851,
        "authors": "Donovan Platt, Tim Gebbie",
        "title": "Can Agent-Based Models Probe Market Microstructure?",
        "comments": "17 pages, 8 figures",
        "journal-ref": "Physica A, 503(2018) 1092-1106",
        "doi": "10.1016/j.physa.2018.08.055",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend prior evidence that naively using intraday agent-based models that\ninvolve realistic order-matching processes for modeling continuous-time double\nauction markets seems to fail to be able to provide a robust link between data\nand many model parameters, even when these models are able to reproduce a\nnumber of well-known stylized facts of return time series. We demonstrate that\nwhile the parameters of intraday agent-based models rooted in market\nmicrostructure can be meaningfully calibrated, those exclusively related to\nagent behaviors and incentives remain problematic. This could simply be a\nfailure of the calibration techniques used but we argue that the observed\nparameter degeneracies are most likely a consequence of the realistic matching\nprocesses employed in these models. This suggests that alternative approaches\nto linking data, phenomenology and market structure may be necessary and that\nit is conceivable that one could construct a useful model that does not\ndirectly depend on the nuances of agent behaviors, even when it is known that\nthe real agents engage in complex behaviors.\n"
    },
    {
        "paper_id": 1611.09062,
        "authors": "N.S. Gonchar",
        "title": "Generalization of Doob Decomposition Theorem and Risk Assessment in\n  Incomplete Markets",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1601.03574",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, we introduce the notion of a local regular supermartingale\nrelative to a convex set of equivalent measures and prove for it the necessary\nand sufficient conditions of optional Doob decomposition in the discrete case.\nThis Theorem is a generalization of the famous Doob decomposition onto the case\nof supermartingales relative to a convex set of equivalent measures. The\ndescription of all local regular supermartingales relative to a convex set of\nequivalent measures is presented. A notion of complete set of equivalent\nmeasures is introduced. We prove that every non negative bounded\nsupermartingale relative to a complete set of equivalent measures is local\nregular. A new definition of fair price of contingent claim in incomplete\nmarket is given and a formula for fair price of Standard option of European\ntype is found.\n"
    },
    {
        "paper_id": 1611.09179,
        "authors": "Miryana Grigorova, Peter Imkeller, Youssef Ouknine, Marie-Claire\n  Quenez (LPMA)",
        "title": "Optimal stopping with f -expectations: the irregular case",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal stopping problem with non-linear $f$-expectation\n(induced by a BSDE) without making any regularity assumptions on the reward\nprocess $\\xi$. and with general filtration. We show that the value family can\nbe aggregated by an optional process $Y$. We characterize the process $Y$ as\nthe $\\mathcal{E}^f$-Snell envelope of $\\xi$. We also establish an infinitesimal\ncharacterization of the value process $Y$ in terms of a Reflected BSDE with\n$\\xi$ as the obstacle. To do this, we first establish a comparison theorem for\nirregular RBSDEs. We give an application to the pricing of American options\nwith irregular pay-off in an imperfect market model.\n"
    },
    {
        "paper_id": 1611.093,
        "authors": "Rohini Kumar and Hussein Nasralah",
        "title": "Asymptotic approximation of optimal portfolio for small time horizons",
        "comments": "17 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of portfolio optimization in a simple incomplete\nmarket and under a general utility function. By working with the associated\nHamilton-Jacobi-Bellman partial differential equation (HJB PDE), we obtain a\nclosed-form formula for a trading strategy which approximates the optimal\ntrading strategy when the time horizon is small. This strategy is generated by\na first order approximation to the value function. The approximate value\nfunction is obtained by constructing classical sub- and super-solutions to the\nHJB PDE using a formal expansion in powers of horizon time. Martingale\ninequalities are used to sandwich the true value function between the\nconstructed sub- and super-solutions. A rigorous proof of the accuracy of the\napproximation formulas is given. We end with a heuristic scheme for extending\nour small-time approximating formulas to approximating formulas in a finite\ntime horizon.\n"
    },
    {
        "paper_id": 1611.09631,
        "authors": "Christa Cuchiero, Walter Schachermayer and Ting-Kam Leonard Wong",
        "title": "Cover's universal portfolio, stochastic portfolio theory and the\n  numeraire portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cover's celebrated theorem states that the long run yield of a properly\nchosen \"universal\" portfolio is as good as the long run yield of the best\nretrospectively chosen constant rebalanced portfolio. The \"universality\"\npertains to the fact that this result is model-free, i.e., not dependent on an\nunderlying stochastic process. We extend Cover's theorem to the setting of\nstochastic portfolio theory as initiated by R. Fernholz: the rebalancing rule\nneed not to be constant anymore but may depend on the present state of the\nstock market. This model-free result is complemented by a comparison with the\nlog-optimal numeraire portfolio when fixing a stochastic model of the stock\nmarket. Roughly speaking, under appropriate assumptions, the optimal long run\nyield coincides for the three approaches mentioned in the title of this paper.\nWe present our results in discrete and continuous time.\n"
    },
    {
        "paper_id": 1611.09893,
        "authors": "Michele Coscia, Ricardo Hausmann, Frank Neffke",
        "title": "Exploring the Uncharted Export: an Analysis of Tourism-Related Foreign\n  Expenditure with International Spend Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tourism is one of the most important economic activities in the world: for\nmany countries it represents the single largest product in their export basket.\nHowever, it is a product difficult to chart: \"exporters\" of tourism do not ship\nit abroad, but they welcome importers inside the country. Current research uses\nsocial accounting matrices and general equilibrium models, but the standard\nindustry classifications they use make it hard to identify which domestic\nindustries cater to foreign visitors. In this paper, we make use of open source\ndata and of anonymized and aggregated transaction data giving us insights about\nthe spend behavior of foreigners inside two countries, Colombia and the\nNetherlands, to inform our research. With this data, we are able to describe\nwhat constitutes the tourism sector, and to map the most attractive\ndestinations for visitors. In particular, we find that countries might observe\ndifferent geographical tourists' patterns -- concentration versus\ndecentralization --; we show the importance of distance, a country's reported\nwealth and cultural affinity in informing tourism; and we show the potential of\ncombining open source data and anonymized and aggregated transaction data on\nforeign spend patterns in gaining insight as to the evolution of tourism from\none year to another.\n"
    },
    {
        "paper_id": 1611.09926,
        "authors": "Mikhail Timonin",
        "title": "Choquet integral in decision analysis - lessons from the axiomatization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Choquet integral is a powerful aggregation operator which lists many\nwell-known models as its special cases. We look at these special cases and\nprovide their axiomatic analysis. In cases where an axiomatization has been\npreviously given in the literature, we connect the existing results with the\nframework that we have developed. Next we turn to the question of learning,\nwhich is especially important for the practical applications of the model. So\nfar, learning of the Choquet integral has been mostly confined to the learning\nof the capacity. Such an approach requires making a powerful assumption that\nall dimensions (e.g. criteria) are evaluated on the same scale, which is rarely\njustified in practice. Too often categorical data is given arbitrary numerical\nlabels (e.g. AHP), and numerical data is considered cardinally and ordinally\ncommensurate, sometimes after a simple normalization. Such approaches clearly\nlack scientific rigour, and yet they are commonly seen in all kinds of\napplications. We discuss the pros and cons of making such an assumption and\nlook at the consequences which axiomatization uniqueness results have for the\nlearning problems. Finally, we review some of the applications of the Choquet\nintegral in decision analysis. Apart from MCDA, which is the main area of\ninterest for our results, we also discuss how the model can be interpreted in\nthe social choice context. We look in detail at the state-dependent utility,\nand show how comonotonicity, central to the previous axiomatizations, actually\nimplies state-independency in the Choquet integral model. We also discuss the\nconditions required to have a meaningful state-dependent utility representation\nand show the novelty of our results compared to the previous methods of\nbuilding state-dependent models.\n"
    },
    {
        "paper_id": 1612.00221,
        "authors": "Sven Banisch and Eckehard Olbrich",
        "title": "The Coconut Model with Heterogeneous Strategies and Learning",
        "comments": "Accepted for publication in the Journal of Artificial Societies and\n  Social Simulation (JASSS)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop an agent-based version of the Diamond search\nequilibrium model - also called Coconut Model. In this model, agents are faced\nwith production decisions that have to be evaluated based on their expectations\nabout the future utility of the produced entity which in turn depends on the\nglobal production level via a trading mechanism. While the original dynamical\nsystems formulation assumes an infinite number of homogeneously adapting agents\nobeying strong rationality conditions, the agent-based setting allows to\ndiscuss the effects of heterogeneous and adaptive expectations and enables the\nanalysis of non-equilibrium trajectories. Starting from a baseline\nimplementation that matches the asymptotic behavior of the original model, we\nshow how agent heterogeneity can be accounted for in the aggregate dynamical\nequations. We then show that when agents adapt their strategies by a simple\ntemporal difference learning scheme, the system converges to one of the fixed\npoints of the original system. Systematic simulations reveal that this is the\nonly stable equilibrium solution.\n"
    },
    {
        "paper_id": 1612.0027,
        "authors": "Boris Podobnik and Marko Jusup and H. Eugene Stanley",
        "title": "Predicting the rise of right-wing populism in response to unbalanced\n  immigration",
        "comments": "11 pages, 7 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Among the central tenets of globalization is free migration of labor.\nAlthough much has been written about its benefits, little is known about the\nlimitations of globalization, including how immigration affects the\nanti-globalist sentiment. Analyzing polls data, we find that over the last\nthree years in a group of EU countries affected by the recent migrant crisis,\nthe percentage of right-wing (RW) populist voters in a given country depends on\nthe prevalence of immigrants in this country's population and the total\nimmigration inflow into the entire EU. The latter is likely due to the EU\nresembling a supranational state, where the lack of inner borders causes that\n\"somebody else's problem\" easily turns into \"my problem\". We further find that\nthe increase in the percentage of RW voters substantially surpasses the\nimmigration inflow, implying that if this process continues, RW populism may\ndemocratically prevail and eventually lead to a demise of globalization. We\npresent evidence for tipping points in relation to the rise of RW populism.\nFinally, we model these empirical findings using a complex network framework\nwherein the success of globalization largely rests on the balance between\nimmigration and immigrant integration.\n"
    },
    {
        "paper_id": 1612.00402,
        "authors": "Maciej Balajewicz and Jari Toivanen",
        "title": "Reduced Order Models for Pricing European and American Options under\n  Stochastic Volatility and Jump-Diffusion Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  European options can be priced by solving parabolic partial(-integro)\ndifferential equations under stochastic volatility and jump-diffusion models\nlike Heston, Merton, and Bates models. American option prices can be obtained\nby solving linear complementary problems (LCPs) with the same operators. A\nfinite difference discretization leads to a so-called full order model (FOM).\nReduced order models (ROMs) are derived employing proper orthogonal\ndecomposition (POD). The early exercise constraint of American options is\nenforced by a penalty on subset of grid points. The presented numerical\nexperiments demonstrate that pricing with ROMs can be orders of magnitude\nfaster within a given model parameter variation range.\n"
    },
    {
        "paper_id": 1612.0072,
        "authors": "David Hobson, Alex S.L. Tse, Yeqi Zhu",
        "title": "Optimal consumption and investment under transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider the Merton problem in a market with a single\nrisky asset and transaction costs. We give a complete solution of the problem\nup to the solution of a free-boundary problem for a first-order differential\nequation, and find that the form of the solution (whether the problem is\nwell-posed, whether the problem is well-posed only for large transaction costs,\nwhether the no-transaction wedge lies in the first, second or fourth quadrants)\ndepends only on a quadratic whose co-efficients are functions of the parameters\nof the problem, and then only through the value and slope of this quadratic at\nzero, one and the turning point.\n  We find that for some parameter values and for large transaction costs the\nlocation of the boundary at which sales of the risky asset occur is independent\nof the transaction cost on purchases. We give both a mathematical and financial\nreason for this phenomena.\n"
    },
    {
        "paper_id": 1612.0078,
        "authors": "Jun Maeda and Saul D. Jacka",
        "title": "A Market Driver Volatility Model via Policy Improvement Algorithm",
        "comments": "24 pages with 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the over-the-counter market in derivatives, we sometimes see large numbers\nof traders taking the same position and risk. When there is this kind of\nconcentration in the market, the position impacts the pricings of all other\nderivatives and changes the behaviour of the underlying volatility in a\nnonlinear way. We model this effect using Heston's stochastic volatility model\nmodified to take into account the impact. The impact can be incorporated into\nthe model using a special product called a market driver, potentially with a\nlarge face value, affecting the underlying volatility itself. We derive a\nrevised version of Heston's partial differential equation which is to be\nsatisfied by arbitrary derivatives products in the market. This enables us to\nobtain valuations that reflect the actual market and helps traders identify the\nrisks and hold appropriate assets to correctly hedge against the impact of the\nmarket driver.\n"
    },
    {
        "paper_id": 1612.00828,
        "authors": "Abootaleb Shirvani, Stoyan V. Stoyanov, Svetlozar T. Rachev, and Frank\n  J. Fabozzi",
        "title": "A New Set of Financial Instruments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In complete markets, there are risky assets and a riskless asset. It is\nassumed that the riskless asset and the risky asset are traded continuously in\ntime and that the market is frictionless. In this paper, we propose a new\nmethod for hedging derivatives assuming that a hedger should not always rely on\ntrading existing assets that are used to form a linear portfolio comprised of\nthe risky asset, the riskless asset, and standard derivatives, but rather\nshould design a set of specific, most-suited financial instruments for the\nhedging problem. We introduce a sequence of new financial instruments best\nsuited for hedging jump-diffusion and stochastic volatility market models. The\nnew instruments we introduce are perpetual derivatives. More specifically, they\nare options with perpetual maturities. In a financial market where perpetual\nderivatives are introduced, there is a new set of partial and partial-integro\ndifferential equations for pricing derivatives. Our analysis demonstrates that\nthe set of new financial instruments together with a risk measure called the\ntail-loss ratio measure defined by the new instrument's return series can be\npotentially used as an early warning system for a market crash.\n"
    },
    {
        "paper_id": 1612.00833,
        "authors": "Mahmood Mahmoudzadeh, Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Measuring and Analyzing the Shares of Economic Growth Sources in the\n  Mining Sector of Iran: A Neoclassical Growth Accounting Approach",
        "comments": "25 pages",
        "journal-ref": "Iranian Economic Journal - Macroeconomics (IEJM), No. 13, pp.\n  121-142",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this study is to measure the Total Factor Productivity (TFP)\ngrowth and determine the share of each of the economic growth sources in the\nmining sector of Iran. The time period of this study is 1355-1385 of the Solar\nHijri calendar (roughly overlaying with the time period of 1976-2006 of the\nGregorian calendar). In this paper, the shares of total factor productivity\ngrowth (TFPG) and factors' accumulations in the economic growth of the mining\nsector are estimated using a neoclassical growth accounting approach. Based on\nthe estimated restricted Cobb-Douglas production function and the results\nobtained from the Solow residual equation, the annual growth rates of TFP were\nmeasured for each year. According to the findings, the average annual growth\nrate of TFP has been 2.94% during the time period of the present study. The\nother findings of this study indicate that the average contributions of TFPG,\nlabor accumulation and capital accumulation in the economic growth of the\nmining sector have been 56%, 23%, and 21%, respectively, during the time period\nof the study. As such, it can be concluded that the policy of benefiting from\navailable factors in the mining sector together with the policy of accumulating\nfactors have simultaneously caused the value-added growth of this sector.\nTherefore, considering the desired performance of the mining sector in terms of\nits sizable productivity growth, it can be argued that the mining sector can\naid Iran's economic development plans to achieve their assigned economic\nobjectives, one of which is to increase the share of total factor productivity\ngrowth in economic growth.\n"
    },
    {
        "paper_id": 1612.00981,
        "authors": "V\\'it Per\\v{z}ina and Jan M. Swart",
        "title": "How much market making does a market need?",
        "comments": "Changed the title. 15 pages, 2 figures. The 2016 Applied Probability\n  Trust Lecture",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a simple model for the evolution of a limit order book in which\nlimit orders of unit size arrive according to independent Poisson processes.\nThe frequencies of buy limit orders below a given price level, respectively\nsell limit orders above a given level are described by fixed demand and supply\nfunctions. Buy (resp. sell) limit orders that arrive above (resp. below) the\ncurrent ask (resp. bid) price are converted into market orders. There is no\ncancellation of limit orders. This model has independently been reinvented by\nseveral authors, including Stigler in 1964 and Luckock in 2003, who was able to\ncalculate the equilibrium distribution of the bid and ask prices. We extend the\nmodel by introducing market makers that simultaneously place both a buy and\nsell limit order at the current bid and ask price. We show how the introduction\nof market makers reduces the spread, which in the original model is\nunrealistically large. In particular, we are able to calculate the exact rate\nat which market makers need to place orders in order to close the spread\ncompletely. If this rate is exceeded, we show that the price settles at a\nrandom level that in general does not correspond the Walrasian equilibrium\nprice.\n"
    },
    {
        "paper_id": 1612.01013,
        "authors": "Tim Leung, Hyungbin Park",
        "title": "Long-Term Growth Rate of Expected Utility for Leveraged ETFs: Martingale\n  Extraction Approach",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the long-term growth rate of expected utility from holding\na leveraged exchanged-traded fund (LETF), which is a constant proportion\nportfolio of the reference asset. Working with the power utility function, we\ndevelop an analytical approach that employs martingale extraction and involves\nfinding the eigenpair associated with the infinitesimal generator of a\nMarkovian time-homogeneous diffusion. We derive explicitly the long-term growth\nrates under a number of models for the reference asset, including the geometric\nBrownian motion model, GARCH model, inverse GARCH model, extended CIR model,\n3/2 model, quadratic model, as well as the Heston and 3/2 stochastic volatility\nmodels. We also investigate the impact of stochastic interest rate such as the\nVasicek model and the inverse GARCH short rate model. We determine the optimal\nleverage ratio for the long-term investor and examine the effects of model\nparameters.\n"
    },
    {
        "paper_id": 1612.01132,
        "authors": "Jake J. Xia",
        "title": "A Model of Synchronization for Self-Organized Crowding Behavior",
        "comments": "21 pages, 6 figures, revised from 2006 working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a general model for synchronized crowding behavior. An\norder parameter is introduced to quantify the level of synchronization which is\nshown a function of percentage of agents in reactive state. Further,\nsynchronization is shown to be driven by the most active agents with the\nhighest volatility. A tipping point is identified when crowd becomes\nself-amplifying and unstable. By applying this model, financial bubbles, market\nmomentum and volatility patterns are simulated.\n"
    },
    {
        "paper_id": 1612.01155,
        "authors": "Xu Wang, Ryan P. Badman",
        "title": "A Multifaceted Panel Data Gravity Model Analysis of Peru's Foreign Trade",
        "comments": "16 pages, 8 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Peru's abundant natural resources and friendly trade policies has made the\ncountry a major economic player in both South America and the global community.\nConsequently, exports are playing an increasingly important role in Peru's\nnational economy. Indeed, growing from 13.1% as of 1994, exports now contribute\napproximately 21% of the GDP of Peru as of 2015. Given Peru's growing global\ninfluence, the time is ripe for a thorough analysis of the most important\nfactors governing its export performance. Thus, within the framework of the\naugmented gravity model of trade, this paper examines Peru's export performance\nand attempts to identify the dominant economic factors that should be further\ndeveloped to increase the value of exports. The analysis was conducted from\nthree different aspects: (1) general economic parameters' effect on Peru's\nexport value, (2) more specific analysis into a major specific trade good,\ncopper, and (3) the impact that regional trade agreements have had on Peru's\nexport performance. Our panel data analysis results for each dataset revealed\ninteresting economic trends and were consistent with the theoretical\nexpectations of the gravity model: namely positive coefficients for economic\nsize and negative coefficients for distance. This report's results can be a\nreference for the proper direction of Peruvian economic policy so as to enhance\neconomic growth in a sustainable direction.\n"
    },
    {
        "paper_id": 1612.01232,
        "authors": "Takaki Hayashi, Yuta Koike",
        "title": "Wavelet-based methods for high-frequency lead-lag analysis",
        "comments": "37 pages, 2 figures. To appear in SIAM Journal on Financial\n  Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel framework to investigate lead-lag relationships between\ntwo financial assets. Our framework bridges a gap between continuous-time\nmodeling based on Brownian motion and the existing wavelet methods for lead-lag\nanalysis based on discrete-time models and enables us to analyze the\nmulti-scale structure of lead-lag effects. We also present a statistical\nmethodology for the scale-by-scale analysis of lead-lag effects in the proposed\nframework and develop an asymptotic theory applicable to a situation including\nstochastic volatilities and irregular sampling. Finally, we report several\nnumerical experiments to demonstrate how our framework works in practice.\n"
    },
    {
        "paper_id": 1612.01302,
        "authors": "Johannes Muhle-Karbe and Max Reppen and H. Mete Soner",
        "title": "A Primer on Portfolio Choice with Small Transaction Costs",
        "comments": "30 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey is an introduction to asymptotic methods for portfolio-choice\nproblems with small transaction costs. We outline how to derive the\ncorresponding dynamic programming equations and simplify them in the small-cost\nlimit. This allows to obtain explicit solutions in a wide range of settings,\nwhich we illustrate for a model with mean-reverting expected returns and\nproportional transaction costs. For even more complex models, we present a\npolicy iteration scheme that allows to compute the solution numerically.\n"
    },
    {
        "paper_id": 1612.01327,
        "authors": "David Hobson, Alex S.L. Tse, Yeqi Zhu",
        "title": "A multi-asset investment and consumption problem with transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we study a multi-asset version of the Merton investment and\nconsumption problem with proportional transaction costs. In general it is\ndifficult to make analytical progress towards a solution in such problems, but\nwe specialise to a case where transaction costs are zero except for sales and\npurchases of a single asset which we call the illiquid asset.\n  Assuming agents have CRRA utilities and asset prices follow exponential\nBrownian motions we show that the underlying HJB equation can be transformed\ninto a boundary value problem for a first order differential equation. The\noptimal strategy is to trade the illiquid asset only when the fraction of the\ntotal portfolio value invested in this asset falls outside a fixed interval.\nImportant properties of the multi-asset problem (including when the problem is\nwell-posed, ill-posed, or well-posed only for large transaction costs) can be\ninferred from the behaviours of a quadratic function of a single variable and\nanother algebraic function.\n"
    },
    {
        "paper_id": 1612.01624,
        "authors": "Yong Tao and Xiangjun Wu and Tao Zhou and Weibo Yan and Yanyuxiang\n  Huang and Han Yu and Benedict Mondal and Victor M. Yakovenko",
        "title": "Exponential Structure of Income Inequality: Evidence from 67 Countries",
        "comments": "V.1: 22 pages, 4 figures, 4 tables; V.2: 39 pages, 4 figures, 5\n  tables, new section on consistent estimate of fit parameters, accepted to\n  Journal of Economic Interaction and Coordination",
        "journal-ref": "Journal of Economic Interaction and Coordination, 14, 345 (2019)",
        "doi": "10.1007/s11403-017-0211-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic competition between humans leads to income inequality, but, so far,\nthere has been little understanding of underlying quantitative mechanisms\ngoverning such a collective behavior. We analyze datasets of household income\nfrom 67 countries, ranging from Europe to Latin America, North America and\nAsia. For all of the countries, we find a surprisingly uniform rule: Income\ndistribution for the great majority of populations (low and middle income\nclasses) follows an exponential law. To explain this empirical observation, we\npropose a theoretical model within the standard framework of modern economics\nand show that free competition and Rawls' fairness are the underlying\nmechanisms producing the exponential pattern. The free parameters of the\nexponential distribution in our model have an explicit economic interpretation\nand direct relevance to policy measures intended to alleviate income\ninequality.\n"
    },
    {
        "paper_id": 1612.01951,
        "authors": "Yiran Cui and Sebastian del Bano Rollin and Guido Germano",
        "title": "Stability of calibration procedures: fractals in the Black-Scholes model",
        "comments": "13 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Usually, in the Black-Scholes pricing theory the volatility is a positive\nreal parameter. Here we explore what happens if it is allowed to be a complex\nnumber. The function for pricing a European option with a complex volatility\nhas essential singularities at zero and infinity. The singularity at zero\nreflects the put-call parity. Solving for the implied volatility that\nreproduces a given market price yields not only a real root, but also\ninfinitely many complex roots in a neighbourhood of the origin. The\nNewton-Raphson calculation of the complex implied volatility has a chaotic\nnature described by fractals.\n"
    },
    {
        "paper_id": 1612.01979,
        "authors": "Y. S. Kim, S. Stoyanov, S. Rachev, F. Fabozzi",
        "title": "Multi-Purpose Binomial Model: Fitting all Moments to the Underlying\n  Geometric Brownian Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a binomial tree model fitting all moments to the approximated\ngeometric Brownian motion. Our construction generalizes the classical\nCox-Ross-Rubinstein, the Jarrow-Rudd, and the Tian binomial tree models. The\nnew binomial model is used to resolve a discontinuity problem in option\npricing.\n"
    },
    {
        "paper_id": 1612.02024,
        "authors": "Marinho Bertanha and Marcelo J. Moreira",
        "title": "Impossible Inference in Econometrics: Theory and Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies models in which hypothesis tests have trivial power, that\nis, power smaller than size. This testing impossibility, or impossibility type\nA, arises when any alternative is not distinguishable from the null. We also\nstudy settings in which it is impossible to have almost surely bounded\nconfidence sets for a parameter of interest. This second type of impossibility\n(type B) occurs under a condition weaker than the condition for type A\nimpossibility: the parameter of interest must be nearly unidentified. Our\ntheoretical framework connects many existing publications on impossible\ninference that rely on different notions of topologies to show models are not\ndistinguishable or nearly unidentified. We also derive both types of\nimpossibility using the weak topology induced by convergence in distribution.\nImpossibility in the weak topology is often easier to prove, it is applicable\nfor many widely-used tests, and it is useful for robust hypothesis testing. We\nconclude by demonstrating impossible inference in multiple economic\napplications of models with discontinuity and time-series models.\n"
    },
    {
        "paper_id": 1612.02112,
        "authors": "Svetlozar Rachev and Frank Fabozzi",
        "title": "Financial market with no riskless (safe) asset",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study markets with no riskless (safe) asset. We derive the corresponding\nBlack-Scholes-Merton option pricing equations for markets where there are only\nrisky assets which have the following price dynamics: (i) continuous\ndiffusions; (ii) jump-diffusions; (iii) diffusions with stochastic\nvolatilities, and; (iv) geometric fractional Brownian and Rosenblatt motions.\nNo arbitrage and market completeness conditions are derived in all four cases.\n"
    },
    {
        "paper_id": 1612.02312,
        "authors": "Alet Roux and Tomasz Zastawniak",
        "title": "Game options with gradual exercise and cancellation under proportional\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Game (Israeli) options in a multi-asset market model with proportional\ntransaction costs are studied in the case when the buyer is allowed to exercise\nthe option and the seller has the right to cancel the option gradually at a\nmixed (or randomised) stopping time, rather than instantly at an ordinary\nstopping time. Allowing gradual exercise and cancellation leads to increased\nflexibility in hedging, and hence tighter bounds on the option price as\ncompared to the case of instantaneous exercise and cancellation. Algorithmic\nconstructions for the bid and ask prices, and the associated superhedging\nstrategies and optimal mixed stopping times for both exercise and cancellation\nare developed and illustrated. Probabilistic dual representations for bid and\nask prices are also established.\n"
    },
    {
        "paper_id": 1612.02444,
        "authors": "Jos\\'e-Luis P\\'erez and Kazutoshi Yamazaki",
        "title": "Hybrid continuous and periodic barrier strategies in the dual model:\n  optimality and fluctuation identities",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Avanzi et al. (2016) recently studied an optimal dividend problem where\ndividends are paid both periodically and continuously with different\ntransaction costs. In the Brownian model with Poissonian periodic dividend\npayment opportunities, they showed that the optimal strategy is either of the\npure-continuous, pure-periodic, or hybrid-barrier type. In this paper, we\ngeneralize the results of their previous study to the dual (spectrally positive\nL\\'evy) model. The optimal strategy is again of the hybrid-barrier type and can\nbe concisely expressed using the scale function. These results are confirmed\nthrough a sequence of numerical experiments.\n"
    },
    {
        "paper_id": 1612.02567,
        "authors": "Peter A. Bebbington and Julius Bonart",
        "title": "Order statistics of horse racing and the randomly broken stick",
        "comments": "6 pages, 1 figure, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We find a remarkable agreement between the statistics of a randomly divided\ninterval and the observed statistical patterns and distributions found in horse\nracing betting markets. We compare the distribution of implied winning odds,\nthe average true winning probabilities, the implied odds conditional on a win,\nand the average implied odds of the winning horse with the corresponding\nquantities from the \"randomly broken stick problem\". We observe that the market\nis at least to some degree informationally efficient. From the mapping between\nexponential random variables and the statistics of the random division we\nconclude that horses' true winning abilities are exponentially distributed.\n"
    },
    {
        "paper_id": 1612.02653,
        "authors": "Lu-Yi Qiu and Ling-Yun He",
        "title": "Are Chinese transport policies effective? A new perspective from direct\n  pollution rebound effect, and empirical evidence from road transport sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The air pollution has become a serious challenge in China. Emissions from\nmotor vehicles have been found as one main source of air pollution. Although\nthe Chinese government has taken numerous policies to mitigate the harmful\nemissions from road transport sector, it is still uncertain for both policy\nmakers and researchers to know to what extent the policies are effective in the\nshort and long terms. Inspired by the concept and empirical results from\ncurrent literature on energy rebound effect (ERE), we first propose a new\nconcept of pollution rebound effect (PRE). Then, we estimate direct air PRE as\na measure for the effectiveness of the policies of reducing air pollution from\ntransport sector based on time-series data from the period 1986-2014. We find\nthat the short-term direct air PRE is -1.4105, and the corresponding long-run\nPRE is -1.246. The negative results indicate that the direct air PRE does not\nexist in road passenger transport sector in China, either in the short term or\nin the long term during the period 1986-2014. This implies that the Chinese\ntransport policies are effective in terms of harmful emissions reduction in the\ntransport sector. This research, to the best of our knowledge, is the first\nattempt to quantify the effectiveness of the transport policies in the\ntransitional China.\n"
    },
    {
        "paper_id": 1612.02654,
        "authors": "Ling-Yun He, Wei Wei",
        "title": "China building energy consumption: definitions and measures from an\n  operational perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is an increasing awareness of the significance of Chinese building\nenergy consumption(BEC). However, something worth discussing is that estimate\nthe building energy consumption adopting the definition of life cycle or\noperation. In the existing studies with various evaluation methods, the issue\nabout the amount of energy consumed by China buildings has not been understood.\nIn order to settle the disputes over the calculation of BEC, this paper\nestablish an appropriate accounting method of building energy to present BEC\nsituation in China and lay the foundation for building energy efficiency.\nAdopting the conception of building operational energy consumption, we find\nthat the energy consumption of buildings just accounts for 15% - 16% of the\nfinal total energy consumption in China; by contrast, the previous calculations\nusually have double accounting through top-down approach if central heat-supply\nof buildings was given into additional consideration.\n"
    },
    {
        "paper_id": 1612.02656,
        "authors": "Ling-yun He and Li Liu",
        "title": "The demand for road transport in China: imposing theoretical regularity\n  and flexible functional forms selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Road transport sector is found to be one of the major emitters, and\nresponsible for serious air pollution and huge pubic health losses. One\nimportant parameter for determining the consequences of transport demand shocks\nfor the macroeconomy, air pollution and public health is the elasticity of the\ndemand for transport. Most published studies that use flexible functional forms\nhave ignored the theoretical regularity conditions implied by microeconomic\ntheories. Moreover, even a few studies have checked and/or imposed regularity\nconditions, most of them equate curvature alone with regularity, thus ignoring\nor minimizing the importance of other regularities. And then, the results\nappear biased and may in fact be biased. Therefore, we select three of the most\nwidely used flexible functional forms, the Rotterdam model, the Almost Ideal\nDemand System (AIDS), and the quadratic AIDS (QUAIDS) to investigate the demand\nfor road transport in China using recent annual expenditure data, over a 13\nyear period from 2002 to 2014, on three expenditure categories in the\ntransportation sector: private transportation, local transportation and\nintercity transportation. Estimation shows that the AIDS model is the only\nmodel that is able to provide theoretically consistent estimates of the\nresidents demand for road transport in China. Our estimates show that the\nprivate transportation is a luxury among the transportation goods, and is\nelastic in price changes relatively. The empirical results imply that the\nprivate and the local transportation, the local and intercity transportation\nare gross complements. And, the private transportation is a substitute for the\ninter-city transportation, while the intercity transportation is a complement\nof the private transportation.\n"
    },
    {
        "paper_id": 1612.02657,
        "authors": "Jian-Xin Wu, Ling-Yun He",
        "title": "How do Chinese cities grow? A distribution dynamics approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2016.11.112",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the dynamic behavior of city size using a distribution\ndynamics approach with Chinese city data for the period 1984-2010. Instead of\nconvergence, divergence or paralleled growth, multimodality and persistence are\nthe dominant characteristics in the distribution dynamics of Chinese\nprefectural cities. Moreover, initial city size matters, initially small and\nmedium-sized cities exhibit strong tendency of convergence, while large cities\nshow significant persistence and multimodality in the sample period.\nExamination on the regional city groups shows that locational fundamentals have\nimportant impact on the distribution dynamics of city size.\n"
    },
    {
        "paper_id": 1612.02658,
        "authors": "Jian-Xin Wu and Ling-Yun He",
        "title": "The distribution dynamics of Carbon Dioxide Emission intensity across\n  Chinese provinces: A weighted Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the distribution dynamics of carbon dioxide (CO2)\nemission intensity across 30 Chinese provinces using a weighted distribution\ndynamics approach. The results show that CO2 emission intensity tends to\ndiverge during the sample period of 1995-2014. However, convergence clubs are\nfound in the ergodic distributions of the full sample and two sub-sample\nperiods. Divergence, polarization and stratification are the dominant\ncharacteristics in the distribution dynamics. Weightings with economic and\npopulation sizes have important impacts on current distributions and hence long\nrun steady distributions. Neglecting economic size may under-estimate the\ndeterioration in the long run steady state. The result also shows that\nconditioning on space and income cannot eliminate the multimodality in the long\nrun distribution. However, capital intensity has important impact on the\nformation of convergence clubs. Our findings have contributions in the\nunderstanding of the spatial dynamic behaviours of CO2 emissions across Chinese\nprovinces, and have important policy implications for CO2 emissions reduction\nin China.\n"
    },
    {
        "paper_id": 1612.02666,
        "authors": "Barack Wamkaya Wanjawa",
        "title": "Evaluating the Performance of ANN Prediction System at Shanghai Stock\n  Market in the Period 21-Sep-2016 to 11-Oct-2016",
        "comments": "13 pages, 4 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n"
    },
    {
        "paper_id": 1612.02985,
        "authors": "Stanislaus Maier-Paape",
        "title": "Risk averse fractional trading using the current drawdown",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the fractional trading ansatz of money management is\nreconsidered with special attention to chance and risk parts in the goal\nfunction of the related optimization problem. By changing the goal function\nwith due regards to other risk measures like current drawdowns, the optimal\nfraction solutions reflect the needs of risk averse investors better than the\noriginal optimal f solution of Ralph Vince.\n  Keywords: fractional trading, optimal f, current drawdown, terminal wealth\nrelative, risk aversion\n"
    },
    {
        "paper_id": 1612.03031,
        "authors": "Antonio Cosma, Stefano Galluccio, Paola Pederzoli, Olivier Scaillet",
        "title": "Early exercise decision in American options with dividends, stochastic\n  volatility and jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a fast numerical technique, we investigate a large database of investor\nsuboptimal non-exercise of short maturity American call options on\ndividend-paying stocks listed on the Dow Jones. The correct modelling of the\ndiscrete dividend is essential for a correct calculation of the early exercise\nboundary as confirmed by theoretical insights. Pricing with stochastic\nvolatility and jumps instead of the Black-Scholes-Merton benchmark cuts by a\nquarter the amount lost by investors through suboptimal exercise. The remaining\nthree quarters are largely unexplained by transaction fees and may be\ninterpreted as an opportunity cost for the investors to monitor optimal\nexercise.\n"
    },
    {
        "paper_id": 1612.03066,
        "authors": "Andreas Fr\\\"ohlich and Annegret Weng",
        "title": "Parameter uncertainty and reserve risk under Solvency II",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we consider the parameter risk in the context of internal\nmodelling of the reserve risk under Solvency II.\n  We discuss two opposed perspectives on parameter uncertainty and point out\nthat standard methods of classical reserving focusing on the estimation error\nof claims reserves are in general not appropriate to model the impact of\nparameter uncertainty upon the actual risk of economic losses from the\nundertakings's perspective.\n  Referring to the requirements of Solvency II we assess methods to model\nparameter uncertainty for the reserve risk by comparing the probability of\nsolvency actually attained when modelling the solvency risk capital requirement\nbased on the respective method to the required confidence level. Using the\nsimple example of a normal model we show that the bootstrapping approach is not\nappropriate to model parameter uncertainty according to this criterion. We then\npresent an adaptation of the approach proposed in \\cite {froehlich2014}.\nExperimental results demonstrate that this new method yields a risk capital\nmodel for the reserve risk achieving the required confidence level in good\napproximation.\n"
    },
    {
        "paper_id": 1612.03347,
        "authors": "Louis R. Eeckhoudt, Roger J. A. Laeven",
        "title": "Dual Moments and Risk Attitudes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In decision under risk, the primal moments of mean and variance play a\ncentral role to define the local index of absolute risk aversion. In this\npaper, we show that in canonical non-EU models dual moments have to be used\ninstead of, or on par with, their primal counterparts to obtain an equivalent\nindex of absolute risk aversion.\n"
    },
    {
        "paper_id": 1612.03698,
        "authors": "Sergey Kamenshchikov, Ilia Drozdov",
        "title": "Fractal Optimization of Market Neutral Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A fractal approach to the long-short portfolio optimization is proposed. The\nalgorithmic system based on the composition of market-neutral spreads into a\nsingle entity was considered. The core of the optimization scheme is a fractal\nwalk model of returns, optimizing a risk aversion according to the investment\nhorizon. The covariance matrix of spread returns has been used for the\noptimization and modified according to the Hurst stability analysis.\nOut-of-sample performance data has been represented for the space of exchange\ntraded funds in five period time period of observation. The considered\nportfolio system has turned out to be statistically more stable than a passive\ninvestment into benchmark with higher risk adjusted cumulative return over the\nobserved period.\n"
    },
    {
        "paper_id": 1612.04126,
        "authors": "Alicja Wolny-Dominiak",
        "title": "The hierarchical generalized linear model and the bootstrap estimator of\n  the error of prediction of loss reserves in a non-life insurance company",
        "comments": "Loss Reserves; Hierarchical Generalized Linear Model; Error of\n  prediction; MSE; Parametric Bootstrap",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper presents the hierarchical generalized linear model (HGLM) for loss\nreserving in a non-life insurance company. Because in this case the error of\nprediction is expressed by a complex analytical formula, the error bootstrap\nestimator is proposed instead. Moreover, the bootstrap procedure is used to\nobtain full information about the error by applying quantiles of the absolute\nprediction error. The full R code is available on the Github\nhttps://github.com/woali/BootErrorLossReserveHGLM.\n"
    },
    {
        "paper_id": 1612.0437,
        "authors": "Panagiotis Papaioannou, Thomas Dionysopoulos, Dietmar Janetzko,\n  Constantinos Siettos",
        "title": "S&P500 Forecasting and Trading using Convolution Analysis of Major Asset\n  Classes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By monitoring the time evolution of the most liquid Futures contracts traded\nglobally as acquired using the Bloomberg API from 03 January 2000 until 15\nDecember 2014 we were able to forecast the S&P 500 index beating the Buy and\nHold trading strategy. Our approach is based on convolution computations of 42\nof the most liquid Futures contracts of four basic financial asset classes,\nnamely, equities, bonds, commodities and foreign exchange. These key assets\nwere selected on the basis of the global GDP ranking across countries worldwide\naccording to the lists published by the International Monetary Fund (IMF,\nReport for Selected Country Groups and Subjects, 2015). The main hypothesis is\nthat the shifts between the asset classes are smooth and are shaped by slow\ndynamics as trading decisions are shaped by several constraints associated with\nthe portfolios allocation, as well as rules restrictions imposed by state\nfinancial authorities. This hypothesis is grounded on recent research based on\nthe added value generated by diversification targets of market participants\nspecialized on active asset management, who try to efficiently and smoothly\nnavigate the market's volatility.\n"
    },
    {
        "paper_id": 1612.04407,
        "authors": "Yusong Li and Harry Zheng",
        "title": "Dynamic Convex Duality in Constrained Utility Maximization",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a constrained utility maximization problem following\nthe convex duality approach. After formulating the primal and dual problems, we\nconstruct the necessary and sufficient conditions for both the primal and dual\nproblems in terms of FBSDEs plus additional conditions. Such formulation then\nallows us to explicitly characterize the primal optimal control as a function\nof the adjoint process coming from the dual FBSDEs in a dynamic fashion and\nvice versa. Moreover, we also find that the optimal primal wealth process\ncoincides with the adjoint process of the dual problem and vice versa. Finally\nwe solve three constrained utility maximization problems, which contrasts the\nsimplicity of the duality approach we propose and the technical complexity of\nsolving the primal problems directly.\n"
    },
    {
        "paper_id": 1612.04507,
        "authors": "Jos\\'e E. Figueroa-L\\'opez and Cheng Li",
        "title": "Optimal Kernel Estimation of Spot Volatility of Stochastic Differential\n  Equations",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kernel Estimation is one of the most widely used estimation methods in\nnon-parametric Statistics, having a wide-range of applications, including spot\nvolatility estimation of stochastic processes. The selection of bandwidth and\nkernel function is of great importance, especially for the finite sample\nsettings commonly encountered in econometric applications. In the context of\nspot volatility estimation, most of the proposed selection methods are either\nlargely heuristic or just formally stated without any feasible implementation.\nIn this work, an objective method of bandwidth and kernel selection is\nproposed, under some mild conditions on the volatility, which not only cover\nclassical Brownian motion driven dynamics but also some processes driven by\nlong-memory fractional Brownian motions or other Gaussian processes. We\ncharacterize the leading order terms of the Mean Squared Error, which are also\nratified by central limit theorems for the estimation error. As a byproduct, an\napproximated optimal bandwidth is then obtained in closed form. This result\nallows us to develop a feasible plug-in type bandwidth selection procedure, for\nwhich, as a sub-problem, we propose a new estimator of the volatility of\nvolatility. The optimal selection of kernel function is also discussed. For\nBrownian Motion type volatilities, the optimal kernel function is proved to be\nthe exponential kernel. For fractional Brownian motion type volatilities,\nnumerical results to compute the optimal kernel are devised and, for the\ndeterministic volatility case, explicit optimal kernel functions of different\norders are derived. Simulation studies further confirm the good performance of\nthe proposed methods.\n"
    },
    {
        "paper_id": 1612.04512,
        "authors": "Florian K\\\"uhnlenz and Pedro H. J. Nardelli",
        "title": "Agent-based Model for Spot and Balancing Electricity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple, yet realistic, agent-based model of an electricity\nmarket. The proposed model combines the spot and balancing markets with a\nresolution of one minute, which enables a more accurate depiction of the\nphysical properties of the power grid. As a test, we compare the results\nobtained from our simulation to data from Nord Pool.\n"
    },
    {
        "paper_id": 1612.0499,
        "authors": "Patrick Gagliardini, Elisa Ossola and Olivier Scaillet",
        "title": "A diagnostic criterion for approximate factor structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build a simple diagnostic criterion for approximate factor structure in\nlarge cross-sectional equity datasets. Given a model for asset returns with\nobservable factors, the criterion checks whether the error terms are weakly\ncross-sectionally correlated or share at least one unobservable common factor.\nIt only requires computing the largest eigenvalue of the empirical\ncross-sectional covariance matrix of the residuals of a large unbalanced panel.\nA general version of this criterion allows us to determine the number of\nomitted common factors. The panel data model accommodates both time-invariant\nand time-varying factor structures. The theory applies to random coefficient\npanel models with interactive fixed effects under large cross-section and\ntime-series dimensions. The empirical analysis runs on monthly and quarterly\nreturns for about ten thousand US stocks from January 1968 to December 2011 for\nseveral time-invariant and time-varying specifications. For monthly returns, we\ncan choose either among time-invariant specifications with at least four\nfinancial factors, or a scaled three-factor specification. For quarterly\nreturns, we cannot select macroeconomic models without the market factor.\n"
    },
    {
        "paper_id": 1612.05021,
        "authors": "Jaeyong An, P. R. Kumar, and Le Xie",
        "title": "Dynamic Modeling of Price Responsive Demand in Real-time Electricity\n  Market: Empirical Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the price responsiveness of electricity consumption\nfrom empirical commercial and industrial load data obtained from Texas.\nEmploying a dynamical system perspective, we show that price responsive demand\ncan be modeled as a hybrid of a Hammerstein model with delay following a price\nsurge, and a linear ARX model under moderate price changes. It is observed that\nelectricity consumption therefore has unique characteristics including (1)\nqualitatively distinct response between moderate and extremely high prices; and\n(2) a time delay associated with the response to high prices. It is shown that\nthese observed features may render traditional approaches to demand response\nand retail pricing based on classical economic theories ineffective. In\nparticular, ultimate real-time retail pricing may be limitedly beneficial than\nas considered in classical economic theories.\n"
    },
    {
        "paper_id": 1612.05072,
        "authors": "Lorenzo Camponovo, Olivier Scaillet and Fabio Trojani",
        "title": "Predictability Hidden by Anomalous Observations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Testing procedures for predictive regressions with lagged autoregressive\nvariables imply a suboptimal inference in presence of small violations of ideal\nassumptions. We propose a novel testing framework resistant to such violations,\nwhich is consistent with nearly integrated regressors and applicable to\nmulti-predictor settings, when the data may only approximately follow a\npredictive regression model. The Monte Carlo evidence demonstrates large\nimprovements of our approach, while the empirical analysis produces a strong\nrobust evidence of market return predictability hidden by anomalous\nobservations, both in- and out-of-sample, using predictive variables such as\nthe dividend yield or the volatility risk premium.\n"
    },
    {
        "paper_id": 1612.05227,
        "authors": "Simone Manduchi",
        "title": "European banking supervision, the role of stress test. Some brief\n  considerations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A quick review of European financial stability institutions and the role of\nstress tests in the current juridical system.\n"
    },
    {
        "paper_id": 1612.05229,
        "authors": "Laurie Davies, Walter Kr\\\"amer",
        "title": "Stylized Facts and Simulating Long Range Financial Data",
        "comments": "24 pages 12 figures, Discussion papers SFB 823, Technische\n  Universit\\\"at Dortmund, Germany 2015 48/15",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method (implemented in an R-program) to simulate long-range\ndaily stock-price data. The program reproduces various stylized facts much\nbetter than various parametric models from the extended GARCH-family. In\nparticular, the empirically observed changes in unconditional variance are\ntruthfully mirrored in the simulated data.\n"
    },
    {
        "paper_id": 1612.05255,
        "authors": "Denis Belomestny, Stefan H\\\"afner and Mikhail Urusov",
        "title": "Stratified regression-based variance reduction approach for weak\n  approximation schemes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.matcom.2017.05.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we suggest a modification of the regression-based variance\nreduction approach recently proposed in Belomestny et al. This modification is\nbased on the stratification technique and allows for a further significant\nvariance reduction. The performance of the proposed approach is illustrated by\nseveral numerical examples.\n"
    },
    {
        "paper_id": 1612.05525,
        "authors": "Mario Mureddu and Hildegard Meyer-Ortmanns",
        "title": "Extreme prices in electricity balancing markets from an approach of\n  statistical physics",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.09.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An increase in energy production from renewable energy sources is viewed as a\ncrucial achievement in most industrialized countries.\n  The higher variability of power production via renewables leads to a rise in\nancillary service costs over the power system, in particular costs within the\nelectricity balancing markets, mainly due to an increased number of extreme\nprice spikes. This study focuses on forecasting the behavior of price and\nvolumes of the Italian balancing market in the presence of an increased share\nof renewable energy sources. Starting from configurations of load and power\nproduction, which guarantee a stable performance, we implement fluctuations in\nthe load and in renewables; in particular we artificially increase the\ncontribution of renewables as compared to conventional power sources to cover\nthe total load. We then forecast the amount of provided energy in the balancing\nmarket and its fluctuations, which are induced by production and consumption.\nWithin an approach of agent based modeling we estimate the resulting energy\nprices and costs. While their average values turn out to be only slightly\naffected by an increased contribution from renewables, the probability for\nextreme price events is shown to increase along with undesired peaks in the\ncosts.\n"
    },
    {
        "paper_id": 1612.05681,
        "authors": "Roxana Dumitrescu, Marie-Claire Quenez and Agn\\`es Sulem",
        "title": "BSDEs with default jump",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the properties of nonlinear Backward Stochastic Differential\nEquations (BSDEs) driven by a Brownian motion and a martingale measure\nassociated with a default jump with intensity process $(\\lambda_t)$. We give a\npriori estimates for these equations and prove comparison and strict comparison\ntheorems. These results are generalized to drivers involving a singular\nprocess. The special case of a $\\lambda$-linear driver is studied, leading to a\nrepresentation of the solution of the associated BSDE in terms of a conditional\nexpectation and an adjoint exponential semi-martingale. We then apply these\nresults to nonlinear pricing of European contingent claims in an imperfect\nfinancial market with a totally defaultable risky asset. The case of claims\npaying dividends is also studied via a singular process.\n"
    },
    {
        "paper_id": 1612.05855,
        "authors": "Jiang Wu, Ricardas Zitikis",
        "title": "Should we opt for the Black Friday discounted price or wait until the\n  Boxing Day?",
        "comments": "16 pages;5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an optimal strategy for minimizing the expected loss in the\ntwo-period economy when a pivotal decision needs to be made during the first\ntime period and cannot be subsequently reversed. Our interest in the problem\nhas been motivated by the classical shopper's dilemma during the Black Friday\npromotion period, and our solution crucially relies on the pioneering work of\nMcDonnell and Abbott on the two-envelope paradox.\n"
    },
    {
        "paper_id": 1612.05952,
        "authors": "Kiran Sharma, Balagopal Gopalakrishnan, Anindya S. Chakrabarti and\n  Anirban Chakraborti",
        "title": "Co-movements in financial fluctuations are anchored to economic\n  fundamentals: A mesoscopic mapping",
        "comments": "30 pages, 6 figures, including supplementary material",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate the existence of an empirical linkage between the nominal\nfinancial networks and the underlying economic fundamentals across countries.\nWe construct the nominal return correlation networks from daily data to\nencapsulate sector-level dynamics and figure the relative importance of the\nsectors in the nominal network through a measure of centrality and clustering\nalgorithms. The eigenvector centrality robustly identifies the backbone of the\nminimum spanning tree defined on the return networks as well as the primary\ncluster in the multidimensional scaling map. We show that the sectors that are\nrelatively large in size, defined with the metrics market capitalization,\nrevenue and number of employees, constitute the core of the return networks,\nwhereas the periphery is mostly populated by relatively smaller sectors.\nTherefore, sector-level nominal return dynamics is anchored to the real size\neffect, which ultimately shapes the optimal portfolios for risk management. Our\nresults are reasonably robust across 27 countries of varying degrees of\nprosperity and across periods of market turbulence (2008-09) as well as\nrelative calmness (2015-16).\n"
    },
    {
        "paper_id": 1612.06133,
        "authors": "Lijun Bo and Agostino Capponi",
        "title": "Optimal Investment under Information Driven Contagious Distress",
        "comments": "38 pages, 12 figures, SIAM Journal on Control and Optimization,\n  forthcoming, 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a dynamic optimization framework to analyze optimal portfolio\nallocations within an information driven contagious distress model. The\ninvestor allocates his wealth across several stocks whose growth rates and\ndistress intensities are driven by a hidden Markov chain, and also influenced\nby the distress state of the economy. We show that the optimal investment\nstrategies depend on the gradient of value functions, recursively linked to\neach other via the distress states. We establish uniform bounds for the\nsolutions to a sequence of approximation problems, show their convergence to\nthe unique Sobolev solution of the recursive system of Hamilton-Jacobi-Bellman\npartial differential equations (HJB PDEs), and prove a verification theorem. We\nprovide a numerical study to illustrate the sensitivity of the strategies to\ncontagious distress, stock volatilities and risk aversion.\n"
    },
    {
        "paper_id": 1612.06186,
        "authors": "Vahid Moosavi, Giulio Isacchini",
        "title": "A Markovian Model of the Evolving World Input-Output Network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0186746",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The initial theoretical connections between Leontief input-output models and\nMarkov chains were established back in 1950s. However, considering the wide\nvariety of mathematical properties of Markov chains, there has not been a full\ninvestigation of evolving world economic networks with Markov chain formalism.\nUsing the recently available world input-output database, we modeled the\nevolution of the world economic network from 1995 to 2011 through analysis of a\nseries of finite Markov chains. We assessed different aspects of this evolving\nsystem via different properties of the Markov chains such as mixing time,\nKemeny constant, steady state probabilities and perturbation analysis of the\ntransition matrices. First, we showed how the time series of mixing times and\nKemeny constants could be used as an aggregate index of globalization. Next, we\nfocused on the steady state probabilities as a measure of structural power of\nthe economies that are comparable to GDP shares of economies as the traditional\nindex of economies. Further, we introduced two measures of systemic risk,\ncalled systemic influence and systemic fragility, where the former is the ratio\nof number of influenced nodes to the total number of nodes, caused by a shock\nin the activity of a node and the latter is based on the number of times a\nspecific economic node is affected by a shock in the activity of any of the\nother nodes. Finally, focusing on Kemeny constant as a global indicator of\nmonetary flow across the network, we showed that there is a paradoxical effect\nof a change in activity levels of economic nodes on the overall flow of the\nnetwork. While the economic slowdown of the majority of nodes with high\nstructural power results to a slower average monetary flow over the network,\nthere are some nodes, where their slowdowns improve the overall quality of the\nnetwork in terms of connectivity and the average monetary flow.\n"
    },
    {
        "paper_id": 1612.062,
        "authors": "Jamal Bouoiyour (CATT), Refk Selmi (CATT)",
        "title": "The Price of Political Uncertainty: Evidence from the 2016 U.S.\n  Presidential Election and the U.S. Stock Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is bountiful evidence that political uncertainty stemming from\npresidential elections or doubt about the direction of future policy make\nfinancial markets significantly volatile, especially in proximity to close\nelections or elections that may prompt radical policy changes. Although several\nstudies have examined the association between presidential elections and stock\nreturns, very little attention has been given to the impacts of elections and\nelection induced uncertainty on stock markets. This paper explores, at sectoral\nlevel, the uncertain information hypothesis (UIH) as a means of explaining the\nreaction of markets to the arrival of unanticipated information. This\nhypothesis postulates that political uncertainty is greater prior to the\nelections (relative to pre-election period) but is resolved once the outcome of\nthe elections is determined (relative to post-election period). To this end, we\nadopt an event-study methodology that examines abnormal return behavior around\nthe election date. We show that collapsing stock returns around the election\nresult is reversed by positive abnormal return on the next day, except some\ncases where we note negative responses following the vote count. Although\nTrump's win plunges US into uncertain future, positive reactions of abnormal\nreturn are found. Therefore, our results do not support the UIH hypothesis.\nBesides, the effect of political uncertainty is sector-specific. While some\nsectors emerged winners (healthcare, oil and gas, real estate, defense,\nfinancials and consumer goods and services), others took the opposite route\n(technology and utilities). The winning industries are generally those that\nwill benefit from the new administration's focus on rebuilding infrastructure,\nrenegotiating trade agreements, reforming tax policy and labour laws,\nincreasing defense funding, easing restrictions on energy production, and\nrolling back Obamacare.\n"
    },
    {
        "paper_id": 1612.06244,
        "authors": "Jan Hendrik Witte",
        "title": "The Blockchain: A Gentle Four Page Introduction",
        "comments": "4 Pages, 0 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain is a distributed database that keeps a chronologically-growing\nlist (chain) of records (blocks) secure from tampering and revision. While\ncomputerisation has changed the nature of a ledger from clay tables in the old\ndays to digital records in modern days, blockchain technology is the first true\ninnovation in record keeping that could potentially revolutionise the basic\nprinciples of information keeping. In this note, we provide a brief\nself-contained introduction to how the blockchain works.\n"
    },
    {
        "paper_id": 1612.06291,
        "authors": "Tanya Ara\\'ujo and Rui Faustino",
        "title": "The Topology of Inter-industry Relations from the Portuguese National\n  Accounts",
        "comments": "21 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.03.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In last years, the Portuguese economy has gone through a severe adjustment\nprocess, affecting almost all industrial sectors, the building blocks of\neconomic structures. Research on economic structural changes has made use of\ninput/output tables to define networks of industrial relations. Here, these\nnetworks are induced from output tables of the Portuguese national accounting\nsystem, being each inter-industry relation defined by the output made by any\ntwo industries for the products that they both produce. The topological\nanalysis of these networks allows to uncover a particular structure that comes\nout during the Portuguese adjustment program. The evolution of the industrial\nnetworks shows an important structural change in 2011-2014, confirming the\nusefulness of inducting similarity networks from output tables and the\nconsequent promising power of the graph formulation for the analysis of\ninter-industry relations.\n"
    },
    {
        "paper_id": 1612.06441,
        "authors": "Duccio Piovani, Vassilis Zachariadis and Michael Batty",
        "title": "Quantifying Retail Agglomeration using Diverse Spatial Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Newly available data on the spatial distribution of retail activities in\ncities makes it possible to build models formalized at the level of the single\nretailer. Current models tackle consumer location choices at an aggregate level\nand the opportunity new data offers for modeling at the retail unit level lacks\na theoretical framework. The model we present here helps to address these\nissues. It is a particular case of the Cross-Nested Logit model, based on\nrandom utility theory built with the idea of quantifying the role of floor\nspace and agglomeration in retail location choice. We test this model on the\ncity of London: the results are consistent with a super linear scaling of a\nretailer's attractiveness with its floor space, and with an agglomeration\neffect approximated as the total retail floorspace within a $325m$ radius from\neach shop.\n"
    },
    {
        "paper_id": 1612.06451,
        "authors": "Chiara Perillo (1), Angelos Antonopoulos (2) and Christos Verikoukis\n  (2) ((1) University of Zurich, Department of Banking and Finance, Zurich,\n  Switzerland, (2) Telecommunications Technological Centre of Catalonia (CTTC),\n  Castelldefels, Barcelona, Spain)",
        "title": "Panel dataset description for econometric analysis of the ISP-OTT\n  relationship in the years 2008-2013",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The latest technological advancements in the telecommunications domain (e.g.,\nwidespread adoption of mobile devices, introduction of 5G wireless\ncommunications, etc.) have brought new stakeholders into the spotlight. More\nspecifically, Over-the-Top (OTT) providers have recently appeared, offering\ntheir services over the existing deployed telecommunication networks. The entry\nof the new players has changed the dynamics in the domain, as it creates\nconflicting situations with the Internet Service Providers (ISPs), who\ntraditionally dominate the area, motivating the necessity for novel analytical\nstudies for this relationship. However, despite the importance of accessing\nreal observational data, there is no database with the aggregate information\nthat can serve as a solid base for this research. To that end, this document\nprovides a detailed summary report for financial and statistic data for the\nperiod 2008-2013 that can be exploited for realistic econometric models that\nwill provide useful insights on this topic. The document summarizes data from\nvarious sources with regard to the ISP revenues and Capital Expenditures\n(CAPEX), the OTT revenues, the Internet penetration and the Gross Domestic\nProduct (GDP), taking into account three big OTT providers (i.e., Facebook,\nSkype, WhatsApp) and ten major ISPs that operate in seven different countries.\n"
    },
    {
        "paper_id": 1612.06616,
        "authors": "Thorsten Schmidt",
        "title": "Shot-Noise Processes in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Shot-Noise processes constitute a useful tool in various areas, in particular\nin finance. They allow to model abrupt changes in a more flexible way than\nprocesses with jumps and hence are an ideal tool for modelling stock prices,\ncredit portfolio risk, systemic risk, or electricity markets. Here we consider\na general formulation of shot-noise processes, in particular time-inhomogeneous\nshot-noise processes. This flexible class allows to obtain the Fourier\ntransforms in explicit form and is highly tractable. We prove that Markovianity\nis equivalent to exponential decay of the noise function. Moreover, we study\nthe relation to semimartingales and equivalent measure changes which are\nessential for the financial application. In particular we derive a drift\ncondition which guarantees absence of arbitrage. Examples include the minimal\nmartingale measure and the Esscher measure.\n"
    },
    {
        "paper_id": 1612.06654,
        "authors": "Julia Eisenberg, Paul Kr\\\"uhner",
        "title": "The Impact of Negative Interest Rates on Optimal Capital Injections",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, we investigate the optimal capital injection behaviour\nof an insurance company if the interest rate is allowed to become negative. The\nsurplus process of the considered insurance entity is assumed to follow a\nBrownian motion with drift. The changes in the interest rate are described via\na Markov-switching process. It turns out that in times with a positive rate, it\nis optimal to inject capital only if the company becomes insolvent. However, if\nthe rate is negative it might be optimal to hold a strictly positive reserve.\nWe establish an algorithm for finding the value function and the optimal\nstrategy, which is proved to be of barrier type. Using the iteration argument,\nwe show that the value function solves the Hamilton--Jacobi--Bellman equation,\ncorresponding to the problem.\n"
    },
    {
        "paper_id": 1612.06665,
        "authors": "Foad Shokrollahi",
        "title": "Subdiffusive fractional Brownian motion regime for pricing currency\n  options under transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new framework for pricing the European currency option is developed in the\ncase where the spot exchange rate fellows a time-changed fractional Brownian\nmotion. An analytic formula for pricing European foreign currency option is\nproposed by a mean self-financing delta-hedging argument in a discrete time\nsetting. The minimal price of a currency option under transaction costs is\nobtained as time-step $\\Delta\nt=\\left(\\frac{t^{\\beta-1}}{\\Gamma(\\beta)}\\right)^{-1}\\left(\\frac{2}{\\pi}\\right)^{\\frac{1}{2H}}\\left(\\frac{\\alpha}{\\sigma}\\right)^{\\frac{1}{H}}$\n, which can be used as the actual price of an option. In addition, we also show\nthat time-step and long-range dependence have a significant impact on option\npricing.\n"
    },
    {
        "paper_id": 1612.06855,
        "authors": "Bruce Knuteson",
        "title": "Information, Impact, Ignorance, Illegality, Investing, and Inequality",
        "comments": "2 pages; v2: comment added in the TeX source file with the SHA\n  checksum of a longer version of this article sent to the Securities and\n  Exchange Commission on September 10, 2015; v3: added the checksum of an\n  article detailing the impact arb described in Section II",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We note a simple mechanism that may at least partially resolve several\noutstanding economic puzzles, including why the cyclically adjusted price to\nearnings ratio of the S&P 500 index has been oddly high for the past two\ndecades, why gains to capital have outpaced gains to wages, and the persistence\nof the equity premium.\n"
    },
    {
        "paper_id": 1612.07016,
        "authors": "Svetlozar T. Rachev, Stefan Mittnik, Frank J. Fabozzi",
        "title": "Pricing Derivatives in Hermite Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce Hermite fractional financial markets, where market uncertainties\nare described by multidimensional Hermite motions. Hermite markets include as\nparticular cases financial markets driven by multivariate fractional Brownian\nmotion and multivariate Rosenblatt motion. Conditions for no-arbitrage and\nmarket completeness for Hermite markets are derived. Perpetual derivatives,\nbonds forwards, and futures are priced. The corresponding partial and\npartial-differential equations are derived.\n"
    },
    {
        "paper_id": 1612.07067,
        "authors": "Imre Kondor, G\\'abor Papp, Fabio Caccioli",
        "title": "Analytic solution to variance optimization with no short-selling",
        "comments": "27 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa9684",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large portfolio of independent returns is optimized under the variance risk\nmeasure with a ban on short positions. The no-short selling constraint acts as\nan asymmetric $\\ell_1$ regularizer, setting some of the portfolio weights to\nzero and keeping the out of sample estimator for the variance bounded, avoiding\nthe divergence present in the non-regularized case. However, the\nsusceptibility, i.e. the sensitivity of the optimal portfolio weights to\nchanges in the returns, diverges at a critical value $r=2$. This means that a\nban on short positions does not prevent the phase transition in the\noptimization problem, it merely shifts the critical point from its\nnon-regularized value of $r=1$ to $2$. At $r=2$ the out of sample estimator for\nthe portfolio variance stays finite and the estimated in-sample variance\nvanishes. We have performed numerical simulations to support the analytic\nresults and found perfect agreement for $N/T<2$. Numerical experiments on\nfinite size samples of symmetrically distributed returns show that above this\ncritical point the probability of finding solutions with zero in-sample\nvariance increases rapidly with increasing $N$, becoming one in the large $N$\nlimit. However, these are not legitimate solutions of the optimization problem,\nas they are infinitely sensitive to any change in the input parameters, in\nparticular they will wildly fluctuate from sample to sample. We also calculate\nthe distribution of the optimal weights over the random samples and show that\nthe regularizer preferentially removes the assets with large variances, in\naccord with one's natural expectation.\n"
    },
    {
        "paper_id": 1612.07132,
        "authors": "Claudia Kl\\\"uppelberg, Miriam Isabel Seifert",
        "title": "Conditional loss probabilities for systems of economic agents sharing\n  light-tailed claims with analysis of portfolio diversification benefits",
        "comments": "29 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze systems of agents sharing light-tailed risky claims issued by\ndifferent financial objects. Assuming exponentially distributed claims, we\nobtain that both agents' and system's losses follow generalized exponential\nmixture distributions. We show that this leads to qualitatively different\nresults on individual and system risks compared to heavy-tailed claims\npreviously studied in the literature. By deducing conditional loss\ndistributions we investigate the impact of stress situations on agents' and\nsystem's losses. Moreover, we present a criterion for agents to decide whether\nholding few objects or portfolio diversification minimizes their risks in\nsystem crisis situations.\n"
    },
    {
        "paper_id": 1612.07194,
        "authors": "Mihail Turlakov",
        "title": "Leverage and Uncertainty",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk and uncertainty will always be a matter of experience, luck, skills, and\nmodelling. Leverage is another concept, which is critical for the investor\ndecisions and results. Adaptive skills and quantitative probabilistic methods\nneed to be used in successful management of risk, uncertainty and leverage. The\nauthor explores how uncertainty beyond risk determines consistent leverage in a\nsimple model of the world with fat tails due to significant, not fully\nquantifiable and not too rare events. Among particular technical results, for\nthe single asset fractional Kelly criterion is derived in the presence of the\nfat tails associated with subjective uncertainty. For the multi-asset\nportfolio, Kelly criterion provides an insightful perspective on Risk Parity\nstrategies, which can be extended for the assets with fat tails.\n"
    },
    {
        "paper_id": 1612.07543,
        "authors": "Ilya Solntsev, Anatoly Vorobyev, Elnura Irmatova, Nikita Osokin",
        "title": "Rating evaluation of sports development efficiency using statistical\n  analysis: evidence from Russian football",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Increasing investments into various dimensions of sports draw a significant\namount of attention to the way these resources are being managed and which\norganizations achieve development goals with higher efficiency. This paper\nreviews the methodology of designing an efficiency rating model for assessing\nsports entities, focusing on the experience of Russian football. The Russian\nRegional Efficiency of Football Development model aims to evaluate the regional\nfederations of the Football Union of Russian via 5 dimensions. The scoring\nmethod of the model is based on the three-sigma rule of distribution. Support\nfactors in the form of population density and climate were also included, since\nRussian regions significantly differentiate in these aspects. The findings of\nthis paper showcased that not a single region was able to achieve a maximum 5-\nstar rating, while regions set to host the 2018 FIFA World Cup did not score\nbetter compared to others. In conclusion the authors provide various\nsuggestions on further developing and implementing rating models within global\nsports organizations.\n"
    },
    {
        "paper_id": 1612.07618,
        "authors": "Matteo Burzoni, Marco Frittelli, Zhaoxu Hou, Marco Maggis and Jan\n  Ob{\\l}\\'oj",
        "title": "Pointwise Arbitrage Pricing Theory in Discrete Time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a robust framework for pricing and hedging of derivative\nsecurities in discrete-time financial markets. We consider markets with both\ndynamically and statically traded assets and make minimal measurability\nassumptions. We obtain an abstract (pointwise) Fundamental Theorem of Asset\nPricing and Pricing--Hedging Duality. Our results are general and in particular\ninclude so-called model independent results of Acciao et al. (2016), Burzoni et\nal. (2016) as well as seminal results of Dalang et al. (1990) in a classical\nprobabilistic approach. Our analysis is scenario--based: a model specification\nis equivalent to a choice of scenarios to be considered. The choice can vary\nbetween all scenarios and the set of scenarios charged by a given probability\nmeasure. In this way, our framework interpolates between a model with\nuniversally acceptable broad assumptions and a model based on a specific\nprobabilistic view of future asset dynamics.\n"
    },
    {
        "paper_id": 1612.07742,
        "authors": "Michael Schneider and Fabrizio Lillo",
        "title": "Cross-impact and no-dynamic-arbitrage",
        "comments": "33 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the \"No-dynamic-arbitrage and market impact\"-framework of Jim\nGatheral [Quantitative Finance, 10(7): 749-759 (2010)] to the multi-dimensional\ncase where trading in one asset has a cross-impact on the price of other\nassets. From the condition of absence of dynamical arbitrage we derive\ntheoretical limits for the size and form of cross-impact that can be directly\nverified on data. For bounded decay kernels we find that cross-impact must be\nan odd and linear function of trading intensity and cross-impact from asset $i$\nto asset $j$ must be equal to the one from $j$ to $i$. To test these\nconstraints we estimate cross-impact among sovereign bonds traded on the\nelectronic platform MOT. While we find significant violations of the above\nsymmetry condition of cross-impact, we show that these are not arbitrageable\nwith simple strategies because of the presence of the bid-ask spread.\n"
    },
    {
        "paper_id": 1612.07802,
        "authors": "Michele Caraglio, Fulvio Baldovin and Attilio L. Stella",
        "title": "How fast does the clock of Finance run? - A time-definition enforcing\n  scale invariance and quantifying overnights",
        "comments": "9 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A symmetry-guided definition of time may enhance and simplify the analysis of\nhistorical series with recurrent patterns and seasonalities. By enforcing\nsimple-scaling and stationarity of the distributions of returns, we identify a\nsuccessful protocol of time definition in Finance. The essential structure of\nthe stochastic process underlying the series can thus be analyzed within a most\nparsimonious symmetry scheme in which multiscaling is reduced in the quest of a\ntime scale additive and independent of moment-order in the distribution of\nreturns. At the same time, duration of periods in which markets remain inactive\nare properly quantified by the novel clock, and the corresponding (e.g.,\novernight) returns are consistently taken into account for financial\napplications.\n"
    },
    {
        "paper_id": 1612.07903,
        "authors": "Vasily E. Tarasov, Valentina V. Tarasova",
        "title": "Long and Short Memory in Economics: Fractional-Order Difference and\n  Differentiation",
        "comments": "6 pages, PDF",
        "journal-ref": "IRA-International Journal of Management and Social Sciences. 2016.\n  Vol.5. No.2. P.327-334",
        "doi": "10.21013/jmss.v5.n2.p10",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long and short memory in economic processes is usually described by the\nso-called discrete fractional differencing and fractional integration. We prove\nthat the discrete fractional differencing and integration are the\nGrunwald-Letnikov fractional differences of non-integer order d. Equations of\nARIMA(p,d,q) and ARFIMA(p,d,q) models are the fractional-order difference\nequations with the Grunwald-Letnikov differences of order d. We prove that the\nlong and short memory with power law should be described by the exact\nfractional-order differences, for which the Fourier transform demonstrates the\npower law exactly. The fractional differencing and the Grunwald-Letnikov\nfractional differences cannot give exact results for the long and short memory\nwith power law, since the Fourier transform of these discrete operators satisfy\nthe power law in the neighborhood of zero only. We prove that the economic\nprocesses with the continuous time long and short memory, which is\ncharacterized by the power law, should be described by the fractional\ndifferential equations.\n"
    },
    {
        "paper_id": 1612.07913,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Economic Accelerator with Memory: Discrete Time Approach",
        "comments": "8 pages, PDF",
        "journal-ref": "Problems of Modern Science and Education. 2016. No.36(78). P.37-42",
        "doi": "10.20861/2304-2338-2016-78-002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accelerators with power-law memory are proposed in the framework of the\ndiscrete time approach. To describe discrete accelerators we use the capital\nstock adjustment principle, which has been suggested by Matthews.The suggested\ndiscrete accelerators with memory describe the economic processes with the\npower-law memory and the periodic sharp splashes (kicks). In continuous time\napproach the memory is described by fractional-order differential equations. In\ndiscrete time approach the accelerators with memory are described by discrete\nmaps with memory, which are derived from the fractional-order differential\nequation without approximations. In order to derive these maps we use the\nequivalence of fractional-order differential equations and the Volterra\nintegral equations.\n"
    },
    {
        "paper_id": 1612.08111,
        "authors": "James B. T. Sanders, J. Doyne Farmer, Tobias Galla",
        "title": "The prevalence of chaotic dynamics in games with many players",
        "comments": "21 pages, 11 figures",
        "journal-ref": "Scientific Reports, volume 8, Article number: 4902 (2018)",
        "doi": "10.1038/s41598-018-22013-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study adaptive learning in a typical p-player game. The payoffs of the\ngames are randomly generated and then held fixed. The strategies of the players\nevolve through time as the players learn. The trajectories in the strategy\nspace display a range of qualitatively different behaviors, with attractors\nthat include unique fixed points, multiple fixed points, limit cycles and\nchaos. In the limit where the game is complicated, in the sense that the\nplayers can take many possible actions, we use a generating-functional approach\nto establish the parameter range in which learning dynamics converge to a\nstable fixed point. The size of this region goes to zero as the number of\nplayers goes to infinity, suggesting that complex non-equilibrium behavior,\nexemplified by chaos, may be the norm for complicated games with many players.\n"
    },
    {
        "paper_id": 1612.08338,
        "authors": "James PL Tan",
        "title": "A Generalized Population Dynamics Model of a City and an Algorithm for\n  Engineering Regime Shifts",
        "comments": "30 pages, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Measures of wealth and production have been found to scale superlinearly with\nthe population of a city. Therefore, it makes economic sense for humans to\ncongregate together in dense settlements. A recent model of population dynamics\nshowed that population growth can become superexponential due to the\nsuperlinear scaling of production with population in a city. Here, we\ngeneralize this population dynamics model and demonstrate the existence of\nmultiple stable equilibrium points, showing how population growth can be\nstymied by a poor economic environment. This occurs when the goods and services\nproduced by the city become less profitable due to a lack of diversification in\nthe city's economy. Then, relying on critical slowing down signals related to\nthe stability of an equilibrium point, we present an algorithm for engineering\nregime shifts such that a city at a stable equilibrium point may continue to\ngrow again. The generality of the model and the algorithm used here implies\nthat the model and algorithm need not be restricted to urban systems; they are\neasily applicable to other types of systems where the assumptions used are\nvalid.\n"
    },
    {
        "paper_id": 1612.08486,
        "authors": "Linlin Ye",
        "title": "Understanding the Impacts of Dark Pools on Price Discovery",
        "comments": "Dark Pool, Market Fragmentation, Price Discovery",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the impact of dark pools on price discovery (the\nefficiency of prices on stock exchanges to aggregate information). Assets are\ntraded in either an exchange or a dark pool, with the dark pool offering better\nprices but lower execution rates. Informed traders receive noisy and\nheterogeneous signals about an asset's fundamental. We find that informed\ntraders use dark pools to mitigate their information risk and there is a\nsorting effect: in equilibrium, traders with strong signals trade in exchanges,\ntraders with moderate signals trade in dark pools, and traders with weak\nsignals do not trade. As a result, dark pools have an amplification effect on\nprice discovery. That is, when information precision is high (information risk\nis low), the majority of informed traders trade in the exchange hence adding a\ndark pool enhances price discovery, whereas when information precision is low\n(information risk is high), the majority of the informed traders trade in the\ndark pool hence adding a dark pool impairs price discovery. The paper\nreconciles the conflicting empirical evidence and produces novel empirical\npredictions. The paper also provides regulatory suggestions with dark pools on\ncurrent equity markets and in emerging markets.\n"
    },
    {
        "paper_id": 1612.08488,
        "authors": "Richard Gerlach, Chao Wang",
        "title": "Bayesian Semi-parametric Realized-CARE Models for Tail Risk Forecasting\n  Incorporating Realized Measures",
        "comments": "40 pages, 6 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new model framework called Realized Conditional Autoregressive Expectile\n(Realized-CARE) is proposed, through incorporating a measurement equation into\nthe conventional CARE model, in a manner analogous to the Realized-GARCH model.\nCompeting realized measures (e.g. Realized Variance and Realized Range) are\nemployed as the dependent variable in the measurement equation and to drive\nexpectile dynamics. The measurement equation here models the contemporaneous\ndependence between the realized measure and the latent conditional expectile.\nWe also propose employing the quantile loss function as the target criterion,\ninstead of the conventional violation rate, during the expectile level grid\nsearch. For the proposed model, the usual search procedure and asymmetric least\nsquares (ALS) optimization to estimate the expectile level and CARE parameters\nproves challenging and often fails to convergence. We incorporate a fast random\nwalk Metropolis stochastic search method, combined with a more targeted grid\nsearch procedure, to allow reasonably fast and improved accuracy in estimation\nof this level and the associated model parameters. Given the convergence issue,\nBayesian adaptive Markov Chain Monte Carlo methods are proposed for estimation,\nwhilst their properties are assessed and compared with ALS via a simulation\nstudy. In a real forecasting study applied to 7 market indices and 2 individual\nasset returns, compared to the original CARE, the parametric GARCH and\nRealized-GARCH models, one-day-ahead Value-at-Risk and Expected Shortfall\nforecasting results favor the proposed Realized-CARE model, especially when\nincorporating the Realized Range and the sub-sampled Realized Range as the\nrealized measure in the model.\n"
    },
    {
        "paper_id": 1612.08583,
        "authors": "Diederik Aerts, Emmanuel Haven and Sandro Sozzo",
        "title": "A Proposal to Extend Expected Utility in a Quantum Probabilistic\n  Framework",
        "comments": "25 pages, standard latex. arXiv admin note: text overlap with\n  arXiv:1510.09058",
        "journal-ref": "Economic Theory, 65, pp. 1079-1109 (2018)",
        "doi": "10.1007/s00199-017-1051-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expected utility theory (EUT) is widely used in economic theory. However, its\nsubjective probability formulation, first elaborated by Savage, is linked to\nEllsberg-like paradoxes and ambiguity aversion. This has led various scholars\nto work out non-Bayesian extensions of EUT which cope with its paradoxes and\nincorporate attitudes toward ambiguity. A variant of the Ellsberg paradox,\nrecently proposed by Mark Machina and confirmed experimentally, challenges\nexisting non-Bayesian models of decision-making under uncertainty. Relying on a\ndecade of research which has successfully applied the formalism of quantum\ntheory to model cognitive entities and fallacies of human reasoning, we put\nforward a non-Bayesian extension of EUT in which subjective probabilities are\nrepresented by quantum probabilities, while the preference relation between\nacts depends on the state of the situation that is the object of the decision.\nWe show that the benefits of using the quantum theoretical framework enables\nthe modeling of the Ellsberg and Machina paradoxes, as the representation of\nambiguity and behavioral attitudes toward it. The theoretical framework\npresented here is a first step toward the development of a `state-dependent\nnon-Bayesian extension of EUT' and it has potential applications in economic\nmodeling.\n"
    },
    {
        "paper_id": 1612.08689,
        "authors": "Hristian Daskalov",
        "title": "Crisis' Heritage Management - New Business Opportunities Out of the\n  Financial Collapse",
        "comments": "Presented at the 2013 Sofia Business School Master Classes in Global\n  Risks Management",
        "journal-ref": null,
        "doi": "10.6084/m9.figshare.4497317.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper intends to present the opportunities emerging for the national\neconomy, out of the financial crisis. In particular the management of those,\nwhich arise from the commercial real estate owned property sector, defined by\nthe author as crisis heritage management. On one hand, as real estate property\nprices are subject of wide fluctuations, the longer possession of such assets\ncan seriously impact the financial condition of the already shattered financial\ninstitutions, but on the on other - with the help of professional and proactive\nmanagement, and the right kind of attitude by all the stakeholders, the\nheritage left out of the financial collapse, can not only help stabilize the\nsystem - bringing liquidity into it, but can also support its healthy corporate\ngovernance in the long-term. The properties themselves (business buildings,\nwarehouses, retail-and-office spaces), being an object of optimization of\nmaintenance costs, re-engineering, intensive marketing, as a result of the\ncrisis, can serve as a solid base for number of new and profitable business and\ninvestment opportunities, described in the article, as a proof of the healing\neffect of the financial crisis and the second chance it gives.\n"
    },
    {
        "paper_id": 1612.08705,
        "authors": "Sabiou Inoua",
        "title": "Speculation and Power Law",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is now well established empirically that financial price changes are\ndistributed according to a power law, with cubic exponent. This is a\nfascinating regularity, as it holds for various classes of securities, on\nvarious markets, and on various time scales. The universality of this law\nsuggests that there must be some basic, general and stable mechanism behind it.\nThe standard (neoclassical) paradigm implies no such mechanism. Agent-based\nmodels of financial markets, on the other hand, exhibit realistic price\nchanges, but they involve relatively complicated, and often mathematically\nintractable, mechanisms. This paper identifies a simple principle behind the\npower law: the feedback intrinsic to the very idea of speculation, namely\nbuying when one expects a price rise (and selling when one expects a price\nfall). By this feedback, price changes follow a random coefficient\nautoregressive process, and therefore they have a power law by Kesten theorem.\n"
    },
    {
        "paper_id": 1612.08767,
        "authors": "Alexander Novikov, Scott Alexander, Nino Kordzakhia and Timothy Ling",
        "title": "Pricing of Asian-type and Basket Options via Upper and Lower Bounds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper sets out to provide a general framework for the pricing of\naverage-type options via lower and upper bounds. This class of options includes\nAsian, basket and options on the volume-weighted average price. We demonstrate\nthat in cases under discussion lower bounds allow for the dimensionality of the\nproblem to be reduced and that these methods provide reasonable approximations\nto the price of the option.\n  Keywords: Asian options, Basket options, Lower and Upper bounds,\nVolume-weighted average prices (VWAP), Levy processes.\n"
    },
    {
        "paper_id": 1612.0906,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Fractional Dynamics of Natural Growth and Memory Effect in Economics",
        "comments": "11 pages, PDF",
        "journal-ref": "European Research. 2016. No. 12 (23). P. 30-37",
        "doi": "10.20861/2410-2873-2016-23-004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A generalization of the economic model of natural growth, which takes into\naccount the power-law memory effect, is suggested. The memory effect means the\ndependence of the process not only on the current state of the process, but\nalso on the history of changes of this process in the past. For the\nmathematical description of the economic process with power-law memory we used\nthe theory of derivatives of non-integer order and fractional-order\ndifferential equation. We propose equations take into account the effects of\nmemory with one-parameter power-law damping. Solutions of these fractional\ndifferential equations are suggested. We proved that the growth and downturn of\noutput depend on the memory effects. We demonstrate that the memory effect can\nlead to decrease of output instead of its growth, which is described by model\nwithout memory effect. Memory effect can lead to increase of output, rather\nthan decrease, which is described by model without memory effect.\n"
    },
    {
        "paper_id": 1612.09103,
        "authors": "Daniel Bartl",
        "title": "Conditional nonlinear expectations",
        "comments": "previous title: \"Pointwise dual representation of dynamic convex\n  expectations\"",
        "journal-ref": "Stochastic Processes and their Applications, 130(2), 785-805, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\Omega$ be a Polish space with Borel $\\sigma$-field $\\mathcal{F}$ and\ncountably generated sub $\\sigma$-field $\\mathcal{G}\\subset\\mathcal{F}$. Denote\nby $\\mathcal{L}(\\mathcal{F})$ the set of all bounded $\\mathcal{F}$-upper\nsemianalytic functions from $\\Omega$ to the reals and by\n$\\mathcal{L}(\\mathcal{G})$ the subset of $\\mathcal{G}$-upper semianalytic\nfunctions. Let\n$\\mathcal{E}(\\cdot|\\mathcal{G})\\colon\\mathcal{L}(\\mathcal{F})\\to\\mathcal{L}(\\mathcal{G})$\nbe a sublinear increasing functional which leaves $\\mathcal{L}(\\mathcal{G})$\ninvariant. It is shown that there exists a $\\mathcal{G}$-analytic set-valued\nmapping $\\mathcal{P}_{\\mathcal{G}}$ from $\\Omega$ to the set of probabilities\nwhich are concentrated on atoms of $\\mathcal{G}$ with compact convex values\nsuch that $\\mathcal{E}(X|\\mathcal{G})(\\omega)=$\n$\\sup_{P\\in\\mathcal{P}_{\\mathcal{G}}(\\omega)} E_P[X]$ if and only if\n$\\mathcal{E}(\\cdot |\\mathcal{G})$ is pointwise continuous from below and\ncontinuous from above on the continuous functions. Further, given another\nsublinear increasing functional\n$\\mathcal{E}(\\cdot)\\colon\\mathcal{L}(\\mathcal{F})\\to\\mathbb{R}$ which leaves\nthe constants invariant, the tower property\n$\\mathcal{E}(\\cdot)=\\mathcal{E}(\\mathcal{E}(\\cdot|\\mathcal{G}))$ is\ncharacterized via a pasting property of the representing sets of probabilities,\nand the importance of analytic functions is explained. Finally, it is\ncharacterized when a nonlinear version of Fubini's theorem holds true and when\nthe product of a set of probabilities and a set of kernels is compact.\n"
    },
    {
        "paper_id": 1612.09123,
        "authors": "Richard S.J. Tol",
        "title": "Population and trends in the global mean temperature",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend.\n"
    },
    {
        "paper_id": 1612.09152,
        "authors": "Johannes Muhle-Karbe, Marcel Nutz",
        "title": "A Risk-Neutral Equilibrium Leading to Uncertain Volatility Pricing",
        "comments": "Forthcoming in 'Finance&Stochastics'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the formation of derivative prices in equilibrium between\nrisk-neutral agents with heterogeneous beliefs about the dynamics of the\nunderlying. Under the condition that the derivative cannot be shorted, we prove\nthe existence of a unique equilibrium price and show that it incorporates the\nspeculative value of possibly reselling the derivative. This value typically\nleads to a bubble; that is, the price exceeds the autonomous valuation of any\ngiven agent. Mathematically, the equilibrium price operator is of the same\nnonlinear form that is obtained in single-agent settings with strong aversion\nagainst model uncertainty. Thus, our equilibrium leads to a novel\ninterpretation of this price.\n"
    },
    {
        "paper_id": 1612.09189,
        "authors": "Askar Akaev and Andrey Korotayev",
        "title": "Global economic dynamics of the forthcoming years. A forecast",
        "comments": "34 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper analyzes the current state of the world economy and offers a\nshort-term forecast of its development. Our analysis of log-periodic\noscillations in the DJIA dynamics suggests that in the second half of 2017 the\nUnited States and other more developed countries could experience a new\nrecession, due to the third phase of the global financial crisis. The economies\nof developing countries will continue their slowdown due to lower prices of raw\ncommodities and the increased pressure of dollar debt load. The bottom of the\nslowdown in global economic growth is likely to be achieved in 2017-2018. Then\nwe expect the start of a new acceleration of global economic growth at the\nupswing phase of the 6th Kondratieff cycle (2018-2050). A speedy and steady\nwithdrawal from the third phase of the global financial crisis requires\ncooperative action between developed and developing countries within G20 to\nstimulate global demand, world trade and a fair solution of the debt problem of\ndeveloping countries.\n"
    },
    {
        "paper_id": 1612.09244,
        "authors": "Michael J Bommarito II, Daniel Martin Katz",
        "title": "Measuring the temperature and diversity of the U.S. regulatory ecosystem",
        "comments": "Preprint, first of three papers released in paper series; updated on\n  2017-01-10 with additional text and figures in response to feedback",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the last 23 years, the U.S. Securities and Exchange Commission has\nrequired over 34,000 companies to file over 165,000 annual reports. These\nreports, the so-called \"Form 10-Ks,\" contain a characterization of a company's\nfinancial performance and its risks, including the regulatory environment in\nwhich a company operates. In this paper, we analyze over 4.5 million references\nto U.S. Federal Acts and Agencies contained within these reports to build a\nmean-field measurement of temperature and diversity in this regulatory\necosystem, where companies are organisms inhabiting the regulatory environment.\nWhile individuals across the political, economic, and academic world frequently\nrefer to trends in this regulatory ecosystem, far less attention has been paid\nto supporting such claims with large-scale, longitudinal data. In this paper,\nwe document an increase in the regulatory energy per filing, i.e., a warming\n\"temperature.\" We also find that the diversity of the regulatory ecosystem has\nbeen increasing over the past two decades, as measured by the dimensionality of\nthe regulatory space and distance between the \"regulatory bitstrings\" of\ncompanies. These findings support the claim that regulatory activity and\ncomplexity are increasing, and this measurement framework contributes an\nimportant step towards improving academic and policy discussions around legal\ncomplexity and regulation.\n"
    },
    {
        "paper_id": 1612.09344,
        "authors": "Sabiou Inoua",
        "title": "The Random Walk behind Volatility Clustering",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial price changes obey two universal properties: they follow a power\nlaw and they tend to be clustered in time. The second regularity, known as\nvolatility clustering, entails some predictability in the price changes: while\ntheir sign is uncorrelated in time, their amplitude (or volatility) is\nlong-range correlated. Many models have been proposed to account for these\nregularities, notably agent-based models; but these models often invoke\nrelatively complicated mechanisms. This paper identifies a basic reason behind\nvolatility clustering: the impact of exogenous news on expectations. Indeed the\nexpectations of financial agents clearly vary with the advent of news; the\nsimplest way of modeling this idea is to assume the expectations follow a\nrandom walk. We show that this random walk implies volatility clustering in a\ngeneric way.\n"
    },
    {
        "paper_id": 1612.09469,
        "authors": "Javier de Frutos, Victor Gaton",
        "title": "A spectral method for an Optimal Investment problem with Transaction\n  Costs under Potential Utility",
        "comments": null,
        "journal-ref": "Journal of Computational and Applied Mathematics, 319 (2017),\n  262-276",
        "doi": "10.1016/j.cam.2017.01.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns the numerical solution of the finite-horizon Optimal\nInvestment problem with transaction costs under Potential Utility. The problem\nis initially posed in terms of an evolutive HJB equation with gradient\nconstraints. In Finite-Horizon Optimal Investment with Transaction Costs: A\nParabolic Double Obstacle Problem, Day-Yi, the problem is reformulated as a\nnon-linear parabolic double obstacle problem posed in one spatial variable and\ndefined in an unbounded domain where several explicit properties and formulas\nare obtained. The restatement of the problem in polar coordinates allows to\npose the problem in one spatial variable in a finite domain, avoiding some of\nthe technical difficulties of the numerical solution of the previous statement\nof the problem. If high precision is required, the spectral numerical method\nproposed becomes more efficient than simpler methods as finite differences for\nexample.\n"
    },
    {
        "paper_id": 1612.09549,
        "authors": "Ignacio Esponda and Demian Pouzo",
        "title": "The Industry Supply Function and the Long-Run Competitive Equilibrium\n  with Heterogeneous Firms",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In developing the theory of long-run competitive equilibrium (LRCE), Marshall\n(1890) used the notion of a representative firm. The identity of this firm,\nhowever, remained unclear. Subsequent theory either focused on the case where\nall firms are identical or else incorporated heterogeneity but disregarded the\nnotion of a representative firm. Using Hopenhayn's (1992) model of competitive\nindustry dynamics, we extend the theory of LRCE to account for heterogeneous\nfirms and show that the long-run supply function can indeed be characterized as\nthe solution to the minimization of a representative average cost function.\n"
    },
    {
        "paper_id": 1612.09553,
        "authors": "Ulrike Malmendier and Demian Pouzo and Victoria Vanasco",
        "title": "Investor Experiences and Financial Market Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do macro-financial shocks affect investor behavior and market dynamics?\nRecent evidence on experience effects suggests a long-lasting influence of\npersonally experienced outcomes on investor beliefs and investment, but also\nsignificant differences across older and younger generations. We formalize\nexperience-based learning in an OLG model, where different cross-cohort\nexperiences generate persistent heterogeneity in beliefs, portfolio choices,\nand trade. The model allows us to characterize a novel link between investor\ndemographics and the dependence of prices on past dividends, while also\ngenerating known features of asset prices, such as excess volatility and return\npredictability. The model produces new implications for the cross-section of\nasset holdings, trade volume, and investors' heterogenous responses to recent\nfinancial crises, which we show to be in line with the data.\n"
    },
    {
        "paper_id": 1701.0003,
        "authors": "Vadim Kaushansky and Alexander Lipton and Christoph Reisinger",
        "title": "Numerical analysis of an extended structural default model with mutual\n  liabilities and jump risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a structural default model in an interconnected banking network\nas in Lipton [International Journal of Theoretical and Applied Finance, 19(6),\n2016], with mutual obligations between each pair of banks. We analyse the model\nnumerically for two banks with jumps in their asset value processes.\nSpecifically, we develop a finite difference method for the resulting\ntwo-dimensional partial integro-differential equation, and study its stability\nand consistency. We then compute joint and marginal survival probabilities, as\nwell as prices of credit default swaps (CDS), first-to-default swaps (FTD),\ncredit and debt value adjustments (CVA and DVA). Finally, we calibrate the\nmodel to market data and assess the impact of jump risk.\n"
    },
    {
        "paper_id": 1701.00112,
        "authors": "Nicola Cantarutti, Jo\\~ao Guerra",
        "title": "Multinomial method for option pricing under Variance Gamma",
        "comments": null,
        "journal-ref": "International Journal of Computer Mathematics, 96:6, 2019,\n  1087-1106,",
        "doi": "10.1080/00207160.2018.1427853",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a multinomial method for option pricing when the\nunderlying asset follows an exponential Variance Gamma process. The continuous\ntime Variance Gamma process is approximated by a discrete time Markov chain\nwith the same firsts four cumulants. This approach is particularly convenient\nfor pricing American and Bermudan options, which can be exercised at any time\nup to expiration date. Numerical computations of European and American options\nare presented, and compared with results obtained with finite differences\nmethods and with the Black Scholes model.\n"
    },
    {
        "paper_id": 1701.0054,
        "authors": "Medya Siadat, Ola Hammarlid",
        "title": "Net Stable Funding Ratio: Impact on Funding Value Adjustment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the relationship between Funding Value\nAdjustment (FVA) and Net Stable Funding Ratio (NSFR). FVA is defined in a\nconsistent way with NSFR such that the new framework of FVA monitors the costs\ndue to keeping NSFR at an acceptable level, as well.\n  In addition, the problem of choosing the optimal funding strategy is\nformulated as a shortest path problem where the proposed FVA framework is\napplied in the optimization process. The solution provides us with the optimal\nfunding decisions that lead to the minimum funding cost of the transaction. We\nalso provide numerical experiments for FVA calculation and optimization\nproblem.\n"
    },
    {
        "paper_id": 1701.00875,
        "authors": "Tim Leung and Yerkin Kitapbayev",
        "title": "Optimal Mean-Reverting Spread Trading: Nonlinear Integral Equation\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study several optimal stopping problems that arise from trading a\nmean-reverting price spread over a finite horizon. Modeling the spread by the\nOrnstein-Uhlenbeck process, we analyze three different trading strategies: (i)\nthe long-short strategy; (ii) the short-long strategy, and (iii) the chooser\nstrategy, i.e. the trader can enter into the spread by taking either long or\nshort position. In each of these cases, we solve an optimal double stopping\nproblem to determine the optimal timing for starting and subsequently closing\nthe position. We utilize the local time-space calculus of Peskir (2005a) and\nderive the nonlinear integral equations of Volterra-type that uniquely char-\nacterize the boundaries associated with the optimal timing decisions in all\nthree problems. These integral equations are used to numerically compute the\noptimal boundaries.\n"
    },
    {
        "paper_id": 1701.00886,
        "authors": "Chunfa Wang",
        "title": "Pricing European Options by Stable Fourier-Cosine Series Expansions",
        "comments": "14 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COS method proposed in Fang and Oosterlee (2008), although highly\nefficient, may lack robustness for a number of cases. In this paper, we present\na Stable pricing of call options based on Fourier cosine series expansion. The\nStability of the pricing methods is demonstrated by error analysis, as well as\nby a series of numerical examples, including the Heston stochastic volatility\nmodel, Kou jump-diffusion model, and CGMY model.\n"
    },
    {
        "paper_id": 1701.00993,
        "authors": "Friedrich Hubalek and Paul Kr\\\"uhner and Thorsten Rheinl\\\"ander",
        "title": "Brownian trading excursions and avalanches",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a parsimonious but non-trivial model of the latent limit order book\nwhere orders get placed with a fixed displacement from a center price process,\ni.e.\\ some process in-between best bid and best ask, and get executed whenever\nthis center price reaches their level. This mechanism corresponds to the\nfundamental solution of the stochastic heat equation with multiplicative noise\nfor the relative order volume distribution. We classify various types of\ntrades, and introduce the trading excursion process which is a Poisson point\nprocess. This allows to derive the Laplace transforms of the times to various\ntrading events under the corresponding intensity measure. As a main\napplication, we study the distribution of order avalanches, i.e.\\ a series of\norder executions not interrupted by more than an $\\varepsilon$-time interval,\nwhich moreover generalizes recent results about Parisian options.\n"
    },
    {
        "paper_id": 1701.01185,
        "authors": "Simon Clinet and Yoann Potiron",
        "title": "Efficient asymptotic variance reduction when estimating volatility in\n  high frequency data",
        "comments": "60 pages, 8 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows how to carry out efficient asymptotic variance reduction\nwhen estimating volatility in the presence of stochastic volatility and\nmicrostructure noise with the realized kernels (RK) from [Barndorff-Nielsen et\nal., 2008] and the quasi-maximum likelihood estimator (QMLE) studied in [Xiu,\n2010]. To obtain such a reduction, we chop the data into B blocks, compute the\nRK (or QMLE) on each block, and aggregate the block estimates. The ratio of\nasymptotic variance over the bound of asymptotic efficiency converges as B\nincreases to the ratio in the parametric version of the problem, i.e. 1.0025 in\nthe case of the fastest RK Tukey-Hanning 16 and 1 for the QMLE. The impact of\nstochastic sampling times and jump in the price process is examined carefully.\nThe finite sample performance of both estimators is investigated in\nsimulations, while empirical work illustrates the gain in practice.\n"
    }
]