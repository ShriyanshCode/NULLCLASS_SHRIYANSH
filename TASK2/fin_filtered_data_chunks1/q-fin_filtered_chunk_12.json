[
    {
        "paper_id": 2012.0084,
        "authors": "S{\\i}la Ada and Nadia Abou Nabout and Elea McDonnell Feit",
        "title": "Context information increases revenue in ad auctions: Evidence from a\n  policy change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ad exchanges, i.e., platforms where real-time auctions for ad impressions\ntake place, have developed sophisticated technology and data ecosystems to\nallow advertisers to target users, yet advertisers may not know which sites\ntheir ads appear on, i.e., the ad context. In practice, ad exchanges can\nrequire publishers to provide accurate ad placement information to ad buyers\nprior to submitting their bids, allowing them to adjust their bids for ads at\nspecific domains, subdomains or URLs. However, ad exchanges have historically\nbeen reluctant to disclose placement information due to fears that buyers will\nstart buying ads only on the most desirable sites leaving inventory on other\nsites unsold and lowering average revenue. This paper explores the empirical\neffect of ad placement disclosure using a unique data set describing a change\nin context information provided by a major private European ad exchange.\nAnalyzing this as a quasi-experiment using diff-in-diff, we find that average\nrevenue per impression rose when more context information was provided. This\nshows that ad context information is important to ad buyers and that providing\nmore context information will not lead to deconflation. The exception to this\nare sites which had a low number of buyers prior to the policy change;\nconsistent with theory, these sites with thin markets do not show a rise in\nprices. Our analysis adds evidence that ad exchanges with reputable publishers,\nparticularly smaller volume, high quality sites, should provide ad buyers with\nsite placement information, which can be done at almost no cost.\n"
    },
    {
        "paper_id": 2012.00934,
        "authors": "Kelvin Say, Michele John",
        "title": "Molehills into mountains: Transitional pressures from household\n  PV-battery adoption under flat retail and feed-in tariffs",
        "comments": "69 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With Australia's significant capacity of household PV, decreasing battery\ncosts may lead to widespread use of household PV-battery systems. As the\nadoption of these systems are heavily influenced by retail tariffs, this paper\nanalyses the effects of flat retail tariffs with households free to invest in\nPV battery systems. Using Perth, Australia for context, an open-source model is\nused to simulate household PV battery investments over a 20-year period. We\nfind that flat usage and feed-in tariffs lead to distinct residual demand\npatterns as households' transition from PV-only to PV-battery systems.\nAnalysing these patterns qualitatively from the bottom-up, we identify tipping\npoint transitions that may challenge future electricity system management,\nmarket participation and energy policy. The continued use of flat tariffs\nincentivises PV-battery households to maximise self-consumption, which reduces\nannual grid-imports, increases annual grid-exports, and shifts residual demand\ntowards winter. Diurnal and seasonal demand patterns continue to change as\nPV-battery households eventually become net-generators. Unmanaged, these\nbottom-up changes may complicate energy decarbonisation efforts within\ncentralised electricity markets and suggests that policymakers should prepare\nfor PV-battery households to play a more active role in the energy system.\n"
    },
    {
        "paper_id": 2012.01016,
        "authors": "Diti Goswami and Sourabh Bikas Paul",
        "title": "Labor Reforms in Rajasthan: A boon or a bane?",
        "comments": "23 pages, 11 tables and 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the impact of labour law deregulations in the Indian state of\nRajasthan on plant employment and productivity. In 2014, after a long time,\nRajasthan was the first Indian state that introduced labour reforms in the\nIndustrial Disputes Act (1947), the Factories Act (1948), the Contract Labour\n(Regulation and Abolition) Act (1970), and the Apprentices Act (1961).\nExploiting this unique quasi-natural experiment, we apply a\ndifference-in-difference framework using the Annual Survey of Industries panel\ndata of manufacturing plants. Our results show that reforms had an unintended\nconsequence of the decline in labour use. Also, worryingly, the flexibility\nresulted in the disproportionate decline in the directly employed worker.\nEvidence suggests that the reforms positively impacted the plants' value-added\nand productivity. The strength of these effects varies depending on the\nunderlying industry and reform structure. These findings prove robust to a set\nof specifications.\n"
    },
    {
        "paper_id": 2012.01121,
        "authors": "Frank Phillipson and Harshil Singh Bhatia",
        "title": "Portfolio Optimisation Using the D-Wave Quantum Annealer",
        "comments": "14 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The first quantum computers are expected to perform well at quadratic\noptimisation problems. In this paper a quadratic problem in finance is taken,\nthe Portfolio Optimisation problem. Here, a set of assets is chosen for\ninvestment, such that the total risk is minimised, a minimum return is realised\nand a budget constraint is met. This problem is solved for several instances in\ntwo main indices, the Nikkei225 and the S\\&P500 index, using the\nstate-of-the-art implementation of D-Wave's quantum annealer and its hybrid\nsolvers. The results are benchmarked against conventional, state-of-the-art,\ncommercially available tooling. Results show that for problems of the size of\nthe used instances, the D-Wave solution, in its current, still limited size,\ncomes already close to the performance of commercial solvers.\n"
    },
    {
        "paper_id": 2012.0116,
        "authors": "Devansh Jain, Manthan Patel, Aman Narsaria and Siddharth Malik",
        "title": "A Study on the Efficiency of the Indian Stock Market",
        "comments": "9 pages, 4 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The efficiency of the stock market has a significant impact on the potential\nreturn on investment. An efficient market eliminates the possibility of\narbitrage and unexploited profit opportunities. This study analyzes the weak\nform efficiency of the Indian Stock market based on the two major Indian stock\nexchanges, viz., BSE and NSE. The daily closing values of Sensex and Nifty\nindices for the period from April 2010 to March 2019 are used to perform the\nRuns test, the Autocorrelation test, and the Autoregression test. The study\nconfirms that the Indian Stock market is weak form inefficient and can thus be\noutperformed.\n"
    },
    {
        "paper_id": 2012.01235,
        "authors": "Goncalo dos Reis and Vadim Platonov",
        "title": "Forward utility and market adjustments in relative\n  investment-consumption games of many players",
        "comments": "40 pages, 4 Figure, 1 Table; With full length calculations for arXiv\n  submission. arXiv admin note: text overlap with arXiv:1703.07685 by other\n  authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a portfolio management problem featuring many-player and mean field\ncompetition, investment and consumption, and relative performance concerns\nunder the forward performance processes (FPP) framework. We focus on agents\nusing power (CRRA) type FPPs for their investment-consumption optimization\nproblem under a common noise Merton market model. We solve both the many-player\nand mean field game providing closed-form expressions for the solutions where\nthe limit of the former yields the latter. In our case, the FPP framework\nyields a continuum of solutions for the consumption component as indexed to a\nmarket parameter we coin \"market-risk relative consumption preference\". The\nparameter permits the agent to set a preference for their consumption going\nforward in time that, in the competition case, reflects a common market\nbehaviour. We show the FPP framework, under both competition and\nno-competition, allows the agent to disentangle her risk-tolerance and\nelasticity of intertemporal substitution (EIS) just like Epstein-Zin\npreferences under recursive utility framework and unlike the classical utility\ntheory one. This, in turn, allows a finer analysis on the agent's consumption\n\"income\" and \"substitution\" regimes, and, of independent interest, motivates a\nnew strand of economics research on EIS under the FPP framework. We find that\ncompetition rescales the agent's perception of consumption in a non-trivial\nmanner. We provide numerical illustrations of our results.\n"
    },
    {
        "paper_id": 2012.01257,
        "authors": "Yuri Kifer",
        "title": "Error estimates for discrete approximations of game options with\n  multivariate diffusion asset prices",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2011.07907",
        "journal-ref": "Journal of Stochastic Analysis 2 (2021), no.3, article 8",
        "doi": "10.31390/josa.2.3.08",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain error estimates for strong approximations of a diffusion with a\ndiffusion matrix $\\sigma$ and a drift b by the discrete time process defined\nrecursively X_N((n+1)/N) =\nX_N(n/N)+N^{1/2}\\sigma(X_N(n/N))\\xi(n+1)+N^{-1}b(XN(n/N)); where \\xi(n); n\\geq\n1 are i.i.d. random vectors, and apply this in order to approximate the fair\nprice of a game option with a diffusion asset price evolution by values of\nDynkin's games with payoffs based on the above discrete time processes. This\nprovides an effective tool for computations of fair prices of game options with\npath dependent payoffs in a multi asset market with diffusion evolution.\n"
    },
    {
        "paper_id": 2012.01272,
        "authors": "Tatiana Kozitsina (Babkina), Anna Mikhaylova, Anna Komkova, Anastasia\n  Peshkovskaya, Anna Sedush, Olga Menshikova, Mikhail Myagkov, and Ivan\n  Menshikov",
        "title": "Ethnicity and gender influence the decision making in a multinational\n  state: The case of Russia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Individuals' behavior in economic decisions depends on such factors as\nethnicity, gender, social environment, personal traits. However, the\ndistinctive features of decision making have not been studied properly so far\nbetween indigenous populations from different ethnicities in a modern and\nmultinational state like the Russian Federation. Addressing this issue, we\nconducted a series of experiments between the Russians in Moscow (the capital\nof Russia) and the Yakuts in Yakutsk (the capital of Russian region with the\nmostly non-Russian residents). We investigated the effect of socialization on\nparticipants' strategies in the Prisoner's Dilemma game, Ultimatum game, and\nTrust game. At the baseline stage, before socialization, the rates of\ncooperation, egalitarianism, and trust for the Yakuts are higher than for the\nRussians in groups composed of unfamiliar people. After socialization, for the\nRussians all these indicators increase considerably; whereas, for the Yakuts\nonly the rate of cooperation demonstrates a rising trend. The Yakuts are\ncharacterized by relatively unchanged indicators regardless of the\nsocialization stage. Furthermore, the Yakutsk females have higher rates of\ncooperation and trust than the Yakuts males before socialization. After\nsocialization, we observed the alignment in indicators for males and females\nboth for the Russians and for the Yakuts. Hence, we concluded that cultural\ndifferences can exist inside one country despite the equal economic, politic,\nand social conditions.\n"
    },
    {
        "paper_id": 2012.01294,
        "authors": "Jann Michael Weinand, Kenneth S\\\"orensen, Pablo San Segundo, Max\n  Kleinebrahm, Russell McKenna",
        "title": "Research trends in combinatorial optimisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Real-world problems are becoming highly complex and, therefore, have to be\nsolved with combinatorial optimisation (CO) techniques. Motivated by the strong\nincrease of publications on CO, 8,393 articles from this research field are\nsubjected to a bibliometric analysis. The corpus of literature is examined\nusing mathematical methods and a novel algorithm for keyword analysis. In\naddition to the most relevant countries, organisations and authors as well as\ntheir collaborations, the most relevant CO problems, solution methods and\napplication areas are presented. Publications on CO focus mainly on the\ndevelopment or enhancement of metaheuristics like genetic algorithms. The\nincreasingly problem-oriented studies deal particularly with real-world\napplications within the energy sector, production sector or data management,\nwhich are of increasing relevance due to various global developments. The\ndemonstration of global research trends in CO can support researchers in\nidentifying the relevant issues regarding this expanding and transforming\nresearch area.\n"
    },
    {
        "paper_id": 2012.01331,
        "authors": "Liqun Liu",
        "title": "Accountability and Motivation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We build a formal model that examines how different policymaking environments\nshape career-concerned officials' reform decisions and implementation. When\ncareer concerns are strong, officials will inefficiently initiate reforms to\nsignal to the central government that they are congruent. To improve the\nquality of reform policymaking, the central government must hold officials\naccountable to policy outcomes. We demonstrate that the central government can\nexercise this accountability by requiring officials to publicize policy\noutcomes while maintain secrecy on implementation details. In this situation,\nofficials can signal their congruence only through a desirable policy outcome,\nso they are highly motivated to carry out a reform well. We also demonstrate\nthat the accountability on policy outcomes is infeasible under alternative\npolicymaking environments. We apply the results to China's recent practice in\ndecentralized reform policymaking.\n"
    },
    {
        "paper_id": 2012.01505,
        "authors": "Burin Gumjudpai (NAS Mahidol, IF Naresuan, ThEP)",
        "title": "Thermodynamics Formulation of Economics",
        "comments": "5 pages, 5 figures, This is an extended abstract for an oral\n  presentation at the International Conference on Thermodynamics 2.0. This work\n  is awarded Richard Newbold Adams Medal for integration of natural and social\n  sciences (https://iaisae.org/index.php/honors/). In the later versions, minor\n  corrections and other corrections to Sec. III(G) are made",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider demand-side economy. Using Caratheodory's approach, we define\nempirical existence of equation of state (EoS) and coordinates. We found new\ninsights of thermodynamics EoS, the {\\it effect structure}. Rules are proposed\nas criteria in promoting and classifying an empirical law to EoS status. Four\nlaws of thermodynamics are given for economics. We proposed a method to model\nthe EoS with econometrics. Consumer surplus in economics can not be considered\nas utility. Concepts such as total wealth, generalized utility and generalized\nsurplus are introduced. EoS provides solid foundation in statistical mechanics\nmodelling of economics and finance.\n"
    },
    {
        "paper_id": 2012.01538,
        "authors": "Michael Thaler",
        "title": "Gender Differences in Motivated Reasoning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Men and women systematically differ in their beliefs about their performance\nrelative to others; in particular, men tend to be more overconfident. This\npaper provides support for one explanation for gender differences in\noverconfidence, performance-motivated reasoning, in which people distort how\nthey process new information in ways that make them believe they outperformed\nothers. Using a large online experiment, I find that male subjects distort\ninformation processing in ways that favor their performance, while female\nsubjects do not systematically distort information processing in either\ndirection. These statistically-significant gender differences in\nperformance-motivated reasoning mimic gender differences in overconfidence;\nbeliefs of male subjects are systematically overconfident, while beliefs of\nfemale subjects are well-calibrated on average. The experiment also includes\npolitical questions, and finds that politically-motivated reasoning is similar\nfor both men and women. These results suggest that, while men and women are\nboth susceptible to motivated reasoning in general, men find it particularly\nattractive to believe that they outperformed others.\n"
    },
    {
        "paper_id": 2012.01548,
        "authors": "Michael Thaler",
        "title": "Good News Is Not a Sufficient Condition for Motivated Reasoning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  People often receive good news that makes them feel better about the world\naround them, or bad news that makes them feel worse about it. This paper\nstudies how the valence of news affects belief updating, absent functional and\nego-relevant factors. Using experiments with over 1,500 participants and 5,600\nobservations, I test whether people engage in motivated reasoning to overly\ntrust good news versus bad news on valence-relevant issues like cancer survival\nrates, others' happiness, and infant mortality. The estimate for motivated\nreasoning towards good news is a precisely-estimated null. Modest effects, of\none-third the size of motivated reasoning in politics and performance, can be\nruled out. Complementary survey evidence shows that most people expect good\nnews to increase happiness, but to not systematically lead to motivated\nreasoning. These results suggest that belief-based utility is not sufficient in\nleading people to distort belief updating in order to favor those beliefs.\n"
    },
    {
        "paper_id": 2012.01663,
        "authors": "Michael Thaler",
        "title": "The Fake News Effect: Experimentally Identifying Motivated Reasoning\n  Using Trust in News",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated reasoning posits that people distort how they process information\nin the direction of beliefs they find attractive. This paper creates a novel\nexperimental design to identify motivated reasoning from Bayesian updating when\npeople have preconceived beliefs. It analyzes how subjects assess the veracity\nof information sources that tell them the median of their belief distribution\nis too high or too low. Bayesians infer nothing about the source veracity, but\nmotivated beliefs are evoked. Evidence supports politically-motivated reasoning\nabout immigration, income mobility, crime, racial discrimination, gender,\nclimate change, and gun laws. Motivated reasoning helps explain belief biases,\npolarization, and overconfidence.\n"
    },
    {
        "paper_id": 2012.01814,
        "authors": "Sara R. Machado",
        "title": "Estimating the Blood Supply Elasticity: Evidence from a Universal Scale\n  Benefit Scheme",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I estimate the semi-elasticity of blood donations with respect to a monetary\nbenefit, namely the waiver of user fees when using the National Health Service,\nin Portugal. Using within-county variation over time in the value of the\nbenefitI estimate both the unconditional elasticity, which captures overall\nresponse of the market, and the conditional elasticity, which holds constant\nthe number of blood drives. This amounts to fixing a measure of the cost of\ndonation to the blood donor. I instrument for the number of blood drives, which\nis endogenous, using a variable based on the number of weekend days and the\nproportion of blood drives on weekends. A one euro increase in the subsidy\nleads 1.8% more donations per 10000 inhabitants, conditional on the number of\nblood drives. The unconditional effect is smaller. The benefit does not attract\nnew donors, instead it fosters repeated donation. Furthermore, the\ndiscontinuation of the benefit lead to a predicted decrease in donations of\naround 18%, on average. However, I show that blood drives have the potential to\neffectively substitute monetary incentives in solving market imbalances.\n"
    },
    {
        "paper_id": 2012.01819,
        "authors": "Taras Bodnar, Mathias Lindholm, Vilhelm Niklasson and Erik Thors\\'en",
        "title": "Bayesian Quantile-Based Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal portfolio allocation problem from a Bayesian perspective\nusing value at risk (VaR) and conditional value at risk (CVaR) as risk\nmeasures. By applying the posterior predictive distribution for the future\nportfolio return, we derive relevant quantiles needed in the computations of\nVaR and CVaR, and express the optimal portfolio weights in terms of observed\ndata only. This is in contrast to the conventional method where the optimal\nsolution is based on unobserved quantities which are estimated, leading to\nsuboptimality. We also obtain the expressions for the weights of the global\nminimum VaR and CVaR portfolios, and specify conditions for their existence. It\nis shown that these portfolios may not exist if the confidence level used for\nthe VaR or CVaR computation are too low. Moreover, analytical expressions for\nthe mean-VaR and mean-CVaR efficient frontiers are presented and the extension\nof theoretical results to general coherent risk measures is provided. One of\nthe main advantages of the suggested Bayesian approach is that the theoretical\nresults are derived in the finite-sample case and thus they are exact and can\nbe applied to large-dimensional portfolios.\n  By using simulation and real market data, we compare the new Bayesian\napproach to the conventional method by studying the performance and existence\nof the global minimum VaR portfolio and by analysing the estimated efficient\nfrontiers. It is concluded that the Bayesian approach outperforms the\nconventional one, in particular at predicting the out-of-sample VaR.\n"
    },
    {
        "paper_id": 2012.01883,
        "authors": "Louis Abraham",
        "title": "Competition analysis on the over-the-counter credit default swap market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study two questions related to competition on the OTC CDS market using\ndata collected as part of the EMIR regulation.\n  First, we study the competition between central counterparties through\ncollateral requirements. We present models that successfully estimate the\ninitial margin requirements. However, our estimations are not precise enough to\nuse them as input to a predictive model for CCP choice by counterparties in the\nOTC market.\n  Second, we model counterpart choice on the interdealer market using a novel\nsemi-supervised predictive task. We present our methodology as part of the\nliterature on model interpretability before arguing for the use of conditional\nentropy as the metric of interest to derive knowledge from data through a\nmodel-agnostic approach. In particular, we justify the use of deep neural\nnetworks to measure conditional entropy on real-world datasets. We create the\n$\\textit{Razor entropy}$ using the framework of algorithmic information theory\nand derive an explicit formula that is identical to our semi-supervised\ntraining objective. Finally, we borrow concepts from game theory to define\n$\\textit{top-k Shapley values}$. This novel method of payoff distribution\nsatisfies most of the properties of Shapley values, and is of particular\ninterest when the value function is monotone submodular. Unlike classical\nShapley values, top-k Shapley values can be computed in quadratic time of the\nnumber of features instead of exponential. We implement our methodology and\nreport the results on our particular task of counterpart choice.\n  Finally, we present an improvement to the $\\textit{node2vec}$ algorithm that\ncould for example be used to further study intermediation. We show that the\nneighbor sampling used in the generation of biased walks can be performed in\nlogarithmic time with a quasilinear time pre-computation, unlike the current\nimplementations that do not scale well.\n"
    },
    {
        "paper_id": 2012.01903,
        "authors": "Asier Minondo",
        "title": "Impact of COVID-19 on the trade of goods and services in Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The COVID-19 crisis has led to the sharpest collapse in the Spanish trade of\ngoods and services in recent decades. The containment measures adopted to\narrest the spread of the virus have caused an especially intense fall of trade\nin services. Spain's export specialization in transport equipment, capital and\noutdoor goods, and services that rely on the movement of people has made the\nCOVID-19 trade crisis more intense in Spain than in the rest of the European\nUnion. However, the nature of the collapse suggests that trade in goods can\nrecover swiftly when the health crisis ends. On the other hand, COVID-19 may\nhave a long-term negative impact on the trade of services that rely on the\nmovement of people.\n"
    },
    {
        "paper_id": 2012.02091,
        "authors": "Oscar Claveria",
        "title": "Business and consumer uncertainty in the face of the pandemic: A sector\n  analysis in European countries",
        "comments": "29 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the evolution of business and consumer uncertainty amid\nthe coronavirus pandemic in 32 European countries and the European Union\n(EU).Since uncertainty is not directly observable, we approximate it using the\ngeometric discrepancy indicator of Claveria et al. (2019).This approach allows\nus quantifying the proportion of disagreement in business and consumer\nexpectations of 32 countries.We have used information from all monthly\nforward-looking questions contained in Joint Harmonised Programme of Business\nand Consumer Surveys conducted by the European Commission (the industry survey,\nthe service survey, the retail trade survey, the building survey and the\nconsumer survey).First, we have calculated a discrepancy indicator for each of\nthe 17 survey questions analysed, which allows us to approximate the proportion\nof uncertainty about different aspects of economic activity, both form the\ndemand and the supply sides of the economy.We then use these indicators to\ncalculate disagreement indices at the sector level.We graphic the evolution of\nthe degree of uncertainty in the main economic sectors of the analysed\neconomies up to June 2020.We observe marked differences, both across variables,\nsectors and countries since the inception of the COVID-19 crisis.Finally, by\nadding the sectoral indicators, an indicator of business uncertainty is\ncalculated and compared with that of consumers.Again, we find substantial\ndifferences in the evolution of uncertainty between managers and consumers.This\nanalysis seeks to offer a global overview of the degree of economic uncertainty\nin the midst of the coronavirus crisis at the sectoral level.\n"
    },
    {
        "paper_id": 2012.02098,
        "authors": "Luis Felipe Guti\\'errez, Sima Siami-Namini, Neda Tavakoli, Akbar Siami\n  Namin",
        "title": "A Concern Analysis of FOMC Statements Comparing The Great Recession and\n  The COVID-19 Pandemic",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is important and informative to compare and contrast major economic crises\nin order to confront novel and unknown cases such as the COVID-19 pandemic. The\n2006 Great Recession and then the 2019 pandemic have a lot to share in terms of\nunemployment rate, consumption expenditures, and interest rates set by Federal\nReserve. In addition to quantitative historical data, it is also interesting to\ncompare the contents of Federal Reserve statements for the period of these two\ncrises and find out whether Federal Reserve cares about similar concerns or\nthere are some other issues that demand separate and unique monetary policies.\nThis paper conducts an analysis to explore the Federal Reserve concerns as\nexpressed in their statements for the period of 2005 to 2020. The concern\nanalysis is performed using natural language processing (NLP) algorithms and a\ntrend analysis of concern is also presented. We observe that there are some\nsimilarities between the Federal Reserve statements issued during the Great\nRecession with those issued for the 2019 COVID-19 pandemic.\n"
    },
    {
        "paper_id": 2012.02277,
        "authors": "Bahman Angoshtari, Erhan Bayraktar, Virginia R. Young",
        "title": "Optimal Consumption under a Habit-Formation Constraint: the\n  Deterministic Case",
        "comments": "43 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate and solve a deterministic optimal consumption problem to\nmaximize the discounted CRRA utility of an individual's consumption-to-habit\nprocess assuming she only invests in a riskless market and that she is\nunwilling to consume at a rate below a certain proportion $\\alpha\\in(0,1]$ of\nher consumption habit. Increasing $\\alpha$, increases the degree of\naddictiveness of habit formation, with $\\alpha=0$ (respectively, $\\alpha=1$)\ncorresponding to non-addictive (respectively, completely addictive) model. We\nderive the optimal consumption policies explicitly in terms of the solution of\na nonlinear free-boundary problem, which we analyze in detail. Impatient\nindividuals (or, equivalently, those with more addictive habits) always consume\nabove the minimum rate; thus, they eventually attain the minimum\nwealth-to-habit ratio. Patient individuals (or, equivalently, those with less\naddictive habits) consume at the minimum rate if their wealth-to-habit ratio is\nbelow a threshold, and above it otherwise. By consuming patiently, these\nindividuals maintain a wealth-to-habit ratio that is greater than the minimum\nacceptable level. Additionally, we prove that the optimal consumption path is\nhump-shaped if the initial wealth-to-habit ratio is either: (1) larger than a\nhigh threshold; or (2) below a low threshold and the agent is more risk seeking\n(that is, less risk averse). Thus, we provide a simple explanation for the\nconsumption hump observed by various empirical studies.\n"
    },
    {
        "paper_id": 2012.02393,
        "authors": "Bo Cowgill, Fabrizio Dell'Acqua and Sandra Matz",
        "title": "The Managerial Effects of Algorithmic Fairness Activism",
        "comments": "Part of the Navigating the Broader Impacts of AI Research Workshop at\n  NeurIPS 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How do ethical arguments affect AI adoption in business? We randomly expose\nbusiness decision-makers to arguments used in AI fairness activism. Arguments\nemphasizing the inescapability of algorithmic bias lead managers to abandon AI\nfor manual review by humans and report greater expectations about lawsuits and\nnegative PR. These effects persist even when AI lowers gender and racial\ndisparities and when engineering investments to address AI fairness are\nfeasible. Emphasis on status quo comparisons yields opposite effects. We also\nmeasure the effects of \"scientific veneer\" in AI ethics arguments. Scientific\nveneer changes managerial behavior but does not asymmetrically benefit\nfavorable (versus critical) AI activism.\n"
    },
    {
        "paper_id": 2012.02394,
        "authors": "Bo Cowgill, Fabrizio Dell'Acqua, Samuel Deng, Daniel Hsu, Nakul Verma\n  and Augustin Chaintreau",
        "title": "Biased Programmers? Or Biased Data? A Field Experiment in\n  Operationalizing AI Ethics",
        "comments": "Part of the Navigating the Broader Impacts of AI Research Workshop at\n  NeurIPS 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Why do biased predictions arise? What interventions can prevent them? We\nevaluate 8.2 million algorithmic predictions of math performance from\n$\\approx$400 AI engineers, each of whom developed an algorithm under a randomly\nassigned experimental condition. Our treatment arms modified programmers'\nincentives, training data, awareness, and/or technical knowledge of AI ethics.\nWe then assess out-of-sample predictions from their algorithms using randomized\naudit manipulations of algorithm inputs and ground-truth math performance for\n20K subjects. We find that biased predictions are mostly caused by biased\ntraining data. However, one-third of the benefit of better training data comes\nthrough a novel economic mechanism: Engineers exert greater effort and are more\nresponsive to incentives when given better training data. We also assess how\nperformance varies with programmers' demographic characteristics, and their\nperformance on a psychological test of implicit bias (IAT) concerning gender\nand careers. We find no evidence that female, minority and low-IAT engineers\nexhibit lower bias or discrimination in their code. However, we do find that\nprediction errors are correlated within demographic groups, which creates\nperformance improvements through cross-demographic averaging. Finally, we\nquantify the benefits and tradeoffs of practical managerial or policy\ninterventions such as technical advice, simple reminders, and improved\nincentives for decreasing algorithmic bias.\n"
    },
    {
        "paper_id": 2012.02395,
        "authors": "Ilya Archakov and Peter Reinhard Hansen",
        "title": "A New Parametrization of Correlation Matrices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a novel parametrization of the correlation matrix. The\nreparametrization facilitates modeling of correlation and covariance matrices\nby an unrestricted vector, where positive definiteness is an innate property.\nThis parametrization can be viewed as a generalization of Fisther's\nZ-transformation to higher dimensions and has a wide range of potential\napplications. An algorithm for reconstructing the unique n x n correlation\nmatrix from any d-dimensional vector (with d = n(n-1)/2) is provided, and we\nderive its numerical complexity.\n"
    },
    {
        "paper_id": 2012.02662,
        "authors": "Jean-Bernard Chatelain, Kirsten Ralf",
        "title": "Imperfect Credibility versus No Credibility of Optimal Monetary Policy",
        "comments": null,
        "journal-ref": "Revue Economique, (2021), 72(1), 43-63",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A minimal central bank credibility, with a non-zero probability of not\nrenegning his commitment (\"quasi-commitment\"), is a necessary condition for\nanchoring inflation expectations and stabilizing inflation dynamics. By\ncontrast, a complete lack of credibility, with the certainty that the policy\nmaker will renege his commitment (\"optimal discretion\"), leads to the local\ninstability of inflation dynamics. In the textbook example of the new-Keynesian\nPhillips curve, the response of the policy instrument to inflation gaps for\noptimal policy under quasi-commitment has an opposite sign than in optimal\ndiscretion, which explains this bifurcation.\n"
    },
    {
        "paper_id": 2012.02698,
        "authors": "Ilya Archakov and Peter Reinhard Hansen",
        "title": "A Canonical Representation of Block Matrices with Applications to\n  Covariance and Correlation Matrices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We obtain a canonical representation for block matrices. The representation\nfacilitates simple computation of the determinant, the matrix inverse, and\nother powers of a block matrix, as well as the matrix logarithm and the matrix\nexponential. These results are particularly useful for block covariance and\nblock correlation matrices, where evaluation of the Gaussian log-likelihood and\nestimation are greatly simplified. We illustrate this with an empirical\napplication using a large panel of daily asset returns. Moreover, the\nrepresentation paves new ways to regularizing large covariance/correlation\nmatrices, test block structures in matrices, and estimate regressions with many\nvariables.\n"
    },
    {
        "paper_id": 2012.02794,
        "authors": "John Aoga, Juhee Bae, Stefanija Veljanoska, Siegfried Nijssen, Pierre\n  Schaus",
        "title": "Impact of weather factors on migration intention using machine learning\n  algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A growing attention in the empirical literature has been paid to the\nincidence of climate shocks and change in migration decisions. Previous\nliterature leads to different results and uses a multitude of traditional\nempirical approaches.\n  This paper proposes a tree-based Machine Learning (ML) approach to analyze\nthe role of the weather shocks towards an individual's intention to migrate in\nthe six agriculture-dependent-economy countries such as Burkina Faso, Ivory\nCoast, Mali, Mauritania, Niger, and Senegal. We perform several tree-based\nalgorithms (e.g., XGB, Random Forest) using the train-validation-test workflow\nto build robust and noise-resistant approaches. Then we determine the important\nfeatures showing in which direction they are influencing the migration\nintention. This ML-based estimation accounts for features such as weather\nshocks captured by the Standardized Precipitation-Evapotranspiration Index\n(SPEI) for different timescales and various socioeconomic features/covariates.\n  We find that (i) weather features improve the prediction performance although\nsocioeconomic characteristics have more influence on migration intentions, (ii)\ncountry-specific model is necessary, and (iii) international move is influenced\nmore by the longer timescales of SPEIs while general move (which includes\ninternal move) by that of shorter timescales.\n"
    },
    {
        "paper_id": 2012.02806,
        "authors": "Jean-Bernard Chatelain, Kirsten Ralf",
        "title": "Policy Maker's Credibility with Predetermined Instruments for\n  Forward-Looking Targets",
        "comments": null,
        "journal-ref": "Revue d'Economie Politique (2000), 130(5), 823-846",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of the present paper is to provide criteria for a central bank of how\nto choose among different monetary-policy rules when caring about a number of\npolicy targets such as the output gap and expected inflation. Special attention\nis given to the question if policy instruments are predetermined or only\nforward looking. Using the new-Keynesian Phillips curve with a cost-push-shock\npolicy-transmission mechanism, the forward-looking case implies an extreme lack\nof robustness and of credibility of stabilization policy. The backward-looking\ncase is such that the simple-rule parameters can be the solution of Ramsey\noptimal policy under limited commitment. As a consequence, we suggest to model\nexplicitly the rational behavior of the policy maker with Ramsey optimal\npolicy, rather than to use simple rules with an ambiguous assumption leading to\npolicy advice that is neither robust nor credible.\n"
    },
    {
        "paper_id": 2012.02966,
        "authors": "Daria Loginova, Marco Portmann, Martin Huber",
        "title": "Assessing the effects of seasonal tariff-rate quotas on vegetable prices\n  in Switzerland",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Causal estimation of the short-term effects of tariff-rate quotas (TRQs) on\nvegetable producer prices is hampered by the large variety and different\ngrowing seasons of vegetables and is therefore rarely performed. We quantify\nthe effects of Swiss seasonal TRQs on domestic producer prices of a variety of\nvegetables based on a difference-in-differences estimation using a novel\ndataset of weekly producer prices for Switzerland and neighbouring countries.\nWe find that TRQs increase prices of most vegetables by more than 20% above the\nprices in neighbouring countries during the main harvest time for most\nvegetables and even more than 50% for some vegetables. The effects are stronger\nfor more perishable vegetables and for conventionally produced ones compared\nwith organic vegetables. However, we do not find clear-cut effects of TRQs on\nthe week-to-week price volatility of vegetables although the overall lower\nprice volatility in Switzerland compared with neighbouring countries might be a\nresult of the TRQ system in place.\n"
    },
    {
        "paper_id": 2012.02968,
        "authors": "Amitesh Saha",
        "title": "Decision making in Economics -- a behavioral approach",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We review economic research regarding the decision making processes of\nindividuals in economics, with a particular focus on papers which tried\nanalyzing factors that affect decision making with the evolution of the history\nof economic thought. The factors that are discussed here are psychological,\nemotional, cognitive systems, and social norms. Apart from analyzing these\nfactors, it deals with the reasons behind the limitations of rational\ndecision-making theory in individual decision making and the need for a\nbehavioral theory of decision making. In this regard, it has also reviewed the\nrole of situated learning in the decision-making process.\n"
    },
    {
        "paper_id": 2012.03012,
        "authors": "Anish Rai, Ajit Mahata, Md Nurujjaman and Om Prakash",
        "title": "Statistical properties of the aftershocks of stock market crashes\n  revisited: Analysis based on the 1987 crash, financial-crisis-2008 and\n  COVID-19 pandemic",
        "comments": "Accepted in International Journal of Modern Physics C (World\n  Scientific)",
        "journal-ref": "International Journal of Modern Physics C, 2021",
        "doi": "10.1142/S012918312250019X",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  During any unique crisis, panic sell-off leads to a massive stock market\ncrash that may continue for more than a day, termed as mainshock. The effect of\na mainshock in the form of aftershocks can be felt throughout the recovery\nphase of stock price. As the market remains in stress during recovery, any\nsmall perturbation leads to a relatively smaller aftershock. The duration of\nthe recovery phase has been estimated using structural break analysis. We have\ncarried out statistical analyses of the 1987 stock market crash, 2008 financial\ncrisis and 2020 COVID-19 pandemic considering the actual crash-times of the\nmainshock and aftershocks. Earlier, such analyses were done considering an\nabsolute one-day return, which cannot capture a crash properly. The results\nshow that the mainshock and aftershock in the stock market follow the\nGutenberg-Richter (GR) power law. Further, we obtained a higher $\\beta$ value\nfor the COVID-19 crash compared to the financial-crisis-2008 from the GR law.\nThis implies that the recovery of stock price during COVID-19 may be faster\nthan the financial-crisis-2008. The result is consistent with the present\nrecovery of the market from the COVID-19 pandemic. The analysis shows that the\nhigh magnitude aftershocks are rare, and low magnitude aftershocks are frequent\nduring the recovery phase. The analysis also shows that the distribution\n$P(\\tau_i)$ follows the generalized Pareto distribution, i.e.,\n$\\displaystyle~P(\\tau_i)\\propto\\frac{1}{\\{1+\\lambda(q-1)\\tau_i\\}^{\\frac{1}{(q-1)}}}$,\nwhere $\\lambda$ and $q$ are constants and $\\tau_i$ is the inter-occurrence\ntime. This analysis may help investors to restructure their portfolios during a\nmarket crash.\n"
    },
    {
        "paper_id": 2012.03078,
        "authors": "Michal Balcerak, Thomas Schmelzer",
        "title": "Constructing trading strategy ensembles by classifying market states",
        "comments": "20 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rather than directly predicting future prices or returns, we follow a more\nrecent trend in asset management and classify the state of a market based on\nlabels. We use numerous standard labels and even construct our own ones. The\nlabels rely on future data to be calculated, and can be used a target for\ntraining a market state classifier using an appropriate set of market features,\ne.g. moving averages. The construction of those features relies on their label\nseparation power. Only a set of reasonable distinct features can approximate\nthe labels. For each label we use a specific neural network to classify the\nstate using the market features from our feature space. Each classifier gives a\nprobability to buy or to sell and combining all their recommendations (here\nonly done in a linear way) results in what we call a trading strategy. There\nare many such strategies and some of them are somewhat dubious and misleading.\nWe construct our own metric based on past returns but penalising for a low\nnumber of transactions or small capital involvement. Only top\nscore-performance-wise trading strategies end up in final ensembles. Using the\nBitcoin market we show that the strategy ensembles outperform both in returns\nand risk-adjusted returns in the out-of-sample period. Even more so we\ndemonstrate that there is a clear correlation between the success achieved in\nthe past (if measured in our custom metric) and the future.\n"
    },
    {
        "paper_id": 2012.03144,
        "authors": "Misnal Munir, Amaliyah, Moses Glorino Rumambo Pandin",
        "title": "New Perspectives to Reduce Stress through Digital Humor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study aimed to find new perspectives on the use of humor through digital\nmedia. A qualitative approach was used to conduct this study, where data were\ncollected through a literature review. Stress is caused by the inability of a\nperson to adapt between desires and reality. All forms of stress are basically\ncaused by a lack of understanding of human's own limitations. Inability to\nfight limitations that will cause frustration, conflict, anxiety, and guilt.\nToo much stress can threaten a person's ability to deal with the environment.\nAs a result, employees develop various kinds of stress symptoms that can\ninterfere with their work performance. Thus, the management of work stress is\nimportant to do, one of which uses humor. However, in the digital age, the\nspread of humor can be easily facilitated. The results of this review article\nfind new perspectives to reduce stress through digital humor, namely\ninteractive humor, funny photos, manipulations, phanimation, celebrity\nsoundboards, and PowerPoint humor. The research shows that the use of humor as\na coping strategy is able to predict positive affect and well-being\nwork-related. Moreover, digital humor which has various forms as well as easy,\nfast, and wide spread, then the effect is felt increasingly significant\n"
    },
    {
        "paper_id": 2012.032,
        "authors": "Xiaowei Chen, Wing Fung Chong, Runhuan Feng, Linfeng Zhang",
        "title": "Pandemic risk management: resources contingency planning and allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Repeated history of pandemics, such as SARS, H1N1, Ebola, Zika, and COVID-19,\nhas shown that pandemic risk is inevitable. Extraordinary shortages of medical\nresources have been observed in many parts of the world. Some attributing\nfactors include the lack of sufficient stockpiles and the lack of coordinated\nefforts to deploy existing resources to the location of greatest needs. The\npaper investigates contingency planning and resources allocation from a risk\nmanagement perspective, as opposed to the prevailing supply chain perspective.\nThe key idea is that the competition of limited critical resources is not only\npresent in different geographical locations but also at different stages of a\npandemic. This paper draws on an analogy between risk aggregation and capital\nallocation in finance and pandemic resources planning and allocation for\nhealthcare systems. The main contribution is to introduce new strategies for\noptimal stockpiling and allocation balancing spatio-temporal competitions of\nmedical supply and demand.\n"
    },
    {
        "paper_id": 2012.03327,
        "authors": "Benson Tsz Kin Leung and Pinar Yildirim",
        "title": "Competition, Politics, & Social Media",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An increasing number of politicians are relying on cheaper, easier to access\ntechnologies such as online social media platforms to communicate with their\nconstituency. These platforms present a cheap and low-barrier channel of\ncommunication to politicians, potentially intensifying political competition by\nallowing many to enter political races. In this study, we demonstrate that\nlowering costs of communication, which allows many entrants to come into a\ncompetitive market, can strengthen an incumbent's position when the newcomers\ncompete by providing more information to the voters. We show an asymmetric\nbad-news-good-news effect where early negative news hurts the challengers more\nthan the positive news benefit them, such that in aggregate, an incumbent\npolitician's chances of winning is higher with more entrants in the market. Our\nfindings indicate that communication through social media and other platforms\ncan intensify competition, how-ever incumbency advantage may be strengthened\nrather than weakened as an outcome of higher number of entrants into a\npolitical market.\n"
    },
    {
        "paper_id": 2012.03606,
        "authors": "Saeed Nosratabadi, Nesrine Khazami, Marwa Ben Abdallah, Zoltan\n  Lackner, Shahab S. Band, Amir Mosavi, and Csaba Mako",
        "title": "Social Capital Contributions to Food Security: A Comprehensive\n  Literature Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social capital creates a synergy that benefits all members of a community.\nThis review examines how social capital contributes to the food security of\ncommunities. A systematic literature review, based on Prisma, is designed to\nprovide a state-of-the-art review on capacity social capital in this realm. The\noutput of this method led to finding 39 related articles. Studying these\narticles illustrates that social capital improves food security through two\nmechanisms of knowledge sharing and product sharing (i.e., sharing food\nproducts). It reveals that social capital through improving the food security\npillars (i.e., food availability, food accessibility, food utilization, and\nfood system stability) affects food security. In other words, the interaction\namong the community members results in sharing food products and information\namong community members, which facilitates food availability and access to\nfood. There are many shreds of evidence in the literature that sharing food and\nfood products among the community member decreases household food security and\nprovides healthy nutrition to vulnerable families and improves the food\nutilization pillar of food security. It is also disclosed that belonging to the\nsocial networks increases the community members' resilience and decreases the\ncommunity's vulnerability that subsequently strengthens the stability of a food\nsystem. This study contributes to the common literature on food security and\nsocial capital by providing a conceptual model based on the literature. In\naddition to researchers, policymakers can use this study's findings to provide\nsolutions to address food insecurity problems.\n"
    },
    {
        "paper_id": 2012.03744,
        "authors": "Bojing Feng, Wenfang Xue, Bindang Xue, Zeyu Liu",
        "title": "Every Corporation Owns Its Image: Corporate Credit Ratings via\n  Convolutional Neural Networks",
        "comments": "6 pages. arXiv admin note: text overlap with arXiv:2012.01933",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit rating is an analysis of the credit risks associated with a\ncorporation, which reflect the level of the riskiness and reliability in\ninvesting. There have emerged many studies that implement machine learning\ntechniques to deal with corporate credit rating. However, the ability of these\nmodels is limited by enormous amounts of data from financial statement reports.\nIn this work, we analyze the performance of traditional machine learning models\nin predicting corporate credit rating. For utilizing the powerful convolutional\nneural networks and enormous financial data, we propose a novel end-to-end\nmethod, Corporate Credit Ratings via Convolutional Neural Networks, CCR-CNN for\nbrevity. In the proposed model, each corporation is transformed into an image.\nBased on this image, CNN can capture complex feature interactions of data,\nwhich are difficult to be revealed by previous machine learning models.\nExtensive experiments conducted on the Chinese public-listed corporate rating\ndataset which we build, prove that CCR-CNN outperforms the state-of-the-art\nmethods consistently.\n"
    },
    {
        "paper_id": 2012.03749,
        "authors": "Lara Marie Demajo, Vince Vella and Alexiei Dingli",
        "title": "Explainable AI for Interpretable Credit Scoring",
        "comments": "19 pages, David C. Wyld et al. (Eds): ACITY, DPPR, VLSI, WeST, DSA,\n  CNDC, IoTE, AIAA, NLPTA - 2020",
        "journal-ref": null,
        "doi": "10.5121/csit.2020.101516",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  With the ever-growing achievements in Artificial Intelligence (AI) and the\nrecent boosted enthusiasm in Financial Technology (FinTech), applications such\nas credit scoring have gained substantial academic interest. Credit scoring\nhelps financial experts make better decisions regarding whether or not to\naccept a loan application, such that loans with a high probability of default\nare not accepted. Apart from the noisy and highly imbalanced data challenges\nfaced by such credit scoring models, recent regulations such as the `right to\nexplanation' introduced by the General Data Protection Regulation (GDPR) and\nthe Equal Credit Opportunity Act (ECOA) have added the need for model\ninterpretability to ensure that algorithmic decisions are understandable and\ncoherent. An interesting concept that has been recently introduced is\neXplainable AI (XAI), which focuses on making black-box models more\ninterpretable. In this work, we present a credit scoring model that is both\naccurate and interpretable. For classification, state-of-the-art performance on\nthe Home Equity Line of Credit (HELOC) and Lending Club (LC) Datasets is\nachieved using the Extreme Gradient Boosting (XGBoost) model. The model is then\nfurther enhanced with a 360-degree explanation framework, which provides\ndifferent explanations (i.e. global, local feature-based and local\ninstance-based) that are required by different people in different situations.\nEvaluation through the use of functionallygrounded, application-grounded and\nhuman-grounded analysis show that the explanations provided are simple,\nconsistent as well as satisfy the six predetermined hypotheses testing for\ncorrectness, effectiveness, easy understanding, detail sufficiency and\ntrustworthiness.\n"
    },
    {
        "paper_id": 2012.03798,
        "authors": "Bahman Angoshtari, Virginia R. Young",
        "title": "Optimal Insurance to Minimize the Probability of Ruin: Inverse Survival\n  Function Formulation",
        "comments": "15 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We find the optimal indemnity to minimize the probability of ruin when\npremium is calculated according to the distortion premium principle with a\nproportional risk load, and admissible indemnities are such that both the\nindemnity and retention are non-decreasing functions of the underlying loss. We\nreformulate the problem with the inverse survival function as the control\nvariable and show that deductible insurance with maximum limit is optimal. Our\nmain contribution is in solving this problem via the inverse survival function.\n"
    },
    {
        "paper_id": 2012.03819,
        "authors": "Shouvanik Chakrabarti, Rajiv Krishnakumar, Guglielmo Mazzola, Nikitas\n  Stamatopoulos, Stefan Woerner and William J. Zeng",
        "title": "A Threshold for Quantum Advantage in Derivative Pricing",
        "comments": "Version to be published at Quantum",
        "journal-ref": "Quantum 5, 463 (2021)",
        "doi": "10.22331/q-2021-06-01-463",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We give an upper bound on the resources required for valuable quantum\nadvantage in pricing derivatives. To do so, we give the first complete resource\nestimates for useful quantum derivative pricing, using autocallable and Target\nAccrual Redemption Forward (TARF) derivatives as benchmark use cases. We\nuncover blocking challenges in known approaches and introduce a new method for\nquantum derivative pricing - the re-parameterization method - that avoids them.\nThis method combines pre-trained variational circuits with fault-tolerant\nquantum computing to dramatically reduce resource requirements. We find that\nthe benchmark use cases we examine require 8k logical qubits and a T-depth of\n54 million. We estimate that quantum advantage would require executing this\nprogram at the order of a second. While the resource requirements given here\nare out of reach of current systems, we hope they will provide a roadmap for\nfurther improvements in algorithms, implementations, and planned hardware\narchitectures.\n"
    },
    {
        "paper_id": 2012.03834,
        "authors": "Francesco Furno",
        "title": "The Testing Multiplier: Fear vs Containment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the economic effects of testing during the outbreak of a novel\ndisease. I propose a model where testing permits isolation of the infected and\nprovides agents with information about the prevalence and lethality of the\ndisease. Additional testing reduces the perceived lethality of the disease, but\nmight increase the perceived risk of infection. As a result, more testing could\nincrease the perceived risk of dying from the disease - i.e. \"stoke fear\" - and\ncause a fall in economic activity, despite improving health outcomes. Two main\ninsights emerge. First, increased testing is beneficial to the economy and pays\nfor itself if performed at a sufficiently large scale, but not necessarily\notherwise. Second, heterogeneous risk perceptions across age-groups can have\nimportant aggregate consequences. For a SARS-CoV-2 calibration of the model,\nheterogeneous risk perceptions across young and old individuals mitigate GDP\nlosses by 50% and reduce the death toll by 30% relative to a scenario in which\nall individuals have the same perceptions of risk.\n"
    },
    {
        "paper_id": 2012.04103,
        "authors": "Robin Nicole, Aleksandra Alori\\'c, Peter Sollich",
        "title": "Fragmentation in trader preferences among multiple markets: Market\n  coexistence versus single market dominance",
        "comments": "25 pages, 7 figures; minor revisions based on referee remarks\n  included; published: August 18, 2021",
        "journal-ref": "R. Soc. Open Sci. (2021) 8:202233",
        "doi": "10.1098/rsos.202233",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Technological advancement has lead to an increase in number and type of\ntrading venues and diversification of goods traded. These changes have\nre-emphasized the importance of understanding the effects of market\ncompetition: does proliferation of trading venues and increased competition\nlead to dominance of a single market or coexistence of multiple markets? In\nthis paper, we address these questions in a stylized model of Zero Intelligence\ntraders who make repeated decisions at which of three available markets to\ntrade. We analyse the model numerically and analytically and find that\nparameters that govern traders' decisions -- memory length and intensity of\nchoice, e.g. how strongly decisions are based on past success -- make the key\ndistinctions between consolidated and fragmented steady states of the\npopulation of traders. All three markets coexist with equal shares of traders\nonly when either learning is too weak and traders choose randomly, or when\nmarkets are identical. In the latter case, the population of traders is\nfragmented across the markets. For the more general case of markets with\ndifferent biases, we note that market dominance is the more typical scenario.\nThese results are interesting because previously either strong differentiation\nof markets or heterogeneity in the needs of traders was found to be a necessary\ncondition for market coexistence. We show that, in contrast, these states can\nemerge simply as a consequence of co-adaptation of an initially homogeneous\npopulation of traders.\n"
    },
    {
        "paper_id": 2012.04181,
        "authors": "Hyun Jin Jang, Kiseop Lee, Kyungsub Lee",
        "title": "Systemic Risk in Market Microstructure of Crude Oil and Gasoline Futures\n  Prices: A Hawkes Flocking Model Approach",
        "comments": null,
        "journal-ref": "Journal of Futures Markets, 40, 2020, 247-275",
        "doi": "10.1002/fut.22048",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose the Hawkes flocking model that assesses systemic risk in\nhigh-frequency processes at the two perspectives -- endogeneity and\ninteractivity. We examine the futures markets of WTI crude oil and gasoline for\nthe past decade, and perform a comparative analysis with conditional\nvalue-at-risk as a benchmark measure. In terms of high-frequency structure, we\nderive the empirical findings. The endogenous systemic risk in WTI was\nsignificantly higher than that in gasoline, and the level at which gasoline\naffects WTI was constantly higher than in the opposite case. Moreover, although\nthe relative influence's degree was asymmetric, its difference has gradually\nreduced.\n"
    },
    {
        "paper_id": 2012.04333,
        "authors": "Enayat A. Moallemi, Sibel Eker, Lei Gao, Michalis Hadjikakou, Qi Liu,\n  Jan Kwakkel, Patrick M. Reed, Michael Obersteiner, Zhaoxia Guo, and Brett A.\n  Bryan",
        "title": "Early systems change necessary for catalyzing long-term sustainability\n  in a post-2030 agenda",
        "comments": "This manuscript is the journal published version of the preprint\n  'Global pathways to sustainable development to 2030 and beyond'",
        "journal-ref": "One Earth 5 (2022) 1-20",
        "doi": "10.1016/j.oneear.2022.06.003",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Progress to-date towards the Sustainable Development Goals (SDGs) has fallen\nshort of expectations and is unlikely to fully meet 2030 targets. Despite the\nlittle chance of imminent success, past assessments have mostly focused on\nshort- and medium-term evaluations, limiting the ability to explore the\nlonger-term effects of systemic interactions with time lags and delay. Here we\nundertake global systems modelling with a longer-term view than previous\nassessments to explore the drivers of sustainability progress and how they\ncould emerge by 2030, 2050, and 2100 under different development pathways and\ntowards quantitative targets. We find that early planning for systems change to\nshift from business-as-usual to more sustainable pathways is important for\naccelerating progress towards increasingly ambitious targets by 2030, 2050, and\n2100. These findings indicate the importance of adopting longer-term timeframes\nand pathways to ensure that the necessary pre-conditions are in place for\nsustainability beyond the current 2030 Agenda.\n"
    },
    {
        "paper_id": 2012.04364,
        "authors": "Karim Barigou (SAF), Valeria Bignozzi, Andreas Tsanakas",
        "title": "Insurance valuation: A two-step generalised regression approach",
        "comments": null,
        "journal-ref": "ASTIN Bull. 52 (2022) 211-245",
        "doi": "10.1017/asb.2021.31",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Current approaches to fair valuation in insurance often follow a two-step\napproach, combining quadratic hedging with application of a risk measure on the\nresidual liability, to obtain a cost-of-capital margin. In such approaches, the\npreferences represented by the regulatory risk measure are not reflected in the\nhedging process. We address this issue by an alternative two-step hedging\nprocedure, based on generalised regression arguments, which leads to portfolios\nthat are neutral with respect to a risk measure, such as Value-at-Risk or the\nexpectile. First, a portfolio of traded assets aimed at replicating the\nliability is determined by local quadratic hedging. Second, the residual\nliability is hedged using an alternative objective function. The risk margin is\nthen defined as the cost of the capital required to hedge the residual\nliability. In the case quantile regression is used in the second step, yearly\nsolvency constraints are naturally satisfied; furthermore, the portfolio is a\nrisk minimiser among all hedging portfolios that satisfy such constraints. We\npresent a neural network algorithm for the valuation and hedging of insurance\nliabilities based on a backward iterations scheme. The algorithm is fairly\ngeneral and easily applicable, as it only requires simulated paths of risk\ndrivers.\n"
    },
    {
        "paper_id": 2012.04473,
        "authors": "Isaiah Hull, Or Sattath, Eleni Diamanti, G\\\"oran Wendin",
        "title": "Quantum Technology for Economists",
        "comments": "106 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research on quantum technology spans multiple disciplines: physics, computer\nscience, engineering, and mathematics. The objective of this manuscript is to\nprovide an accessible introduction to this emerging field for economists that\nis centered around quantum computing and quantum money. We proceed in three\nsteps. First, we discuss basic concepts in quantum computing and quantum\ncommunication, assuming knowledge of linear algebra and statistics, but not of\ncomputer science or physics. This covers fundamental topics, such as qubits,\nsuperposition, entanglement, quantum circuits, oracles, and the no-cloning\ntheorem. Second, we provide an overview of quantum money, an early invention of\nthe quantum communication literature that has recently been partially\nimplemented in an experimental setting. One form of quantum money offers the\nprivacy and anonymity of physical cash, the option to transact without the\ninvolvement of a third party, and the efficiency and convenience of a debit\ncard payment. Such features cannot be achieved in combination with any other\nform of money. Finally, we review all existing quantum speedups that have been\nidentified for algorithms used to solve and estimate economic models. This\nincludes function approximation, linear systems analysis, Monte Carlo\nsimulation, matrix inversion, principal component analysis, linear regression,\ninterpolation, numerical differentiation, and true random number generation. We\nalso discuss the difficulty of achieving quantum speedups and comment on common\nmisconceptions about what is achievable with quantum computing.\n"
    },
    {
        "paper_id": 2012.045,
        "authors": "Silvana Pesenti and Sebastian Jaimungal",
        "title": "Portfolio Optimisation within a Wasserstein Ball",
        "comments": "37 pages, 2 tables, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the problem of active portfolio management where an investor aims to\noutperform a benchmark strategy's risk profile while not deviating too far from\nit. Specifically, an investor considers alternative strategies whose terminal\nwealth lie within a Wasserstein ball surrounding a benchmark's -- being\ndistributionally close -- and that have a specified dependence/copula -- tying\nstate-by-state outcomes -- to it. The investor then chooses the alternative\nstrategy that minimises a distortion risk measure of terminal wealth. In a\ngeneral (complete) market model, we prove that an optimal dynamic strategy\nexists and provide its characterisation through the notion of isotonic\nprojections.\n  We further propose a simulation approach to calculate the optimal strategy's\nterminal wealth, making our approach applicable to a wide range of market\nmodels. Finally, we illustrate how investors with different copula and risk\npreferences invest and improve upon the benchmark using the Tail Value-at-Risk,\ninverse S-shaped, and lower- and upper-tail distortion risk measures as\nexamples. We find that investors' optimal terminal wealth distribution has\nlarger probability masses in regions that reduce their risk measure relative to\nthe benchmark while preserving the benchmark's structure.\n"
    },
    {
        "paper_id": 2012.04506,
        "authors": "Victor Olkhov",
        "title": "Business Cycles as Collective Risk Fluctuations",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We suggest use continuous numerical risk grades [0,1] of R for a single risk\nor the unit cube in Rn for n risks as the economic domain. We consider risk\nratings of economic agents as their coordinates in the economic domain.\nEconomic activity of agents, economic or other factors change agents risk\nratings and that cause motion of agents in the economic domain. Aggregations of\nvariables and transactions of individual agents in small volume of economic\ndomain establish the continuous economic media approximation that describes\ncollective variables, transactions and their flows in the economic domain as\nfunctions of risk coordinates. Any economic variable A(t,x) defines mean risk\nXA(t) as risk weighted by economic variable A(t,x). Collective flows of\neconomic variables in bounded economic domain fluctuate from secure to risky\narea and back. These fluctuations of flows cause time oscillations of\nmacroeconomic variables A(t) and their mean risks XA(t) in economic domain and\nare the origin of any business and credit cycles. We derive equations that\ndescribe evolution of collective variables, transactions and their flows in the\neconomic domain. As illustration we present simple self-consistent equations of\nsupply-demand cycles that describe fluctuations of supply, demand and their\nmean risks.\n"
    },
    {
        "paper_id": 2012.04521,
        "authors": "Nicole B\\\"auerle and Alexander Glauner",
        "title": "Minimizing Spectral Risk Measures Applied to Markov Decision Processes",
        "comments": null,
        "journal-ref": "Mathenatical Methods of Operations Research 94, 35-69, (2021)",
        "doi": "10.1007/s00186-021-00746-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the minimization of a spectral risk measure of the total discounted\ncost generated by a Markov Decision Process (MDP) over a finite or infinite\nplanning horizon. The MDP is assumed to have Borel state and action spaces and\nthe cost function may be unbounded above. The optimization problem is split\ninto two minimization problems using an infimum representation for spectral\nrisk measures. We show that the inner minimization problem can be solved as an\nordinary MDP on an extended state space and give sufficient conditions under\nwhich an optimal policy exists. Regarding the infinite dimensional outer\nminimization problem, we prove the existence of a solution and derive an\nalgorithm for its numerical approximation. Our results include the findings in\nB\\\"auerle and Ott (2011) in the special case that the risk measure is Expected\nShortfall. As an application, we present a dynamic extension of the classical\nstatic optimal reinsurance problem, where an insurance company minimizes its\ncost of capital.\n"
    },
    {
        "paper_id": 2012.04591,
        "authors": "Yuki Takahashi",
        "title": "The Role of Gender and Cognitive Skills on Other People's Generosity",
        "comments": "This draft is still preliminary; future versions can differ\n  substantially from this version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cognitive skills are an important personal attribute that affects career\nsuccess. However, colleagues' support is also vital as most works are done in\ngroups, and the degree of their support is influenced by their generosity.\nSocial norms enter in groups, and gender may interact with cognitive skills\nthrough gender norms in society. Because these gender norms penalize women with\nhigh potential, they can reduce colleagues' generosity towards these women.\nUsing a novel experimental design where I exogenously vary gender and cognitive\nskills and sufficiently powered analysis, I find neither the two attributes nor\ntheir interactions affect other people's generosity; if anything, people are\nmore generous to women with high potential. I argue that my findings have\nimplications for the role of gender norms in labor markets.\n"
    },
    {
        "paper_id": 2012.04908,
        "authors": "Giovanni Abramo, Ciriaco Andrea D'Angelo, Flavia Di Costa",
        "title": "The relative impact of private research on scientific advancement",
        "comments": "28 pages, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Literature about the scholarly impact of scientific research offers very few\ncontributions on private sector research, and the comparison with public\nsector. In this work, we try to fill this gap examining the citation-based\nimpact of Italian 2010-2017 publications distinguishing authorship by the\nprivate sector from the public sector. In particular, we investigate the\nrelation between different forms of collaboration and impact: how intra-sector\nprivate publications compare to public, and how private-public joint\npublications compare to intra-sector extramural collaborations. Finally, we\nassess the different effect of international collaboration on private and\npublic research impact, and whether there occur differences across research\nfields.\n"
    },
    {
        "paper_id": 2012.05021,
        "authors": "Dilnoza Muslimova, Hans van Kippersluis, Cornelius A. Rietveld,\n  Stephanie von Hinke, S. Fleur W. Meddens",
        "title": "Nature-nurture interplay in educational attainment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper shows how nature (i.e., one's genetic endowments) and nurture\n(i.e., one's environment) interact in producing educational attainment. Genetic\nendowments are measured using a polygenic score for educational attainment,\nwhile we use birth order as an important environmental determinant of\neducational attainment. Since genetic endowments are randomly assigned\nwithin-families and orthogonal to one's birth order, our family fixed effects\napproach exploits exogenous variation in genetic endowments as well as\nenvironments. We find that those with higher genetic endowments benefit\ndisproportionally more from being firstborn compared to those with lower\ngenetic endowments.\n"
    },
    {
        "paper_id": 2012.05088,
        "authors": "Apostolos Chalkis, Emmanouil Christoforou, Ioannis Z. Emiris and\n  Theodore Dalamagas",
        "title": "Modeling asset allocation strategies and a new portfolio performance\n  score",
        "comments": "36 pages, 4 Figures, 8 Tables",
        "journal-ref": null,
        "doi": "10.1007/s42521-021-00040-8",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We discuss and extend a powerful, geometric framework to represent the set of\nportfolios, which identifies the space of asset allocations with the points\nlying in a convex polytope. Based on this viewpoint, we survey certain\nstate-of-the-art tools from geometric and statistical computing in order to\nhandle important and difficult problems in digital finance. Although our tools\nare quite general, in this paper we focus on two specific questions.\n  The first concerns crisis detection, which is of prime interest for the\npublic in general and for policy makers in particular because of the\nsignificant impact that crises have on the economy. Certain features in stock\nmarkets lead to this type of anomaly detection: Given the assets' returns, we\ndescribe the relationship between portfolios' return and volatility by means of\na copula, without making any assumption on investor strategies. We examine a\nrecent method relying on copulae to construct an appropriate indicator that\nallows us to automate crisis detection. On real data, the indicator detects all\npast crashes in the cryptocurrency market, whereas from the DJ600-Europe index,\nfrom 1990 to 2008, the indicator identifies correctly 4 crises and issues one\nfalse positive for which we offer an explanation.\n  Our second contribution is to introduce an original computational framework\nto model asset allocation strategies, which is of independent interest for\ndigital finance and its applications. Our approach addresses the crucial\nquestion of evaluating portfolio management, and is relevant to individual\nmanagers as well as financial institutions. To evaluate portfolio performance,\nwe provide a new portfolio score, based on the aforementioned framework and\nconcepts. In particular, our score relies on the statistical properties of\nportfolios, and we show how they can be computed efficiently.\n"
    },
    {
        "paper_id": 2012.05202,
        "authors": "Th\\'eo Dessertaine, Jos\\'e Moran, Michael Benzaquen, Jean-Philippe\n  Bouchaud",
        "title": "Out-of-Equilibrium Dynamics and Excess Volatility in Firm Networks",
        "comments": "59 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the conditions under which input-output networks can dynamically\nattain a competitive equilibrium, where markets clear and profits are zero. We\nendow a classical firm network model with minimal dynamical rules that reduce\nsupply/demand imbalances and excess profits. We show that the time needed to\nreach equilibrium diverges to infinity as the system approaches an instability\npoint beyond which the Hawkins-Simons condition is violated and competitive\nequilibrium is no longer admissible. We argue that such slow dynamics is a\nsource of excess volatility, through accumulation and amplification of\nexogenous shocks. Factoring in essential physical constraints absent in our\nminimal model, such as causality or inventory management, we then propose a\ndynamically consistent model that displays a rich variety of phenomena.\nCompetitive equilibrium can only be reached after some time and within some\nrestricted region of parameter space, outside of which one observes spontaneous\nperiodic and chaotic dynamics, reminiscent of real business cycles. This\nsuggests an alternative explanation of excess volatility in terms of purely\nendogenous fluctuations. Diminishing return to scale and increased\nperishability of goods are found to ease convergence towards equilibrium.\n"
    },
    {
        "paper_id": 2012.05219,
        "authors": "Fabio Bellini, Tolulope Fadina, Ruodu Wang, Yunran Wei",
        "title": "Parametric measures of variability induced by risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general framework for a comparative theory of variability\nmeasures, with a particular focus on the recently introduced one-parameter\nfamilies of inter-Expected Shortfall differences and inter-expectile\ndifferences, that are explored in detail and compared with the widely known and\napplied inter-quantile differences. From the mathematical point of view, our\nmain result is a characterization of symmetric and comonotonic variability\nmeasures as mixtures of inter-Expected Shortfall differences, under a few\nadditional technical conditions. Further, we study the stochastic orders\ninduced by the pointwise comparison of inter-Expected Shortfall and\ninter-expectile differences, and discuss their relationship with the dilation\norder. From the statistical point of view, we establish asymptotic consistency\nand normality of the natural estimators and provide a rule of the thumb for\ncross-comparisons. Finally, we study the empirical behaviour of the considered\nclasses of variability measures on the S&P 500 Index under various economic\nregimes, and explore the comparability of different time series according to\nthe introduced stochastic orders.\n"
    },
    {
        "paper_id": 2012.05237,
        "authors": "Rene Carmona",
        "title": "Applications of Mean Field Games in Financial Engineering and Economic\n  Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This is an expanded version of the lecture given at the AMS Short Course on\nMean Field Games, on January 13, 2020 in Denver CO. The assignment was to\ndiscuss applications of Mean Field Games in finance and economics. I need to\nadmit upfront that several of the examples reviewed in this chapter were\nalready discussed in book form. Still, they are here accompanied with\ndiscussions of, and references to, works which appeared over the last three\nyears. Moreover, several completely new sections are added to show how recent\ndevelopments in financial engineering and economics can benefit from being\nviewed through the lens of the Mean Field Game paradigm. The new financial\nengineering applications deal with bitcoin mining and the energy markets, while\nthe new economic applications concern models offering a smooth transition\nbetween macro-economics and finance, and contract theory.\n"
    },
    {
        "paper_id": 2012.05519,
        "authors": "Liyang Han, Jalal Kazempour, Pierre Pinson",
        "title": "Monetizing Customer Load Data for an Energy Retailer: A Cooperative Game\n  Approach",
        "comments": "6 pages, 6 figures, accepted to and presented at the 14th IEEE\n  PowerTech 2021 Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  When energy customers schedule loads ahead of time, this information, if\nacquired by their energy retailer, can improve the retailer's load forecasts.\nBetter forecasts lead to wholesale purchase decisions that are likely to result\nin lower energy imbalance costs, and thus higher profits for the retailer.\nTherefore, this paper monetizes the value of the customer schedulable load data\nby quantifying the retailer's profit gain from adjusting the wholesale purchase\nbased on such data. Using a cooperative game theoretic approach, the retailer\ntranslates their increased profit in expectation into the value of cooperation,\nand redistributes a portion of it among the customers as monetary incentives\nfor them to continue providing their load data. Through case studies, this\npaper demonstrates the significance of the additional profit for the retailer\nfrom using the proposed framework, and evaluates the long-term monetary\nbenefits to the customers based on different payoff allocation methods.\n"
    },
    {
        "paper_id": 2012.05564,
        "authors": "Lucian-Ionut Gavrila and Alexandru Popa",
        "title": "A novel algorithm for clearing financial obligations between companies\n  -- an application within the Romanian Ministry of Economy",
        "comments": "13 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The concept of clearing or netting, as defined in the glossaries of European\nCentral Bank, has a great impact on the economy of a country influencing the\nexchanges and the interactions between companies. On short, netting refers to\nan alternative to the usual way in which the companies make the payments to\neach other: it is an agreement in which each party sets off amounts it owes\nagainst amounts owed to it. Based on the amounts two or more parties owe\nbetween them, the payment is substituted by a direct settlement. In this paper\nwe introduce a set of graph algorithms which provide optimal netting solutions\nfor the scale of a country economy. The set of algorithms computes results in\nan efficient time and is tested on invoice data provided by the Romanian\nMinistry of Economy. Our results show that classical graph algorithms are still\ncapable of solving very important modern problems.\n"
    },
    {
        "paper_id": 2012.05757,
        "authors": "Vincent Tan, Stefan Zohren",
        "title": "Estimation of Large Financial Covariances: A Cross-Validation Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel covariance estimator for portfolio selection that adapts\nto the non-stationary or persistent heteroskedastic environments of financial\ntime series by employing exponentially weighted averages and nonlinearly\nshrinking the sample eigenvalues through cross-validation. Our estimator is\nstructure agnostic, transparent, and computationally feasible in large\ndimensions. By correcting the biases in the sample eigenvalues and aligning our\nestimator to more recent risk, we demonstrate that our estimator performs well\nin large dimensions against existing state-of-the-art static and dynamic\ncovariance shrinkage estimators through simulations and with an empirical\napplication in active portfolio management.\n"
    },
    {
        "paper_id": 2012.05906,
        "authors": "Justina Deveikyte, Helyette Geman, Carlo Piccari, Alessandro Provetti",
        "title": "A Sentiment Analysis Approach to the Prediction of Market Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction and quantification of future volatility and returns play an\nimportant role in financial modelling, both in portfolio optimization and risk\nmanagement. Natural language processing today allows to process news and social\nmedia comments to detect signals of investors' confidence. We have explored the\nrelationship between sentiment extracted from financial news and tweets and\nFTSE100 movements. We investigated the strength of the correlation between\nsentiment measures on a given day and market volatility and returns observed\nthe next day. The findings suggest that there is evidence of correlation\nbetween sentiment and stock market movements: the sentiment captured from news\nheadlines could be used as a signal to predict market returns; the same does\nnot apply for volatility. Also, in a surprising finding, for the sentiment\nfound in Twitter comments we obtained a correlation coefficient of -0.7, and\np-value below 0.05, which indicates a strong negative correlation between\npositive sentiment captured from the tweets on a given day and the volatility\nobserved the next day. We developed an accurate classifier for the prediction\nof market volatility in response to the arrival of new information by deploying\ntopic modelling, based on Latent Dirichlet Allocation, to extract feature\nvectors from a collection of tweets and financial news. The obtained features\nwere used as additional input to the classifier. Thanks to the combination of\nsentiment and topic modelling our classifier achieved a directional prediction\naccuracy for volatility of 63%.\n"
    },
    {
        "paper_id": 2012.06173,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat",
        "title": "Portfolio optimization with two quasiconvex risk measures",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a static portfolio optimization problem with two risk measures: a\nprinciple risk measure in the objective function and a secondary risk measure\nwhose value is controlled in the constraints. This problem is of interest when\nit is necessary to consider the risk preferences of two parties, such as a\nportfolio manager and a regulator, at the same time. A special case of this\nproblem where the risk measures are assumed to be coherent (positively\nhomogeneous) is studied recently in a joint work of the author. The present\npaper extends the analysis to a more general setting by assuming that the two\nrisk measures are only quasiconvex. First, we study the case where the\nprincipal risk measure is convex. We introduce a dual problem, show that there\nis zero duality gap between the portfolio optimization problem and the dual\nproblem, and finally identify a condition under which the Lagrange multiplier\nassociated to the dual problem at optimality gives an optimal portfolio. Next,\nwe study the general case without the convexity assumption and show that an\napproximately optimal solution with prescribed optimality gap can be achieved\nby using the well-known bisection algorithm combined with a duality result that\nwe prove.\n"
    },
    {
        "paper_id": 2012.06192,
        "authors": "Mudit Kapoor, Shamika Ravi",
        "title": "Bihar Assembly Elections 2020: An Analysis",
        "comments": "15 pages, 5 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyse the Bihar assembly elections of 2020, and find that poverty was\nthe key driving factor, over and above female voters as determinants. The\nresults show that the poor were more likely to support the NDA. The relevance\nof this result for an election held in the midst of a pandemic, is very\ncrucial, given that the poor were the hardest hit. Secondly, in contrast to\nconventional commentary, the empirical results show that the AIMIM-factor and\nthe LJP-factor hurt the NDA while benefitting the MGB, with their presence in\nthese elections. The methodological novelty in this paper is combining\nelections data with wealth index data to study the effect of poverty on\nelections outcomes.\n"
    },
    {
        "paper_id": 2012.06211,
        "authors": "Kathrin Glau and Linus Wunderlich",
        "title": "The Deep Parametric PDE Method: Application to Option Pricing",
        "comments": "Some examples can be reproduced in our Jupyter Notebook:\n  https://github.com/LWunderlich/DeepPDE/blob/main/TwoAssetsExample/DeepParametricPDEExample.ipynb",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the deep parametric PDE method to solve high-dimensional\nparametric partial differential equations. A single neural network approximates\nthe solution of a whole family of PDEs after being trained without the need of\nsample solutions. As a practical application, we compute option prices in the\nmultivariate Black-Scholes model. After a single training phase, the prices for\ndifferent time, state and model parameters are available in milliseconds. We\nevaluate the accuracy in the price and a generalisation of the implied\nvolatility with examples of up to 25 dimensions. A comparison with alternative\nmachine learning approaches, confirms the effectiveness of the approach.\n"
    },
    {
        "paper_id": 2012.06283,
        "authors": "Dong An, Noah Linden, Jin-Peng Liu, Ashley Montanaro, Changpeng Shao,\n  Jiasu Wang",
        "title": "Quantum-accelerated multilevel Monte Carlo methods for stochastic\n  differential equations in mathematical finance",
        "comments": "37 pages, 6 figures",
        "journal-ref": "Quantum 5, 481 (2021)",
        "doi": "10.22331/q-2021-06-24-481",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inspired by recent progress in quantum algorithms for ordinary and partial\ndifferential equations, we study quantum algorithms for stochastic differential\nequations (SDEs). Firstly we provide a quantum algorithm that gives a quadratic\nspeed-up for multilevel Monte Carlo methods in a general setting. As\napplications, we apply it to compute expectation values determined by classical\nsolutions of SDEs, with improved dependence on precision. We demonstrate the\nuse of this algorithm in a variety of applications arising in mathematical\nfinance, such as the Black-Scholes and Local Volatility models, and Greeks. We\nalso provide a quantum algorithm based on sublinear binomial sampling for the\nbinomial option pricing model with the same improvement.\n"
    },
    {
        "paper_id": 2012.06325,
        "authors": "Le Trung Hieu",
        "title": "Deep Reinforcement Learning for Stock Portfolio Optimization",
        "comments": null,
        "journal-ref": "International Journal of Modeling and Optimization vol. 10, no. 5,\n  pp. 139-144, 2020",
        "doi": "10.7763/IJMO.2020.V10.761",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Stock portfolio optimization is the process of constant re-distribution of\nmoney to a pool of various stocks. In this paper, we will formulate the problem\nsuch that we can apply Reinforcement Learning for the task properly. To\nmaintain a realistic assumption about the market, we will incorporate\ntransaction cost and risk factor into the state as well. On top of that, we\nwill apply various state-of-the-art Deep Reinforcement Learning algorithms for\ncomparison. Since the action space is continuous, the realistic formulation\nwere tested under a family of state-of-the-art continuous policy gradients\nalgorithms: Deep Deterministic Policy Gradient (DDPG), Generalized\nDeterministic Policy Gradient (GDPG) and Proximal Policy Optimization (PPO),\nwhere the former two perform much better than the last one. Next, we will\npresent the end-to-end solution for the task with Minimum Variance Portfolio\nTheory for stock subset selection, and Wavelet Transform for extracting\nmulti-frequency data pattern. Observations and hypothesis were discussed about\nthe results, as well as possible future research directions.1\n"
    },
    {
        "paper_id": 2012.06573,
        "authors": "Alexis Marchal",
        "title": "Risk & returns around FOMC press conferences: a novel perspective from\n  computer vision",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I propose a new tool to characterize the resolution of uncertainty around\nFOMC press conferences. It relies on the construction of a measure capturing\nthe level of discussion complexity between the Fed Chair and reporters during\nthe Q&A sessions. I show that complex discussions are associated with higher\nequity returns and a drop in realized volatility. The method creates an\nattention score by quantifying how much the Chair needs to rely on reading\ninternal documents to be able to answer a question. This is accomplished by\nbuilding a novel dataset of video images of the press conferences and\nleveraging recent deep learning algorithms from computer vision. This\nalternative data provides new information on nonverbal communication that\ncannot be extracted from the widely analyzed FOMC transcripts. This paper can\nbe seen as a proof of concept that certain videos contain valuable information\nfor the study of financial markets.\n"
    },
    {
        "paper_id": 2012.06645,
        "authors": "Manuel Lopez Galvan",
        "title": "An approximate closed formula for European Mortgage Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to investigate the use of close formula\napproximation for pricing European mortgage options. Under the assumption of\nlogistic duration and normal mortgage rates the underlying price at the option\nexpiry is approximated by shifted lognormal or regular lognormal distribution\nby matching moments. Once the price function is approximated by lognormal\ndistributions, the option price can be computed directly as an integration of\nthe distribution function over the payoff at the option expiry by using\nBlack-Scholes-Merton close formula. We will see that lower curvature levels\ncorrespond to positively skewness price distributions and in this case\nlognormal approximation leads to close parametric formula representation in\nterms of all model parameters. The proposed methodologies are tested against\nMonte Carlo approach under different market and contract parameters and the\ntests confirmed that the close form approximation have a very good accuracy.\n"
    },
    {
        "paper_id": 2012.06703,
        "authors": "Zhuo Jin, Zuo Quan Xu, and Bin Zou",
        "title": "A Perturbation Approach to Optimal Investment, Liability Ratio, and\n  Dividend Strategies",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal dividend problem for an insurer who simultaneously\ncontrols investment weights in a financial market, liability ratio in the\ninsurance business, and dividend payout rate. The insurer seeks an optimal\nstrategy to maximize her expected utility of dividend payments over an infinite\nhorizon. By applying a perturbation approach, we obtain the optimal strategy\nand the value function in closed form for log and power utility. We conduct an\neconomic analysis to investigate the impact of various model parameters and\nrisk aversion on the insurer's optimal strategy.\n"
    },
    {
        "paper_id": 2012.06716,
        "authors": "Gyu Hyun Kim",
        "title": "Non-fundamental Home Bias in International Equity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the relationship of the equity home bias with 1) the\ncountry-level behavioral unfamiliarity, and 2) the home-foreign return\ncorrelation. We set the hypotheses that 1) unfamiliarity about foreign equities\nplays a role in the portfolio set up and 2) the correlation of return on home\nand foreign equities affects the equity home bias when there is a lack of\ninformation about foreign equities. For the empirical analysis, the proportion\nof respondents to the question \"How much do you trust? - People you meet for\nthe first time\" is used as a proxy measure for country-specific unfamiliarity.\nBased on the eleven developed countries for which such data are available, we\nimplement a feasible generalized linear squares (FGLS) method. Empirical\nresults suggest that country-specific unfamiliarity has a significant and\npositive correlation with the equity home bias. When it comes to the\ncorrelation of return between home and foreign equities, we identify that there\nis a negative correlation with the equity home bias, which is against our\nhypothesis. Moreover, an excess return on home equities compared to foreign\nones is found to have a positive correlation with the equity home bias, which\nis consistent with the comparative statics only if foreign investors have a\nsufficiently higher risk aversion than domestic investors. We check the\nrobustness of our empirical analysis by fitting alternative specifications and\nuse a log-transformed measure of the equity home bias, resulting in consistent\nresults with ones with the original measure.\n"
    },
    {
        "paper_id": 2012.06742,
        "authors": "Ruda Zhang and Roger Ghanem",
        "title": "Multi-market Oligopoly of Equal Capacity",
        "comments": "arXiv admin note: text overlap with arXiv:2008.10775",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a variant of Cournot competition, where multiple firms allocate\nthe same amount of resource across multiple markets. We prove that the game has\na unique pure-strategy Nash equilibrium (NE), which is symmetric and is\ncharacterized by the maximal point of a \"potential function\". The NE is\nglobally asymptotically stable under the gradient adjustment process, and is\nnot socially optimal in general. An application is in transportation, where\ndrivers allocate time over a street network.\n"
    },
    {
        "paper_id": 2012.06751,
        "authors": "Guangyan Jia, Jianming Xia, Rongjie Zhao",
        "title": "Monetary Risk Measures",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study general monetary risk measures (without any convexity\nor weak convexity). A monetary (respectively, positively homogeneous) risk\nmeasure can be characterized as the lower envelope of a family of convex\n(respectively, coherent) risk measures. The proof does not depend on but easily\nleads to the classical representation theorems for convex and coherent risk\nmeasures. When the law-invariance and the SSD (second-order stochastic\ndominance)-consistency are involved, it is not the convexity (respectively,\ncoherence) but the comonotonic convexity (respectively, comonotonic coherence)\nof risk measures that can be used for such kind of lower envelope\ncharacterizations in a unified form. The representation of a law-invariant risk\nmeasure in terms of VaR is provided.\n"
    },
    {
        "paper_id": 2012.06856,
        "authors": "Ioannis P. Antoniades, Leonidas P. Karakatsanis, Evgenios G. Pavlos",
        "title": "Dynamical Characteristics of Global Stock Markets Based on Time\n  Dependent Tsallis Non-Extensive Statistics and Generalized Hurst Exponents",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126121",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform non-linear analysis on stock market indices using time-dependent\nextended Tsallis statistics. Specifically, we evaluate the q-triplet for\nparticular time periods with the purpose of demonstrating the temporal\ndependence of the extended characteristics of the underlying market dynamics.\nWe apply the analysis on daily close price timeseries of four major global\nmarkets (S&P 500, Tokyo-NIKKEI, Frankfurt-DAX, London-LSE). For comparison, we\nalso compute time-dependent Generalized Hurst Exponents (GHE) Hq using the GHE\nmethod, thus estimating the temporal evolution of the multiscaling\ncharacteristics of the index dynamics. We focus on periods before and after\ncritical market events such as stock market bubbles (2000 dot.com bubble,\nJapanese 1990 bubble, 2008 US real estate crisis) and find that the temporal\ntrends of q-triplet values significantly differ among these periods indicating\nthat in the rising period before a bubble break, the underlying extended\nstatistics of the market dynamics strongly deviates from purely stochastic\nbehavior, whereas, after the breakdown, it gradually converges to the\nGaussian-like behavior which is a characteristic of an efficient market. We\nalso conclude that relative temporal variation patterns of the Tsallis\nq-triplet can be connected to different aspects of market dynamics and reveals\nuseful information about market conditions especially those underlying the\ndevelopment of a stock market bubble. We found specific temporal patterns and\ntrends in the relative variation of the indices in the q-triplet that\ndistinguish periods just before and just after a stock-market bubble break.\nDifferences between endogenous and exogenous stock market crises are also\ncaptured by the temporal changes in the Tsallis q-triplet. Finally, we\nintroduce two new time-dependent empirical metrics (Q-metrics) that are\nfunctions of the Tsallis q-triplet.\n"
    },
    {
        "paper_id": 2012.07008,
        "authors": "Xuejian Wang",
        "title": "Product Differentiation and Geographical Expansion of Exports Network at\n  Industry level",
        "comments": "34 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Industries can enter one country first, and then enter its neighbors'\nmarkets. Firms in the industry can expand trade network through the export\nbehavior of other firms in the industry. If a firm is dependent on a few\nforeign markets, the political risks of the markets will hurt the firm. The\nfrequent trade disputes reflect the importance of the choice of export\ndestinations. Although the market diversification strategy was proposed before,\nmost firms still focus on a few markets, and the paper shows reasons.In this\npaper, we assume the entry cost of firms is not all sunk cost, and show 2 ways\nthat product heterogeneity impacts extensive margin of exports theoretically\nand empirically. Firstly, the increase in product heterogeneity promotes the\nincrease in market power and profit, and more firms are able to pay the entry\ncost. If more firms enter the market, the information of the market will be\nknown by other firms in the industry. Firms can adjust their behavior according\nto other firms, so the information changes entry cost and is not sunk cost\ncompletely. The information makes firms more likely to entry the market, and\nenter the surrounding markets of existing markets of other firms in the\nindustry. When firms choose new markets, they tend to enter the markets with\nfew competitors first.Meanwhile, product heterogeneity will directly affect the\nfirms' network expansion, and the reduction of product heterogeneity will\nincrease the value of peer information. This makes firms more likely to entry\nthe market, and firms in the industry concentrate on the markets.\n"
    },
    {
        "paper_id": 2012.07027,
        "authors": "Xuejian Wang",
        "title": "Impact of Regional Reactions to War on Contemporary Chinese Trade",
        "comments": "29 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Different regional reactions to war in 1894 and 1900 can significantly impact\nChinese imports in 2001. As international relationship gets tense and China\nrises, international conflicts could decrease trade.We analyze impact of\nhistoric political conflict. We measure regional change of number of people\npassing imperial exam because of war. War leads to an unsuccessful reform and\nshocks elites. Elites in different regions have different ideas about\nmodernization, and the change of number of people passing exam is quite\ndifferent in different regions after war. Regional number of people passing\nexam increases 1% after war, imports from then empires decrease 2.050% in 2001,\nand this shows impact of cultural barrier. Manufactured goods can be impacted\nbecause brands can be identified easily. Risk aversion of expensive products in\nconservative regions can increase imports of equipment. Value chains need deep\ntrust, and this decreases imports of foreign company and assembly trade.\n"
    },
    {
        "paper_id": 2012.07149,
        "authors": "Daniel Poh, Bryan Lim, Stefan Zohren and Stephen Roberts",
        "title": "Building Cross-Sectional Systematic Strategies By Learning to Rank",
        "comments": "12 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The success of a cross-sectional systematic strategy depends critically on\naccurately ranking assets prior to portfolio construction. Contemporary\ntechniques perform this ranking step either with simple heuristics or by\nsorting outputs from standard regression or classification models, which have\nbeen demonstrated to be sub-optimal for ranking in other domains (e.g.\ninformation retrieval). To address this deficiency, we propose a framework to\nenhance cross-sectional portfolios by incorporating learning-to-rank\nalgorithms, which lead to improvements of ranking accuracy by learning pairwise\nand listwise structures across instruments. Using cross-sectional momentum as a\ndemonstrative case study, we show that the use of modern machine learning\nranking algorithms can substantially improve the trading performance of\ncross-sectional strategies -- providing approximately threefold boosting of\nSharpe Ratios compared to traditional approaches.\n"
    },
    {
        "paper_id": 2012.07238,
        "authors": "Yingkai Li, Harry Pei",
        "title": "Misspecified Beliefs about Time Lags",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the long-term behavior of a Bayesian agent who has a misspecified\nbelief about the time lag between actions and feedback, and learns about the\npayoff consequences of his actions over time. Misspecified beliefs about time\nlags result in attribution errors, which have no long-term effect when the\nagent's action converges, but can lead to arbitrarily large long-term\ninefficiencies when his action cycles. Our proof uses concentration\ninequalities to bound the frequency of action switches, which are useful to\nstudy learning problems with history dependence. We apply our methods to study\na policy choice game between a policy-maker who has a correctly specified\nbelief about the time lag and the public who has a misspecified belief.\n"
    },
    {
        "paper_id": 2012.07245,
        "authors": "Kentaro Imajo and Kentaro Minami and Katsuya Ito and Kei Nakagawa",
        "title": "Deep Portfolio Optimization via Distributional Prediction of Residual\n  Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent developments in deep learning techniques have motivated intensive\nresearch in machine learning-aided stock trading strategies. However, since the\nfinancial market has a highly non-stationary nature hindering the application\nof typical data-hungry machine learning methods, leveraging financial inductive\nbiases is important to ensure better sample efficiency and robustness. In this\nstudy, we propose a novel method of constructing a portfolio based on\npredicting the distribution of a financial quantity called residual factors,\nwhich is known to be generally useful for hedging the risk exposure to common\nmarket factors. The key technical ingredients are twofold. First, we introduce\na computationally efficient extraction method for the residual information,\nwhich can be easily combined with various prediction algorithms. Second, we\npropose a novel neural network architecture that allows us to incorporate\nwidely acknowledged financial inductive biases such as amplitude invariance and\ntime-scale invariance. We demonstrate the efficacy of our method on U.S. and\nJapanese stock market data. Through ablation experiments, we also verify that\neach individual technique contributes to improving the performance of trading\nstrategies. We anticipate our techniques may have wide applications in various\nfinancial problems.\n"
    },
    {
        "paper_id": 2012.07368,
        "authors": "Hezhi Luo, Yuanyuan Chen, Xianye Zhang, Duan Li (deceased), Huixian Wu",
        "title": "Effective Algorithms for Optimal Portfolio Deleveraging Problem with\n  Cross Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the optimal portfolio deleveraging (OPD) problem with\npermanent and temporary price impacts, where the objective is to maximize\nequity while meeting a prescribed debt/equity requirement. We take the real\nsituation with cross impact among different assets into consideration. The\nresulting problem is, however, a non-convex quadratic program with a quadratic\nconstraint and a box constraint, which is known to be NP-hard. In this paper,\nwe first develop a successive convex optimization (SCO) approach for solving\nthe OPD problem and show that the SCO algorithm converges to a KKT point of its\ntransformed problem. Second, we propose an effective global algorithm for the\nOPD problem, which integrates the SCO method, simple convex relaxation and a\nbranch-and-bound framework, to identify a global optimal solution to the OPD\nproblem within a pre-specified $\\epsilon$-tolerance. We establish the global\nconvergence of our algorithm and estimate its complexity. We also conduct\nnumerical experiments to demonstrate the effectiveness of our proposed\nalgorithms with both the real data and the randomly generated medium- and\nlarge-scale OPD problem instances.\n"
    },
    {
        "paper_id": 2012.0744,
        "authors": "Mariano Zeron, Ignacio Ruiz",
        "title": "Tensoring volatility calibration",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by a series of remarkable papers in recent years that use Deep\nNeural Nets to substantially speed up the calibration of pricing models, we\ninvestigate the use of Chebyshev Tensors instead of Deep Neural Nets. Given\nthat Chebyshev Tensors can be, under certain circumstances, more efficient than\nDeep Neural Nets at exploring the input space of the function to be\napproximated, due to their exponential convergence, the problem of calibration\nof pricing models seems, a priori, a good case where Chebyshev Tensors can\nexcel.\n  In this piece of research, we built Chebyshev Tensors, either directly or\nwith the help of the Tensor Extension Algorithms, to tackle the computational\nbottleneck associated with the calibration of the rough Bergomi volatility\nmodel. Results are encouraging as the accuracy of model calibration via\nChebyshev Tensors is similar to that when using Deep Neural Nets, but with\nbuilding efforts that range between 5 and 100 times more efficient in the\nexperiments run. Our tests indicate that when using Chebyshev Tensors, the\ncalibration of the rough Bergomi volatility model is around 40,000 times more\nefficient than if calibrated via brute-force (using the pricing function).\n"
    },
    {
        "paper_id": 2012.07509,
        "authors": "Jianming Xia",
        "title": "Decision Making under Uncertainty: A Game of Two Selves",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we characterize the niveloidal preferences that satisfy the\nWeak Order, Monotonicity, Archimedean, and Weak C-Independence Axioms from the\npoint of view of an intra-personal, leader-follower game. We also show that the\nleader's strategy space can serve as an ambiguity aversion index.\n"
    },
    {
        "paper_id": 2012.07669,
        "authors": "Curtis Atkisson, Monique Borgerhoff Mulder",
        "title": "The structure of multiplex networks predicts play in economic games and\n  real-world cooperation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Explaining why humans cooperate in anonymous contexts is a major goal of\nhuman behavioral ecology, cultural evolution, and related fields. What predicts\ncooperation in anonymous contexts is inconsistent across populations, levels of\nanalysis, and games. For instance, market integration is a key predictor across\nethnolinguistic groups but has inconsistent predictive power at the individual\nlevel. We adapt an idea from 19th-century sociology: people in societies with\ngreater overlap in ties across domains among community members (Durkheim's\n\"mechanical\" solidarity) will cooperate more with their network partners and\nless in anonymous contexts than people in societies with less overlap\n(\"organic\" solidarity). This hypothesis, which can be tested at the individual\nand community level, assumes that these two types of societies differ in the\nimportance of keeping existing relationships as opposed to recruiting new\npartners. Using multiplex networks, we test this idea by comparing cooperative\ntendencies in both anonymous experimental games and real-life communal labor\ntasks across 9 Makushi villages in Guyana that vary in the degree of\nwithin-village overlap. Average overlap in a village predicts both real-world\ncooperative and anonymous interactions in the predicted direction; individual\noverlap also has effects in the expected direction. These results reveal a\nconsistent patterning of cooperative tendencies at both individual and local\nlevels and contribute to the debate over the emergence of norms for cooperation\namong humans. Multiplex overlap can help us understand inconsistencies in\nprevious studies of cooperation in anonymous contexts and is an unexplored\ndimension with explanatory power at multiple levels of analysis.\n"
    },
    {
        "paper_id": 2012.07787,
        "authors": "Varanya Chaubey",
        "title": "Treating Research Writing: Symptoms and Maladies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article is a response to a question many economists ask: how can I\nimprove my first draft? The first section addresses a common approach to doing\nthis: treating problems visible on the surface. This paper presents six such\nsymptoms along with treatments for them. The second section addresses another\napproach, one that often turns out to be more effective: looking deeper for the\nunderlying malady that causes several symptoms to show up on the surface and\ntreating it. This paper presents five common maladies that matter for eventual\noutcomes, such as publishing and hiring.\n"
    },
    {
        "paper_id": 2012.07789,
        "authors": "D\\'avid Csercsik",
        "title": "Strategic bidding via the interplay of minimum income condition orders\n  in day-ahead power exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eneco.2021.105126",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study the so-called minimum income condition order, which is\nused in some day-ahead electricity power exchanges to represent the\nproduction-related costs of generating units. This order belongs to the family\nof complex orders, which imply non-convexities in the market clearing problem.\nWe demonstrate via simple numerical examples that if more of such bids are\npresent in the market, their interplay may open the possibility of strategic\nbidding. More precisely, we show that by the manipulation of bid parameters, a\nstrategic player may increase its own profit and potentially induce the\ndeactivation of an other minimum income condition order, which would be\naccepted under truthful bidding. Furthermore, we show that if we modify the\nobjective function used in the market clearing according to principles\nsuggested in the literature, it is possible to prevent the possibility of such\nstrategic bidding, but the modification raises other issues.\n"
    },
    {
        "paper_id": 2012.08002,
        "authors": "Maxim Bichuch and Zachary Feinstein",
        "title": "Endogenous inverse demand functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we present an equilibrium formulation for price impacts. This is\nmotivated by the Buhlmann equilibrium in which assets are sold into a system of\nmarket participants, e.g. a fire sale in systemic risk, and can be viewed as a\ngeneralization of the Esscher premium. Existence and uniqueness of clearing\nprices for the liquidation of a portfolio are studied. We also investigate\nother desired portfolio properties including monotonicity and concavity. Price\nper portfolio unit sold is also calculated. In special cases, we study price\nimpacts generated by market participants who follow the exponential utility and\npower utility.\n"
    },
    {
        "paper_id": 2012.0804,
        "authors": "Guillermo Angeris, Alex Evans, Tarun Chitra",
        "title": "When does the tail wag the dog? Curvature and market making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Liquidity and trading activity on constant function market makers (CFMMs)\nsuch as Uniswap, Curve, and Balancer has grown significantly in the second half\nof 2020. Much of the growth of these protocols has been driven by incentivized\npools or 'yield farming', which reward participants in crypto assets for\nproviding liquidity to CFMMs. As a result, CFMMs and associated protocols,\nwhich were historically very small markets, now constitute the most liquid\ntrading venues for a large number of crypto assets. But what does it mean for a\nCFMM to be the most liquid market? In this paper, we propose a basic definition\nof price sensitivity and liquidity. We show that this definition is tightly\nrelated to the curvature of a CFMM's trading function and can be used to\nexplain a number of heuristic results. For example, we show that low-curvature\nmarkets are good for coins whose market value is approximately fixed and that\nhigh-curvature markets are better for liquidity providers when traders have an\ninformational edge. Additionally, the results can also be used to model\ninteracting markets and explain the rise of incentivized liquidity provision,\nalso known as 'yield farming.'\n"
    },
    {
        "paper_id": 2012.08133,
        "authors": "Corrado Giulietti and Brendon McConnell",
        "title": "Kicking You When You're Already Down: The Multipronged Impact of\n  Austerity on Crime",
        "comments": "draft 3.0",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The UK Welfare Reform Act 2012 imposed a series of deep welfare cuts, which\ndisproportionately affected ex-ante poorer areas. In this paper, we provide the\nfirst evidence of the impact of these austerity measures on two different but\ncomplementary elements of crime -- the crime rate and the less-studied\nconcentration of crime -- over the period 2011-2015 in England and Wales, and\ndocument four new facts. First, areas more exposed to the welfare reforms\nexperience increased levels of crime, an effect driven by a rise in violent\ncrime. Second, both violent and property crime become more concentrated within\nan area due to the welfare reforms. Third, it is ex-ante more deprived\nneighborhoods that bear the brunt of the crime increases over this period.\nFourth, we find no evidence that the welfare reforms increased recidivism,\nsuggesting that the changes in crime we find are likely driven by new\ncriminals. Combining these results, we document unambiguous evidence of a\nnegative spillover of the welfare reforms at the heart of the UK government's\nausterity program on social welfare, which reinforced the direct\ninequality-worsening effect of this program. Guided by a hedonic house price\nmodel, we calculate the welfare effects implied by the cuts in order to provide\na financial quantification of the impact of the reform. We document an implied\nwelfare loss of the policy -- borne by the public -- that far exceeds the\nsavings made to government coffers.\n"
    },
    {
        "paper_id": 2012.08163,
        "authors": "Bahar Akhtari, Francesca Biagini, Andrea Mazzon, Katharina Oberpriller",
        "title": "Generalized Feynman-Kac Formula under volatility uncertainty",
        "comments": "35 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a generalization of a Feynmac-Kac formula under\nvolatility uncertainty in presence of a linear term in the PDE due to\ndiscounting. We state our result under different hypothesis with respect to the\nderivation given by Hu, Ji, Peng and Song (Comparison theorem, Feynman-Kac\nformula and Girsanov transformation for BSDEs driven by G-Brownian motion,\nStochastic Processes and their Application, 124 (2)), where the Lipschitz\ncontinuity of some functionals is assumed which is not necessarily satisfied in\nour setting. In particular, we show that the $G$-conditional expectation of a\ndiscounted payoff is a viscosity solution of a nonlinear PDE. In applications,\nthis permits to calculate such a sublinear expectation in a computationally\nefficient way.\n"
    },
    {
        "paper_id": 2012.08351,
        "authors": "Maria Arduca, Cosimo Munari",
        "title": "Fundamental theorem of asset pricing with acceptable risk in markets\n  with frictions",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the range of prices at which a rational agent should contemplate\ntransacting a financial contract outside a given securities market. Trading is\nsubject to nonproportional transaction costs and portfolio constraints and full\nreplication by way of market instruments is not always possible. Rationality is\ndefined in terms of consistency with market prices and acceptable risk\nthresholds. We obtain a direct and a dual description of market-consistent\nprices with acceptable risk. The dual characterization requires an appropriate\nextension of the classical Fundamental Theorem of Asset Pricing where the role\nof arbitrage opportunities is played by good deals, i.e., costless investment\nopportunities with acceptable risk-reward tradeoff. In particular, we highlight\nthe importance of scalable good deals, i.e., investment opportunities that are\ngood deals regardless of their volume.\n"
    },
    {
        "paper_id": 2012.08353,
        "authors": "Martino Grasselli, Andrea Mazzoran, Andrea Pallavicini",
        "title": "A general framework for a joint calibration of VIX and VXX options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the VIX futures market with a focus on the exchange-traded notes\nwritten on such contracts, in particular we investigate the VXX notes tracking\nthe short-end part of the futures term structure. Inspired by recent\ndevelopments in commodity smile modelling, we present a multi-factor\nstochastic-local volatility model that is able to jointly calibrate plain\nvanilla options both on VIX futures and VXX notes, thus going beyond the\nfailure of purely stochastic or simply local volatility models. We discuss\nnumerical results on real market data by highlighting the impact of model\nparameters on implied volatilities.\n"
    },
    {
        "paper_id": 2012.08355,
        "authors": "Conor Goold, Simone Pfuderer, William H. M. James, Nik Lomax, Fiona\n  Smith, Lisa M. Collins",
        "title": "A mathematical model of national-level food system sustainability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The global food system faces various endogeneous and exogeneous, biotic and\nabiotic risk factors, including a rising human population, higher population\ndensities, price volatility and climate change. Quantitative models play an\nimportant role in understanding food systems' expected responses to shocks and\nstresses. Here, we present a stylised mathematical model of a national-level\nfood system that incorporates domestic supply of a food commodity,\ninternational trade, consumer demand, and food commodity price. We derive a\ncritical compound parameter signalling when domestic supply will become\nunsustainable and the food system entirely dependent on imports, which results\nin higher commodity prices, lower consumer demand and lower inventory levels.\nUsing Bayesian estimation, we apply the dynamic food systems model to infer the\nsustainability of the UK pork industry. We find that the UK pork industry is\ncurrently sustainable but because the industry is dependent on imports to meet\ndemand, a decrease in self-sufficiency below 50% (current levels are 60-65%)\nwould lead it close to the critical boundary signalling its collapse. Our model\nprovides a theoretical foundation for future work to determine more complex\ncausal drivers of food system vulnerability.\n"
    },
    {
        "paper_id": 2012.08517,
        "authors": "Mateusz Denys",
        "title": "Model of cunning agents",
        "comments": "20 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.125987",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A numerical agent-based spin model of financial markets, based on the Potts\nmodel from statistical mechanics, with a novel interpretation of the spin\nvariable (as regards financial-market models) is presented. In this model, a\nvalue of the spin variable is only the agent's opinion concerning current\nmarket situation, which he communicates to his nearest neighbors. Instead, the\nagent's action (i.e., buying, selling, or staying inactive) is connected with a\nchange of the spin variable. Hence, the agents can be considered as cunning in\nthis model. That is, these agents encourage their neighbors to buy stocks if\nthe agents have an opportunity to sell them, and the agents encourage their\nneighbors to sell stocks if the agents have a reversed opportunity. Predictions\nof the model are in good agreement with empirical data from various real-life\nfinancial markets. The model reproduces the shape of the usual and\nabsolute-value autocorrelation function of returns as well as the distribution\nof times between superthreshold losses.\n"
    },
    {
        "paper_id": 2012.0884,
        "authors": "Kenneth Lomas and Dave Cliff",
        "title": "Exploring Narrative Economics: An Agent-Based-Modeling Platform that\n  Integrates Automated Traders with Opinion Dynamics",
        "comments": "To be presented at the 13th International Conference on Agents and\n  Artificial Intelligence (ICAART2021), Vienna, 4th--6th February 2021. 18\n  pages; 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In seeking to explain aspects of real-world economies that defy easy\nunderstanding when analysed via conventional means, Nobel Laureate Robert\nShiller has since 2017 introduced and developed the idea of Narrative\nEconomics, where observable economic factors such as the dynamics of prices in\nasset markets are explained largely as a consequence of the narratives (i.e.,\nthe stories) heard, told, and believed by participants in those markets.\nShiller argues that otherwise irrational and difficult-to-explain behaviors,\nsuch as investors participating in highly volatile cryptocurrency markets, are\nbest explained and understood in narrative terms: people invest because they\nbelieve, because they have a heartfelt opinions, about the future prospects of\nthe asset, and they tell to themselves and others stories (narratives) about\nthose beliefs and opinions. In this paper we describe what is, to the best of\nour knowledge, the first ever agent-based modelling platform that allows for\nthe study of issues in narrative economics. We have created this by integrating\nand synthesizing research in two previously separate fields: opinion dynamics\n(OD), and agent-based computational economics (ACE) in the form of\nminimally-intelligent trader-agents operating in accurately modelled financial\nmarkets. We show here for the first time how long-established models in OD and\nin ACE can be brought together to enable the experimental study of issues in\nnarrative economics, and we present initial results from our system. The\nprogram-code for our simulation platform has been released as freely-available\nopen-source software on GitHub, to enable other researchers to replicate and\nextend our work\n"
    },
    {
        "paper_id": 2012.08864,
        "authors": "Ali R. Baghirzade",
        "title": "Development of cloud, digital technologies and the introduction of chip\n  technologies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hardly any other area of research has recently attracted as much attention as\nmachine learning (ML) through the rapid advances in artificial intelligence\n(AI). This publication provides a short introduction to practical concepts and\nmethods of machine learning, problems and emerging research questions, as well\nas an overview of the participants, an overview of the application areas and\nthe socio-economic framework conditions of the research.\n  In expert circles, ML is used as a key technology for modern artificial\nintelligence techniques, which is why AI and ML are often used interchangeably,\nespecially in an economic context. Machine learning and, in particular, deep\nlearning (DL) opens up entirely new possibilities in automatic language\nprocessing, image analysis, medical diagnostics, process management and\ncustomer management. One of the important aspects in this article is\nchipization. Due to the rapid development of digitalization, the number of\napplications will continue to grow as digital technologies advance. In the\nfuture, machines will more and more provide results that are important for\ndecision making. To this end, it is important to ensure the safety, reliability\nand sufficient traceability of automated decision-making processes from the\ntechnological side. At the same time, it is necessary to ensure that ML\napplications are compatible with legal issues such as responsibility and\nliability for algorithmic decisions, as well as technically feasible. Its\nformulation and regulatory implementation is an important and complex issue\nthat requires an interdisciplinary approach. Last but not least, public\nacceptance is critical to the continued diffusion of machine learning processes\nin applications. This requires widespread public discussion and the involvement\nof various social groups.\n"
    },
    {
        "paper_id": 2012.0897,
        "authors": "Silvia de Juan, Maria Dulce Subida, Andres Ospina-Alvarez, Ainara\n  Aguilar, Miriam Fernandez",
        "title": "Disentangling the socio-ecological drivers behind illegal fishing in a\n  small-scale fishery managed by a TURF system",
        "comments": "24 pages, 6 figures, 2 tables, supplementary information",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A substantial increase in illegal extraction of the benthic resources in\ncentral Chile is likely driven by an interplay of numerous socio-economic local\nfactors that threatens the success of the fisheries management areas (MA)\nsystem. To assess this problem, the exploitation state of a commercially\nimportant benthic resource (i.e., keyhole limpet) in the MAs was related with\nsocio-economic drivers of the small-scale fisheries. The potential drivers of\nillegal extraction included rebound effect of fishing effort displacement by\nMAs, level of enforcement, distance to surveillance authorities, wave exposure\nand land-based access to the MA, and alternative economic activities in the\nfishing village. The exploitation state of limpets was assessed by the\nproportion of the catch that is below the minimum legal size, with high\nproportions indicating a poor state, and by the relative median size of limpets\nfished within the MAs in comparison with neighbouring OA areas, with larger\nrelative sizes in the MA indicating a good state. A Bayesian-Belief Network\napproach was adopted to assess the effects of potential drivers of illegal\nfishing on the status of the benthic resource in the MAs. Results evidenced the\nabsence of a direct link between the level of enforcement and the status of the\nresource, with other socio-economic (e.g., alternative economic activities in\nthe village) and context variables (e.g., fishing effort or distance to\nsurveillance authorities) playing important roles. Scenario analysis explored\nvariables that are susceptible to be managed, evidencing that BBN is a powerful\napproach to explore the role of multiple external drivers, and their impact on\nmarine resources, in complex small-scale fisheries.\n"
    },
    {
        "paper_id": 2012.09041,
        "authors": "Ricardo Cris\\'ostomo",
        "title": "Estimating real-world probabilities: A forward-looking behavioral\n  framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that disentangling sentiment-induced biases from fundamental\nexpectations significantly improves the accuracy and consistency of\nprobabilistic forecasts. Using data from 1994 to 2017, we analyze 15 stochastic\nmodels and risk-preference combinations and in all possible cases a simple\nbehavioral transformation delivers substantial forecast gains. Our results are\nrobust across different evaluation methods, risk-preference hypotheses and\nsentiment calibrations, demonstrating that behavioral effects can be\neffectively used to forecast asset prices. Further analyses confirm that our\nreal-world densities outperform densities recalibrated to avoid past mistakes\nand improve predictive models where risk aversion is dynamically estimated from\noption prices.\n"
    },
    {
        "paper_id": 2012.09082,
        "authors": "Filippo de Feo",
        "title": "The Averaging Principle for Non-autonomous Slow-fast Stochastic\n  Differential Equations and an Application to a Local Stochastic Volatility\n  Model",
        "comments": "Key words: averaging principle, slow fast, stochastic differential\n  equations, non autonomous, invariant measure, Khasminskii, ergodicity,\n  dissipativity, local stochastic volatility, local volatility, path dependent\n  derivative, path dependent options",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we study the averaging principle for non-autonomous slow-fast\nsystems of stochastic differential equations. In particular in the first part\nwe prove the averaging principle assuming the sublinearity, the Lipschitzianity\nand the Holder's continuity in time of the coefficients, an ergodic hypothesis\nand an $\\mathcal{L}^2$-bound of the fast component. In this setting we prove\nthe weak convergence of the slow component to the solution of the averaged\nequation. Moreover we provide a suitable dissipativity condition under which\nthe ergodic hypothesis and the $\\mathcal{L}^2$-bound of the fast component,\nwhich are implicit conditions, are satisfied.\n  In the second part we propose a financial application of this result: we\napply the theory developed to a slow-fast local stochastic volatility model.\nFirst we prove the weak convergence of the model to a local volatility one.\nThen under a risk neutral measure we show that the prices of the derivatives,\npossibly path-dependent, converge to the ones calculated using the limit model.\n"
    },
    {
        "paper_id": 2012.09113,
        "authors": "Shteryo Nozharov",
        "title": "Economic dimension of crimes against cultural-historical and\n  archaeological heritage (EN)",
        "comments": "This study was published for the first time in 2015 only in\n  Bulgarian. The current version is its first English translation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The publication is one of the first studies of its kind, devoted to the\neconomic dimension of crimes against cultural and archaeological heritage. Lack\nof research in this area is largely due to irregular global prevalence vague\ndefinition of economic value of the damage these crimes cause to the society at\nnational and global level, to present and future generations. The author uses\nclassical models of Becker and Freeman, by modifying and complementing them\nwith the tools of economics of culture based on the values of non-use. The\nmodel tries to determine the opportunity costs of this type of crime in several\nscenarios and based on this to determine the extent of their limitation at an\naffordable cost to society and raising public benefits of conservation of World\nand National Heritage.\n"
    },
    {
        "paper_id": 2012.09115,
        "authors": "Vladimir Vargas-Calder\\'on and Jorge E. Camargo",
        "title": "Towards robust and speculation-reduction real estate pricing models\n  based on a data-driven strategy",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/01605682.2021.2023672",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In many countries, real estate appraisal is based on conventional methods\nthat rely on appraisers' abilities to collect data, interpret it and model the\nprice of a real estate property. With the increasing use of real estate online\nplatforms and the large amount of information found therein, there exists the\npossibility of overcoming many drawbacks of conventional pricing models such as\nsubjectivity, cost, unfairness, among others. In this paper we propose a\ndata-driven real estate pricing model based on machine learning methods to\nestimate prices reducing human bias. We test the model with 178,865 flats\nlistings from Bogot\\'a, collected from 2016 to 2020. Results show that the\nproposed state-of-the-art model is robust and accurate in estimating real\nestate prices. This case study serves as an incentive for local governments\nfrom developing countries to discuss and build real estate pricing models based\non large data sets that increases fairness for all the real estate market\nstakeholders and reduces price speculation.\n"
    },
    {
        "paper_id": 2012.09306,
        "authors": "Matthias Nadler, Fabian Sch\\\"ar",
        "title": "Decentralized Finance, Centralized Ownership? An Iterative Mapping\n  Process to Measure Protocol Token Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze various Decentralized Finance (DeFi) protocols in\nterms of their token distributions. We propose an iterative mapping process\nthat allows us to split aggregate token holdings from custodial and escrow\ncontracts and assign them to their economic beneficiaries. This method accounts\nfor liquidity-, lending-, and staking-pools, as well as token wrappers, and can\nbe used to break down token holdings, even for high nesting levels. We compute\nindividual address balances for several snapshots and analyze intertemporal\ndistribution changes. In addition, we study reallocation and protocol usage\ndata, and propose a proxy for measuring token dependencies and ecosystem\nintegration. The paper offers new insights on DeFi interoperability as well as\ntoken ownership distribution and may serve as a foundation for further\nresearch.\n"
    },
    {
        "paper_id": 2012.09336,
        "authors": "John Gathergood and Fabian Gunzinger and Benedict Guttman-Kenney and\n  Edika Quispe-Torreblanca and Neil Stewart",
        "title": "Levelling Down and the COVID-19 Lockdowns: Uneven Regional Recovery in\n  UK Consumer Spending",
        "comments": "arXiv admin note: text overlap with arXiv:2010.04129",
        "journal-ref": "Covid Economics 67: 24-52, February 2021",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show the recovery in consumer spending in the United Kingdom through the\nsecond half of 2020 is unevenly distributed across regions. We utilise Fable\nData: a real-time source of consumption data that is a highly correlated,\nleading indicator of Bank of England and Office for National Statistics data.\nThe UK's recovery is heavily weighted towards the \"home counties\" around outer\nLondon and the South. We observe a stark contrast between strong online\nspending growth while offline spending contracts. The strongest recovery in\nspending is seen in online spending in the \"commuter belt\" areas in outer\nLondon and the surrounding localities and also in areas of high second home\nownership, where working from home (including working from second homes) has\nsignificantly displaced the location of spending. Year-on-year spending growth\nin November 2020 in localities facing the UK's new tighter \"Tier 3\"\nrestrictions (mostly the midlands and northern areas) was 38.4% lower compared\nwith areas facing the less restrictive \"Tier 2\" (mostly London and the South).\nThese patterns had been further exacerbated during November 2020 when a second\nnational lockdown was imposed. To prevent such COVID-19-driven regional\ninequalities from becoming persistent we propose governments introduce\ntemporary, regionally-targeted interventions in 2021. The availability of\nreal-time, regional data enables policymakers to efficiently decide when, where\nand how to implement such regional interventions and to be able to rapidly\nevaluate their effectiveness to consider whether to expand, modify or remove\nthem.\n"
    },
    {
        "paper_id": 2012.09448,
        "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Nanbo Peng, Dongdong\n  Wang, Zhixiang Huang",
        "title": "The Causal Learning of Retail Delinquency",
        "comments": "This paper was accepted and will be published in the Thirty-Fifth\n  AAAI Conference on Artificial Intelligence (AAAI-21)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the expected difference in borrower's repayment when\nthere is a change in the lender's credit decisions. Classical estimators\noverlook the confounding effects and hence the estimation error can be\nmagnificent. As such, we propose another approach to construct the estimators\nsuch that the error can be greatly reduced. The proposed estimators are shown\nto be unbiased, consistent, and robust through a combination of theoretical\nanalysis and numerical testing. Moreover, we compare the power of estimating\nthe causal quantities between the classical estimators and the proposed\nestimators. The comparison is tested across a wide range of models, including\nlinear regression models, tree-based models, and neural network-based models,\nunder different simulated datasets that exhibit different levels of causality,\ndifferent degrees of nonlinearity, and different distributional properties.\nMost importantly, we apply our approaches to a large observational dataset\nprovided by a global technology firm that operates in both the e-commerce and\nthe lending business. We find that the relative reduction of estimation error\nis strikingly substantial if the causal effects are accounted for correctly.\n"
    },
    {
        "paper_id": 2012.09493,
        "authors": "Paolo Falbo, Giorgio Ferrari, Giorgio Rizzini, Maren Diane Schmeck",
        "title": "Optimal switch from a fossil-fueled to an electric vehicle",
        "comments": null,
        "journal-ref": "Decisions in Economics and Finance 44 (2021)",
        "doi": "10.1007/s10203-021-00359-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose and solve a real options model for the optimal\nadoption of an electric vehicle. A policymaker promotes the abeyance of\nfossil-fueled vehicles through an incentive, and the representative\nfossil-fueled vehicle's owner decides the time at which buying an electric\nvehicle, while minimizing a certain expected cost. This involves a combination\nof various types of costs: the stochastic opportunity cost of driving one unit\ndistance with a traditional fossil-fueled vehicle instead of an electric one,\nthe cost associated to traffic bans, and the net purchase cost. After\ndetermining the optimal switching time and the minimal cost function for a\ngeneral diffusive opportunity cost, we specialize to the case of a\nmean-reverting process. In such a setting, we provide a model calibration on\nreal data from Italy, and we study the dependency of the optimal switching time\nwith respect to the model's parameters. Moreover, we study the effect of\ntraffic bans and incentive on the expected optimal switching time. We observe\nthat incentive and traffic bans on fossil-fueled transport can be used as\neffective tools in the hand of the policymaker to encourage the adoption of\nelectric vehicles, and hence to reduce air pollution.\n"
    },
    {
        "paper_id": 2012.09606,
        "authors": "Jir\\^o Akahori, Yuuki Ida, Maho Nishida and Shuji Tamada",
        "title": "The Thermodynamic Approach to Whole-Life Insurance: A Method for\n  Evaluation of Surrender Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a collective model for life insurance where the heterogeneity of\neach insured, including the health state, is modeled by a diffusion process.\nThis model is influenced by concepts in statistical mechanics. Using the\nproposed framework, one can describe the total pay-off as a functional of the\ndiffusion process, which can be used to derive a level premium that evaluates\nthe risk of lapses due tothe so-called adverse selection. Two numerically\ntractable models are presented to exemplify the flexibility of the proposed\nframework.\n"
    },
    {
        "paper_id": 2012.09648,
        "authors": "Alexander Glauner",
        "title": "Dynamic Reinsurance in Discrete Time Minimizing the Insurer's Cost of\n  Capital",
        "comments": "arXiv admin note: text overlap with arXiv:2010.07220",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the classical static optimal reinsurance problem, the cost of capital for\nthe insurer's risk exposure determined by a monetary risk measure is minimized\nover the class of reinsurance treaties represented by increasing Lipschitz\nretained loss functions. In this paper, we consider a dynamic extension of this\nreinsurance problem in discrete time which can be viewed as a risk-sensitive\nMarkov Decision Process. The model allows for both insurance claims and premium\nincome to be stochastic and operates with general risk measures and premium\nprinciples. We derive the Bellman equation and show the existence of a\nMarkovian optimal reinsurance policy. Under an infinite planning horizon, the\nmodel is shown to be contractive and the optimal reinsurance policy to be\nstationary. The results are illustrated with examples where the optimal policy\ncan be determined explicitly.\n"
    },
    {
        "paper_id": 2012.09661,
        "authors": "Runhuan Feng, Pingping Jiang, Hans Volkmer",
        "title": "Geometric Brownian motion with affine drift and its time-integral",
        "comments": "The paper has been accepted by Applied Mathematics and Computation",
        "journal-ref": "Applied Mathematics and Computation, 2021",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The joint distribution of a geometric Brownian motion and its time-integral\nwas derived in a seminal paper by Yor (1992) using Lamperti's transformation,\nleading to explicit solutions in terms of modified Bessel functions. In this\npaper, we revisit this classic result using the simple Laplace transform\napproach in connection to the Heun differential equation. We extend the\nmethodology to the geometric Brownian motion with affine drift and show that\nthe joint distribution of this process and its time-integral can be determined\nby a doubly-confluent Heun equation. Furthermore, the joint Laplace transform\nof the process and its time-integral is derived from the asymptotics of the\nsolutions. In addition, we provide an application by using the results for the\nasymptotics of the double-confluent Heun equation in pricing Asian options.\nNumerical results show the accuracy and efficiency of this new method.\n"
    },
    {
        "paper_id": 2012.09726,
        "authors": "Andrei Cozma and Christoph Reisinger",
        "title": "Simulation of conditional expectations under fast mean-reverting\n  stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short paper, we study the simulation of a large system of stochastic\nprocesses subject to a common driving noise and fast mean-reverting stochastic\nvolatilities. This model may be used to describe the firm values of a large\npool of financial entities. We then seek an efficient estimator for the\nprobability of a default, indicated by a firm value below a certain threshold,\nconditional on common factors. We consider approximations where coefficients\ncontaining the fast volatility are replaced by certain ergodic averages (a type\nof law of large numbers), and study a correction term (of central limit\ntheorem-type). The accuracy of these approximations is assessed by numerical\nsimulation of pathwise losses and the estimation of payoff functions as they\nappear in basket credit derivatives.\n"
    },
    {
        "paper_id": 2012.0982,
        "authors": "Chinonso Nwankwo, Weizhong Dai",
        "title": "Explicit RKF-Compact Scheme for Pricing Regime Switching American\n  Options with Varying Time Step",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research work, an explicit Runge-Kutta-Fehlberg (RKF) time\nintegration with a fourth-order compact finite difference scheme in space and a\nhigh order analytical approximation of the optimal exercise boundary is\nemployed for solving the regime-switching pricing model. In detail, we recast\nthe free boundary problem into a system of nonlinear partial differential\nequations with a multi-fixed domain. We then introduce a transformation based\non the square root function with a Lipschitz character from which a high order\nanalytical approximation is obtained to compute the derivative of the optimal\nexercise boundary in each regime. We further compute the boundary values, asset\noption, and the option Greeks for each regime using fourth-order spatial\ndiscretization and adaptive time integration. In particular, the coupled assets\noptions and option Greeks are estimated using Hermite interpolation with Newton\nbasis. Finally, a numerical experiment is carried out with two- and\nfour-regimes examples and results are compared with the existing methods. The\nresults obtained from the numerical experiment show that the present method\nprovides better performance in terms of computational speed and more accurate\nsolutions with a large step size.\n"
    },
    {
        "paper_id": 2012.09906,
        "authors": "Niklas Potrafke, Kaspar Wuthrich",
        "title": "Green governments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine how Green governments influence environmental, macroeconomic, and\neducation outcomes. We exploit that the Fukushima nuclear disaster in Japan\ngave rise to an unanticipated change in government in the German state\nBaden-Wuerttemberg in 2011. Using the synthetic control method, we find no\nevidence that the Green government influenced CO2 emissions or increased\nrenewable energy usage overall. The share of wind power usage even decreased.\nIntra-ecological conflicts prevented the Green government from implementing\ndrastic changes in environmental policies. The results do not suggest that the\nGreen government influenced macroeconomic outcomes. Inclusive education\npolicies caused comprehensive schools to become larger.\n"
    },
    {
        "paper_id": 2012.10145,
        "authors": "M. Derksen, B. Kleijn and R. de Vilder",
        "title": "Heavy tailed distributions in closing auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the tails of closing auction return distributions for a sample of\nliquid European stocks. We use the stochastic call auction model of Derksen et\nal. (2020a), to derive a relation between tail exponents of limit order\nplacement distributions and tail exponents of the resulting closing auction\nreturn distribution and we verify this relation empirically.\nCounter-intuitively, large closing price fluctuations are typically not caused\nby large market orders, instead tails become heavier when market orders are\nremoved. The model explains this by the observation that limit orders are\nsubmitted so as to counter existing market order imbalance.\n"
    },
    {
        "paper_id": 2012.10215,
        "authors": "Katsuya Ito and Kentaro Minami and Kentaro Imajo and Kei Nakagawa",
        "title": "Trader-Company Method: A Metaheuristic for Interpretable Stock Price\n  Prediction",
        "comments": "AAMAS 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investors try to predict returns of financial assets to make successful\ninvestment. Many quantitative analysts have used machine learning-based methods\nto find unknown profitable market rules from large amounts of market data.\nHowever, there are several challenges in financial markets hindering practical\napplications of machine learning-based models. First, in financial markets,\nthere is no single model that can consistently make accurate prediction because\ntraders in markets quickly adapt to newly available information. Instead, there\nare a number of ephemeral and partially correct models called \"alpha factors\".\nSecond, since financial markets are highly uncertain, ensuring interpretability\nof prediction models is quite important to make reliable trading strategies. To\novercome these challenges, we propose the Trader-Company method, a novel\nevolutionary model that mimics the roles of a financial institute and traders\nbelonging to it. Our method predicts future stock returns by aggregating\nsuggestions from multiple weak learners called Traders. A Trader holds a\ncollection of simple mathematical formulae, each of which represents a\ncandidate of an alpha factor and would be interpretable for real-world\ninvestors. The aggregation algorithm, called a Company, maintains multiple\nTraders. By randomly generating new Traders and retraining them, Companies can\nefficiently find financially meaningful formulae whilst avoiding overfitting to\na transient state of the market. We show the effectiveness of our method by\nconducting experiments on real market data.\n"
    },
    {
        "paper_id": 2012.10262,
        "authors": "Ilija I. Zovko",
        "title": "Matching in size: How market impact depends on the concentration of\n  trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that filling an order with a large number of distinct counterparts\nincurs additional market impact, as opposed to filling the order with a small\nnumber of counterparts. For best execution, therefore, it may be beneficial to\nopportunistically fill orders with as few counterparts as possible in\nLarge-in-scale (LIS) venues.\n  This article introduces the concept of concentrated trading, a situation that\noccurs when a large fraction of buying or selling in a given time period is\ndone by one or a few traders, for example when executing a large order. Using\nLondon Stock Exchange data, we show that concentrated trading suffers price\nimpact in addition to impact caused by (smart) order routing. However, when\nmatched with similarly concentrated counterparts on the other side of the\nmarket, the impact is greatly reduced. This suggests that exposing an order on\nLIS venues is expected to result in execution performance improvement.\n"
    },
    {
        "paper_id": 2012.10601,
        "authors": "Markus Kreer and Ayse Kizilersu and Anthony W. Thomas",
        "title": "Censored EM algorithm for Weibull mixtures: application to arrival times\n  of market orders",
        "comments": "10 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In a previous analysis the problem of \"zero-inflated\" time data (caused by\nhigh frequency trading in the electronic order book) was handled by\nleft-truncating the inter-arrival times. We demonstrated, using rigorous\nstatistical methods, that the Weibull distribution describes the corresponding\nstochastic dynamics for all inter-arrival time differences except in the region\nnear zero. However, since the truncated Weibull distribution was not able to\ndescribe the huge \"zero-inflated\" probability mass in the neighbourhood of zero\n(making up approximately 50\\% of the data for limit orders), it became clear\nthat the entire probability distribution is a mixture distribution of which the\nWeibull distribution is a significant part. Here we use a censored EM algorithm\nto analyse data for the difference of the arrival times of market orders, which\nusually have a much lower percentage of zero inflation, for four selected\nstocks trading on the London Stock Exchange.\n"
    },
    {
        "paper_id": 2012.10632,
        "authors": "Hansjoerg Albrecher and Pablo Azcue and Nora Muler",
        "title": "Optimal ratcheting of dividends in a Brownian risk model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of optimal dividend payout from a surplus process\ngoverned by Brownian motion with drift under the additional constraint of\nratcheting, i.e. the dividend rate can never decrease. We solve the resulting\ntwo-dimensional optimal control problem, identifying the value function to be\nthe unique viscosity solution of the corresponding Hamilton-Jacobi-Bellman\nequation. For finitely many admissible dividend rates we prove that threshold\nstrategies are optimal, and for any finite continuum of admissible dividend\nrates we establish the $\\varepsilon$-optimality of curve strategies. This work\nis a counterpart of Albrecher et al. (2020), where the ratcheting problem was\nstudied for a compound Poisson surplus process with drift. In the present\nBrownian setup, calculus of variation techniques allow to obtain a much more\nexplicit analysis and description of the optimal dividend strategies. We also\ngive some numerical illustrations of the optimality results.\n"
    },
    {
        "paper_id": 2012.10847,
        "authors": "Levon Avanesyan, Ronnie Sircar",
        "title": "Power mixture forward performance processes",
        "comments": "28 pages, 6 figures, submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the forward investment problem in market models where the stock\nprices are continuous semimartingales adapted to a Brownian filtration. We\nconstruct a broad class of forward performance processes with initial\nconditions of power mixture type, $u(x) = \\int_{\\mathbb{I}}\n\\frac{x^{1-\\gamma}}{1-\\gamma }\\nu(\\mathrm{d} \\gamma)$. We proceed to define and\nfully characterize two-power mixture forward performance processes with\nconstant risk aversion coefficients in the interval $(0,1)$, and derive\nproperties of two-power mixture forward performance processes when the risk\naversion coefficients are continuous stochastic processes. Finally, we discuss\nthe problem of managing an investment pool of two investors, whose respective\npreferences evolve as power forward performance processes.\n"
    },
    {
        "paper_id": 2012.10875,
        "authors": "Bastien Baldacci",
        "title": "High-frequency dynamics of the implied volatility surface",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a Hawkes modeling of the volatility surface's high-frequency\ndynamics and show how the Hawkes kernel coefficients govern the surface's skew\nand convexity. We provide simple sufficient conditions on the coefficients to\nensure no-arbitrage opportunities of the surface. Moreover, these conditions\nreduce the number of the kernel's parameters to estimate. Finally, we show that\nat the macroscopic level, the surface is driven by a sum of risk factors whose\nvolatility processes are rough.\n"
    },
    {
        "paper_id": 2012.11282,
        "authors": "Matti Estola and Kristian Veps\\\"al\\\"ainen",
        "title": "National Accounts as a Stock-Flow Consistent System, Part 1: The Real\n  Accounts",
        "comments": "29 pages, 41 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The 2008 economic crisis was not forecastable by at that time existing models\nof macroeconomics. Thus macroeconomics needs new tools. We introduce a model\nbased on National Accounts that shows how macroeconomic sectors are\ninterconnected. These connections explain the spread of business cycles from\none industry to another and from financial sector to the real economy. These\nlingages cannot be explained by General Equilibrium type of models. Our model\ndescribes the real part of National Accounts (NA) of an economy. The accounts\nare presented in the form of a money flow diagram between the following\nmacro-sectors: Non-financial firms, financial firms, households, government,\nand rest of the world. The model contains all main items in NA and the\ncorresponding simulation model creates time paths for 59 key macroeconomic\nquantities for an unlimited future. Finnish data of NA from time period\n1975-2012 is used in calibrating the parameters of the model, and the model\nfollows the historical data with sufficient accuracy. Our study serves as a\nbasis for systems analytic macro-models that can explain the positive and\nnegative feed-backs in the production system of an economy. These feed-backs\nare born from interactions between economic units and between real and\nfinancial markets. JEL E01, E10.\n  Key words: Stock-Flow Models, National Accounts, Simulation model.\n"
    },
    {
        "paper_id": 2012.11286,
        "authors": "Tung Yu Marco Chan",
        "title": "An Empirical Evaluation On The Effectiveness Of Medicaid Expansion\n  Across 49 States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 2014 the Patient Protection and Affordable Care Act (ACA) introduced the\nexpansion of Medicaid where states can opt to expand the eligibility for those\nin need of free health insurance. In this paper, we attempt to assess the\neffectiveness of Medicaid expansion on health outcomes of state populations\nusing Difference-in-Difference (DD) regressions to seek for causal impacts of\nexpanding Medicaid on health outcomes in 49 states. We find that in the time\nframe of 2013 to 2016, Medicaid expansion seems to have had no significant\nimpact on the health outcomes of states that have chosen to expand.\n"
    },
    {
        "paper_id": 2012.11594,
        "authors": "Rebecaa Pham and Marcel Ausloos",
        "title": "Insider trading in the run-up to merger announcements. Before and after\n  the UK's Financial Services Act 2012",
        "comments": "to be published in International Journal of Finance and Economics; 5\n  figures, 62 references; 36 pages",
        "journal-ref": null,
        "doi": "10.1002/ijfe.2325",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After the 2007/2008 financial crisis, the UK government decided that a change\nin regulation was required to amend the poor control of financial markets. The\nFinancial Services Act 2012 was developed as a result in order to give more\ncontrol and authority to the regulators of financial markets. Thus, the\nFinancial Conduct Authority (FCA) succeeded the Financial Services Authority\n(FSA). An area requiring an improvement in regulation was insider trading. Our\nstudy examines the effectiveness of the FCA in its duty of regulating insider\ntrading through utilising the event study methodology to assess abnormal\nreturns in the run-up to the first announcement of mergers. Samples of abnormal\nreturns are examined on periods, under regulation either by the FSA or by the\nFCA. Practically, stock price data on the London Stock Exchange from 2008-2012\nand 2015-2019 is investigated. The results from this study determine that\nabnormal returns are reduced after the implementation of the Financial Services\nAct 2012; prices are also found to be noisier in the period before the 2012\nAct. Insignificant abnormal returns are found in the run-up to the first\nannouncement of mergers in the 2015-2019 period. This concludes that the FCA is\nefficient in regulating insider trading.\n"
    },
    {
        "paper_id": 2012.11595,
        "authors": "Marcel Ausloos",
        "title": "Valuation Models Applied to Value-Based Management. Application to the\n  Case of UK Companies with Problems",
        "comments": "15 pages; 52 references; 3 Tables",
        "journal-ref": "Forecasting 2, 549-565 (2020)",
        "doi": "10.3390/forecast2040029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many still rightly wonder whether accounting numbers affect business value.\nBasic questions are why? and how? I aim at promoting an objective choice on how\noptimizing the most suitable valuation methods under a value-based management\nframework through some performance measurement systems. First, I present a\ncomprehensive review of valuation methods. Three valuations methods, (i) Free\nCash Flow Valuation Model (FCFVM), (ii) Residual Earning Valuation Model (REVM)\nand (iii) Abnormal Earning Growth Model (AEGM), are presented. I point out to\nadvantages and limitations. As applications, the proofs of the findings are\nillustrated on three study cases: Marks & Spencer's business pattern (size and\ngrowth prospect), which had a recently advertised valuation problem, and two\ncomparable companies, Tesco and Sainsbury's, all three chosen for\nmultiple-based valuation. For the purpose, two value drivers are chosen,\nEnV/EBIT (entity value/earnings before interests and taxes) and the\ncorresponding EnV/Sales. Thus, the question whether accounting numbers through\nmodels based on mathematical economics truly affect business value has an\nanswer: Maybe, yes.\n"
    },
    {
        "paper_id": 2012.11649,
        "authors": "Francis X. Diebold, Minchul Shin, and Boyuan Zhang",
        "title": "On the Aggregation of Probability Assessments: Regularized Mixtures of\n  Predictive Densities for Eurozone Inflation and Real Interest Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose methods for constructing regularized mixtures of density\nforecasts. We explore a variety of objectives and regularization penalties, and\nwe use them in a substantive exploration of Eurozone inflation and real\ninterest rate density forecasts. All individual inflation forecasters (even the\nex post best forecaster) are outperformed by our regularized mixtures. From the\nGreat Recession onward, the optimal regularization tends to move density\nforecasts' probability mass from the centers to the tails, correcting for\noverconfidence.\n"
    },
    {
        "paper_id": 2012.11715,
        "authors": "Nymisha Bandi and Theja Tulabandhula",
        "title": "Off-Policy Optimization of Portfolio Allocation Policies under\n  Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The dynamic portfolio optimization problem in finance frequently requires\nlearning policies that adhere to various constraints, driven by investor\npreferences and risk. We motivate this problem of finding an allocation policy\nwithin a sequential decision making framework and study the effects of: (a)\nusing data collected under previously employed policies, which may be\nsub-optimal and constraint-violating, and (b) imposing desired constraints\nwhile computing near-optimal policies with this data. Our framework relies on\nsolving a minimax objective, where one player evaluates policies via off-policy\nestimators, and the opponent uses an online learning strategy to control\nconstraint violations. We extensively investigate various choices for\noff-policy estimation and their corresponding optimization sub-routines, and\nquantify their impact on computing constraint-aware allocation policies. Our\nstudy shows promising results for constructing such policies when back-tested\non historical equities data, under various regimes of operation, dimensionality\nand constraints.\n"
    },
    {
        "paper_id": 2012.11768,
        "authors": "Jeffrey D. Michler, Anna Josephson, Talip Kilic, Siobhan Murray",
        "title": "Estimating the Impact of Weather on Agriculture",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper quantifies the significance and magnitude of the effect of\nmeasurement error in remote sensing weather data in the analysis of smallholder\nagricultural productivity. The analysis leverages 17 rounds of\nnationally-representative, panel household survey data from six countries in\nSub-Saharan Africa. These data are spatially-linked with a range of geospatial\nweather data sources and related metrics. We provide systematic evidence on\nmeasurement error introduced by 1) different methods used to obfuscate the\nexact GPS coordinates of households, 2) different metrics used to quantify\nprecipitation and temperature, and 3) different remote sensing measurement\ntechnologies. First, we find no discernible effect of measurement error\nintroduced by different obfuscation methods. Second, we find that simple\nweather metrics, such as total seasonal rainfall and mean daily temperature,\noutperform more complex metrics, such as deviations in rainfall from the\nlong-run average or growing degree days, in a broad range of settings. Finally,\nwe find substantial amounts of measurement error based on remote sensing\nproduct. In extreme cases, data drawn from different remote sensing products\nresult in opposite signs for coefficients on weather metrics, meaning that\nprecipitation or temperature draw from one product purportedly increases crop\noutput while the same metrics drawn from a different product purportedly\nreduces crop output. We conclude with a set of six best practices for\nresearchers looking to combine remote sensing weather data with socioeconomic\nsurvey data.\n"
    },
    {
        "paper_id": 2012.11825,
        "authors": "Isao Shoji and Masahiro Nozawa",
        "title": "A geometric analysis of nonlinear dynamics and its application to\n  financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A geometric method to analyze nonlinear oscillations is discussed. We\nconsider a nonlinear oscillation modeled by a second order ordinary\ndifferential equation without specifying the function form. By transforming the\ndifferential equation into the system of first order ordinary differential\nequations, the trajectory is embedded in $R^3$ as a curve, and thereby the time\nevolution of the original state can be translated into the behavior of the\ncurve in $R^3$, or the vector field along the curve. We analyze the vector\nfield to investigate the dynamic properties of a nonlinear oscillation. While\nthe function form of the model is unspecified, the vector fields and those\nassociated quantities can be estimated by a nonparametric filtering method. We\napply the proposed analysis to the time series of the Japanese stock price\nindex. The application shows that the vector field and its derivative will be\nused as the tools of picking up various signals that help understanding of the\ndynamic properties of the stock price index.\n"
    },
    {
        "paper_id": 2012.11935,
        "authors": "Antonio Martin Arroyo, Aranzazu de Juan Fernandez",
        "title": "Split-then-Combine simplex combination and selection of forecasters",
        "comments": "33 pages, 8 btables and 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper considers the Split-Then-Combine (STC) approach (Arroyo and de\nJuan, 2014) to combine forecasts inside the simplex space, the sample space of\npositive weights adding up to one. As it turns out, the simplicial statistic\ngiven by the center of the simplex compares favorably against the fixed-weight,\naverage forecast. Besides, we also develop a Combine-After-Selection (CAS)\nmethod to get rid of redundant forecasters. We apply these two approaches to\nmake out-of-sample one-step ahead combinations and subcombinations of forecasts\nfor several economic variables. This methodology is particularly useful when\nthe sample size is smaller than the number of forecasts, a case where other\nmethods (e.g., Least Squares (LS) or Principal Component Analysis (PCA)) are\nnot applicable.\n"
    },
    {
        "paper_id": 2012.11972,
        "authors": "Gabriela Kov\\'a\\v{c}ov\\'a, Birgit Rudloff, Igor Cialenco",
        "title": "Acceptability maximization",
        "comments": "31 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to study the optimal investment problem by using\ncoherent acceptability indices (CAIs) as a tool to measure the portfolio\nperformance. We call this problem the acceptability maximization. First, we\nstudy the one-period (static) case, and propose a numerical algorithm that\napproximates the original problem by a sequence of risk minimization problems.\nThe results are applied to several important CAIs, such as the gain-to-loss\nratio, the risk-adjusted return on capital and the tail-value-at-risk based\nCAI. In the second part of the paper we investigate the acceptability\nmaximization in a discrete time dynamic setup. Using robust representations of\nCAIs in terms of a family of dynamic coherent risk measures (DCRMs), we\nestablish an intriguing dichotomy: if the corresponding family of DCRMs is\nrecursive (i.e. strongly time consistent) and assuming some recursive structure\nof the market model, then the acceptability maximization problem reduces to\njust a one period problem and the maximal acceptability is constant across all\nstates and times. On the other hand, if the family of DCRMs is not recursive,\nwhich is often the case, then the acceptability maximization problem ordinarily\nis a time-inconsistent stochastic control problem, similar to the classical\nmean-variance criteria. To overcome this form of time-inconsistency, we adapt\nto our setup the set-valued Bellman's principle recently proposed in\n\\cite{KovacovaRudloff2019} applied to two particular dynamic CAIs - the dynamic\nrisk-adjusted return on capital and the dynamic gain-to-loss ratio. The\nobtained theoretical results are illustrated via numerical examples that\ninclude, in particular, the computation of the intermediate mean-risk efficient\nfrontiers.\n"
    },
    {
        "paper_id": 2012.1202,
        "authors": "Gabriel Ziegler",
        "title": "How many people are infected? A case study on SARS-CoV-2 prevalence in\n  Austria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using recent data from voluntary mass testing, I provide credible bounds on\nprevalence of SARS-CoV-2 for Austrian counties in early December 2020. When\nestimating prevalence, a natural missing data problem arises: no test results\nare generated for non-tested people. In addition, tests are not perfectly\npredictive for the underlying infection. This is particularly relevant for mass\nSARS-CoV-2 testing as these are conducted with rapid Antigen tests, which are\nknown to be somewhat imprecise. Using insights from the literature on partial\nidentification, I propose a framework addressing both issues at once. I use the\nframework to study differing selection assumptions for the Austrian data.\nWhereas weak monotone selection assumptions provide limited identification\npower, reasonably stronger assumptions reduce the uncertainty on prevalence\nsignificantly.\n"
    },
    {
        "paper_id": 2012.12118,
        "authors": "Edoardo Gallo, Darija Barak, Alastair Langtry",
        "title": "Social distancing in networks: A web-based interactive experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Governments have used social distancing to stem the spread of COVID-19, but\nlack evidence on the most effective policy to ensure compliance. We examine the\neffectiveness of fines and informational messages (nudges) in promoting social\ndistancing in a web-based interactive experiment conducted during the first\nwave of the pandemic on a near-representative sample of the US population.\nFines promote distancing, but nudges only have a marginal impact. Individuals\ndo more social distancing when they are aware they are a superspreader. Using\nan instrumental variable approach, we argue progressives are more likely to\npractice distancing, and they are marginally more responsive to fines.\n"
    },
    {
        "paper_id": 2012.12154,
        "authors": "Pedro Cadenas (1), Henryk Gzyl (2), Hyun Woong Park (1) ((1) Denison\n  University, (2) IESA)",
        "title": "How dark is the dark side of diversification?",
        "comments": "The manuscript is currently under revision by the Journal of Risk\n  Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Against the widely held belief that diversification at banking institutions\ncontributes to the stability of the financial system, Wagner (2010) found that\ndiversification actually makes systemic crisis more likely. While it is true,\nas Wagner asserts, that the probability of joint default of the diversified\nportfolios is larger; we contend that, as common practice, the effect of\ndiversification is examined with respect to a risk measure like VaR. We find\nthat when banks use VaR, diversification does reduce individual and systemic\nrisk. This, in turn, generates a different set of incentives for banks and\nregulators.\n"
    },
    {
        "paper_id": 2012.12199,
        "authors": "Yasuhito Tanaka",
        "title": "Involuntary unemployment in overlapping generations model due to\n  instability of the economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existence of involuntary unemployment advocated by J. M. Keynes is a very\nimportant problem of the modern economic theory. Using a three-generations\noverlapping generations model, we show that the existence of involuntary\nunemployment is due to the instability of the economy. Instability of the\neconomy is the instability of the difference equation about the equilibrium\nprice around the full-employment equilibrium, which means that a fall in the\nnominal wage rate caused by the presence of involuntary unemployment further\nreduces employment. This instability is due to the negative real balance effect\nthat occurs when consumers' net savings (the difference between savings and\npensions) are smaller than their debt multiplied by the marginal propensity to\nconsume from childhood consumption.\n"
    },
    {
        "paper_id": 2012.12263,
        "authors": "Joseph Bae, Darshan Gandhi, Jil Kothari, Sheshank Shankar, Jonah Bae,\n  Parth Patwa, Rohan Sukumaran, Aviral Chharia, Sanjay Adhikesaven, Shloak\n  Rathod, Irene Nandutu, Sethuraman TV, Vanessa Yu, Krutika Misra, Srinidhi\n  Murali, Aishwarya Saxena, Kasia Jakimowicz, Vivek Sharma, Rohan Iyer, Ashley\n  Mehra, Alex Radunsky, Priyanshi Katiyar, Ananthu James, Jyoti Dalal, Sunaina\n  Anand, Shailesh Advani, Jagjit Dhaliwal, and Ramesh Raskar",
        "title": "Challenges of Equitable Vaccine Distribution in the COVID-19 Pandemic",
        "comments": "18 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has led to a need for widespread and rapid vaccine\ndevelopment. As several vaccines have recently been approved for human use or\nare in different stages of development, governments across the world are\npreparing comprehensive guidelines for vaccine distribution and monitoring. In\nthis early article, we identify challenges in logistics, health outcomes,\nuser-centric matters, and communication associated with disease-related,\nindividual, societal, economic, and privacy consequences. Primary challenges\ninclude difficulty in equitable distribution, vaccine efficacy, duration of\nimmunity, multi-dose adherence, and privacy-focused record-keeping to be HIPAA\ncompliant. While many of these challenges have been previously identified and\naddressed, some have not been acknowledged from a comprehensive view accounting\nfor unprecedented interactions between challenges and specific populations. The\nlogistics of equitable widespread vaccine distribution in disparate populations\nand countries of various economic, racial, and cultural constitutions must be\nthoroughly examined and accounted for. We also describe unique challenges\nregarding the efficacy of vaccines in specialized populations including\nchildren, the elderly, and immunocompromised individuals. Furthermore, we\nreport the potential for understudied drug-vaccine interactions as well as the\npossibility that certain vaccine platforms may increase susceptibility to HIV.\nGiven these complicated issues, the importance of privacy-focused, user-centric\nsystems for vaccine education and incentivization along with clear\ncommunication from governments, organizations, and academic institutions is\nimperative. These challenges are by no means insurmountable, but require\ncareful attention to avoid consequences spanning a range of disease-related,\nindividual, societal, economic, and security domains.\n"
    },
    {
        "paper_id": 2012.12346,
        "authors": "Riu Naito, Toshihiro Yamada",
        "title": "A machine learning solver for high-dimensional integrals: Solving\n  Kolmogorov PDEs by stochastic weighted minimization and stochastic gradient\n  descent through a high-order weak approximation scheme of SDEs with Malliavin\n  weights",
        "comments": "11 pages, 2 figures; some typos are corrected; numerical results are\n  added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper introduces a very simple and fast computation method for\nhigh-dimensional integrals to solve high-dimensional Kolmogorov partial\ndifferential equations (PDEs). The new machine learning-based method is\nobtained by solving a stochastic weighted minimization with stochastic gradient\ndescent which is inspired by a high-order weak approximation scheme for\nstochastic differential equations (SDEs) with Malliavin weights. Then solutions\nto high-dimensional Kolmogorov PDEs or expectations of functionals of solutions\nto high-dimensional SDEs are accurately approximated without suffering from the\ncurse of dimensionality. Numerical examples for PDEs and SDEs up to 100\ndimensions are shown by using second and third-order discretization schemes in\norder to demonstrate the effectiveness of our method.\n"
    },
    {
        "paper_id": 2012.12422,
        "authors": "Jun Ru Anderson, Fahrudin Memic, Ismar Volic",
        "title": "Topological data analysis and UNICEF Multiple Indicator Cluster Surveys",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Multiple Indicator Cluster Surveys (MICS), supported by UNICEF, are one of\nthe most important global household survey programs that provide data on health\nand education of women and children. We analyze the Serbia 2014-15 MICS dataset\nusing topological data analysis which treats the data cloud as a topological\nspace and extracts information about its intrinsic geometric properties. In\nparticular, our analysis uses the Mapper algorithm, a dimension-reduction and\nclustering method which produces a graph from the data cloud. The resulting\nMapper graph provides insight into various relationships between household\nwealth - as expressed by the wealth index, an important indicator extracted\nfrom the MICS data - and other parameters such as urban/rural setting,\nownership of items, and prioritization of possessions. Among other uses, these\nfindings can serve to inform policy by providing a hierarchy of essential\namenities. They can also potentially be used to refine the wealth index or\ndeepen our understanding of what it captures.\n"
    },
    {
        "paper_id": 2012.12503,
        "authors": "Tomoya Mori, Minoru Osawa",
        "title": "Cities in a world of diminishing transport costs",
        "comments": "4 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic activities favor mutual geographical proximity and concentrate\nspatially to form cities. In a world of diminishing transport costs, however,\nthe advantage of physical proximity is fading, and the role of cities in the\neconomy may be declining. To provide insights into the long-run evolution of\ncities, we analyzed Japan's census data over the 1970--2015 period. We found\nthat fewer and larger cities thrived at the national scale, suggesting an\neventual mono-centric economy with a single megacity; simultaneously, each\nlarger city flattened out at the local scale, suggesting an eventual extinction\nof cities. We interpret this multi-scale phenomenon as an instance of pattern\nformation by self-organization, which is widely studied in mathematics and\nbiology. However, cities' dynamics are distinct from mathematical or biological\nmechanisms because they are governed by economic interactions mediated by\ntransport costs between locations. Our results call for the synthesis of\nknowledge in mathematics, biology, and economics to open the door for a general\npattern formation theory that is applicable to socioeconomic phenomena.\n"
    },
    {
        "paper_id": 2012.12555,
        "authors": "Zhen Zhang and Dave Cliff",
        "title": "Market Impact in Trader-Agents: Adding Multi-Level Order-Flow\n  Imbalance-Sensitivity to Automated Trading Systems",
        "comments": "To be presented at the 13th International Conference on Agents and\n  Artificial Intelligence (ICAART2021), Vienna, 4th--6th February 2021. 15\n  pages; 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets populated by human traders often exhibit \"market impact\",\nwhere the traders' quote-prices move in the direction of anticipated change,\nbefore any transaction has taken place, as an immediate reaction to the arrival\nof a large (i.e., \"block\") buy or sell order in the market: e.g., traders in\nthe market know that a block buy order will push the price up, and so they\nimmediately adjust their quote-prices upwards. Most major financial markets now\ninvolve many \"robot traders\", autonomous adaptive software agents, rather than\nhumans. This paper explores how to give such trader-agents a reliable\nanticipatory sensitivity to block orders, such that markets populated entirely\nby robot traders also show market-impact effects. In a 2019 publication Church\n& Cliff presented initial results from a simple deterministic robot trader,\nISHV, which exhibits this market impact effect via monitoring a metric of\nimbalance between supply and demand in the market. The novel contributions of\nour paper are: (a) we critique the methods used by Church & Cliff, revealing\nthem to be weak, and argue that a more robust measure of imbalance is required;\n(b) we argue for the use of multi-level order-flow imbalance (MLOFI: Xu et al.,\n2019) as a better basis for imbalance-sensitive robot trader-agents; and (c) we\ndemonstrate the use of the more robust MLOFI measure in extending ISHV, and\nalso the well-known AA and ZIP trading-agent algorithms (which have both been\npreviously shown to consistently outperform human traders). We demonstrate that\nthe new imbalance-sensitive trader-agents introduced here do exhibit market\nimpact effects, and hence are better-suited to operating in markets where\nimpact is a factor of concern or interest, but do not suffer the weaknesses of\nthe methods used by Church & Cliff. The source-code for our work reported here\nis freely available on GitHub.\n"
    },
    {
        "paper_id": 2012.12702,
        "authors": "Matthew O. Jackson and Agathe Pernoud",
        "title": "Systemic Risk in Financial Networks: A Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an overview of the relationship between financial networks and\nsystemic risk. We present a taxonomy of different types of systemic risk,\ndifferentiating between direct externalities between financial organizations\n(e.g., defaults, correlated portfolios and firesales), and perceptions and\nfeedback effects (e.g., bank runs, credit freezes). We also discuss optimal\nregulation and bailouts, measurements of systemic risk and financial\ncentrality, choices by banks' regarding their portfolios and partnerships, and\nthe changing nature of financial networks.\n"
    },
    {
        "paper_id": 2012.12704,
        "authors": "Tung Yu Marco Chan, Yue Zhang and Tsun Yi Yeung",
        "title": "Estimating The Effect Of Subscription based Streaming Services On The\n  Demand For Game Consoles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we attempt to estimate the effect of the implementation of\nsubscription-based streaming services on the demand of the associated game\nconsoles. We do this by applying the BLP demand estimation model proposed by\nBerry (1994). This results in a linear demand specification which can be\nidentified using conventional identification methods such as instrumental\nvariables estimation and fixed-effects models. We find that given our dataset,\nthe two-stage least squares (2SLS) regression provides us with convincing\nestimates that subscription-based streaming services does have a positive\neffect on the demand of game consoles as proposed by the general principle of\ncomplementary goods.\n"
    },
    {
        "paper_id": 2012.12861,
        "authors": "Matthew O. Jackson and Agathe Pernoud",
        "title": "Credit Freezes, Equilibrium Multiplicity, and Optimal Bailouts in\n  Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze how interdependencies between organizations in financial networks\ncan lead to multiple possible equilibrium outcomes. A multiplicity arises if\nand only if there exists a certain type of dependency cycle in the network that\nallows for self-fulfilling chains of defaults. We provide necessary and\nsufficient conditions for banks' solvency in any equilibrium. Building on these\nconditions, we characterize the minimum bailout payments needed to ensure\nsystemic solvency, as well as how solvency can be ensured by guaranteeing a\nspecific set of debt payments. Bailout injections needed to eliminate\nself-fulfilling cycles of defaults (credit freezes) are fully recoverable,\nwhile those needed to prevent cascading defaults outside of cycles are not. We\nshow that the minimum bailout problem is computationally hard, but provide an\nupper bound on optimal payments and show that the problem has intuitive\nsolutions in specific network structures such as those with disjoint cycles or\na core-periphery structure.\n"
    },
    {
        "paper_id": 2012.12945,
        "authors": "Bastien Baldacci, Jerome Benveniste, Gordon Ritter",
        "title": "Optimal trading without optimal control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A hypothetical risk-neutral agent who trades to maximize the expected profit\nof the next trade will approximately exhibit long-term optimal behavior as long\nas this agent uses the vector $p = \\nabla V (t, x)$ as effective microstructure\nalphas, where V is the Bellman value function for a smooth relaxation of the\nproblem. Effective microstructure alphas are the steepest-ascent direction of V\n, equal to the generalized momenta in a dual Hamiltonian formulation. This\nsimple heuristics has wide-ranging practical implications; indeed, most\nutility-maximization problems that require implementation via discrete\nlimit-order-book markets can be treated by our method.\n"
    },
    {
        "paper_id": 2012.12951,
        "authors": "Jing Shi, Marcel Ausloos, and Tingting Zhu",
        "title": "If Global or Local Investor Sentiments are Prone to Developing an Impact\n  on Stock Returns, is there an Industry Effect?",
        "comments": "24 pages, 47 references, 4 Tables, 1 figure; to be published in\n  International Journal of Finance and Economics",
        "journal-ref": null,
        "doi": "10.1002/ijfe.2216",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the heterogeneous impacts of either Global or Local\nInvestor Sentiments on stock returns. We study 10 industry sectors through the\nlens of 6 (so called) emerging countries: China, Brazil, India, Mexico,\nIndonesia and Turkey, over the 2000 to 2014 period. Using a panel data\nframework, our study sheds light on a significant effect of Local Investor\nSentiments on expected returns for basic materials, consumer goods, industrial,\nand financial industries. Moreover, our results suggest that from Global\nInvestor Sentiments alone, one cannot predict expected stock returns in these\nmarkets.\n"
    },
    {
        "paper_id": 2012.13121,
        "authors": "Yaquan Zhang, Qi Wu, Nanbo Peng, Min Dai, Jing Zhang, Hu Wang",
        "title": "Memory-Gated Recurrent Networks",
        "comments": "This paper was accepted and will be published in the Thirty-Fifth\n  AAAI Conference on Artificial Intelligence (AAAI-21)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The essence of multivariate sequential learning is all about how to extract\ndependencies in data. These data sets, such as hourly medical records in\nintensive care units and multi-frequency phonetic time series, often time\nexhibit not only strong serial dependencies in the individual components (the\n\"marginal\" memory) but also non-negligible memories in the cross-sectional\ndependencies (the \"joint\" memory). Because of the multivariate complexity in\nthe evolution of the joint distribution that underlies the data generating\nprocess, we take a data-driven approach and construct a novel recurrent network\narchitecture, termed Memory-Gated Recurrent Networks (mGRN), with gates\nexplicitly regulating two distinct types of memories: the marginal memory and\nthe joint memory. Through a combination of comprehensive simulation studies and\nempirical experiments on a range of public datasets, we show that our proposed\nmGRN architecture consistently outperforms state-of-the-art architectures\ntargeting multivariate time series.\n"
    },
    {
        "paper_id": 2012.1323,
        "authors": "Massimo Bartoletti, James Hsin-yu Chiang, Alberto Lluch-Lafuente",
        "title": "SoK: Lending Pools in Decentralized Finance",
        "comments": "20 pages. Under submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lending pools are decentralized applications which allow mutually untrusted\nusers to lend and borrow crypto-assets. These applications feature complex,\nhighly parametric incentive mechanisms to equilibrate the loan market. This\ncomplexity makes the behaviour of lending pools difficult to understand and to\npredict: indeed, ineffective incentives and attacks could potentially lead to\nemergent unwanted behaviours. Reasoning about lending pools is made even harder\nby the lack of executable models of their behaviour: to precisely understand\nhow users interact with lending pools, eventually one has to inspect their\nimplementations, where the incentive mechanisms are intertwined with low-level\nimplementation details. Further, the variety of existing implementations makes\nit difficult to distill the common aspects of lending pools. We systematize the\nexisting knowledge about lending pools, leveraging a new formal model of\ninteractions with users, which reflects the archetypal features of mainstream\nimplementations. This enables us to prove some general properties of lending\npools, such as the correct handling of funds, and to precisely describe\nvulnerabilities and attacks. We also discuss the role of lending pools in the\nbroader context of decentralized finance.\n"
    },
    {
        "paper_id": 2012.13331,
        "authors": "Panagiotis Andrianesis, Dimitris Bertsimas, Michael C. Caramanis, and\n  William W. Hogan",
        "title": "Computation of Convex Hull Prices in Electricity Markets with\n  Non-Convexities using Dantzig-Wolfe Decomposition",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The presence of non-convexities in electricity markets has been an active\nresearch area for about two decades. The -- inevitable under current marginal\ncost pricing -- problem of guaranteeing that no market participant incurs\nlosses in the day-ahead market is addressed in current practice through\nmake-whole payments a.k.a. uplift. Alternative pricing rules have been studied\nto deal with this problem. Among them, Convex Hull (CH) prices associated with\nminimum uplift have attracted significant attention. Several US Independent\nSystem Operators (ISOs) have considered CH prices but resorted to\napproximations, mainly because determining exact CH prices is computationally\nchallenging, while providing little intuition about the price formation\nrationale. In this paper, we describe the CH price estimation problem by\nrelying on Dantzig-Wolfe decomposition and Column Generation, as a tractable,\nhighly paralellizable, and exact method -- i.e., yielding exact, not\napproximate, CH prices -- with guaranteed finite convergence. Moreover, the\napproach provides intuition on the underlying price formation rationale. A test\nbed of stylized examples provide an exposition of the intuition in the CH price\nformation. In addition, a realistic ISO dataset is used to support scalability\nand validate the proof-of-concept.\n"
    },
    {
        "paper_id": 2012.13514,
        "authors": "Ritwik Banerjee and Priyoma Mustafi",
        "title": "Using social recognition to address the gender difference in\n  volunteering for low-promotability tasks",
        "comments": "20 pages with references. Additional appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Research shows that women volunteer significantly more for tasks that people\nprefer others to complete. Such tasks carry little monetary incentives because\nof their very nature. We use a modified version of the volunteer's dilemma game\nto examine if non-monetary interventions, particularly, social recognition can\nbe used to change the gender norms associated with such tasks. We design three\ntreatments, where a) a volunteer receives positive social recognition, b) a\nnon-volunteer receives negative social recognition, and c) a volunteer receives\npositive, but a non-volunteer receives negative social recognition. Our results\nindicate that competition for social recognition increases the overall\nlikelihood that someone in a group has volunteered. Positive social recognition\ncloses the gender gap observed in the baseline treatment, so does the\ncombination of positive and negative social recognition. Our results,\nconsistent with the prior literature on gender differences in competition,\nsuggest that public recognition of volunteering can change the default gender\nnorms in organizations and increase efficiency at the same time.\n"
    },
    {
        "paper_id": 2012.13657,
        "authors": "Karthik H. Shankar",
        "title": "Negative votes to depolarize politics",
        "comments": null,
        "journal-ref": "Group Decision and Negotiation (2022), Vol 31, Pages 1097--1120",
        "doi": "10.1007/s10726-022-09799-6",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The controversies around the 2020 US presidential elections certainly casts\nserious concerns on the efficiency of the current voting system in representing\nthe people's will. Is the naive Plurality voting suitable in an extremely\npolarized political environment? Alternate voting schemes are gradually gaining\npublic support, wherein the voters rank their choices instead of just voting\nfor their first preference. However they do not capture certain crucial aspects\nof voter preferences like disapprovals and negativities against candidates. I\nargue that these unexpressed negativities are the predominant source of\npolarization in politics. I propose a voting scheme with an explicit expression\nof these negative preferences, so that we can simultaneously decipher the\npopularity as well as the polarity of each candidate. The winner is picked by\nan optimal tradeoff between the most popular and the least polarizing\ncandidate. By penalizing the candidates for their polarization, we can\ndiscourage the divisive campaign rhetorics and pave way for potential third\nparty candidates.\n"
    },
    {
        "paper_id": 2012.13753,
        "authors": "Seunghyun Lee and Hyungbin Park",
        "title": "Conditions for bubbles to arise under heterogeneous beliefs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the equilibrium price of a continuous time asset traded in\na market with heterogeneous investors. We consider a positive mean reverting\nasset and two groups of investors who have different beliefs on the speed of\nmean reversion and the mean level. We provide an equivalent condition for\nbubbles to exist and show that price bubbles may not form even though there are\nheterogeneous beliefs. This condition is directly related to the drift term of\nthe asset. In addition, we characterize the minimal equilibrium price as a\nunique $C^2$ solution of a differential equation and express it using confluent\nhypergeometric functions.\n"
    },
    {
        "paper_id": 2012.13773,
        "authors": "Gang Huang, Xiaohua Zhou, Qingyang Song",
        "title": "Deep reinforcement learning for portfolio management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In our paper, we apply deep reinforcement learning approach to optimize\ninvestment decisions in portfolio management. We make several innovations, such\nas adding short mechanism and designing an arbitrage mechanism, and applied our\nmodel to make decision optimization for several randomly selected portfolios.\nThe experimental results show that our model is able to optimize investment\ndecisions and has the ability to obtain excess return in stock market, and the\noptimized agent maintains the asset weights at fixed value throughout the\ntrading periods and trades at a very low transaction cost rate. In addition, we\nredesigned the formula for calculating portfolio asset weights in continuous\ntrading process which can make leverage trading, that fills the theoretical gap\nin the calculation of portfolio weights when shorting.\n"
    },
    {
        "paper_id": 2012.13813,
        "authors": "Tom Pape",
        "title": "Prioritising data items for business analytics: Framework and\n  application to human resources",
        "comments": "Published",
        "journal-ref": "European Journal of Operational Research 252(2): 687-698 (2016)",
        "doi": "10.1016/j.ejor.2016.01.052",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The popularity of business intelligence (BI) systems to support business\nanalytics has tremendously increased in the last decade. The determination of\ndata items that should be stored in the BI system is vital to ensure the\nsuccess of an organisation's business analytic strategy. Expanding conventional\nBI systems often leads to high costs of internally generating, cleansing and\nmaintaining new data items whilst the additional data storage costs are in many\ncases of minor concern -- what is a conceptual difference to big data systems.\nThus, potential additional insights resulting from a new data item in the BI\nsystem need to be balanced with the often high costs of data creation. While\nthe literature acknowledges this decision problem, no model-based approach to\ninform this decision has hitherto been proposed. The present research describes\na prescriptive framework to prioritise data items for business analytics and\napplies it to human resources. To achieve this goal, the proposed framework\ncaptures core business activities in a comprehensive process map and assesses\ntheir relative importance and possible data support with multi-criteria\ndecision analysis.\n"
    },
    {
        "paper_id": 2012.13816,
        "authors": "Tom Pape",
        "title": "Value of agreement in decision analysis: Concept, measures and\n  application",
        "comments": null,
        "journal-ref": "Computers and Operations Research 80: 82-93 (2017)",
        "doi": "10.1016/j.ejor.2016.01.052",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In multi-criteria decision analysis workshops, participants often appraise\nthe options individually before discussing the scoring as a group. The\nindividual appraisals lead to score ranges within which the group then seeks\nthe necessary agreement to identify their preferred option. Preference\nprogramming enables some options to be identified as dominated even before the\ngroup agrees on a precise scoring for them. Workshop participants usually face\ntime pressure to make a decision. Decision support can be provided by flagging\noptions for which further agreement on their scores seems particularly\nvaluable. By valuable, we mean the opportunity to identify other options as\ndominated (using preference programming) without having their precise scores\nagreed beforehand. The present paper quantifies this Value of Agreement and\nextends the concept to portfolio decision analysis and criterion weights. The\nnew concept is validated through a case study in recruitment.\n"
    },
    {
        "paper_id": 2012.1383,
        "authors": "Stanislav Shalunov, Alexei Kitaev, Yakov Shalunov, Arseniy Akopyan",
        "title": "Calculated Boldness: Optimizing Financial Decisions with Illiquid Assets",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider games of chance played by someone with external capital that\ncannot be applied to the game and determine how this affects risk-adjusted\noptimal betting. Specifically, we focus on Kelly optimization as a metric,\noptimizing the expected logarithm of total capital including both capital in\nplay and the external capital. For games with multiple rounds, we determine the\noptimal strategy through dynamic programming and construct a close\napproximation through the WKB method. The strategy can be described in terms of\nshort-term utility functions, with risk aversion depending on the ratio of the\namount in the game to the external money. Thus, a rational player's behavior\nvaries between conservative play that approaches Kelly strategy as they are\nable to invest a larger fraction of total wealth and extremely aggressive play\nthat maximizes linear expectation when a larger portion of their capital is\nlocked away. Because you always have expected future productivity to account\nfor as external resources, this goes counter to the conventional wisdom that\nsuper-Kelly betting is a ruinous proposition.\n"
    },
    {
        "paper_id": 2012.14153,
        "authors": "Rebecka Ericsdotter Engstrom, David Collste, Sarah E. Cornell, Francis\n  X Johnson, Henrik Carlsen, Fernando Jaramillo, Goran Finnveden, Georgia\n  Destouni, Mark Howells, Nina Weitz, Viveka Palm, Francesco Fuso-Nerini",
        "title": "Succeeding at home and abroad -- Accounting for the international\n  spillovers of cities' SDG actions",
        "comments": "Comment article, 10 pages. Original submission, a later (post\n  peer-review) version is accepted for publication in NPJ Urban Sustainability",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Local SDG action is imperative to reach the 2030 Agenda, but different\nstrategies for progressing on one SDG locally may cause different 'spillovers'\non the same and other SDGs beyond local and national borders. We call for\nresearch efforts to empower local authorities to 'account globally' when acting\nlocally.\n"
    },
    {
        "paper_id": 2012.14297,
        "authors": "Marcel Ausloos, Francesca Bartolacci, Nicola G. Castellano, and Roy\n  Cerqueti",
        "title": "Simple approaches on how to discover promising strategies for efficient\n  enterprise performance, at time of crisis in the case of SMEs : Voronoi\n  clustering and outlier effects perspective",
        "comments": "31 pages; 13 figures; 6 tables; 51 references ; ; in D. Grech and J.\n  Miskiewicz (Eds.), Simplicity of Complexity in Economic and Social Systems,\n  Springer Proceedings in Complexity (Springer, Cham, 2020) pp.1-20. arXiv\n  admin note: substantial text overlap with arXiv:1808.05893",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-56160-4_-1$",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the connection between innovation activities of companies\n-- implemented before a financial crisis -- and their performance -- measured\nafter such a time of crisis. Pertinent data about companies listed in the STAR\nMarket Segment of the Italian Stock Exchange is analyzed. Innovation is\nmeasured through the level of investments in total tangible and intangible\nfixed assets in 2006-2007, while performance is captured through growth --\nexpressed by variations of sales or of total assets, -- profitability --\nthrough ROI or ROS evolution, - and productivity -- through asset turnover or\nsales/employee in the period 2008-2010. The variables of interest are analyzed\nand compared through statistical techniques and by adopting a cluster analysis.\nIn particular, a Voronoi tessellation is implemented in a varying centroids\nframework. In accord with a large part of the literature, we find that the\nbehavior of the performance of the companies is not univocal when they\ninnovate. The statistical outliers are the best cases in order to suggest\nefficient strategies. In brief, it is found that a positive rate of investments\nis preferable.\n"
    },
    {
        "paper_id": 2012.14372,
        "authors": "Tiziana Carpi, Airo Hino, Stefano Maria Iacus, Giuseppe Porro",
        "title": "On a Japanese Subjective Well-Being Indicator Based on Twitter data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/ssjj/jyac002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study presents for the first time the SWB-J index, a subjective\nwell-being indicator for Japan based on Twitter data. The index is composed by\neight dimensions of subjective well-being and is estimated relying on Twitter\ndata by using human supervised sentiment analysis. The index is then compared\nwith the analogous SWB-I index for Italy, in order to verify possible analogies\nand cultural differences. Further, through structural equation models, a causal\nassumption is tested to see whether the economic and health conditions of the\ncountry influence the well-being latent variable and how this latent dimension\naffects the SWB-J and SWB-I indicators. It turns out that, as expected, the\neconomic and health welfare is only one aspect of the multidimensional\nwell-being that is captured by the Twitter-based indicator.\n"
    },
    {
        "paper_id": 2012.14503,
        "authors": "Torsten Heinrich and Jangho Yang and Shuanping Dai",
        "title": "Growth, development, and structural change at the firm-level: The\n  example of the PR China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the microeconomic details of technological catch-up processes\noffers great potential for informing both innovation economics and development\npolicy. We study the economic transition of the PR China from an agrarian\ncountry to a high-tech economy as one example for such a case. It is clear from\npast literature that rapidly rising productivity levels played a crucial role.\nHowever, the distribution of labor productivity in Chinese firms has not been\ncomprehensively investigated and it remains an open question if this can be\nused to guide economic development. We analyze labor productivity and the\ndynamic change of labor productivity in firm-level data for the years 1998-2013\nfrom the Chinese Industrial Enterprise Database. We demonstrate that both\nvariables are conveniently modeled as L\\'evy alpha-stable distributions,\nprovide parameter estimates and analyze dynamic changes to this distribution.\nWe find that the productivity gains were not due to super-star firms, but due\nto a systematic shift of the entire distribution with otherwise mostly\nunchanged characteristics. We also found an emerging right-skew in the\ndistribution of labor productivity change. While there are significant\ndifferences between the 31 provinces and autonomous regions of the P.R. China,\nwe also show that there are systematic relations between micro-level and\nprovince-level variables. We conclude with some implications of these findings\nfor development policy.\n"
    },
    {
        "paper_id": 2012.1486,
        "authors": "Misha Perepelitsa and Ilya Timofeyev",
        "title": "Self-sustained price bubbles driven by Bitcoin innovations and adaptive\n  behavior",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.10733.00480",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that infinite divisibility of a trading commodity leads to a\nself-sustained price bubble when traders use adaptive investment strategies.\nThe adaptive strategy can be viewed as a psychological response of a trader to\nthe situation when the trader's estimation of future prices does not match the\nactual, realized price. We use a multi-agent model to illustrate the price\nbubble formation and to quantify its main statistical properties such as the\nreturn, the volatility, and the systematic risk of the price bubble to crash.\nWe discuss the plausibility for bubbles to drive prices of digital currencies.\n"
    },
    {
        "paper_id": 2012.14941,
        "authors": "Adel Daoud",
        "title": "The wealth of nations and the health of populations: A\n  quasi-experimental design of the impact of sovereign debt crises on child\n  mortality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The wealth of nations and the health of populations are intimately strongly\nassociated, yet the extent to which economic prosperity (GDP per capita) causes\nimproved health remains disputed. The purpose of this article is to analyze the\nimpact of sovereign debt crises (SDC) on child mortality, using a sample of 57\nlow- and middle-income countries surveyed by the Demographic and Health Survey\nbetween the years 1990 and 2015. These surveys supply 229 household data and\ncontaining about 3 million childbirth history records. This focus on SDC\ninstead of GDP provides a quasi-experimental moment in which the influence of\nunobserved confounding is less than a moment analyzing the normal fluctuations\nof GDP. This study measures child mortality at six thresholds: neonatal,\nunder-one (infant), under-two, under-three, under-four, and under-five\nmortality. Using a machine-learning (ML) model for causal inference, this study\nfinds that while an SDC causes an adverse yet statistically insignificant\neffect on neonatal mortality, all other child mortality group samples are\nadversely affected between a probability of 0.12 to 0.14 (all statistically\nsignificant at the 95-percent threshold). Through this ML, this study also\nfinds that the most important treatment heterogeneity moderator, in the entire\nadjustment set, is whether a child is born in a low-income country.\n"
    },
    {
        "paper_id": 2012.14962,
        "authors": "Patrick Mellacher",
        "title": "The Impact of Corona Populism: Empirical Evidence from Austria and\n  Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  I study the co-evolution between public opinion and party policy in\nsituations of crises by investigating a policy U-turn of a major Austrian\nright-wing party (FPOE) during the Covid-19 pandemic. My analysis suggests the\nexistence of both i) a \"Downsian\" effect, which causes voters to adapt their\nparty preferences based on policy congruence and ii) a \"party identification\"\neffect, which causes partisans to realign their policy preferences based on\n\"their\" party's platform. Specifically, I use individual-level panel data to\nshow that i) \"corona skeptical\" voters who did not vote for the FPOE in the\npre-Covid-19 elections of 2019 were more likely to vote for the party after it\nembraced \"corona populism\", and ii) beliefs of respondents who declared that\nthey voted for the FPOE in 2019 diverged from the rest of the population in\nthree out of four health-dimensions only after the turn, causing them to\nunderestimate the threat posed by Covid-19 compared to the rest of the\npopulation. Using aggregate-level panel data, I study whether the turn has\nproduced significant behavioral differences which could be observed in terms of\nreported cases and deaths per capita. Paradoxically, after the turn the FPOE\nvote share is significantly positively correlated with deaths per capita, but\nnot with the reported number of infections. I hypothesize that this due to a\nself-selection bias in testing, which causes a correlation between the number\nof \"corona skeptics\" and the share of unreported cases after the turn. I find\nempirical support for this hypothesis in individual-level data from a Covid-19\nprevalence study that involves information about participants' true vs.\nreported infection status. I finally study a simple heterogeneous mixing\nepidemiological model and show that a testing bias can indeed explain the\napparent paradox of an increase in deaths without an increase in reported\ncases.\n"
    },
    {
        "paper_id": 2012.14976,
        "authors": "Peter Eisenberger",
        "title": "REME -- Renewable Energy and Materials Economy -- The Path to Energy\n  Security, Prosperity and Climate Stability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A Renewable Energy and Materials Economy (REME) is proposed as the solution\nto the climate change threat. REME mimics nature to produce carbon neutral\nliquid fuels and chemicals as well as carbon negative materials by using water,\nCO$_2$ from the atmosphere and renewable energy as inputs. By being in harmony\nwith nature REME has a positive feedback between economic development and\nclimate change protection. In REME the feedback driven accelerated rate of\neconomic growth enables the climate change threat to be addressed in a timely\nmanner. It is also cost-effective protection because it sequesters by\nmonetizing the carbon removed from the air in carbon-based building materials.\nThus, addressing the climate change threat is not a cost to the economy but a\nresult of REME driven prosperity.\n"
    },
    {
        "paper_id": 2012.14999,
        "authors": "Jin Quan Zhou, Wen Jin He",
        "title": "The Involution of Industrial Life Cycle on Atlantic City Gambling\n  Industry",
        "comments": "15 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The industrial life cycle theory has proved to be helpful for describing the\nevolution of industries from birth to maturity. This paper is to highlight the\nhistorical evolution stage of Atlantic City's gambling industry in a structural\nframework covered by industrial market, industrial organization, industrial\npolicies and innovation. Data mining was employed to obtain from local official\ndocuments, to verify the module of industrial life cycle in differential phases\nas introduction, development, maturity and decline. The trajectory of Atlantic\nCity's gambling sector evolution reveals the process from the stages of\nintroduction to decline via a set of variables describing structural properties\nof this industry such as product, market and organization of industry under a\nspecial industry environment in which industry recession as a result of theory\nof industry life cycle is a particular evidence be proved again. Innovation of\nthe gambling industry presents the ongoing recovering process of the Atlantic\nCity gambling industry enriches the theory of industrial life cycle in service\nsectors.\n"
    },
    {
        "paper_id": 2012.15007,
        "authors": "Kevin He, Jonathan Libgober",
        "title": "Evolutionarily Stable (Mis)specifications: Theory and Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Toward explaining the persistence of biased inferences, we propose a\nframework to evaluate competing (mis)specifications in strategic settings.\nAgents with heterogeneous (mis)specifications coexist and draw Bayesian\ninferences about their environment through repeated play. The relative\nstability of (mis)specifications depends on their adherents' equilibrium\npayoffs. A key mechanism is the learning channel: the endogeneity of perceived\nbest replies due to inference. We characterize when a rational society is only\nvulnerable to invasion by some misspecification through the learning channel.\nThe learning channel leads to new stability phenomena, and can confer an\nevolutionary advantage to otherwise detrimental biases in economically relevant\napplications.\n"
    },
    {
        "paper_id": 2012.15035,
        "authors": "Minkyu Shin, Jin Kim, Minkyung Kim",
        "title": "Measuring Human Adaptation to AI in Decision Making: Application to\n  Evaluate Changes after AlphaGo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Across a growing number of domains, human experts are expected to learn from\nand adapt to AI with superior decision making abilities. But how can we\nquantify such human adaptation to AI? We develop a simple measure of human\nadaptation to AI and test its usefulness in two case studies. In Study 1, we\nanalyze 1.3 million move decisions made by professional Go players and find\nthat a positive form of adaptation to AI (learning) occurred after the players\ncould observe the reasoning processes of AI, rather than mere actions of AI.\nThese findings based on our measure highlight the importance of explainability\nfor human learning from AI. In Study 2, we test whether our measure is\nsufficiently sensitive to capture a negative form of adaptation to AI (cheating\naided by AI), which occurred in a match between professional Go players. We\ndiscuss our measure's applications in domains other than Go, especially in\ndomains in which AI's decision making ability will likely surpass that of human\nexperts.\n"
    },
    {
        "paper_id": 2012.15078,
        "authors": "Anna Denkowska and Stanis{\\l}aw Wanat",
        "title": "Development and similarity of insurance markets of European Union\n  countries after the enlargement in 2004",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The enlargement of the European Union to new countries in 2004 launched\nmechanisms supporting the development of various social and economic areas, as\nwell as levelling the differences between the Community members in these areas.\nThis article focuses on the insurance sector. Its main purpose is to analyze\nthe development and similarity of the insurance markets of old and new members\nof the European Union after the enlargement in 2004.\n"
    },
    {
        "paper_id": 2012.15144,
        "authors": "Edouard Ribes (CERNA i3)",
        "title": "What is the impact of labor displacement on management consulting\n  services?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Labor displacement off-or nearshore is a performance improvement instrument\nthat currently sparks a lot of interest in the service sector. This article\nproposes a model to understand the consequences of such a decision on\nmanagement consulting firms. Its calibration on the market of consulting\nservices for the German transportation industry highlights that, under\nrealistic assumptions, labor displacement translates in price decrease by-0.5%\non average per year and that for MC practices to remain competitive/profitable\nthey have to at least increase the amount of work they off/nears shore by +0.7%\na year.\n"
    },
    {
        "paper_id": 2012.15158,
        "authors": "Daisuke Ikeda, Shangshang Li, Sophocles Mavroeidis, Francesco Zanetti",
        "title": "Testing the effectiveness of unconventional monetary policy in Japan and\n  the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unconventional monetary policy (UMP) may make the effective lower bound (ELB)\non the short-term interest rate irrelevant. We develop a theoretical model that\nunderpins our empirical test of this `irrelevance hypothesis' based on the\nsimple idea that under the hypothesis, the short rate can be excluded in any\nempirical model that accounts for alternative measures of monetary policy. We\ntest the hypothesis for Japan and the United States using a structural vector\nautoregressive model with the ELB. We firmly reject the hypothesis but find\nthat UMP has had strong delayed effects.\n"
    },
    {
        "paper_id": 2012.1533,
        "authors": "Jillian M. Clements, Di Xu, Nooshin Yousefi, Dmitry Efimov",
        "title": "Sequential Deep Learning for Credit Risk Monitoring with Tabular\n  Financial Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning plays an essential role in preventing financial losses in\nthe banking industry. Perhaps the most pertinent prediction task that can\nresult in billions of dollars in losses each year is the assessment of credit\nrisk (i.e., the risk of default on debt). Today, much of the gains from machine\nlearning to predict credit risk are driven by gradient boosted decision tree\nmodels. However, these gains begin to plateau without the addition of expensive\nnew data sources or highly engineered features. In this paper, we present our\nattempts to create a novel approach to assessing credit risk using deep\nlearning that does not rely on new model inputs. We propose a new credit card\ntransaction sampling technique to use with deep recurrent and causal\nconvolution-based neural networks that exploits long historical sequences of\nfinancial data without costly resource requirements. We show that our\nsequential deep learning approach using a temporal convolutional network\noutperformed the benchmark non-sequential tree-based model, achieving\nsignificant financial savings and earlier detection of credit risk. We also\ndemonstrate the potential for our approach to be used in a production\nenvironment, where our sampling technique allows for sequences to be stored\nefficiently in memory and used for fast online learning and inference.\n"
    },
    {
        "paper_id": 2012.15371,
        "authors": "Javier L\\'opez Prol and Wolf-Peter Schill",
        "title": "The Economics of Variable Renewables and Electricity Storage",
        "comments": null,
        "journal-ref": "Annual Review of Resource Economics, Vol. 13, 2021, pp. 443-467",
        "doi": "10.1146/annurev-resource-101620-081246",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The transformation of the electricity sector is a main element of the\ntransition to a decarbonized economy. Conventional generators powered by fossil\nfuels have to be replaced by variable renewable energy (VRE) sources in\ncombination with electricity storage and other options for providing temporal\nflexibility. We discuss the market dynamics of increasing VRE penetration and\ntheir integration in the electricity system. We describe the merit-order effect\n(decline of wholesale electricity prices as VRE penetration increases) and the\ncannibalization effect (decline of VRE value as their penetration increases).\nWe further review the role of electricity storage and other flexibility options\nfor integrating variable renewables, and how storage can contribute to\nmitigating the two mentioned effects. We also use a stylized open-source model\nto provide some graphical intuition on this. While relatively high shares of\nVRE are achievable with moderate amounts of electricity storage, the role of\nlong-term storage increases as the VRE share approaches 100%.\n"
    },
    {
        "paper_id": 2012.1541,
        "authors": "Jos\\'e Vin\\'icius de Miranda Cardoso and Jiaxi Ying and Daniel Perez\n  Palomar",
        "title": "Algorithms for Learning Graphs in Financial Markets",
        "comments": "62 pages, 25 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the past two decades, the field of applied finance has tremendously\nbenefited from graph theory. As a result, novel methods ranging from asset\nnetwork estimation to hierarchical asset selection and portfolio allocation are\nnow part of practitioners' toolboxes. In this paper, we investigate the\nfundamental problem of learning undirected graphical models under Laplacian\nstructural constraints from the point of view of financial market times series\ndata. In particular, we present natural justifications, supported by empirical\nevidence, for the usage of the Laplacian matrix as a model for the precision\nmatrix of financial assets, while also establishing a direct link that reveals\nhow Laplacian constraints are coupled to meaningful physical interpretations\nrelated to the market index factor and to conditional correlations between\nstocks. Those interpretations lead to a set of guidelines that practitioners\nshould be aware of when estimating graphs in financial markets. In addition, we\ndesign numerical algorithms based on the alternating direction method of\nmultipliers to learn undirected, weighted graphs that take into account\nstylized facts that are intrinsic to financial data such as heavy tails and\nmodularity. We illustrate how to leverage the learned graphs into practical\nscenarios such as stock time series clustering and foreign exchange network\nestimation. The proposed graph learning algorithms outperform the\nstate-of-the-art methods in an extensive set of practical experiments.\nFurthermore, we obtain theoretical and empirical convergence results for the\nproposed algorithms. Along with the developed methodologies for graph learning\nin financial markets, we release an R package, called fingraph, accommodating\nthe code and data to obtain all the experimental results.\n"
    },
    {
        "paper_id": 2012.15435,
        "authors": "Markus Brueckner, Tomoo Kikuchi, and George Vachadze",
        "title": "Transitional Dynamics of the Saving Rate and Economic Growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We estimate the relationship between GDP per capita growth and the growth\nrate of the national savings rate using a panel of 130 countries over the\nperiod 1960-2017. We find that GDP per capita growth increases (decreases) the\ngrowth rate of the national savings rate in poor countries (rich countries),\nand a higher credit-to-GDP ratio decreases the national savings rate as well as\nthe income elasticity of the national savings rate. We develop a model with a\ncredit constraint to explain the growth-saving relationship by the saving\nbehavior of entrepreneurs at both the intensive and extensive margins. We\nfurther present supporting evidence for our theoretical findings by utilizing\ncross-country time series data of the number of new businesses registered and\nthe corporate savings rate.\n"
    },
    {
        "paper_id": 2012.15487,
        "authors": "Sung-Gook Choi and Deok-Sun Lee",
        "title": "Skewness of local logarithmic exports",
        "comments": "12 pages, 5 figures, Appendix with 7 supplementary figures",
        "journal-ref": "Phys. Rev. E 103, 032314 (2021)",
        "doi": "10.1103/PhysRevE.103.032314",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The distributions of trade values and relationships among countries and\nproduct categories reflect how countries select their trade partners and design\nexport portfolios. Here we consider the exporter-importer network and the\nexporter-product network with directed links weighted by the logarithm of the\ncorresponding export values each year from 1962 to 2018, and study how the\nweights of the outgoing links from each country are distributed. Such local\nlogarithmic export distributions by destinations and products are found to\nfollow approximately the Gaussian distribution across exporters and time,\nimplying random assignment of export values on logarithmic scale. However, a\nnon-zero skewness is identified, changing from positive to negative as\nexporters have more partner importers and more product categories in their\nportfolios. Seeking the origin, we analyze how local exports depend on the\nout-degree of exporter and the in-degrees of destinations/products and\nformulate their quantitative and measurable relation incorporating randomness,\nwhich uncovers the fundamental nature of the export strategies of individual\ncountries.\n"
    },
    {
        "paper_id": 2012.15541,
        "authors": "David R. Ba\\~nos",
        "title": "Life insurance policies with cash flows subject to random interest rate\n  changes",
        "comments": "23 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main purpose of this work is to derive a partial differential equation\nfor the reserves of life insurance liabilities subject to stochastic interest\nrates where the benefits and premiums depend directly on changes in the\ninterest rate curve. In particular, we allow the payment streams to depend on\nthe performance of an overnight technical interest rate, making them stochastic\nas well. This opens up for considering new types of contracts based on the\nperformance of the insurer's returns on their own investments. We provide\nexplicit solutions for the reserves when the premiums and benefits vary\naccording to interest rate levels or averages under the Vasicek model and\nconduct some simulations computing reserve surfaces numerically. We also give\nan example of a reinsurance treaty taking over pension payments when the\ninsurer's average returns fall under some specified threshold.\n"
    },
    {
        "paper_id": 2012.15705,
        "authors": "Joffrey Derchu",
        "title": "A Bayesian viewpoint on the price formation process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a simple framework in which market participants update their\nprior about an efficient price with a model-based learning process. We show\nthat exponential intensities for the arrival of aggressive orders arise\nnaturally in this setting. Our approach allows us to fully describe market\ndynamics in the case with Brownian efficient price and informed market takers.\nWe are also able to revisit the emergence of market impact due to meta-order\nsplitting, making several connections with existing literature.\n"
    },
    {
        "paper_id": 2012.15753,
        "authors": "Lukas Bolte and Nicole Immorlica and Matthew O. Jackson",
        "title": "The Role of Referrals in Immobility, Inequality, and Inefficiency in\n  Labor Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the consequences of job markets' heavy reliance on referrals.\nReferrals screen candidates and lead to better matches and increased\nproductivity, but disadvantage job-seekers who have few or no connections to\nemployed workers, leading to increased inequality. Coupled with homophily,\nreferrals also lead to immobility: a demographic group's low current employment\nrate leads that group to have relatively low future employment as well. We\nidentify conditions under which distributing referrals more evenly across a\npopulation not only reduces inequality, but also improves future productivity\nand economic mobility. We use the model to examine optimal policies, showing\nthat one-time affirmative action policies involve short-run production losses,\nbut lead to long-term improvements in equality, mobility, and productivity due\nto induced changes in future referrals. We also examine how the possibility of\nfiring workers changes the effects of referrals.\n"
    },
    {
        "paper_id": 2101.00223,
        "authors": "Kevin Shuai Zhang, Traian Pirvu",
        "title": "Pricing spread option with liquidity adjustments",
        "comments": "21 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the pricing and hedging of European spread options on correlated\nassets when, in contrast to the standard framework and consistent with\nimperfect liquidity markets, the trading in the stock market has a direct\nimpact on stocks prices. We consider a partial-impact and a full-impact model\nin which the price impact is caused by every trading strategy in the market.\nThe generalized Black-Scholes pricing partial differential equations (PDEs) are\nobtained and analysed. We perform a numerical analysis to exhibit the\nilliquidity effect on the replication strategy of the European spread option.\nCompared to the Black-Scholes model or a partial impact model, the trader in\nthe full impact model buys more stock to replicate the option, and this leads\nto a higher option price.\n"
    },
    {
        "paper_id": 2101.00251,
        "authors": "Mahan Tahvildari",
        "title": "Forward indifference valuation and hedging of basis risk under partial\n  information",
        "comments": "63 pages, 1 figure, MSc dissertation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the hedging and valuation of European and American claims on a\nnon-traded asset $Y$, when a traded stock $S$ is available for hedging, with\n$S$ and $Y$ following correlated geometric Brownian motions. This is an\nincomplete market, often called a basis risk model. The market agent's risk\npreferences are modelled using a so-called forward performance process (forward\nutility), which is a time-decreasing utility of exponential type. Moreover, the\nmarket agent (investor) does not know with certainty the values of the asset\nprice drifts. This market setting with drift parameter uncertainty is the\npartial information scenario. We discuss the stochastic control problem\nobtained by setting up the hedging portfolio and derive the optimal hedging\nstrategy. Furthermore, a (dual) forward indifference price representation of\nthe claim and its PDE are obtained. With these results, the residual risk\nprocess representing the basis risk (hedging error), pay-off decompositions and\nasymptotic expansions of the indifference price in the European case are\nderived. We develop the analogous stochastic control and stopping problem with\nan American claim and obtain the corresponding forward indifference price\nvaluation formula.\n"
    },
    {
        "paper_id": 2101.00281,
        "authors": "Rim Krouk and Fernando Almeida",
        "title": "Exploring the Impact of COVID-19 in the Sustainability of Airbnb\n  Business Model",
        "comments": "18 pages, 1 figure, journal",
        "journal-ref": "Journal of Smart Economic Growth, 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Society is undergoing many transformations and faces economic crises,\nenvironmental, social, and public health issues. At the same time, the\nInternet, mobile communications, cloud technologies, and social networks are\ngrowing rapidly and fostering the digitalization processes of business and\nsociety. It is in this context that the shared economy has assumed itself as a\nnew social and economic system based on the sharing of resources and has\nallowed the emergence of innovative businesses like Airbnb. However, COVID-19\nhas challenged this business model in the face of restrictions imposed in the\ntourism sector. Its consequences are not exclusively short-term and may also\ncall into question the sustainability of Airbnb. In this sense, this study aims\nto explore the sustainability of the Airbnb business model considering two\ntheories which advocate that hosts can cover the short-term financial effects,\nwhile another defends a paradigm shift in the demand for long-term\naccommodations to ensure greater stability for hosts.\n"
    },
    {
        "paper_id": 2101.00299,
        "authors": "Andrew Papanicolaou",
        "title": "Extreme-Strike Comparisons and Structural Bounds for SPX and VIX Options",
        "comments": "Special Thank You to Roger Lee for your help in this paper",
        "journal-ref": "SIAM Journal on Financial Mathematics (2018) Vol. 9, No, 3, pp.\n  401-434",
        "doi": "10.1137/141001615",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article explores the relationship between the SPX and VIX options\nmarkets. High-strike VIX call options are used to hedge tail risk in the SPX,\nwhich means that SPX options are a reflection of the extreme-strike asymptotics\nof VIX options, and vice versa. This relationship can be quantified using\nmoment formulas in a model-free way. Comparisons are made between VIX and SPX\nimplied volatilities along with various examples of stochastic volatility\nmodels.\n"
    },
    {
        "paper_id": 2101.00327,
        "authors": "Ruiqiang Song, Min Shu, Wei Zhu",
        "title": "The 2020 Global Stock Market Crash: Endogenous or Exogenous?",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126425",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Starting on February 20, 2020, the global stock markets began to suffer the\nworst decline since the Great Recession in 2008, and the COVID-19 has been\nwidely blamed on the stock market crashes. In this study, we applied the\nlog-periodic power law singularity (LPPLS) methodology based on multilevel time\nseries to unravel the underlying mechanisms of the 2020 global stock market\ncrash by analyzing the trajectories of 10 major stock market indexes from both\ndeveloped and emergent stock markets, including the S&P 500, DJIA, NASDAQ,\nFTSE, DAX, NIKKEI, CSI 300, HSI, BSESN, and BOVESPA. In order to effectively\ndistinguish between endogenous crash and exogenous crash, we proposed using the\nLPPLS confidence indicator as a classification proxy. The results show that the\napparent LPPLS bubble patterns of the super-exponential increase, corrected by\nthe accelerating logarithm-periodic oscillations, have indeed presented in the\nprice trajectories of the seven indexes: S&P 500, DJIA, NASDAQ, DAX, CSI 300,\nBSESN, and BOVESPA, indicating that the large positive bubbles have formed\nendogenously prior to the 2020 stock market crash, and the subsequent crashes\nfor the seven indexes are endogenous, stemming from the increasingly systemic\ninstability of the stock markets, while the well-known external shocks such as\nthe COVID-19 pandemic etc. only acted as sparks during the 2020 global stock\nmarket crash. In contrast, the obvious signatures of the LPPLS model have not\nbeen observed in the price trajectories of the three remaining indexes: FTSE,\nNIKKEI, and HSI, signifying that the crashes in these three indexes are\nexogenous, stemming from external shocks. The novel classification method of\ncrash types proposed in this study can also be used to analyze regime changes\nof any price trajectories in global financial markets.\n"
    },
    {
        "paper_id": 2101.00343,
        "authors": "Yu-Jui Huang, Zhou Zhou",
        "title": "A Time-Inconsistent Dynkin Game: from Intra-personal to Inter-personal\n  Equilibria",
        "comments": null,
        "journal-ref": "Finance and Stochastics, Vol. 26 (2022), Issue 2, pp 301-334",
        "doi": "10.1007/s00780-021-00468-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a nonzero-sum Dynkin game in discrete time under\nnon-exponential discounting. For both players, there are two levels of\ngame-theoretic reasoning intertwined. First, each player looks for an\nintra-personal equilibrium among her current and future selves, so as to\nresolve time inconsistency triggered by non-exponential discounting. Next,\ngiven the other player's chosen stopping policy, each player selects a best\nresponse among her intra-personal equilibria. A resulting inter-personal\nequilibrium is then a Nash equilibrium between the two players, each of whom\nemploys her best intra-personal equilibrium with respect to the other player's\nstopping policy. Under appropriate conditions, we show that an inter-personal\nequilibrium exists, based on concrete iterative procedures along with Zorn's\nlemma. To illustrate our theoretic results, we investigate a two-player real\noptions valuation problem: two firms negotiate a deal of cooperation to\ninitiate a project jointly. By deriving inter-personal equilibria explicitly,\nwe find that coercive power in negotiation depends crucially on the impatience\nlevels of the two firms.\n"
    },
    {
        "paper_id": 2101.00422,
        "authors": "Billio Monica, Casarin Roberto, Costola Michele, Iacopini Matteo",
        "title": "COVID-19 spreading in financial networks: A semiparametric matrix\n  regression model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Network models represent a useful tool to describe the complex set of\nfinancial relationships among heterogeneous firms in the system. In this paper,\nwe propose a new semiparametric model for temporal multilayer causal networks\nwith both intra- and inter-layer connectivity. A Bayesian model with a\nhierarchical mixture prior distribution is assumed to capture heterogeneity in\nthe response of the network edges to a set of risk factors including the\nEuropean COVID-19 cases. We measure the financial connectedness arising from\nthe interactions between two layers defined by stock returns and volatilities.\nIn the empirical analysis, we study the topology of the network before and\nafter the spreading of the COVID-19 disease.\n"
    },
    {
        "paper_id": 2101.00565,
        "authors": "Jos\\'e E. Figueroa-L\\'opez, Ruoting Gong, and Yuchen Han",
        "title": "Estimation of Tempered Stable L\\'{e}vy Models of Infinite Variation",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method for the estimation of a semiparametric tempered\nstable L\\'{e}vy model. The estimation procedure combines iteratively an\napproximate semiparametric method of moment estimator, Truncated Realized\nQuadratic Variations (TRQV), and a newly found small-time high-order\napproximation for the optimal threshold of the TRQV of tempered stable\nprocesses. The method is tested via simulations to estimate the volatility and\nthe Blumenthal-Getoor index of the generalized CGMY model as well as the\nintegrated volatility of a Heston-type model with CGMY jumps. The method\noutperforms other efficient alternatives proposed in the literature when\nworking with a L\\'evy process (i.e., the volatility is constant), or when the\nindex of jump intensity $Y$ is larger than $3/2$ in the presence of stochastic\nvolatility.\n"
    },
    {
        "paper_id": 2101.00576,
        "authors": "Nick James",
        "title": "Dynamics, behaviours, and anomaly persistence in cryptocurrencies and\n  equities surrounding COVID-19",
        "comments": "As published in Physica A",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 570 (2021)\n  125831",
        "doi": "10.1016/j.physa.2021.125831",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper uses new and recently introduced methodologies to study the\nsimilarity in the dynamics and behaviours of cryptocurrencies and equities\nsurrounding the COVID-19 pandemic. We study two collections; 45\ncryptocurrencies and 72 equities, both independently and in conjunction. First,\nwe examine the evolution of cryptocurrency and equity market dynamics, with a\nparticular focus on their change during the COVID-19 pandemic. We demonstrate\nmarkedly more similar dynamics during times of crisis. Next, we apply recently\nintroduced methods to contrast trajectories, erratic behaviours, and extreme\nvalues among the two multivariate time series. Finally, we introduce a new\nframework for determining the persistence of market anomalies over time.\nSurprisingly, we find that although cryptocurrencies exhibit stronger\ncollective dynamics and correlation in all market conditions, equities behave\nmore similarly in their trajectories, extremes, and show greater persistence in\nanomalies over time.\n"
    },
    {
        "paper_id": 2101.00625,
        "authors": "Guillermo Jos\\'e Navarro del Toro",
        "title": "What does the consumer know about the environmental damage caused by the\n  disposable cup and the need to replace it",
        "comments": "in Spanish",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The objective of this work was to know the amount and frequency with which\nthe people of Arandas in the Altos de Jalisco region use disposable cups and\nthen know how willing they are to use edible cups made with natural gelatin. In\nthis regard, it is worth commenting that these can not only be nutritious for\nthose who consume them (since gelatin is a fortifying nutrient created from the\nskin and bone of pigs and cows), but they could also be degraded in a few days\nor be ingested by animals. To collect the information, a survey consisting of\nsix questions was used, which was applied to 31 people by telephone and another\n345 personally (in both cases they were applied to young people and adults).\nThe results show that the residents of that town considerably use plastic cups\nin the different events that take place each week, which are more numerous\nduring the patron saint festivities or at the end of the year. Even so, these\npeople would be willing to change these habits, although for this, measures\nmust be taken that do not affect the companies in that area, which work mainly\nwith plastics and generate a high percentage of jobs.\n"
    },
    {
        "paper_id": 2101.00648,
        "authors": "Bastien Baldacci, Dylan Possama\\\"i",
        "title": "Governmental incentives for green bonds investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated by the recent studies on the green bond market, we build a model in\nwhich an investor trades on a portfolio of green and conventional bonds, both\nissued by the same governmental entity. The government provides incentives to\nthe bondholder in order to increase the amount invested in green bonds. These\nincentives are, optimally, indexed on the prices of the bonds, their quadratic\nvariation and covariation. We show numerically on a set of French governmental\nbonds that our methodology outperforms the current tax-incentives systems in\nterms of green investments. Moreover, it is robust to model specification for\nbond prices and can be applied to a large portfolio of bonds using classical\noptimisation methods.\n"
    },
    {
        "paper_id": 2101.00669,
        "authors": "Siyu Chen, Ravi Seshadri, Carlos Lima Azevedo, Arun P. Akkinepally,\n  Renming Liu, Andrea Araldo, Yu Jiang, Moshe E. Ben-Akiva",
        "title": "Market Design for Tradable Mobility Credits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Tradable mobility credit (TMC) schemes are an approach to travel demand\nmanagement that have received significant attention in recent years. This paper\nproposes and analyzes alternative market models for a TMC system -- focusing on\nmarket design aspects such as allocation/expiration of tokens, rules governing\ntrading, transaction fees, and regulator intervention -- and develops a\nmethodology to explicitly model the dis-aggregate behavior of individuals\nwithin the market. Extensive simulation experiments are conducted within a\ncombined mode and departure time context for the morning commute problem to\ncompare the performance of the alternative designs relative to congestion\npricing and a no-control scenario. The simulation experiments employ a\nday-to-day assignment framework wherein transportation demand is modeled using\na logit-mixture model with income effects and supply is modeled using a\nstandard bottleneck model. The results indicate that small fixed transaction\nfees can effectively mitigate undesirable behavior in the market without a\nsignificant loss in efficiency (total welfare) whereas proportional transaction\nfees are less effective both in terms of efficiency and in avoiding undesirable\nmarket behavior. Further, an allocation of tokens in continuous time can be\nbeneficial in dealing with non-recurrent events and avoiding concentrated\ntrading activity. In the presence of income effects, despite small fixed\ntransaction fees, the TMC system yields a marginally higher social welfare than\ncongestion pricing while attaining revenue neutrality. Further, it is more\nrobust in the presence of forecasting errors and non-recurrent events due to\nthe adaptiveness of the market. Finally, as expected, the TMC scheme is more\nequitable (when revenues from congestion pricing are not redistributed)\nalthough it is not guaranteed to be Pareto-improving when tokens are\ndistributed equally.\n"
    },
    {
        "paper_id": 2101.00719,
        "authors": "Sridhar Ravula",
        "title": "Bankruptcy prediction using disclosure text features",
        "comments": "36 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A public firm's bankruptcy prediction is an important financial research\nproblem because of the security price downside risks. Traditional methods rely\non accounting metrics that suffer from shortcomings like window dressing and\nretrospective focus. While disclosure text-based metrics overcome some of these\nissues, current methods excessively focus on disclosure tone and sentiment.\nThere is a requirement to relate meaningful signals in the disclosure text to\nfinancial outcomes and quantify the disclosure text data. This work proposes a\nnew distress dictionary based on the sentences used by managers in explaining\nfinancial status. It demonstrates the significant differences in linguistic\nfeatures between bankrupt and non-bankrupt firms. Further, using a large sample\nof 500 bankrupt firms, it builds predictive models and compares the performance\nagainst two dictionaries used in financial text analysis. This research shows\nthat the proposed stress dictionary captures unique information from\ndisclosures and the predictive models based on its features have the highest\naccuracy.\n"
    },
    {
        "paper_id": 2101.00878,
        "authors": "Anna Baiardi and Andrea A. Naghi",
        "title": "The Value Added of Machine Learning to Causal Inference: Evidence from\n  Revisited Studies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A new and rapidly growing econometric literature is making advances in the\nproblem of using machine learning methods for causal inference questions. Yet,\nthe empirical economics literature has not started to fully exploit the\nstrengths of these modern methods. We revisit influential empirical studies\nwith causal machine learning methods and identify several advantages of using\nthese techniques. We show that these advantages and their implications are\nempirically relevant and that the use of these methods can improve the\ncredibility of causal analysis.\n"
    },
    {
        "paper_id": 2101.00913,
        "authors": "Menno Schellekens and Taha Yasseri",
        "title": "Credit Crunch: The Role of Household Lending Capacity in the Dutch\n  Housing Boom and Bust 1995-2018",
        "comments": "under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  What causes house prices to rise and fall? Economists identify household\naccess to credit as a crucial factor. \"Loan-to-Value\" and \"Debt-to-GDP\" ratios\nare the standard measures for credit access. However, these measures fail to\nexplain the depth of the Dutch housing bust after the 2009 Financial Crisis.\nThis work is the first to model household lending capacity based on the\nformulas that Dutch banks use in the mortgage application process. We compare\nthe ability of regression models to forecast housing prices when different\nmeasures of credit access are utilised. We show that our measure of household\nlending capacity is a forward-looking, highly predictive variable that\noutperforms `Loan-to-Value' and debt ratios in forecasting the Dutch crisis.\nSharp declines in lending capacity foreshadow the market deceleration.\n"
    },
    {
        "paper_id": 2101.0094,
        "authors": "Max Kleinebrahm, Jacopo Torriti, Russell McKenna, Armin Ardone, Wolf\n  Fichtner",
        "title": "Using attention to model long-term dependencies in occupancy behavior",
        "comments": "10 pages, Tackling Climate Change with Machine Learning workshop at\n  NeurIPS 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Models simulating household energy demand based on different occupant and\nhousehold types and their behavioral patterns have received increasing\nattention over the last years due the need to better understand fundamental\ncharacteristics that shape the demand side. Most of the models described in the\nliterature are based on Time Use Survey data and Markov chains. Due to the\nnature of the underlying data and the Markov property, it is not sufficiently\npossible to consider day to day dependencies in occupant behavior. An accurate\nmapping of day to day dependencies is of increasing importance for accurately\nreproducing mobility patterns and therefore for assessing the charging\nflexibility of electric vehicles. This study bridges the gap between energy\nrelated activity modelling and novel machine learning approaches with the\nobjective to better incorporate findings from the field of social practice\ntheory in the simulation of occupancy behavior. Weekly mobility data are merged\nwith daily time use survey data by using attention based models. In a first\nstep an autoregressive model is presented, which generates synthetic weekly\nmobility schedules of individual occupants and thereby captures day to day\ndependencies in mobility behavior. In a second step, an imputation model is\npresented, which enriches the weekly mobility schedules with detailed\ninformation about energy relevant at home activities. The weekly activity\nprofiles build the basis for modelling consistent electricity, heat and\nmobility demand profiles of households. Furthermore, the approach presented\nforms the basis for providing data on socio-demographically differentiated\noccupant behavior to the general public.\n"
    },
    {
        "paper_id": 2101.01006,
        "authors": "Richard J. Martin",
        "title": "Design and analysis of momentum trading strategies",
        "comments": "v2: Expanded the Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We give a complete description of the third-moment (skewness) characteristics\nof both linear and nonlinear momentum trading strategies, the latter being\nunderstood as transformations of a normalised moving-average filter (EMA). We\nexplain in detail why the skewness is generally positive and has a term\nstructure.\n  This paper is a synthesis of two papers published by the author in RISK in\n2012, with some updates and comments.\n"
    },
    {
        "paper_id": 2101.01024,
        "authors": "Ariel Neufeld, Julian Sester",
        "title": "Model-free price bounds under dynamic option trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend discrete time semi-static trading strategies by also\nallowing for dynamic trading in a finite amount of options, and we study the\nconsequences for the model-independent super-replication prices of exotic\nderivatives. These include duality results as well as a precise\ncharacterization of pricing rules for the dynamically tradable options\ntriggering an improvement of the price bounds for exotic derivatives in\ncomparison with the conventional price bounds obtained through the martingale\noptimal transport approach.\n"
    },
    {
        "paper_id": 2101.01065,
        "authors": "Anthony D Stephens and David R Walwyn",
        "title": "Predicting the Performance of a Future United Kingdom Grid and Wind\n  Fleet When Providing Power to a Fleet of Battery Electric Vehicles",
        "comments": "2 tables, 17 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sales of new petrol and diesel passenger vehicles may not be permitted in the\nUnited Kingdom (UK) post-2030. Should this happen, it is likely that vehicles\npresently powered by hydrocarbons will be progressively replaced by Battery\nElectric Vehicles (BEVs). This paper describes the use of mathematical\nmodelling, drawing on real time records of the UK electricity grid, to\ninvestigate the likely performance of the grid when supplying power to a fleet\nof up to 35 million BEVs. The model highlights the importance of understanding\nhow the grid will cope when powering a BEV fleet under conditions similar to\nthose experienced during an extended wind lull during the 3rd week of January\n2017. Allowing a two-way flow of electricity between the BEVs and the grid,\nknown as the vehicle-to-grid (V2G) configuration, turns out to be of key\nimportance in minimising the need for additional gas turbine generation or\nenergy storage during wind lulls. This study has shown that with the use of\nV2G, it should be possible to provide power to about 15 million BEVs with the\ngas turbine capacity currently available. Without V2G, it is likely that the\ncurrent capacity of the gas turbines and associated gas infrastructure might be\noverwhelmed by even a relatively small BEV fleet. Since it is anticipated that\n80% of BEV owners will be able to park the vehicles at their residences,\nwidespread V2G will enable both the powering of residences when supply from the\ngrid is constrained and the charging of BEVs when supply is in excess. The\nmodel shows that this configuration will maintain a constant load on the grid\nand avoid the use of either expensive alternative storage or hydrogen obtained\nby reforming methane. There should be no insuperable problem in providing power\nto the 20% of BEV owners who do not have parking at their residences; their\npower could come directly from the grid.\n"
    },
    {
        "paper_id": 2101.01085,
        "authors": "Michele Cantarella, Andrea Neri and Maria Giovanna Ranalli",
        "title": "Mind the wealth gap: a new allocation method to match micro and macro\n  statistics for household wealth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The financial and economic crisis recently experienced by many European\ncountries has increased demand for timely, coherent and consistent\ndistributional information for the household sector. In the Euro area, most of\nthe NCBs collect such information through income and wealth surveys, which are\noften used to inform their decisions. These surveys, however, can often suffer\nfrom biases, usually caused by non-response and under-reporting behaviours,\nleading to a mismatch with macroeconomic aggregates. In this paper, we develop\na novel allocation method which combines information from a power law (Pareto)\nmodel and imputation procedures so to address these issues simultaneously, when\nonly limited external information is available. We provide two important\ncontributions: first, we adjust the weights of observed survey households for\nnon-response bias, then, we correct for measurement error. Finally, we produce\ndistributional indicators for four Euro-Area countries.\n"
    },
    {
        "paper_id": 2101.01128,
        "authors": "Shiyu Zhang and Daniel Guth",
        "title": "The OxyContin Reformulation Revisited: New Evidence From Improved\n  Definitions of Markets and Substitutes",
        "comments": "Paper was updated to cluster regression results at the MSA level",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The opioid epidemic began with prescription pain relievers. In 2010 Purdue\nPharma reformulated OxyContin to make it more difficult to abuse. OxyContin\nmisuse fell dramatically, and concurrently heroin deaths began to rise.\nPrevious research overlooked generic oxycodone and argued that the\nreformulation induced OxyContin users to switch directly to heroin. Using a\nnovel and fine-grained source of all oxycodone sales from 2006-2014, we show\nthat the reformulation led users to substitute from OxyContin to generic\noxycodone, and the reformulation had no overall impact on opioid or heroin\nmortality. In fact, generic oxycodone, instead of OxyContin, was the driving\nfactor in the transition to heroin. Finally, we show that by omitting generic\noxycodone we recover the results of the literature. These findings highlight\nthe important role generic oxycodone played in the opioid epidemic and the\nlimited effectiveness of a partial supply-side intervention.\n"
    },
    {
        "paper_id": 2101.0117,
        "authors": "Marinho Bertanha and Andrew H. McCallum and Nathan Seegert",
        "title": "Better Bunching, Nicer Notching",
        "comments": "PDF file contains main text and supplemental appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the bunching identification strategy for an elasticity\nparameter that summarizes agents' responses to changes in slope (kink) or\nintercept (notch) of a schedule of incentives. We show that current bunching\nmethods may be very sensitive to implicit assumptions in the literature about\nunobserved individual heterogeneity. We overcome this sensitivity concern with\nnew non- and semi-parametric estimators. Our estimators allow researchers to\nshow how bunching elasticities depend on different identifying assumptions and\nwhen elasticities are robust to them. We follow the literature and derive our\nmethods in the context of the iso-elastic utility model and an income tax\nschedule that creates a piece-wise linear budget constraint. We demonstrate\nbunching behavior provides robust estimates for self-employed and not-married\ntaxpayers in the context of the U.S. Earned Income Tax Credit. In contrast,\nestimates for self-employed and married taxpayers depend on specific\nidentifying assumptions, which highlight the value of our approach. We provide\nthe Stata package \"bunching\" to implement our procedures.\n"
    },
    {
        "paper_id": 2101.01261,
        "authors": "Carol Alexander, Jun Deng, Bin Zou",
        "title": "Hedging with Bitcoin Futures: The Effect of Liquidation Loss Aversion\n  and Aggressive Trading",
        "comments": "51 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider the hedging problem where a futures position can be automatically\nliquidated by the exchange without notice. We derive a semi-closed form for an\noptimal hedging strategy with dual objectives - to minimise both the variance\nof the hedged portfolio and the probability of liquidations due to insufficient\ncollateral. The optimal solution depends on the statistical characteristics of\nthe spot and futures extreme returns and parameters that characterise the\nhedger by loss aversion, choice of leverage and collateral management. An\nempirical analysis of bitcoin shows that the optimal strategy combines superior\nhedge effectiveness with a reduction in the probability of liquidation. We\ncompare the performance of seven major direct and inverse hedging instruments\ntraded on five different exchanges, based on minute-level data. We also link\nthis performance to novel speculative trading metrics, which differ markedly\nbetween venues.\n"
    },
    {
        "paper_id": 2101.01385,
        "authors": "Jiequn Han, Ruimeng Hu",
        "title": "Recurrent Neural Networks for Stochastic Control Problems with Delay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stochastic control problems with delay are challenging due to the\npath-dependent feature of the system and thus its intrinsic high dimensions. In\nthis paper, we propose and systematically study deep neural networks-based\nalgorithms to solve stochastic control problems with delay features.\nSpecifically, we employ neural networks for sequence modeling (\\emph{e.g.},\nrecurrent neural networks such as long short-term memory) to parameterize the\npolicy and optimize the objective function. The proposed algorithms are tested\non three benchmark examples: a linear-quadratic problem, optimal consumption\nwith fixed finite delay, and portfolio optimization with complete memory.\nParticularly, we notice that the architecture of recurrent neural networks\nnaturally captures the path-dependent feature with much flexibility and yields\nbetter performance with more efficient and stable training of the network\ncompared to feedforward networks. The superiority is even evident in the case\nof portfolio optimization with complete memory, which features infinite delay.\n"
    },
    {
        "paper_id": 2101.01388,
        "authors": "Angad Singh",
        "title": "A Model of Market Making and Price Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traders constantly consider the price impact associated with changing their\npositions. This paper seeks to understand how price impact emerges from the\nquoting strategies of market makers. To this end, market making is modeled as a\ndynamic auction using the mathematical framework of Stochastic Differential\nGames. In Nash Equilibrium, the market makers' quoting strategies generate a\nprice impact function that is of the same form as the celebrated Almgren-Chriss\nmodel. The key insight is that price impact is the mechanism through which\nmarket makers earn profits while matching their books. As such, price impact is\nan essential feature of markets where flow is intermediated by market makers.\n"
    },
    {
        "paper_id": 2101.01531,
        "authors": "Lee Whieldon and Huthaifa Ashqar",
        "title": "Predicting Residential Property Value in Catonsville, Maryland: A\n  Comparison of Multiple Regression Techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Predicting Residential Property Value in Catonsville, Maryland: A Comparison\nof Multiple Regression Techniques\n"
    },
    {
        "paper_id": 2101.02044,
        "authors": "Xavier Warin",
        "title": "Deep learning for efficient frontier calculation in finance",
        "comments": "23 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose deep neural network algorithms to calculate efficient frontier in\nsome Mean-Variance and Mean-CVaR portfolio optimization problems. We show that\nwe are able to deal with such problems when both the dimension of the state and\nthe dimension of the control are high. Adding some additional constraints, we\ncompare different formulations and show that a new projected feedforward\nnetwork is able to deal with some global constraints on the weights of the\nportfolio while outperforming classical penalization methods. All developed\nformulations are compared in between. Depending on the problem and its\ndimension, some formulations may be preferred.\n"
    },
    {
        "paper_id": 2101.0211,
        "authors": "Thierry Roncalli, Fatma Karray-Meziou, Fran\\c{c}ois Pan, Margaux\n  Regnault",
        "title": "Liquidity Stress Testing in Asset Management -- Part 1. Modeling the\n  Liability Liquidity Risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.14893.72165",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article is part of a comprehensive research project on liquidity risk in\nasset management, which can be divided into three dimensions. The first\ndimension covers liability liquidity risk (or funding liquidity) modeling, the\nsecond dimension focuses on asset liquidity risk (or market liquidity)\nmodeling, and the third dimension considers asset-liability liquidity risk\nmanagement (or asset-liability matching). The purpose of this research is to\npropose a methodological and practical framework in order to perform liquidity\nstress testing programs, which comply with regulatory guidelines (ESMA, 2019)\nand are useful for fund managers. The review of the academic literature and\nprofessional research studies shows that there is a lack of standardized and\nanalytical models. The aim of this research project is then to fill the gap\nwith the goal to develop mathematical and statistical approaches, and provide\nappropriate answers.\n  In this first part that focuses on liability liquidity risk modeling, we\npropose several statistical models for estimating redemption shocks. The\nhistorical approach must be complemented by an analytical approach based on\nzero-inflated models if we want to understand the true parameters that\ninfluence the redemption shocks. Moreover, we must also distinguish aggregate\npopulation models and individual-based models if we want to develop behavioral\napproaches. Once these different statistical models are calibrated, the second\nbig issue is the risk measure to assess normal and stressed redemption shocks.\nFinally, the last issue is to develop a factor model that can translate stress\nscenarios on market risk factors into stress scenarios on fund liabilities.\n"
    },
    {
        "paper_id": 2101.02287,
        "authors": "Farnoush Ronaghi, Mohammad Salimibeni, Farnoosh Naderkhani, and Arash\n  Mohammadi",
        "title": "COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information\n  Fusion Framework for Stock Price Movement Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The novel of coronavirus (COVID-19) has suddenly and abruptly changed the\nworld as we knew at the start of the 3rd decade of the 21st century.\nParticularly, COVID-19 pandemic has negatively affected financial econometrics\nand stock markets across the globe. Artificial Intelligence (AI) and Machine\nLearning (ML)-based prediction models, especially Deep Neural Network (DNN)\narchitectures, have the potential to act as a key enabling factor to reduce the\nadverse effects of the COVID-19 pandemic and future possible ones on financial\nmarkets. In this regard, first, a unique COVID-19 related PRIce MOvement\nprediction (COVID19 PRIMO) dataset is introduced in this paper, which\nincorporates effects of social media trends related to COVID-19 on stock market\nprice movements. Afterwards, a novel hybrid and parallel DNN-based framework is\nproposed that integrates different and diversified learning architectures.\nReferred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework\nfor Stock price Movement Prediction (COVID19-HPSMP), innovative fusion\nstrategies are used to combine scattered social media news related to COVID-19\nwith historical mark data. The proposed COVID19-HPSMP consists of two parallel\npaths (hence hybrid), one based on Convolutional Neural Network (CNN) with\nLocal/Global Attention modules, and one integrated CNN and Bi-directional Long\nShort term Memory (BLSTM) path. The two parallel paths are followed by a\nmultilayer fusion layer acting as a fusion centre that combines localized\nfeatures. Performance evaluations are performed based on the introduced COVID19\nPRIMO dataset illustrating superior performance of the proposed framework.\n"
    },
    {
        "paper_id": 2101.02288,
        "authors": "Masoud Ataei, Shengyuan Chen, Zijiang Yang, M.Reza Peyghami",
        "title": "Theory and Applications of Financial Chaos Index",
        "comments": null,
        "journal-ref": "PHYSA 126160 2021",
        "doi": "10.1016/j.physa.2021.126160",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a new stock market index that captures the chaos existing in the\nmarket by measuring the mutual changes of asset prices. This new index relies\non a tensor-based embedding of the stock market information, which in turn\nfrees it from the restrictive value- or capitalization-weighting assumptions\nthat commonly underlie other various popular indexes. We show that our index is\na robust estimator of the market volatility which enables us to characterize\nthe market by performing the task of segmentation with a high degree of\nreliability. In addition, we analyze the dynamics and kinematics of the\nrealized market volatility as compared to the implied volatility by introducing\na time-dependent dynamical system model. Our computational results which\npertain to the time period from January 1990 to December 2019 imply that there\nexist a bidirectional causal relation between the processes underlying the\nrealized and implied volatility of the stock market within the given time\nperiod, where it is shown that the later has a stronger causal effect on the\nformer as compared to the opposite. This result connotes that the implied\nvolatility of the market plays a key role in characterization of the market's\nrealized volatility.\n"
    },
    {
        "paper_id": 2101.02296,
        "authors": "Joseph Haimberg and Stephen Portnoy",
        "title": "Predicting CEO Compensation in Non-Controlled Public Corporations with\n  the Canonical Regression Quantile Method",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2011.08896",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The use of the Canonical Regression Quantiles Index proved that\nnon-controlled companies that engage in long-term operational and financial\ngoals post superior future performance. The Index indicates that current CEO\ncompensation influences future performance. The Index provides a method for\ndetermining CEO pay for the next 1-2 year and is a useful method to distinguish\nover/underpaid CEOs as an unbiased alternative to the peer groups comparison\nused by most compensation consultants. This determination is statistically\nweak, but future research using the Canonical Regression Quantiles with a\nlarger data set may lead to increased sensitivity and a powerful unbiased\nmethod for replacing compensation consultants who are responsible for the\ndecoupling of CEO compensation and corporate performance.\n"
    },
    {
        "paper_id": 2101.02478,
        "authors": "Sourish Dutta",
        "title": "Measurement of Global Value Chain (GVC) Participation in World\n  Development Report 2020",
        "comments": null,
        "journal-ref": "SSRN Electronic Journal, 2021",
        "doi": "10.2139/ssrn.3763103",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As we can understand with the spread of GVCs, a lot of new questions emerge\nregarding the measurement of participation and positioning in the globalised\nproduction process. The World Development Report (WDR) 2020 explains the GVC\nphenomenon and then focus on participation and the prospects especially in a\nworld of change in technology. From the overview section, we can figure out\nthat nowadays, goods and services flow across borders as intermediate inputs\nrather than final goods. In traditional trade, we need two countries with the\nnotions of export and import. However, in GVC trade, the goods and services\ncross borders multiple times requiring more than two countries. Remarkable\nimprovements in information, communication, and transport technologies have\nmade it possible to fragment production across national boundaries. So the\nquestion is: how to conceptualise this type of new trade to justify the\nmeasurement of participation.\n"
    },
    {
        "paper_id": 2101.02587,
        "authors": "Ziyuan Xia, Jeffery Chen, Anchen Sun",
        "title": "Mining the Relationship Between COVID-19 Sentiment and Market\n  Performance",
        "comments": "18 pages, 7 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0306520",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  At the beginning of the COVID-19 outbreak in March, we observed one of the\nlargest stock market crashes in history. Within the months following this, a\nvolatile bullish climb back to pre-pandemic performances and higher. In this\npaper, we study the stock market behavior during the initial few months of the\nCOVID-19 pandemic in relation to COVID-19 sentiment. Using text sentiment\nanalysis of Twitter data, we look at tweets that contain key words in relation\nto the COVID-19 pandemic and the sentiment of the tweet to understand whether\nsentiment can be used as an indicator for stock market performance. There has\nbeen previous research done on applying natural language processing and text\nsentiment analysis to understand the stock market performance, given how\nprevalent the impact of COVID-19 is to the economy, we want to further the\napplication of these techniques to understand the relationship that COVID-19\nhas with stock market performance. Our findings show that there is a strong\nrelationship to COVID-19 sentiment derived from tweets that could be used to\npredict stock market performance in the future.\n"
    },
    {
        "paper_id": 2101.02588,
        "authors": "Iraj Daizadeh",
        "title": "Leveraging latent persistency in United States patent and trademark\n  applications to gain insight into the evolution of an innovation-driven\n  economy",
        "comments": "3 Figures; 2 Tables",
        "journal-ref": "(2021) Iberoamerican Journal of Science Measurement and\n  Communication, 1(3), 1-23",
        "doi": "10.47909/ijsmc.32",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Objective: An understanding of when one or more external factors may\ninfluence the evolution of innovation tracking indices (such as US patent and\ntrademark applications (PTA)) is an important aspect of examining economic\nprogress/regress. Using exploratory statistics, the analysis uses a novel tool\nto leverage the long-range dependency (LRD) intrinsic to PTA to resolve when\nsuch factor(s) may have caused significant disruptions in the evolution of the\nindices, and thus give insight into substantive economic growth dynamics.\nApproach: This paper explores the use of the Chronological Hurst Exponent (CHE)\nto explore the LRD using overlapping time windows to quantify long-memory\ndynamics in the monthly PTA time-series spanning 1977 to 2016.\nResults/Discussion: The CHE is found to increase in a clear S-curve pattern,\nachieving persistence (H~1) from non-persistence (H~0.5). For patents, the\ninflection occurred over a span of 10 years (1980-1990), while it was much\nsharper (3 years) for trademarks (1977-1980). Conclusions/Originality/Value:\nThis analysis suggests (in part) that the rapid augmentation in R&D expenditure\nand the introduction of the various patent directed policy acts (e.g.,\nBayh-Dole, Stevenson-Wydler) are the key impetuses behind persistency, latent\nin PTA. The post-1990s exogenic factors seem to be simply maintaining the high\ndegree and consistency of the persistency metric. These findings suggest\ninvestigators should consider latent persistency when using these data and the\nCHE may be an important tool to investigate the impact of substantive exogenous\nvariables on growth dynamics.\n"
    },
    {
        "paper_id": 2101.0259,
        "authors": "Nahid Tavassoli, Hamid Noghanibehambari, Farzaneh Noghani, Mostafa\n  Toranji",
        "title": "Upswing in Industrial Activity and Infant Mortality during Late 19th\n  Century US",
        "comments": null,
        "journal-ref": "Journal of Environments, 6(1), 1-13 (2020)",
        "doi": "10.20448/journal.505.2020.61.1.13",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to assess the effects of industrial pollution on infant\nmortality between the years 1850-1940 using full count decennial censuses. In\nthis period, US economy experienced a tremendous rise in industrial activity\nwith significant variation among different counties in absorbing manufacturing\nindustries. Since manufacturing industries are shown to be the main source of\npollution, we use the share of employment at the county level in this industry\nto proxy for space-time variation in industrial pollution. Since male embryos\nare more vulnerable to external stressors like pollution during prenatal\ndevelopment, they will face higher likelihood of fetal death. Therefore, we\nproxy infant mortality with different measures of gender ratio. We show that\nthe upswing in industrial pollution during late nineteenth century and early\ntwentieth century has led to an increase in infant mortality. The results are\nconsistent and robust across different scenarios, measures for our proxies, and\naggregation levels. We find that infants and more specifically male infants had\npaid the price of pollution during upswing in industrial growth at the dawn of\nthe 20th century. Contemporary datasets are used to verify the validity of the\nproxies. Some policy implications are discussed.\n"
    },
    {
        "paper_id": 2101.02628,
        "authors": "Sandeep Ranjan",
        "title": "Analyzing the response to TV serials retelecast during COVID19 lockdown\n  in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  TV serials are a popular source of entertainment. The ongoing COVID19\nlockdown has a high probability of degrading the publics mental health. The\nGovernment of India started the retelecast of yesteryears popular TV serials on\npublic broadcaster Doordarshan from 28th March 2020 to 31st July 2020. Tweets\ncorresponding to the Doordarshan hashtag were mined to create a dataset. The\nexperiment aims to analyze the publics response to the retelecast of TV serials\nby calculating the sentiment score of the tweet dataset. Datasets mean\nsentiment score of 0.65 and high share 64.58% of positive tweets signifies the\nacceptance of Doordarshans retelecast decision. The sentiment analysis result\nalso reflects the positive state of mind of the public.\n"
    },
    {
        "paper_id": 2101.02701,
        "authors": "Mengyi Sun, Jainabou Barry Danfa, Misha Teplitskiy",
        "title": "Does double-blind peer-review reduce bias? Evidence from a top computer\n  science conference",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/asi.24582",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Peer review is widely regarded as essential for advancing scientific\nresearch. However, reviewers may be biased by authors' prestige or other\ncharacteristics. Double-blind peer review, in which the authors' identities are\nmasked from the reviewers, has been proposed as a way to reduce reviewer bias.\nAlthough intuitive, evidence for the effectiveness of double-blind peer review\nin reducing bias is limited and mixed. Here, we examine the effects of\ndouble-blind peer review on prestige bias by analyzing the peer review files of\n5027 papers submitted to the International Conference on Learning\nRepresentations (ICLR), a top computer science conference that changed its\nreviewing policy from single-blind peer review to double-blind peer review in\n2018. We find that after switching to double-blind review, the scores given to\nthe most prestigious authors significantly decreased. However, because many of\nthese papers were above the threshold for acceptance, the change did not affect\npaper acceptance decisions significantly. Nevertheless, we show that\ndouble-blind peer review may have improved the quality of the selections by\nlimiting other (non-author-prestige) biases. Specifically, papers rejected in\nthe single-blind format are cited more than those rejected under the\ndouble-blind format, suggesting that double-blind review better identifies\npoorer quality papers. Interestingly, an apparently unrelated change - the\nchange of rating scale from 10 to 4 points - likely reduced prestige bias\nsignificantly, to an extent that affected papers' acceptance. These results\nprovide some support for the effectiveness of double-blind review in reducing\nprestige bias, while opening new research directions on the impact of peer\nreview formats.\n"
    },
    {
        "paper_id": 2101.02731,
        "authors": "Max O. Souza, Yuri Thamsten",
        "title": "On regularized optimal execution problems and their singular limits",
        "comments": "23 pages, 7 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the portfolio execution problem under a framework in which\nvolatility and liquidity are both uncertain. In our model, we assume that a\nmultidimensional Markovian stochastic factor drives both of them. Moreover, we\nmodel indirect liquidity costs as temporary price impact, stipulating a power\nlaw to relate it to the agent's turnover rate. We first analyze the regularized\nsetting, in which the admissible strategies do not ensure complete execution of\nthe initial inventory. We prove the existence and uniqueness of a continuous\nand bounded viscosity solution of the Hamilton-Jacobi-Bellman equation, whence\nwe obtain a characterization of the optimal trading rate. As a byproduct of our\nproof, we obtain a numerical algorithm. Then, we analyze the constrained\nproblem, in which admissible strategies must guarantee complete execution to\nthe trader. We solve it through a monotonicity argument, obtaining the optimal\nstrategy as a singular limit of the regularized counterparts.\n"
    },
    {
        "paper_id": 2101.02736,
        "authors": "Yong Shi, Wei Dai, Wen Long, Bo Li",
        "title": "Improved ACD-based financial trade durations prediction leveraging LSTM\n  networks and Attention Mechanism",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The liquidity risk factor of security market plays an important role in the\nformulation of trading strategies. A more liquid stock market means that the\nsecurities can be bought or sold more easily. As a sound indicator of market\nliquidity, the transaction duration is the focus of this study. We concentrate\non estimating the probability density function p({\\Delta}t_(i+1) |G_i) where\n{\\Delta}t_(i+1) represents the duration of the (i+1)-th transaction, G_i\nrepresents the historical information at the time when the (i+1)-th transaction\noccurs. In this paper, we propose a new ultra-high-frequency (UHF) duration\nmodelling framework by utilizing long short-term memory (LSTM) networks to\nextend the conditional mean equation of classic autoregressive conditional\nduration (ACD) model while retaining the probabilistic inference ability. And\nthen the attention mechanism is leveraged to unveil the internal mechanism of\nthe constructed model. In order to minimize the impact of manual parameter\ntuning, we adopt fixed hyperparameters during the training process. The\nexperiments applied to a large-scale dataset prove the superiority of the\nproposed hybrid models. In the input sequence, the temporal positions which are\nmore important for predicting the next duration can be efficiently highlighted\nvia the added attention mechanism layer.\n"
    },
    {
        "paper_id": 2101.0276,
        "authors": "Peter A. Forsyth and Kenneth R. Vetzal and Graham Westmacott",
        "title": "Optimal control of the decumulation of a retirement portfolio with\n  variable spending and dynamic asset allocation",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extend the Annually Recalculated Virtual Annuity (ARVA) spending rule for\nretirement savings decumulation to include a cap and a floor on withdrawals.\nWith a minimum withdrawal constraint, the ARVA strategy runs the risk of\ndepleting the investment portfolio. We determine the dynamic asset allocation\nstrategy which maximizes a weighted combination of expected total withdrawals\n(EW) and expected shortfall (ES), defined as the average of the worst five per\ncent of the outcomes of real terminal wealth. We compare the performance of our\ndynamic strategy to simpler alternatives which maintain constant asset\nallocation weights over time accompanied by either our same modified ARVA\nspending rule or withdrawals that are constant over time in real terms. Tests\nare carried out using both a parametric model of historical asset returns as\nwell as bootstrap resampling of historical data. Consistent with previous\nliterature that has used different measures of reward and risk than EW and ES,\nwe find that allowing some variability in withdrawals leads to large\nimprovements in efficiency. However, unlike the prior literature, we also\ndemonstrate that further significant enhancements are possible through\nincorporating a dynamic asset allocation strategy rather than simply keeping\nasset allocation weights constant throughout retirement.\n"
    },
    {
        "paper_id": 2101.02778,
        "authors": "Bhaskar Krishnamachari, Qi Feng, Eugenio Grippo",
        "title": "Dynamic Curves for Decentralized Autonomous Cryptocurrency Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the exciting recent developments in decentralized finance (DeFi) has\nbeen the development of decentralized cryptocurrency exchanges that can\nautonomously handle conversion between different cryptocurrencies.\nDecentralized exchange protocols such as Uniswap, Curve and other types of\nAutomated Market Makers (AMMs) maintain a liquidity pool (LP) of two or more\nassets constrained to maintain at all times a mathematical relation to each\nother, defined by a given function or curve. Examples of such functions are the\nconstant-sum and constant-product AMMs. Existing systems however suffer from\nseveral challenges. They require external arbitrageurs to restore the price of\ntokens in the pool to match the market price. Such activities can potentially\ndrain resources from the liquidity pool. In particular, dramatic market price\nchanges can result in low liquidity with respect to one or more of the assets\nand reduce the total value of the LP. We propose in this work a new approach to\nconstructing the AMM by proposing the idea of dynamic curves. It utilizes input\nfrom a market price oracle to modify the mathematical relationship between the\nassets so that the pool price continuously and automatically adjusts to be\nidentical to the market price. This approach eliminates arbitrage opportunities\nand, as we show through simulations, maintains liquidity in the LP for all\nassets and the total value of the LP over a wide range of market prices.\n"
    },
    {
        "paper_id": 2101.02857,
        "authors": "Francis Bloch, Matthew Olckers",
        "title": "Friend-Based Ranking in Practice",
        "comments": "Forthcoming in AEA Papers & Proceedings",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A planner aims to target individuals who exceed a threshold in a\ncharacteristic, such as wealth or ability. The individuals can rank their\nfriends according to the characteristic. We study a strategy-proof mechanism\nfor the planner to use the rankings for targeting. We discuss how the mechanism\nworks in practice, when the rankings may contain errors.\n"
    },
    {
        "paper_id": 2101.02917,
        "authors": "Boris C. Boonstra, Cornelis W. Oosterlee",
        "title": "Valuation of electricity storage contracts using the COS method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Storage of electricity has become increasingly important, due to the gradual\nreplacement of fossil fuels by more variable and uncertain renewable energy\nsources. In this paper, we provide details on how to mathematically formalize a\ncorresponding electricity storage contract, taking into account the physical\nlimitations of a storage facility and the operational constraints of the\nelectricity grid. We give details of a valuation technique to price these\ncontracts, where the electricity prices follow a structural model based on a\nstochastic polynomial process. In particular, we show that the Fourier-based\nCOS method can be used to price the contracts accurately and efficiently.\n"
    },
    {
        "paper_id": 2101.03086,
        "authors": "Agostino Capponi, Jos\\'e E. Figueroa-L\\'opez, and Chuyi Yu",
        "title": "Market Making with Stochastic Liquidity Demand: Simultaneous Order\n  Arrival and Price Change Forecasts",
        "comments": "55 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an explicit characterization of the optimal market making strategy\nin a discrete-time Limit Order Book (LOB). In our model, the number of filled\norders during each period depends linearly on the distance between the\nfundamental price and the market maker's limit order quotes, with random slope\nand intercept coefficients. The high-frequency market maker (HFM) incurs an\nend-of-the-day liquidation cost resulting from linear price impact. The optimal\nplacement strategy incorporates in a novel and parsimonious way forecasts about\nfuture changes in the asset's fundamental price. We show that the randomness in\nthe demand slope reduces the inventory management motive, and that a positive\ncorrelation between demand slope and investors' reservation prices leads to\nwider spreads. Our analysis reveals that the simultaneous arrival of buy and\nsell market orders (i) reduces the shadow cost of inventory, (ii) leads the HFM\nto reduce price pressures to execute larger flows, and (iii) introduces\npatterns of nonlinearity in the intraday dynamics of bid and ask spreads. Our\nempirical study shows that the market making strategy outperforms those which\nignores randomness in demand, simultaneous arrival of buy and sell market\norders, and local drift in the fundamental price.\n"
    },
    {
        "paper_id": 2101.03087,
        "authors": "Racine Ly, Fousseini Traore, Khadim Dia",
        "title": "Forecasting Commodity Prices Using Long Short-Term Memory Neural\n  Networks",
        "comments": "13 pages, 8 figures, 7 tables, 27 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper applies a recurrent neural network (RNN) method to forecast cotton\nand oil prices. We show how these new tools from machine learning, particularly\nLong-Short Term Memory (LSTM) models, complement traditional methods. Our\nresults show that machine learning methods fit reasonably well the data but do\nnot outperform systematically classical methods such as Autoregressive\nIntegrated Moving Average (ARIMA) models in terms of out of sample forecasts.\nHowever, averaging the forecasts from the two type of models provide better\nresults compared to either method. Compared to the ARIMA and the LSTM, the Root\nMean Squared Error (RMSE) of the average forecast was 0.21 and 21.49 percent\nlower respectively for cotton. For oil, the forecast averaging does not provide\nimprovements in terms of RMSE. We suggest using a forecast averaging method and\nextending our analysis to a wide range of commodity prices.\n"
    },
    {
        "paper_id": 2101.03117,
        "authors": "Helge Liebert",
        "title": "Does external medical review reduce disability insurance inflow?",
        "comments": null,
        "journal-ref": "Journal of Health Economics, 2019, Vol. 64, 108-128",
        "doi": "10.1016/j.jhealeco.2018.12.005",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates the effects of introducing external medical review\nfor disability insurance (DI) in a system relying on treating physician\ntestimony for eligibility determination. Using a unique policy change and\nadministrative data from Switzerland, I show that medical review reduces DI\nincidence by 23%. Incidence reductions are closely tied to\ndifficult-to-diagnose conditions, suggesting inaccurate assessments by treating\nphysicians. Due to a partial benefit system, reductions in full benefit awards\nare partly offset by increases in partial benefits. More intense screening also\nincreases labor market participation. Existing benefit recipients are\ndowngraded and lose part of their benefit income when scheduled medical reviews\noccur. Back-of-the-envelope calculations indicate that external medical review\nis highly cost-effective. Under additional assumptions, the results provide a\nlower bound of the effect on the false positive award error rate.\n"
    },
    {
        "paper_id": 2101.03127,
        "authors": "Filippo Neri",
        "title": "How to Identify Investor's types in real financial markets by means of\n  agent based simulation",
        "comments": "18 pages, in press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a computational adaptation of the principles underlying\nprincipal component analysis with agent based simulation in order to produce a\nnovel modeling methodology for financial time series and financial markets.\nGoal of the proposed methodology is to find a reduced set of investor s models\n(agents) which is able to approximate or explain a target financial time\nseries. As computational testbed for the study, we choose the learning system L\nFABS which combines simulated annealing with agent based simulation for\napproximating financial time series. We will also comment on how L FABS s\narchitecture could exploit parallel computation to scale when dealing with\nmassive agent simulations. Two experimental case studies showing the efficacy\nof the proposed methodology are reported.\n"
    },
    {
        "paper_id": 2101.03128,
        "authors": "Alexandre Miot",
        "title": "Adversarial trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adversarial samples have drawn a lot of attention from the Machine Learning\ncommunity in the past few years. An adverse sample is an artificial data point\ncoming from an imperceptible modification of a sample point aiming at\nmisleading. Surprisingly, in financial research, little has been done in\nrelation to this topic from a concrete trading point of view. We show that\nthose adversarial samples can be implemented in a trading environment and have\na negative impact on certain market participants. This could have far reaching\nimplications for financial markets either from a trading or a regulatory point\nof view.\n"
    },
    {
        "paper_id": 2101.03131,
        "authors": "Marco Rogna and Diep Bich Nguyen",
        "title": "Firearms Law and Fatal Police Shootings: A Panel Data Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Among industrialized countries, U.S. holds two somehow inglorious records:\nthe highest rate of fatal police shootings and the highest rate of deaths\nrelated to firearms. The latter has been associated with strong diffusion of\nfirearms ownership largely due to loose legislation in several member states.\nThe present paper investigates the relation between firearms\nlegislation\\diffusion and the number of fatal police shooting episodes using a\nseven-year panel dataset. While our results confirm the negative impact of\nstricter firearms regulations found in previous cross-sectional studies, we\nfind that the diffusion of guns ownership has no statistically significant\neffect. Furthermore, regulations pertaining to the sphere of gun owner\naccountability seem to be the most effective in reducing fatal police\nshootings.\n"
    },
    {
        "paper_id": 2101.03138,
        "authors": "Tae Wan Kim, Matloob Khushi",
        "title": "Portfolio Optimization with 2D Relative-Attentional Gated Transformer",
        "comments": "Accepted to be published in the Proceedings of the IEEE Asia Pacific\n  Conference on Computer Science and Data Engineering 2020 (CSDE 2020)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization is one of the most attentive fields that have been\nresearched with machine learning approaches. Many researchers attempted to\nsolve this problem using deep reinforcement learning due to its efficient\ninherence that can handle the property of financial markets. However, most of\nthem can hardly be applicable to real-world trading since they ignore or\nextremely simplify the realistic constraints of transaction costs. These\nconstraints have a significantly negative impact on portfolio profitability. In\nour research, a conservative level of transaction fees and slippage are\nconsidered for the realistic experiment. To enhance the performance under those\nconstraints, we propose a novel Deterministic Policy Gradient with 2D\nRelative-attentional Gated Transformer (DPGRGT) model. Applying learnable\nrelative positional embeddings for the time and assets axes, the model better\nunderstands the peculiar structure of the financial data in the portfolio\noptimization domain. Also, gating layers and layer reordering are employed for\nstable convergence of Transformers in reinforcement learning. In our experiment\nusing U.S. stock market data of 20 years, our model outperformed baseline\nmodels and demonstrated its effectiveness.\n"
    },
    {
        "paper_id": 2101.03205,
        "authors": "Ujwal Kandi, Sasikanth Gujjula, Venkatesh Buddha and V S Bhagavan",
        "title": "Visualizing the Financial Impact of Presidential Tweets on Stock Markets",
        "comments": "10 pages, 14 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As more and more data being created every day, all of it can help take better\ndecisions with data analysis. It is not different from data generated in\nfinancial markets. Here we examine the process of how the global economy is\naffected by the market sentiment influenced by the micro-blogging data (tweets)\nof American President Donald Trump. The news feed is gathered from The Guardian\nand Bloomberg from the period between December 2016 and October 2019, which are\nused to further identify the potential tweets that influenced the markets as\nmeasured by changes in equity indices.\n"
    },
    {
        "paper_id": 2101.03214,
        "authors": "Fernando Almeida, Nelson Amoedo",
        "title": "Exploring the association between R&D expenditure and the job quality in\n  the European Union",
        "comments": "18 pages, 1 figure, 5 tables",
        "journal-ref": "Studies and Scientific Researches: Economic Edition, 2020, 32,\n  6-23",
        "doi": "10.29358/sceco.v0i32.476",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Investment in research and development is a key factor in increasing\ncountries' competitiveness. However, its impact can potentially be broader and\ninclude other socially relevant elements like job quality. In effect, the\nquantity of generated jobs is an incomplete indicator since it does not allow\nto conclude on the quality of the job generated. In this sense, this paper\nintends to explore the relevance of R&D investments for the job quality in the\nEuropean Union between 2009 and 2018. For this purpose, we investigate the\neffects of R&D expenditures made by the business sector, government, and higher\neducation sector on three dimensions of job quality. Three research methods are\nemployed, i.e. univariate linear analysis, multiple linear analysis, and\ncluster analysis. The findings only confirm the association between R&D\nexpenditure and the number of hours worked, such that the European Union\ncountries with the highest R&D expenses are those with the lowest average\nweekly working hours.\n"
    },
    {
        "paper_id": 2101.03231,
        "authors": "Ardenghi Juan Sebastian",
        "title": "Quantum credit loans",
        "comments": "15 pages",
        "journal-ref": "Physica A, 567, 125656 (2021)",
        "doi": "10.1016/j.physa.2020.125656",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum models based on the mathematics of quantum mechanics (QM) have been\ndeveloped in cognitive sciences, game theory and econophysics. In this work a\ngeneralization of credit loans is introduced by using the vector space\nformalism of QM. Operators for the debt, amortization, interest and periodic\ninstallments are defined and its mean values in an arbitrary orthonormal basis\nof the vectorial space give the corresponding values at each period of the\nloan. Endowing the vector space of dimension M, where M is the loan duration,\nwith a SO(M) symmetry, it is possible to rotate the eigenbasis to obtain better\nschedule periodic payments for the borrower, by using the rotation angles of\nthe SO(M) transformation. Given that a rotation preserves the length of the\nvectors, the total amortization, debt and periodic installments are not\nchanged. For a general description of the formalism introduced, the loan\noperator relations are given in terms of a generalized Heisenberg algebra,\nwhere finite dimensional representations are considered and commutative\noperators are defined for the specific loan types. The results obtained are an\nimprovement of the usual financial instrument of credit because introduce\nseveral degrees of freedom through the rotation angles, which allows to select\nsuperposition states of the corresponding commutative operators that enables\nthe borrower to tune the periodic installments in order to obtain better\nbenefits without changing what the lender earns.\n"
    },
    {
        "paper_id": 2101.03234,
        "authors": "Susan Martonosi and Banafsheh Behzad and Kayla Cummings",
        "title": "Pricing the COVID-19 Vaccine: A Mathematical Approach",
        "comments": "Submitted to journal on 29 Dec. 2020",
        "journal-ref": null,
        "doi": "10.1016/j.omega.2021.102451",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  According to the World Health Organization, development of the COVID-19\nvaccine is occurring in record time. Administration of the vaccine has started\nthe same year as the declaration of the COVID-19 pandemic. The United Nations\nemphasized the importance of providing COVID-19 vaccines as \"a global public\ngood\", which is accessible and affordable world-wide. Pricing the COVID-19\nvaccines is a controversial topic. We use optimization and game theoretic\napproaches to model the COVID-19 U.S. vaccine market as a duopoly with two\nmanufacturers Pfizer-BioNTech and Moderna. The results suggest that even in the\ncontext of very high production and distribution costs, the government can\nnegotiate prices with the manufacturers to keep public sector prices as low as\npossible while meeting demand and ensuring each manufacturer earns a target\nprofit. Furthermore, these prices are consistent with those currently predicted\nin the media.\n"
    },
    {
        "paper_id": 2101.03239,
        "authors": "Jonghyeon Min",
        "title": "Comparison of the effects of investor attention using search volume data\n  before and after mobile device popularization",
        "comments": "37 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we will study investor attention measurement using the Search\nVolume Index in the recent market. Since 2009, the popularity of mobile devices\nand the spread of the Internet have made the speed of information delivery\nfaster and the investment information retrieval data for obtaining investment\ninformation has increased dramatically. In these circumstances, investor\nattention measurement using search volume data can be measured more accurately\nand faster than before mobile device popularization. To confirm this, we will\ncompare the effect of measuring investor attention using search volume data\nbefore and after mobile device popularization. In addition, it is confirmed\nthat the measured investor attention is that of retail traders, not\ninstitutional traders or professional traders, and the relationship between\ninvestor attention and short-term price pressure theory. Using SVI data\nprovided by Google Trends, we will experiment with Russell 3000 stocks and IPO\nstocks and compare the results. In addition, the results of investigating the\ninvestor's interest using the search volume data from various angles through\nexperiments such as the comparison of the results based on the inclusion of the\nnoise ticker group, the comparison of the limitations of the existing investor\nattention measurement method, and the comparison of explanatory variables with\nexisting IPO related studies. We would like to verify its practicality and\nsignificance.\n"
    },
    {
        "paper_id": 2101.03259,
        "authors": "Hossein Abbaszadeh Shahri",
        "title": "Ramadan and Infants Health Outcomes",
        "comments": null,
        "journal-ref": "Journal of Economic and Social Thought, 7(4), 225-233 (2021)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous studies show that prenatal shocks to embryos could have adverse\nimpacts on health endowment at birth. Using the universe of birth data and a\ndifference-in-difference-in-difference strategy, I find that exposure to\nRamadan during prenatal development has negative birth outcomes. Exposure to a\nfull month of fasting is associated with 96 grams lower birth-weight. These\nresults are robust across specifications and do not appear to be driven by\nmothers selective fertility.\n"
    },
    {
        "paper_id": 2101.03358,
        "authors": "Sourish Dutta",
        "title": "Mechanistic Framework of Global Value Chains",
        "comments": null,
        "journal-ref": "SSRN Electronic Journal, 2021",
        "doi": "10.2139/ssrn.3762963",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Indeed, the global production (as a system of creating values) is eventually\nforming like a gigantic and complex network/web of value chains that explains\nthe transitional structures of global trade and development of the global\neconomy. It's truly a new wave of globalisation, and we term it as the global\nvalue chains (GVCs), creating the nexus among firms, workers and consumers\naround the globe. The emergence of this new scenario asks: how an economy's\nfirms, producers and workers connect in the global economy. And how are they\ncapturing the gains out of it in terms of different dimensions of economic\ndevelopment? This GVC approach is very crucial for understanding the\norganisation of the global industries and firms. It requires the statics and\ndynamics of diverse players involved in this complex global production network.\nIts broad notion deals with different global issues (including regional value\nchains also) from the top down to the bottom up, founding a scope for policy\nanalysis (Gereffi & Fernandez-Stark 2011). But it is true that, as Feenstra\n(1998) points out, any single computational framework is not sufficient to\nquantification this whole range of economic activities. We should adopt an\nintegrative framework for accurate projection of this dynamic multidimensional\nphenomenon.\n"
    },
    {
        "paper_id": 2101.03418,
        "authors": "Sophia Gu",
        "title": "Deep Reinforcement Learning with Function Properties in Mean Reversion\n  Strategies",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Over the past decades, researchers have been pushing the limits of Deep\nReinforcement Learning (DRL). Although DRL has attracted substantial interest\nfrom practitioners, many are blocked by having to search through a plethora of\navailable methodologies that are seemingly alike, while others are still\nbuilding RL agents from scratch based on classical theories. To address the\naforementioned gaps in adopting the latest DRL methods, I am particularly\ninterested in testing out if any of the recent technology developed by the\nleads in the field can be readily applied to a class of optimal trading\nproblems. Unsurprisingly, many prominent breakthroughs in DRL are investigated\nand tested on strategic games: from AlphaGo to AlphaStar and at about the same\ntime, OpenAI Five. Thus, in this writing, I want to show precisely how to use a\nDRL library that is initially built for games in a fundamental trading problem;\nmean reversion. And by introducing a framework that incorporates\neconomically-motivated function properties, I also demonstrate, through the\nlibrary, a highly-performant and convergent DRL solution to decision-making\nfinancial problems in general.\n"
    },
    {
        "paper_id": 2101.03598,
        "authors": "Lorenc Kociu and Kledian Kodra",
        "title": "Using the Econometric Models for Identification of Risk Factors for\n  Albanian SMEs (Case study: SMEs of Gjirokastra region)",
        "comments": "8 pages",
        "journal-ref": "WSEAS TRANSACTIONS on BUSINESS and ECONOMICS, 2021",
        "doi": "10.37394/23207.2021.18.17",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using the econometric models, this paper addresses the ability of Albanian\nSmall and Medium-sized Enterprises (SMEs) to identify the risks they face. To\nwrite this paper, we studied SMEs operating in the Gjirokastra region. First,\nqualitative data gathered through a questionnaire was used. Next, the 5-level\nLikert scale was used to measure it. Finally, the data was processed through\nstatistical software SPSS version 21, using the binary logistic regression\nmodel, which reveals the probability of occurrence of an event when all\nindependent variables are included. Logistic regression is an integral part of\na category of statistical models, which are called General Linear Models.\nLogistic regression is used to analyze problems in which one or more\nindependent variables interfere, which influences the dichotomous dependent\nvariable. In such cases, the latter is seen as the random variable and is\ndependent on them. To evaluate whether Albanian SMEs can identify risks, we\nanalyzed the factors that SMEs perceive as directly affecting the risks they\nface. At the end of the paper, we conclude that Albanian SMEs can identify risk\n"
    },
    {
        "paper_id": 2101.03625,
        "authors": "Min Shu, Ruiqiang Song, Wei Zhu",
        "title": "The 'COVID' Crash of the 2020 U.S. Stock Market",
        "comments": "19 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2101.00327",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employed the log-periodic power law singularity (LPPLS) methodology to\nsystematically investigate the 2020 stock market crash in the U.S. equities\nsectors with different levels of total market capitalizations through four\nmajor U.S. stock market indexes, including the Wilshire 5000 Total Market\nindex, the S&P 500 index, the S&P MidCap 400 index, and the Russell 2000 index,\nrepresenting the stocks overall, the large capitalization stocks, the middle\ncapitalization stocks and the small capitalization stocks, respectively. During\nthe 2020 U.S. stock market crash, all four indexes lost more than a third of\ntheir values within five weeks, while both the middle capitalization stocks and\nthe small capitalization stocks have suffered much greater losses than the\nlarge capitalization stocks and stocks overall. Our results indicate that the\nprice trajectories of these four stock market indexes prior to the 2020 stock\nmarket crash have clearly featured the obvious LPPLS bubble pattern and were\nindeed in a positive bubble regime. Contrary to the popular belief that the\nCOVID-19 led to the 2020 stock market crash, the 2020 U.S. stock market crash\nwas endogenous, stemming from the increasingly systemic instability of the\nstock market itself. We also performed the complementary post-mortem analysis\nof the 2020 U.S. stock market crash. Our analyses indicate that the 2020 U.S.\nstock market crash originated from a bubble which began to form as early as\nSeptember 2018; and the bubbles in stocks with different levels of total market\ncapitalizations have significantly different starting time profiles. This study\nnot only sheds new light on the making of the 2020 U.S. stock market crash but\nalso creates a novel pipeline for future real-time crash detection and\nmechanism dissection of any financial market and/or economic index.\n"
    },
    {
        "paper_id": 2101.03626,
        "authors": "Ben Boukai",
        "title": "On the RND under Heston's stochastic volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider Heston's (1993) stochastic volatility model for valuation of\nEuropean options to which (semi) closed form solutions are available and are\ngiven in terms of characteristic functions. We prove that the class of\nscale-parameter distributions with mean being the forward spot price satisfies\nHeston's solution. Thus, we show that any member of this class could be used\nfor the direct risk-neutral valuation of the option price under Heston's SV\nmodel. In fact, we also show that any RND with mean being the forward spot\nprice that satisfies Hestons' option valuation solution, must be a member of a\nscale-family of distributions in that mean. As particular examples, we show\nthat one-parameter versions of the {\\it Log-Normal, Inverse-Gaussian, Gamma,\nWeibull} and the {\\it Inverse-Weibull} distributions are all members of this\nclass and thus provide explicit risk-neutral densities (RND) for Heston's\npricing model. We demonstrate, via exact calculations and Monte-Carlo\nsimulations, the applicability and suitability of these explicit RNDs using\nalready published Index data with a calibrated Heston model (S\\&P500, Bakshi,\nCao and Chen (1997), and ODAX, Mr\\'azek and Posp\\'i\\v{s}il (2017)), as well as\ncurrent option market data (AMD).\n"
    },
    {
        "paper_id": 2101.03759,
        "authors": "Bruno Bouchard, Gr\\'egoire Loeper, Xiaolu Tan",
        "title": "A $C^{0,1}$-functional It\\^o's formula and its applications in\n  mathematical finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Dupire's notion of vertical derivative, we provide a functional\n(path-dependent) extension of the It\\^o's formula of Gozzi and Russo (2006)\nthat applies to C^{0,1}-functions of continuous weak Dirichlet processes. It is\nmotivated and illustrated by its applications to the hedging or superhedging\nproblems of path-dependent options in mathematical finance, in particular in\nthe case of model uncertainty\n"
    },
    {
        "paper_id": 2101.03867,
        "authors": "Mehran Taghian, Ahmad Asadi, Reza Safabakhsh",
        "title": "A Reinforcement Learning Based Encoder-Decoder Framework for Learning\n  Stock Trading Rules",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A wide variety of deep reinforcement learning (DRL) models have recently been\nproposed to learn profitable investment strategies. The rules learned by these\nmodels outperform the previous strategies specially in high frequency trading\nenvironments. However, it is shown that the quality of the extracted features\nfrom a long-term sequence of raw prices of the instruments greatly affects the\nperformance of the trading rules learned by these models. Employing a neural\nencoder-decoder structure to extract informative features from complex input\ntime-series has proved very effective in other popular tasks like neural\nmachine translation and video captioning in which the models face a similar\nproblem. The encoder-decoder framework extracts highly informative features\nfrom a long sequence of prices along with learning how to generate outputs\nbased on the extracted features. In this paper, a novel end-to-end model based\non the neural encoder-decoder framework combined with DRL is proposed to learn\nsingle instrument trading strategies from a long sequence of raw prices of the\ninstrument. The proposed model consists of an encoder which is a neural\nstructure responsible for learning informative features from the input\nsequence, and a decoder which is a DRL model responsible for learning\nprofitable strategies based on the features extracted by the encoder. The\nparameters of the encoder and the decoder structures are learned jointly, which\nenables the encoder to extract features fitted to the task of the decoder DRL.\nIn addition, the effects of different structures for the encoder and various\nforms of the input sequences on the performance of the learned strategies are\ninvestigated. Experimental results showed that the proposed model outperforms\nother state-of-the-art models in highly dynamic environments.\n"
    },
    {
        "paper_id": 2101.03891,
        "authors": "Andrew Spurr and Marcel Ausloos",
        "title": "Challenging Practical Features of Bitcoin by the Main Altcoins",
        "comments": "33 pages; 50 references; 3 figures; 3 tables; to be published in\n  Quality and Quantity",
        "journal-ref": null,
        "doi": "10.1007/s11135-020-01062-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the fundamental differences that separate: Litecoin; Bitcoin Gold;\nBitcoin Cash; Ethereum; and Zcash from Bitcoin, and draw analysis to how these\nfeatures are appreciated by the market, to ultimately make an inference as to\nhow future successful cryptocurrencies may behave. We use Google Trend data, as\nwell as price, volume and market capitalization data sourced from\ncoinmarketcap.com to support this analysis. We find that Litecoin's shorter\nblock times offer benefits in commerce, but drawbacks in the mining process\nthrough orphaned blocks. Zcash holds a niche use for anonymous transactions,\nbenefitting areas of the world lacking in economic freedom. Bitcoin Cash\nsuffers from centralization in the mining process, while the greater\ndecentralization of Bitcoin Gold has generally left it to stagnate. Ether's\ngreater functionality offers the greatest threat to Bitcoin's dominance in the\nmarket. A coin that incorporates several of these features can be technically\nbetter than Bitcoin, but the first-to-marketadvantage of Bitcoin should keep\nits dominant position in the market.\n"
    },
    {
        "paper_id": 2101.03943,
        "authors": "Hamid NoghaniBehambari, Farzaneh Noghani, Nahid Tavassoli",
        "title": "Early-life Income Shocks and Old-Age Cause-Specific Mortality",
        "comments": null,
        "journal-ref": "Economic Analysis 2020",
        "doi": "10.28934/ea.20.53.2.pp1-19",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the causal relationship between income shocks during\nthe first years of life and adulthood mortality due to specific causes of\ndeath. Using all death records in the United States during 1968-2004 for\nindividuals who were born in the first half of the 20th century, we document a\nsizable and statistically significant association between income shocks early\nin life, proxied by GDP per capita fluctuations, and old age cause-specific\nmortality. Conditional on individual characteristics and controlling for a\nbroad array of current and early-life conditions, we find that a 1 percent\ndecrease in the aggregate business cycle in the year of birth is associated\nwith 2.2, 2.3, 3.1, 3.7, 0.9, and 2.1 percent increase in the likelihood of\nmortality in old ages due to malignant neoplasms, Diabetes Mellitus,\ncardiovascular diseases, Influenza, chronic respiratory diseases, and all other\ndiseases, respectively.\n"
    },
    {
        "paper_id": 2101.03954,
        "authors": "Yang Shen and Bin Zou",
        "title": "Mean-Variance Investment and Risk Control Strategies -- A\n  Time-Consistent Approach via A Forward Auxiliary Process",
        "comments": "29 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider an optimal investment and risk control problem for an insurer\nunder the mean-variance (MV) criterion. By introducing a deterministic\nauxiliary process defined forward in time, we formulate an alternative\ntime-consistent problem related to the original MV problem, and obtain the\noptimal strategy and the value function to the new problem in closed-form. We\ncompare our formulation and optimal strategy to those under the precommitment\nand game-theoretic framework. Numerical studies show that, when the financial\nmarket is negatively correlated with the risk process, optimal investment may\ninvolve short selling the risky asset and, if that happens, a less risk averse\ninsurer short sells more risky asset.\n"
    },
    {
        "paper_id": 2101.04023,
        "authors": "Javier Gonzalez-Conde, \\'Angel Rodr\\'iguez-Rozas, Enrique Solano,\n  Mikel Sanz",
        "title": "Efficient Hamiltonian Simulation for Solving Option Price Dynamics",
        "comments": null,
        "journal-ref": "Phys. Rev. Research 5, 043220 (2023)",
        "doi": "10.1103/PhysRevResearch.5.043220",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing financial derivatives, in particular European-style options at\ndifferent time-maturities and strikes, means a relevant problem in finance. The\ndynamics describing the price of vanilla options when constant volatilities and\ninterest rates are assumed, is governed by the Black-Scholes model, a linear\nparabolic partial differential equation with terminal value given by the\npay-off of the option contract and no additional boundary conditions. Here, we\npresent a digital quantum algorithm to solve Black-Scholes equation on a\nquantum computer by mapping it to the Schr\\\"odinger equation. The non-Hermitian\nnature of the resulting Hamiltonian is solved by embedding its propagator into\nan enlarged Hilbert space by using only one additional ancillary qubit.\nMoreover, due to the choice of periodic boundary conditions, given by the\ndefinition of the discretized momentum operator, we duplicate the initial\ncondition, which substantially improves the stability and performance of the\nprotocol. The algorithm shows a feasible approach for using efficient\nHamiltonian simulation techniques as Quantum Signal Processing to solve the\nprice dynamics of financial derivatives on a digital quantum computer. Our\napproach differs from those based on Monte Carlo integration, exclusively\nfocused on sampling the solution assuming the dynamics is known. We report\nexpected accuracy levels comparable to classical numerical algorithms by using\n9 qubits to simulate its dynamics on a fault-tolerant quantum computer, and an\nexpected success probability of the post-selection procedure due to the\nembedding protocol above 60%.\n"
    },
    {
        "paper_id": 2101.04113,
        "authors": "Jonathan Tuck and Shane Barratt and Stephen Boyd",
        "title": "Portfolio Construction Using Stratified Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we develop models of asset return mean and covariance that\ndepend on some observable market conditions, and use these to construct a\ntrading policy that depends on these conditions, and the current portfolio\nholdings. After discretizing the market conditions, we fit Laplacian\nregularized stratified models for the return mean and covariance. These models\nhave a different mean and covariance for each market condition, but are\nregularized so that nearby market conditions have similar models. This\ntechnique allows us to fit models for market conditions that have not occurred\nin the training data, by borrowing strength from nearby market conditions for\nwhich we do have data. These models are combined with a Markowitz-inspired\noptimization method to yield a trading policy that is based on market\nconditions. We illustrate our method on a small universe of 18 ETFs, using\nthree well known and publicly available market variables to construct 1000\nmarket conditions, and show that it performs well out of sample. The method,\nhowever, is general, and scales to much larger problems, that presumably would\nuse proprietary data sources and forecasts along with publicly available data.\n"
    },
    {
        "paper_id": 2101.0428,
        "authors": "Santosh Kumar Radha",
        "title": "Quantum option pricing using Wick rotated imaginary time evolution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we reformulate the problem of pricing options in a quantum\nsetting. Our proposed algorithm involves preparing an initial state,\nrepresenting the option price, and then evolving it using existing imaginary\ntime simulation algorithms. This way of pricing options boils down to mapping\nan initial option price to a quantum state and then simulating the time\ndependence in Wick's imaginary time space. We numerically verify our algorithm\nfor European options using a particular imaginary time evolution algorithm as\nproof of concept and show how it can be extended to path dependent options like\nAsian options. As the proposed method uses a hybrid variational algorithm, it\nis bound to be relevant for near-term quantum computers.\n"
    },
    {
        "paper_id": 2101.04308,
        "authors": "Karol Gellert and Erik Schl\\\"ogl",
        "title": "Short Rate Dynamics: A Fed Funds and SOFR perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Secured Overnight Funding Rate (SOFR) is becoming the main Risk-Free Rate\nbenchmark in US dollars, thus interest rate term structure models need to be\nupdated to reflect the key features exhibited by the dynamics of SOFR and the\nforward rates implied by SOFR futures. Historically, interest rate term\nstructure modelling has been based on rates of substantially longer time to\nmaturity than overnight, but with SOFR the overnight rate now is the primary\nmarket observable. This means that the empirical idiosyncrasies of the\novernight rate cannot be ignored when constructing interest rate models in a\nSOFR-based world.\n  As a rate reflecting transactions in the Treasury overnight repurchase\nmarket, the dynamics of SOFR are closely linked to the dynamics of the\nEffective Federal Funds Rate (EFFR), which is the interest rate most directly\nimpacted by US monetary policy target rate decisions. Therefore, these rates\nfeature jumps at known times (Federal Open Market Committee meeting dates), and\nmarket expectations of these jumps are reflected in prices for futures written\non these rates. On the other hand, forward rates implied by Fed Funds and SOFR\nfutures continue to evolve diffusively. The model presented in this paper\nreflects the key empirical features of SOFR dynamics and is calibrated to\nfutures prices. In particular, the model reconciles diffusive forward rate\ndynamics with piecewise constant paths of the target short rate.\n"
    },
    {
        "paper_id": 2101.04447,
        "authors": "Sourish Dutta",
        "title": "Learning and Upgrading in Global Value Chains: An Analysis of India's\n  Manufacturing Sector",
        "comments": "26 pages, PhD Proposal",
        "journal-ref": "SSRN Electronic Journal, 24 Jun 2020",
        "doi": "10.2139/ssrn.3615725",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The topic of my research is \"Learning and Upgrading in Global Value Chains:\nAn Analysis of India's Manufacturing Sector\". To analyse India's learning and\nupgrading through position, functions, specialisation & value addition of\nmanufacturing GVCs, it is required to quantify the extent, drivers, and impacts\nof India's Manufacturing links in GVCs. I have transformed this overall broad\nobjective into three fundamental questions: (1) What is the extent of India's\nManufacturing Links in GVCs? (2) What are the determinants of India's\nManufacturing Links in GVCs? (3) What are the impacts of India's Manufacturing\nLinks in GVCs? These three objectives represent my three chapters in my PhD\nthesis.\n"
    },
    {
        "paper_id": 2101.0448,
        "authors": "Sridhar Ravula",
        "title": "Text analysis in financial disclosures",
        "comments": "24 pages, 1 figure, Text analysis in financial disclosure analysis\n  survey",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial disclosure analysis and Knowledge extraction is an important\nfinancial analysis problem. Prevailing methods depend predominantly on\nquantitative ratios and techniques, which suffer from limitations like window\ndressing and past focus. Most of the information in a firm's financial\ndisclosures is in unstructured text and contains valuable information about its\nhealth. Humans and machines fail to analyze it satisfactorily due to the\nenormous volume and unstructured nature, respectively. Researchers have started\nanalyzing text content in disclosures recently. This paper covers the previous\nwork in unstructured data analysis in Finance and Accounting. It also explores\nthe state of art methods in computational linguistics and reviews the current\nmethodologies in Natural Language Processing (NLP). Specifically, it focuses on\nresearch related to text source, linguistic attributes, firm attributes, and\nmathematical models employed in the text analysis approach. This work\ncontributes to disclosure analysis methods by highlighting the limitations of\nthe current focus on sentiment metrics and highlighting broader future research\nareas\n"
    },
    {
        "paper_id": 2101.04529,
        "authors": "Francesco Fallucchi, Marc Kaufmann",
        "title": "Narrow Bracketing in Work Choices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many important economic outcomes result from cumulative effects of smaller\nchoices, so the best outcomes require accounting for other choices at each\ndecision point. We document narrow bracketing -- the neglect of such accounting\n-- in work choices in a pre-registered experiment on MTurk: bracketing changes\naverage willingness to work by 13-28%. In our experiment, broad bracketing is\nso simple to implement that narrow bracketing cannot possibly be due to optimal\nconservation of cognitive resources, so it must be suboptimal. We jointly\nestimate disutility of work and bracketing, finding gender differences in\nconvexity of disutility, but not in bracketing.\n"
    },
    {
        "paper_id": 2101.04604,
        "authors": "Will Hicks",
        "title": "Wild Randomness, and the application of Hyperbolic Diffusion in\n  Financial Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The application of the Cauchy distribution has often been discussed as a\npotential model of the financial markets. In particular the way in which single\nextreme, or \"Black Swan\", events can impact long term historical moments, is\noften cited. In this article we show how one can construct Martingale\nprocesses, which have marginal distributions that tend to the Cauchy\ndistribution in the large volatility limit. This provides financial\njustification to approaches discussed by other authors, and highlights an\nexample of how quantum probability can be used to construct non-Gaussian\nMartingales. We go on to illustrate links with hyperbolic diffusion, and\ndiscuss the insight this provides.\n"
    },
    {
        "paper_id": 2101.04618,
        "authors": "Yi Liu, Pinar Yildirim, Z. John Zhang",
        "title": "Social Media, Content Moderation, and Technology",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1287/mksc.2022.1361",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a theoretical model to study the economic incentives for\na social media platform to moderate user-generated content. We show that a\nself-interested platform can use content moderation as an effective marketing\ntool to expand its installed user base, to increase the utility of its users,\nand to achieve its positioning as a moderate or extreme content platform. The\noptimal content moderation strategy differs for platforms with different\nrevenue models, advertising or subscription. We also show that a platform's\ncontent moderation strategy depends on its technical sophistication. Because of\nimperfect technology, a platform may optimally throw away the moderate content\nmore than the extreme content. Therefore, one cannot judge how extreme a\nplatform is by just looking at its content moderation strategy. Furthermore, we\nshow that a platform under advertising does not necessarily benefit from a\nbetter technology for content moderation, but one under subscription does. This\nmeans that platforms under different revenue models can have different\nincentives to improve their content moderation technology. Finally, we draw\nmanagerial and policy implications from our insights.\n"
    },
    {
        "paper_id": 2101.04975,
        "authors": "Matteo Brachetta and Claudia Ceci",
        "title": "Optimal reinsurance problem under fixed cost and exponential preferences",
        "comments": "18 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate an optimal reinsurance problem for an insurance company facing\na constant fixed cost when the reinsurance contract is signed. The insurer\nneeds to optimally choose both the starting time of the reinsurance contract\nand the retention level in order to maximize the expected utility of terminal\nwealth. This leads to a mixed optimal control/optimal stopping time problem,\nwhich is solved by a two-step procedure: first considering the pure reinsurance\nstochastic control problem and next discussing a time-inhomogeneous optimal\nstopping problem with discontinuous reward. Using the classical\nCram\\'er-Lundberg approximation risk model, we prove that the optimal strategy\nis deterministic and depends on the model parameters. In particular, we show\nthat there exists a maximum fixed cost that the insurer is willing to pay for\nthe contract activation. Finally, we provide some economical interpretations\nand numerical simulations.\n"
    },
    {
        "paper_id": 2101.0501,
        "authors": "Yan Zhang, Frank Schweitzer",
        "title": "Quantifying the importance of firms by means of reputation and network\n  control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The reputation of firms is largely channeled through their ownership\nstructure. We use this relation to determine reputation spillovers between\ntransnational companies and their participated companies in an ownership\nnetwork core of 1318 firms. We then apply concepts of network controllability\nto identify minimum sets of driver nodes (MDS) of 314 firms in this network.\nThe importance of these driver nodes is classified regarding their control\ncontribution, their operating revenue, and their reputation. The latter two are\nalso taken as proxies for the access costs when utilizing firms as driver\nnodes. Using an enrichment analysis, we find that firms with high reputation\nmaintain the controllability of the network, but rarely become top drivers,\nwhereas firms with medium reputation most likely become top driver nodes. We\nfurther show that MDSs with lower access costs can be used to control the\nreputation dynamics in the whole network.\n"
    },
    {
        "paper_id": 2101.05249,
        "authors": "Wei Li and Denis Mike Becker",
        "title": "Day-ahead electricity price prediction applying hybrid models of\n  LSTM-based deep learning methods and feature selection algorithms under\n  consideration of market coupling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The availability of accurate day-ahead electricity price forecasts is pivotal\nfor electricity market participants. In the context of trade liberalisation and\nmarket harmonisation in the European markets, accurate price forecasting\nbecomes difficult for electricity market participants to obtain because\nelectricity forecasting requires the consideration of features from\never-growing coupling markets. This study provides a method of exploring the\ninfluence of market coupling on electricity price prediction. We apply\nstate-of-the-art long short-term memory (LSTM) deep neural networks combined\nwith feature selection algorithms for electricity price prediction under the\nconsideration of market coupling. LSTM models have a good performance in\nhandling nonlinear and complex problems and processing time series data. In our\nempirical study of the Nordic market, the proposed models obtain considerably\naccurate results. The results show that feature selection is essential to\nachieving accurate prediction, and features from integrated markets have an\nimpact on prediction. The feature importance analysis implies that the German\nmarket has a salient role in the price generation of Nord Pool.\n"
    },
    {
        "paper_id": 2101.05364,
        "authors": "Hamid NoghaniBehambari, Nahid Tavassoli, Farzaneh Noghani",
        "title": "Intergenerational transmission of culture among immigrants: Gender gap\n  in education among first and second generations",
        "comments": null,
        "journal-ref": "Journal of Economics and Political Economy, 7(4), 284-318 (2020)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper illustrates the intergenerational transmission of the gender gap\nin education among first and second-generation immigrants. Using the Current\nPopulation Survey (1994-2018), we find that the difference in female-male\neducation persists from the home country to the new environment. A one standard\ndeviation increase of the ancestral country female-male difference in schooling\nis associated with 17.2% and 2.5% of a standard deviation increase in the\ngender gap among first and second generations, respectively. Since gender\nperspective in education uncovers a new channel for cultural transmission among\nfamilies, we interpret the findings as evidence of cultural persistence among\nfirst generations and partial cultural assimilation of second generations.\nMoreover, Disaggregation into country-groups reveals different paths for this\ntransmission: descendants of immigrants of lower-income countries show fewer\nattachments to the gender opinions of their home country. Average local\neducation of natives can facilitate the acculturation process. Immigrants\nresiding in states with higher education reveal a lower tendency to follow\ntheir home country attitudes regarding the gender gap.\n"
    },
    {
        "paper_id": 2101.05365,
        "authors": "Mike Lindow, David DeFranza, Arul Mishra, Himanshu Mishra",
        "title": "Scared into Action: How Partisanship and Fear are Associated with\n  Reactions to Public Health Directives",
        "comments": "54 pages, 11 figures",
        "journal-ref": null,
        "doi": "10.31234/osf.io/8me7q",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Differences in political ideology are increasingly appearing as an impediment\nto successful bipartisan communication from local leadership. For example,\nrecent empirical findings have shown that conservatives are less likely to\nadhere to COVID-19 health directives. This behavior is in direct contradiction\nto past research which indicates that conservatives are more rule abiding,\nprefer to avoid loss, and are more prevention-motivated than liberals. We\nreconcile this disconnect between recent empirical findings and past research\nby using insights gathered from press releases, millions of tweets, and\nmobility data capturing local movement in retail, grocery, workplace, parks,\nand transit domains during COVID-19 shelter-in-place orders. We find that\nconservatives adhere to health directives when they express more fear of the\nvirus. In order to better understand this phenomenon, we analyze both official\nand citizen communications and find that press releases from local and federal\ngovernment, along with the number of confirmed COVID-19 cases, lead to an\nincrease in expressions of fear on Twitter.\n"
    },
    {
        "paper_id": 2101.0558,
        "authors": "Frank Schweitzer, Luca Verginer, Giacomo Vaccario",
        "title": "Should the government reward cooperation? Insights from an agent-based\n  model of wealth redistribution",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0219525920500186",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our multi-agent model agents generate wealth from repeated interactions\nfor which a prisoner's dilemma payoff matrix is assumed. Their gains are taxed\nby a government at a rate $\\alpha$. The resulting budget is spent to cover\nadministrative costs and to pay a bonus to cooperative agents, which can be\nidentified correctly only with a probability $p$. Agents decide at each time\nstep to choose either cooperation or defection based on different information.\nIn the local scenario, they compare their potential gains from both strategies.\nIn the global scenario, they compare the gains of the cooperative and defective\nsubpopulations. We derive analytical expressions for the critical bonus needed\nto make cooperation as attractive as defection. We show that for the local\nscenario the government can establish only a medium level of cooperation,\nbecause the critical bonus increases with the level of cooperation. In the\nglobal scenario instead full cooperation can be achieved once the cold-start\nproblem is solved, because the critical bonus decreases with the level of\ncooperation. This allows to lower the tax rate, while maintaining high\ncooperation.\n"
    },
    {
        "paper_id": 2101.05588,
        "authors": "Federico Guglielmo Morelli, Michael Benzaquen, Jean-Philippe Bouchaud\n  and Marco Tarzia",
        "title": "Crisis Propagation in a Heterogeneous Self-Reflexive DSGE Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a self-reflexive DSGE model with heterogeneous households, aimed at\ncharacterising the impact of economic recessions on the different strata of the\nsociety. Our framework allows to analyse the combined effect of income\ninequalities and confidence feedback mediated by heterogeneous social networks.\nBy varying the parameters of the model, we find different crisis typologies:\nloss of confidence may propagate mostly within high income households, or\nmostly within low income households, with a rather sharp crossover between the\ntwo. We find that crises are more severe for segregated networks (where\nconfidence feedback is essentially mediated between agents of the same social\nclass), for which cascading contagion effects are stronger. For the same\nreason, larger income inequalities tend to reduce, in our model, the\nprobability of global crises. Finally, we are able to reproduce a perhaps\ncounter-intuitive empirical finding: in countries with higher Gini\ncoefficients, the consumption of the lowest income households tends to drop\nless than that of the highest incomes in crisis times.\n"
    },
    {
        "paper_id": 2101.05655,
        "authors": "Alexey A. Burluka",
        "title": "Dynamics of contentment",
        "comments": null,
        "journal-ref": "Physica D: Nonlinear Phenomena 2021 133012",
        "doi": "10.1016/j.physd.2021.133012",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A continuous variable changing between 0 and 1 is introduced to characterise\ncontentment, or satisfaction with life, of an individual and an equation\ngoverning its evolution is postulated from analysis of several factors likely\nto affect the contentment. As contentment is strongly affected by material\nwell-being, a similar equation is formulated for wealth of an individual and\nfrom these two equations derived an evolution equation for the joint\ndistribution of individuals' wealth and contentment within a society. The\nequation so obtained is used to compute evolution of this joint distribution in\na society with initially low variation of wealth and contentment over a long\nperiod time. As illustration of this model capabilities, effects of the wealth\ntax rate are simulated and it is shown that a higher taxation in the longer run\nmay lead to a wealthier and more content society. It is also shown that lower\nrates of the wealth tax lead to pronounced stratification of the society in\nterms of both wealth and contentment and that there is no direct relationship\nbetween the average values of these two variables.\n"
    },
    {
        "paper_id": 2101.05744,
        "authors": "L\\'aszl\\'o Csat\\'o",
        "title": "A comparative study of scoring systems by simulations",
        "comments": "17 pages, 4 figures, 6 tables",
        "journal-ref": "Journal of Sports Economics, 24(4): 526-545, 2023",
        "doi": "10.1177/15270025221134241",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scoring rules aggregate individual rankings by assigning some points to each\nposition in each ranking such that the total sum of points provides the overall\nranking of the alternatives. They are widely used in sports competitions\nconsisting of multiple contests. We study the tradeoff between two risks in\nthis setting: (1) the threat of early clinch when the title has been clinched\nbefore the last contest(s) of the competition take place; (2) the danger of\nwinning the competition without finishing first in any contest. In particular,\nfour historical points scoring systems of the Formula One World Championship\nare compared with the family of geometric scoring rules, recently proposed by\nan axiomatic approach. The schemes used in practice are found to be competitive\nwith respect to these goals, and the current rule seems to be a reasonable\ncompromise close to the Pareto frontier. Our results shed more light on the\nevolution of the Formula One points scoring systems and contribute to the issue\nof choosing the set of point values.\n"
    },
    {
        "paper_id": 2101.05847,
        "authors": "Minji Bang, Wayne Yuan Gao, Andrew Postlewaite, and Holger Sieg",
        "title": "Using Monotonicity Restrictions to Identify Models with Partially Latent\n  Covariates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a new method for identifying econometric models with\npartially latent covariates. Such data structures arise in industrial\norganization and labor economics settings where data are collected using an\ninput-based sampling strategy, e.g., if the sampling unit is one of multiple\nlabor input factors. We show that the latent covariates can be\nnonparametrically identified, if they are functions of a common shock\nsatisfying some plausible monotonicity assumptions. With the latent covariates\nidentified, semiparametric estimation of the outcome equation proceeds within a\nstandard IV framework that accounts for the endogeneity of the covariates. We\nillustrate the usefulness of our method using a new application that focuses on\nthe production functions of pharmacies. We find that differences in technology\nbetween chains and independent pharmacies may partially explain the observed\ntransformation of the industry structure.\n"
    },
    {
        "paper_id": 2101.059,
        "authors": "Emanuel Vespa, Taylor Weidman and Alistair J. Wilson",
        "title": "Testing Models of Strategic Uncertainty: Equilibrium Selection in\n  Repeated Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In repeated-game applications where both the collusive and non-collusive\noutcomes can be supported as equilibria, researchers must resolve underlying\nselection questions if theory will be used to understand counterfactual\npolicies. One guide to selection, based on clear theoretical underpinnings, has\nshown promise in predicting when collusive outcomes will emerge in controlled\nrepeated-game experiments. In this paper we both expand upon and experimentally\ntest this model of selection, and its underlying mechanism: strategic\nuncertainty. Adding an additional source of strategic uncertainty (the number\nof players) to the more-standard payoff sources, we stress test the model. Our\nresults affirm the model as a tool for predicting when tacit collusion is\nlikely/unlikely to be successful. Extending the analysis, we corroborate the\nmechanism of the model. When we remove strategic uncertainty through an\nexplicit coordination device, the model no longer predicts the selected\nequilibrium.\n"
    },
    {
        "paper_id": 2101.06077,
        "authors": "Florian Gach, Simon Hochgerner",
        "title": "Estimation of future discretionary benefits in traditional life\n  insurance",
        "comments": null,
        "journal-ref": "ASTIN Bulletin: The Journal of the IAA , Volume 52, Issue 3\n  (2022), pp. 835 - 876",
        "doi": "10.1017/asb.2022.16",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of life insurance with profit participation, the future\ndiscretionary benefits ($FDB$), which are a central item for Solvency~II\nreporting, are generally calculated by computationally expensive Monte Carlo\nalgorithms. We derive analytic formulas to estimate lower and upper bounds for\nthe $FDB$. This yields an estimation interval for the $FDB$, and the average of\nlower and upper bound is a simple estimator. These formulae are designed for\nreal world applications, and we compare the results to publicly available\nreporting data.\n"
    },
    {
        "paper_id": 2101.06149,
        "authors": "Shihas Abdul-Razak, Upasak Das, Rupayan Pal",
        "title": "Moving Away from the Joneses to Move Ahead: Migration, Information Gap\n  and Signalling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Signalling social status through the consumption of visible goods has often\nbeen perceived as a way in which individuals seek to emulate or move up\ncompared to others within the community. Using representative migration survey\ndata from the Indian state of Kerala, this paper assesses the impact of\ntransnational migration on consumption of visible goods. We utilize the\nplausibly exogenous variation in migration networks in the neighbourhood and\nreligious communities to account for the potential endogeneity. The findings\nindicate a significantly positive and robust effect of migration on conspicuous\nconsumption, even after controlling for household income. In terms of the\nmechanisms, while we are unable to rule out the associated taste-based changes\nin preferences and the peer group effects driving up the spending on status\ngoods, we observe only limited effects of these channels. A potential channel\nthat we propose is information gap among permanent residents about the income\nlevels of an out-migrant, which is leveraged by them to signal higher status in\nsociety. We explore this channel through a theoretical model where we connect\nmigration, information gap and status good consumption. We derive a set of\nconditions to test whether migrants exhibit snobbish or conformist behaviour.\nEmpirical observations indicate predominance of a snobbish behaviour.\n"
    },
    {
        "paper_id": 2101.06221,
        "authors": "Leonard G\\\"oke and Mario Kendziorski",
        "title": "Adequacy of time-series reduction for renewable energy systems",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.energy.2021.121701",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To reduce computational complexity, macro-energy system models commonly\nimplement reduced time-series data. For renewable energy systems dependent on\nseasonal storage and characterized by intermittent renewables, like wind and\nsolar, adequacy of time-series reduction is in question. Using a capacity\nexpansion model, we evaluate different methods for creating and implementing\nreduced time-series regarding loss of load and system costs.\n  Results show that adequacy greatly depends on the length of the reduced\ntime-series and how it is implemented into the model. Implementation as a\nchronological sequence with re-scaled time-steps prevents loss of load best but\nimposes a positive bias on seasonal storage resulting in an overestimation of\nsystem costs. Compared to chronological sequences, grouped periods require more\ntime so solve for the same number of time-steps, because the approach requires\nadditional variables and constraints. Overall, results suggest further efforts\nto improve time-series reduction and other methods for reducing computational\ncomplexity.\n"
    },
    {
        "paper_id": 2101.06236,
        "authors": "Fabin Shi, Nathan Aden, Shengda Huang, Neil Johnson, Xiaoqian Sun,\n  Jinhua Gao, Li Xu, Huawei Shen, Xueqi Cheng, and Chaoming Song",
        "title": "Modelling Universal Order Book Dynamics in Bitcoin Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Understanding the emergence of universal features such as the stylized facts\nin markets is a long-standing challenge that has drawn much attention from\neconomists and physicists. Most existing models, such as stochastic volatility\nmodels, focus mainly on price changes, neglecting the complex trading dynamics.\nRecently, there are increasing studies on order books, thanks to the\navailability of large-scale trading datasets, aiming to understand the\nunderlying mechanisms governing the market dynamics. In this paper, we collect\norder-book datasets of Bitcoin platforms across three countries over millions\nof users and billions of daily turnovers. We find a 1+1D field theory, govern\nby a set of KPZ-like stochastic equations, predicts precisely the order book\ndynamics observed in empirical data. Despite the microscopic difference of\nmarkets, we argue the proposed effective field theory captures the correct\nuniversality class of market dynamics. We also show that the model agrees with\nthe existing stochastic volatility models at the long-wavelength limit.\n"
    },
    {
        "paper_id": 2101.06348,
        "authors": "Marcos Costa Santos Carreira",
        "title": "Exponential Kernels with Latency in Hawkes Processes: Applications in\n  Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Tick library allows researchers in market microstructure to simulate and\nlearn Hawkes process in high-frequency data, with optimized parametric and\nnon-parametric learners. But one challenge is to take into account the correct\ncausality of order book events considering latency: the only way one order book\nevent can influence another is if the time difference between them (by the\ncentral order book timestamps) is greater than the minimum amount of time for\nan event to be (i) published in the order book, (ii) reach the trader\nresponsible for the second event, (iii) influence the decision (processing time\nat the trader) and (iv) the 2nd event reach the order book and be processed.\nFor this we can use exponential kernels shifted to the right by the latency\namount. We derive the expression for the log-likelihood to be minimized for the\n1-D and the multidimensional cases, and test this method with simulated data\nand real data. On real data we find that, although not all decays are the same,\nthe latency itself will determine most of the decays. We also show how the\ndecays are related to the latency. Code is available on GitHub at\nhttps://github.com/MarcosCarreira/Hawkes-With-Latency.\n"
    },
    {
        "paper_id": 2101.06458,
        "authors": "Gian Maria Campedelli and Maria Rita D'Orsogna",
        "title": "Temporal Clustering of Disorder Events During the COVID-19 Pandemic",
        "comments": "37 pages, 16 figures",
        "journal-ref": "PLOS ONE, 16(4), e0250433 (2021)",
        "doi": "10.1371/journal.pone.0250433",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The COVID-19 pandemic has unleashed multiple public health, socio-economic,\nand institutional crises. Measures taken to slow the spread of the virus have\nfostered significant strain between authorities and citizens, leading to waves\nof social unrest and anti-government demonstrations. We study the temporal\nnature of pandemic-related disorder events as tallied by the \"COVID-19 Disorder\nTracker\" initiative by focusing on the three countries with the largest number\nof incidents, India, Israel, and Mexico. By fitting Poisson and Hawkes\nprocesses to the stream of data, we find that disorder events are\ninter-dependent and self-excite in all three countries. Geographic clustering\nconfirms these features at the subnational level, indicating that nationwide\ndisorders emerge as the convergence of meso-scale patterns of self-excitation.\nConsiderable diversity is observed among countries when computing correlations\nof events between subnational clusters; these are discussed in the context of\nspecific political, societal and geographic characteristics. Israel, the most\nterritorially compact and where large scale protests were coordinated in\nresponse to government lockdowns, displays the largest reactivity and the\nshortest period of influence following an event, as well as the strongest\nnationwide synchrony. In Mexico, where complete lockdown orders were never\nmandated, reactivity and nationwide synchrony are lowest. Our work highlights\nthe need for authorities to promote local information campaigns to ensure that\nlivelihoods and virus containment policies are not perceived as mutually\nexclusive.\n"
    },
    {
        "paper_id": 2101.06476,
        "authors": "Arunav Das",
        "title": "Visual Analytics approach for finding spatiotemporal patterns from\n  COVID19",
        "comments": "Coursework / concept paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Bounce Back Loan is amongst a number of UK business financial support schemes\nlaunched by UK Government in 2020 amidst pandemic lockdown. Through these\nschemes, struggling businesses are provided financial support to weather\neconomic slowdown from pandemic lockdown. {\\pounds}43.5bn loan value has been\nprovided as of 17th Dec2020. However, with no major checks for granting these\nloans and looming prospect of loan losses from write-offs from failed\nbusinesses and fraud, this paper theorizes prospect of applying spatiotemporal\nmodelling technique to explore if geospatial patterns and temporal analysis\ncould aid design of loan grant criteria for schemes. Application of Clustering\nand Visual Analytics framework to business demographics, survival rate and\nSector concentration shows Inner and Outer London spatial patterns which\nhistoric business failures and reversal of the patterns under COVID-19 implying\nsector influence on spatial clusters. Combination of unsupervised clustering\ntechnique with multinomial logistic regression modelling on research datasets\ncomplimented by additional datasets on other support schemes, business\nstructure and financial crime, is recommended for modelling business\nvulnerability to certain types of financial market or economic condition. The\nlimitations of clustering technique for high dimensional is discussed along\nwith relevance of an applicable model for continuing the research through next\nsteps.\n"
    },
    {
        "paper_id": 2101.06585,
        "authors": "Sayuj Choudhari, Richard Licheng Zhu",
        "title": "Diagnosis of systemic risk and contagion across financial sectors",
        "comments": "21 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In normal times, it is assumed that financial institutions operating in\nnon-overlapping sectors have complementary and distinct outcomes, typically\nreflected in mostly uncorrelated outcomes and asset returns. Such is the\nreasoning behind common \"free lunches\" to be had in investing, like\ndiversifying assets across equity and bond sectors. Unfortunately, the\nrecurrence of crises like the Great Financial Crisis of 2007-2008 demonstrate\nthat such convenient assumptions often break down, with dramatic consequences\nfor all financial actors. In hindsight, the emergence of systemic risk (as\nexemplified by failure in one part of a system spreading to ostensibly\nunrelated parts of the system) has been explained by narratives such as\nderegulation and leverage. But can we diagnose and quantify the ongoing\nemergence of systemic risk in financial systems? In this study, we focus on two\npreviously-documented measures of systemic risk that require only easily\navailable time series data (eg monthly asset returns): cross-correlation and\nprincipal component analysis. We apply these tests to daily and monthly returns\non hedge fund indexes and broad-based market indexes, and discuss their\nresults. We hope that a frank discussion of these simple, non-parametric\nmeasures can help inform legislators, lawmakers, and financial actors of\npotential crises looming on the horizon.\n"
    },
    {
        "paper_id": 2101.06603,
        "authors": "Guillermo Jose Navarro del Toro",
        "title": "The Impact of Digital Marketing on Sausage Manufacturing Companies in\n  the Altos of Jalisco",
        "comments": "19 Pages, in Spanish",
        "journal-ref": null,
        "doi": "10.23913/ricea.v9i18.148",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  One of the goals of any business, in addition to producing high-quality,\ncommunity-accepted products, is to significantly increase sales. Unfortunately,\nthere are regions where new marketing technologies that make it possible to\nreach a larger number of potential consumers, not only at the regional level,\nbut also at the state and national level, are not yet used. This research,\nwhich included qualitative and quantitative methods, as well as interviews\napplied to owners, employees and clients of three sausage companies, seeks to\nmeasure the impact of digital marketing in the Altos of Jalisco, Mexico. Thus,\nin addition to inquiring about the degree of knowledge they have regarding\ninformation and communication technologies (ICT) to expand their markets to\nareas with higher population density, another goal is to know the opinion about\ntheir manufactured products, their quality and acceptance. It should not be\nforgotten that companies are moving to an increasingly connected world, which\nenables entrepreneurs to get their products to a greater number of consumers\nthrough the Internet and smart devices, such as cell phones, tablets and\ncomputers; and thus ensure the survival of the company and a longer stay in the\nmarket.\n"
    },
    {
        "paper_id": 2101.06675,
        "authors": "Zongxia Liang, Yang Liu, Litian Zhang",
        "title": "A Framework of State-dependent Utility Optimization with General\n  Benchmarks",
        "comments": "44 pages, 8figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Benchmarks in the utility function have various interpretations, including\nperformance guarantees and risk constraints in fund contracts and reference\nlevels in cumulative prospect theory. In most literature, benchmarks are a\ndeterministic constant or a fraction of the underlying wealth variable; thus,\nthe utility is also a function of the wealth. In this paper, we propose a\ngeneral framework of state-dependent utility optimization with stochastic\nbenchmark variables, which includes stochastic reference levels as typical\nexamples. We provide the optimal solution(s) and investigate the issues of\nwell-definedness, feasibility, finiteness, and attainability. The major\ndifficulties include: (i) various reasons for the non-existence of the Lagrange\nmultiplier and corresponding results on the optimal solution; (ii)\nmeasurability issues of the concavification of a state-dependent utility and\nthe selection of the optimal solutions. Finally, we show how to apply the\nframework to solve some constrained utility optimization problems with\nstate-dependent performance and risk benchmarks as some nontrivial examples.\n"
    },
    {
        "paper_id": 2101.0669,
        "authors": "Selin \\\"Ozen and \\c{S}ule \\c{S}ahin",
        "title": "A Two-Population Mortality Model to Assess Longevity Basis Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Index-based hedging solutions are used to transfer the longevity risk to the\ncapital markets. However, mismatches between the liability of the hedger and\nthe hedging instrument cause longevity basis risk. Therefore, an appropriate\ntwo-population model to measure and assess the longevity basis risk is\nrequired. In this paper, we aim to construct a two-population mortality model\nto provide an effective hedge against the longevity basis risk. The reference\npopulation is modelled by using the Lee-Carter model with the renewal process\nand exponential jumps proposed by \\\"Ozen and \\c{S}ahin (2020) and the dynamics\nof the book population are specified. The analysis based on the UK mortality\ndata indicates that the proposed model for the reference population and the\ncommon age effect model for the book population provide a better fit compared\nto the other models considered in the paper. Different two-population models\nare used to investigate the impact of the sampling risk on the index-based\nhedge as well as to analyse the risk reduction regarding hedge effectiveness.\nThe results show that the proposed model provides a significant risk reduction\nwhen mortality jumps and the sampling risk are taken into account.\n"
    },
    {
        "paper_id": 2101.06834,
        "authors": "Mohsen Kayal, Jane Ballard, Ehsan Kayal",
        "title": "Towards a more sustainable academic publishing system",
        "comments": "15 pages including 1 figure",
        "journal-ref": "Ideas in Ecology and Evolution 14, 22-30, 2021",
        "doi": "10.24908/iee.2021.14.3.f",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Communicating new scientific discoveries is key to human progress. Yet, this\nendeavor is hindered by monetary restrictions for publishing one's findings and\naccessing other scientists' reports. This process is further exacerbated by a\nlarge portion of publishing media owned by private, for-profit companies that\ndo not reinject academic publishing benefits into the scientific community, in\ncontrast with journals from scientific societies. As the academic world is not\nexempt from economic crises, new alternatives are necessary to support a fair\npublishing system for society. After summarizing the general issues of academic\npublishing today, we present several solutions at the levels of the individual\nscientist, the scientific community, and the publisher towards more sustainable\nscientific publishing. By providing a voice to the many scientists who are\nfundamental protagonists, yet often powerless witnesses, of the academic\npublishing system, and a roadmap for implementing solutions, this initiative\ncan spark increased awareness and promote shifts towards impactful practices.\n"
    },
    {
        "paper_id": 2101.06957,
        "authors": "Jozef Barunik and Mattia Bevilacqua and Robert Faff",
        "title": "Dynamic industry uncertainty networks and the business cycle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We argue that uncertainty network structures extracted from option prices\ncontain valuable information for business cycles. Classifying U.S. industries\naccording to their contribution to system-related uncertainty across business\ncycles, we uncover an uncertainty hub role for the communications, industrials\nand information technology sectors, while shocks to materials, real estate and\nutilities do not create strong linkages in the network. Moreover, we find that\nthis ex-ante network of uncertainty is a useful predictor of business cycles,\nespecially when it is based on uncertainty hubs. The industry uncertainty\nnetwork behaves counter-cyclically in that a tighter network tends to associate\nwith future business cycle contractions.\n"
    },
    {
        "paper_id": 2101.07084,
        "authors": "Patrick Mijatovic",
        "title": "Beating the Market with Generalized Generating Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic portfolio theory aims at finding relative arbitrages, i.e. trading\nstrategies which outperform the market with probability one. Functionally\ngenerated portfolios, which are deterministic functions of the market weights,\nare an invaluable tool in doing so. Driven by a practitioner point of view,\nwhere investment decisions are based upon consideration of various financial\nvariables, we generalize functionally generated portfolios and allow them to\ndepend on continuous-path semimartingales, in addition to the market weights.\nBy means of examples we demonstrate how the inclusion of additional processes\ncan reduce time horizons beyond which relative arbitrage is possible, boost\nperformance of generated portfolios, and how investor preferences and specific\ninvestment views can be included in the context of stochastic portfolio theory.\nStriking is also the construction of a relative arbitrage opportunity which is\ngenerated by the volatility of the additional semimartingale. An in-depth\nempirical analysis of the performance of the proposed strategies confirms our\ntheoretical findings and demonstrates that our portfolios represent profitable\ninvestment opportunities even in the presence of transaction costs.\n"
    },
    {
        "paper_id": 2101.07107,
        "authors": "Antonio Briola, Jeremy Turiel, Riccardo Marcaccioli, Alvaro Cauderan,\n  Tomaso Aste",
        "title": "Deep Reinforcement Learning for Active High Frequency Trading",
        "comments": "9 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We introduce the first end-to-end Deep Reinforcement Learning (DRL) based\nframework for active high frequency trading in the stock market. We train DRL\nagents to trade one unit of Intel Corporation stock by employing the Proximal\nPolicy Optimization algorithm. The training is performed on three contiguous\nmonths of high frequency Limit Order Book data, of which the last month\nconstitutes the validation data. In order to maximise the signal to noise ratio\nin the training data, we compose the latter by only selecting training samples\nwith largest price changes. The test is then carried out on the following month\nof data. Hyperparameters are tuned using the Sequential Model Based\nOptimization technique. We consider three different state characterizations,\nwhich differ in their LOB-based meta-features. Analysing the agents'\nperformances on test data, we argue that the agents are able to create a\ndynamic representation of the underlying environment. They identify occasional\nregularities present in the data and exploit them to create long-term\nprofitable trading strategies. Indeed, agents learn trading strategies able to\nproduce stable positive returns in spite of the highly stochastic and\nnon-stationary environment.\n"
    },
    {
        "paper_id": 2101.0741,
        "authors": "Ken Chung and Anthony Bellotti",
        "title": "Evidence and Behaviour of Support and Resistance Levels in Financial\n  Time Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates the phenomenon of support and resistance levels (SR\nlevels) in financial time series, which act as temporary price barriers that\nreverses price trends. We develop a heuristic discovery algorithm for this\npurpose, to discover and evaluate SR levels for intraday price series. Our\nsimple approach discovers SR levels which are able to reverse price trends\nstatistically significantly. Asset price entering SR levels with higher number\nof price bounces before are more likely to bounce on such SR levels again. We\nalso show that the decay aspect of the discovered SR levels as decreasing\nprobability of price bounce over time. We conclude SR levels are features in\nfinancial time series are not explained simply by AR(1) processes, stationary\nor otherwise; and that they contribute to the temporary predictability and\nstationarity of the investigated price series.\n"
    },
    {
        "paper_id": 2101.07467,
        "authors": "Chirag Dhara and Vandana Singh",
        "title": "The Elephant in the Room: Why Transformative Education Must Address the\n  Problem of Endless Exponential Economic Growth",
        "comments": "To be published in: Iyengar, R. and Kwauk, C. (Eds.), \"Charting an\n  SDG 4.7 roadmap for radical, transformative change in the midst of climate\n  breakdown\". Brill Publishers",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A transformative approach to addressing complex social-environmental problems\nwarrants reexamining our most fundamental assumptions about sustainability and\nprogress, including the entrenched imperative for limitless economic growth.\nOur global resource footprint has grown in lock-step with GDP since the\nindustrial revolution, spawning the climate and ecological crises. Faith that\ntechnology will eventually decouple resource use from GDP growth is pervasive,\ndespite there being practically no empirical evidence of decoupling in any\ncountry. We argue that complete long-term decoupling is, in fact, well-nigh\nimpossible for fundamental physical, mathematical, logical, pragmatic and\nbehavioural reasons. We suggest that a crucial first step toward a\ntransformative education is to acknowledge this incompatibility, and provide\nexamples of where and how our arguments may be incorporated in education. More\nbroadly, we propose that foregrounding SDG 12 with a functional definition of\nsustainability, and educating and upskilling students to this end, must be a\nnecessary minimum goal of any transformative approach to sustainability\neducation. Our aim is to provide a conceptual scaffolding around which learning\nframeworks may be developed to make room for diverse alternative paths to truly\nsustainable social-ecological cultures.\n"
    },
    {
        "paper_id": 2101.07661,
        "authors": "Simon Berset, Martin Huber, Mark Schelker",
        "title": "The fiscal response to revenue shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the impact of fiscal revenue shocks on local fiscal policy. We focus\non the very volatile revenues from the immovable property gains tax in the\ncanton of Zurich, Switzerland, and analyze fiscal behavior following large and\nrare positive and negative revenue shocks. We apply causal machine learning\nstrategies and implement the post-double-selection LASSO estimator to identify\nthe causal effect of revenue shocks on public finances. We show that local\npolicymakers overall predominantly smooth fiscal shocks. However, we also find\nsome patterns consistent with fiscal conservatism, where positive shocks are\nsmoothed, while negative ones are mitigated by spending cuts.\n"
    },
    {
        "paper_id": 2101.07695,
        "authors": "Tiziana Carpi, Airo Hino, Stefano Maria Iacus, Giuseppe Porro",
        "title": "Twitter Subjective Well-Being Indicator During COVID-19 Pandemic: A\n  Cross-Country Comparative Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study analyzes the impact of the COVID-19 pandemic on the subjective\nwell-being as measured through Twitter data indicators for Japan and Italy. It\nturns out that, overall, the subjective well-being dropped by 11.7% for Italy\nand 8.3% for Japan in the first nine months of 2020 compared to the last two\nmonths of 2019 and even more compared to the historical mean of the indexes.\nThrough a data science approach we try to identify the possible causes of this\ndrop down by considering several explanatory variables including, climate and\nair quality data, number of COVID-19 cases and deaths, Facebook Covid and flu\nsymptoms global survey, Google Trends data and coronavirus-related searches,\nGoogle mobility data, policy intervention measures, economic variables and\ntheir Google Trends proxies, as well as health and stress proxy variables based\non big data. We show that a simple static regression model is not able to\ncapture the complexity of well-being and therefore we propose a dynamic elastic\nnet approach to show how different group of factors may impact the well-being\nin different periods, even over a short time length, and showing further\ncountry-specific aspects. Finally, a structural equation modeling analysis\ntries to address the causal relationships among the COVID-19 factors and\nsubjective well-being showing that, overall, prolonged mobility\nrestrictions,flu and Covid-like symptoms, economic uncertainty, social\ndistancing and news about the pandemic have negative effects on the subjective\nwell-being.\n"
    },
    {
        "paper_id": 2101.07794,
        "authors": "Daniel Bartl and Shahar Mendelson",
        "title": "On Monte-Carlo methods in convex stochastic optimization",
        "comments": null,
        "journal-ref": "Annals of Applied Probability, 2022+",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a novel procedure for estimating the optimizer of general convex\nstochastic optimization problems of the form $\\min_{x\\in\\mathcal{X}}\n\\mathbb{E}[F(x,\\xi)]$, when the given data is a finite independent sample\nselected according to $\\xi$. The procedure is based on a median-of-means\ntournament, and is the first procedure that exhibits the optimal statistical\nperformance in heavy tailed situations: we recover the asymptotic rates\ndictated by the central limit theorem in a non-asymptotic manner once the\nsample size exceeds some explicitly computable threshold. Additionally, our\nresults apply in the high-dimensional setup, as the threshold sample size\nexhibits the optimal dependence on the dimension (up to a logarithmic factor).\nThe general setting allows us to recover recent results on multivariate mean\nestimation and linear regression in heavy-tailed situations and to prove the\nfirst sharp, non-asymptotic results for the portfolio optimization problem.\n"
    },
    {
        "paper_id": 2101.07818,
        "authors": "Anton Pichler, J. Doyne Farmer",
        "title": "Simultaneous supply and demand constraints in input-output networks: The\n  case of Covid-19 in Germany, Italy, and Spain",
        "comments": "29 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Natural and anthropogenic disasters frequently affect both the supply and\ndemand side of an economy. A striking recent example is the Covid-19 pandemic\nwhich has created severe disruptions to economic output in most countries.\nThese direct shocks to supply and demand will propagate downstream and upstream\nthrough production networks. Given the exogenous shocks, we derive a lower\nbound on total shock propagation. We find that even in this best case scenario\nnetwork effects substantially amplify the initial shocks. To obtain more\nrealistic model predictions, we study the propagation of shocks bottom-up by\nimposing different rationing rules on industries if they are not able to\nsatisfy incoming demand. Our results show that economic impacts depend strongly\non the emergence of input bottlenecks, making the rationing assumption a key\nvariable in predicting adverse economic impacts. We further establish that the\nmagnitude of initial shocks and network density heavily influence model\npredictions.\n"
    },
    {
        "paper_id": 2101.0782,
        "authors": "Edward J Oughton and Niccol\\`o Comini and Vivien Foster and Jim W Hall",
        "title": "Policy choices can help keep 4G and 5G universal broadband affordable",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The United Nations Broadband Commission has committed the international\ncommunity to accelerate universal broadband. However, the cost of meeting this\nobjective, and the feasibility of doing so on a commercially viable basis, are\nnot well understood. Using scenario analysis, this paper compares the global\ncost-effectiveness of different infrastructure strategies for the developing\nworld to achieve universal 4G or 5G mobile broadband. Utilizing remote sensing\nand demand forecasting, least-cost network designs are developed for eight\nrepresentative low and middle-income countries (Malawi, Uganda, Kenya, Senegal,\nPakistan, Albania, Peru and Mexico), the results from which form the basis for\naggregation to the global level. The cost of meeting a minimum 10 Mbps per user\nis estimated at USD 1.7 trillion using 5G Non-Standalone, approximately 0.6% of\nannual GDP for the developing world over the next decade. However, by creating\na favorable regulatory environment, governments can bring down these costs by\nas much as three quarters, to USD 0.5 trillion (approximately 0.2% of annual\nGDP), and avoid the need for public subsidy. Providing governments make\njudicious choices, adopting fiscal and regulatory regimes conducive to lowering\ncosts, universal broadband may be within reach of most developing countries\nover the next decade.\n"
    },
    {
        "paper_id": 2101.08008,
        "authors": "Prateek Bansal, Rajeev Ranjan Kumar, Alok Raj, Subodh Dubey, Daniel J.\n  Graham",
        "title": "Willingness to Pay and Attitudinal Preferences of Indian Consumers for\n  Electric Vehicles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consumer preference elicitation is critical to devise effective policies for\nthe diffusion of electric vehicles (EVs) in India. This study contributes to\nthe EV demand literature in the Indian context by (a) analysing the EV\nattributes and attitudinal factors of Indian car buyers that determine\nconsumers' preferences for EVs, (b) estimating Indian consumers' willingness to\npay (WTP) to buy EVs with improved attributes, and c) quantifying how the\nreference dependence affects the WTP estimates. We adopt a hybrid choice\nmodelling approach for the above analysis. The results indicate that accounting\nfor reference dependence provides more realistic WTP estimates than the\nstandard utility estimation approach. Our results suggest that Indian consumers\nare willing to pay an additional USD 10-34 in the purchase price to reduce the\nfast charging time by 1 minute, USD 7-40 to add a kilometre to the driving\nrange of EVs at 200 kilometres, and USD 104-692 to save USD 1 per 100\nkilometres in operating cost. These estimates and the effect of attitudes on\nthe likelihood to adopt EVs provide insights about EV design, marketing\nstrategies, and pro-EV policies (e.g., specialised lanes and reserved parking\nfor EVs) to expedite the adoption of EVs in India.\n"
    },
    {
        "paper_id": 2101.08041,
        "authors": "Rafa{\\l} M. {\\L}ochowski, Nicolas Perkowski, David J. Pr\\\"omel",
        "title": "One-dimensional game-theoretic differential equations",
        "comments": null,
        "journal-ref": "Int. J. Approx. Reason.: Probability and Statistics: Foundations\n  and History. In honor of Glenn Shafer, vol. 141, p. 11--27, 2022",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a very brief introduction to typical paths and the corresponding\nIt\\^o type integration. Relying on this robust It\\^o integration, we prove an\nexistence and uniqueness result for one-dimensional differential equations\ndriven by typical paths with non-Lipschitz continuous coefficients in the\nspirit of Yamada--Watanabe as well as an approximation result in the spirit of\nDoss--Sussmann.\n"
    },
    {
        "paper_id": 2101.08068,
        "authors": "Maximilien Germain, Huy\\^en Pham, Xavier Warin",
        "title": "Neural networks-based algorithms for stochastic control and PDEs in\n  finance",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2006.01496",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents machine learning techniques and deep reinforcement\nlearningbased algorithms for the efficient resolution of nonlinear partial\ndifferential equations and dynamic optimization problems arising in investment\ndecisions and derivative pricing in financial engineering. We survey recent\nresults in the literature, present new developments, notably in the fully\nnonlinear case, and compare the different schemes illustrated by numerical\ntests on various financial applications. We conclude by highlighting some\nfuture research directions.\n"
    },
    {
        "paper_id": 2101.08145,
        "authors": "Vimal Raval and Antoine Jacquier",
        "title": "The Log Moment formula for implied volatility",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the foundational Moment Formula proved by Roger Lee fifteen years\nago. We show that when the underlying stock price martingale admits finite\nlog-moments E[|log(S)|^q] for some positive q, the arbitrage-free growth in the\nleft wing of the implied volatility smile is less constrained than Lee's bound.\nThe result is rationalised by a market trading discretely monitored variance\nswaps wherein the payoff is a function of squared log-returns, and requires no\nassumption for the underlying martingale to admit any negative moment. In this\nrespect, the result can derived from a model-independent setup. As a byproduct,\nwe relax the moment assumptions on the stock price to provide a new proof of\nthe notorious Gatheral-Fukasawa formula expressing variance swaps in terms of\nthe implied volatility.\n"
    },
    {
        "paper_id": 2101.08424,
        "authors": "Marcel Nutz, Florian Stebegg",
        "title": "Climate Change Adaptation under Heterogeneous Beliefs",
        "comments": "Forthcoming in 'Mathematics and Financial Economics'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study strategic interactions between firms with heterogeneous beliefs\nabout future climate impacts. To that end, we propose a Cournot-type\nequilibrium model where firms choose mitigation efforts and production\nquantities such as to maximize the expected profits under their subjective\nbeliefs. It is shown that optimal mitigation efforts are increased by the\npresence of uncertainty and act as substitutes; i.e., one firm's lack of\nmitigation incentivizes others to act more decidedly, and vice versa.\n"
    },
    {
        "paper_id": 2101.08476,
        "authors": "Eiji Yamamura and Yoshiro Tsutsui",
        "title": "Impact of closing schools on mental health during the COVID-19 pandemic:\n  Evidence using panel data from Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The spread of the novel coronavirus disease caused schools in Japan to close\nto cope with the pandemic. In response to this, parents of students were\nobliged to care for their children during the daytime when they were usually at\nschool. Does the increase in burden of childcare influence parents mental\nhealth? Based on short panel data from mid-March to mid-April 2020, we explored\nhow school closures influenced the mental health of parents with school-aged\nchildren. Using the fixed effects model, we found that school closures lead to\nstudents mothers suffering from worse mental health than other females, while\nthe fathers mental health did not differ from other males. This tendency was\nonly observed for less educated mothers who had children attending primary\nschool, but not those attending junior high school. The contribution of this\npaper is to show that school closures increased the inequality of mental health\nbetween genders and the educational background of parents.\n"
    },
    {
        "paper_id": 2101.0848,
        "authors": "Eiji Yamamura and Yoshiro Tsutsui",
        "title": "Changing views about remote working during the COVID-19 pandemic:\n  Evidence using panel data from Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  COVID-19 has led to school closures in Japan to cope with the pandemic. Under\nthe state of emergency, in addition to school closure, after-school care has\nnot been sufficiently supplied. We independently collected individual level\ndata through internet surveys to construct short panel data from mid-March to\nmid-June 2020, which covered before and after the state of emergency. We\nanalyze how the presence of school-aged children influences their parents views\nabout working from home. After controlling for various factors using a fixed\neffects model, we find that in cases where parents were workers, and the\nchildren are (1) in primary school, parents are willing to promote working from\nhome. If children are (2) in junior high school, the parents view is hardly\naffected. (3) Surprisingly, workers whose children are primary school pupils\nare most likely to support promotion of working from home after schools reopen.\nDue to school closure and a lack of after-school care, parents need to work\nfrom home, and this experience motivated workers with small children to\ncontinue doing so to improve work-life balance even after schools reopen.\n"
    },
    {
        "paper_id": 2101.08487,
        "authors": "Eiji Yamamura",
        "title": "Female teachers effect on male pupils' voting behavior and preference\n  formation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the influence of learning in a female teacher homeroom\nclass in elementary school on pupils' voting behavior later in life, using\nindependently collected individual-level data. Further, we evaluate its effect\non preference for women's participation in the workplace in adulthood. Our\nstudy found that having a female teacher in the first year of school makes\nindividuals more likely to vote for female candidates, and to prefer policy for\nfemale labor participation in adulthood. However, the effect is only observed\namong males, and not female pupils. These findings offer new evidence for the\nfemale socialization hypothesis.\n"
    },
    {
        "paper_id": 2101.08488,
        "authors": "Eiji Yamamura",
        "title": "Long-term effects of female teacher on her pupils' smoking behaviour\n  later in life",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Japan, teacher and student is randomly matched in the first year of\nelementary school. Under the quasi-natural experimental setting, we examine how\nlearning in female teacher homeroom class in the elementary school influence\npupils' smoking behavior after they become adult. We found that pupils are\nunlikely to smoke later in life if they belonged to female teacher homeroom\nclass in pupil's first year of school.\n"
    },
    {
        "paper_id": 2101.08559,
        "authors": "Victor Olkhov",
        "title": "To VaR, or Not to VaR, That is the Question",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider economic obstacles that limit the reliability and accuracy of\nvalue-at-risk (VaR). Investors who manage large market transactions should take\ninto account the impact of the randomness of large trade volumes on predictions\nof price probability and VaR assessments. We introduce market-based\nprobabilities of price and return that depend on the randomness of market trade\nvalues and volumes. Contrary to them, the conventional frequency-based price\nprobability describes the case of constant trade volumes. We derive the\ndependence of market-based price volatility on the volatilities and correlation\nof trade values and volumes. In the coming years, that will limit the accuracy\nof price probability predictions to Gaussian approximations, and even the\nforecasts of market-based price volatility will be inaccurate and highly\nuncertain.\n"
    },
    {
        "paper_id": 2101.08778,
        "authors": "Sam M. Werner, Daniel Perez, Lewis Gudgeon, Ariah Klages-Mundt,\n  Dominik Harz, William J. Knottenbelt",
        "title": "SoK: Decentralized Finance (DeFi)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized Finance (DeFi), a blockchain powered peer-to-peer financial\nsystem, is mushrooming. Two years ago the total value locked in DeFi systems\nwas approximately 700m USD, now, as of April 2022, it stands at around 150bn\nUSD. The frenetic evolution of the ecosystem has created challenges in\nunderstanding the basic principles of these systems and their security risks.\nIn this Systematization of Knowledge (SoK) we delineate the DeFi ecosystem\nalong the following axes: its primitives, its operational protocol types and\nits security. We provide a distinction between technical security, which has a\nhealthy literature, and economic security, which is largely unexplored,\nconnecting the latter with new models and thereby synthesizing insights from\ncomputer science, economics and finance. Finally, we outline the open research\nchallenges in the ecosystem across these security types.\n"
    },
    {
        "paper_id": 2101.08813,
        "authors": "Jackie Shen",
        "title": "Nine Challenges in Modern Algorithmic Trading and Controls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This editorial article partially informs the algorithmic trading community\nabout launching of the new journal \"Algorithmic Trading and Controls\" (ATC).\nATC is an online open-access journal that publishes novel works on algorithmic\ntrading and its control methodologies. In this inaugural article, we discuss\nnine major challenges that contemporary Algo trading faces. There is nothing\nsuperstitiously magical about the number \"nine,\" but so is any other one.\nSeveral of these challenges are at the strategy level, including for example,\ntrading of illiquid securities or optimal portfolio execution. Others are more\nat the level of risk management and controls, such as on how to develop\nautomated controls, testing and simulations. The editorial views could be\ninevitably personal and biased, but have been explored with the most innocent\nintention of contributing to this important field in modern financial services\nand technologies.\n"
    },
    {
        "paper_id": 2101.08914,
        "authors": "Saleh Afroogh",
        "title": "A Contextualist Decision Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decision theorists propose a normative theory of rational choice.\nTraditionally, they assume that they should provide some constant and invariant\nprinciples as criteria for rational decisions, and indirectly, for agents. They\nseek a decision theory that invaribably works for all agents all the time. They\nbelieve that a rational agent should follow a certain principle, perhaps the\nprinciple of maximizing expected utility everywhere, all the time. As a result\nof the given context, these principles are considered, in this sense,\ncontext-independent.\n  Furthermore, decision theorists usually assume that the relevant agents at\nwork are ideal agents, and they believe that non-ideal agents should follow\nthem so that their decisions qualify as rational. These principles are\nuniversal rules. I will refer to this context-independent and universal\napproach in traditional decision theory as Invariantism. This approach is,\nimplicitly or explicitly, adopted by theories which are proposed on the basis\nof these two assumptions.\n"
    },
    {
        "paper_id": 2101.08922,
        "authors": "Eiji Yamamura and Yoshiro Tsutsui",
        "title": "How does COVID-19 change insurance and vaccine demand? Evidence from\n  short-panel data in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we explored how the coronavirus disease (COVID-19) affected\nthe demand for insurance and vaccines in Japan from mid-March to mid-April\n2020. Through independent internet surveys, respondents were asked hypothetical\nquestions concerning the demand for insurance and vaccines for protection\nagainst COVID-19. Using the collected short-panel data, after controlling for\nindividual characteristics using the fixed effects model, the key findings,\nwithin the context of the pandemic, were as follows: (1) Contrary to extant\nstudies, the demand for insurance by females was smaller than that by their\nmale counterparts; (2) The gap in demand for insurance between genders\nincreased as the pandemic prevailed; (3) The demand for a vaccine by females\nwas higher than that for males; and (4) As COVID-19 spread throughout Japan,\ndemand for insurance decreased, whereas the demand for a vaccine increased.\n"
    },
    {
        "paper_id": 2101.08964,
        "authors": "Petar Jevtic and Nicolas Lanchier",
        "title": "Probabilistic Framework For Loss Distribution Of Smart Contract Risk",
        "comments": "26 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Smart contract risk can be defined as a financial risk of loss due to cyber\nattacks on or contagious failures of smart contracts. Its quantification is of\nparamount importance to technology platform providers as well as companies and\nindividuals when considering the deployment of this new technology. That is\nwhy, as our primary contribution, we propose a structural framework of\naggregate loss distribution for smart contract risk under the assumption of a\ntree-stars graph topology representing the network of interactions among smart\ncontracts and their users. Up to our knowledge, there exist no theoretical\nframeworks or models of an aggregate loss distribution for smart contracts in\nthis setting. To achieve our goal, we contextualize the problem in the\nprobabilistic graph-theoretical framework using bond percolation models. We\nassume that the smart contract network topology is represented by a random tree\ngraph of finite size, and that each smart contract is the center of a {random}\nstar graph whose leaves represent the users of the smart contract. We allow for\nheterogeneous loss topology superimposed on this smart contract and user\ntopology and provide analytical results and instructive numerical examples.\n"
    },
    {
        "paper_id": 2101.08984,
        "authors": "Xianfei Hui, Baiqing Sun, Hui Jiang, Indranil SenGupta",
        "title": "Analysis of stock index with a generalized BN-S model: an approach based\n  on machine learning and fuzzy parameters",
        "comments": "13 figures, 12 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we implement a combination of data-science and fuzzy theory to\nimprove the classical Barndorff-Nielsen and Shephard model, and implement this\nto analyze the S&P 500 index. We pre-process the index data based on fuzzy\ntheory. After that, S&P 500 stock index data for the past ten years are\nanalyzed, and a deterministic parameter is extracted using various machine and\ndeep learning methods. The results show that the new model, where fuzzy\nparameters are incorporated, can incorporate the long-term dependence in the\nclassical Barndorff-Nielsen and Shephard model. The modification is based on\nonly a few changes compared to the classical model. At the same time, the\nresulting analysis effectively captures the stochastic dynamics of the stock\nindex time series.\n"
    },
    {
        "paper_id": 2101.09064,
        "authors": "Jaegi Jeon, Kyunghyun Park, Jeonggyu Huh",
        "title": "Extensive networks would eliminate the demand for pricing formulas",
        "comments": "18 pages,5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we generate a large number of implied volatilities for the\nStochastic Alpha Beta Rho (SABR) model using a graphics processing unit (GPU)\nbased simulation and enable an extensive neural network to learn them. This\nmodel does not have any exact pricing formulas for vanilla options, and neural\nnetworks have an outstanding ability to approximate various functions.\nSurprisingly, the network reduces the simulation noises by itself, thereby\nachieving as much accuracy as the Monte-Carlo simulation. Extremely high\naccuracy cannot be attained via existing approximate formulas. Moreover, the\nnetwork is as efficient as the approaches based on the formulas. When\nevaluating based on high accuracy and efficiency, extensive networks can\neliminate the necessity of the pricing formulas for the SABR model. Another\nsignificant contribution is that a novel method is proposed to examine the\nerrors based on nonlinear regression. This approach is easily extendable to\nother pricing models for which it is hard to induce analytic formulas.\n"
    },
    {
        "paper_id": 2101.09214,
        "authors": "Ni Zhan, Yijia Sun, Aman Jakhar, He Liu",
        "title": "Graphical Models for Financial Time Series and Portfolio Selection",
        "comments": "Published at ACM International Conference on AI in Finance (ICAIF\n  '20)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine a variety of graphical models to construct optimal portfolios.\nGraphical models such as PCA-KMeans, autoencoders, dynamic clustering, and\nstructural learning can capture the time varying patterns in the covariance\nmatrix and allow the creation of an optimal and robust portfolio. We compared\nthe resulting portfolios from the different models with baseline methods. In\nmany cases our graphical strategies generated steadily increasing returns with\nlow risk and outgrew the S&P 500 index. This work suggests that graphical\nmodels can effectively learn the temporal dependencies in time series data and\nare proved useful in asset management.\n"
    },
    {
        "paper_id": 2101.0923,
        "authors": "Ni Zhan",
        "title": "Where does the Stimulus go? Deep Generative Model for Commercial Banking\n  Deposits",
        "comments": null,
        "journal-ref": "NeurIPS 2020 workshop on ML for Economic Policy",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines deposits of individuals (\"retail\") and large companies\n(\"wholesale\") in the U.S. banking industry, and how these deposit types are\nimpacted by macroeconomic factors, such as quantitative easing (QE). Actual\ndata for deposits by holder are unavailable. We use a dataset on banks'\nfinancial information and probabilistic generative model to predict industry\nretail-wholesale deposit split from 2000 to 2020. Our model assumes account\nbalances arise from separate retail and wholesale lognormal distributions and\nfit parameters of distributions by minimizing error between actual bank metrics\nand simulated metrics using the model's generative process. We use time-series\nregression to forward predict retail-wholesale deposits as function of loans,\nretail loans, and reserve balances at Fed banks. We find increase in reserves\n(representing QE) increases wholesale but not retail deposits, and increase in\nloans increase both wholesale and retail deposits evenly. The result shows that\nQE following the 2008 financial crisis benefited large companies more than\naverage individuals, a relevant finding for economic decision making. In\naddition, this work benefits bank management strategy by providing forecasting\ncapability for retail-wholesale deposits.\n"
    },
    {
        "paper_id": 2101.09357,
        "authors": "Nicolas Fayard and Chabane Mazri and Alexis Tsouki\\`as",
        "title": "Is the Capability approach a useful tool for decision aiding in public\n  policy making?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper aims at proposing a model representing individuals' welfare using\nSen's capability approach (CA). It is the first step of an attempt to measure\nthe negative impact caused by the damage at a Common on a given population's\nwelfare, and widely speaking, a first step into modelling collective threat.\nThe CA is a multidimensional representation of persons' well-beings which\naccount for human diversity. It has received substantial attention from\nscholars from different disciplines such as philosophy, economics and social\nscientist. Nevertheless, there is no empirical work that really fits the\ntheoretical framework. Our goal is to show that the capability approach can be\nvery useful for decision aiding, especially if we fill the gap between the\ntheory and the empirical work; thus we will propose a framework that is both\nusable and a close representation of what capability is.\n"
    },
    {
        "paper_id": 2101.09373,
        "authors": "Xiaowei Hu, Peng Li",
        "title": "Relief and Stimulus in A Cross-sector Multi-product Scarce Resource\n  Supply Chain Network",
        "comments": null,
        "journal-ref": "Transportation Research Part E: Logistics and Transportation\n  Review, 168, 102932 (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the era of a growing population, systemic changes to the world, and the\nrising risk of crises, humanity has been facing an unprecedented challenge of\nresource scarcity. Confronting and addressing the issues concerning the scarce\nresource's conservation, competition, and stimulation by grappling its\ncharacteristics and adopting viable policy instruments calls the\ndecision-maker's attention with a paramount priority. In this paper, we develop\nthe first general decentralized cross-sector supply chain network model that\ncaptures the unique features of scarce resources under a unifying fiscal policy\nscheme. We formulate the problem as a network equilibrium model with\nfinite-dimensional variational inequality theories. We then characterize the\nnetwork equilibrium with a set of classic theoretical properties, as well as\nwith a set of properties that are novel to the network games application\nliterature, namely, the lowest eigenvalue of the game Jacobian. Lastly, we\nprovide a series of illustrative examples, including a medical glove supply\nnetwork, to showcase how our model can be used to investigate the efficacy of\nthe imposed policies in relieving supply chain distress and stimulating\nwelfare. Our managerial insights inform and expand the political dialogues on\nfiscal policy design, public resource legislation, social welfare\nredistribution, and supply chain practice toward sustainability.\n"
    },
    {
        "paper_id": 2101.09395,
        "authors": "Xiaodong Wang and Fushing Hsieh",
        "title": "Unraveling S&P500 stock volatility and networks -- An\n  encoding-and-decoding approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Volatility of financial stock is referring to the degree of uncertainty or\nrisk embedded within a stock's dynamics. Such risk has been received huge\namounts of attention from diverse financial researchers. By following the\nconcept of regime-switching model, we proposed a non-parametric approach, named\nencoding-and-decoding, to discover multiple volatility states embedded within a\ndiscrete time series of stock returns. The encoding is performed across the\nentire span of temporal time points for relatively extreme events with respect\nto a chosen quantile-based threshold. As such the return time series is\ntransformed into Bernoulli-variable processes. In the decoding phase, we\ncomputationally seek for locations of change points via estimations based on a\nnew searching algorithm in conjunction with the information criterion applied\non the observed collection of recurrence times upon the binary process. Besides\nthe independence required for building the Geometric distributional likelihood\nfunction, the proposed approach can functionally partition the entire return\ntime series into a collection of homogeneous segments without any assumptions\nof dynamic structure and underlying distributions. In the numerical\nexperiments, our approach is found favorably compared with parametric models\nlike Hidden Markov Model. In the real data applications, we introduce the\napplication of our approach in forecasting stock returns. Finally, volatility\ndynamic of every single stock of S&P500 is revealed, and a stock network is\nconsequently established to represent dependency relations derived through\nconcurrent volatility states among S&P500.\n"
    },
    {
        "paper_id": 2101.09543,
        "authors": "Max-Sebastian Dov\\`i",
        "title": "Inference on the New Keynesian Phillips Curve with Very Many\n  Instrumental Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Limited-information inference on New Keynesian Phillips Curves (NKPCs) and\nother single-equation macroeconomic relations is characterised by weak and\nhigh-dimensional instrumental variables (IVs). Beyond the efficiency concerns\npreviously raised in the literature, I show by simulation that ad-hoc selection\nprocedures can lead to substantial biases in post-selection inference. I\npropose a Sup Score test that remains valid under dependent data, arbitrarily\nweak identification, and a number of IVs that increases exponentially with the\nsample size. Conducting inference on a standard NKPC with 359 IVs and 179\nobservations, I find substantially wider confidence sets than those commonly\nfound.\n"
    },
    {
        "paper_id": 2101.09682,
        "authors": "John Ery and Loris Michel",
        "title": "Solving optimal stopping problems with Deep Q-Learning",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a reinforcement learning (RL) approach to model optimal exercise\nstrategies for option-type products. We pursue the RL avenue in order to learn\nthe optimal action-value function of the underlying stopping problem. In\naddition to retrieving the optimal Q-function at any time step, one can also\nprice the contract at inception. We first discuss the standard setting with one\nexercise right, and later extend this framework to the case of multiple\nstopping opportunities in the presence of constraints. We propose to\napproximate the Q-function with a deep neural network, which does not require\nthe specification of basis functions as in the least-squares Monte Carlo\nframework and is scalable to higher dimensions. We derive a lower bound on the\noption price obtained from the trained neural network and an upper bound from\nthe dual formulation of the stopping problem, which can also be expressed in\nterms of the Q-function. Our methodology is illustrated with examples covering\nthe pricing of swing options.\n"
    },
    {
        "paper_id": 2101.09738,
        "authors": "Mykola Babiak and Jozef Barunik",
        "title": "Currency Network Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper identifies new currency risk stemming from a network of\nidiosyncratic option-based currency volatilities and shows how such network\nrisk is priced in the cross-section of currency returns. A portfolio that buys\nnet-receivers and sells net-transmitters of short-term linkages between\ncurrency volatilities generates a significant Sharpe ratio. The network\nstrategy formed on causal connections is uncorrelated with popular benchmarks\nand generates a significant alpha, while network returns formed on aggregate\nconnections, which are driven by a strong correlation component, are partially\nsubsumed by standard factors. Long-term linkages are priced less, indicating a\ndownward-sloping term structure of network risk.\n"
    },
    {
        "paper_id": 2101.09777,
        "authors": "Mikhail Zhitlukhin",
        "title": "Capital growth and survival strategies in a market with endogenous\n  prices",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We call an investment strategy survival, if an agent who uses it maintains a\nnon-vanishing share of market wealth over the infinite time horizon. In a\ndiscrete-time multi-agent model with endogenous asset prices determined through\na short-run equilibrium of supply and demand, we show that a survival strategy\ncan be constructed as follows: an agent should assume that only their actions\ndetermine the prices and use a growth optimal (log-optimal) strategy with\nrespect to these prices, disregarding the actual prices. Then any survival\nstrategy turns out to be close to this strategy asymptotically. The main\nresults are obtained under the assumption that the assets are short-lived.\n"
    },
    {
        "paper_id": 2101.09817,
        "authors": "John C. Stevenson",
        "title": "Population and Inequality Dynamics in Simple Economies",
        "comments": "29 pages with 22 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the use of spatial agent-based and individual-based models has\nflourished across many scientific disciplines, the complexities these models\ngenerate are often difficult to manage and quantify. This research reduces\npopulation-driven, spatial modeling of individuals to the simplest\nconfigurations and parameters: an equal resource opportunity landscape with\nequally capable individuals; and asks the question, \"Will valid complex\npopulation and inequality dynamics emerge from this simple economic model?\" Two\nforaging economies are modeled: subsistence and surplus. The resulting,\nemergent population dynamics are characterized by their sensitivities to agent\nand landscape parameters. The various steady and oscillating regimes of\nsingle-species population dynamics are generated by appropriate selection of\nmodel growth parameters. These emergent dynamics are shown to be consistent\nwith the equation-based, continuum modeling of single-species populations in\nbiology and ecology. The intrinsic growth rates, carry capacities, and delay\nparameters of these models are implied for these simple economies. Aggregate\nmeasures of individual distributions are used to understand the sensitivities\nto model parameters. New local measures are defined to describe complex\nbehaviors driven by spatial effects, especially extinctions. This simple\neconomic model is shown to generate significantly complex population and\ninequality dynamics. Model parameters generating the intrinsic growth rate have\nstrong effects on these dynamics, including large variations in inequality.\nSignificant inequality effects are shown to be caused by birth costs above and\nbeyond their contribution to the intrinsic growth rate. The highest levels of\ninequality are found during the initial non-equilibrium period and are driven\nby factors different than those driving steady state inequality.\n"
    },
    {
        "paper_id": 2101.09886,
        "authors": "Seung Bin Baik",
        "title": "The Two-Sided Market Network Analysis Based on Transfer Entropy & Labelr",
        "comments": "5 pages, 3figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study more complex digital platforms in early stages in the two-sided\nmarket to produce powerful network effects. In this study, I use Transfer\nEntropy to look for super users who connect hominids in different networks to\nachieve higher network effects in the digital platform in the two-sided market,\nwhich has recently become more complex. And this study also aims to redefine\nthe decision criteria of product managers by helping them define users with\nstronger network effects. With the development of technology, the structure of\nthe industry is becoming more difficult to interpret and the complexity of\nbusiness logic is increasing. This phenomenon is the biggest problem that makes\nit difficult for start-ups to challenge themselves. I hope this study will help\nproduct managers create new digital economic networks, enable them to make\nprioritized, data-driven decisions, and find users who can be the hub of the\nnetwork even in small products.\n"
    },
    {
        "paper_id": 2101.0989,
        "authors": "Akihiko Takahashi, Yoshifumi Tsuchida and Toshihiro Yamada",
        "title": "A new efficient approximation scheme for solving high-dimensional\n  semilinear PDEs: control variate method for Deep BSDE solver",
        "comments": "29 page, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.jcp.2022.110956",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new approximation scheme for solving high-dimensional\nsemilinear partial differential equations (PDEs) and backward stochastic\ndifferential equations (BSDEs). First, we decompose a target semilinear PDE\n(BSDE) into two parts, namely \"dominant\" linear and \"small\" nonlinear PDEs.\nThen, we employ a Deep BSDE solver with a new control variate method to solve\nthose PDEs, where approximations based on an asymptotic expansion technique are\neffectively applied to the linear part and also used as control variates for\nthe nonlinear part. Moreover, our theoretical result indicates that errors of\nthe proposed method become much smaller than those of the original Deep BSDE\nsolver. Finally, we show numerical experiments to demonstrate the validity of\nour method, which is consistent with the theoretical result in this paper.\n"
    },
    {
        "paper_id": 2101.09936,
        "authors": "Jin Hyuk Choi, Tae Ung Gang",
        "title": "Optimal investment in illiquid market with search frictions and\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider an optimal investment problem to maximize expected utility of the\nterminal wealth, in an illiquid market with search frictions and transaction\ncosts. In the market model, an investor's attempt of transaction is successful\nonly at arrival times of a Poisson process, and the investor pays proportional\ntransaction costs when the transaction is successful. We characterize the\nno-trade region describing the optimal trading strategy. We provide asymptotic\nexpansions of the boundaries of the no-trade region and the value function, for\nsmall transaction costs. The asymptotic analysis implies that the effects of\nthe transaction costs are more pronounced.\n"
    },
    {
        "paper_id": 2101.0996,
        "authors": "Guilhem Cassan and Milan Van Steenvoort",
        "title": "Political Regime and COVID 19 death rate: efficient, biasing or simply\n  different autocracies ?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The difference in COVID 19 death rates across political regimes has caught a\nlot of attention. The \"efficient autocracy\" view suggests that autocracies may\nbe more efficient at putting in place policies that contain COVID 19 spread. On\nthe other hand, the \"biasing autocracy\" view underlines that autocracies may be\nunder reporting their COVID 19 data. We use fixed effect panel regression\nmethods to discriminate between the two sides of the debate. Our results show\nthat a third view may in fact be prevailing: once pre-determined\ncharacteristics of countries are accounted for, COVID 19 death rates equalize\nacross political regimes. The difference in death rate across political regime\nseems therefore to be primarily due to omitted variable bias.\n"
    },
    {
        "paper_id": 2101.10053,
        "authors": "Jean-Pierre Fouque and Sebastian Jaimungal and Yuri F. Saporito",
        "title": "Optimal Trading with Signals and Stochastic Price Impact",
        "comments": "21 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Trading frictions are stochastic. They are, moreover, in many instances\nfast-mean reverting. Here, we study how to optimally trade in a market with\nstochastic price impact and study approximations to the resulting optimal\ncontrol problem using singular perturbation methods. We prove, by constructing\nsub- and super-solutions, that the approximations are accurate to the specified\norder. Finally, we perform some numerical experiments to illustrate the effect\nthat stochastic trading frictions have on optimal trading.\n"
    },
    {
        "paper_id": 2101.1009,
        "authors": "Faxi Yuan, Amir Esmalian, Bora Oztekin and Ali Mostafavi",
        "title": "Unveiling Spatial Patterns of Disaster Impacts and Recovery Using Credit\n  Card Transaction Variances",
        "comments": "31 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this study is to examine spatial patterns of impacts and\nrecovery of communities based on variances in credit card transactions. Such\nvariances could capture the collective effects of household impacts, disrupted\naccesses, and business closures, and thus provide an integrative measure for\nexamining disaster impacts and community recovery in disasters. Existing\nstudies depend mainly on survey and sociodemographic data for disaster impacts\nand recovery effort evaluations, although such data has limitations, including\nlarge data collection efforts and delayed timeliness results. In addition,\nthere are very few studies have concentrated on spatial patterns and\ndisparities of disaster impacts and short-term recovery of communities,\nalthough such investigation can enhance situational awareness during disasters\nand support the identification of disparate spatial patterns of disaster\nimpacts and recovery in the impacted regions. This study examines credit card\ntransaction data Harris County (Texas, USA) during Hurricane Harvey in 2017 to\nexplore spatial patterns of disaster impacts and recovery during from the\nperspective of community residents and businesses at ZIP code and county\nscales, respectively, and to further investigate their spatial disparities\nacross ZIP codes. The results indicate that individuals in ZIP codes with\npopulations of higher income experienced more severe disaster impact and\nrecovered more quickly than those located in lower-income ZIP codes for most\nbusiness sectors. Our findings not only enhance the understanding of spatial\npatterns and disparities in disaster impacts and recovery for better community\nresilience assessment, but also could benefit emergency managers, city\nplanners, and public officials in harnessing population activity data, using\ncredit card transactions as a proxy for activity, to improve situational\nawareness and resource allocation.\n"
    },
    {
        "paper_id": 2101.10092,
        "authors": "Maximilian Parzen, Fabian Neumann, Addrian H. Van Der Weijde, Daniel\n  Friedrich, Aristides Kiprakis",
        "title": "Beyond cost reduction: Improving the value of energy storage in\n  electricity systems",
        "comments": "17 pages, 10 figures",
        "journal-ref": "Carbon Neutrality volume 1, Article number: 26 (2022)",
        "doi": "10.1007/s43979-022-00027-3",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  An energy storage technology is valuable if it makes energy systems cheaper.\nTraditional ways to improve storage technologies are to reduce their costs;\nhowever, the cheapest energy storage is not always the most valuable in energy\nsystems. Modern techno-economical evaluation methods try to address the cost\nand value situation but do not judge the competitiveness of multiple\ntechnologies simultaneously. This paper introduces the market potential method\nas a new complementary valuation method guiding innovation of multiple energy\nstorage. The market potential method derives the value of technologies by\nexamining common deployment signals from energy system model outputs in a\nstructured way. We apply and compare this method to cost evaluation approaches\nin a renewables-based European power system model, covering diverse energy\nstorage technologies. We find that characteristics of high-cost hydrogen\nstorage can be more valuable than low-cost hydrogen storage. Additionally, we\nshow that modifying the freedom of storage sizing and component interactions\ncan make the energy system 10% cheaper and impact the value of technologies.\nThe results suggest looking beyond the pure cost reduction paradigm and focus\non developing technologies with suitable value approaches that can lead to\ncheaper electricity systems in future.\n"
    },
    {
        "paper_id": 2101.10151,
        "authors": "Cong Chen, Lang Tong, and Ye Guo",
        "title": "Pricing Energy Storage in Real-time Market",
        "comments": "7 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of pricing utility-scale energy storage resources (ESRs) in the\nreal-time electricity market is considered. Under a rolling-window dispatch\nmodel where the operator centrally dispatches generation and consumption under\nforecasting uncertainty, it is shown that almost all uniform pricing schemes,\nincluding the standard locational marginal pricing (LMP), result in lost\nopportunity costs that require out-of-the-market settlements. It is also shown\nthat such settlements give rise to disincentives for generating firms and\nstorage participants to bid truthfully, even when these market participants are\nrational price-takers in a competitive market. Temporal locational marginal\npricing (TLMP) is proposed for ESRs as a generalization of LMP to an in-market\ndiscriminative form. TLMP is a sum of the system-wide energy price, LMP, and\nthe individual state-of-charge price. It is shown that, under arbitrary\nforecasting errors, the rolling-window implementation of TLMP eliminates the\nlost opportunity costs and provides incentives to price-taking firms to bid\ntruthfully with their marginal costs. Numerical examples show insights into the\neffects of uniform and non-uniform pricing mechanisms on dispatch following and\ntruthful bidding incentives.\n"
    },
    {
        "paper_id": 2101.10293,
        "authors": "Nikolaos Athanasios Anagnostopoulos",
        "title": "The Role of Cost in the Integration of Security Features in Integrated\n  Circuits for Smart Cards",
        "comments": "Submission to Research Topics in University of Twente under the\n  auspices of the EIT ICT Labs Master School in the academic year 2013-14",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This essay investigates the role of cost in the development and production of\nsecure integrated circuits. Initially, I make a small introduction on hardware\nattacks on smart cards and some of the reasons behind them. Subsequently, I\nintroduce the production phases of chips that are integrated to smart cards and\ntry to identify the costs affecting each one of them. I proceed to identify how\nadding security features on such integrated circuits may affect the costs of\ntheir development and production. I then make a more thorough investigation on\nthe costs of developing a hardware attack for such chips and try to estimate\nthe potential damages and losses of such an attack. I also go on to examine\npotential ways of reducing the cost of production for secure chips, while\nidentifying the difficulties in adopting them.\n  This essay ends with the conclusion that adding security features to chips\nmeant to be used for secure applications is well worth it, because the costs of\ndeveloping attacks are of comparable amounts to the costs of developing and\nproducing a chip and the potential damages and losses caused by such attacks\ncan be way higher than these costs. Therefore, although the production and\ndevelopment of integrated circuits come at a certain cost and security\nintroduces further additional costs, security is inherently unavoidable in such\nchips. Finally, I additionally identify that security is an evolving concept\nand does not aim to make a chip totally impenetrable, as this may be\nimpossible, but to lower the potential risks, including that of being\ncompromised, to acceptable levels. Thus, a balance needs be found between the\nlevel of security and the levels of cost and risk.\n"
    },
    {
        "paper_id": 2101.10408,
        "authors": "Eiji Yamamura and Yoshiro Tsutsui",
        "title": "How COVID-19 influences healthcare workers' happiness: Panel data\n  analysis in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Healthcare workers are more likely to be infected with the 2019 novel\ncoronavirus (COVID-19) because of unavoidable contact with infected people.\nAlthough they are equipped to reduce the likelihood of infection, their\ndistress has increased. This study examines how COVID-19 influences healthcare\nworkers' happiness, compared to other workers. We constructed panel data via\nInternet surveys during the COVID-19 epidemic in Japan, from March to June\n2020, by surveying the same respondents at different times. The survey period\nstarted before the state of emergency, and ended after deregulation. The key\nfindings are as follows. (1) Overall, the happiness level of healthcare workers\nis lower than that of other workers. (2) The biggest disparity in happiness\nlevel, between healthcare workers and others, was observed after deregulation\nand not during the state of emergency. After deregulation, the difference was\nlarger by 0.26 points, on an 11-point scale, than in the initial wave before\nthe state of emergency.\n"
    },
    {
        "paper_id": 2101.1051,
        "authors": "Nicholas Moehle, Stephen Boyd",
        "title": "A Certainty Equivalent Merton Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Merton problem is the well-known stochastic control problem of choosing\nconsumption over time, as well as an investment mix, to maximize expected\nconstant relative risk aversion (CRRA) utility of consumption. Merton\nformulated the problem and provided an analytical solution in 1970; since then\na number of extensions of the original formulation have been solved. In this\nnote we identify a certainty equivalent problem, i.e., a deterministic optimal\ncontrol problem with the same optimal value function and optimal policy, for\nthe base Merton problem, as well as a number of extensions. When time is\ndiscretized, the certainty equivalent problem becomes a second-order cone\nprogram (SOCP), readily formulated and solved using domain specific languages\nfor convex optimization. This makes it a good starting point for model\npredictive control, a policy that can handle extensions that are either too\ncumbersome or impossible to handle exactly using standard dynamic programming\nmethods.\n"
    },
    {
        "paper_id": 2101.10548,
        "authors": "Nikolaos Athanasios Anagnostopoulos",
        "title": "Exploring the Complicated Relationship Between Patents and Standards,\n  With a Particular Focus on the Telecommunications Sector",
        "comments": "Submission to Strategic Standardisation in Technical University of\n  Berlin under the auspices of the EIT ICT Labs Master School in the academic\n  year 2012-13",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While patents and standards have been identified as essential driving\ncomponents of innovation and market growth, the inclusion of a patent in a\nstandard poses many difficulties. These difficulties arise from the\ncontradicting natures of patents and standards, which makes their combination\nreally challenging, but, also, from the opposing business and market strategies\nof different patent owners involved in the standardisation process. However, a\nvarying set of policies has been adopted to address the issues occurring from\nthe unavoidable inclusion of patents in standards concerning certain industry\nsectors with a constant high degree of innovation, such as telecommunications.\nAs these policies have not always proven adequate enough, constant efforts are\nbeing made to improve and expand them. The intriguing and complicated\nrelationship between patents and standards is finally examined through a review\nof the use cases of well-known standards of the telecommunications sector which\ninclude a growing set of essential patents.\n"
    },
    {
        "paper_id": 2101.10635,
        "authors": "Th\\'eo Roncalli, Th\\'eo Le Guenedal, Fr\\'ed\\'eric Lepetit, Thierry\n  Roncalli, Takaya Sekine",
        "title": "The Market Measure of Carbon Risk and its Impact on the Minimum Variance\n  Portfolio",
        "comments": "15 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:2008.13198",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Like ESG investing, climate change is an important concern for asset managers\nand owners, and a new challenge for portfolio construction. Until now,\ninvestors have mainly measured carbon risk using fundamental approaches, such\nas with carbon intensity metrics. Nevertheless, it has not been proven that\nasset prices are directly impacted by these fundamental-based measures. In this\npaper, we focus on another approach, which consists in measuring the\nsensitivity of stock prices with respect to a carbon risk factor. In our\nopinion, carbon betas are market-based measures that are complementary to\ncarbon intensities or fundamental-based measures when managing investment\nportfolios, because carbon betas may be viewed as an extension or\nforward-looking measure of the current carbon footprint. In particular, we show\nhow this new metric can be used to build minimum variance strategies and how\nthey impact their portfolio construction.\n"
    },
    {
        "paper_id": 2101.10941,
        "authors": "Clint Harris",
        "title": "Identifying and Estimating Perceived Returns to Binary Investments",
        "comments": "60 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I describe a method for estimating agents' perceived returns to investments\nthat relies on cross-sectional data containing binary choices and prices, where\nprices may be imperfectly known to agents. This method identifies the scale of\nperceived returns by assuming agent knowledge of an identity that relates\nprofits, revenues, and costs rather than by eliciting or assuming agent beliefs\nabout structural parameters that are estimated by researchers. With this\nassumption, modest adjustments to standard binary choice estimators enable\nconsistent estimation of perceived returns when using price instruments that\nare uncorrelated with unobserved determinants of agents' price misperceptions\nas well as other unobserved determinants of their perceived returns. I\ndemonstrate the method, and the importance of using price variation that is\nknown to agents, in a series of data simulations.\n"
    },
    {
        "paper_id": 2101.10942,
        "authors": "Yi Wei",
        "title": "Absolute Value Constraint: The Reason for Invalid Performance Evaluation\n  Results of Neural Network Models for Stock Price Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Neural networks for stock price prediction(NNSPP) have been popular for\ndecades. However, most of its study results remain in the research paper and\ncannot truly play a role in the securities market. One of the main reasons\nleading to this situation is that the prediction error(PE) based evaluation\nresults have statistical flaws. Its prediction results cannot represent the\nmost critical financial direction attributes. So it cannot provide investors\nwith convincing, interpretable, and consistent model performance evaluation\nresults for practical applications in the securities market. To illustrate, we\nhave used data selected from 20 stock datasets over six years from the Shanghai\nand Shenzhen stock market in China, and 20 stock datasets from NASDAQ and NYSE\nin the USA. We implement six shallow and deep neural networks to predict stock\nprices and use four prediction error measures for evaluation. The results show\nthat the prediction error value only partially reflects the model accuracy of\nthe stock price prediction, and cannot reflect the change in the direction of\nthe model predicted stock price. This characteristic determines that PE is not\nsuitable as an evaluation indicator of NNSPP. Otherwise, it will bring huge\npotential risks to investors. Therefore, this paper establishes an experiment\nplatform to confirm that the PE method is not suitable for the NNSPP\nevaluation, and provides a theoretical basis for the necessity of creating a\nnew NNSPP evaluation method in the future.\n"
    },
    {
        "paper_id": 2101.10947,
        "authors": "Hampus Engsner",
        "title": "Least Squares Monte Carlo applied to Dynamic Monetary Utility Functions",
        "comments": "30 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we explore ways of numerically computing recursive dynamic\nmonetary risk measures and utility functions. Computationally, this problem\nsuffers from the curse of dimensionality and nested simulations are unfeasible\nif there are more than two time steps. The approach considered in this paper is\nto use a Least Squares Monte Carlo (LSM) algorithm to tackle this problem, a\nmethod which has been primarily considered for valuing American derivatives, or\nmore general stopping time problems, as these also give rise to backward\nrecursions with corresponding challenges in terms of numerical computation. We\ngive some overarching consistency results for the LSM algorithm in a general\nsetting as well as explore numerically its performance for recursive\nCost-of-Capital valuation, a special case of a dynamic monetary utility\nfunction.\n"
    },
    {
        "paper_id": 2101.11001,
        "authors": "Young Shin Kim",
        "title": "Sample path generation of the stochastic volatility CGMY process and its\n  application to path-dependent option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/jrfm14020077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes the sample path generation method for the stochastic\nvolatility version of CGMY process. We present the Monte-Carlo method for\nEuropean and American option pricing with the sample path generation and\ncalibrate model parameters to the American style S\\&P 100 index options market,\nusing the least square regression method. Moreover, we discuss path-dependent\noptions such as Asian and Barrier options.\n"
    },
    {
        "paper_id": 2101.11036,
        "authors": "Dimitrios Tsiotas and Vassilis Tselios",
        "title": "Understanding the uneven spread of COVID-19 in the context of the global\n  interconnected economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using network analysis, this paper develops a multidimensional methodological\nframework for understanding the uneven (cross-country) spread of COVID-19 in\nthe context of the global interconnected economy. The globally interconnected\nsystem of tourism mobility is modeled as a complex network, where two main\nstages in the temporal spread of COVID-19 are revealed and defined by the\ncutting-point of the 44th day from Wuhan. The first stage describes the\noutbreak in Asia and North America, the second one in Europe, South America,\nand Africa, while the outbreak in Oceania is spread along both stages. The\nanalysis shows that highly connected nodes in the global tourism network (GTN)\nare infected early by the pandemic, while nodes of lower connectivity are late\ninfected. Moreover, countries with the same network centrality as China were\nearly infected on average by COVID-19. The paper also finds that network\ninterconnectedness, economic openness, and transport integration are key\ndeterminants in the early global spread of the pandemic, and it reveals that\nthe spatio-temporal patterns of the worldwide spread of COVID-19 are more a\nmatter of network interconnectivity than of spatial proximity.\n"
    },
    {
        "paper_id": 2101.11104,
        "authors": "Bent Flyvbjerg",
        "title": "Four Ways to Scale Up: Smart, Dumb, Forced, and Fumbled",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Scale-up is the process of growing a venture in size. The paper identifies\nmodularity and speed as keys to successful scale-up. On that basis four types\nof scale-up are identified: Smart, dumb, forced, and fumbled. Smart scale-up\ncombines modularity and speed. Dumb scale-up is bespoke and slow, and very\ncommon. The paper presents examples of each type of scale-up, explaining why\nthey were successful or not. Whether you are a small startup or Elon Musk\ntrying to grow Tesla and SpaceX or Jeff Bezos scaling up Amazon - or you are\nthe US, UK, Chinese, or other government trying to increase power production,\nexpand your infrastructure, or make your health, education, and social services\nwork better - modularity and speed are the answer to effective delivery, or so\nthe paper argues. How well you deal with modularity and speed decides whether\nyour efforts succeed or fail. Most ventures, existing or planned, are neither\nfully smart nor fully dumb, but have elements of both. Successful organizations\nwork to tip the balance towards smart by (a) introducing elements of smart\nscale-up into existing ventures and (b) starting new, fully smart-scaled\nventures, to make themselves less dumb and ever smarter.\n"
    },
    {
        "paper_id": 2101.11366,
        "authors": "Julia Schmitt, Klaus M. Miller, Bernd Skiera",
        "title": "The Impact of Privacy Laws on Online User Behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Policymakers worldwide draft privacy laws that require trading-off between\nsafeguarding consumer privacy and preventing economic loss to companies that\nuse consumer data. However, little empirical knowledge exists as to how privacy\nlaws affect companies' performance. Accordingly, this paper empirically\nquantifies the effects of the enforcement of the EU's General Data Protection\nRegulation (GDPR) on online user behavior over time, analyzing data from 6,286\nwebsites spanning 24 industries during the 10 months before and 18 months after\nthe GDPR's enforcement in 2018. A panel differences estimator, with a synthetic\ncontrol group approach, isolates the short- and long-term effects of the GDPR\non user behavior. The results show that, on average, the GDPR's effects on user\nquantity and usage intensity are negative; e.g., the numbers of total visits to\na website decrease by 4.9% and 10% due to GDPR in respectively the short- and\nlong-term. These effects could translate into average revenue losses of $7\nmillion for e-commerce websites and almost $2.5 million for ad-based websites\n18 months after GDPR. The GDPR's effects vary across websites, with some\nindustries even benefiting from it; moreover, more-popular websites suffer\nless, suggesting that the GDPR increased market concentration.\n"
    },
    {
        "paper_id": 2101.11465,
        "authors": "Till Feier, Jan Gogoll, Matthias Uhl",
        "title": "Hiding Behind Machines: When Blame Is Shifted to Artificial Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The transfer of tasks with sometimes far-reaching moral implications to\nautonomous systems raises a number of ethical questions. In addition to\nfundamental questions about the moral agency of these systems, behavioral\nissues arise. This article focuses on the responsibility of agents who decide\non our behalf. We investigate the empirically accessible question of whether\nthe production of moral outcomes by an agent is systematically judged\ndifferently when the agent is artificial and not human. The results of a\nlaboratory experiment suggest that decision-makers can actually rid themselves\nof guilt more easily by delegating to machines than by delegating to other\npeople. Our results imply that the availability of artificial agents could\nprovide stronger incentives for decision makers to delegate morally sensitive\ndecisions.\n"
    },
    {
        "paper_id": 2101.11496,
        "authors": "Song-Ju Kim, Taiki Takahashi, and Kazuo Sano",
        "title": "A Balance for Fairness: Fair Distribution Utilising Physics in Games of\n  Characteristic Function Form",
        "comments": "13 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In chaotic modern society, there is an increasing demand for the realization\nof true 'fairness'. In Greek mythology, Themis, the 'goddess of justice', has a\nsword in her right hand to protect society from vices, and a 'balance of\njudgment' in her left hand that measures good and evil. In this study, we\npropose a fair distribution method 'utilising physics' for the profit in games\nof characteristic function form. Specifically, we show that the linear\nprogramming problem for calculating 'nucleolus' can be efficiently solved by\nconsidering it as a physical system in which gravity works. In addition to\nbeing able to significantly reduce computational complexity thereby, we believe\nthat this system could have flexibility necessary to respond to real-time\nchanges in the parameter.\n"
    },
    {
        "paper_id": 2101.11532,
        "authors": "Soheil Ghili",
        "title": "A Characterization for Optimal Bundling of Products with Non-Additive\n  Values",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies optimal bundling of products with non-additive values.\nUnder monotonic preferences and single-peaked profits, I show a monopolist\nfinds pure bundling optimal if and only if the optimal sales volume for the\ngrand bundle is larger than the optimal sales volume for any smaller bundle. I\nthen (i) detail how my analysis relates to \"ratio monotonicity\" results on\nbundling; and (ii) describe the implications for non-linear pricing.\n"
    },
    {
        "paper_id": 2101.11551,
        "authors": "Lilian Korir, Archie Drake, Martin Collison, Tania Carolina\n  Camacho-Villa, Elizabeth Sklar, and Simon Pearson",
        "title": "Current and Emergent Economic Impacts of Covid-19 and Brexit on UK Fresh\n  Produce and Horticultural Businesses",
        "comments": "15pages, 1 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper describes a study designed to investigate the current and emergent\nimpacts of Covid-19 and Brexit on UK horticultural businesses. Various\ncharacteristics of UK horticultural production, notably labour reliance and\nimport dependence, make it an important sector for policymakers concerned to\nunderstand the effects of these disruptive events as we move from 2020 into\n2021. The study design prioritised timeliness, using a rapid survey to gather\ninformation from a relatively small (n = 19) but indicative group of producers.\nThe main novelty of the results is to suggest that a very substantial majority\nof producers either plan to scale back production in 2021 (47%) or have been\nunable to make plans for 2021 because of uncertainty (37%). The results also\nadd to broader evidence that the sector has experienced profound labour supply\nchallenges, with implications for labour cost and quality. The study discusses\nthe implications of these insights from producers in terms of productivity and\nautomation, as well as in terms of broader economic implications. Although\nautomation is generally recognised as the long-term future for the industry\n(89%), it appeared in the study as the second most referred short-term option\n(32%) only after changes to labour schemes and policies (58%). Currently,\nautomation plays a limited role in contributing to the UK's horticultural\nworkforce shortage due to economic and socio-political uncertainties. The\nconclusion highlights policy recommendations and future investigative\nintentions, as well as suggesting methodological and other discussion points\nfor the research community.\n"
    },
    {
        "paper_id": 2101.1159,
        "authors": "Mark Kiermayer",
        "title": "Modeling surrender risk in life insurance: theoretical and experimental\n  insight",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Surrender poses one of the major risks to life insurance and a sound modeling\nof its true probability has direct implication on the risk capital demanded by\nthe Solvency II directive. We add to the existing literature by performing\nextensive experiments that present highly practical results for various\nmodeling approaches, including XGBoost, random forest, GLM and neural networks.\nFurther, we detect shortcomings of prevalent model assessments, which are in\nessence based on a confusion matrix. Our results indicate that accurate label\npredictions and a sound modeling of the true probability can be opposing\nobjectives. We illustrate this with the example of resampling. While resampling\nis capable of improving label prediction in rare event settings, such as\nsurrender, and thus is commonly applied, we show theoretically and numerically\nthat models trained on resampled data predict significantly biased event\nprobabilities. Following a probabilistic perspective on surrender, we further\npropose time-dependent confidence bands on predicted mean surrender rates as a\ncomplementary assessment and demonstrate its benefit. This evaluation takes a\nvery practical, going concern perspective, which respects that the composition\nof a portfolio, as well as the nature of underlying risk drivers might change\nover time.\n"
    },
    {
        "paper_id": 2101.11897,
        "authors": "Lukas Gonon and Christoph Schwab",
        "title": "Deep ReLU Network Expression Rates for Option Prices in\n  high-dimensional, exponential L\\'evy models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the expression rates of deep neural networks (DNNs for short) for\noption prices written on baskets of $d$ risky assets, whose log-returns are\nmodelled by a multivariate L\\'evy process with general correlation structure of\njumps. We establish sufficient conditions on the characteristic triplet of the\nL\\'evy process $X$ that ensure $\\varepsilon$ error of DNN expressed option\nprices with DNNs of size that grows polynomially with respect to\n$\\mathcal{O}(\\varepsilon^{-1})$, and with constants implied in\n$\\mathcal{O}(\\cdot)$ which grow polynomially with respect $d$, thereby\novercoming the curse of dimensionality and justifying the use of DNNs in\nfinancial modelling of large baskets in markets with jumps.\n  In addition, we exploit parabolic smoothing of Kolmogorov partial\nintegrodifferential equations for certain multivariate L\\'evy processes to\npresent alternative architectures of ReLU DNNs that provide $\\varepsilon$\nexpression error in DNN size $\\mathcal{O}(|\\log(\\varepsilon)|^a)$ with exponent\n$a \\sim d$, however, with constants implied in $\\mathcal{O}(\\cdot)$ growing\nexponentially with respect to $d$. Under stronger, dimension-uniform\nnon-degeneracy conditions on the L\\'evy symbol, we obtain algebraic expression\nrates of option prices in exponential L\\'evy models which are free from the\ncurse of dimensionality. In this case the ReLU DNN expression rates of prices\ndepend on certain sparsity conditions on the characteristic L\\'evy triplet. We\nindicate several consequences and possible extensions of the present results.\n"
    },
    {
        "paper_id": 2101.12008,
        "authors": "Christopher Cassion, Yuhang Qian, Constant Bossou, Margareta Ackerman",
        "title": "Investors Embrace Gender Diversity, Not Female CEOs: The Role of Gender\n  in Startup Fundraising",
        "comments": "20 pages, 9 figures, appeared in EAI Intetain 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The allocation of venture capital is one of the primary factors determining\nwho takes products to market, which startups succeed or fail, and as such who\ngets to participate in the shaping of our collective economy. While gender\ndiversity contributes to startup success, most funding is allocated to\nmale-only entrepreneurial teams. In the wake of COVID-19, 2020 is seeing a\nnotable decline in funding to female and mixed-gender teams, giving raise to an\nurgent need to study and correct the longstanding gender bias in startup\nfunding allocation. We conduct an in-depth data analysis of over 48,000\ncompanies on Crunchbase, comparing funding allocation based on the gender\ncomposition of founding teams. Detailed findings across diverse industries and\ngeographies are presented. Further, we construct machine learning models to\npredict whether startups will reach an equity round, revealing the surprising\nfinding that the CEO's gender is the primary determining factor for attaining\nfunding. Policy implications for this pressing issue are discussed.\n"
    },
    {
        "paper_id": 2101.12262,
        "authors": "Takaaki Koike, Shogo Kato, Marius Hofert",
        "title": "Measuring non-exchangeable tail dependence using tail copulas",
        "comments": "26 pages, 5 figures, 2 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantifying tail dependence is an important issue in insurance and risk\nmanagement. The prevalent tail dependence coefficient (TDC), however, is known\nto underestimate the degree of tail dependence and it does not capture\nnon-exchangeable tail dependence since it evaluates the limiting tail\nprobability only along the main diagonal. To overcome these issues, two novel\ntail dependence measures called the maximal tail concordance measure (MTCM) and\nthe average tail concordance measure (ATCM) are proposed. Both measures are\nconstructed based on tail copulas and possess clear probabilistic\ninterpretations in that the MTCM evaluates the largest limiting probability\namong all comparable rectangles in the tail, and the ATCM is a normalized\naverage of these limiting probabilities. In contrast to the TDC, the proposed\nmeasures can capture non-exchangeable tail dependence. Analytical forms of the\nproposed measures are also derived for various copulas. A real data analysis\nreveals striking tail dependence and tail non-exchangeability of the return\nseries of stock indices, particularly in periods of financial distress.\n"
    },
    {
        "paper_id": 2101.12306,
        "authors": "Peter G. Hansen",
        "title": "New Formulations of Ambiguous Volatility with an Application to Optimal\n  Dynamic Contracting",
        "comments": "38 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I introduce novel preference formulations which capture aversion to ambiguity\nabout unknown and potentially time-varying volatility. I compare these\npreferences with Gilboa and Schmeidler's maxmin expected utility as well as\nvariational formulations of ambiguity aversion. The impact of ambiguity\naversion is illustrated in a simple static model of portfolio choice, as well\nas a dynamic model of optimal contracting under repeated moral hazard.\nImplications for investor beliefs, optimal design of corporate securities, and\nasset pricing are explored.\n"
    },
    {
        "paper_id": 2101.12387,
        "authors": "Daeyung Gim and Hyungbin Park",
        "title": "A deep learning algorithm for optimal investment strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper treats the Merton problem how to invest in safe assets and risky\nassets to maximize an investor's utility, given by investment opportunities\nmodeled by a $d$-dimensional state process. The problem is represented by a\npartial differential equation with optimizing term: the Hamilton-Jacobi-Bellman\nequation. The main purpose of this paper is to solve partial differential\nequations derived from the Hamilton-Jacobi-Bellman equations with a deep\nlearning algorithm: the Deep Galerkin method, first suggested by Sirignano and\nSpiliopoulos (2018). We then apply the algorithm to get the solution of the PDE\nbased on some model settings and compare with the one from the finite\ndifference method.\n"
    },
    {
        "paper_id": 2101.12402,
        "authors": "Suman Thapa and Yiqiang Q. Zhao",
        "title": "Estimating value at risk and conditional tail expectation for extreme\n  and aggregate risks",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate risk measures such as value at risk (VaR) and\nthe conditional tail expectation (CTE) of the extreme (maximum and minimum) and\nthe aggregate (total) of two dependent risks. In finance, insurance and the\nother fields, when people invest their money in two or more dependent or\nindependent markets, it is very important to know the extreme and total risk\nbefore the investment. To find these risk measures for dependent cases is quite\nchallenging, which has not been reported in the literature to the best of our\nknowledge. We use the FGM copula for modelling the dependence as it is\nrelatively simple for computational purposes and has empirical successes. The\nmarginal of the risks are considered as exponential and pareto, separately, for\nthe case of extreme risk and as exponential for the case of the total risk. The\neffect of the degree of dependency on the VaR and CTE of the extreme and total\nrisks is analyzed. We also make comparisons for the dependent and independent\nrisks. Moreover, we propose a new risk measure called median of tail (MoT) and\ninvestigate MoT for the extreme and aggregate dependent risks.\n"
    },
    {
        "paper_id": 2101.12526,
        "authors": "Sebastian Kr\\\"ugel, Matthias Uhl",
        "title": "The Behavioral Economics of Intrapersonal Conflict: A Critical\n  Assessment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Preferences often change -- even in short time intervals -- due to either the\nmere passage of time (present-biased preferences) or changes in environmental\nconditions (state-dependent preferences). On the basis of the empirical\nfindings in the context of state-dependent preferences, we critically discuss\nthe Aristotelian view of unitary decision makers in economics and urge a more\nHeraclitean perspective on human decision-making. We illustrate that the\nconceptualization of preferences as present-biased or state-dependent has very\ndifferent normative implications under the Aristotelian view, although both\nconcepts are empirically hard to distinguish. This is highly problematic, as it\nrenders almost any paternalistic intervention justifiable.\n"
    },
    {
        "paper_id": 2101.12684,
        "authors": "Bart H.L. Overes and Michel van der Wel",
        "title": "Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving\n  Factors using Machine Learning Techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sovereign credit ratings summarize the creditworthiness of countries. These\nratings have a large influence on the economy and the yields at which\ngovernments can issue new debt. This paper investigates the use of a Multilayer\nPerceptron (MLP), Classification and Regression Trees (CART), Support Vector\nMachines (SVM), Na\\\"ive Bayes (NB), and an Ordered Logit (OL) model for the\nprediction of sovereign credit ratings. We show that MLP is best suited for\npredicting sovereign credit ratings, with a random cross-validated accuracy of\n68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation\nof the determining factors shows that there is some heterogeneity in the\nimportant variables across the models. However, the two models with the highest\nout-of-sample predictive accuracy, MLP and CART, show a lot of similarities in\nthe influential variables, with regulatory quality, and GDP per capita as\ncommon important variables. Consistent with economic theory, a higher\nregulatory quality and/or GDP per capita are associated with a higher credit\nrating.\n"
    },
    {
        "paper_id": 2101.12693,
        "authors": "Carol Alexander, Michael Coulon, Yang Han, Xiaochun Meng",
        "title": "Evaluating the Discrimination Ability of Proper Multivariate Scoring\n  Rules",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Proper scoring rules are commonly applied to quantify the accuracy of\ndistribution forecasts. Given an observation they assign a scalar score to each\ndistribution forecast, with the the lowest expected score attributed to the\ntrue distribution. The energy and variogram scores are two rules that have\nrecently gained some popularity in multivariate settings because their\ncomputation does not require a forecast to have parametric density function and\nso they are broadly applicable. Here we conduct a simulation study to compare\nthe discrimination ability between the energy score and three variogram scores.\nCompared with other studies, our simulation design is more realistic because it\nis supported by a historical data set containing commodity prices, currencies\nand interest rates, and our data generating processes include a diverse\nselection of models with different marginal distributions, dependence\nstructure, and calibration windows. This facilitates a comprehensive comparison\nof the performance of proper scoring rules in different settings. To compare\nthe scores we use three metrics: the mean relative score, error rate and a\ngeneralised discrimination heuristic. Overall, we find that the variogram score\nwith parameter p=0.5 outperforms the energy score and the other two variogram\nscores.\n"
    },
    {
        "paper_id": 2102.00001,
        "authors": "Jessica Martin (INSA Toulouse), St\\'ephane Villeneuve (TSE)",
        "title": "A Class of Explicit optimal contracts in the face of shutdown",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What type of delegation contract should be offered when facing a risk of the\nmagnitude of the pandemic we are currently experiencing and how does the\nlikelihood of an exogenous early termination of the relationship modify the\nterms of a full-commitment contract? We study these questions by considering a\ndynamic principal-agent model that naturally extends the classical\nHolmstr{\\\"o}m-Milgrom setting to include a risk of default whose origin is\nindependent of the inherent agency problem. We obtain an explicit\ncharacterization of the optimal wage along with the optimal action provided by\nthe agent. The optimal contract is linear by offering both a fixed share of the\noutput which is similar to the standard shutdown-free Holmstr{\\\"o}m-Milgrom\nmodel and a linear prevention mechanism that is proportional to the random\nlifetime of the contract. We then tweak the model to add a possibility for risk\nmitigation through investment and study its optimality.\n"
    },
    {
        "paper_id": 2102.0007,
        "authors": "Areejit Samal, Sunil Kumar, Yasharth Yadav, and Anirban Chakraborti",
        "title": "Network-centric indicators for fragility in global financial indices",
        "comments": "32 pages, 18 figures, including supplementary material",
        "journal-ref": "Front. Phys. 8:624373 (2021)",
        "doi": "10.3389/fphy.2020.624373",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the last two decades, financial systems have been studied and analysed\nfrom the perspective of complex networks, where the nodes and edges in the\nnetwork represent the various financial components and the strengths of\ncorrelations between them. Here, we adopt a similar network-based approach to\nanalyse the daily closing prices of 69 global financial market indices across\n65 countries over a period of 2000-2014. We study the correlations among the\nindices by constructing threshold networks superimposed over minimum spanning\ntrees at different time frames. We investigate the effect of critical events in\nfinancial markets (crashes and bubbles) on the interactions among the indices\nby performing both static and dynamic analyses of the correlations. We compare\nand contrast the structures of these networks during periods of crashes and\nbubbles, with respect to the normal periods in the market. In addition, we\nstudy the temporal evolution of traditional market indicators, various global\nnetwork measures and the recently developed edge-based curvature measures. We\nshow that network-centric measures can be extremely useful in monitoring the\nfragility in the global financial market indices.\n"
    },
    {
        "paper_id": 2102.00197,
        "authors": "Nicola Melluso, Andrea Bonaccorsi, Filippo Chiarello, Gualtiero\n  Fantoni",
        "title": "Rapid detection of fast innovation under the pressure of COVID-19",
        "comments": "Published in PlOs One in 12/31/2020",
        "journal-ref": "PLOS ONE (2020) 15(12): e0244175",
        "doi": "10.1371/journal.pone.0244175",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Covid-19 has rapidly redefined the agenda of technological research and\ndevelopment both for academics and practitioners. If the medical scientific\npublication system has promptly reacted to this new situation, other domains,\nparticularly in new technologies, struggle to map what is happening in their\ncontexts. The pandemic has created the need for a rapid detection of\ntechnological convergence phenomena, but at the same time it has made clear\nthat this task is impossible on the basis of traditional patent and publication\nindicators. This paper presents a novel methodology to perform a rapid\ndetection of the fast technological convergence phenomenon that is occurring\nunder the pressure of the Covid-19 pandemic. The fast detection has been\nperformed thanks to the use of a novel source: the online blogging platform\nMedium. We demonstrate that the hybrid structure of this social journalism\nplatform allows a rapid detection of innovation phenomena, unlike other\ntraditional sources. The technological convergence phenomenon has been modelled\nthrough a network-based approach, analysing the differences of networks\ncomputed during two time periods (pre and post COVID-19). The results led us to\ndiscuss the repurposing of technologies regarding \"Remote Control\", \"Remote\nWorking\", \"Health\" and \"Remote Learning\".\n"
    },
    {
        "paper_id": 2102.00233,
        "authors": "Matheus E. Leusin, Bjoern Jindra, Daniel S. Hain",
        "title": "An evolutionary view on the emergence of Artificial Intelligence",
        "comments": "Keywords: Artificial Intelligence; technological space; evolutionary\n  economic geography; technological relatedness; knowledge complexity",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper draws upon the evolutionary concepts of technological relatedness\nand knowledge complexity to enhance our understanding of the long-term\nevolution of Artificial Intelligence (AI). We reveal corresponding patterns in\nthe emergence of AI - globally and in the context of specific geographies of\nthe US, Japan, South Korea, and China. We argue that AI emergence is associated\nwith increasing related variety due to knowledge commonalities as well as\nincreasing complexity. We use patent-based indicators for the period between\n1974-2018 to analyse the evolution of AI's global technological space, to\nidentify its technological core as well as changes to its overall relatedness\nand knowledge complexity. At the national level, we also measure countries'\noverall specialisations against AI-specific ones. At the global level, we find\nincreasing overall relatedness and complexity of AI. However, for the\ntechnological core of AI, which has been stable over time, we find decreasing\nrelated variety and increasing complexity. This evidence points out that AI\ninnovations related to core technologies are becoming increasingly distinct\nfrom each other. At the country level, we find that the US and Japan have been\nincreasing the overall relatedness of their innovations. The opposite is the\ncase for China and South Korea, which we associate with the fact that these\ncountries are overall less technologically developed than the US and Japan.\nFinally, we observe a stable increasing overall complexity for all countries\napart from China, which we explain by the focus of this country in technologies\nnot strongly linked to AI.\n"
    },
    {
        "paper_id": 2102.00242,
        "authors": "Kiyoshi Kanazawa and Didier Sornette",
        "title": "Ubiquitous power law scaling in nonlinear self-excited Hawkes processes",
        "comments": "To appear in Phys. Rev. Lett. (5 pages, 2 figures + Appendices)",
        "journal-ref": "Phys. Rev. Lett. 127, 188301 (2021)",
        "doi": "10.1103/PhysRevLett.127.188301",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The origin(s) of the ubiquity of probability distribution functions (PDF)\nwith power law tails is still a matter of fascination and investigation in many\nscientific fields from linguistic, social, economic, computer sciences to\nessentially all natural sciences. In parallel, self-excited dynamics is a\nprevalent characteristic of many systems, from the physics of shot noise and\nintermittent processes, to seismicity, financial and social systems. Motivated\nby activation processes of the Arrhenius form, we bring the two threads\ntogether by introducing a general class of nonlinear self-excited point\nprocesses with fast-accelerating intensities as a function of \"tension\".\nSolving the corresponding master equations, we find that a wide class of such\nnonlinear Hawkes processes have the PDF of their intensities described by a\npower law on the condition that (i) the intensity is a fast-accelerating\nfunction of tension, (ii) the distribution of marks is two-sided with\nnon-positive mean, and (iii) it has fast-decaying tails. In particular, Zipf's\nscaling is obtained in the limit where the average mark is vanishing. This\nunearths a novel mechanism for power laws including Zipf's law, providing a new\nunderstanding of their ubiquity.\n"
    },
    {
        "paper_id": 2102.00298,
        "authors": "Stefano Bianchini, Moritz M\\\"uller, Pierre Pelletier, Kevin Wirtz",
        "title": "Global health science leverages established collaboration network to\n  fight COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How has the science system reacted to the early stages of the COVID-19\npandemic? Here we compare the (growing) international network for coronavirus\nresearch with the broader international health science network. Our findings\nshow that, before the outbreak, coronavirus research realized a relatively\nsmall and rather peculiar niche within the global health sciences. As a\nresponse to the pandemic, the international network for coronavirus research\nexpanded rapidly along the hierarchical structure laid out by the global health\nscience network. Thus, in face of the crisis, the global health science system\nproved to be structurally stable yet versatile in research. The observed\nversatility supports optimistic views on the role of science in meeting future\nchallenges. However, the stability of the global core-periphery structure may\nbe worrying, because it reduces learning opportunities and social capital of\nscientifically peripheral countries -- not only during this pandemic but also\nin its \"normal\" mode of operation.\n"
    },
    {
        "paper_id": 2102.00444,
        "authors": "Clare Leaver, Owen Ozier, Pieter Serneels, and Andrew Zeitlin",
        "title": "Recruitment, effort, and retention effects of performance contracts for\n  civil servants: Experimental evidence from Rwandan primary schools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper reports on a two-tiered experiment designed to separately identify\nthe selection and effort margins of pay-for-performance (P4P). At the\nrecruitment stage, teacher labor markets were randomly assigned to a\n'pay-for-percentile' or fixed-wage contract. Once recruits were placed, an\nunexpected, incentive-compatible, school-level re-randomization was performed,\nso that some teachers who applied for a fixed-wage contract ended up being paid\nby P4P, and vice versa. By the second year of the study, the within-year effort\neffect of P4P was 0.16 standard deviations of pupil learning, with the total\neffect rising to 0.20 standard deviations after allowing for selection.\n"
    },
    {
        "paper_id": 2102.00447,
        "authors": "A. Singh, A. Forcina, K. Muniyoor",
        "title": "Social Mobility in India",
        "comments": "11 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rapid rise in income inequality in India is a serious concern. While the\nemphasis is on inclusive growth, it seems difficult to tackle the problem\nwithout looking at the intricacies of the problem. Social mobility is one such\nimportant tool which helps in reaching the cause of the problem and focuses on\nbringing long term equality in the country. The purpose of this study is to\nexamine the role of social background and education attainment in generating\noccupation mobility in the country. By applying an extended version of the RC\nassociation model to 68th round (2011-12) of the Employment and Unemployment\nSurvey by the National Sample Survey Office of India, we found that the role of\neducation is not important in generating occupation mobility in India, while\nsocial background plays a critical role in determining one's occupation. This\nstudy successfully highlights the strong intergenerational occupation\nimmobility in the country and also the need to focus on education. In this\nregard, further studies are needed to uncover other crucial factors limiting\nthe growth of individuals in the country.\n"
    },
    {
        "paper_id": 2102.00477,
        "authors": "Bruno Scalzo, Alvaro Arroyo, Ljubisa Stankovic, Danilo P. Mandic",
        "title": "Nonstationary Portfolios: Diversification in the Spectral Domain",
        "comments": "5 pages, 3 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2007.13855",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical portfolio optimization methods typically determine an optimal\ncapital allocation through the implicit, yet critical, assumption of\nstatistical time-invariance. Such models are inadequate for real-world markets\nas they employ standard time-averaging based estimators which suffer\nsignificant information loss if the market observables are non-stationary. To\nthis end, we reformulate the portfolio optimization problem in the spectral\ndomain to cater for the nonstationarity inherent to asset price movements and,\nin this way, allow for optimal capital allocations to be time-varying. Unlike\nexisting spectral portfolio techniques, the proposed framework employs\naugmented complex statistics in order to exploit the interactions between the\nreal and imaginary parts of the complex spectral variables, which in turn\nallows for the modelling of both harmonics and cyclostationarity in the time\ndomain. The advantages of the proposed framework over traditional methods are\ndemonstrated through numerical simulations using real-world price data.\n"
    },
    {
        "paper_id": 2102.00497,
        "authors": "Massimo Riccaboni, Luca Verginer",
        "title": "The Impact of the COVID-19 Pandemic on Scientific Research in the Life\n  Sciences",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0263001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The COVID-19 outbreak has posed an unprecedented challenge to humanity and\nscience. On the one side, public and private incentives have been put in place\nto promptly allocate resources toward research areas strictly related to the\nCOVID-19 emergency. But on the flip side, research in many fields not directly\nrelated to the pandemic has lagged behind. In this paper, we assess the impact\nof COVID-19 on world scientific production in the life sciences. We investigate\nhow the usage of medical subject headings (MeSH) has changed following the\noutbreak. We estimate through a difference-in-differences approach the impact\nof COVID-19 on scientific production through PubMed. We find that COVID-related\nresearch topics have risen to prominence, displaced clinical publications,\ndiverted funds away from research areas not directly related to COVID-19 and\nthat the number of publications on clinical trials in unrelated fields has\ncontracted. Our results call for urgent targeted policy interventions to\nreactivate biomedical research in areas that have been neglected by the\nCOVID-19 emergency.\n"
    },
    {
        "paper_id": 2102.00587,
        "authors": "Hans Lustfeld",
        "title": "Controlling volatility of wind-solar power",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The main advantage of wind and solar power plants is the power production\nfree of CO2. Their main disadvantage is the volatility of the generated power.\nAccording to the estimates of H.-W. Sinn[1], suppressing this volatility\nrequires pumped-storage plants with a huge capacity, several orders of\nmagnitude larger than the present available capacity in Germany[2]. Sinn\nconcluded that wind-solar power can be used only together with conventional\npower plants as backups. However, based on German power data[3] of 2019 we show\nthat the required storage capacity can significantly be reduced, provided i) a\nsurplus of wind-solar power plants is supplied, ii) smart meters are installed,\niii) partly a different kind of wind turbines and solar panels are used in\nGermany. Our calculations suggest that all the electric energy, presently\nproduced in Germany, can be obtained from wind-solar power alone. And our\nresults let us predict that wind-solar power can be used to produce in addition\nthe energy for transportation, warm water, space heating and in part for\nprocess heating, meaning an increase of the present electric energy production\nby a factor of about 5[1]. Of course, to put such a prediction on firm ground\nthe present calculations have to be confirmed for a period of many years. And\nit should be kept in mind, that in any case a huge number of wind turbines and\nsolar panels is required.\n"
    },
    {
        "paper_id": 2102.00626,
        "authors": "Yixin Cao and Chuanwei Zou and Xianfeng Cheng",
        "title": "Flashot: A Snapshot of Flash Loan Attack on DeFi Ecosystem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Flash Loan attack can grab millions of dollars from decentralized vaults in\none single transaction, drawing increasing attention from the Decentralized\nFinance (DeFi) players. It has also demonstrated an exciting opportunity that a\nhuge wealth could be created by composing DeFi's building blocks and exploring\nthe arbitrage change. However, a fundamental framework to study the field of\nDeFi has not yet reached a consensus and there's a lack of standard tools or\nlanguages to help better describe, design and improve the running processes of\nthe infant DeFi systems, which naturally makes it harder to understand the\nbasic principles behind the complexity of Flash Loan attacks.\n  In this paper, we are the first to propose Flashot, a prototype that is able\nto transparently illustrate the precise asset flows intertwined with smart\ncontracts in a standardized diagram for each Flash Loan event. Some use cases\nare shown and specifically, based on Flashot, we study a typical Pump and\nArbitrage case and present in-depth economic explanations to the attacker's\nbehaviors. Finally, we conclude the development trends of Flash Loan attacks\nand discuss the great impact on DeFi ecosystem brought by Flash Loan. We\nenvision a brand new quantitative financial industry powered by highly\nefficient automatic risk and profit detection systems based on the blockchain.\n"
    },
    {
        "paper_id": 2102.00659,
        "authors": "Peter P. Rohde, Vijay Mohan, Sinclair Davidson, Chris Berg, Darcy\n  Allen, Gavin K. Brennen, Jason Potts",
        "title": "Quantum crypto-economics: Blockchain prediction markets for the\n  evolution of quantum technology",
        "comments": "12 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Two of the most important technological advancements currently underway are\nthe advent of quantum technologies, and the transitioning of global financial\nsystems towards cryptographic assets, notably blockchain-based cryptocurrencies\nand smart contracts. There is, however, an important interplay between the two,\ngiven that, in due course, quantum technology will have the ability to directly\ncompromise the cryptographic foundations of blockchain. We explore this complex\ninterplay by building financial models for quantum failure in various\nscenarios, including pricing quantum risk premiums. We call this quantum\ncrypto-economics.\n"
    },
    {
        "paper_id": 2102.00687,
        "authors": "Kota Ogasawara, Erika Igarashi",
        "title": "The Impacts of the Gender Imbalance on the Marriage Market: Evidence\n  from World War II in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study uses the unprecedented changes in the sex ratio due to the losses\nof men during World War II to identify the impacts of the gender imbalance on\nmarriage market outcomes in Japan. Using newly digitized census-based\nhistorical statistics, we find evidence that men have a stronger bargaining\nposition in the marriage market than women do. Under the conditions of relative\nmale scarcity, women are less likely to marry. Although the entry of younger\ncohorts with a natural gender balance into the marriage market attenuated its\nmagnitude, this tendency persisted until the mid-1950s. Widowed women facing\nmale scarcity are particularly unable to remarry. Our results suggest that\nreinstating military pensions in the early 1950s further reduced their\nincentive to remarry.\n"
    },
    {
        "paper_id": 2102.00968,
        "authors": "Jonathan Berrisch, Florian Ziel",
        "title": "CRPS Learning",
        "comments": "Accepted for publication in Journal of Econometrics",
        "journal-ref": null,
        "doi": "10.1016/j.jeconom.2021.11.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Combination and aggregation techniques can significantly improve forecast\naccuracy. This also holds for probabilistic forecasting methods where\npredictive distributions are combined. There are several time-varying and\nadaptive weighting schemes such as Bayesian model averaging (BMA). However, the\nquality of different forecasts may vary not only over time but also within the\ndistribution. For example, some distribution forecasts may be more accurate in\nthe center of the distributions, while others are better at predicting the\ntails. Therefore, we introduce a new weighting method that considers the\ndifferences in performance over time and within the distribution. We discuss\npointwise combination algorithms based on aggregation across quantiles that\noptimize with respect to the continuous ranked probability score (CRPS). After\nanalyzing the theoretical properties of pointwise CRPS learning, we discuss B-\nand P-Spline-based estimation techniques for batch and online learning, based\non quantile regression and prediction with expert advice. We prove that the\nproposed fully adaptive Bernstein online aggregation (BOA) method for pointwise\nCRPS online learning has optimal convergence properties. They are confirmed in\nsimulations and a probabilistic forecasting study for European emission\nallowance (EUA) prices.\n"
    },
    {
        "paper_id": 2102.01071,
        "authors": "Pramod C. Mane, Nagarajan Krishnamurthy, Kapil Ahuja",
        "title": "Resource Availability in the Social Cloud: An Economics Perspective",
        "comments": "11 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on social cloud formation, where agents are involved in a\ncloseness-based conditional resource sharing and build their resource sharing\nnetwork themselves. The objectives of this paper are: (1) to investigate the\nimpact of agents' decisions of link addition and deletion on their local and\nglobal resource availability, (2) to analyze spillover effects in terms of the\nimpact of link addition between a pair of agents on others' utility, (3) to\nstudy the role of agents' closeness in determining what type of spillover\neffects these agents experience in the network, and (4) to model the choices of\nagents that suggest with whom they want to add links in the social cloud. The\nfindings include the following. Firstly, agents' decision of link addition\n(deletion) increases (decreases) their local resource availability. However,\nthese observations do not hold in the case of global resource availability.\nSecondly, in a connected network, agents experience either positive or negative\nspillover effect and there is no case with no spillover effects. Agents observe\nno spillover effects if and only if the network is disconnected and consists of\nmore than two components (sub-networks). Furthermore, if there is no change in\nthe closeness of an agent (not involved in link addition) due to a newly added\nlink, then the agent experiences negative spillover effect. Although an\nincrease in the closeness of agents is necessary in order to experience\npositive spillover effects, the condition is not sufficient. By focusing on\nparameters such as closeness and shortest distances, we provide conditions\nunder which agents choose to add links so as to maximise their resource\navailability.\n"
    },
    {
        "paper_id": 2102.01269,
        "authors": "Rabab Haider, David D'Achiardi, Venkatesh Venkataramanan, Anurag\n  Srivastava, Anjan Bose, and Anuradha M. Annaswamy",
        "title": "Reinventing the Utility for DERs: A Proposal for a DSO-Centric Retail\n  Electricity Market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.adapen.2021.100026",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing penetration of intermittent renewables, storage devices, and\nflexible loads is introducing operational challenges in distribution grids. The\nproper coordination and scheduling of these resources using a distributed\napproach is warranted, and can only be achieved through local retail markets\nemploying transactive energy schemes. To this end, we propose a\ndistribution-level retail market operated by a Distribution System Operator\n(DSO), which schedules DERs and determines the real-time distribution-level\nLocational Marginal Price (d-LPM). The retail market is built using a\ndistributed Proximal Atomic Coordination (PAC) algorithm, which solves the\noptimal power flow model while accounting for network physics, rendering\nlocationally and temporally varying d-LMPs. A numerical study of the market\nstructure is carried out via simulations of the IEEE-123 node network using\ndata from ISO-NE and Eversource in Massachusetts, US. The market performance is\ncompared to existing retail practices, including demand response (DR) with\nno-export rules and net metering. The DSO-centric market increases DER\nutilization, permits continual market participation for DR, lowers electricity\nrates for customers, and eliminates the subsidies inherent to net metering\nprograms. The resulting lower revenue stream for the DSO highlights the\nevolving business model of the modern utility, moving from commoditized markets\ntowards performance-based ratemaking.\n"
    },
    {
        "paper_id": 2102.0129,
        "authors": "Pratyush Muthukumar, Jie Zhong",
        "title": "A Stochastic Time Series Model for Predicting Financial Trends using NLP",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock price forecasting is a highly complex and vitally important field of\nresearch. Recent advancements in deep neural network technology allow\nresearchers to develop highly accurate models to predict financial trends. We\npropose a novel deep learning model called ST-GAN, or Stochastic Time-series\nGenerative Adversarial Network, that analyzes both financial news texts and\nfinancial numerical data to predict stock trends. We utilize cutting-edge\ntechnology like the Generative Adversarial Network (GAN) to learn the\ncorrelations among textual and numerical data over time. We develop a new\nmethod of training a time-series GAN directly using the learned representations\nof Naive Bayes' sentiment analysis on financial text data alongside technical\nindicators from numerical data. Our experimental results show significant\nimprovement over various existing models and prior research on deep neural\nnetworks for stock price forecasting.\n"
    },
    {
        "paper_id": 2102.01499,
        "authors": "Ling Qi, Matloob Khushi and Josiah Poon",
        "title": "Event-Driven LSTM For Forex Price Prediction",
        "comments": null,
        "journal-ref": "2020 IEEE Asia-Pacific Conference on Computer Science and Data\n  Engineering (CSDE), Gold Coast, QLD, Australia, 16-18 December 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The majority of studies in the field of AI guided financial trading focus on\npurely applying machine learning algorithms to continuous historical price and\ntechnical analysis data. However, due to non-stationary and high volatile\nnature of Forex market most algorithms fail when put into real practice. We\ndeveloped novel event-driven features which indicate a change of trend in\ndirection. We then build long deep learning models to predict a retracement\npoint providing a perfect entry point to gain maximum profit. We use a simple\nrecurrent neural network (RNN) as our baseline model and compared with\nshort-term memory (LSTM), bidirectional long short-term memory (BiLSTM) and\ngated recurrent unit (GRU). Our experiment results show that the proposed\nevent-driven feature selection together with the proposed models can form a\nrobust prediction system which supports accurate trading strategies with\nminimal risk. Our best model on 15-minutes interval data for the EUR/GBP\ncurrency achieved RME 0.006x10^(-3) , RMSE 2.407x10^(-3), MAE 1.708x10^(-3),\nMAPE 0.194% outperforming previous studies.\n"
    },
    {
        "paper_id": 2102.01527,
        "authors": "Asim Ghosh, Bikas K Chakrabarti",
        "title": "Limiting Value of the Kolkata Index for Social Inequality and a Possible\n  Social Constant",
        "comments": "Accepted for publication in Physica A",
        "journal-ref": "Physica A 573 (2021) 125944",
        "doi": "10.1016/j.physa.2021.125944",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Based on some analytic structural properties of the Gini and Kolkata indices\nfor social inequality, as obtained from a generic form of the Lorenz function,\nwe make a conjecture that the limiting (effective saturation) value of the\nabove-mentioned indices is about 0.865. This, together with some more new\nobservations on the citation statistics of individual authors (including Nobel\nlaureates), suggests that about $14\\%$ of people or papers or social conflicts\ntend to earn or attract or cause about $86\\%$ of wealth or citations or deaths\nrespectively in very competitive situations in markets, universities or wars.\nThis is a modified form of the (more than a) century old $80-20$ law of Pareto\nin economy (not visible today because of various welfare and other strategies)\nand gives an universal value ($0.86$) of social (inequality) constant or\nnumber.\n"
    },
    {
        "paper_id": 2102.01533,
        "authors": "Denis Belomestny and John Schoenmakers",
        "title": "From optimal martingales to randomized dual optimal stopping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we study and classify optimal martingales in the dual\nformulation of optimal stopping problems. In this respect we distinguish\nbetween weakly optimal and surely optimal martingales. It is shown that the\nfamily of weakly optimal and surely optimal martingales may be quite large. On\nthe other hand it is shown that the Doob-martingale, that is, the martingale\npart of the Snell envelope, is in a certain sense the most robust surely\noptimal martingale under random perturbations. This new insight leads to a\nnovel randomized dual martingale minimization algorithm that doesn't require\nnested simulation. As a main feature, in a possibly large family of optimal\nmartingales the algorithm efficiently selects a martingale that is as close as\npossible to the Doob martingale. As a result, one obtains the dual upper bound\nfor the optimal stopping problem with low variance.\n"
    },
    {
        "paper_id": 2102.01609,
        "authors": "Ateeb Akhter Shah Syed and Kaneez Fatima and Riffat Arshad",
        "title": "The Macroeconomic Impacts of Entitlements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The worries expressed by Alan Greenspan that the long run economic growth of\nthe United States will fade away due to increasing burden of entitlements\nmotivated us to empirically investigate the impact of entitlements of key\nmacroeconomic variables. To examine this contemporary issue, we estimate a\nvector error-correction model is used to analyze the impact of entitlements on\nthe price level, real output, and the long-term interest rate. The results show\nthat a shock to entitlements leads to decrease in output and lends support to\nthe assertion made by Alan Greenspan. Several robustness checks are conducted\nand the results of the model qualitatively remains unchanged.\n"
    },
    {
        "paper_id": 2102.01716,
        "authors": "Alfred Galichon",
        "title": "A survey of some recent applications of optimal transport methods to\n  econometrics",
        "comments": null,
        "journal-ref": "The Econometrics Journal 20-2 (2017) C1-C11",
        "doi": "10.1111/ectj.12083",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper surveys recent applications of methods from the theory of optimal\ntransport to econometric problems.\n"
    },
    {
        "paper_id": 2102.018,
        "authors": "Ariah Klages-Mundt, Andreea Minca",
        "title": "Optimal Intervention in Economic Networks using Influence Maximization\n  Methods",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2021.10.042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider optimal intervention in the Elliott-Golub-Jackson network model\n\\cite{jackson14} and we show that it can be transformed into an influence\nmaximization-like form, interpreted as the reverse of a default cascade. Our\nanalysis of the optimal intervention problem extends well-established targeting\nresults to the economic network setting, which requires additional theoretical\nsteps. We prove several results about optimal intervention: it is NP-hard and\ncannot be approximated to a constant factor in polynomial time. In turn, we\nshow that randomizing failure thresholds leads to a version of the problem\nwhich is monotone submodular, for which existing powerful approximations in\npolynomial time can be applied. In addition to optimal intervention, we also\nshow practical consequences of our analysis to other economic network problems:\n(1) it is computationally hard to calculate expected values in the economic\nnetwork, and (2) influence maximization algorithms can enable efficient\nimportance sampling and stress testing of large failure scenarios. We\nillustrate our results on a network of firms connected through input-output\nlinkages inferred from the World Input Output Database.\n"
    },
    {
        "paper_id": 2102.01962,
        "authors": "Blanka Horvath, Josef Teichmann, Zan Zuric",
        "title": "Deep Hedging under Rough Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the performance of the Deep Hedging framework under training\npaths beyond the (finite dimensional) Markovian setup. In particular we analyse\nthe hedging performance of the original architecture under rough volatility\nmodels with view to existing theoretical results for those. Furthermore, we\nsuggest parsimonious but suitable network architectures capable of capturing\nthe non-Markoviantity of time-series. Secondly, we analyse the hedging\nbehaviour in these models in terms of P\\&L distributions and draw comparisons\nto jump diffusion models if the the rebalancing frequency is realistically\nsmall.\n"
    },
    {
        "paper_id": 2102.0198,
        "authors": "Nicolas Curin, Michael Kettler, Xi Kleisinger-Yu, Vlatka Komaric,\n  Thomas Krabichler, Josef Teichmann, Hanna Wutte",
        "title": "A deep learning model for gas storage optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To the best of our knowledge, the application of deep learning in the field\nof quantitative risk management is still a relatively recent phenomenon. In\nthis article, we utilize techniques inspired by reinforcement learning in order\nto optimize the operation plans of underground natural gas storage facilities.\nWe provide a theoretical framework and assess the performance of the proposed\nmethod numerically in comparison to a state-of-the-art least-squares\nMonte-Carlo approach. Due to the inherent intricacy originating from the\nhigh-dimensional forward market as well as the numerous constraints and\nfrictions, the optimization exercise can hardly be tackled by means of\ntraditional techniques.\n"
    },
    {
        "paper_id": 2102.02071,
        "authors": "Liang Chen, Eugene Choo, Alfred Galichon, Simon Weber",
        "title": "Matching Function Equilibria with Partial Assignment: Existence,\n  Uniqueness and Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We argue that models coming from a variety of fields, such as matching models\nand discrete choice models among others, share a common structure that we call\nmatching function equilibria with partial assignment. This structure includes\nan aggregate matching function and a system of nonlinear equations. We provide\na proof of existence and uniqueness of an equilibrium and propose an efficient\nalgorithm to compute it. For a subclass of matching models, we also develop a\nnew parameter-free approach for constructing the counterfactual matching\nequilibrium. It has the advantage of not requiring parametric estimation when\ncomputing counterfactuals. We use our procedure to analyze the impact of the\nelimination of the Social Security Student Benefit Program in 1982 on the\nmarriage market in the United States. We estimate several candidate models from\nour general class of matching functions and select the best fitting model using\ninformation based criterion.\n"
    },
    {
        "paper_id": 2102.02119,
        "authors": "Ali Hirsa, Joerg Osterrieder, Branka Hadji Misheva, Wenxin Cao, Yiwen\n  Fu, Hanze Sun, Kin Wai Wong",
        "title": "The VIX index under scrutiny of machine learning techniques and neural\n  networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The CBOE Volatility Index, known by its ticker symbol VIX, is a popular\nmeasure of the market's expected volatility on the SP 500 Index, calculated and\npublished by the Chicago Board Options Exchange (CBOE). It is also often\nreferred to as the fear index or the fear gauge. The current VIX index value\nquotes the expected annualized change in the SP 500 index over the following 30\ndays, based on options-based theory and current options-market data. Despite\nits theoretical foundation in option price theory, CBOE's Volatility Index is\nprone to inadvertent and deliberate errors because it is weighted average of\nout-of-the-money calls and puts which could be illiquid. Many claims of market\nmanipulation have been brought up against VIX in recent years.\n  This paper discusses several approaches to replicate the VIX index as well as\nVIX futures by using a subset of relevant options as well as neural networks\nthat are trained to automatically learn the underlying formula. Using subset\nselection approaches on top of the original CBOE methodology, as well as\nbuilding machine learning and neural network models including Random Forests,\nSupport Vector Machines, feed-forward neural networks, and long short-term\nmemory (LSTM) models, we will show that a small number of options is sufficient\nto replicate the VIX index. Once we are able to actually replicate the VIX\nusing a small number of SP options we will be able to exploit potential\narbitrage opportunities between the VIX index and its underlying derivatives.\nThe results are supposed to help investors to better understand the options\nmarket, and more importantly, to give guidance to the US regulators and CBOE\nthat have been investigating those manipulation claims for several years.\n"
    },
    {
        "paper_id": 2102.02121,
        "authors": "Daniele Petrone, Neofytos Rodosthenous, Vito Latora",
        "title": "Artificial intelligence applied to bailout decisions in financial\n  systemic risk management",
        "comments": "12 pages, 6 figures",
        "journal-ref": "Nature Communications 13, 6815 (2022)",
        "doi": "10.1038/s41467-022-34102-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe the bailout of banks by governments as a Markov Decision Process\n(MDP) where the actions are equity investments. The underlying dynamics is\nderived from the network of financial institutions linked by mutual exposures,\nand the negative rewards are associated to the banks' default. Each node\nrepresents a bank and is associated to a probability of default per unit time\n(PD) that depends on its capital and is increased by the default of\nneighbouring nodes. Governments can control the systemic risk of the network by\nproviding additional capital to the banks, lowering their PD at the expense of\nan increased exposure in case of their failure. Considering the network of\nEuropean global systemically important institutions, we find the optimal\ninvestment policy that solves the MDP, providing direct indications to\ngovernments and regulators on the best way of action to limit the effects of\nfinancial crises.\n"
    },
    {
        "paper_id": 2102.02142,
        "authors": "Paul Goldsmith-Pinkham, Maxim Pinkovskiy, and Jacob Wallace",
        "title": "The Great Equalizer: Medicare and the Geography of Consumer Financial\n  Strain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a five percent sample of Americans' credit bureau data, combined with\na regression discontinuity approach, to estimate the effect of universal health\ninsurance at age 65-when most Americans become eligible for Medicare-at the\nnational, state, and local level. We find a 30 percent reduction in debt\ncollections-and a two-thirds reduction in the geographic variation in\ncollections-with limited effects on other financial outcomes. The areas that\nexperienced larger reductions in collections debt at age 65 were concentrated\nin the Southern United States, and had higher shares of black residents, people\nwith disabilities, and for-profit hospitals.\n"
    },
    {
        "paper_id": 2102.02176,
        "authors": "Zachary Feinstein",
        "title": "Clearing prices under margin calls and the short squeeze",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a clearing model for prices in a financial markets\ndue to margin calls on short sold assets. In doing so, we construct an explicit\nformulation for the prices that would result immediately following asset\npurchases and a margin call. The key result of this work is the determination\nof a threshold short interest ratio which, if exceeded, results in the\ndiscontinuity of the clearing prices due to a feedback loop.\n"
    },
    {
        "paper_id": 2102.02179,
        "authors": "Yong Shi, Bo Li and Guangle Du",
        "title": "Pyramid scheme in stock market: a kind of financial market simulation",
        "comments": "23 pages, 11 figures, in this version we have corrected some\n  expression errors in the previous version",
        "journal-ref": null,
        "doi": "10.1088/1674-1056/abeef3",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Artificial stock market simulation based on agent is an important means to\nstudy financial market. Based on the assumption that the investors are composed\nof a main fund, small trend and contrarian investors characterized by four\nparameters, we simulate and research a kind of financial phenomenon with the\ncharacteristics of pyramid schemes. Our simulation results and theoretical\nanalysis reveal the relationships between the rate of return of the main fund\nand the proportion of the trend investors in all small investors, the small\ninvestors' parameters of taking profit and stopping loss, the order size of the\nmain fund and the strategies adopted by the main fund. Our work are helpful to\nexplain the financial phenomenon with the characteristics of pyramid schemes in\nfinancial markets, design trading rules for regulators and develop trading\nstrategies for investors.\n"
    },
    {
        "paper_id": 2102.02298,
        "authors": "Huy N. Chau, Masaaki Fukasawa, Miklos Rasonyi",
        "title": "Super-replication with transaction costs under model uncertainty for\n  continuous processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate a superhedging theorem in the presence of transaction costs and\nmodel uncertainty. Asset prices are assumed continuous and uncertainty is\nmodelled in a parametric setting. Our proof relies on a new topological\nframework in which no Krein-Smulian type theorem is available.\n"
    },
    {
        "paper_id": 2102.02379,
        "authors": "Branko Bubalo",
        "title": "Airport Capacity and Performance in Europe -- A study of transport\n  economics, service quality and sustainability",
        "comments": "358 pages, 99 figures and 63 tables",
        "journal-ref": null,
        "doi": "10.25592/uhhfdm.8631",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this dissertation is to present an overview of the operational\nand financial performance of airports in Europe. In benchmarking studies,\nairports are assessed and compared with other airports based on key indicators\nfrom a technical and an economic point of view. The interest lies primarily in\nthe question, which key figures can best measure the perception of quality of\nservice from the point of view of the passenger for the services at an airport.\n"
    },
    {
        "paper_id": 2102.02577,
        "authors": "Alfred Galichon",
        "title": "The VAR at Risk",
        "comments": "4 Pages",
        "journal-ref": "International Journal of Theoretical and Applied Finance 13-4\n  (2010) pp. 503-506",
        "doi": "10.1142/S0219024910005875",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  I show that the structure of the firm is not neutral in respect to regulatory\ncapital budgeted under rules which are based on the Value-at-Risk.\n"
    },
    {
        "paper_id": 2102.02593,
        "authors": "Ivar Ekeland and Alfred Galichon",
        "title": "The housing problem and revealed preference theory: duality and an\n  application",
        "comments": "19 pages",
        "journal-ref": "Economic Theory 54 (2013) pp. 425-441",
        "doi": "10.1007/s00199-012-0719-x",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper exhibits a duality between the theory of Revealed Preference of\nAfriat and the housing allocation problem of Shapley and Scarf. In particular,\nit is shown that Afriat's theorem can be interpreted as a second welfare\ntheorem in the housing problem. Using this duality, the revealed preference\nproblem is connected to an optimal assignment problem, and a geometrical\ncharacterization of the rationalizability of experiment data is given. This\nallows in turn to give new indices of rationalizability of the data, and to\ndefine weaker notions of rationalizability, in the spirit of Afriat's\nefficiency index.\n"
    },
    {
        "paper_id": 2102.02612,
        "authors": "Dietmar Pfeifer, Vivien Langen",
        "title": "Insurance Business and Sustainable Development",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we will discuss recent developments in risk management of the\nglobal financial and insurance business with respect to sustainable\ndevelopment. So far climate change aspects have been the dominant aspect in\nmanaging sustainability risks and opportunities, accompanied by the development\nof several legislative initiatives triggered by supervisory authorities.\nHowever, a sole concentration on these aspects misses out other important\neconomic and social facets of sustainable development goals formulated by the\nUN. Such aspects have very recently come into the focus of the European\nCommittee concerning the Solvency II project for the European insurance\nindustry. Clearly the new legislative expectations can be better handled by\nlarger insurance companies and holdings than by small- and medium-sized mutual\ninsurance companies which are numerous in central Europe, due to their historic\ndevelopment starting in the late medieval ages and early modern times. We\ntherefore also concentrate on strategies within the risk management of such\nsmall- and medium-sized enterprises that can be achieved without much effort,\nin particular those that are not directly related to climate change.\n"
    },
    {
        "paper_id": 2102.02718,
        "authors": "Ariel Neufeld, Julian Sester",
        "title": "On the stability of the martingale optimal transport problem: A\n  set-valued map approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Continuity of the value of the martingale optimal transport problem on the\nreal line w.r.t. its marginals was recently established in Backhoff-Veraguas\nand Pammer [2] and Wiesel [21]. We present a new perspective of this result\nusing the theory of set-valued maps. In particular, using results from\nBeiglb\\\"ock, Jourdain, Margheriti, and Pammer [5], we show that the set of\nmartingale measures with fixed marginals is continuous, i.e., lower- and upper\nhemicontinuous, w.r.t. its marginals. Moreover, we establish compactness of the\nset of optimizers as well as upper hemicontinuity of the optimizers w.r.t. the\nmarginals.\n"
    },
    {
        "paper_id": 2102.02834,
        "authors": "Mehdi Tomas, Iacopo Mastromatteo, Michael Benzaquen",
        "title": "Cross impact in derivative markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Trading a financial asset pushes its price as well as the prices of other\nassets, a phenomenon known as cross-impact. The empirical estimation of this\neffect on complex financial instruments, such as derivatives, is an open\nproblem. To address this, we consider a setting in which the prices of\nderivatives is a deterministic function of stochastic factors where trades on\nboth factors and derivatives induce price impact. We show that a specific\ncross-impact model satisfies key properties which make its estimation tractable\nin applications. Using E-Mini futures, European call and put options and VIX\nfutures, we estimate cross-impact and show our simple framework successfully\ncaptures some of the empirical phenomenology. Our framework for estimating\ncross-impact on derivatives may be used in practice for estimating hedging\ncosts or building liquidity metrics on derivative markets.\n"
    },
    {
        "paper_id": 2102.02865,
        "authors": "Shinji Kakinaka and Ken Umeno",
        "title": "Exploring asymmetric multifractal cross-correlations of price-volatility\n  and asymmetric volatility dynamics in cryptocurrency markets",
        "comments": "15 pages, 6 figures",
        "journal-ref": "Physica A 581 (2021) 126237",
        "doi": "10.1016/j.physa.2021.126237",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Asymmetric relationship between price and volatility is a prominent feature\nof the financial market time series. This paper explores the price-volatility\nnexus in cryptocurrency markets and investigates the presence of asymmetric\nvolatility effect between uptrend (bull) and downtrend (bear) regimes. The\nconventional GARCH-class models have shown that in cryptocurrency markets,\nasymmetric reactions of volatility to returns differ from those of other\ntraditional financial assets. We address this issue from a viewpoint of fractal\nanalysis, which can cover the nonlinear interactions and the self-similarity\nproperties widely acknowledged in the field of econophysics. The asymmetric\ncross-correlations between price and volatility for Bitcoin (BTC), Ethereum\n(ETH), Ripple (XRP), and Litecoin (LTC) during the period from June 1, 2016 to\nDecember 28, 2020 are investigated using the MF-ADCCA method and quantified via\nthe asymmetric DCCA coefficient. The approaches take into account the\nnonlinearity and asymmetric multifractal scaling properties, providing new\ninsights in investigating the relationships in a dynamical way. We find that\ncross-correlations are stronger in downtrend markets than in uptrend markets\nfor maturing BTC and ETH. In contrast, for XRP and LTC, inverted reactions are\npresent where cross-correlations are stronger in uptrend markets.\n"
    },
    {
        "paper_id": 2102.02877,
        "authors": "Mike Weber, Iuliia Manziuk, Bastien Baldacci",
        "title": "Liquidity Stress Testing using Optimal Portfolio Liquidation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We build an optimal portfolio liquidation model for OTC markets, aiming at\nminimizing the trading costs via the choice of the liquidation time. We work in\nthe Locally Linear Order Book framework of \\cite{toth2011anomalous} to obtain\nthe market impact as a function of the traded volume. We find that the optimal\nterminal time for a linear execution of a small order is proportional to the\nsquare root of the ratio between the amount being bought or sold and the\naverage daily volume. Numerical experiments on real market data illustrate the\nmethod on a portfolio of corporate bonds.\n"
    },
    {
        "paper_id": 2102.02884,
        "authors": "Meenakshi Balakrishna and Kenneth C. Wilbur",
        "title": "How the Massachusetts Assault Weapons Ban Enforcement Notice Changed\n  Firearm Sales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Massachusetts Attorney General issued an Enforcement Notice in 2016 to\nannounce a new interpretation of a key phrase in the state's assault weapons\nban. The Enforcement Notice increased sales of tagged assault rifles by 616% in\nthe first 5 days, followed by a 9% decrease over the next three weeks. Sales of\nHandguns and Shotguns did not change significantly. Tagged assault rifle sales\nfell 28-30% in 2017 compared to previous years, suggesting that the Enforcement\nNotice reduced assault weapon sales but also that many banned weapons continued\nto be sold. Tagged assault rifles sold most in 2017 in zip codes with higher\nhousehold incomes and proportions of white males. Overall, the results suggest\nthat the firearm market reacts rapidly to policy changes and partially complies\nwith firearm restrictions.\n"
    },
    {
        "paper_id": 2102.02935,
        "authors": "Albert Alex Zevelev",
        "title": "Does Collateral Value Affect Asset Prices? Evidence from a Natural\n  Experiment in Texas",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/rfs/hhaa117",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Does the ability to pledge an asset as collateral, after purchase, affect its\nprice? This paper identifies the impact of collateral service flows on house\nprices, exploiting a plausibly exogenous constitutional amendment in Texas\nwhich legalized home equity loans in 1998. The law change increased Texas house\nprices 4%; this is price-based evidence that households are credit-constrained\nand value home equity loans to facilitate consumption smoothing. Prices rose\nmore in locations with inelastic supply, higher pre-law house prices, higher\nincome, and lower unemployment. These estimates reveal that richer households\nvalue the option to pledge their home as collateral more strongly.\n"
    },
    {
        "paper_id": 2102.03157,
        "authors": "Sang Hu, Jan Obloj, Xun Yu Zhou",
        "title": "When to Quit Gambling, if You Must!",
        "comments": "50 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an approach to solve Barberis (2012)'s casino gambling model in\nwhich a gambler whose preferences are specified by the cumulative prospect\ntheory (CPT) must decide when to stop gambling by a prescribed deadline. We\nassume that the gambler can assist their decision using an independent\nrandomization, and explain why it is a reasonable assumption. The problem is\ninherently time-inconsistent due to the probability weighting in CPT, and we\nstudy both precommitted and naive stopping strategies. We turn the original\nproblem into a computationally tractable mathematical program, based on which\nwe derive an optimal precommitted rule which is randomized and Markovian. The\nanalytical treatment enables us to make several predictions regarding a\ngambler's behavior, including that with randomization they may enter the casino\neven when allowed to play only once, that whether they will play longer once\nthey are granted more bets depends on whether they are in a gain or at a loss,\nand that it is prevalent that a naivite never stops loss.\n"
    },
    {
        "paper_id": 2102.03172,
        "authors": "Francesco C. De Vecchi, Elisa Mastrogiacomo, Mattia Turra and Stefania\n  Ugolini",
        "title": "Noether theorem in stochastic optimal control problems via contact\n  symmetries",
        "comments": null,
        "journal-ref": "Mathematics (MDPI) 9, no. 9 (2021)",
        "doi": "10.3390/math9090953",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a generalization of Noether theorem for stochastic optimal\ncontrol problems. Exploiting the tools of jet bundles and contact geometry, we\nprove that from any (contact) symmetry of the Hamilton-Jacobi-Bellman equation\nassociated to an optimal control problem it is possible to build a related\nlocal martingale. Moreover, we provide an application of the theoretical\nresults to Merton's optimal portfolio problem, showing that this model admits\ninfinitely many conserved quantities in the form of local martingales.\n"
    },
    {
        "paper_id": 2102.03187,
        "authors": "Achmad Firman and Ratna Ayu Saptati",
        "title": "Econometric model of children participation in family dairy farming in\n  the center of dairy farming, West Java Province, Indonesia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The involvement of children in the family dairy farming is pivotal point to\nreduce the cost of production input, especially in smallholder dairy farming.\nThe purposes of the study are to analysis the factors that influence children's\nparticipation in working in the family dairy farm. The study was held December\n2020 in the development center of dairy farming in Pangalengan subdistrict,\nWest Java Province, Indonesia. The econometric method used in the study was the\nlogit regression model. The results of the study determine that the there were\nnumber of respondents who participates in family farms was 52.59% of total\nrespondents, and the rest was no participation in the family farms. There are 3\nvariables in the model that are very influential on children's participation in\nthe family dairy farming, such as X1 (number of dairy farm land ownership), X2\n(number of family members), and X6 (the amount of work spent on the family's\ndairy farm). Key words: Participation, children, family, dairy farming, logit\nmodel\n"
    },
    {
        "paper_id": 2102.0324,
        "authors": "Zhu Liu, Biqing Zhu, Philippe Ciais, Steven J. Davis, Chenxi Lu,\n  Haiwang Zhong, Piyu Ke, Yanan Cui, Zhu Deng, Duo Cui, Taochun Sun, Xinyu Dou,\n  Jianguang Tan, Rui Guo, Bo Zheng, Katsumasa Tanaka, Wenli Zhao, Pierre\n  Gentine",
        "title": "De-carbonization of global energy use during the COVID-19 pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The COVID-19 pandemic has disrupted human activities, leading to\nunprecedented decreases in both global energy demand and GHG emissions. Yet a\nlittle known that there is also a low carbon shift of the global energy system\nin 2020. Here, using the near-real-time data on energy-related GHG emissions\nfrom 30 countries (about 70% of global power generation), we show that the\npandemic caused an unprecedented de-carbonization of global power system,\nrepresenting by a dramatic decrease in the carbon intensity of power sector\nthat reached a historical low of 414.9 tCO2eq/GWh in 2020. Moreover, the share\nof energy derived from renewable and low-carbon sources (nuclear, hydro-energy,\nwind, solar, geothermal, and biomass) exceeded that from coal and oil for the\nfirst time in history in May of 2020. The decrease in global net energy demand\n(-1.3% in the first half of 2020 relative to the average of the period in\n2016-2019) masks a large down-regulation of fossil-fuel-burning power plants\nsupply (-6.1%) coincident with a surge of low-carbon sources (+6.2%).\nConcomitant changes in the diurnal cycle of electricity demand also favored\nlow-carbon generators, including a flattening of the morning ramp, a lower\nmidday peak, and delays in both the morning and midday load peaks in most\ncountries. However, emission intensities in the power sector have since\nrebounded in many countries, and a key question for climate mitigation is thus\nto what extent countries can achieve and maintain lower, pandemic-level carbon\nintensities of electricity as part of a green recovery.\n"
    },
    {
        "paper_id": 2102.03414,
        "authors": "Bahman Angoshtari, Erhan Bayraktar, Virginia R. Young",
        "title": "Optimal Investment and Consumption under a Habit-Formation Constraint",
        "comments": "31 pages, 7 figures",
        "journal-ref": "SIAM J. Financial Math., 13(1), pp. 321-352, 2022",
        "doi": "10.1137/21M1397891",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate an infinite-horizon optimal investment and consumption problem,\nin which an individual forms a habit based on the exponentially weighted\naverage of her past consumption rate, and in which she invests in a\nBlack-Scholes market. The individual is constrained to consume at a rate higher\nthan a certain proportion $\\alpha$ of her consumption habit. Our\nhabit-formation model allows for both addictive ($\\alpha=1$) and nonaddictive\n($0<\\alpha<1$) habits. The optimal investment and consumption policies are\nderived explicitly in terms of the solution of a system of differential\nequations with free boundaries, which is analyzed in detail. If the\nwealth-to-habit ratio is below (resp. above) a critical level $x^*$, the\nindividual consumes at (resp. above) the minimum rate and invests more (resp.\nless) aggressively in the risky asset. Numerical results show that the\naddictive habit formation requires significantly more wealth to support the\nsame consumption rate compared to a moderately nonaddictive habit. Furthermore,\nan individual with a more addictive habit invests less in the risky asset\ncompared to an individual with a less addictive habit but with the same\nwealth-to-habit ratio and risk aversion, which provides an explanation for the\nequity-premium puzzle.\n"
    },
    {
        "paper_id": 2102.03417,
        "authors": "Marcel Nutz, Yuchong Zhang",
        "title": "Reward Design in Risk-Taking Contests",
        "comments": "To appear in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the risk-taking model of Seel and Strack, $n$ players decide when\nto stop privately observed Brownian motions with drift and absorption at zero.\nThey are then ranked according to their level of stopping and paid a\nrank-dependent reward. We study the problem of a principal who aims to induce a\ndesirable equilibrium performance of the players by choosing how much reward is\nattributed to each rank. Specifically, we determine optimal reward schemes for\nprincipals interested in the average performance and the performance at a given\nrank. While the former can be related to reward inequality in the Lorenz sense,\nthe latter can have a surprising shape.\n"
    },
    {
        "paper_id": 2102.03502,
        "authors": "Zhenhan Huang, Fumihide Tanaka",
        "title": "MSPM: A Modularized and Scalable Multi-Agent Reinforcement\n  Learning-based System for Financial Portfolio Management",
        "comments": null,
        "journal-ref": "PLoS ONE 17(2): e0263689 (2022)",
        "doi": "10.1371/journal.pone.0263689",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial portfolio management (PM) is one of the most applicable problems in\nreinforcement learning (RL) owing to its sequential decision-making nature.\nHowever, existing RL-based approaches rarely focus on scalability or\nreusability to adapt to the ever-changing markets. These approaches are rigid\nand unscalable to accommodate the varying number of assets of portfolios and\nincreasing need for heterogeneous data. Also, RL agents in the existing systems\nare ad-hoc trained and hardly reusable for different portfolios. To confront\nthe above problems, a modular design is desired for the systems to be\ncompatible with reusable asset-dedicated agents. In this paper, we propose a\nmulti-agent RL-based system for PM (MSPM). MSPM involves two types of\nasynchronously-updated modules: Evolving Agent Module (EAM) and Strategic Agent\nModule (SAM). An EAM is an information-generating module with a DQN agent, and\nit receives heterogeneous data and generates signal-comprised information for a\nparticular asset. An SAM is a decision-making module with a PPO agent for\nportfolio optimization, and it connects to EAMs to reallocate the assets in a\nportfolio. Trained EAMs can be connected to any SAM at will. With its\nmodularized architecture, the multi-step condensation of volatile market\ninformation, and the reusable design of EAM, MSPM simultaneously addresses the\ntwo challenges in RL-based PM: scalability and reusability. Experiments on\n8-year U.S. stock market data prove the effectiveness of MSPM in profit\naccumulation by its outperformance over five baselines in terms of accumulated\nrate of return (ARR), daily rate of return, and Sortino ratio. MSPM improves\nARR by at least 186.5% compared to CRP, a widely-used PM strategy. To validate\nthe indispensability of EAM, we back-test and compare MSPMs on four portfolios.\nEAM-enabled MSPMs improve ARR by at least 1341.8% compared to EAM-disabled\nMSPMs.\n"
    },
    {
        "paper_id": 2102.03561,
        "authors": "Edward Oughton",
        "title": "Policy options for digital infrastructure strategies: A simulation model\n  for broadband universal service in Africa",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Internet access is essential for economic development and helping to deliver\nthe Sustainable Development Goals, especially as even basic broadband can\nrevolutionize available economic opportunities. Yet, more than one billion\npeople still live without internet access. Governments must make strategic\nchoices to connect these citizens, but currently have few independent,\ntransparent and scientifically reproducible assessments to rely on. This paper\ndevelops open-source software to test broadband universal service strategies\nwhich meet the 10 Mbps target being considered by the UN Broadband Commission.\nThe private and government costs of different infrastructure decisions are\nquantified in six East and West African countries (C\\^ote D`Ivoire, Mali,\nSenegal, Kenya, Tanzania and Uganda). The results provide strong evidence that\n`leapfrogging` straight to 4G in unconnected areas is the least-cost option for\nproviding broadband universal service, with savings between 13-51% over 3G. The\nresults also demonstrate how the extraction of spectrum and tax revenues in\nunviable markets provide no net benefit, as for every $1 taken in revenue, a $1\ninfrastructure subsidy is required from government to achieve broadband\nuniversal service. Importantly, the use of a Shared Rural Network in unviable\nlocations provides impressive cost savings (up to 78%), while retaining the\nbenefits of dynamic infrastructure competition in viable urban and suburban\nareas. This paper provides evidence to design national and international\npolicies aimed at broadband universal service.\n"
    },
    {
        "paper_id": 2102.03644,
        "authors": "Shahin Esmaeili",
        "title": "Prisoner Dilemma in maximization constrained: the rationality of\n  cooperation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  David Gauthier in his article, Maximization constrained: the rationality of\ncooperation, tries to defend the joint strategy in situations in which no\noutcome is both equilibrium and optimal. Prisoner Dilemma is the most familiar\nexample of these situations. He first starts with some quotes by Hobbes in\nLeviathan; Hobbes, in chapter 15 discusses an objection by someone is called\nFoole, and then will reject his view. In response to Foole, Hobbes presents two\nstrategies (i.e. joint and individual) and two kinds of agents in such problems\nincluding Prisoner Dilemma, i.e. straightforward maximizer (SM) and constrained\nmaximizer(CM). Then he considers two arguments respectively for SM and CM, and\nhe will show that why in an ideal and transparent situation, the first argument\nfails and the second one would be the only valid argument. Likewise, in the\nfollowing part of his article, he considers more realistic situations with\ntranslucency and he concludes that under some conditions, the joint strategy\nwould be still the rational decision.\n"
    },
    {
        "paper_id": 2102.03698,
        "authors": "Rajat Verma, Takahiro Yabe, Satish V. Ukkusuri",
        "title": "Mobility-based contact exposure explains the disparity of spread of\n  COVID-19 in urban neighborhoods",
        "comments": "13 pages (overall) with 7 figures and 1 table; currently under review\n  in another journal - Scientific Reports",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid early spread of COVID-19 in the U.S. was experienced very\ndifferently by different socioeconomic groups and business industries. In this\nstudy, we study aggregate mobility patterns of New York City and Chicago to\nidentify the relationship between the amount of interpersonal contact between\npeople in urban neighborhoods and the disparity in the growth of positive cases\namong these groups. We introduce an aggregate Contact Exposure Index (CEI) to\nmeasure exposure due to this interpersonal contact and combine it with social\ndistancing metrics to show its effect on positive case growth. With the help of\nstructural equations modeling, we find that the effect of exposure on case\ngrowth was consistently positive and that it remained consistently higher in\nlower-income neighborhoods, suggesting a causal path of income on case growth\nvia contact exposure. Using the CEI, schools and restaurants are identified as\nhigh-exposure industries, and the estimation suggests that implementing\nspecific mobility restrictions on these point-of-interest categories are most\neffective. This analysis can be useful in providing insights for government\nofficials targeting specific population groups and businesses to reduce\ninfection spread as reopening efforts continue to expand across the nation.\n"
    },
    {
        "paper_id": 2102.03945,
        "authors": "Maxime Bergeron, Nicholas Fung, John Hull and Zissis Poulos",
        "title": "Variational Autoencoders: A Hands-Off Approach to Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A volatility surface is an important tool for pricing and hedging\nderivatives. The surface shows the volatility that is implied by the market\nprice of an option on an asset as a function of the option's strike price and\nmaturity. Often, market data is incomplete and it is necessary to estimate\nmissing points on partially observed surfaces. In this paper, we show how\nvariational autoencoders can be used for this task. The first step is to derive\nlatent variables that can be used to construct synthetic volatility surfaces\nthat are indistinguishable from those observed historically. The second step is\nto determine the synthetic surface generated by our latent variables that fits\navailable data as closely as possible. As a dividend of our first step, the\nsynthetic surfaces produced can also be used in stress testing, in market\nsimulators for developing quantitative investment strategies, and for the\nvaluation of exotic options. We illustrate our procedure and demonstrate its\npower using foreign exchange market data.\n"
    },
    {
        "paper_id": 2102.03956,
        "authors": "Fabio Italo Martinenghi",
        "title": "Increasing the price of a university degree does not significantly\n  affect enrolment if income contingent loans are available: evidence from HECS\n  in Australia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I provide evidence that, when income-contingent loans are available, student\nenrolment in university courses is not significantly affected by large\nincreases in the price of those courses. I use publicly available domestic\nenrolment data from Australia. I study whether large increases in the price of\nhigher education for selected disciplines in Australia in 2009 and in 2012 was\nassociated with changes in their enrolment growth. I find that large increases\nin the price of a course did not lead to significant changes in their enrolment\ngrowth for that course.\n"
    },
    {
        "paper_id": 2102.0409,
        "authors": "A. Sardi and E. Sorano",
        "title": "Dynamic Performance Management: An Approach for Managing the Common\n  Goods",
        "comments": null,
        "journal-ref": "https://www.mdpi.com/2071-1050/11/22/6435",
        "doi": "10.3390/su11226435",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public organizations need innovative approaches for managing common goods and\nto explain the dynamics linking the (re)generation of common goods and\norganizational performance. Although system dynamics is recognised as a useful\napproach for managing common goods, public organizations rarely adopt the\nsystem dynamics for this goal. The paper aims to review the literature on the\nsystem dynamics and its recent application, known as dynamic performance\nmanagement, to highlight the state of the art and future opportunities on the\nmanagement of common goods. The authors analyzed 144 documents using a\nsystematic literature review. The results obtained outline a fair number of\ndocuments, countries and journals involving the study of system dynamics, but\ndo not cover sufficient research on the linking between the (re)generation of\ncommon goods and organizational performance. This paper outlines academic and\npractical contributions. Firstly, it contributes to the theory of common goods.\nIt provides insight for linking the management of common goods and\norganizational performance through the use of dynamic performance management\napproach. Furthermore, it shows scholars the main research opportunities.\nSecondly, it indicates to practitioners the documents providing useful ideas on\nthe adoption of system dynamics for managing common goods.\n"
    },
    {
        "paper_id": 2102.04093,
        "authors": "Alberto Sardi, Alessandro Rizzi, Enrico Sorano, Anna Guerrieri",
        "title": "Cyber Risk in Health Facilities: A Systematic Literature Review",
        "comments": null,
        "journal-ref": "https://www.mdpi.com/2071-1050/12/17/7002",
        "doi": "10.3390/su12177002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The current world challenges include issues such as infectious disease\npandemics, environmental health risks, food safety, and crime prevention.\nThrough this article, a special emphasis is given to one of the main challenges\nin the healthcare sector during the COVID-19 pandemic, the cyber risk. Since\nthe beginning of the Covid-19 pandemic, the World Health Organization has\ndetected a dramatic increase in the number of cyber-attacks. For instance, in\nItaly the COVID-19 emergency has heavily affected cybersecurity; from January\nto April 2020, the total of attacks, accidents, and violations of privacy to\nthe detriment of companies and individuals has doubled. Using a systematic and\nrigorous approach, this paper aims to analyze the literature on the cyber risk\nin the healthcare sector to understand the real knowledge on this topic. The\nfindings highlight the poor attention of the scientific community on this\ntopic, except in the United States. The literature lacks research contributions\nto support cyber risk management in subject areas such as Business, Management\nand Accounting; Social Science; and Mathematics. This research outlines the\nneed to empirically investigate the cyber risk, giving a practical solution to\nhealth facilities. Keywords: cyber risk; cyber-attack; cybersecurity; computer\nsecurity; COVID-19; coronavirus;information technology risk; risk management;\nrisk assessment; health facilities; healthcare sector;systematic literature\nreview; insurance\n"
    },
    {
        "paper_id": 2102.04137,
        "authors": "Szilvia Borbely, Csaba Mako, Miklos Illessy, Saeed Nostrabadi",
        "title": "Trade Union Strategies towards Platform Workers: Exploration Instead of\n  Action (The Case of Hungarian Trade Unions)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although the share of platform work is very small compared to conventional\nand traditional employment system, research shows that the use of platform work\nis increasingly growing all over the world. Trade unions have also paid special\nattention to the platform work because they know that the transfer of a\npercentage of human resources to the platforms is undeniable. To this end, the\ntrade unions prepare themselves for the challenges and dynamics of this\nemerging phenomenon in the field of human resources. Using a qualitative\nresearch method and a case study of Hungary, the present study aimed to\nidentify the strategies adopted by Trade Unions to manage the transition to\nplatform works and provide suggestions for both practitioners and future\nresearch.\n"
    },
    {
        "paper_id": 2102.0416,
        "authors": "Vladim\\'ir Hol\\'y and Michal \\v{C}ern\\'y",
        "title": "Bertram's Pairs Trading Strategy with Bounded Risk",
        "comments": null,
        "journal-ref": "Hol\\'y, V., & \\v{C}ern\\'y, M. (2022). Bertram's Pairs Trading\n  Strategy with Bounded Risk. Central European Journal of Operations Research,\n  30(2), 667-682",
        "doi": "10.1007/s10100-021-00763-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finding Bertram's optimal trading strategy for a pair of cointegrated assets\nfollowing the Ornstein--Uhlenbeck price difference process can be formulated as\nan unconstrained convex optimization problem for maximization of expected\nprofit per unit of time. This model is generalized to the form where the\nriskiness of profit, measured by its per-time-unit volatility, is controlled\n(e.g. in case of existence of limits on riskiness of trading strategies imposed\nby regulatory bodies). The resulting optimization problem need not be convex.\nIn spite of this undesirable fact, it is demonstrated that the problem is still\nefficiently solvable. In addition, the problem that parameters of the price\ndifference process are never known exactly and are imprecisely estimated from\nan observed finite sample is investigated (recalling that this problem is\ncritical for practice). It is shown how the imprecision affects the optimal\ntrading strategy by quantification of the loss caused by the imprecise estimate\ncompared to a theoretical trader knowing the parameters exactly. The main\nresults focus on the geometric and optimization-theoretic viewpoint of the\nrisk-bounded trading strategy and the imprecision resulting from the\nstatistical estimates.\n"
    },
    {
        "paper_id": 2102.04176,
        "authors": "Sourish Dutta",
        "title": "Research Methods of Assessing Global Value Chains",
        "comments": "15 pages",
        "journal-ref": "SSRN Electronic Journal, 2021",
        "doi": "10.2139/ssrn.3784173",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The global production (as a system of creating values) is eventually forming\na vast web of value chains that explains the transitional structures of global\ntrade and development of the world economy. It is truly a new wave of\nglobalisation, and we can term it as the global value chains (GVCs), creating\nthe nexus among firms, workers and consumers around the globe. The emergence of\nthis new scenario is asking how an economy's businesses, producers and\nemployees are connecting to the global economy and capturing the gains out of\nit regarding different dimensions of economic development. Indeed, this GVC\napproach is very crucial for understanding the organisation of the global\nindustries (including firms) through analysing the statics and dynamics of\ndifferent economic players involved in this complex global production network.\nIts widespread notion deals with various global issues (including regional\nvalue chains also) from the top down to the bottom up, founding a scope for\npolicy analysis.\n"
    },
    {
        "paper_id": 2102.04263,
        "authors": "Samuel Cohen and Tanut Treetanthiploet",
        "title": "Generalised correlated batched bandits via the ARC algorithm with\n  application to dynamic pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Asymptotic Randomised Control (ARC) algorithm provides a rigorous\napproximation to the optimal strategy for a wide class of Bayesian bandits,\nwhile retaining low computational complexity. In particular, the ARC approach\nprovides nearly optimal choices even when the payoffs are correlated or more\nthan the reward is observed. The algorithm is guaranteed to asymptotically\noptimise the expected discounted payoff, with error depending on the initial\nuncertainty of the bandit. In this paper, we extend the ARC framework to\nconsider a batched bandit problem where observations arrive from a generalised\nlinear model. In particular, we develop a large sample approximation to allow\ncorrelated and generally distributed observation. We apply this to a classic\ndynamic pricing problem based on a Bayesian hierarchical model and demonstrate\nthat the ARC algorithm outperforms alternative approaches.\n"
    },
    {
        "paper_id": 2102.04337,
        "authors": "Federico Echenique and Alfred Galichon",
        "title": "Ordinal and cardinal solution concepts for two-sided matching",
        "comments": "29 pages, 2 figures",
        "journal-ref": "Games and Economic Behavior 101 (2017) pp. 63-77",
        "doi": "10.1016/j.geb.2015.10.002",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We characterize solutions for two-sided matching, both in the transferable\nand in the nontransferable-utility frameworks, using a cardinal formulation.\nOur approach makes the comparison of the matching models with and without\ntransfers particularly transparent. We introduce the concept of a no-trade\nmatching to study the role of transfers in matching. A no-trade matching is one\nin which the availability of transfers do not affect the outcome.\n"
    },
    {
        "paper_id": 2102.04532,
        "authors": "Sandhya Devi",
        "title": "Asymmetric Tsallis distributions for modelling financial market dynamics",
        "comments": "13 figures, 28 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126109",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial markets are highly non-linear and non-equilibrium systems. Earlier\nworks have suggested that the behavior of market returns can be well described\nwithin the framework of non-extensive Tsallis statistics or superstatistics.\nFor small time scales (delays), a good fit to the distributions of stock\nreturns is obtained with q-Gaussian distributions, which can be derived either\nfrom Tsallis statistics or superstatistics. These distributions are symmetric.\nHowever, as the time lag increases, the distributions become increasingly\nnon-symmetric. In this work, we address this problem by considering the data\ndistribution as a linear combination of two independent normalized\ndistributions - one for negative returns and one for positive returns. Each of\nthese two independent distributions are half q-Gaussians with different\nnon-extensivity parameter q and temperature parameter beta. Using this model,\nwe investigate the behavior of stock market returns over time scales from 1 to\n80 days. The data covers both the .com bubble and the 2008 crash periods. These\ninvestigations show that for all the time lags, the fits to the data\ndistributions are better using asymmetric distributions than symmetric\nq-Gaussian distributions. The behaviors of the q parameter are quite different\nfor positive and negative returns. For positive returns, q approaches a\nconstant value of 1 after a certain lag, indicating the distributions have\nreached equilibrium. On the other hand, for negative returns, the q values do\nnot reach a stationary value over the time scales studied. In the present\nmodel, the markets show a transition from normal to superdiffusive behavior (a\npossible phase transition) during the 2008 crash period. Such behavior is not\nobserved with a symmetric q-Gaussian distribution model with q independent of\ntime lag.\n"
    },
    {
        "paper_id": 2102.04591,
        "authors": "Zhiyong Cheng, Jun Deng, Tianyi Wang, Mei Yu",
        "title": "Liquidation, Leverage and Optimal Margin in Bitcoin Futures Markets",
        "comments": "21 pages, 5 fugures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using the generalized extreme value theory to characterize tail\ndistributions, we address liquidation, leverage, and optimal margins for\nbitcoin long and short futures positions. The empirical analysis of perpetual\nbitcoin futures on BitMEX shows that (1) daily forced liquidations to out-\nstanding futures are substantial at 3.51%, and 1.89% for long and short; (2)\ninvestors got forced liquidation do trade aggressively with average leverage of\n60X; and (3) exchanges should elevate current 1% margin requirement to 33% (3X\nleverage) for long and 20% (5X leverage) for short to reduce the daily margin\ncall probability to 1%. Our results further suggest normality assumption on\nreturn significantly underestimates optimal margins. Policy implications are\nalso discussed.\n"
    },
    {
        "paper_id": 2102.04658,
        "authors": "Eiji Yamamura",
        "title": "View about consumption tax and grandchildren",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Japan, the increase in the consumption tax rate, a measure of balanced\npublic finance, reduces the inequality of fiscal burden between the present and\nfuture generations. This study estimates the effect of grandchildren on an\nolder person's view of consumption tax, using independently collected data. The\nresults show that having grandchildren is positively associated with supporting\nan increase in consumption tax. Further, this association is observed strongly\nbetween granddaughters and grandparents. However, the association between\ngrandsons and grandparents depends on the sub-sample. This implies that people\nof the old generation are likely to accept the tax burden to reduce the burden\non their grandchildren, especially granddaughters. In other words, grandparents\nshow intergenerational altruism.\n"
    },
    {
        "paper_id": 2102.04757,
        "authors": "Samuel N. Cohen and Derek Snow and Lukasz Szpruch",
        "title": "Black-box model risk in finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning models are increasingly used in a wide variety of financial\nsettings. The difficulty of understanding the inner workings of these systems,\ncombined with their wide applicability, has the potential to lead to\nsignificant new risks for users; these risks need to be understood and\nquantified. In this sub-chapter, we will focus on a well studied application of\nmachine learning techniques, to pricing and hedging of financial options. Our\naim will be to highlight the various sources of risk that the introduction of\nmachine learning emphasises or de-emphasises, and the possible risk mitigation\nand management strategies that are available.\n"
    },
    {
        "paper_id": 2102.04758,
        "authors": "Jacob Janssen and Yaneer Bar-Yam",
        "title": "Lowest-cost virus suppression",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Analysis of policies for managing epidemics require simultaneously an\neconomic and epidemiological perspective. We adopt a cost-of-policy framework\nto model both the virus spread and the cost of handling the pandemic. Because\nit is harder and more costly to fight the pandemic when the circulation is\nhigher, we find that the optimal policy is to go to zero or near-zero case\nnumbers. Without imported cases, if a region is willing to implement measures\nto prevent spread at one level in number of cases, it must also be willing to\nprevent the spread with at a lower level, since it will be cheaper to do so and\nhas only positive other effects. With imported cases, if a region is not\ncoordinating with other regions, we show the cheapest policy is continually low\nbut nonzero cases due to decreasing cost of halting imported cases. When it is\ncoordinating, zero is cost-optimal. Our analysis indicates that within Europe\ncooperation targeting a reduction of both within country transmission, and\nbetween country importation risk, should help achieve lower transmission and\nreduced costs.\n"
    },
    {
        "paper_id": 2102.04861,
        "authors": "Yiqi Zhao and Matloob Khushi",
        "title": "Wavelet Denoised-ResNet CNN and LightGBM Method to Predict Forex Rate of\n  Change",
        "comments": null,
        "journal-ref": "2020 IEEE International Conference on Data Mining Workshops\n  (ICDMW), Sorrento, Italy, 11-17 November 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Foreign Exchange (Forex) is the largest financial market in the world. The\ndaily trading volume of the Forex market is much higher than that of stock and\nfutures markets. Therefore, it is of great significance for investors to\nestablish a foreign exchange forecast model. In this paper, we propose a\nWavelet Denoised-ResNet with LightGBM model to predict the rate of change of\nForex price after five time intervals to allow enough time to execute trades.\nAll the prices are denoised by wavelet transform, and a matrix of 30 time\nintervals is formed by calculating technical indicators. Image features are\nobtained by feeding the maxtrix into a ResNet. Finally, the technical\nindicators and image features are fed to LightGBM. Our experiments on 5-minutes\nUSDJPY demonstrate that the model outperforms baseline modles with MAE:\n0.240977x10EXP-3 MSE: 0.156x10EXP-6 and RMSE: 0.395185x10EXP-3. An accurate\nprice prediction after 25 minutes in future provides a window of opportunity\nfor hedge funds algorithm trading. The code is available from\nhttps://mkhushi.github.io/\n"
    },
    {
        "paper_id": 2102.04936,
        "authors": "Rajiv Sethi, Julie Seager, Emily Cai, Daniel M. Benjamin, Fred\n  Morstatter",
        "title": "Models, Markets, and the Forecasting of Elections",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine probabilistic forecasts for battleground states in the 2020 US\npresidential election, using daily data from two sources over seven months: a\nmodel published by The Economist, and prices from the PredictIt exchange. We\nfind systematic differences in accuracy over time, with markets performing\nbetter several months before the election, and the model performing better as\nthe election approached. A simple average of the two forecasts performs better\nthan either one of them overall, even though no average can outperform both\ncomponent forecasts for any given state-date pair. This effect arises because\nthe model and the market make different kinds of errors in different states:\nthe model was confidently wrong in some cases, while the market was excessively\nuncertain in others. We conclude that there is value in using hybrid\nforecasting methods, and propose a market design that incorporates model\nforecasts via a trading bot to generate synthetic predictions. We also propose\nand conduct a profitability test that can be used as a novel criterion for the\nevaluation of forecasting performance.\n"
    },
    {
        "paper_id": 2102.05003,
        "authors": "Nawaf Mohammed, Edward Furman and Jianxi Su",
        "title": "Can a regulatory risk measure induce profit-maximizing risk capital\n  allocations? The case of Conditional Tail Expectation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Risk capital allocations (RCAs) are an important tool in quantitative risk\nmanagement, where they are utilized to, e.g., gauge the profitability of\ndistinct business units, determine the price of a new product, and conduct the\nmarginal economic capital analysis. Nevertheless, the notion of RCA has been\nliving in the shadow of another, closely related notion, of risk measure (RM)\nin the sense that the latter notion often shapes the fashion in which the\nformer notion is implemented. In fact, as the majority of the RCAs known\nnowadays are induced by RMs, the popularity of the two are apparently very much\ncorrelated. As a result, it is the RCA that is induced by the Conditional Tail\nExpectation (CTE) RM that has arguably prevailed in scholarly literature and\napplications. Admittedly, the CTE RM is a sound mathematical object and an\nimportant regulatory RM, but its appropriateness is controversial in, e.g.,\nprofitability analysis and pricing. In this paper, we address the question as\nto whether or not the RCA induced by the CTE RM may concur with alternatives\nthat arise from the context of profit maximization. More specifically, we\nprovide exhaustive description of all those probabilistic model settings, in\nwhich the mathematical and regulatory CTE RM may also reflect the risk\nperception of a profit-maximizing insurer.\n"
    },
    {
        "paper_id": 2102.05338,
        "authors": "Santiago Garcia",
        "title": "Group Quantization of Quadratic Hamiltonians in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Group Quantization formalism is a scheme for constructing a functional\nspace that is an irreducible infinite dimensional representation of the Lie\nalgebra belonging to a dynamical symmetry group. We apply this formalism to the\nconstruction of functional space and operators for quadratic potentials --\ngaussian pricing kernels in finance. We describe the Black-Scholes theory, the\nHo-Lee interest rate model and the Euclidean repulsive and attractive\noscillators. The symmetry group used in this work has the structure of a\nprincipal bundle with base (dynamical) group a semi-direct extension of the\nHeisenberg-Weyl group by SL(2,R), and structure group (fiber) the positive real\nline.\n"
    },
    {
        "paper_id": 2102.05358,
        "authors": "Giovanni Abramo, Ciriaco Andrea D'Angelo, Leonardo Grilli",
        "title": "The effects of citation-based research evaluation schemes on\n  self-citation behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the changes in the self-citation behavior of Italian\nprofessors following the introduction of a citation-based incentive scheme, for\nnational accreditation to academic appointments. Previous contributions on\nself-citation behavior have either focused on small samples or relied on simple\nmodels, not controlling for all confounding factors. The present work adopts a\ncomplex statistics model implemented on bibliometric individual data for over\n15,000 Italian professors. Controlling for a number of covariates (number of\ncitable papers published by the author; presence of international authors;\nnumber of co-authors; degree of the professor's specialization), the average\nincrease in self-citation rates following introduction of the ASN is of 9.5%.\nThe increase is common to all disciplines and academic ranks, albeit with\ndiverse magnitude. Moreover, the increase is sensitive to the relative\nincentive, depending on the status of the scholar with respect to the\nscientific accreditation. A further analysis shows that there is much\nheterogeneity in the individual patterns of self-citing behavior, albeit with\nvery few outliers.\n"
    },
    {
        "paper_id": 2102.0536,
        "authors": "Giovanni Abramo, Ciriaco Andrea D'Angelo, Ida Mele",
        "title": "Gendered impact of COVID-19 pandemic on research production: a\n  cross-country analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The massive shock of the COVID-19 pandemic is already showing its negative\neffects on economies around the world, unprecedented in recent history.\nCOVID-19 infections and containment measures have caused a general slowdown in\nresearch and new knowledge production. Because of the link between R&D spending\nand economic growth, it is to be expected then that a slowdown in research\nactivities will slow in turn the global recovery from the pandemic. Many recent\nstudies also claim an uneven impact on scientific production across gender. In\nthis paper, we investigate the phenomenon across countries, analysing preprint\ndepositions. Differently from other works, that compare the number of preprint\ndepositions before and after the pandemic outbreak, we analyse the depositions\ntrends across geographical areas, and contrast after-pandemic depositions with\nexpected ones. Differently from common belief and initial evidence, in few\ncountries female scientists increased their scientific output while males\nplunged.\n"
    },
    {
        "paper_id": 2102.05364,
        "authors": "Giovanni Abramo, Francesca Apponi, Ciriaco Andrea D'Angelo",
        "title": "Do the propensity and drivers of academics' engagement in research\n  collaboration with industry vary over time?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study is about public-private research collaboration. In particular, we\nwant to measure how the propensity of academics to collaborate with their\ncolleagues from private firms varies over time and whether the typical profile\nof such academics change. Furthermore, we investigate the change of the weights\nof main drivers underlying the academics' propensity to collaborate with\nindustry. In order to achieve such goals, we apply an inferential model on a\ndataset of professors working in Italian universities in two subsequent\nperiods, 2010-2013 and 2014-2017. Results can be useful for supporting the\ndefinition of policies aimed at fostering public-private research\ncollaborations, and should be taken into account when assessing their\neffectiveness afterwards.\n"
    },
    {
        "paper_id": 2102.05398,
        "authors": "Souhir Ben Amor, Michael Althof, Wolfgang Karl H\\\"ardle",
        "title": "FRM Financial Risk Meter for Emerging Markets",
        "comments": "47 pages, 35 figures, submitted in SoFie Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The fast-growing Emerging Market (EM) economies and their improved\ntransparency and liquidity have attracted international investors. However, the\nexternal price shocks can result in a higher level of volatility as well as\ndomestic policy instability. Therefore, an efficient risk measure and hedging\nstrategies are needed to help investors protect their investments against this\nrisk. In this paper, a daily systemic risk measure, called FRM (Financial Risk\nMeter) is proposed. The FRM-EM is applied to capture systemic risk behavior\nembedded in the returns of the 25 largest EMs FIs, covering the BRIMST (Brazil,\nRussia, India, Mexico, South Africa, and Turkey), and thereby reflects the\nfinancial linkages between these economies. Concerning the Macro factors, in\naddition to the Adrian and Brunnermeier (2016) Macro, we include the EM\nsovereign yield spread over respective US Treasuries and the above-mentioned\ncountries currencies. The results indicated that the FRM of EMs FIs reached its\nmaximum during the US financial crisis following by COVID 19 crisis and the\nMacro factors explain the BRIMST FIs with various degrees of sensibility. We\nthen study the relationship between those factors and the tail event network\nbehavior to build our policy recommendations to help the investors to choose\nthe suitable market for in-vestment and tail-event optimized portfolios. For\nthat purpose, an overlapping region between portfolio optimization strategies\nand FRM network centrality is developed. We propose a robust and\nwell-diversified tail-event and cluster risk-sensitive portfolio allocation\nmodel and compare it to more classical approaches\n"
    },
    {
        "paper_id": 2102.05405,
        "authors": "Andrea Vandin, Daniele Giachini, Francesco Lamperti, Francesca\n  Chiaromonte",
        "title": "Automated and Distributed Statistical Analysis of Economic Agent-Based\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jedc.2022.104458",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach to the statistical analysis of stochastic\nsimulation models and, especially, agent-based models (ABMs). Our main goal is\nto provide fully automated, model-independent and tool-supported techniques and\nalgorithms to inspect simulations and perform counterfactual analysis. Our\napproach: (i) is easy-to-use by the modeller, (ii) improves reproducibility of\nresults, (iii) optimizes running time given the modeller's machine, (iv)\nautomatically chooses the number of required simulations and simulation steps\nto reach user-specified statistical confidence, and (v) automates a variety of\nstatistical tests. In particular, our techniques are designed to distinguish\nthe transient dynamics of the model from its steady-state behaviour (if any),\nestimate properties in both 'phases', and provide indications on the\n(non-)ergodic nature of the simulated processes - which, in turn, allows one to\ngauge the reliability of a steady-state analysis. Estimates are equipped with\nstatistical guarantees, allowing for robust comparisons across computational\nexperiments. To demonstrate the effectiveness of our approach, we apply it to\ntwo models from the literature: a large-scale macro-financial ABM and a small\nscale prediction market model. Compared to prior analyses of these models, we\nobtain new insights and we are able to identify and fix some erroneous\nconclusions.\n"
    },
    {
        "paper_id": 2102.05411,
        "authors": "Vladim\\'ir Hol\\'y and Tom\\'a\\v{s} Evan",
        "title": "The Role of a Nation's Culture in the Country's Governance: Stochastic\n  Frontier Analysis",
        "comments": null,
        "journal-ref": "Hol\\'y, V. & Evan, T. (2022). The Role of a Nation's Culture in\n  the Country's Governance: Stochastic Frontier Analysis. Central European\n  Journal of Operations Research, 30(2), 507-520",
        "doi": "10.1007/s10100-021-00754-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What role does culture play in determining institutions in a country? This\npaper argues that the establishment of institutions is a process originating\npredominantly in a nation's culture and tries to discern the role of a cultural\nbackground in the governance of countries. We use the six Hofstede's Cultural\nDimensions and the six Worldwide Governance Indicators to test the strength of\nthe relationship on 94 countries between 1996 and 2019. We find that the\nstrongest cultural characteristics are Power Distance with negative effect on\ngovernance and Long-Term Orientation with positive effect. We also determine\nhow well countries transform their cultural characteristics into institutions\nusing stochastic frontier analysis.\n"
    },
    {
        "paper_id": 2102.05448,
        "authors": "Yifan Yao, Lina Wang",
        "title": "Combination of window-sliding and prediction range method based on LSTM\n  model for predicting cryptocurrency",
        "comments": "30 pages,23 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The present study aims to establish the model of the cryptocurrency price\ntrend based on financial theory using the LSTM model with multiple combinations\nbetween the window length and the predicting horizons, the random walk model is\nalso applied with different parameter settings.\n"
    },
    {
        "paper_id": 2102.05506,
        "authors": "Anindya Ghose, Xitong Guo, Beibei Li, Yuanyuan Dang",
        "title": "Empowering Patients Using Smart Mobile Health Platforms: Evidence From A\n  Randomized Field Experiment",
        "comments": "Forthcoming at MIS Quarterly (2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With today's technological advancements, mobile phones and wearable devices\nhave become extensions of an increasingly diffused and smart digital\ninfrastructure. In this paper, we examine mobile health (mHealth) platforms and\ntheir health and economic impacts on the outcomes of chronic disease patients.\nWe partnered with a major mHealth firm that provides one of the largest mHealth\napps in Asia specializing in diabetes care. We designed a randomized field\nexperiment based on detailed patient health activities (e.g., exercises, sleep,\nfood intake) and blood glucose values from 1,070 diabetes patients over several\nmonths. We find the adoption of the mHealth app leads to an improvement in\nhealth behavior, which leads to both short term metrics (reduction in patients'\nblood glucose and glycated hemoglobin levels) and longer-term metrics (hospital\nvisits and medical expenses). Patients who adopted the mHealth app undertook\nmore exercise, consumed healthier food, walked more steps and slept for longer\ntimes. They also were more likely to substitute offline visits with telehealth.\nA comparison of mobile vs. PC version of the same app demonstrates that mobile\nhas a stronger effect than PC in helping patients make these behavioral\nmodifications with respect to diet, exercise and lifestyle, which leads to an\nimprovement in their healthcare outcomes. We also compared outcomes when the\nplatform facilitates personalized health reminders to patients vs. generic\nreminders. Surprisingly, we find personalized mobile messages with\npatient-specific guidance can have an inadvertent (smaller) effect on patient\napp engagement and lifestyle changes, leading to a lower health improvement.\nHowever, they are more like to encourage a substitution of offline visits by\ntelehealth. Overall, our findings indicate the massive potential of mHealth\ntechnologies and platform design in achieving better healthcare outcomes.\n"
    },
    {
        "paper_id": 2102.05554,
        "authors": "Rupam Bhattacharyya, Sheo Rama, Atul Kumar, Indrajit Banerjee",
        "title": "Dynamic Structural Impact of the COVID-19 Outbreak on the Stock Market\n  and the Exchange Rate: A Cross-country Analysis Among BRICS Nations",
        "comments": "18 pages, 4 figures, 2 appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  COVID-19 has impacted the economy of almost every country in the world. Of\nparticular interest are the responses of the economic indicators of developing\nnations (such as BRICS) to the COVID-19 shock. As an extension to our earlier\nwork on the dynamic associations of pandemic growth, exchange rate, and stock\nmarket indices in the context of India, we look at the same question with\nrespect to the BRICS nations. We use structural variable autoregression (SVAR)\nto identify the dynamic underlying associations across the normalized growth\nmeasurements of the COVID-19 cumulative case, recovery, and death counts, and\nthose of the exchange rate, and stock market indices, using data over 203 days\n(March 12 - September 30, 2020). Using impulse response analyses, the COVID-19\nshock to the growth of exchange rate was seen to persist for around 10+ days,\nand that for stock exchange was seen to be around 15 days. The models capture\nthe contemporaneous nature of these shocks and the subsequent responses,\npotentially guiding to inform policy decisions at a national level. Further,\ncausal inference-based analyses would allow us to infer relationships that are\nstronger than mere associations.\n"
    },
    {
        "paper_id": 2102.05568,
        "authors": "Qikun Xiang, Ariel Neufeld, Gareth W. Peters, Ido Nevat, Anwitaman\n  Datta",
        "title": "A Bonus-Malus Framework for Cyber Risk Insurance and Optimal\n  Cybersecurity Provisioning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cyber risk insurance market is at a nascent stage of its development,\neven as the magnitude of cyber losses is significant and the rate of cyber loss\nevents is increasing. Existing cyber risk insurance products as well as\nacademic studies have been focusing on classifying cyber loss events and\ndeveloping models of these events, but little attention has been paid to\nproposing insurance risk transfer strategies that incentivise mitigation of\ncyber loss through adjusting the premium of the risk transfer product. To\naddress this important gap, we develop a Bonus-Malus model for cyber risk\ninsurance. Specifically, we propose a mathematical model of cyber risk\ninsurance and cybersecurity provisioning supported with an efficient numerical\nalgorithm based on dynamic programming. Through a numerical experiment, we\ndemonstrate how a properly designed cyber risk insurance contract with a\nBonus-Malus system can resolve the issue of moral hazard and benefit the\ninsurer.\n"
    },
    {
        "paper_id": 2102.05739,
        "authors": "Gaurab Aryal and Federico Ciliberto and Benjamin T. Leyden",
        "title": "Coordinated Capacity Reductions and Public Communication in the Airline\n  Industry",
        "comments": "forthcoming The Review of Economic Studies",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the allegation that legacy U.S. airlines communicated via\nearnings calls to coordinate with other legacy airlines in offering fewer seats\non competitive routes. To this end, we first use text analytics to build a\nnovel dataset on communication among airlines about their capacity choices.\nEstimates from our preferred specification show that the number of offered\nseats is 2% lower when all legacy airlines in a market discuss the concept of\n\"capacity discipline.\" We verify that this reduction materializes only when\nlegacy airlines communicate concurrently, and that it cannot be explained by\nother possibilities, including that airlines are simply announcing to investors\ntheir unilateral plans to reduce capacity, and then following through on those\nannouncements.\n"
    },
    {
        "paper_id": 2102.05751,
        "authors": "Gaurab Aryal and Charles Murry and Jonathan W. Williams",
        "title": "Price Discrimination in International Airline Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a model of inter-temporal and intra-temporal price discrimination\nby monopoly airlines to study the ability of different discriminatory pricing\nmechanisms to increase efficiency and the associated distributional\nimplications. To estimate the model, we use unique data from international\nairline markets with flight-level variation in prices across time, cabins, and\nmarkets and information on passengers' reasons for travel and time of purchase.\nThe current pricing practice yields approximately 77% of the first-best\nwelfare. The source of this inefficiency arises primarily from private\ninformation about passenger valuations, not dynamic uncertainty about demand.\nWe also find that if airlines could discriminate between business and leisure\npassengers, total welfare would improve at the expense of business passenger\nsurplus. Also, replacing the current pricing that involves screening passengers\nacross cabin classes with offering a single cabin class has minimal effect on\ntotal welfare.\n"
    },
    {
        "paper_id": 2102.05799,
        "authors": "Nicholas Moehle, Stephen Boyd, Andrew Ang",
        "title": "Portfolio Performance Attribution via Shapley Value",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investment process that includes a number of features, each of\nwhich can be active or inactive. Our goal is to attribute or decompose an\nachieved performance to each of these features, plus a baseline value. There\nare many ways to do this, which lead to potentially different attributions in\nany specific case. We argue that a specific attribution method due to Shapley\nis the preferred method, and discuss methods that can be used to compute this\nattribution exactly, or when that is not practical, approximately.\n"
    },
    {
        "paper_id": 2102.05803,
        "authors": "Alina Malkova, Klara Sabirianova Peter, Jan Svejnar",
        "title": "Labor Informality and Credit Market Accessibility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper investigates the effects of the credit market development on the\nlabor mobility between the informal and formal labor sectors. In the case of\nRussia, due to the absence of a credit score system, a formal lender may set a\ncredit limit based on the verified amount of income. To get a loan, an informal\nworker must first formalize his or her income (switch to a formal job), and\nthen apply for a loan. To show this mechanism, the RLMS data was utilized, and\nthe empirical method is the dynamic multinomial logit model of employment. The\nempirical results show that a relaxation of credit constraints increases the\nprobability of transition from an informal to a formal job, and improved CMA\n(by one standard deviation) increases the chances of informal sector workers to\nformalize by 5.4 ppt. These results are robust in different specifications of\nthe model. Policy simulations show strong support for a reduction in informal\nemployment in response to better CMA in credit-constrained communities.\n"
    },
    {
        "paper_id": 2102.05876,
        "authors": "Changkuk Im and Jinkwon Lee",
        "title": "On the Fragility of Third-party Punishment: The Context Effect of a\n  Dominated Risky Investment Option",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Experimental studies regularly show that third-party punishment (TPP)\nsubstantially exists in various settings. This study further investigates the\nrobustness of TPP under an environment where context effects are involved. In\nour experiment, we offer a third party an additional but unattractive risky\ninvestment option. We find that, when the dominated investment option\nirrelevant to prosocial behavior is available, the demand for punishment\ndecreases, whereas the demand for investment increases. These findings support\nour hypothesis that the seemingly unrelated and dominated investment option may\nwork as a compromise and suggest the fragility of TPP in this setting.\n"
    },
    {
        "paper_id": 2102.06233,
        "authors": "Kumar Yashaswi",
        "title": "Deep Reinforcement Learning for Portfolio Optimization using Latent\n  Feature State Space (LFSS) Module",
        "comments": "14 Pages, 13 Figures, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic Portfolio optimization is the process of distribution and rebalancing\nof a fund into different financial assets such as stocks, cryptocurrencies,\netc, in consecutive trading periods to maximize accumulated profits or minimize\nrisks over a time horizon. This field saw huge developments in recent years,\nbecause of the increased computational power and increased research in\nsequential decision making through control theory. Recently Reinforcement\nLearning(RL) has been an important tool in the development of sequential and\ndynamic portfolio optimization theory. In this paper, we design a Deep\nReinforcement Learning(DRL) framework as an autonomous portfolio optimization\nagent consisting of a Latent Feature State Space(LFSS) Module for filtering and\nfeature extraction of financial data which is used as a state space for deep RL\nmodel. We develop an extensive RL agent with high efficiency and performance\nadvantages over several benchmarks and model-free RL agents used in prior work.\nThe noisy and non-stationary behaviour of daily asset prices in the financial\nmarket is addressed through Kalman Filter. Autoencoders, ZoomSVD, and\nrestricted Boltzmann machines were the models used and compared in the module\nto extract relevant time series features as state space. We simulate weekly\ndata, with practical constraints and transaction costs, on a portfolio of S&P\n500 stocks. We introduce a new benchmark based on technical indicator Kd-Index\nand Mean-Variance Model as compared to equal weighted portfolio used in most of\nthe prior work. The study confirms that the proposed RL portfolio agent with\nstate space function in the form of LFSS module gives robust results with an\nattractive performance profile over baseline RL agents and given benchmarks.\n"
    },
    {
        "paper_id": 2102.06274,
        "authors": "Oleg Szehr",
        "title": "Hedging of Financial Derivative Contracts via Monte Carlo Tree Search",
        "comments": "Corrected typos. Shorter Presentation. 15 pages, 5 figures",
        "journal-ref": "Journal of Computational Finance, Volume 27, Number 2, Pages:\n  47-80, 2023",
        "doi": "10.21314/JCF.2023.009",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The construction of approximate replication strategies for pricing and\nhedging of derivative contracts in incomplete markets is a key problem of\nfinancial engineering. Recently Reinforcement Learning algorithms for hedging\nunder realistic market conditions have attracted significant interest. While\nresearch in the derivatives area mostly focused on variations of $Q$-learning,\nin artificial intelligence Monte Carlo Tree Search is the recognized\nstate-of-the-art method for various planning problems, such as the games of\nHex, Chess, Go,... This article introduces Monte Carlo Tree Search as a method\nto solve the stochastic optimal control problem behind the pricing and hedging\ntasks. As compared to $Q$-learning it combines Reinforcement Learning with tree\nsearch techniques. As a consequence Monte Carlo Tree Search has higher sample\nefficiency, is less prone to over-fitting to specific market models and\ngenerally learns stronger policies faster. In our experiments we find that\nMonte Carlo Tree Search, being the world-champion in games like Chess and Go,\nis easily capable of maximizing the utility of investor's terminal wealth\nwithout setting up an auxiliary mathematical framework.\n"
    },
    {
        "paper_id": 2102.06299,
        "authors": "Jean-Philippe Aguilar, Nicolas Pesci and Victor James",
        "title": "A structural approach to default modelling with pure jump processes",
        "comments": "V3 (final version): correction of minor typos and bibliographical\n  references",
        "journal-ref": null,
        "doi": "10.1080/1350486X.2021.1957956",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general framework for the estimation of corporate default based\non a firm's capital structure, when its assets are assumed to follow a pure\njump L\\'evy processes; this setup provides a natural extension to usual default\nmetrics defined in diffusion (log-normal) models, and allows to capture extreme\nmarket events such as sudden drops in asset prices, which are closely linked to\ndefault occurrence. Within this framework, we introduce several pure jump\nprocesses featuring negative jumps only and derive practical closed formulas\nfor equity prices, which enable us to use a moment-based algorithm to calibrate\nthe parameters from real market data and to estimate the associated default\nmetrics. A notable feature of these models is the redistribution of credit risk\ntowards shorter maturity: this constitutes an interesting improvement to\ndiffusion models, which are known to underestimate short term default\nprobabilities. We also provide extensions to a model featuring both positive\nand negative jumps and discuss qualitative and quantitative features of the\nresults. For readers convenience, practical tools for model implementation and\nGitHub links are also included.\n"
    },
    {
        "paper_id": 2102.06331,
        "authors": "Federico Echenique, Kota Saito, Taisuke Imai",
        "title": "Approximate Expected Utility Rationalization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new measure of deviations from expected utility theory. For any\npositive number~$e$, we give a characterization of the datasets with a\nrationalization that is within~$e$ (in beliefs, utility, or perceived prices)\nof expected utility theory. The number~$e$ can then be used as a measure of how\nfar the data is to expected utility theory. We apply our methodology to data\nfrom three large-scale experiments. Many subjects in those experiments are\nconsistent with utility maximization, but not with expected utility\nmaximization. Our measure of distance to expected utility is correlated with\nsubjects' demographic characteristics.\n"
    },
    {
        "paper_id": 2102.06404,
        "authors": "Emanuele Bacchiocchi and Catalin Dragomirescu-Gaina",
        "title": "Uncertainty spill-overs: when policy and financial realms overlap",
        "comments": "52 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  No matter its source, financial- or policy-related, uncertainty can feed onto\nitself, inflicting the real economic sector, altering expectations and\nbehaviours, and leading to identification challenges in empirical applications.\nThe strong intertwining between policy and financial realms prevailing in\nEurope, and in Euro Area in particular, might complicate the problem and create\namplification mechanisms difficult to pin down. To reveal the complex\ntransmission of country-specific uncertainty shocks in a multi-country setting,\nand to properly account for cross-country interdependencies, we employ a global\nVAR specification for which we adapt an identification approach based on\nmagnitude restrictions. Once we separate policy uncertainty from financial\nuncertainty shocks, we find evidence of important cross-border uncertainty\nspill-overs. We also uncover a new amplification mechanism for domestic\nuncertainty shocks, whose true nature becomes more blurred once they cross the\nnational boundaries and spill over to other countries. With respect to ECB\npolicy reactions, we reveal stronger but less persistent responses to financial\nuncertainty shocks compared to policy uncertainty shocks. This points to ECB\nadopting a more (passive or) accommodative stance towards the former, but a\nmore pro-active stance towards the latter shocks, possibly as an attempt to\ntame policy uncertainty spill-overs and prevent the fragmentation of the Euro\nArea financial markets.\n"
    },
    {
        "paper_id": 2102.06487,
        "authors": "Pierre-Andr\\'e Chiappori, Alfred Galichon and Bernard Salani\\'e",
        "title": "On Human Capital and Team Stability",
        "comments": "30 pages",
        "journal-ref": "Journal of Human Capital 13-2 (2019) pp. 236-259",
        "doi": "10.1086/702925",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In many economic contexts, agents from a same population team up to better\nexploit their human capital. In such contexts (often called \"roommate matching\nproblems\"), stable matchings may fail to exist even when utility is\ntransferable. We show that when each individual has a close substitute, a\nstable matching can be implemented with minimal policy intervention. Our\nresults shed light on the stability of partnerships on the labor market.\nMoreover, they imply that the tools crafted in empirical studies of the\nmarriage problem can easily be adapted to many roommate problems.\n"
    },
    {
        "paper_id": 2102.06547,
        "authors": "Edoardo Ciscato, Alfred Galichon and Marion Gouss\\'e",
        "title": "Like Attract Like? A Structural Comparison of Homogamy across Same-Sex\n  and Different-Sex Households",
        "comments": "48 pages, 8 tables",
        "journal-ref": "Journal of Political Economy 128-2 (2020) pp. 740-781",
        "doi": "10.1086/704611",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we extend Gary Becker's empirical analysis of the marriage\nmarket to same-sex couples. Becker's theory rationalizes the well-known\nphenomenon of homogamy among different-sex couples: individuals mate with their\nlikes because many characteristics, such as education, consumption behaviour,\ndesire to nurture children, religion, etc., exhibit strong complementarities in\nthe household production function. However, because of asymmetries in the\ndistributions of male and female characteristics, men and women may need to\nmarry \"up\" or \"down\" according to the relative shortage of their\ncharacteristics among the populations of men and women. Yet, among same-sex\ncouples, this limitation does not exist as partners are drawn from the same\npopulation, and thus the theory of assortative mating would boldly predict that\nindividuals will choose a partner with nearly identical characteristics.\nEmpirical evidence suggests a very different picture: a robust stylized fact is\nthat the correlation of the characteristics is in fact weaker among same-sex\ncouples. In this paper, we build an equilibrium model of same-sex marriage\nmarket which allows for straightforward identification of the gains to\nmarriage. We estimate the model with 2008-2012 ACS data on California and show\nthat positive assortative mating is weaker for homosexuals than for\nheterosexuals with respect to age and race. Our results suggest that positive\nassortative mating with respect to education is stronger among lesbians, and\nnot significantly different when comparing gay men and married different-sex\ncouples. As regards labor market outcomes, such as hourly wages and working\nhours, we find some indications that the process of specialization within the\nhousehold mainly applies to different-sex couples.\n"
    },
    {
        "paper_id": 2102.06693,
        "authors": "Jos\\'e Manuel Corcuera",
        "title": "The Golden Age of the Mathematical Finance",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is devoted to show that the last quarter of the past century can\nbe considered as the golden age of the Mathematical Finance. In this period the\ncollaboration of great economists and the best generation of probabilists, most\nof them from the Strasbourg's School led by Paul Andr\\'e Meyer, gave rise to\nthe foundations of this discipline. They established the two fundamentals\ntheorems of arbitrage theory, close formulas for options, the main modelling a\n"
    },
    {
        "paper_id": 2102.07001,
        "authors": "Friedhelm Victor, Andrea Marie Weintraud",
        "title": "Detecting and Quantifying Wash Trading on Decentralized Cryptocurrency\n  Exchanges",
        "comments": "Accepted at the Web Conference 2021 (WWW '21) 10 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1145/3442381.3449824",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptoassets such as cryptocurrencies and tokens are increasingly traded on\ndecentralized exchanges. The advantage for users is that the funds are not in\ncustody of a centralized external entity. However, these exchanges are prone to\nmanipulative behavior. In this paper, we illustrate how wash trading activity\ncan be identified on two of the first popular limit order book-based\ndecentralized exchanges on the Ethereum blockchain, IDEX and EtherDelta. We\nidentify a lower bound of accounts and trading structures that meet the legal\ndefinitions of wash trading, discovering that they are responsible for a wash\ntrading volume in equivalent of 159 million U.S. Dollars. While self-trades and\ntwo-account structures are predominant, complex forms also occur. We quantify\nthese activities, finding that on both exchanges, more than 30\\% of all traded\ntokens have been subject to wash trading activity. On EtherDelta, 10% of the\ntokens have almost exclusively been wash traded. All data is made available for\nfuture research. Our findings underpin the need for countermeasures that are\napplicable in decentralized systems.\n"
    },
    {
        "paper_id": 2102.07147,
        "authors": "Xiaoyan Wang, Xi Lin and Meng Li",
        "title": "Aggregate Modeling and Equilibrium Analysis of the Crowdsourcing Market\n  for Autonomous Vehicles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Autonomous vehicles (AVs) have the potential of reshaping the human mobility\nin a wide variety of aspects. This paper focuses on a new possibility that the\nAV owners have the option of \"renting\" their AVs to a company, which can use\nthese collected AVs to provide on-demand ride services without any drivers. We\ncall such a mobility market with AV renting options the \"AV crowdsourcing\nmarket\". This paper establishes an aggregate equilibrium model with multiple\ntransport modes to analyze the AV crowdsourcing market. The modeling framework\ncan capture the customers' mode choices and AV owners' rental decisions with\nthe presence of traffic congestion. Then, we explore different scenarios that\neither maximize the crowdsourcing platform's profit or maximize social welfare.\nGradient-based optimization algorithms are designed for solving the problems.\nThe results obtained by numerical examples reveal the welfare enhancement and\nthe strong profitability of the AV crowdsourcing service. However, when the\ncrowdsourcing scale is small, the crowdsourcing platform might not be\nprofitable. A second-best pricing scheme is able to avoid such undesirable\ncases. The insights generated from the analyses provide guidance for\nregulators, service providers and citizens to make future decisions regarding\nthe utilization of the AV crowdsourcing markets for serving the good of the\nsociety.\n"
    },
    {
        "paper_id": 2102.07198,
        "authors": "Shailesh Bharati, Rahul Batra",
        "title": "How Misuse of Statistics Can Spread Misinformation: A Study of\n  Misrepresentation of COVID-19 Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates various ways in which a pandemic such as the novel\ncoronavirus, could be predicted using different mathematical models. It also\nstudies the various ways in which these models could be depicted using various\nvisualization techniques. This paper aims to present various statistical\ntechniques suggested by the Centres for Disease Control and Prevention in order\nto represent the epidemiological data. The main focus of this paper is to\nanalyse how epidemiological data or contagious diseases are theorized using any\navailable information and later may be presented wrongly by not following the\nguidelines, leading to inaccurate representation and interpretations of the\ncurrent scenario of the pandemic; with a special reference to the Indian\nSubcontinent.\n"
    },
    {
        "paper_id": 2102.07222,
        "authors": "Andrea Moro, Martin Van der Linden",
        "title": "Exclusion of Extreme Jurors and Minority Representation: The Effect of\n  Jury Selection Procedures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare two jury selection procedures meant to safeguard against the\ninclusion of biased jurors that are perceived as causing minorities to be\nunder-represented. The Strike and Replace procedure presents potential jurors\none-by-one to the parties, while the Struck procedure presents all potential\njurors before the parties exercise their challenges. Struck more effectively\nexcludes extreme jurors but leads to a worse representation of minorities. The\nadvantage of Struck in terms of excluding extremes is sizable in a wide range\nof cases. In contrast, Strike and Replace better represents minorities only if\nthe minority and majority are polarized. Results are robust to assuming the\nparties statistically discriminate against jurors based on group identity.\n"
    },
    {
        "paper_id": 2102.07372,
        "authors": "Wentao Xu, Weiqing Liu, Chang Xu, Jiang Bian, Jian Yin, Tie-Yan Liu",
        "title": "REST: Relational Event-driven Stock Trend Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3442381.3450032",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock trend forecasting, aiming at predicting the stock future trends, is\ncrucial for investors to seek maximized profits from the stock market. Many\nevent-driven methods utilized the events extracted from news, social media, and\ndiscussion board to forecast the stock trend in recent years. However, existing\nevent-driven methods have two main shortcomings: 1) overlooking the influence\nof event information differentiated by the stock-dependent properties; 2)\nneglecting the effect of event information from other related stocks. In this\npaper, we propose a relational event-driven stock trend forecasting (REST)\nframework, which can address the shortcoming of existing methods. To remedy the\nfirst shortcoming, we propose to model the stock context and learn the effect\nof event information on the stocks under different contexts. To address the\nsecond shortcoming, we construct a stock graph and design a new propagation\nlayer to propagate the effect of event information from related stocks. The\nexperimental studies on the real-world data demonstrate the efficiency of our\nREST framework. The results of investment simulation show that our framework\ncan achieve a higher return of investment than baselines.\n"
    },
    {
        "paper_id": 2102.07425,
        "authors": "Tetsuya Takaishi",
        "title": "Time-varying properties of asymmetric volatility and multifractality in\n  Bitcoin",
        "comments": "27 pages, 11 figures",
        "journal-ref": "PLoS ONE 16(2): e0246209 (2021)",
        "doi": "10.1371/journal.pone.0246209",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the volatility of daily Bitcoin returns and\nmultifractal properties of the Bitcoin market by employing the rolling window\nmethod and examines relationships between the volatility asymmetry and market\nefficiency. Whilst we find an inverted asymmetry in the volatility of Bitcoin,\nits magnitude changes over time, and recently, it has become small. This\nasymmetric pattern of volatility also exists in higher frequency returns. Other\nmeasurements, such as kurtosis, skewness, average, serial correlation, and\nmultifractal degree, also change over time. Thus, we argue that properties of\nthe Bitcoin market are mostly time dependent. We examine efficiency-related\nmeasures: the Hurst exponent, multifractal degree, and kurtosis. We find that\nwhen these measures represent that the market is more efficient, the volatility\nasymmetry weakens. For the recent Bitcoin market, both efficiency-related\nmeasures and the volatility asymmetry prove that the market becomes more\nefficient.\n"
    },
    {
        "paper_id": 2102.07476,
        "authors": "Arnaud Dupuy and Alfred Galichon",
        "title": "Personality Traits and the Marriage Market",
        "comments": "72 pages, 5 tables",
        "journal-ref": "Journal of Political Economy 122-6 (2014) pp. 1271-1319",
        "doi": "10.1086/677191",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Which and how many attributes are relevant for the sorting of agents in a\nmatching market? This paper addresses these questions by constructing indices\nof mutual attractiveness that aggregate information about agents' attributes.\nThe first k indices for agents on each side of the market provide the best\napproximation of the matching surplus by a k-dimensional model. The methodology\nis applied on a unique Dutch households survey containing information about\neducation, height, BMI, health, attitude toward risk and personality traits of\nspouses.\n"
    },
    {
        "paper_id": 2102.07489,
        "authors": "Arnaud Dupuy and Alfred Galichon",
        "title": "Canonical Correlation and Assortative Matching: A Remark",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": "10.15609/annaeconstat2009.119-120.375",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the context of the Beckerian theory of marriage, when men and women match\non a single-dimensional index that is the weighted sum of their respective\nmultivariate attributes, many papers in the literature have used linear\ncanonical correlation, and related techniques, in order to estimate these\nweights. We argue that this estimation technique is inconsistent and suggest\nsome solutions.\n"
    },
    {
        "paper_id": 2102.07536,
        "authors": "Margarita Leib, Nils C. K\\\"obis, Rainer Michael Rilke, Marloes Hagens,\n  Bernd Irlenbusch",
        "title": "The corruptive force of AI-generated advice",
        "comments": "Leib & K\\\"obis share first authorship",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial Intelligence (AI) is increasingly becoming a trusted advisor in\npeople's lives. A new concern arises if AI persuades people to break ethical\nrules for profit. Employing a large-scale behavioural experiment (N = 1,572),\nwe test whether AI-generated advice can corrupt people. We further test whether\ntransparency about AI presence, a commonly proposed policy, mitigates potential\nharm of AI-generated advice. Using the Natural Language Processing algorithm,\nGPT-2, we generated honesty-promoting and dishonesty-promoting advice.\nParticipants read one type of advice before engaging in a task in which they\ncould lie for profit. Testing human behaviour in interaction with actual AI\noutputs, we provide first behavioural insights into the role of AI as an\nadvisor. Results reveal that AI-generated advice corrupts people, even when\nthey know the source of the advice. In fact, AI's corrupting force is as strong\nas humans'.\n"
    },
    {
        "paper_id": 2102.07656,
        "authors": "Silvia Angilella, Maria Rosaria Pappalardo",
        "title": "Assessment of a failure prediction model in the energy sector: a\n  multicriteria discrimination approach with Promethee based classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents the implementation of a non-parametric multiple criteria\ndecision aiding (MCDA) model, the Multi-group Hierarchy Discrimination\n(M.H.DIS) model, with the Preference Ranking Organization Method for Enrichment\nEvaluations (PROMETHEE), on a dataset of 114 European unlisted companies\noperating in the energy sector. Firstly, the M.H.DIS model has been developed\nfollowing a five-fold cross validation procedure to analyze whether the model\nexplains and replicates a two-group pre-defined classification of companies in\nthe considered sample, provided by Bureau van Dijk's Amadeus database. Since\nthe M.H.DIS method achieves a quite limited satisfactory accuracy in predicting\nthe considered Amadeus classification in the holdout sample, the PROMETHEE\nmethod has been performed then to provide a benchmark sorting procedure useful\nfor comparison purposes.\n"
    },
    {
        "paper_id": 2102.08086,
        "authors": "Edward J. Oughton and Ashutosh Jha",
        "title": "Supportive 5G Infrastructure Policies are Essential for Universal 6G:\n  Assessment using an Open-source Techno-economic Simulation Model utilizing\n  Remote Sensing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Work has now begun on the sixth generation of cellular technologies (`6G`)\nand cost-efficient global broadband coverage is already becoming a key pillar.\nIndeed, we are still far from providing universal and affordable broadband\nconnectivity, despite this being a key part of the Sustainable Development\nGoals (Target 9.c). Currently, both Mobile Network Operators and governments\nstill lack independent analysis of the strategies that can help achieve this\ntarget with the cellular technologies available (4G and 5G). Therefore, this\npaper undertakes quantitative assessment demonstrating how current 5G policies\naffect universal broadband, as well as drawing conclusions over how decisions\nmade now affect future evolution to 6G. Using a method based on an open-source\ntechno-economic codebase, combining remote sensing with least-cost network\nalgorithms, performance analytics are provided for different 4G and 5G\nuniversal broadband strategies. As an example, the assessment approach is\napplied to India, the world`s second-largest mobile market and a country with\nvery high spectrum prices. The results demonstrate the trade-offs between\ntechnological decisions. This includes demonstrating how important current\ninfrastructure policy is, particularly given fiber backhaul will be essential\nfor delivering 6G quality of service. We find that by eliminating the spectrum\nlicensing costs, 100% 5G population coverage can viably be achieved using fiber\nbackhaul. Therefore, supportive infrastructure policies are essential in\nproviding a superior foundation for evolution to future cellular generation,\nsuch as 6G.\n"
    },
    {
        "paper_id": 2102.08107,
        "authors": "Pavel Ciaian, d'Artis Kancs, Miroslava Rajcaniova",
        "title": "Interdependencies between Mining Costs, Mining Rewards and Blockchain\n  Security",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper studies to what extent the cost of operating a proof-of-work\nblockchain is intrinsically linked to the cost of preventing attacks, and to\nwhat extent the underlying digital ledger security budgets are correlated with\nthe cryptocurrency market outcomes. We theoretically derive an equilibrium\nrelationship between the cryptocurrency price, mining rewards and mining costs,\nand blockchain security outcomes. Using daily crypto market data for 2014-2021\nand employing the autoregressive distributed lag approach - that allows\ntreating all the relevant moments of the blockchain series as potentially\nendogenous - we provide empirical evidence of cryptocurrency price and mining\nrewards indeed being intrinsically linked to blockchain security outcomes.\n"
    },
    {
        "paper_id": 2102.08162,
        "authors": "Kirill Solovev, Nicolas Pr\\\"ollochs",
        "title": "Integrating Floor Plans into Hedonic Models for Rent Price Appraisal",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3442381.3449967",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online real estate platforms have become significant marketplaces\nfacilitating users' search for an apartment or a house. Yet it remains\nchallenging to accurately appraise a property's value. Prior works have\nprimarily studied real estate valuation based on hedonic price models that take\nstructured data into account while accompanying unstructured data is typically\nignored. In this study, we investigate to what extent an automated visual\nanalysis of apartment floor plans on online real estate platforms can enhance\nhedonic rent price appraisal. We propose a tailored two-staged deep learning\napproach to learn price-relevant designs of floor plans from historical price\ndata. Subsequently, we integrate the floor plan predictions into hedonic rent\nprice models that account for both structural and locational characteristics of\nan apartment. Our empirical analysis based on a unique dataset of 9174 real\nestate listings suggests that current hedonic models underutilize the available\ndata. We find that (1) the visual design of floor plans has significant\nexplanatory power regarding rent prices - even after controlling for structural\nand locational apartment characteristics, and (2) harnessing floor plans\nresults in an up to 10.56% lower out-of-sample prediction error. We further\nfind that floor plans yield a particularly high gain in prediction performance\nfor older and smaller apartments. Altogether, our empirical findings contribute\nto the existing research body by establishing the link between the visual\ndesign of floor plans and real estate prices. Moreover, our approach has\nimportant implications for online real estate platforms, which can use our\nfindings to enhance user experience in their real estate listings.\n"
    },
    {
        "paper_id": 2102.08174,
        "authors": "Alberto Bisin and Andrea Moro",
        "title": "LATE for History",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Historical Economics, Persistence studies document the persistence of some\nhistorical phenomenon or leverage this persistence to identify causal\nrelationships of interest in the present. In this chapter, we analyze the\nimplications of allowing for heterogeneous treatment effects in these studies.\nWe delineate their common empirical structure, argue that heterogeneous\ntreatment effects are likely in their context, and propose minimal abstract\nmodels that help interpret results and guide the development of empirical\nstrategies to uncover the mechanisms generating the effects.\n"
    },
    {
        "paper_id": 2102.08186,
        "authors": "A. Christian Silva and Fernando F. Ferreira",
        "title": "Surrogate Monte Carlo",
        "comments": "2 columns, 6 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article proposes an artificial data generating algorithm that is simple\nand easy to customize. The fundamental concept is to perform random permutation\nof Monte Carlo generated random numbers which conform to the unconditional\nprobability distribution of the original real time series. Similar to\nconstraint surrogate methods, random permutations are only accepted if a given\nobjective function is minimized. The objective function is selected in order to\ndescribe the most important features of the stochastic process. The algorithm\nis demonstrated by producing simulated log-returns of the S\\&P 500 stock index.\n"
    },
    {
        "paper_id": 2102.08187,
        "authors": "T. Takaishi",
        "title": "Power-Law Return-Volatility Cross Correlations of Bitcoin",
        "comments": "10 pages, 5 figures",
        "journal-ref": "EPL, 129 (2020) 28001",
        "doi": "10.1209/0295-5075/129/28001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the return-volatility asymmetry of Bitcoin. We find\nthat the cross correlations between return and volatility (squared return) are\nmostly insignificant on a daily level. In the high-frequency region, we find\nthata power-law appears in negative cross correlation between returns and\nfuture volatilities, which suggests that the cross correlation is\n\\revision{long ranged}. We also calculate a cross correlation between returns\nand the power of absolute returns, and we find that the strength of\n\\revision{the cross correlations} depends on the value of the power.\n"
    },
    {
        "paper_id": 2102.08189,
        "authors": "Marco Ortu, Nicola Uras, Claudio Conversano, Giuseppe Destefanis,\n  Silvia Bartolucci",
        "title": "On Technical Trading and Social Media Indicators in Cryptocurrencies'\n  Price Classification Through Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work aims to analyse the predictability of price movements of\ncryptocurrencies on both hourly and daily data observed from January 2017 to\nJanuary 2021, using deep learning algorithms. For our experiments, we used\nthree sets of features: technical, trading and social media indicators,\nconsidering a restricted model of only technical indicators and an unrestricted\nmodel with technical, trading and social media indicators. We verified whether\nthe consideration of trading and social media indicators, along with the\nclassic technical variables (such as price's returns), leads to a significative\nimprovement in the prediction of cryptocurrencies price's changes. We conducted\nthe study on the two highest cryptocurrencies in volume and value (at the time\nof the study): Bitcoin and Ethereum. We implemented four different machine\nlearning algorithms typically used in time-series classification problems:\nMulti Layers Perceptron (MLP), Convolutional Neural Network (CNN), Long Short\nTerm Memory (LSTM) neural network and Attention Long Short Term Memory (ALSTM).\nWe devised the experiments using the advanced bootstrap technique to consider\nthe variance problem on test samples, which allowed us to evaluate a more\nreliable estimate of the model's performance. Furthermore, the Grid Search\ntechnique was used to find the best hyperparameters values for each implemented\nalgorithm. The study shows that, based on the hourly frequency results, the\nunrestricted model outperforms the restricted one. The addition of the trading\nindicators to the classic technical indicators improves the accuracy of Bitcoin\nand Ethereum price's changes prediction, with an increase of accuracy from a\nrange of 51-55% for the restricted model, to 67-84% for the unrestricted model.\n"
    },
    {
        "paper_id": 2102.08262,
        "authors": "Alisya Putri Rabbani, Andry Alamsyah, Sri Widiyanesti",
        "title": "An Effort to Measure Customer Relationship Performance in Indonesia's\n  Fintech Industry",
        "comments": "5 pages, 2 figures, 5 tables",
        "journal-ref": "The 11th SCBTII 2020 : Sustainable Collaboration in Business,\n  Technology, Information and Innovation presents Virtual International\n  Conference",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The availability of social media simplifies the companies-customers\nrelationship. An effort to engage customers in conversation networks using\nsocial media is called Social Customer Relationship Management (SCRM). Social\nNetwork Analysis helps to understand network characteristics and how active the\nconversation network on social media. Calculating its network properties is\nbeneficial for measuring customer relationship performance. Financial\nTechnology, a new emerging industry that provides digital-based financial\nservices utilize social media to interact with its customers. Measuring SCRM\nperformance is needed in order to stay competitive among others. Therefore, we\naim to explore the SCRM performance of the Indonesia Fintech company. In terms\nof discovering the market majority thought in conversation networks, we perform\nsentiment analysis by classifying into positive and negative opinion. As case\nstudies, we investigate Twitter conversations about GoPay, OVO, Dana, and\nLinkAja during the observation period from 1st October until 1st November 2019.\nThe result of this research is beneficial for business intelligence purposes\nespecially in managing relationships with customers.\n"
    },
    {
        "paper_id": 2102.08338,
        "authors": "A. Itkin, A. Lipton, D. Muravey",
        "title": "Multilayer heat equations: application to finance",
        "comments": "36 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we develop a Multilayer (ML) method for solving one-factor\nparabolic equations. Our approach provides a powerful alternative to the\nwell-known finite difference and Monte Carlo methods. We discuss various\nadvantages of this approach, which judiciously combines semi-analytical and\nnumerical techniques and provides a fast and accurate way of finding solutions\nto the corresponding equations. To introduce the core of the method, we\nconsider multilayer heat equations, known in physics for a relatively long time\nbut never used when solving financial problems. Thus, we expand the analytic\nmachinery of quantitative finance by augmenting it with the ML method. We\ndemonstrate how one can solve various problems of mathematical finance by using\nour approach. Specifically, we develop efficient algorithms for pricing barrier\noptions for time-dependent one-factor short-rate models, such as\nBlack-Karasinski and Verhulst. Besides, we show how to solve the well-known\nDupire equation quickly and accurately. Numerical examples confirm that our\napproach is considerably more efficient for solving the corresponding partial\ndifferential equations than the conventional finite difference method by being\nmuch faster and more accurate than the known alternatives.\n"
    },
    {
        "paper_id": 2102.08378,
        "authors": "Pavel Ciaian, d'Artis Kancs, Miroslava Rajcaniova",
        "title": "The economic dependency of the Bitcoin security",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2102.08107",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study to what extent the Bitcoin blockchain security permanently depends\non the underlying distribution of cryptocurrency market outcomes. We use daily\nblockchain and Bitcoin data for 2014-2019 and employ the ARDL approach. We test\nthree equilibrium hypotheses: (i) sensitivity of the Bitcoin blockchain to\nmining reward; (ii) security outcomes of the Bitcoin blockchain and the\nproof-of-work cost; and (iii) the speed of adjustment of the Bitcoin blockchain\nsecurity to deviations from the equilibrium path. Our results suggest that the\nBitcoin price and mining rewards are intrinsically linked to Bitcoin security\noutcomes. The Bitcoin blockchain security's dependency on mining costs is\ngeographically differenced - it is more significant for the global mining\nleader China than for other world regions. After input or output price shocks,\nthe Bitcoin blockchain security reverts to its equilibrium security level.\n"
    },
    {
        "paper_id": 2102.08811,
        "authors": "Zihao Zhang, Bryan Lim and Stefan Zohren",
        "title": "Deep Learning for Market by Order Data",
        "comments": "17 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market by order (MBO) data - a detailed feed of individual trade instructions\nfor a given stock on an exchange - is arguably one of the most granular sources\nof microstructure information. While limit order books (LOBs) are implicitly\nderived from it, MBO data is largely neglected by current academic literature\nwhich focuses primarily on LOB modelling. In this paper, we demonstrate the\nutility of MBO data for forecasting high-frequency price movements, providing\nan orthogonal source of information to LOB snapshots and expanding the universe\nof alpha discovery. We provide the first predictive analysis on MBO data by\ncarefully introducing the data structure and presenting a specific\nnormalisation scheme to consider level information in order books and to allow\nmodel training with multiple instruments. Through forecasting experiments using\ndeep neural networks, we show that while MBO-driven and LOB-driven models\nindividually provide similar performance, ensembles of the two can lead to\nimprovements in forecasting accuracy - indicating that MBO data is additive to\nLOB-based features.\n"
    },
    {
        "paper_id": 2102.09102,
        "authors": "Farid Naufal Aslam, Andry Alamsyah",
        "title": "The Small World Phenomenon and Network Analysis of ICT Startup\n  Investment in Indonesia and Singapore",
        "comments": "7 pages, 4 figures, 2 tables",
        "journal-ref": "The 7th Smart Collaboration for Business in Technology and\n  Information Industry Conference 2016",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The internet's rapid growth stimulates the emergence of start-up companies\nbased on information technology and telecommunication (ICT) in Indonesia and\nSingapore. As the number of start-ups and its investor growth, the network of\nits relationship become larger and complex, but on the other side feel small.\nEveryone in the ICT start-up investment network can be reached in short steps,\nled to a phenomenon called small-world phenomenon, a principle that we are all\nconnected by a short chain of relationships. We investigate the pattern of the\nrelationship between a start-up with its investor and the small world\ncharacteristics using network analysis methodology. The research is conducted\nby creating the ICT start-up investment network model of each country and\ncalculate its small-world network properties to see the characteristic of the\nnetworks. Then we compare and analyze the result of each network model. The\nresult of this research is to give knowledge about the current condition of ICT\nstart-up investment in Indonesia and Singapore. The research is beneficial for\nbusiness intelligence purposes to support decision-making related to ICT\nstart-up investment.\n"
    },
    {
        "paper_id": 2102.09107,
        "authors": "Andry Alamsyah, Nurlisa Laksmiani, Lies Anisa Rahimi",
        "title": "A Core of E-Commerce Customer Experience based on Conversational Data\n  using Network Text Methodology",
        "comments": "9 pages, 1 figure, 4 tables",
        "journal-ref": "International Journal of Business, 2018, 23(3)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  E-commerce provides an efficient and effective way to exchange goods between\nsellers and customers. E-commerce has been a popular method for doing business,\nbecause of its simplicity of having commerce activity transparently available,\nincluding customer voice and opinion about their own experience. Those\nexperiences can be a great benefit to understand customer experience\ncomprehensively, both for sellers and future customers. This paper applies to\ne-commerces and customers in Indonesia. Many Indonesian customers expressed\ntheir voice to open social network services such as Twitter and Facebook, where\na large proportion of data is in the form of conversational data. By\nunderstanding customer behavior through open social network service, we can\nhave descriptions about the e-commerce services level in Indonesia. Thus, it is\nrelated to the government's effort to improve the Indonesian digital economy\necosystem. A method for finding core topics in large-scale internet\nunstructured text data is needed, where the method should be fast but\nsufficiently accurate. Processing large-scale data is not a straightforward\njob, it often needs special skills of people and complex software and hardware\ncomputer system. We propose a fast methodology of text mining methods based on\nfrequently appeared words and their word association to form network text\nmethodology. This method is adapted from Social Network Analysis by the model\nrelationships between words instead of actors.\n"
    },
    {
        "paper_id": 2102.09122,
        "authors": "Sahar Zandi",
        "title": "Sustainable and Resilient Systems for Intergenerational Justice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Rawls' theory of justice aims at fairness. He does not only think of justice\nbetween exiting parties in existing society, but he also thinks of it between\ngenerations intergenerational justice problem. Rawls' solution to this problem\nis the saving principle, and he says that we are responsible for being just\nwith the next generations. Wolf thinks of our responsibility for future\ngenerations as a kind of financial debt that we have to pay. He also develops\nthe meaning of \"saving\" and says that it is not restricted to the monetary one.\nWolf extends the definition of \"saving\" such that it includes investment on\nbehalf of the next generations as well. In this paper, I want to extend the\nmeaning of \"saving\" to \"using the resources for sustainable and resilient\nsystems.\" By referring to the problem of time, I show that our decision on\nbehalf of the next generations will be rational only if we entirely use the\nnatural resources and wealth to produce sustainable and resilient systems.\n"
    },
    {
        "paper_id": 2102.09139,
        "authors": "Bingyan Han",
        "title": "Understanding algorithmic collusion with experience replay",
        "comments": "References updated. Comments are welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In an infinitely repeated pricing game, pricing algorithms based on\nartificial intelligence (Q-learning) may consistently learn to charge\nsupra-competitive prices even without communication. Although concerns on\nalgorithmic collusion have arisen, little is known on underlying factors. In\nthis work, we experimentally analyze the dynamics of algorithms with three\nvariants of experience replay. Algorithmic collusion still has roots in human\npreferences. Randomizing experience yields prices close to the static Bertrand\nequilibrium and higher prices are easily restored by favoring the latest\nexperience. Moreover, relative performance concerns also stabilize the\ncollusion. Finally, we investigate the scenarios with heterogeneous agents and\ntest robustness on various factors.\n"
    },
    {
        "paper_id": 2102.0918,
        "authors": "Benjamin Patrick Evans, Mikhail Prokopenko",
        "title": "A maximum entropy model of bounded rational decision-making with prior\n  beliefs and market feedback",
        "comments": "39 pages, 15 figures",
        "journal-ref": null,
        "doi": "10.3390/e23060669",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bounded rationality is an important consideration stemming from the fact that\nagents often have limits on their processing abilities, making the assumption\nof perfect rationality inapplicable to many real tasks. We propose an\ninformation-theoretic approach to the inference of agent decisions under\nSmithian competition. The model explicitly captures the boundedness of agents\n(limited in their information-processing capacity) as the cost of information\nacquisition for expanding their prior beliefs. The expansion is measured as the\nKullblack-Leibler divergence between posterior decisions and prior beliefs.\nWhen information acquisition is free, the homo economicus agent is recovered,\nwhile in cases when information acquisition becomes costly, agents instead\nrevert to their prior beliefs. The maximum entropy principle is used to infer\nleast-biased decisions based upon the notion of Smithian competition formalised\nwithin the Quantal Response Statistical Equilibrium framework. The\nincorporation of prior beliefs into such a framework allowed us to\nsystematically explore the effects of prior beliefs on decision-making in the\npresence of market feedback, as well as importantly adding a temporal\ninterpretation to the framework. We verified the proposed model using\nAustralian housing market data, showing how the incorporation of prior\nknowledge alters the resulting agent decisions. Specifically, it allowed for\nthe separation of past beliefs and utility maximisation behaviour of the agent\nas well as the analysis into the evolution of agent beliefs.\n"
    },
    {
        "paper_id": 2102.09207,
        "authors": "Anthony Strittmatter, Conny Wunsch",
        "title": "The Gender Pay Gap Revisited with Big Data: Do Methodological Choices\n  Matter?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The vast majority of existing studies that estimate the average unexplained\ngender pay gap use unnecessarily restrictive linear versions of the\nBlinder-Oaxaca decomposition. Using a notably rich and large data set of 1.7\nmillion employees in Switzerland, we investigate how the methodological\nimprovements made possible by such big data affect estimates of the unexplained\ngender pay gap. We study the sensitivity of the estimates with regard to i) the\navailability of observationally comparable men and women, ii) model flexibility\nwhen controlling for wage determinants, and iii) the choice of different\nparametric and semi-parametric estimators, including variants that make use of\nmachine learning methods. We find that these three factors matter greatly.\nBlinder-Oaxaca estimates of the unexplained gender pay gap decline by up to 39%\nwhen we enforce comparability between men and women and use a more flexible\nspecification of the wage equation. Semi-parametric matching yields estimates\nthat when compared with the Blinder-Oaxaca estimates, are up to 50% smaller and\nalso less sensitive to the way wage determinants are included.\n"
    },
    {
        "paper_id": 2102.09218,
        "authors": "Rania B\\'eji (UM, MRM), Ouidad Yousfi (UM, MRM), Abdelwahed Omri",
        "title": "Corporate Social Responsibility and Corporate Governance: A cognitive\n  approach",
        "comments": "World Scientific Publisher. Financial and Economic Systems:\n  Transformations & New Challenges, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This chapter aims to critically review the existing literature on the\nrelationship between corporate social responsibility (CSR) and corporate\ngovernance features. Drawn on management and corporate governance theories, we\ndevelop a theoretical model that makes explicit the links between board\ndiversity, CSR committees' attributes, CSR and financial performance.\nParticularly, we show that focusing on the cognitive and demographic\ncharacteristics of board members could provide more insights on the link\nbetween corporate governance and CSR. We also highlight how the functioning and\nthe composition of CSR committees, could be valuable to better understand the\nrelationship between corporate governance and CSR.\n"
    },
    {
        "paper_id": 2102.09287,
        "authors": "Andrew Butler and Roy H. Kwon",
        "title": "Integrating prediction in mean-variance portfolio optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction models are traditionally optimized independently from their use in\nthe asset allocation decision-making process. We address this shortcoming and\npresent a framework for integrating regression prediction models in a\nmean-variance optimization (MVO) setting. Closed-form analytical solutions are\nprovided for the unconstrained and equality constrained MVO case. For the\ngeneral inequality constrained case, we make use of recent advances in\nneural-network architecture for efficient optimization of batch\nquadratic-programs. To our knowledge, this is the first rigorous study of\nintegrating prediction in a mean-variance portfolio optimization setting. We\npresent several historical simulations using both synthetic and global futures\ndata to demonstrate the benefits of the integrated approach.\n"
    },
    {
        "paper_id": 2102.09608,
        "authors": "Anton Pichler, Marco Pangallo, R. Maria del Rio-Chanona, Fran\\c{c}ois\n  Lafond, J. Doyne Farmer",
        "title": "In and out of lockdown: Propagation of supply and demand shocks in a\n  dynamic input-output model",
        "comments": "54 pages, 17 figures, 8 tables. arXiv admin note: text overlap with\n  arXiv:2005.10585",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economic shocks due to Covid-19 were exceptional in their severity,\nsuddenness and heterogeneity across industries. To study the upstream and\ndownstream propagation of these industry-specific demand and supply shocks, we\nbuild a dynamic input-output model inspired by previous work on the economic\nresponse to natural disasters. We argue that standard production functions, at\nleast in their most parsimonious parametrizations, are not adequate to model\ninput substitutability in the context of Covid-19 shocks. We use a survey of\nindustry analysts to evaluate, for each industry, which inputs were absolutely\nnecessary for production over a short time period. We calibrate our model on\nthe UK economy and study the economic effects of the lockdown that was imposed\nat the end of March and gradually released in May. Looking back at predictions\nthat we released in May, we show that the model predicted aggregate dynamics\nvery well, and sectoral dynamics to a large extent. We discuss the relative\nextent to which the model's dynamics and performance was due to the choice of\nthe production function or the choice of an exogenous shock scenario. To\nfurther explore the behavior of the model, we use simpler scenarios with only\ndemand or supply shocks, and find that popular metrics used to predict a priori\nthe impact of shocks, such as output multipliers, are only mildly useful.\n"
    },
    {
        "paper_id": 2102.09625,
        "authors": "Giuseppe Francesco Gori, Patrizia Lattarulo, Marco Mariani",
        "title": "The Expediting Effect of Monitoring on Infrastructural Works. A\n  Regression-Discontinuity Approach with Multiple Assignment Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decentralised government levels are often entrusted with the management of\npublic works and required to ensure well-timed infrastructure delivery to their\ncommunities. We investigate whether monitoring the activity of local procuring\nauthorities during the execution phase of the works they manage may expedite\nthe infrastructure delivery process. Focussing on an Italian regional law which\nimposes monitoring by the regional government on \"strategic\" works carried out\nby local buyers, we draw causal claims using a regression-discontinuity\napproach, made unusual by the presence of multiple assignment variables.\nEstimation is performed through discrete-time survival analysis techniques.\nResults show that monitoring does expedite infrastructure delivery.\n"
    },
    {
        "paper_id": 2102.09851,
        "authors": "William Lefebvre (LPSM (UMR\\_8001)), Enzo Miller (LPSM (UMR\\_8001))",
        "title": "Linear-quadratic stochastic delayed control and deep learning resolution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of stochastic control problems with a delayed control,\nboth in drift and diffusion, of the type dX t = $\\alpha$ t--d (bdt + $\\sigma$dW\nt). We provide a new characterization of the solution in terms of a set of\nRiccati partial differential equations. Existence and uniqueness are obtained\nunder a sufficient condition expressed directly as a relation between the\nhorizon T and the quantity d(b/$\\sigma$) 2. Furthermore, a deep learning scheme\nis designed and used to illustrate the effect of delay on the Markowitz\nportfolio allocation problem with execution delay.\n"
    },
    {
        "paper_id": 2102.09974,
        "authors": "Luisa Roa, Andr\\'es Rodr\\'iguez-Rey, Alejandro Correa-Bahnsen, Carlos\n  Valencia",
        "title": "Supporting Financial Inclusion with Graph Machine Learning and Super-App\n  Alternative Data",
        "comments": "Accepted to be appeared in IntelliSys2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The presence of Super-Apps have changed the way we think about the\ninteractions between users and commerce. It then comes as no surprise that it\nis also redefining the way banking is done. The paper investigates how\ndifferent interactions between users within a Super-App provide a new source of\ninformation to predict borrower behavior. To this end, two experiments with\ndifferent graph-based methodologies are proposed, the first uses graph based\nfeatures as input in a classification model and the second uses graph neural\nnetworks. Our results show that variables of centrality, behavior of\nneighboring users and transactionality of a user constituted new forms of\nknowledge that enhance statistical and financial performance of credit risk\nmodels. Furthermore, opportunities are identified for Super-Apps to redefine\nthe definition of credit risk by contemplating all the environment that their\nplatforms entail, leading to a more inclusive financial system.\n"
    },
    {
        "paper_id": 2102.10047,
        "authors": "Emmanuel Coffie, Sindre Duedahl and Frank Proske",
        "title": "Thiele's Differential Equation Based on Markov Jump Processes with\n  Non-countable State Space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In modern life insurance, Markov processes in continuous time on a finite or\nat least countable state space have been over the years an important tool for\nthe modelling of the states of an insured. Motivated by applications in\ndisability insurance, we propose in this paper a model for insurance states\nbased on Markov jump processes with more general state spaces. We use this\nmodel to derive a new type of Thiele's differential equation which e.g. allows\nfor a consistent calculation of reserves in disability insurance based on\ntwo-parameter continuous time rehabilitation rates.\n"
    },
    {
        "paper_id": 2102.10096,
        "authors": "Johannes Rude Jensen, Victor von Wachter, Omri Ross",
        "title": "How Decentralized is the Governance of Blockchain-based Finance:\n  Empirical Evidence from four Governance Token Distributions",
        "comments": "Distributed Ledger Technology, Blockchain, Decentralized Finance\n  (DeFi), Governance, Governance Token. Research in progress",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Novel blockchain technology provides the infrastructure layer for the\ncreation of decentralized appli-cations. A rapidly growing ecosystem of\napplications is built around financial services, commonly referred to as\ndecentralized finance. Whereas the intangible concept of decentralization is\npresented as a key driver for the applications, defining and measuring\ndecentralization is multifaceted. This pa-per provides a framework to quantify\ndecentralization of governance power among blockchain appli-cations. Governance\nof the applications is increasingly important and requires striking a balance\nbe-tween broad distribution, fostering user activity, and financial incentives.\nTherefore, we aggregate, parse, and analyze empirical data of four finance\napplications calculating coefficients for the statistical dispersion of the\ngovernance token distribution. The gauges potentially support IS scholars for\nan objective evaluation of the capabilities and limitations of token governance\nand for fast iteration in design-driven governance mechanisms.\n"
    },
    {
        "paper_id": 2102.10098,
        "authors": "Hans Ole Riddervold, Ellen Krohn Aasg{\\aa}rd, Lisa Haukaas, Magnus\n  Korp{\\aa}s",
        "title": "Internal hydro- and wind portfolio optimisation in real-time market\n  operations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper aspects related to handling of intraday imbalances for hydro\nand wind power are addressed. The definition of imbalance cost is established\nand used to describe the potential benefits of shifting from plant-specific\nschedules to a common load requirement for wind and hydropower units in the\nsame price area. The Nordpool intraday pay-as-bid market has been the basis for\nevaluation of imbalances, and some main characteristics for this market has\nbeen described. We consider how internal handling of complementary imbalances\nwithin the same river system with high inflow uncertainty and constrained\nreservoirs can reduce volatility in short-term marginal cost and risk compared\nto trading in the intraday market. We have also shown the the imbalance cost\nfor a power producer with both wind and hydropower assets can be reduced by\ninternal balancing in combination with sales and purchase in a pay-as-bid\nintraday market\n"
    },
    {
        "paper_id": 2102.10099,
        "authors": "Yusuke Ikeno, James Angel, Shin'ichiro Matsuo and Ryosuke Ushida",
        "title": "Auction Type Resolution on Smart Derivatives",
        "comments": "13 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes an auction type resolution for smart derivatives. It has\nbeen discussed to migrate derivatives contracts to smart contracts (smart\nderivatives). Automation is often discussed in this context. It is also\nimportant to prepare to avoid disputes from practical perspectives. There are\ncontroversial issues to terminate the relationship at defaults. In OTC\nderivative markets, master agreements define a basic policy for the liquidation\nprocess but there happened some disputes over these processes. We propose to\ndefine an auction type resolution in smart derivatives, which each participant\nwould find beneficial.\n"
    },
    {
        "paper_id": 2102.10145,
        "authors": "Alberto Bisin and Andrea Moro",
        "title": "Learning Epidemiology by Doing: The Empirical Implications of a\n  Spatial-SIR Model with Behavioral Responses",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jue.2021.103368",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We simulate a spatial behavioral model of the diffusion of an infection to\nunderstand the role of geographic characteristics: the number and distribution\nof outbreaks, population size, density, and agents' movements. We show that\nseveral invariance properties of the SIR model concerning these variables do\nnot hold when agents interact with neighbors in a (two dimensional)\ngeographical space. Indeed, the spatial model's local interactions generate\nmatching frictions and local herd immunity effects, which play a fundamental\nrole in the infection dynamics. We also show that geographical factors affect\nhow behavioral responses affect the epidemics. We derive relevant implications\nfor estimating the effects of the epidemics and policy interventions that use\npanel data from several geographical units.\n"
    },
    {
        "paper_id": 2102.10211,
        "authors": "Thomas V. Conti",
        "title": "M\\'etodos Emp\\'iricos Aplicados \\`a An\\'alise Econ\\^omica do Direito",
        "comments": "Book chapter manuscript, in Portuguese",
        "journal-ref": "Em: YEUNG, Luciana L. (Org.). An\\'alise Econ\\^omica do Direito:\n  Temas Contempor\\^aneos. S\\~ao Paulo: Actual, 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Nas \\'ultimas d\\'ecadas houve forte mudan\\c{c}a no perfil das\npublica\\c{c}\\~oes em an\\'alise econ\\^omica do direito e nos m\\'etodos\nemp\\'iricos mais utilizados. Por\\'em, nos pr\\'oximos anos a mudan\\c{c}a pode\nser maior e mais r\\'apida, exigindo adapta\\c{c}\\~ao dos pesquisadores da\n\\'area. Neste cap\\'itulo analiso algumas tend\\^encias recentes de mudan\\c{c}a e\noportunidades futuras que se avizinham a partir avan\\c{c}os nas bases de dados,\nestat\\'istica, computa\\c{c}\\~ao e no arcabou\\c{c}o regulat\\'orio dos pa\\'ises.\nAvan\\c{c}o a hip\\'otese de que expans\\~ao de objetos e m\\'etodos favorecer\\'a\nequipes de pesquisa maiores e interdisciplinares e apresento evid\\^encias\ncircunstanciais a partir de dados bibliom\\'etricos de que isso j\\'a vem\nacontecendo no Journal of Law and Economics.\n"
    },
    {
        "paper_id": 2102.10213,
        "authors": "Ju Hong Kim",
        "title": "The relations of Choquet Integral and G-Expectation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In incomplete financial markets, there exists a set of equivalent martingale\nmeasures (or risk-neutral probabilities) in an arbitrage-free pricing of the\ncontingent claims. Minimax expectation is closely related to the\n$g$-expectation which is the solution of a certain stochastic differential\nequation. We show that Choquet expectation and minimax expectation are equal in\npricing European type options, whose payoff is a monotone function of the\nterminal stock price $S_T$.\n"
    },
    {
        "paper_id": 2102.10453,
        "authors": "Victor Chernozhukov, Hiroyuki Kasahara, Paul Schrimpf",
        "title": "The Association of Opening K-12 Schools with the Spread of COVID-19 in\n  the United States: County-Level Panel Data Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.2103420118",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper empirically examines how the opening of K-12 schools and colleges\nis associated with the spread of COVID-19 using county-level panel data in the\nUnited States. Using data on foot traffic and K-12 school opening plans, we\nanalyze how an increase in visits to schools and opening schools with different\nteaching methods (in-person, hybrid, and remote) is related to the 2-weeks\nforward growth rate of confirmed COVID-19 cases. Our debiased panel data\nregression analysis with a set of county dummies, interactions of state and\nweek dummies, and other controls shows that an increase in visits to both K-12\nschools and colleges is associated with a subsequent increase in case growth\nrates. The estimates indicate that fully opening K-12 schools with in-person\nlearning is associated with a 5 (SE = 2) percentage points increase in the\ngrowth rate of cases. We also find that the positive association of K-12 school\nvisits or in-person school openings with case growth is stronger for counties\nthat do not require staff to wear masks at schools. These results have a causal\ninterpretation in a structural model with unobserved county and time\nconfounders. Sensitivity analysis shows that the baseline results are robust to\ntiming assumptions and alternative specifications.\n"
    },
    {
        "paper_id": 2102.10691,
        "authors": "Chris Kenyon and Mourad Berrahoui",
        "title": "Climate Change Valuation Adjustment (CCVA) using parameterized climate\n  change impacts",
        "comments": "16 pages, 4 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce Climate Change Valuation Adjustment (CCVA) to capture climate\nchange impacts on CVA+FVA that are currently invisible assuming typical market\npractice. To discuss such impacts on CVA+FVA from changes to instantaneous\nhazard rates we introduce a flexible and expressive parameterization to capture\nthe path of this impact to climate change endpoints, and transient transition\neffects. Finally we provide quantification of examples of typical interest\nwhere there is risk of economic stress from sea level change up to 2101, and\nfrom transformations of business models. We find that even with the slowest\npossible uniform approach to a climate change impact in 2101 there can still be\nsignificant CVA+FVA impacts on interest rate swaps of 20 years or more\nmaturity. Transformation effects on CVA+FVA are strongly dependent on timing\nand duration of business model transformation. Using a parameterized approach\nenables discussion with stakeholders of economic impacts on CVA+FVA, whatever\nthe details behind the climate impact.\n"
    },
    {
        "paper_id": 2102.10756,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "Equilibrium Price Formation with a Major Player and its Mean Field Limit",
        "comments": "revised. forthcoming in ESAIM: Control, Optimization and Calculus of\n  Variations",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, we consider the problem of equilibrium price formation in an\nincomplete securities market consisting of one major financial firm and a large\nnumber of minor firms. They carry out continuous trading via the securities\nexchange to minimize their cost while facing idiosyncratic and common noises as\nwell as stochastic order flows from their individual clients. The equilibrium\nprice process that balances demand and supply of the securities, including the\nfunctional form of the price impact for the major firm, is derived endogenously\nboth in the market of finite population size and in the corresponding mean\nfield limit.\n"
    },
    {
        "paper_id": 2102.10909,
        "authors": "Semyon Malamud, Anna Cieslak, and Andreas Schrimpf",
        "title": "Optimal Transport of Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the general problem of Bayesian persuasion (optimal information\ndesign) with continuous actions and continuous state space in arbitrary\ndimensions. First, we show that with a finite signal space, the optimal\ninformation design is always given by a partition. Second, we take the limit of\nan infinite signal space and characterize the solution in terms of a\nMonge-Kantorovich optimal transport problem with an endogenous information\ntransport cost. We use our novel approach to: 1. Derive necessary and\nsufficient conditions for optimality based on Bregman divergences for\nnon-convex functions. 2. Compute exact bounds for the Hausdorff dimension of\nthe support of an optimal policy. 3. Derive a non-linear, second-order partial\ndifferential equation whose solutions correspond to regular optimal policies.\nWe illustrate the power of our approach by providing explicit solutions to\nseveral non-linear, multidimensional Bayesian persuasion problems.\n"
    },
    {
        "paper_id": 2102.10925,
        "authors": "Ivan Jericevich and Dharmesh Sing and Tim Gebbie",
        "title": "CoinTossX: An open-source low-latency high-throughput matching engine",
        "comments": "21 pages, 10 figures, 5 tables",
        "journal-ref": "SoftwareX Volume 19, July 2022, 101136",
        "doi": "10.1016/j.softx.2022.101136",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We deploy and demonstrate the CoinTossX low-latency, high-throughput,\nopen-source matching engine with orders sent using the Julia and Python\nlanguages. We show how this can be deployed for small-scale local desk-top\ntesting and discuss a larger scale, but local hosting, with multiple traded\ninstruments managed concurrently and managed by multiple clients. We then\ndemonstrate a cloud based deployment using Microsoft Azure, with large-scale\nindustrial and simulation research use cases in mind. The system is exposed and\ninteracted with via sockets using UDP SBE message protocols and can be\nmonitored using a simple web browser interface using HTTP. We give examples\nshowing how orders can be be sent to the system and market data feeds monitored\nusing the Julia and Python languages. The system is developed in Java with\norders submitted as binary encodings (SBE) via UDP protocols using the Aeron\nMedia Driver as the low-latency, high throughput message transport. The system\nseparates the order-generation and simulation environments e.g. agent-based\nmodel simulation, from the matching of orders, data-feeds and various\nmodularised components of the order-book system. This ensures a more natural\nand realistic asynchronicity between events generating orders, and the events\nassociated with order-book dynamics and market data-feeds. We promote the use\nof Julia as the preferred order submission and simulation environment.\n"
    },
    {
        "paper_id": 2102.10999,
        "authors": "Valentyn Khokhlov",
        "title": "Conditional Value at Risk and Partial Moments for the Metalog\n  Distributions",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The metalog distributions represent a convenient way to approach many\npractical applications. Their distinctive feature is simple closed-form\nexpressions for quantile functions. This paper contributes to further\ndevelopment of the metalog distributions by deriving the closed-form\nexpressions for the Conditional Value at Risk, a risk measure that is closely\nrelated to the tail conditional expectations. It also addressed the derivation\nof the first-order partial moments and shows that they are convex with respect\nto the vector of the metalog distribution parameters.\n"
    },
    {
        "paper_id": 2102.11555,
        "authors": "Salvatore Federico, Giorgio Ferrari, Neofytos Rodosthenous",
        "title": "Two-sided Singular Control of an Inventory with Unknown Demand Trend\n  (Extended Version)",
        "comments": "29 pages; added and revised proofs, updated introduction",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of optimally managing an inventory with unknown demand\ntrend. Our formulation leads to a stochastic control problem under partial\nobservation, in which a Brownian motion with non-observable drift can be\nsingularly controlled in both an upward and downward direction. We first derive\nthe equivalent separated problem under full information, with state-space\ncomponents given by the Brownian motion and the filtering estimate of its\nunknown drift, and we then completely solve this latter problem. Our approach\nuses the transition amongst three different but equivalent problem\nformulations, links between two-dimensional bounded-variation stochastic\ncontrol problems and games of optimal stopping, and probabilistic methods in\ncombination with refined viscosity theory arguments. We show substantial\nregularity of (a transformed version of) the value function, we construct an\noptimal control rule, and we show that the free boundaries delineating\n(transformed) action and inaction regions are bounded globally Lipschitz\ncontinuous functions. To our knowledge this is the first time that such a\nproblem has been solved in the literature.\n"
    },
    {
        "paper_id": 2102.11714,
        "authors": "Jamaal Ahmad",
        "title": "Multivariate higher order moments in multi-state life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that combining life annuities and death benefits introduce\nopposite effects in payments with respect to the mortality risk on the lifetime\nof the insured. In a general multi-state framework with multiple product types,\nsuch joint effects are less trivial. In this paper, we consider a multivariate\npayment process in multi-state life insurance, where the components are defined\nin terms of the same Markovian state process. The multivariate present value of\nfuture payments is introduced, and we derive differential equations and product\nintegral representations of its conditional moments and moment generating\nfunction. Special attention is given to pair-wise covariances between two\npresent values, where results closely connected to Hattendorff type of results\nfor the variance are derived. The results are illustrated in a numerical\nexample in a disability model.\n"
    },
    {
        "paper_id": 2102.11729,
        "authors": "Emmanuel H.Yindi, Immaculate Maumoh and Prisillah L. Mahavile",
        "title": "Exploring the role of Awareness, Government Policy, and Infrastructure\n  in adapting B2C E-Commerce to East African Countries",
        "comments": "14 pages, 2 figure, 4 table, and 1 Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  -It has considered almost 30 years since the emergence of e-commerce, but it\nis still a global phenomenon to this day. E-commerce is replacing the\ntraditional way of doing business. Yet, expectations of sustainable development\nhave been unmet. There are still significant differences between online and\noffline shopping. Although many academic studies have conducted on the adoption\nof various forms of ecommerce, there are little research topics on East African\ncountries, The adoption of B2C e-commerce in East African countries has faced\nmany challenges that have been unaddressed because of the complex nature of\ne-commerce in these nations. This study examines the adaptation of B2C in East\nAfrica using the theory of diffusion of innovation. Data collected from 279\nparticipants in Tanzania were used to test the research model. The results show\nthat awareness, infrastructure innovation and social media play a significant\nrole in the adoption of e-commerce. Lack of good e-commerce policy and\nawareness discourages the adoption of B2C. We also examine how time influences\nthe adaptation of B2C e-commerce to the majority. So, unlike previous adoption\nstudies, which have tended to focus on technology, organizational, and\nenvironmental factors, this study guides the government on how to use social\nmedia to promote B2C e-commerce.\n"
    },
    {
        "paper_id": 2102.11807,
        "authors": "Toshiko Matsui and Daniel Perez",
        "title": "Data-driven analysis of central bank digital currency (CBDC) projects\n  drivers",
        "comments": "14 pages (8 pages, excluding references and annex), 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we use a variety of machine learning methods to quantify the\nextent to which economic and technological factors are predictive of the\nprogression of Central Bank Digital Currencies (CBDC) within a country, using\nas our measure of this progression the CBDC project index (CBDCPI). We find\nthat a financial development index is the most important feature for our model,\nfollowed by the GDP per capita and an index of the voice and accountability of\nthe country's population. Our results are consistent with previous qualitative\nresearch which finds that countries with a high degree of financial development\nor digital infrastructure have more developed CBDC projects. Further, we obtain\nrobust results when predicting the CBDCPI at different points in time.\n"
    },
    {
        "paper_id": 2102.11929,
        "authors": "Bernardo Alves Furtado",
        "title": "PolicySpace2: modeling markets and endogenous public policies",
        "comments": "42 pages, 7 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Policymakers decide on alternative policies facing restricted budgets and\nuncertain, ever-changing future. Designing public policies is further difficult\ndue to the need to decide on priorities and handle effects across policies.\nHousing policies, specifically, involve heterogeneous characteristics of\nproperties themselves and the intricacy of housing markets and the spatial\ncontext of cities. We propose PolicySpace2 (PS2) as an adapted and extended\nversion of the open source PolicySpace agent-based model. PS2 is a computer\nsimulation that relies on empirically detailed spatial data to model real\nestate, along with labor, credit, and goods and services markets. Interaction\namong workers, firms, a bank, households and municipalities follow the\nliterature benchmarks to integrate economic, spatial and transport scholarship.\nPS2 is applied to a comparison among three competing public policies aimed at\nreducing inequality and alleviating poverty: (a) house acquisition by the\ngovernment and distribution to lower income households, (b) rental vouchers,\nand (c) monetary aid. Within the model context, the monetary aid, that is,\nsmaller amounts of help for a larger number of households, makes the economy\nperform better in terms of production, consumption, reduction of inequality,\nand maintenance of financial duties. PS2 as such is also a framework that may\nbe further adapted to a number of related research questions.\n"
    },
    {
        "paper_id": 2102.11968,
        "authors": "Asaf Cohen and Yan Dolinsky",
        "title": "A Scaling Limit for Utility Indifference Prices in the Discretized\n  Bachelier Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the discretized Bachelier model where hedging is done on an\nequidistant set of times. Exponential utility indifference prices are studied\nfor path-dependent European options and we compute their non-trivial scaling\nlimit for a large number of trading times $n$ and when risk aversion is scaled\nlike $n\\ell$ for some constant $\\ell>0$. Our analysis is purely probabilistic.\nWe first use a duality argument to transform the problem into an optimal drift\ncontrol problem with a penalty term. We further use martingale techniques and\nstrong invariance principles and get that the limiting problem takes the form\nof a volatility control problem.\n"
    },
    {
        "paper_id": 2102.12051,
        "authors": "Jean-Fran\\c{c}ois Chassagneux, Junchao Chen, Noufel Frikha and Chao\n  Zhou",
        "title": "A learning scheme by sparse grids and Picard approximations for\n  semilinear parabolic PDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Relying on the classical connection between Backward Stochastic Differential\nEquations (BSDEs) and non-linear parabolic partial differential equations\n(PDEs), we propose a new probabilistic learning scheme for solving\nhigh-dimensional semi-linear parabolic PDEs. This scheme is inspired by the\napproach coming from machine learning and developed using deep neural networks\nin Han and al. [32]. Our algorithm is based on a Picard iteration scheme in\nwhich a sequence of linear-quadratic optimisation problem is solved by means of\nstochastic gradient descent (SGD) algorithm. In the framework of a linear\nspecification of the approximation space, we manage to prove a convergence\nresult for our scheme, under some smallness condition. In practice, in order to\nbe able to treat high-dimensional examples, we employ sparse grid approximation\nspaces. In the case of periodic coefficients and using pre-wavelet basis\nfunctions, we obtain an upper bound on the global complexity of our method. It\nshows in particular that the curse of dimensionality is tamed in the sense that\nin order to achieve a root mean squared error of order ${\\epsilon}$, for a\nprescribed precision ${\\epsilon}$, the complexity of the Picard algorithm grows\npolynomially in ${\\epsilon}^{-1}$ up to some logarithmic factor $\n|log({\\epsilon})| $ which grows linearly with respect to the PDE dimension.\nVarious numerical results are presented to validate the performance of our\nmethod and to compare them with some recent machine learning schemes proposed\nin Han and al. [20] and Hur\\'e and al. [37].\n"
    },
    {
        "paper_id": 2102.12112,
        "authors": "Vladim\\'ir Hol\\'y and Petra Tomanov\\'a",
        "title": "Modeling Price Clustering in High-Frequency Prices",
        "comments": null,
        "journal-ref": "Hol\\'y, V. & Tomanov\\'a, P. (2022). Modeling Price Clustering in\n  High-Frequency Prices. Quantitative Finance, 22(9), 1649-1663",
        "doi": "10.1080/14697688.2022.2050285",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price clustering phenomenon manifesting itself as an increased occurrence\nof specific prices is widely observed and well-documented for various financial\ninstruments and markets. In the literature, however, it is rarely incorporated\ninto price models. We consider that there are several types of agents trading\nonly in specific multiples of the tick size resulting in an increased\noccurrence of these multiples in prices. For example, stocks on the NYSE and\nNASDAQ exchanges are traded with precision to one cent but multiples of five\ncents and ten cents occur much more often in prices. To capture this behavior,\nwe propose a discrete price model based on a mixture of double Poisson\ndistributions with dynamic volatility and dynamic proportions of agent types.\nThe model is estimated by the maximum likelihood method. In an empirical study\nof DJIA stocks, we find that higher instantaneous volatility leads to weaker\nprice clustering at the ultra-high frequency. This is in sharp contrast with\nresults at low frequencies which show that daily realized volatility has a\npositive impact on price clustering.\n"
    },
    {
        "paper_id": 2102.123,
        "authors": "Andry Alamsyah, Fariz Denada Sudrajat, Herry Irawan",
        "title": "Property Business Classification Model Based on Indonesia E-Commerce\n  Data",
        "comments": "6 pages, 3 figures, 4 tables",
        "journal-ref": "The 8th International Conference on Sustainable Collaboration in\n  Business, Technology, Information and Innovation, 2017",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Online property business or known as e-commerce is currently experiencing an\nincrease in home sales. Indonesia's e-commerce property business has positive\ntrending shown by the increasing sales of more than 500% from 2011 to 2015. A\nprediction of the property price is important to help investors or the public\nto have accurate information before buying property. One of the methods for\nprediction is a classification based on several distinctive property industry\nattributes, such as building size, land size, number of rooms, and location.\nToday, data is easily obtained, there are many open data from E-commerce sites.\nE-commerce contains information about homes and other properties advertised to\nsell. People also regularly visit the site to find the right property or to\nsell the property using price information which collectively available as open\ndata. To predict the property sales, this research employed two different\nclassification methods in Data Mining which are Decision Tree and k-NN\nclassification. We compare which model classification is better to predict\nproperty price and their attributes. We use Indonesia's biggest property-based\ne-commerce site Rumah123.com as our open data source, and choose location\nBandung in our experiment. The accuracy result of the decision tree is 75% and\nKNN is 71%, other than that k-NN can explore more data patterns than the\nDecision Tree.\n"
    },
    {
        "paper_id": 2102.12309,
        "authors": "Ravshanbek Khodzhimatov, Stephan Leitner, Friederike Wall",
        "title": "Interactions between social norms and incentive mechanisms in\n  organizations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on how individual behavior that complies with social norms\ninterferes with performance-based incentive mechanisms in organizations with\nmultiple distributed decision-making agents. We model social norms to emerge\nfrom interactions between agents: agents observe other the agents' actions and,\nfrom these observations, induce what kind of behavior is socially acceptable.\nBy complying with the induced socially accepted behavior, agents experience\nutility. Also, agents get utility from a pay-for-performance incentive\nmechanism. Thus, agents pursue two objectives. We place the interaction between\nsocial norms and performance-based incentive mechanisms in the complex\nenvironment of an organization with distributed decision-makers, in which a set\nof interdependent tasks is allocated to multiple agents. The results suggest\nthat, unless the sets of assigned tasks are highly correlated, complying with\nemergent socially accepted behavior is detrimental to the organization's\nperformance. However, we find that incentive schemes can help offset the\nperformance loss by applying individual-based incentives in environments with\nlower task-complexity and team-based incentives in environments with higher\ntask-complexity.\n"
    },
    {
        "paper_id": 2102.1232,
        "authors": "Andry Alamsyah, Endang Sofyan, Tsana Hasti Nabila",
        "title": "Measuring Marketing Communications Mix Effort Using Magnitude Of\n  Influence And Influence Rank Metric",
        "comments": "7 pages, 8 figures",
        "journal-ref": "The 7th Smart Collaboration for Business in Technology and\n  Information Industry, 2016",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the context of modern marketing, Twitter is considered a communication\nplatform to spread information. Many companies create and acquire several\nTwitter accounts to support and perform varieties of marketing mix activities.\nInitially, each accounts used to capture a specific market profile. Together,\nthe accounts create a network of information that provide consumer to the\ninformation they need depends on their contextual utilization. From many\naccounts available, we have the fundamental question on how to measure the\ninfluence of each account in the market based not only on their relations but\nalso on the effects of their postings. The magnitude of Influence (MOI) metric\nis adapted together with Influence Rank (IR) measurement of accounts in their\nsocial network neighborhood. We use social network analysis approach to analyze\n65 accounts in the social network of an Indonesian mobile phone network\noperator, Telkomsel which involved in marketing communications mix activities\nthrough series of related tweets. Using social network provide the idea of the\nactivity in building and maintaining relationships with the target audience.\nThis paper shows the results of the most potential accounts based on the\nnetwork structure and engagement. Based on this research, the more number of\nfollowers one account has, the more responsibility it has to generate the\ninteraction from their followers in order to achieve the expected\neffectiveness. The focus of this paper is to determine the most potential\naccounts in the application of marketing communications mix in Twitter.\n"
    },
    {
        "paper_id": 2102.12337,
        "authors": "Andry Alamsyah, Maribella Syawiluna",
        "title": "Mapping Organization Knowledge Network and Social Media Based Reputation\n  Management",
        "comments": "9 pages, 8 figues, 2 tables",
        "journal-ref": "Journal of Data Science and Its Applications, 2018, Vol 1 No 1",
        "doi": "10.21108/jdsa.2018.1.3",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Knowledge management is an important aspect of an organization, especially in\nthe ICT industry. Having more control of it is essentials for the organization\nto stay competitive in the business. One way to assess the organization's\nknowledge capital is by measuring employee knowledge networks and their\npersonal reputation in social media. Using this measurement, we see how\nemployees build relationships around their peer networks or clients virtually.\nWe are also able to see how knowledge networks support organizational\nperformance. The research objective is to map knowledge network and reputation\nformulation in order to fully understand how knowledge flow and whether\nemployee reputation has a higher degree of influence in the organization's\nknowledge network. We particularly develop formulas to measure knowledge\nnetworks and personal reputation based on their social media activities. As a\ncase study, we pick an Indonesian ICT company that actively build their\nbusiness around their employee peer knowledge outside the company. For the\nknowledge network, we perform data collection by conducting interviews. For\nreputation management, we collect data from several popular social media. We\nbase our work on Social Network Analysis (SNA) methodology. The result shows\nthat employees' knowledge is directly proportional to their reputation, but\nthere are different reputations level on different social media observed in\nthis research.\n"
    },
    {
        "paper_id": 2102.1235,
        "authors": "Andry Alamsyah, Sheila Shafira, Muhamad Alfin Yudhistira",
        "title": "Summarizing Online Conversation of Indonesia Tourism Industry using\n  Network Text Analysis",
        "comments": "6 pages, 2 figures, 2 tables",
        "journal-ref": "The 8th International Conference on Sustainable Collaboration in\n  Business, Technology, Information and Innovation, 2017",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The tourism industry is one of the potential revenues and has an important\nrole in economics in Indonesia. The tourism Industry brings job and business\nopportunities, foreign exchange earnings, and infrastructure development,\ntourism also plays the role of one of the main drivers in socio-economic\nprogress in Indonesia. The number of foreign tourists visiting Indonesia\nincrease cumulatively and has reached 10.41 million visits or an increase of\n10.46 percent from the same period in the previous year. Government trying to\nincrease the number of tourists to visit Indonesia by promoting many Indonesian\ntourist attractions.\n"
    },
    {
        "paper_id": 2102.12378,
        "authors": "Immaculate Maumoh and Emmanuel H. Yindi",
        "title": "Understanding the Farmers, Environmental Citizenship Behaviors Towards\n  Climate Change. The Moderating Mediating Role of Environmental Knowledge and\n  Ascribed Responsibility",
        "comments": "14 pages,6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Knowledge is known to be a pre-condition for an individuals behavior. For the\nmost efficient informational strategies for education, it is essential that we\nidentify the types of knowledge that promote behavior effectively and\ninvestigate their structure. The purpose of this paper is therefore to examine\nthe factors that affect Kenyan farmers, environmental citizenship behavior\n(ECB) in the context of Adaptation and mitigation (Climate smart agriculture).\nTo achieve this objective, a theoretical framework has been developed based on\nvalue belief norm (VBN) theory. Design/methodology/approach, Data were obtained\nfrom 350 farmers using a survey method. Partial lease square structural\nequation modelling (PLS-SEM) was used to examine the hypothetical model. The\nresults of PLS analysis confirm the direct and mediating effect of the causal\nsequences of the variables in the VBN model. The moderating role of\nEnvironmental knowledge has been seen to be impactful in Climate Smart\nAgriculture.\n"
    },
    {
        "paper_id": 2102.12423,
        "authors": "Ren\\'e A\\\"id, Sara Biagini",
        "title": "Optimal dynamic regulation of carbon emissions market: A variational\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider the problem of reducing the carbon emissions of a set of firms\nover a finite horizon. A regulator dynamically allocates emission allowances to\neach firm. Firms face idiosyncratic as well as common economic shocks on\nemissions, and have linear quadratic abatement costs. Firms can trade\nallowances so to minimise total expected costs, from abatement and trading plus\na quadratic terminal penalty. Using variational methods, we exhibit in\nclosed-form the market equilibrium in function of regulator's dynamic\nallocation. We then solve the Stackelberg game between the regulator and the\nfirms. Again, we obtain a closed-form expression of the dynamic allocation\npolicies that allow a desired expected emission reduction. Optimal policies are\nnot unique but share common properties. Surprisingly, all optimal policies\ninduce a constant abatement effort and a constant price of allowances. Dynamic\nallocations outperform static ones because of adjustment costs and uncertainty,\nin particular given the presence of common shocks. Our results are robust to\nsome extensions, like risk aversion of firms or different penalty functions.\n"
    },
    {
        "paper_id": 2102.12454,
        "authors": "Tao Wang, Shiying Xiao, Jun Yan, Panpan Zhang",
        "title": "Regional and Sectoral Structures and Their Dynamics of Chinese Economy:\n  A Network Perspective from Multi-Regional Input-Output Tables",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126196",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A multi-regional input-output table (MRIOT) containing the transactions among\nthe region-sectors in an economy defines a weighted and directed network. Using\nnetwork analysis tools, we analyze the regional and sectoral structure of the\nChinese economy and their temporal dynamics from 2007 to 2012 via the MRIOTs of\nChina. Global analyses are done with network topology measures. Growth-driving\nprovince-sector clusters are identified with community detection methods.\nInfluential province-sectors are ranked by weighted PageRank scores. The\nresults revealed a few interesting and telling insights. The level of\ninter-province-sector activities increased with the rapid growth of the\nnational economy, but not as fast as that of intra-province economic\nactivities. Regional community structures were deeply associated with\ngeographical factors. The community heterogeneity across the regions was high\nand the regional fragmentation increased during the study period. Quantified\nmetrics assessing the relative importance of the province-sectors in the\nnational economy echo the national and regional economic development policies\nto a certain extent.\n"
    },
    {
        "paper_id": 2102.12601,
        "authors": "Tim Leung and Yang Zhou",
        "title": "Optimal Dynamic Futures Portfolios Under a Multiscale Central Tendency\n  Ornstein-Uhlenbeck Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of dynamically trading multiple futures whose underlying\nasset price follows a multiscale central tendency Ornstein-Uhlenbeck (MCTOU)\nmodel. Under this model, we derive the closed-form no-arbitrage prices for the\nfutures contracts. Applying a utility maximization approach, we solve for the\noptimal trading strategies under different portfolio configurations by\nexamining the associated system of Hamilton-Jacobi-Bellman (HJB) equations. The\noptimal strategies depend on not only the parameters of the underlying asset\nprice process but also the risk premia embedded in the futures prices.\nNumerical examples are provided to illustrate the investor's optimal positions\nand optimal wealth over time.\n"
    },
    {
        "paper_id": 2102.12658,
        "authors": "Xiuqin Xu, Ying Chen",
        "title": "Deep Stochastic Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility for financial assets returns can be used to gauge the risk for\nfinancial market. We propose a deep stochastic volatility model (DSVM) based on\nthe framework of deep latent variable models. It uses flexible deep learning\nmodels to automatically detect the dependence of the future volatility on past\nreturns, past volatilities and the stochastic noise, and thus provides a\nflexible volatility model without the need to manually select features. We\ndevelop a scalable inference and learning algorithm based on variational\ninference. In real data analysis, the DSVM outperforms several popular\nalternative volatility models. In addition, the predicted volatility of the\nDSVM provides a more reliable risk measure that can better reflex the risk in\nthe financial market, reaching more quickly to a higher level when the market\nbecomes more risky and to a lower level when the market is more stable,\ncompared with the commonly used GARCH type model with a huge data set on the\nU.S. stock market.\n"
    },
    {
        "paper_id": 2102.12694,
        "authors": "Alexandre Carbonneau and Fr\\'ed\\'eric Godin",
        "title": "Deep Equal Risk Pricing of Financial Derivatives with Multiple Hedging\n  Instruments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the equal risk pricing (ERP) framework for the valuation\nof European financial derivatives. This option pricing approach is consistent\nwith global trading strategies by setting the premium as the value such that\nthe residual hedging risk of the long and short positions in the option are\nequal under optimal hedging. The ERP setup of Marzban et al. (2020) is\nconsidered where residual hedging risk is quantified with convex risk measures.\nThe main objective of this paper is to assess through extensive numerical\nexperiments the impact of including options as hedging instruments within the\nERP framework. The reinforcement learning procedure developed in Carbonneau and\nGodin (2020), which relies on the deep hedging algorithm of Buehler et al.\n(2019b), is applied to numerically solve the global hedging problems by\nrepresenting trading policies with neural networks. Among other findings,\nnumerical results indicate that in the presence of jump risk, hedging long-term\nputs with shorter-term options entails a significant decrease of both equal\nrisk prices and market incompleteness as compared to trading only the stock.\nMonte Carlo experiments demonstrate the potential of ERP as a fair valuation\napproach providing prices consistent with observable market prices. Analyses\nexhibit the ability of ERP to span a large interval of prices through the\nchoice of convex risk measures which is close to encompass the variance-optimal\npremium.\n"
    },
    {
        "paper_id": 2102.12783,
        "authors": "Kwangmin Jung, Donggyu Kim, and Seunghyeon Yu",
        "title": "Next Generation Models for Portfolio Risk Management: An Approach Using\n  Financial Big Data",
        "comments": "59 pages, 6 figures",
        "journal-ref": "Journal of Risk and Insurance (2022), 1-23",
        "doi": "10.1111/jori.12374",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a dynamic process of portfolio risk measurement to\naddress potential information loss. The proposed model takes advantage of\nfinancial big data to incorporate out-of-target-portfolio information that may\nbe missed when one considers the Value at Risk (VaR) measures only from certain\nassets of the portfolio. We investigate how the curse of dimensionality can be\novercome in the use of financial big data and discuss where and when benefits\noccur from a large number of assets. In this regard, the proposed approach is\nthe first to suggest the use of financial big data to improve the accuracy of\nrisk analysis. We compare the proposed model with benchmark approaches and\nempirically show that the use of financial big data improves small portfolio\nrisk analysis. Our findings are useful for portfolio managers and financial\nregulators, who may seek for an innovation to improve the accuracy of portfolio\nrisk estimation.\n"
    },
    {
        "paper_id": 2102.128,
        "authors": "Malkhaz Shashiashvili",
        "title": "The Stochastic Balance Equation for the American Option Value Function\n  and its Gradient",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the paper we consider the problem of valuation and hedging of American\noptions written on dividend-paying assets whose price dynamics follow the\nmultidimensional diffusion model. We derive a stochastic balance equation for\nthe American option value function and its gradient. We prove that the latter\npair is the unique solution of the stochastic balance equation as a result of\nthe uniqueness in the related adapted future-supremum problem.\n"
    },
    {
        "paper_id": 2102.12809,
        "authors": "Guillaume Carlier, Victor Chernozhukov, Gwendoline De Bie and Alfred\n  Galichon",
        "title": "Vector quantile regression and optimal transport, from theory to\n  numerics",
        "comments": "35 pages, 19 figures, 4 tables. arXiv admin note: text overlap with\n  arXiv:1610.06833",
        "journal-ref": "Empirical Economics (2020)",
        "doi": "10.1007/s00181-020-01919-y",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we first revisit the Koenker and Bassett variational approach\nto (univariate) quantile regression, emphasizing its link with latent factor\nrepresentations and correlation maximization problems. We then review the\nmultivariate extension due to Carlier et al. (2016, 2017) which relates vector\nquantile regression to an optimal transport problem with mean independence\nconstraints. We introduce an entropic regularization of this problem, implement\na gradient descent numerical method and illustrate its feasibility on\nunivariate and bivariate examples.\n"
    },
    {
        "paper_id": 2102.12811,
        "authors": "Alfred Galichon and Bernard Salani\\'e",
        "title": "Matching with Trade-offs: Revealed Preferences over Competing\n  Characteristics",
        "comments": "62 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate in this paper the theory and econometrics of optimal matchings\nwith competing criteria. The surplus from a marriage match, for instance, may\ndepend both on the incomes and on the educations of the partners, as well as on\ncharacteristics that the analyst does not observe. Even if the surplus is\ncomplementary in incomes, and complementary in educations, imperfect\ncorrelation between income and education at the individual level implies that\nthe social optimum must trade off matching on incomes and matching on\neducations. Given a flexible specification of the surplus function, we\ncharacterize under mild assumptions the properties of the set of feasible\nmatchings and of the socially optimal matching. Then we show how data on the\ncovariation of the types of the partners in observed matches can be used to\ntest that the observed matches are socially optimal for this specification, and\nto estimate the parameters that define social preferences over matches.\n"
    },
    {
        "paper_id": 2102.13047,
        "authors": "Furkan Sezer, Hossein Khazaei, and Ceyhun Eksin",
        "title": "Maximizing Social Welfare and Agreement via Information Design in\n  Linear-Quadratic-Gaussian Games",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/TAC.2023.3270241",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider linear-quadratic Gaussian (LQG) games in which players have\nquadratic payoffs that depend on the players' actions and an unknown\npayoff-relevant state, and signals on the state that follow a Gaussian\ndistribution conditional on the state realization. An information designer\ndecides the fidelity of information revealed to the players in order to\nmaximize the social welfare of the players or reduce the disagreement among\nplayers' actions. Leveraging the semi-definiteness of the information design\nproblem, we derive analytical solutions for these objectives under specific LQG\ngames. We show that full information disclosure maximizes social welfare when\nthere is a common payoff-relevant state, when there is strategic\nsubstitutability in the actions of players, or when the signals are public.\nNumerical results show that as strategic substitution increases, the value of\nthe information disclosure increases. When the objective is to induce\nconformity among players' actions, hiding information is optimal. Lastly, we\nconsider the information design objective that is a weighted combination of\nsocial welfare and cohesiveness of players' actions. We obtain an interval for\nthe weights where full information disclosure is optimal under public signals\nfor games with strategic substitutability. Numerical solutions show that the\nactual interval where full information disclosure is optimal gets close to the\nanalytical interval obtained as substitution increases.\n"
    },
    {
        "paper_id": 2102.1311,
        "authors": "Richard S.J. Tol",
        "title": "The economic impact of weather and climate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I propose a new conceptual framework to disentangle the impacts of weather\nand climate on economic activity and growth: A stochastic frontier model with\nclimate in the production frontier and weather shocks as a source of\ninefficiency. I test it on a sample of 160 countries over the period 1950-2014.\nTemperature and rainfall determine production possibilities in both rich and\npoor countries; positively in cold countries and negatively in hot ones.\nWeather anomalies reduce inefficiency in rich countries but increase\ninefficiency in poor and hot countries; and more so in countries with low\nweather variability. The climate effect is larger that the weather effect.\n"
    },
    {
        "paper_id": 2102.1315,
        "authors": "Elisa Heinrich Mora, Jacob J. Jackson, Cate Heine, Geoffrey B. West,\n  Vicky Chuqiao Yang, Christopher P. Kempes",
        "title": "Scaling of Urban Income Inequality in the United States",
        "comments": "18 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Urban scaling analysis, the study of how aggregated urban features vary with\nthe population of an urban area, provides a promising framework for discovering\ncommonalities across cities and uncovering dynamics shared by cities across\ntime and space. Here, we use the urban scaling framework to study an important,\nbut under-explored feature in this community - income inequality. We propose a\nnew method to study the scaling of income distributions by analyzing total\nincome scaling in population percentiles. We show that income in the least\nwealthy decile (10%) scales close to linearly with city population, while\nincome in the most wealthy decile scale with a significantly superlinear\nexponent. In contrast to the superlinear scaling of total income with city\npopulation, this decile scaling illustrates that the benefits of larger cities\nare increasingly unequally distributed. For the poorest income deciles, cities\nhave no positive effect over the null expectation of a linear increase. We\nrepeat our analysis after adjusting income by housing cost, and find similar\nresults. We then further analyze the shapes of income distributions. First, we\nfind that mean, variance, skewness, and kurtosis of income distributions all\nincrease with city size. Second, the Kullback-Leibler divergence between a\ncity's income distribution and that of the largest city decreases with city\npopulation, suggesting the overall shape of income distribution shifts with\ncity population. As most urban scaling theories consider densifying\ninteractions within cities as the fundamental process leading to the\nsuperlinear increase of many features, our results suggest this effect is only\nseen in the upper deciles of the cities. Our finding encourages future work to\nconsider heterogeneous models of interactions to form a more coherent\nunderstanding of urban scaling.\n"
    },
    {
        "paper_id": 2102.13157,
        "authors": "Yildiray Yildirim and Albert Alex Zevelev",
        "title": "Does Bankruptcy Protection Affect Asset Prices? Evidence from changes in\n  Homestead Exemptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Does the ability to protect an asset from unsecured creditors affect its\nprice? This paper identifies the impact of bankruptcy protection on house\nprices using 139 changes in homestead exemptions. Large increases in the\nhomestead exemption raised house prices 3% before 2005. Smaller exemption\nincreases, to adjust for inflation, did not affect house prices. The effect\ndisappeared after BAPCPA, a 2005 federal law designed to prevent bankruptcy\nabuse. The effect was bigger in inelastic locations.\n"
    },
    {
        "paper_id": 2102.13404,
        "authors": "Dohyun Chun, Donggyu Kim",
        "title": "State Heterogeneity Analysis of Financial Volatility Using\n  High-Frequency Financial Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, to account for low-frequency market dynamics, several volatility\nmodels, employing high-frequency financial data, have been developed. However,\nin financial markets, we often observe that financial volatility processes\ndepend on economic states, so they have a state heterogeneous structure. In\nthis paper, to study state heterogeneous market dynamics based on\nhigh-frequency data, we introduce a novel volatility model based on a\ncontinuous Ito diffusion process whose intraday instantaneous volatility\nprocess evolves depending on the exogenous state variable, as well as its\nintegrated volatility. We call it the state heterogeneous GARCH-Ito (SG-Ito)\nmodel. We suggest a quasi-likelihood estimation procedure with the realized\nvolatility proxy and establish its asymptotic behaviors. Moreover, to test the\nlow-frequency state heterogeneity, we develop a Wald test-type hypothesis\ntesting procedure. The results of empirical studies suggest the existence of\nleverage, investor attention, market illiquidity, stock market comovement, and\npost-holiday effect in S&P 500 index volatility.\n"
    },
    {
        "paper_id": 2102.13464,
        "authors": "Eiji Yamamura",
        "title": "Granddaughter and voting for a female candidate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the influence of grandchildren's gender on grandparents'\nvoting behavior using independently collected individual-level data. The survey\nwas conducted immediately after the House of Councilors election in Japan. I\nobserved that individuals with a granddaughter were more likely to vote for\nfemale candidates by around 10 % than those without. However, having a daughter\ndid not affect the parents' voting behavior. Furthermore, having a son or a\ngrandson did not influence grandparents' voting behavior. This implies that\ngrandparents voted for their granddaughter's future benefit because\ngranddaughters may be too young vote in a male-dominated and aging society.\n"
    },
    {
        "paper_id": 2102.13467,
        "authors": "Donggyu Kim, Minseok Shin, Yazhen Wang",
        "title": "Overnight GARCH-It\\^o Volatility Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Various parametric volatility models for financial data have been developed\nto incorporate high-frequency realized volatilities and better capture market\ndynamics. However, because high-frequency trading data are not available during\nthe close-to-open period, the volatility models often ignore volatility\ninformation over the close-to-open period and thus may suffer from loss of\nimportant information relevant to market dynamics. In this paper, to account\nfor whole-day market dynamics, we propose an overnight volatility model based\non It\\^o diffusions to accommodate two different instantaneous volatility\nprocesses for the open-to-close and close-to-open periods. We develop a\nweighted least squares method to estimate model parameters for two different\nperiods and investigate its asymptotic properties. We conduct a simulation\nstudy to check the finite sample performance of the proposed model and method.\nFinally, we apply the proposed approaches to real trading data.\n"
    },
    {
        "paper_id": 2102.13503,
        "authors": "Baptiste Barreau, Laurent Carlier",
        "title": "History-Augmented Collaborative Filtering for Financial Recommendations",
        "comments": null,
        "journal-ref": "RecSys '20: Fourteenth ACM Conference on Recommender Systems, Sep\n  2020, Virtual Event, Brazil. pp.492-497",
        "doi": "10.1145/3383313.3412206",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many businesses, and particularly in finance, the behavior of a client\nmight drastically change over time. It is consequently crucial for recommender\nsystems used in such environments to be able to adapt to these changes. In this\nstudy, we propose a novel collaborative filtering algorithm that captures the\ntemporal context of a user-item interaction through the users' and items'\nrecent interaction histories to provide dynamic recommendations. The algorithm,\ndesigned with issues specific to the financial world in mind, uses a custom\nneural network architecture that tackles the non-stationarity of users' and\nitems' behaviors. The performance and properties of the algorithm are monitored\nin a series of experiments on a G10 bond request for quotation proprietary\ndatabase from BNP Paribas Corporate and Institutional Banking.\n"
    },
    {
        "paper_id": 2102.13505,
        "authors": "Aur\\'elien Alfonsi and Ahmed Kebaier",
        "title": "Approximation of Stochastic Volterra Equations with kernels of\n  completely monotone type",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we develop a multifactor approximation for $d$-dimensional\nStochastic Volterra Equations (SVE) with Lipschitz coefficients and kernels of\ncompletely monotone type that may be singular. First, we prove an\n$L^2$-estimation between two SVEs with different kernels, which provides a\nquantification of the error between the SVE and any multifactor Stochastic\nDifferential Equation (SDE) approximation. For the particular rough kernel case\nwith Hurst parameter lying in $(0,1/2)$, we propose various approximating\nmultifactor kernels, state their rates of convergence and illustrate their\nefficiency for the rough Bergomi model. Second, we study a Euler discretization\nof the multifactor SDE and establish a convergence result towards the SVE that\nis uniform with respect to the approximating multifactor kernels. These\nobtained results lead us to build a new multifactor Euler scheme that reduces\nsignificantly the computational cost in an asymptotic way compared to the Euler\nscheme for SVEs. Finally, we show that our multifactor Euler scheme outperforms\nthe Euler scheme for SVEs for option pricing in the rough Heston model.\n"
    },
    {
        "paper_id": 2102.13538,
        "authors": "Valeria Fanghella, Thi-Thanh-Tam Vu, Luigi Mittone",
        "title": "Priming prosocial behavior and expectations in response to the Covid-19\n  pandemic -- Evidence from an online experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies whether and how differently projected information about\nthe impact of the Covid-19 pandemic affects individuals' prosocial behavior and\nexpectations on future outcomes. We conducted an online experiment with British\nparticipants (N=961) when the UK introduced its first lockdown and the outbreak\nwas on its growing stage. Participants were primed with either the\nenvironmental or economic consequences (i.e., negative primes), or the\nenvironmental or economic benefits (i.e., positive primes) of the pandemic, or\nwith neutral information. We measured priming effects on an incentivized\ntake-and-give dictator game and on participants' expectations about future\nenvironmental quality and economic growth. Our results show that primes affect\nparticipants' expectations, but not their prosociality. In particular,\nparticipants primed with environmental consequences hold a more pessimistic\nview on future environmental quality, while those primed with economic benefits\nare more optimistic about future economic growth. Instead, the positive\nenvironmental prime and the negative economic prime do not influence\nexpectations. Our results offer insights into how information affects behavior\nand expectations during the Covid-19 pandemic.\n"
    },
    {
        "paper_id": 2102.13539,
        "authors": "Sam Hamels, Eline Himpe, Jelle Laverge, Marc Delghust, Kjartan Van den\n  Brande, Arnold Janssens and Johan Albrecht",
        "title": "The use of primary energy factors and CO2 intensities -- reviewing the\n  state of play in academic literature",
        "comments": "34 pages, 1 figure",
        "journal-ref": "Renew. Sustain. Energy Rev. 146 (2021)",
        "doi": "10.1016/j.rser.2021.111182",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reaching the 2030 targets for the EU primary energy use (PE) and CO2eq\nemissions (CE) requires an accurate assessment of how different technologies\nperform on these two fronts. In this regard, the focus in academia is\nincreasingly shifting from traditional technologies to electricity consuming\nalternatives. Calculating and comparing their performance with respect to\ntraditional technologies requires conversion factors (CFs) like a primary\nenergy factor and a CO2eq intensity. These reflect the PE and CE associated\nwith each unit of electricity consumed. Previous work has shown that the\ncalculation and use of CFs is a contentious and multifaceted issue. However,\nthis has mostly remained a theoretical discussion. A stock-taking of how CFs\nare actually calculated and used in academic literature has so far been\nmissing, impeding insight into what the contemporary trends and challenges are.\nTherefore, we structurally review 65 publications across six methodological\naspects. We find that 72% of the publications consider only a single country,\n86% apply a purely retrospective perspective, 54% apply a yearly temporal\nresolution, 65% apply a purely operational (instead of a life-cycle)\nperspective, 91% make use of average (rather than marginal) CFs, and 75% ignore\nelectricity imports from surrounding countries. We conclude that there is a\nstrong need in the literature for a publicly available, transparently\ncalculated dataset of CFs, which avoids the shortcomings found in the\nliterature. This would enable more accurate and transparent PE and CE\ncalculations, and support the development of new building energy performance\nassessment methods and smart grid algorithms.\n"
    },
    {
        "paper_id": 2102.13574,
        "authors": "Lars Niemann and Thorsten Schmidt",
        "title": "A conditional version of the second fundamental theorem of asset pricing\n  in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial market in discrete time and study pricing and hedging\nconditional on the information available up to an arbitrary point in time. In\nthis conditional framework, we determine the structure of arbitrage-free\nprices. Moreover, we characterize attainability and market completeness. We\nderive a conditional version of the second fundamental theorem of asset\npricing, which, surprisingly, is not available up to now. The main tool we use\nare time consistency properties of dynamic nonlinear expectations, which we\napply to the super- and subhedging prices. The results obtained extend existing\nresults in the literature, where the conditional setting is considered in most\ncases only on finite probability spaces.\n"
    },
    {
        "paper_id": 2103.00095,
        "authors": "Maria Eugenia Ibarraran, Romeo A. Saldana-Vazquez, Tamara Perez-Garcia",
        "title": "The Cost of Pollution in the Upper Atoyac River Basin: A Systematic\n  Review",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The Atoyac River is among the two most polluted in Mexico. Water quality in\nthe Upper Atoyac River Basin (UARB) has been devastated by industrial and\nmunicipal wastewater, as well as from effluents from local dwellers, that go\nthrough little to no treatment, affecting health, production, ecosystems and\nproperty value. We did a systematic review and mapping of the costs that\npollution imposes on different sectors and localities in the UARB, and\ninitially found 358 studies, of which 17 were of our particular interest. We\nfocus on estimating the cost of pollution through different valuation methods\nsuch as averted costs, hedonic pricing, and contingent valuation, and for that\nwe only use 10 studies. Costs range from less than a million to over $16\nmillion dollars a year, depending on the sector, with agriculture, industry and\ntourism yielding the highest costs. This exercise is the first of its kind in\nthe UARB that maps costs for sectors and localities affected, and sheds light\non the need of additional research to estimate the total cost of pollution\nthroughout the basin. This information may help design further research needs\nin the region.\n"
    },
    {
        "paper_id": 2103.00173,
        "authors": "Yulin Liu, Luyao Zhang and Yinhong Zhao",
        "title": "Deciphering Bitcoin Blockchain Data by Cohort Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin is a peer-to-peer electronic payment system that has rapidly grown in\npopularity in recent years. Usually, the complete history of Bitcoin blockchain\ndata must be queried to acquire variables with economic meaning. This task has\nrecently become increasingly difficult, as there are over 1.6 billion\nhistorical transactions on the Bitcoin blockchain. It is thus important to\nquery Bitcoin transaction data in a way that is more efficient and provides\neconomic insights. We apply cohort analysis that interprets Bitcoin blockchain\ndata using methods developed for population data in the social sciences.\nSpecifically, we query and process the Bitcoin transaction input and output\ndata within each daily cohort. This enables us to create datasets and\nvisualizations for some key Bitcoin transaction indicators, including the daily\nlifespan distributions of spent transaction output (STXO) and the daily age\ndistributions of the cumulative unspent transaction output (UTXO). We provide a\ncomputationally feasible approach for characterizing Bitcoin transactions that\npaves the way for future economic studies of Bitcoin.\n"
    },
    {
        "paper_id": 2103.00231,
        "authors": "Andry Alamsyah, Fatma Saviera",
        "title": "A Comparison of Indonesia E-Commerce Sentiment Analysis for Marketing\n  Intelligence Effort",
        "comments": null,
        "journal-ref": "The 8th International Conference on Sustainable Collaboration in\n  Business, Technology, Information and Innovation, 2017",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The rapid growth of the e-commerce market in Indonesia, making various\ne-commerce companies appear and there has been high competition among them.\nMarketing intelligence is an important activity to measure competitive\nposition. One element of marketing intelligence is to assess customer\nsatisfaction. Many Indonesian customers express their sense of satisfaction or\ndissatisfaction towards the company through social media. Hence, using social\nmedia data provides a new practical way to measure marketing intelligence\neffort. This research performs sentiment analysis using the naive bayes\nclassifier classification method with TF-IDF weighting. We compare the\nsentiments towards of top-3 e-commerce sites visited companies, are Bukalapak,\nTokopedia, and Elevenia. We use Twitter data for sentiment analysis because\nit's faster, cheaper, and easier from both the customer and the researcher\nside. The purpose of this research is to find out how to process the huge\ncustomer sentiment Twitter to become useful information for the e-commerce\ncompany, and which of those top-3 e-commerce companies has the highest level of\ncustomer satisfaction. The experiment results show the method can be used to\nclassify customer sentiments in social media Twitter automatically and Elevenia\nis the highest e-commerce with customer satisfaction.\n"
    },
    {
        "paper_id": 2103.00254,
        "authors": "David Chaum, Christian Grothoff, Thomas Moser",
        "title": "How to Issue a Central Bank Digital Currency",
        "comments": "Swiss National Bank Working Paper3/2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  With the emergence of Bitcoin and recently proposed stablecoins from\nBigTechs, such as Diem (formerly Libra), central banks face growing competition\nfrom private actors offering their own digital alternative to physical cash. We\ndo not address the normative question whether a central bank should issue a\ncentral bank digital currency (CBDC) or not. Instead, we contribute to the\ncurrent research debate by showing how a central bank could do so, if desired.\nWe propose a token-based system without distributed ledger technology and show\nhow earlier-deployed, software-only electronic cash can be improved upon to\npreserve transaction privacy, meet regulatory requirements in a compelling way,\nand offer a level of quantum-resistant protection against systemic privacy\nrisk. Neither monetary policy nor financial stability would be materially\naffected because a CBDC with this design would replicate physical cash rather\nthan bank deposits.\n"
    },
    {
        "paper_id": 2103.00264,
        "authors": "Parley Ruogu Yang",
        "title": "Forecasting high-frequency financial time series: an adaptive learning\n  approach with the order book data",
        "comments": "Key words: forecasting methods, statistical learning, high-frequency\n  order book",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes a forecast-centric adaptive learning model that engages\nwith the past studies on the order book and high-frequency data, with\napplications to hypothesis testing. In line with the past literature, we\nproduce brackets of summaries of statistics from the high-frequency bid and ask\ndata in the CSI 300 Index Futures market and aim to forecast the one-step-ahead\nprices. Traditional time series issues, e.g. ARIMA order selection,\nstationarity, together with potential financial applications are covered in the\nexploratory data analysis, which pave paths to the adaptive learning model. By\ndesigning and running the learning model, we found it to perform well compared\nto the top fixed models, and some could improve the forecasting accuracy by\nbeing more stable and resilient to non-stationarity. Applications to hypothesis\ntesting are shown with a rolling window, and further potential applications to\nfinance and statistics are outlined.\n"
    },
    {
        "paper_id": 2103.00323,
        "authors": "Rajinda Wickrama",
        "title": "Pricing Exchange Rate Options and Quanto Caps in the Cross-Currency\n  Random Field LIBOR Market Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop an arbitrage-free random field LIBOR market model to price\ncross-currency derivatives. The uncertainty of the forward LIBOR rates of our\ncross-currency model is driven by a two time parameter random field instead of\na finite dimensional Brownian motion. To demonstrate the applications of this\nmodel, we develop an approximate closed-form pricing formula for Quanto caps\nand cross-currency swaps. Further, we derive an exact pricing formula for an\nexchange rate option in the random field setting.\n"
    },
    {
        "paper_id": 2103.00366,
        "authors": "Kristof Lommers, Ouns El Harzli, Jack Kim",
        "title": "Confronting Machine Learning With Financial Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to examine the challenges and applications of machine\nlearning for financial research. Machine learning algorithms have been\ndeveloped for certain data environments which substantially differ from the one\nwe encounter in finance. Not only do difficulties arise due to some of the\nidiosyncrasies of financial markets, there is a fundamental tension between the\nunderlying paradigm of machine learning and the research philosophy in\nfinancial economics. Given the peculiar features of financial markets and the\nempirical framework within social science, various adjustments have to be made\nto the conventional machine learning methodology. We discuss some of the main\nchallenges of machine learning in finance and examine how these could be\naccounted for. Despite some of the challenges, we argue that machine learning\ncould be unified with financial research to become a robust complement to the\neconometrician's toolbox. Moreover, we discuss the various applications of\nmachine learning in the research process such as estimation, empirical\ndiscovery, testing, causal inference and prediction.\n"
    },
    {
        "paper_id": 2103.00395,
        "authors": "Nassim Dehouche",
        "title": "Scale matters: The daily, weekly and monthly volatility and\n  predictability of Bitcoin, Gold, and the S&P 500",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A reputation of high volatility accompanies the emergence of Bitcoin as a\nfinancial asset. This paper intends to nuance this reputation and clarify our\nunderstanding of Bitcoin's volatility. Using daily, weekly, and monthly closing\nprices and log-returns data going from September 2014 to January 2021, we find\nthat Bitcoin is a prime example of an asset for which the two conceptions of\nvolatility diverge. We show that, historically, Bitcoin allies both high\nvolatility (high Standard Deviation) and high predictability (low Approximate\nEntropy), relative to Gold and S&P 500.\n  Moreover, using tools from Extreme Value Theory, we analyze the convergence\nof moments, and the mean excess functions of both the closing prices and the\nlog-returns of the three assets. We find that the closing price of Bitcoin is\nconsistent with a generalized Pareto distribution, when the closing prices of\nthe two other assets (Gold and S&P 500) present thin-tailed distributions.\nHowever, returns for all three assets are heavy tailed and second moments\n(variance, standard deviation) non-convergent. In the case of Bitcoin, lower\nsampling frequencies (monthly vs weekly, weekly vs daily) drastically reduce\nthe Kurtosis of log-returns and increase the convergence of empirical moments\nto their true value. The opposite effect is observed for Gold and S&P 500.\nThese properties suggest that Bitcoin's volatility is essentially an intra-day\nand intra-week phenomenon that is strongly attenuated on a weekly time-scale,\nand make it an attractive store of value to investors and speculators, but its\nhigh standard deviation excludes its use a currency.\n"
    },
    {
        "paper_id": 2103.00565,
        "authors": "Jani-Pekka Jokinen",
        "title": "Modelling Optimal Policies of Demand Responsive Transport and\n  Interrelationships between Occupancy Rate and Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a model addressing welfare optimal policies of demand\nresponsive transportation service, where passengers cause external travel time\ncosts for other passengers due to the route changes. Optimal pricing and trip\nproduction policies are modelled both on the aggregate level and on the network\nlevel. The aggregate model is an extension from Jokinen (2016) with flat\npricing model, but occupancy rate is now modelled as an endogenous variable\ndepending on demand and capacity levels. The network model enables to describe\ndifferences between routes from the viewpoint of occupancy rate and efficient\ntrip combining. Moreover, the model defines the optimal differentiated pricing\nfor routes.\n"
    },
    {
        "paper_id": 2103.00591,
        "authors": "Satoshi Fukuda, Nenad Kos, Christoph Wolf",
        "title": "Epidemics with Behavior",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study equilibrium distancing during epidemics. Distancing reduces the\nindividual's probability of getting infected but comes at a cost. It creates a\nsingle-peaked epidemic, flattens the curve and decreases the size of the\nepidemic. We examine more closely the effects of distancing on the outset, the\npeak and the final size of the epidemic. First, we define a behavioral basic\nreproduction number and show that it is concave in the transmission rate. The\ninfection, therefore, spreads only if the transmission rate is in the\nintermediate region. Second, the peak of the epidemic is non-monotonic in the\ntransmission rate. A reduction in the transmission rate can lead to an increase\nof the peak. On the other hand, a decrease in the cost of distancing always\nflattens the curve. Third, both an increase in the infection rate as well as an\nincrease in the cost of distancing increase the size of the epidemic. Our\nresults have important implications on the modeling of interventions. Imposing\nrestrictions on the infection rate has qualitatively different effects on the\ntrajectory of the epidemics than imposing assumptions on the cost of\ndistancing. The interventions that affect interactions rather than the\ntransmission rate should, therefore, be modeled as changes in the cost of\ndistancing.\n"
    },
    {
        "paper_id": 2103.006,
        "authors": "Henry Hanifan, Ben Watson, John Cartlidge, Dave Cliff",
        "title": "Time Matters: Exploring the Effects of Urgency and Reaction Speed in\n  Automated Traders",
        "comments": "22 pages. To be published in A. P. Rocha et al. (Eds.), ICAART 2020,\n  LNAI 12613, 2021. arXiv admin note: substantial text overlap with\n  arXiv:1912.02775",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-71158-0_7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider issues of time in automated trading strategies in simulated\nfinancial markets containing a single exchange with public limit order book and\ncontinuous double auction matching. In particular, we explore two effects: (i)\nreaction speed - the time taken for trading strategies to calculate a response\nto market events; and (ii) trading urgency - the sensitivity of trading\nstrategies to approaching deadlines. Much of the literature on trading agents\nfocuses on optimising pricing strategies only and ignores the effects of time,\nwhile real-world markets continue to experience a race to zero latency, as\nautomated trading systems compete to quickly access information and act in the\nmarket ahead of others. We demonstrate that modelling reaction speed can\nsignificantly alter previously published results, with simple strategies such\nas SHVR outperforming more complex adaptive algorithms such as AA. We also show\nthat adding a pace parameter to ZIP traders (ZIP-Pace, or ZIPP) can create a\nsense of urgency that significantly improves profitability.\n"
    },
    {
        "paper_id": 2103.0068,
        "authors": "Anne de Bortoli and Zoi Christoforou",
        "title": "Consequential LCA for territorial and multimodal transportation\n  policies: method and application to the free-floating e-scooter disruption in\n  Paris",
        "comments": null,
        "journal-ref": "Journal of Cleaner Production, Volume 273, 2020, 122898",
        "doi": "10.1016/j.jclepro.2020.122898",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The indirect environmental impacts of transport disruptions in urban mobility\nare frequently overlooked due to a lack of appropriate assessment methods.\nConsequential Life Cycle Assessment (CLCA) is a method to capture the\nenvironmental consequences of the entire cause and effect chain of these\ndisruptions but has never been adapted to transportat disruption at the city\nscale. This paper proposes a mathematical formalization of CLCA applied to a\nterritorial mobility change. The method is applied to quantify the impact on\nclimate change of the breakthrough of free-floating e-scooters (FFES) in Paris.\nA FFES user survey is conducted to estimate the modal shifts due to FFES. Trip\nsubstitutions from all the Parisian modes concerned are considered - personal\nor shared bicycles and motor scooters, private car, taxi and ride-hailing, bus,\nstreetcar, metro and RER (the Paris metropolitan area mass rapid transit\nsystem). All these Parisian modes are assessed for the first time using LCA.\nFinal results estimate that over one year, the FFES generated an extra thirteen\nthousand tons of CO2eq under an assumption of one million users, mainly due to\nmajor shifts coming from lower-emitting modes (60% from the metro and the RER,\n22% from active modes). Recommendations are given to enhance their carbon\nfootprint. A scenario analysis shows that increasing the lifetime mileage is\ninsufficient to get a positive balance: reducing drastically servicing\nemissions is also required. A sensitivity analysis switching the French\nelectricity mix for eleven other country mixes suggests a better climate change\neffect of the FFES in similar metropolitan areas with higher electricity carbon\nintensity, such as in Germany and China. Finally, the novelty and the limits of\nthe method are discussed, as well as the results and the role of e-scooters,\nmicromobility, and shared vehicles towards a sustainable mobility.\n"
    },
    {
        "paper_id": 2103.00721,
        "authors": "Geoffrey Ducournau",
        "title": "Stock market's physical properties description based on Stokes law",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose in this paper to consider the stock market as a physical system\nassimilate to a fluid evolving in a macroscopic space subject to a Force that\ninfluences its movement over time where this last is arising from the collision\nbetween the supply and the demand of Financial agents. In fluid mechanics, this\nForce also results from the collisions of fluid molecules led by its physical\nproperty such as density, viscosity, and surface tension. The purpose of this\narticle is to show that the dynamism of the stock market behavior can be\nexplained qualitatively and quantitatively by considering the supply & demand\ncollision as the result of Financial agents physical properties defined by\nStokes Law. The first objective of this article is to show theoretically that\nfluid mechanics equations can be used to describe stock market physical\nproperties. The second objective based on the knowledge of stock market\nphysical properties is to propose an Econophysics analog of the stock market\nviscosity and Reynolds number to measure stock market conditions, whether\nlaminar, transitory, or turbulent. The Reynolds Number defined in this way can\nbe applied in research into the study and classification of stock market\ndynamics phases through for instance the creation of Econophysics analog of\nModdy diagram, this last could be seen as a physical way to quantify asset and\nstock index idiosyncratic risk. The last objective is to present evidence from\na computer simulation that the stock market behavior can be a priori, and\nposteriori explained by physical properties (viscosity & density) quantifiable\nby fluid mechanics law (Stokes law) and measurable with the stock market\nReynolds Number.\n"
    },
    {
        "paper_id": 2103.00734,
        "authors": "Ratul Das Chaudhury, Birendra Rai, Liang Choon Wang, Dyuti Banerjee",
        "title": "Welfare v. Consent: On the Optimal Penalty for Harassment",
        "comments": "Needs some changes",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The economic approach to determine optimal legal policies involves maximizing\na social welfare function. We propose an alternative: a consent-approach that\nseeks to promote consensual interactions and deter non-consensual interactions.\nThe consent-approach does not rest upon inter-personal utility comparisons or\nvalue judgments about preferences. It does not require any additional\ninformation relative to the welfare-approach. We highlight the contrast between\nthe welfare-approach and the consent-approach using a stylized model inspired\nby seminal cases of harassment and the #MeToo movement. The social welfare\nmaximizing penalty for harassment in our model can be zero under the\nwelfare-approach but not under the consent-approach.\n"
    },
    {
        "paper_id": 2103.00766,
        "authors": "Ghurumuruhan Ganesan",
        "title": "Computing Prices for Target Profits in Contracts",
        "comments": "Accepted for publication in 7th International Conference on\n  Mathematics and Computing, ICMC (2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Price discrimination for maximizing expected profit is a well-studied concept\nin economics and there are various methods that achieve the maximum given the\nuser type distribution and the budget constraints. In many applications,\nparticularly with regards to engineering and computing, it is often the case\nthan the user type distribution is unknown or not accurately known. In this\npaper, we therefore propose and study a mathematical framework for price\ndiscrimination with \\emph{target} profits under the contract-theoretic model.\nWe first consider service providers with a given user type profile and\ndetermine sufficient conditions for achieving a target profit. Our proof is\nconstructive in that it also provides a method to compute the quality-price tag\nmenu. Next we consider a dual scenario where the offered service qualities are\npredetermined and describe an iterative method to obtain nominal demand values\nthat best match the qualities offered by the service provider while achieving a\ntarget profit-user satisfaction margin. We also illustrate our methods with\ndesign examples in both cases.\n"
    },
    {
        "paper_id": 2103.00788,
        "authors": "Geoffrey Ducournau",
        "title": "Statistical mechanics and Bayesian Inference addressed to the Osborne\n  Paradox",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  One of the greatest contributors of the 20th century among all academician in\nthe field of statistical finance, M. F. M. Osborne published in 1956 [6] an\nessential paper and proposed to treat the question of stock market motion\nthrough the prism of both the Law of Weber-Fechner [1, 4] and the branch of\nphysics developed by James Clerk Maxwell, Ludwig Boltzmann and Josiah Willard\nGibbs [3, 5] namely the statistical mechanics. He proposed an improvement of\nthe known research made by his predecessor Louis Jean-Baptiste Alphonse\nBachelier, by not considering the arithmetic changes of stock prices as means\nof statistical measurement, but by drawing on the Weber-Fechner Law, to treat\nthe changes of prices. Osborne emphasized that as in statistical mechanics, the\nprobability distribution of the steady-state of subjective change in prices is\ndetermined by the condition of maximum probability, a statement close to the\nGibbs distribution conditions. However, Osborne also admitted that the\nempirical observation of the probability distribution of logarithmic changes of\nstock prices was emphasizing obvious asymmetries and consequently could not\nperfectly confirm his prior theory. The purpose of this paper is to propose an\nexplanation to what we could call the Osborne paradox and then address an\nalternative approach via Bayesian inference regarding the description of the\nprobability distribution of changes in logarithms of prices that was\nthenceforth under the prism of frequentist inference. We show that the stock\nmarket returns are locally described by equilibrium statistical mechanics with\nconserved statistics variables, whereas globally there is yet other statistics\nwith persistent flowing variables that can be effectively described by a\nsuperposition of several statistics on different time scales, namely, a\nsuperstatistics.\n"
    },
    {
        "paper_id": 2103.00837,
        "authors": "Maximilien Germain (EDF, LPSM, EDF R&D), Huy\\^en Pham (LPSM, FiME\n  Lab), Xavier Warin (EDF, FiME Lab, EDF R&D)",
        "title": "Rate of convergence for particle approximation of PDEs in Wasserstein\n  space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove a rate of convergence for the $N$-particle approximation of a\nsecond-order partial differential equation in the space of probability\nmeasures, like the Master equation or Bellman equation of mean-field control\nproblem under common noise. The rate is of order $1/N$ for the pathwise error\non the solution $v$ and of order $1/\\sqrt{N}$ for the $L^2$-error on its\n$L$-derivative $\\partial_\\mu v$. The proof relies on backward stochastic\ndifferential equations techniques.\n"
    },
    {
        "paper_id": 2103.00838,
        "authors": "Maximilien Germain (EDF, LPSM (UMR_8001), EDF R&D, EDF R&D OSIRIS),\n  Mathieu Lauri\\`ere (ORFE), Huy\\^en Pham (LPSM (UMR\\_8001), FiME Lab, CREST),\n  Xavier Warin (EDF, FiME Lab, EDF R&D, EDF R&D OSIRIS)",
        "title": "DeepSets and their derivative networks for solving symmetric PDEs",
        "comments": null,
        "journal-ref": "Journal of Scientific Computing, Springer Verlag, In press",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning methods for solving nonlinear partial differential equations\n(PDEs) are hot topical issues, and different algorithms proposed in the\nliterature show efficient numerical approximation in high dimension. In this\npaper, we introduce a class of PDEs that are invariant to permutations, and\ncalled symmetric PDEs. Such problems are widespread, ranging from cosmology to\nquantum mechanics, and option pricing/hedging in multi-asset market with\nexchangeable payoff. Our main application comes actually from the particles\napproximation of mean-field control problems. We design deep learning\nalgorithms based on certain types of neural networks, named PointNet and\nDeepSet (and their associated derivative networks), for computing\nsimultaneously an approximation of the solution and its gradient to symmetric\nPDEs. We illustrate the performance and accuracy of the PointNet/DeepSet\nnetworks compared to classical feedforward ones, and provide several numerical\nresults of our algorithm for the examples of a mean-field systemic risk,\nmean-variance problem and a min/max linear quadratic McKean-Vlasov control\nproblem.\n"
    },
    {
        "paper_id": 2103.00905,
        "authors": "Yanhong Chen, Zachary Feinstein",
        "title": "Set-Valued Dynamic Risk Measures for Processes and Vectors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relationship between set-valued risk measures for processes and vectors\non the optional filtration is investigated. The equivalence of risk measures\nfor processes and vectors and the equivalence of their penalty function\nformulations are provided. In contrast with scalar risk measures, this\nequivalence requires an augmentation of the set-valued risk measures for\nprocesses. We utilize this result to deduce a new dual representation for risk\nmeasures for processes in the set-valued framework. Finally, the equivalence of\nmultiportfolio time consistency between set-valued risk measures for processes\nand vectors is provided; to accomplish this, an augmented definition for\nmultiportfolio time consistency of set-valued risk measures for processes is\nproposed.\n"
    },
    {
        "paper_id": 2103.00949,
        "authors": "Branka Hadji Misheva, Joerg Osterrieder, Ali Hirsa, Onkar Kulkarni,\n  Stephen Fung Lin",
        "title": "Explainable AI in Credit Risk Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial Intelligence (AI) has created the single biggest technology\nrevolution the world has ever seen. For the finance sector, it provides great\nopportunities to enhance customer experience, democratize financial services,\nensure consumer protection and significantly improve risk management. While it\nis easier than ever to run state-of-the-art machine learning models, designing\nand implementing systems that support real-world finance applications have been\nchallenging. In large part because they lack transparency and explainability\nwhich are important factors in establishing reliable technology and the\nresearch on this topic with a specific focus on applications in credit risk\nmanagement. In this paper, we implement two advanced post-hoc model agnostic\nexplainability techniques called Local Interpretable Model Agnostic\nExplanations (LIME) and SHapley Additive exPlanations (SHAP) to machine\nlearning (ML)-based credit scoring models applied to the open-access data set\noffered by the US-based P2P Lending Platform, Lending Club. Specifically, we\nuse LIME to explain instances locally and SHAP to get both local and global\nexplanations. We discuss the results in detail and present multiple comparison\nscenarios by using various kernels available for explaining graphs generated\nusing SHAP values. We also discuss the practical challenges associated with the\nimplementation of these state-of-art eXplainabale AI (XAI) methods and document\nthem for future reference. We have made an effort to document every technical\naspect of this research, while at the same time providing a general summary of\nthe conclusions.\n"
    },
    {
        "paper_id": 2103.01123,
        "authors": "Justo Puerto, Federica Ricca, Mois\\'es Rodr\\'iguez-Madrena, Andrea\n  Scozzari",
        "title": "A combinatorial optimization approach to scenario filtering in portfolio\n  selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent studies stressed the fact that covariance matrices computed from\nempirical financial time series appear to contain a high amount of noise. This\nmakes the classical Markowitz Mean-Variance Optimization model unable to\ncorrectly evaluate the performance associated to selected portfolios. Since the\nMarkowitz model is still one of the most used practitioner-oriented tool,\nseveral filtering methods have been proposed in the literature to fix the\nproblem. Among them, the two most promising ones refer to the Random Matrix\nTheory or to the Power Mapping strategy. The basic idea of these methods is to\ntransform the correlation matrix maintaining the Mean-Variance Optimization\nmodel. However, experimental analysis shows that these two strategies are not\nadequately effective when applied to real financial datasets.\n  In this paper we propose an alternative filtering method based on\nCombinatorial Optimization. We advance a new Mixed Integer Quadratic\nProgramming model to filter those observations that may influence the\nperformance of a portfolio in the future. We discuss the properties of this new\nmodel and we test it on some real financial datasets. We compare the\nout-of-sample performance of our portfolios with the one of the portfolios\nprovided by the two above mentioned alternative strategies. We show that our\nmethod outperforms them. Although our model can be solved efficiently with\nstandard optimization solvers the computational burden increases for large\ndatasets. To overcome this issue we also propose a heuristic procedure that\nempirically showed to be both efficient and effective.\n"
    },
    {
        "paper_id": 2103.0134,
        "authors": "Ayten Kahya, Bhaskar Krishnamachari, Seokgu Yun",
        "title": "Reducing the Volatility of Cryptocurrencies -- A Survey of Stablecoins",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the wake of financial crises, stablecoins are gaining adoption among\ndigital currencies. We discuss how stablecoins help reduce the volatility of\ncryptocurrencies by surveying different types of stablecoins and their\nstability mechanisms. We classify different approaches to stablecoins in three\nmain categories i) fiat or asset backed, ii) crypto-collateralized and iii)\nalgorithmic stablecoins, giving examples of concrete projects in each class. We\nassess the relative tradeoffs between the different approaches. We also discuss\nchallenges associated with the future of stablecoins and their adoption, their\nadoption and point out future research directions.\n"
    },
    {
        "paper_id": 2103.01558,
        "authors": "Olivier Accominotti (LSE), Delio Lucena-Piquero (LEREPS), Stefano\n  Ugolini (LEREPS)",
        "title": "The Origination and Distribution of Money Market Instruments: Sterling\n  Bills of Exchange during the First Globalization",
        "comments": "The Economic History Review, Wiley, In press",
        "journal-ref": null,
        "doi": "10.1111/ehr.13049",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a detailed analysis of how liquid money market\ninstruments -- sterling bills of exchange -- were produced during the first\nglobalisation. We rely on a unique data set that reports systematic information\non all 23,493 bills re-discounted by the Bank of England in the year 1906.\nUsing descriptive statistics and network analysis, we reconstruct the complete\nnetwork of linkages between agents involved in the origination and distribution\nof these bills. Our analysis reveals the truly global dimension of the London\nbill market before the First World War and underscores the crucial role played\nby London intermediaries (acceptors and discounters) in overcoming information\nasymmetries between borrowers and lenders on this market. The complex\nindustrial organisation of the London money market ensured that risky private\ndebts could be transformed into extremely liquid and safe monetary instruments\ntraded throughout the global financial system.\n"
    },
    {
        "paper_id": 2103.0157,
        "authors": "Eudald Romo and Luis Ortiz-Gracia",
        "title": "SWIFT calibration of the Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work, the European option pricing SWIFT method is extended for\nHeston model calibration. The computation of the option price gradient is\nsimplified thanks to the knowledge of the characteristic function in closed\nform. The proposed calibration machinery appears to be extremely fast, in\nparticular for a single expiry and multiples strikes, outperforming the\nstate-of-the-art method we compare with. Further, the a priori knowledge of\nSWIFT parameters makes possible a reliable and practical implementation of the\npresented calibration method. A wide set of stress, speed and convergence\nnumerical experiments is carried out, with deep in-the-money, at-the-money and\ndeep out-of-the-money options for very short and very long maturities.\n"
    },
    {
        "paper_id": 2103.01577,
        "authors": "Sandrine G\\\"umbel and Thorsten Schmidt",
        "title": "Defaultable term structures driven by semimartingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market with a term structure of credit risky bonds in the\nsingle-name case. We aim at minimal assumptions extending existing results in\nthis direction: first, the random field of forward rates is driven by a general\nsemimartingale. Second, the Heath-Jarrow-Morton approach is extended with an\nadditional component capturing those future jumps in the term structure which\nare visible from the current time. Third, the associated recovery scheme is as\ngeneral as possible, it is only assumed to be non-increasing. In this general\nsetting we derive generalized drift conditions which characterize when a given\nmeasure is a local martingale measure, thus yielding no asymptotic free lunch\nwith vanishing risk (NAFLVR), the right notion for this large financial market\nto be free of arbitrage.\n"
    },
    {
        "paper_id": 2103.01669,
        "authors": "Rolando Gonzales Martinez",
        "title": "How good is good? Probabilistic benchmarks and nanofinance+",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Benchmarks are standards that allow to identify opportunities for improvement\namong comparable units. This study suggests a 2-step methodology for\ncalculating probabilistic benchmarks in noisy data sets: (i) double-hyperbolic\nundersampling filters the noise of key performance indicators (KPIs), and (ii)\na relevance vector machine estimates probabilistic benchmarks with denoised\nKPIs. The usefulness of the methods is illustrated with an application to a\ndatabase of nano-finance+. The results indicate that-in the case of\nnano-finance groups-a higher discrimination power is obtained with variables\nthat capture the macro-economic environment of the country where a group\noperates. Also, the estimates show that groups operating in rural regions have\ndifferent probabilistic benchmarks, compared to groups in urban and peri-urban\nareas.\n"
    },
    {
        "paper_id": 2103.0167,
        "authors": "Zijian Shi, Yu Chen, John Cartlidge",
        "title": "The LOB Recreation Model: Predicting the Limit Order Book from TAQ\n  History Using an Ordinary Differential Equation Recurrent Neural Network",
        "comments": "12 pages, preprint accepted for publication in the 35th AAAI\n  Conference on Artificial Intelligence (AAAI-2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an order-driven financial market, the price of a financial asset is\ndiscovered through the interaction of orders - requests to buy or sell at a\nparticular price - that are posted to the public limit order book (LOB).\nTherefore, LOB data is extremely valuable for modelling market dynamics.\nHowever, LOB data is not freely accessible, which poses a challenge to market\nparticipants and researchers wishing to exploit this information. Fortunately,\ntrades and quotes (TAQ) data - orders arriving at the top of the LOB, and\ntrades executing in the market - are more readily available. In this paper, we\npresent the LOB recreation model, a first attempt from a deep learning\nperspective to recreate the top five price levels of the LOB for small-tick\nstocks using only TAQ data. Volumes of orders sitting deep in the LOB are\npredicted by combining outputs from: (1) a history compiler that uses a Gated\nRecurrent Unit (GRU) module to selectively compile prediction relevant quote\nhistory; (2) a market events simulator, which uses an Ordinary Differential\nEquation Recurrent Neural Network (ODE-RNN) to simulate the accumulation of net\norder arrivals; and (3) a weighting scheme to adaptively combine the\npredictions generated by (1) and (2). By the paradigm of transfer learning, the\nsource model trained on one stock can be fine-tuned to enable application to\nother financial assets of the same class with much lower demand on additional\ndata. Comprehensive experiments conducted on two real world intraday LOB\ndatasets demonstrate that the proposed model can efficiently recreate the LOB\nwith high accuracy using only TAQ data as input.\n"
    },
    {
        "paper_id": 2103.01775,
        "authors": "Shota Imaki, Kentaro Imajo, Katsuya Ito, Kentaro Minami, Kei Nakagawa",
        "title": "No-Transaction Band Network: A Neural Network Architecture for Efficient\n  Deep Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3905/jfds.2023.1.125",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep hedging (Buehler et al. 2019) is a versatile framework to compute the\noptimal hedging strategy of derivatives in incomplete markets. However, this\noptimal strategy is hard to train due to action dependence, that is, the\nappropriate hedging action at the next step depends on the current action. To\novercome this issue, we leverage the idea of a no-transaction band strategy,\nwhich is an existing technique that gives optimal hedging strategies for\nEuropean options and the exponential utility. We theoretically prove that this\nstrategy is also optimal for a wider class of utilities and derivatives\nincluding exotics. Based on this result, we propose a no-transaction band\nnetwork, a neural network architecture that facilitates fast training and\nprecise evaluation of the optimal hedging strategy. We experimentally\ndemonstrate that for European and lookback options, our architecture quickly\nattains a better hedging strategy in comparison to a standard feed-forward\nnetwork.\n"
    },
    {
        "paper_id": 2103.01812,
        "authors": "Augusto Cerqua, Roberta Di Stefano, Marco Letta, Sara Miccoli",
        "title": "Was there a COVID-19 harvesting effect in Northern Italy?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the possibility of a harvesting effect, i.e. a temporary\nforward shift in mortality, associated with the COVID-19 pandemic by looking at\nthe excess mortality trends of an area that registered one of the highest death\ntolls in the world during the first wave, Northern Italy. We do not find any\nevidence of a sizable COVID-19 harvesting effect, neither in the summer months\nafter the slowdown of the first wave nor at the beginning of the second wave.\nAccording to our estimates, only a minor share of the total excess deaths\ndetected in Northern Italian municipalities over the entire period under\nscrutiny (February - November 2020) can be attributed to an anticipatory role\nof COVID-19. A slightly higher share is detected for the most severely affected\nareas (the provinces of Bergamo and Brescia, in particular), but even in these\nterritories, the harvesting effect can only account for less than 20% of excess\ndeaths. Furthermore, the lower mortality rates observed in these areas at the\nbeginning of the second wave may be due to several factors other than a\nharvesting effect, including behavioral change and some degree of temporary\nherd immunity. The very limited presence of short-run mortality displacement\nrestates the case for containment policies aimed at minimizing the health\nimpacts of the pandemic.\n"
    },
    {
        "paper_id": 2103.01824,
        "authors": "Rajkumar Byahut, Sourish Dutta, Chidambaran G. Iyer, Manikantha\n  Nataraj",
        "title": "Commentary on World Development Report 2020: Trading for Development in\n  the Age of Global Value Chains",
        "comments": "This commentary is the outcome of a panel discussion held at CDS on\n  January 17, 2020. We are grateful to Prof. Sunil Mani, Prof. Sudip Chaudhuri\n  and other participants at the panel discussion for their comments which has\n  benefited this commentary. We are solely responsible for any errors that\n  remain",
        "journal-ref": "Centre for Development Studies, Thiruvananthapuram, Kerala, India\n  (March 2020)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The importance of trade to an economy needs no emphasis. You sell products or\nservices that you are competitive at and buy those where you are not.\nExperience of countries such as South Korea and China demonstrate that\nresources required for development can be garnered through trade; thus,\nmotivating many countries to embrace trade as a means for development.\nSimultaneously, emergence of 'Global Value Chain' or 'GVC' as they are\npopularly known has changed the way we trade. Though the concept of GVC was\nintroduced in the early 2000s, there are examples of global value chains before\nthe 1980s. However, the scale of the phenomenon and the way in which\ntechnological change, by lowering trade costs, has allowed fragmentation of\nproduction was not possible before (Hernandez et al., 2014). In this context,\nthe World Bank has recently published its 'World Development Report 2020:\nTrading for Development in the Age of Global Value Chains' (WDR). The report\nprescribes that GVCs still offer developing countries a clear path to progress\nand that developing countries can achieve better outcomes by pursuing\nmarket-oriented reforms specific to their stage of development.\n"
    },
    {
        "paper_id": 2103.01907,
        "authors": "Nikita Kozodoi, Johannes Jacob, Stefan Lessmann",
        "title": "Fairness in Credit Scoring: Assessment, Implementation and Profit\n  Implications",
        "comments": "Accepted to European Journal of Operational Research",
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2021.06.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rise of algorithmic decision-making has spawned much research on fair\nmachine learning (ML). Financial institutions use ML for building risk\nscorecards that support a range of credit-related decisions. Yet, the\nliterature on fair ML in credit scoring is scarce. The paper makes three\ncontributions. First, we revisit statistical fairness criteria and examine\ntheir adequacy for credit scoring. Second, we catalog algorithmic options for\nincorporating fairness goals in the ML model development pipeline. Last, we\nempirically compare different fairness processors in a profit-oriented credit\nscoring context using real-world data. The empirical results substantiate the\nevaluation of fairness measures, identify suitable options to implement fair\ncredit scoring, and clarify the profit-fairness trade-off in lending decisions.\nWe find that multiple fairness criteria can be approximately satisfied at once\nand recommend separation as a proper criterion for measuring the fairness of a\nscorecard. We also find fair in-processors to deliver a good balance between\nprofit and fairness and show that algorithmic discrimination can be reduced to\na reasonable level at a relatively low cost. The codes corresponding to the\npaper are available on GitHub.\n"
    },
    {
        "paper_id": 2103.01934,
        "authors": "Christian Bayer, Martin Eigel, Leon Sallandt, Philipp Trunschke",
        "title": "Pricing high-dimensional Bermudan options with hierarchical tensor\n  formats",
        "comments": "26 pages, 3 figures, 5 tables, added affiliations and update\n  acknowledgements",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An efficient compression technique based on hierarchical tensors for popular\noption pricing methods is presented. It is shown that the \"curse of\ndimensionality\" can be alleviated for the computation of Bermudan option prices\nwith the Monte Carlo least-squares approach as well as the dual martingale\nmethod, both using high-dimensional tensorized polynomial expansions. This\ndiscretization allows for a simple and computationally cheap evaluation of\nconditional expectations. Complexity estimates are provided as well as a\ndescription of the optimization procedures in the tensor train format.\nNumerical experiments illustrate the favourable accuracy of the proposed\nmethods. The dynamical programming method yields results comparable to recent\nNeural Network based methods.\n"
    },
    {
        "paper_id": 2103.02016,
        "authors": "M. Avellaneda, T. N. Li, A. Papanicolaou, G. Wang",
        "title": "Trading Signals In VIX Futures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new approach for trading VIX futures. We assume that the term\nstructure of VIX futures follows a Markov model. Our trading strategy selects a\nposition in VIX futures by maximizing the expected utility for a day-ahead\nhorizon given the current shape and level of the term structure.\nComputationally, we model the functional dependence between the VIX futures\ncurve, the VIX futures positions, and the expected utility as a deep neural\nnetwork with five hidden layers. Out-of-sample backtests of the VIX futures\ntrading strategy suggest that this approach gives rise to reasonable portfolio\nperformance, and to positions in which the investor will be either long or\nshort VIX futures contracts depending on the market environment.\n"
    },
    {
        "paper_id": 2103.02272,
        "authors": "Niels-Jakob Harbo Hansen and Hans Henrik Sievertsen",
        "title": "Measuring Vacancies: Firm-level Evidence from Two Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using firm-level survey- and register-data for both Sweden and Denmark we\nshow systematic mis-measurement in both vacancy measures. While the\nregister-based measure on the aggregate constitutes a quarter of the\nsurvey-based measure, the latter is not a super-set of the former. To obtain\nthe full set of unique vacancies in these two databases, the number of survey\nvacancies should be multiplied by approximately 1.2. Importantly, this\nadjustment factor varies over time and across firm characteristics. Our\nfindings have implications for both the search-matching literature and policy\nanalysis based on vacancy measures: Observed changes in vacancies can be an\noutcome of changes in mis-measurement, and are not necessarily changes in the\nactual number of vacancies.\n"
    },
    {
        "paper_id": 2103.02331,
        "authors": "Vicky Henderson, Saul Jacka and Ruiqi Liu",
        "title": "The Support and Resistance Line Method: An Analysis via Optimal Stopping",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a mathematical model capturing the support/resistance line method (a\ntechnique in technical analysis) where the underlying stock price transitions\nbetween two states of nature in a path-dependent manner. For optimal stopping\nproblems with respect to a general class of reward functions and dynamics,\nusing probabilistic methods, we show that the value function is $C^1$ and\nsolves a general free boundary problem. Moreover, for a wide range of\nutilities, we prove that the best time to buy and sell the stock is obtained by\nsolving free boundary problems corresponding to two linked optimal stopping\nproblems. We use this to numerically compute optimal trading strategies for\nseveral types of dynamics and varying degrees of relative risk aversion. We\nthen compare the strategies with the standard trading rule to investigate the\nviability of this form of technical analysis.\n"
    },
    {
        "paper_id": 2103.02345,
        "authors": "Dar\\'io Blanco-Fern\\'andez, Stephan Leitner, Alexandra Rausch",
        "title": "Multi-level Adaptation of Distributed Decision-Making Agents in Complex\n  Task Environments",
        "comments": "12 pages, 4 figures, submitted to the 22nd International Workshop on\n  Multi-Agent-Based Simulation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To solve complex tasks, individuals often autonomously organize in teams.\nExamples of complex tasks include disaster relief rescue operations or project\ndevelopment in consulting. The teams that work on such tasks are adaptive at\nmultiple levels: First, by autonomously choosing the individuals that jointly\nperform a specific task, the team itself adapts to the complex task at hand,\nwhereby the composition of teams might change over time. We refer to this\nprocess as self-organization. Second, the members of a team adapt to the\ncomplex task environment by learning. There is, however, a lack of extensive\nresearch on multi-level adaptation processes that consider self-organization\nand individual learning as simultaneous processes in the field of management\nscience. We introduce an agent-based model based on the NK-framework to study\nthe effects of simultaneous multi-level adaptation on a team's performance. We\nimplement the multi-level adaptation process by a second-price auction\nmechanism for self-organization at the team level. Adaptation at the individual\nlevel follows an autonomous learning mechanism. Our preliminary results suggest\nthat, depending on the task's complexity, different configurations of\nindividual and collective adaptation can be associated with higher overall task\nperformance. Low complex tasks favour high individual and collective\nadaptation, while moderate individual and collective adaptation is associated\nwith better performance in case of moderately complex tasks. For highly complex\ntasks, the results suggest that collective adaptation is harmful to\nperformance.\n"
    },
    {
        "paper_id": 2103.02395,
        "authors": "Piotr Tomasz Makowski and Yuya Kajikawa",
        "title": "Automation-driven innovation management? Toward\n  Innovation-Automation-Strategy cycle",
        "comments": "24 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There is a resurging interest in automation because of rapid progress of\nmachine learning and AI. In our perspective, innovation is not an exemption\nfrom their expansion. This situation gives us an opportunity to reflect on a\ndirection of future innovation studies. In this conceptual paper, we propose a\nframework of innovation process by exploiting the concept of unit process.\nDeploying it in the context of automation, we indicate the important aspects of\ninnovation process, i.e. human, organizational, and social factors. We also\nhighlight the cognitive and interactive underpinnings at micro- and\nmacro-levels of the process. We propose to embrace all those factors in what we\ncall Innovation-Automation-Strategy cycle (IAS). Implications of IAS for future\nresearch are also put forward.\n  Keywords: innovation, automation of innovation, unit process,\ninnovation-automation-strategy cycle\n"
    },
    {
        "paper_id": 2103.02526,
        "authors": "Zhu Liu, Zhu Deng, Philippe Ciais, Jianguang Tan, Biqing Zhu, Steven\n  J. Davis, Robbie Andrew, Olivier Boucher, Simon Ben Arous, Pep Canadel, Xinyu\n  Dou, Pierre Friedlingstein, Pierre Gentine, Rui Guo, Chaopeng Hong, Robert B.\n  Jackson, Daniel M. Kammen, Piyu Ke, Corinne Le Quere, Crippa Monica, Greet\n  Janssens-Maenhout, Glen Peters, Katsumasa Tanaka, Yilong Wang, Bo Zheng,\n  Haiwang Zhong, Taochun Sun, Hans Joachim Schellnhuber",
        "title": "Global Daily CO$_2$ emissions for the year 2020",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41561-022-00965-8",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The diurnal cycle CO$_2$ emissions from fossil fuel combustion and cement\nproduction reflect seasonality, weather conditions, working days, and more\nrecently the impact of the COVID-19 pandemic. Here, for the first time we\nprovide a daily CO$_2$ emission dataset for the whole year of 2020 calculated\nfrom inventory and near-real-time activity data (called Carbon Monitor project:\nhttps://carbonmonitor.org). It was previously suggested from preliminary\nestimates that did not cover the entire year of 2020 that the pandemics may\nhave caused more than 8% annual decline of global CO$_2$ emissions. Here we\nshow from detailed estimates of the full year data that the global reduction\nwas only 5.4% (-1,901 MtCO$_2$, ). This decrease is 5 times larger than the\nannual emission drop at the peak of the 2008 Global Financial Crisis. However,\nglobal CO$_2$ emissions gradually recovered towards 2019 levels from late April\nwith global partial re-opening. More importantly, global CO$_2$ emissions even\nincreased slightly by +0.9% in December 2020 compared with 2019, indicating the\ntrends of rebound of global emissions. Later waves of COVID-19 infections in\nlate 2020 and corresponding lockdowns have caused further CO$_2$ emissions\nreductions particularly in western countries, but to a much smaller extent than\nthe declines in the first wave. That even substantial world-wide lockdowns of\nactivity led to a one-time decline in global CO$_2$ emissions of only 5.4% in\none year highlights the significant challenges for climate change mitigation\nthat we face in the post-COVID era. These declines are significant, but will be\nquickly overtaken with new emissions unless the COVID-19 crisis is utilized as\na break-point with our fossil-fuel trajectory, notably through policies that\nmake the COVID-19 recovery an opportunity to green national energy and\ndevelopment plans.\n"
    },
    {
        "paper_id": 2103.02658,
        "authors": "Devan Taylor",
        "title": "The Importance of Funding Space-Based Research",
        "comments": "Dedicated to David Andrew Taylor",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When it comes to conversations about funding, the questions of whether the\nUnited States should be spending its resources on space-based research often\nrears its head. Opponents of the idea tend to share the opinion that the\nresources would be better spent helping citizens on the ground. With an\nestimated homeless population around 562,000 throughout the country, roughly\n39.4 million Americans (12.3% of the population) living below the poverty\nlevel, and 63.1 million tons of food waste per year, it's hard to argue that\nthe United States does not have its share of problems that need to be\naddressed. However, a history of space-based research has proven time and time\nagain to bring forth advances in technology and scientific understanding that\nbenefit humans across the globe and provide crucial protection for life on\nEarth.\n"
    },
    {
        "paper_id": 2103.02754,
        "authors": "Navin Kartik, SangMok Lee, Tianhao Liu, and Daniel Rappoport",
        "title": "Beyond Unbounded Beliefs: How Preferences and Information Interplay in\n  Social Learning",
        "comments": "This is a revision of the September 2023 version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When does society eventually learn the truth, or take the correct action, via\nobservational learning? In a general model of sequential learning over social\nnetworks, we identify a simple condition for learning dubbed excludability.\nExcludability is a joint property of agents' preferences and their information.\nWe develop two classes of preferences and information that jointly satisfy\nexcludability: (i) for a one-dimensional state, preferences with\nsingle-crossing differences and a new informational condition, directionally\nunbounded beliefs; and (ii) for a multi-dimensional state, intermediate\npreferences and subexponential location-shift information. These applications\nexemplify that with multiple states \"unbounded beliefs\" is not only unnecessary\nfor learning, but incompatible with familiar informational structures like\nnormal information. Unbounded beliefs demands that a single agent can identify\nthe correct action. Excludability, on the other hand, only requires that a\nsingle agent must be able to displace any wrong action, even if she cannot take\nthe correct action.\n"
    },
    {
        "paper_id": 2103.02909,
        "authors": "Pramod Kumar Sur",
        "title": "Understanding Vaccine Hesitancy: Empirical Evidence from India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Why do vaccination rates remain low even in countries where long-established\nimmunization programs exist and vaccines are provided for free? We study this\nparadox in the context of India, which contributes to the world's largest pool\nof under-vaccinated children and about one-third of all vaccine-preventable\ndeaths globally. Combining historical records with survey datasets, we examine\nthe Indian government's forced sterilization policy, a short-term aggressive\nfamily planning program implemented between 1976 and 1977. Using multiple\nestimation methods, including an instrumental variable (IV) and a geographic\nregression discontinuity design (RDD) approach, we document that the current\nvaccination completion rate is low in places where forced sterilization was\nhigh. We also explore the heterogeneous effects, mechanisms, and reasons for\nthe mechanism. Finally, we examine the enduring consequence and present\nevidence that places more exposed to forced sterilization have an average 60\npercent higher child mortality rate today. Together, these findings suggest\nthat government policies implemented in the past can have persistent adverse\nimpacts on demand for health-seeking behavior, even if the burden is\nexceedingly high.\n"
    },
    {
        "paper_id": 2103.0292,
        "authors": "Matteo Burzoni, Marco Frittelli, Federico Zorzi",
        "title": "Robust market-adjusted systemic risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we consider a system of financial institutions and study\nsystemic risk measures in the presence of a financial market and in a robust\nsetting, namely, where no reference probability is assigned. We obtain a dual\nrepresentation for convex robust systemic risk measures adjusted to the\nfinancial market and show its relation to some appropriate no-arbitrage\nconditions.\n"
    },
    {
        "paper_id": 2103.02948,
        "authors": "Jonas Al-Hadad and Zbigniew Palmowski",
        "title": "Pricing Perpetual American put options with asset-dependent discounting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main objective of this paper is to present an algorithm of pricing\nperpetual American put options with asset-dependent discounting. The value\nfunction of such an instrument can be described as \\begin{equation*}\nV^{\\omega}_{\\text{A}^{\\text{Put}}}(s) = \\sup_{\\tau\\in\\mathcal{T}}\n\\mathbb{E}_{s}[e^{-\\int_0^\\tau \\omega(S_w) dw} (K-S_\\tau)^{+}], \\end{equation*}\nwhere $\\mathcal{T}$ is a family of stopping times, $\\omega$ is a discount\nfunction and $\\mathbb{E}$ is an expectation taken with respect to a martingale\nmeasure. Moreover, we assume that the asset price process $S_t$ is a geometric\nL\\'evy process with negative exponential jumps, i.e. $S_t = s e^{\\zeta t +\n\\sigma B_t - \\sum_{i=1}^{N_t} Y_i}$. The asset-dependent discounting is\nreflected in the $\\omega$ function, so this approach is a generalisation of the\nclassic case when $\\omega$ is constant. It turns out that under certain\nconditions on the $\\omega$ function, the value function\n$V^{\\omega}_{\\text{A}^{\\text{Put}}}(s)$ is convex and can be represented in a\nclosed form; see Al-Hadad and Palmowski (2021). We provide an option pricing\nalgorithm in this scenario and we present exact calculations for the particular\nchoices of $\\omega$ such that $V^{\\omega}_{\\text{A}^{\\text{Put}}}(s)$ takes a\nsimplified form.\n"
    },
    {
        "paper_id": 2103.03117,
        "authors": "Raden Johannes, Andry Alamsyah",
        "title": "Sales Prediction Model Using Classification Decision Tree Approach For\n  Small Medium Enterprise Based on Indonesian E-Commerce Data",
        "comments": null,
        "journal-ref": "The 6th Seminar & Conference on Business & Technology in ICT\n  Industry, 2015",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growth of internet users in Indonesia gives an impact on many aspects of\ndaily life, including commerce. Indonesian small-medium enterprises took this\nadvantage of new media to derive their activity by the meaning of online\ncommerce. Until now, there is no known practical implementation of how to\npredict their sales and revenue using their historical transaction. In this\npaper, we build a sales prediction model on the Indonesian footwear industry\nusing real-life data crawled on Tokopedia, one of the biggest e-commerce\nproviders in Indonesia. Data mining is a discipline that can be used to gather\ninformation by processing the data. By using the method of classification in\ndata mining, this research will describe patterns of the market and predict the\npotential of the region in the national market commodities. Our approach is\nbased on the classification decision tree. We managed to determine predicted\nthe number of items sold by the viewers, price, and type of shoes.\n"
    },
    {
        "paper_id": 2103.0312,
        "authors": "Andry Alamsyah, Dian Puteri Ramadhani, Farida Titik Kristanti",
        "title": "Event-Based Dynamic Banking Network Exploration for Economic Anomaly\n  Detection",
        "comments": "12 pages, 16 figures, 5 tables",
        "journal-ref": "Journal of Theoretical and Applied Information Technology, Issue\n  7, Vol. 98, page 1089-1100, 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The instability of financial system issues might trigger a bank failure,\nevoke spillovers, and generate contagion effects which negatively impacted the\nfinancial system, ultimately on the economy. This phenomenon is the result of\nthe highly interconnected banking transaction. The banking transactions network\nis considered as a financial architecture backbone. The strong\ninterconnectedness between banks escalates contagion disruption spreading over\nthe banking network and trigger the entire system collapse. This far, the\nfinancial instability is generally detected using macro approach mainly the\nuncontrolled transaction deficits amount and unpaid foreign debt. This research\nproposes financial instability detection in another point of view, through the\nmacro view where the banking network structure are explored globally and micro\nview where focuses on the detailed network patterns called motif. Network\ntriadic motif patterns used as a denomination to detect financial instability.\nThe most related network triadic motif changes related to the instability\nperiod are determined as a detector. We explore the banking network behavior\nunder financial instability phenomenon along with the major religious event in\nIndonesia, Eid al-Fitr. We discover one motif pattern as the financial\ninstability underlying detector. This research helps to support the financial\nsystem stability supervision.\n"
    },
    {
        "paper_id": 2103.03179,
        "authors": "Jeet Agnihotri and Subhankar Mishra",
        "title": "Indian Economy and Nighttime Lights",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Forecasting economic growth of India has been traditionally an uncertain\nexercise. The indicators and factors affecting economic structures and the\nvariables required to model that captures the situation correctly is point of\nconcern. Although the forecast should be specific to the country we are looking\nat however countries do have interlinkages among them. As the time series can\nbe more volatile and sometimes certain variables are unavailable it is harder\nto predict for the developing economies as compared to stable and developed\nnations. However it is very important to have accurate forecasts for economic\ngrowth for successful policy formations. One of the hypothesized indicators is\nthe nighttime lights. Here we aim to look for a relationship between GDP and\nNighttime lights. Specifically we look at the DMSP and VIIRS dataset. We are\nfinding relationship between various measures of economy.\n"
    },
    {
        "paper_id": 2103.03219,
        "authors": "Ateeb Akhter Shah Syed and Kaneez Fatima",
        "title": "The Impact of COVID-19 on Stock Market Volatility in Pakistan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the impact of coronavirus (COVID-19) on stock market\nvolatility (SMV) in Pakistan by controlling the effect of exchange rate,\ninterest rate and government/central bank interventions to combat the pandemic.\nWe used the vector autoregressive (VAR) model over a sample period ranging from\nFebruary 25, 2020 to December 7, 2020. We find that a shock to total daily\ncoronavirus cases in Pakistan lead to a significant increase in SMV. This\nresult is aligned with a vast literature on pandemics and investors uncertainty\nand remains robust to several robustness checks applied in our analysis.\n"
    },
    {
        "paper_id": 2103.03234,
        "authors": "Furkan G\\\"ursoy and Bertan Badur",
        "title": "An Agent-Based Modelling Approach to Brain Drain",
        "comments": "Changes: minor language improvements, copyright notice",
        "journal-ref": null,
        "doi": "10.1109/TCSS.2021.3066074",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The phenomenon of brain drain, that is the emigration of highly skilled\npeople, has many undesirable effects, particularly for developing countries. In\nthis study, an agent-based model is developed to understand the dynamics of\nsuch emigration. We hypothesise that skilled people's emigration decisions are\nbased on several factors including the overall economic and social difference\nbetween the home and host countries, people's ability and capacity to obtain\ngood jobs and start a life abroad, and the barriers of moving abroad.\nFurthermore, the social network of individuals also plays a significant role.\nThe model is validated using qualitative and quantitative pattern matching with\nreal-world observations. Sensitivity and uncertainty analyses are performed in\naddition to several scenario analyses. Linear and random forest response\nsurface models are created to provide quick predictions on the number of\nemigrants as well as to understand the effect sizes of individual parameters.\nOverall, the study provides an abstract model where brain drain dynamics can be\nexplored. Findings from the simulation outputs show that future socioeconomic\nstate of the country is more important than the current state, lack of barriers\nresults in a large number of emigrants, and network effects ensue compounding\neffects on emigration. Upon further development and customisation, future\nversions can assist in the decision-making of social policymakers regarding\nbrain drain.\n"
    },
    {
        "paper_id": 2103.03275,
        "authors": "Josselin Garnier, Jean-Baptiste Gaudemet, Anne Gruz",
        "title": "The Climate Extended Risk Model (CERM)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses estimates of climate risk embedded within a bank credit\nportfolio. The proposed Climate Extended Risk Model (CERM) adapts well known\ncredit risk models and makes it possible to calculate incremental credit losses\non a loan portfolio that are rooted into physical and transition risks. The\npaper provides detailed description of the model hypotheses and steps.\n"
    },
    {
        "paper_id": 2103.033,
        "authors": "Bradley Sturt",
        "title": "A nonparametric algorithm for optimal stopping based on robust\n  optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Optimal stopping is a fundamental class of stochastic dynamic optimization\nproblems with numerous applications in finance and operations management. We\nintroduce a new approach for solving computationally-demanding stochastic\noptimal stopping problems with known probability distributions. The approach\nuses simulation to construct a robust optimization problem that approximates\nthe stochastic optimal stopping problem to any arbitrary accuracy; we then\nsolve the robust optimization problem to obtain near-optimal Markovian stopping\nrules for the stochastic optimal stopping problem.\n  In this paper, we focus on designing algorithms for solving the robust\noptimization problems that approximate the stochastic optimal stopping\nproblems. These robust optimization problems are challenging to solve because\nthey require optimizing over the infinite-dimensional space of all Markovian\nstopping rules. We overcome this challenge by characterizing the structure of\noptimal Markovian stopping rules for the robust optimization problems. In\nparticular, we show that optimal Markovian stopping rules for the robust\noptimization problems have a structure that is surprisingly simple and\nfinite-dimensional. We leverage this structure to develop an exact\nreformulation of the robust optimization problem as a zero-one bilinear program\nover totally unimodular constraints. We show that the bilinear program can be\nsolved in polynomial time in special cases, establish computational complexity\nresults for general cases, and develop polynomial-time heuristics by relating\nthe bilinear program to the maximal closure problem from graph theory.\nNumerical experiments demonstrate that our algorithms for solving the robust\noptimization problems are practical and can outperform state-of-the-art\nsimulation-based algorithms in the context of widely-studied stochastic optimal\nstopping problems from high-dimensional option pricing.\n"
    },
    {
        "paper_id": 2103.03442,
        "authors": "Guannan He, Dharik S. Mallapragada, Abhishek Bose, Clara F. Heuberger,\n  Emre Gen\\c{c}er",
        "title": "Sector coupling via hydrogen to lower the cost of energy system\n  decarbonization",
        "comments": "19 pages, 7 figures",
        "journal-ref": "Energy & Environmental Science, 2021",
        "doi": "10.1039/D1EE00627D",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There is growing interest in hydrogen (H$_2$) use for long-duration energy\nstorage in a future electric grid dominated by variable renewable energy (VRE)\nresources. Modelling the role of H$_2$ as grid-scale energy storage, often\nreferred as \"power-to-gas-to-power (P2G2P)\" overlooks the cost-sharing and\nemission benefits from using the deployed H$_2$ production and storage assets\nto also supply H$_2$ for decarbonizing other end-use sectors where direct\nelectrification may be challenged. Here, we develop a generalized modelling\nframework for co-optimizing energy infrastructure investment and operation\nacross power and transportation sectors and the supply chains of electricity\nand H$_2$, while accounting for spatio-temporal variations in energy demand and\nsupply. Applying this sector-coupling framework to the U.S. Northeast under a\nrange of technology cost and carbon price scenarios, we find a greater value of\npower-to-H$_2$ (P2G) versus P2G2P routes. P2G provides flexible demand\nresponse, while the extra cost and efficiency penalties of P2G2P routes make\nthe solution less attractive for grid balancing. The effects of sector-coupling\nare significant, boosting VRE generation by 12-55% with both increased\ncapacities and reduced curtailments and reducing the total system cost (or\nlevelized costs of energy) by 6-14% under 96% decarbonization scenarios. Both\nthe cost savings and emission reductions from sector coupling increase with\nH$_2$ demand for other end-uses, more than doubling for a 96% decarbonization\nscenario as H$_2$ demand quadraples. Moreover, we found that the deployment of\ncarbon capture and storage is more cost-effective in the H$_2$ sector because\nof the lower cost and higher utilization rate. These findings highlight the\nimportance of using an integrated multi-sector energy system framework with\nmultiple energy vectors in planning energy system decarbonization pathways.\n"
    },
    {
        "paper_id": 2103.03913,
        "authors": "Hyunsu Kim",
        "title": "Deep Hedging, Generative Adversarial Networks, and Beyond",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a potential application of deep learning and artificial\nintelligence in finance, particularly its application in hedging. The major\ngoal encompasses two objectives. First, we present a framework of a direct\npolicy search reinforcement agent replicating a simple vanilla European call\noption and use the agent for the model-free delta hedging. Through the first\npart of this paper, we demonstrate how the RNN-based direct policy search RL\nagents can perform delta hedging better than the classic Black-Scholes model in\nQ-world based on parametrically generated underlying scenarios, particularly\nminimizing tail exposures at higher values of the risk aversion parameter. In\nthe second part of this paper, with the non-parametric paths generated by\ntime-series GANs from multi-variate temporal space, we illustrate its delta\nhedging performance on various values of the risk aversion parameter via the\nbasic RNN-based RL agent introduced in the first part of the paper, showing\nthat we can potentially achieve higher average profits with a rather evident\nrisk-return trade-off. We believe that this RL-based hedging framework is a\nmore efficient way of performing hedging in practice, addressing some of the\ninherent issues with the classic models, providing promising/intuitive hedging\nresults, and rendering a flexible framework that can be easily paired with\nother AI-based models for many other purposes.\n"
    },
    {
        "paper_id": 2103.04111,
        "authors": "Kerstin H\\\"otte, Angelos Theodorakopoulos, Pantelis Koutroumpis",
        "title": "Automation and Taxation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decomposing taxes by source (labor, capital, sales), we analyze the impact of\nautomation on tax revenues and the structure of taxation in 19 EU countries\nduring 1995-2016. Pre-2008, robot diffusion lead to decreasing factor and tax\nincome, and a shift from taxes on capital to goods. ICTs changed the structure\nof taxation from capital to labor, with decreasing employment, but increasing\nwages and labor income. Post-2008, we find an ICT-induced increase in capital\nincome and services, but no effect on taxation from ICT/robots. Overall,\nautomation goes through various phases with heterogeneous economic effects\nwhich impact the amount and structure of taxes. Whether automation erodes\ntaxation depends on the technology and stage of diffusion, and thus concerns\nabout public budgets might be myopic when focusing on the short-run and\nignoring relevant technological trends.\n"
    },
    {
        "paper_id": 2103.04123,
        "authors": "Gaurab Aryal and Manudeep Bhuller and Fabian Lange",
        "title": "Signaling and Employer Learning with Instruments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the use of instruments to identify and estimate private\nand social returns to education within a model of employer learning. What an\ninstrument identifies depends on whether it is hidden from, or transparent\n(i.e., observed) to, the employers. A hidden instrument identifies private\nreturns to education, and a transparent instrument identifies social returns to\neducation. We use variation in compulsory schooling laws across non-central and\ncentral municipalities in Norway to, respectively, construct hidden and\ntransparent instruments. We estimate a private return of 7.9%, of which 70% is\ndue to increased productivity and the remaining 30% is due to signaling.\n"
    },
    {
        "paper_id": 2103.04255,
        "authors": "Sajad Rahimian",
        "title": "The Determinants of Democracy Revisited: An Instrumental Variable\n  Bayesian Model Averaging Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Identifying the real causes of democracy is an ongoing debate. We contribute\nto the literature by examining the robustness of a comprehensive list of 42\npotential determinants of democracy. We take a step forward and employ\nInstrumental Variable Bayesian Model Averaging (IVBMA) method to tackle\nendogeneity explicitly. Using the data of 111 countries, our IVBMA results mark\narable land as the most persistent predictor of democracy with a posterior\ninclusion probability (PIP) of 0.961. Youth population (PIP: 0.893), life\nexpectancy (PIP: 0.839), and GDP per capita (PIP: 0.758) are the next critical\nindependent variables. In a subsample of 80 developing countries, in addition\nto arable land (PIP: 0.919), state fragility proves to be a significant\ndeterminant of democracy (PIP: 0.779).\n"
    },
    {
        "paper_id": 2103.04352,
        "authors": "Guohui Guan, Zongxia Liang, Yi xia",
        "title": "Optimal management of DC pension fund under relative performance ratio\n  and VaR constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the optimal management of defined contribution\n(abbr. DC) pension plan under relative performance ratio and Value-at-Risk\n(abbr. VaR) constraint. Inflation risk is introduced in this paper and the\nfinancial market consists of cash, inflation-indexed zero coupon bond and a\nstock. The goal of the pension manager is to maximize the performance ratio of\nthe real terminal wealth under VaR constraint. An auxiliary process is\nintroduced to transform the original problem into a self-financing problem\nfirst. Combining linearization method, Lagrange dual method, martingale method\nand concavification method, we obtain the optimal terminal wealth under\ndifferent cases. For convex penalty function, there are fourteen cases while\nfor concave penalty function, there are six cases. Besides, when the penalty\nfunction and reward function are both power functions, the explicit forms of\nthe optimal investment strategies are obtained. Numerical examples are shown in\nthe end of this paper to illustrate the impacts of the performance ratio and\nVaR constraint.\n"
    },
    {
        "paper_id": 2103.04375,
        "authors": "G\\'abor Papp, Imre Kondor, Fabio Caccioli",
        "title": "Optimizing Expected Shortfall under an $\\ell_1$ constraint -- an\n  analytic approach",
        "comments": "29 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.3390/e23050523",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expected Shortfall (ES), the average loss above a high quantile, is the\ncurrent financial regulatory market risk measure. Its estimation and\noptimization are highly unstable against sample fluctuations and become\nimpossible above a critical ratio $r=N/T$, where $N$ is the number of different\nassets in the portfolio, and $T$ is the length of the available time series.\nThe critical ratio depends on the confidence level $\\alpha$, which means we\nhave a line of critical points on the $\\alpha-r$ plane. The large fluctuations\nin the estimation of ES can be attenuated by the application of regularizers.\nIn this paper, we calculate ES analytically under an $\\ell_1$ regularizer by\nthe method of replicas borrowed from the statistical physics of random systems.\nThe ban on short selling, i.e. a constraint rendering all the portfolio weights\nnon-negative, is a special case of an asymmetric $\\ell_1$ regularizer. Results\nare presented for the out-of-sample and the in-sample estimator of the\nregularized ES, the estimation error, the distribution of the optimal portfolio\nweights and the density of the assets eliminated from the portfolio by the\nregularizer. It is shown that the no-short constraint acts as a high volatility\ncutoff, in the sense that it sets the weights of the high volatility elements\nto zero with higher probability than those of the low volatility items. This\ncutoff renormalizes the aspect ratio $r=N/T$, thereby extending the range of\nthe feasibility of optimization. We find that there is a nontrivial mapping\nbetween the regularized and unregularized problems, corresponding to a\nrenormalization of the order parameters.\n"
    },
    {
        "paper_id": 2103.04432,
        "authors": "Yuan Hu and W. Brent Lindquist",
        "title": "Portfolio Optimization Constrained by Performance Attribution",
        "comments": "15 pages, 7 figures. Submitted to Journal of Risk and Financial\n  Management",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates performance attribution measures as a basis for\nconstraining portfolio optimization. We employ optimizations that minimize\nexpected tail loss and investigate both asset allocation (AA) and the selection\neffect (SE) as hard constraints on asset weights. The test portfolio consists\nof stocks from the Dow Jones Industrial Average index; the benchmark is an\nequi-weighted portfolio of the same stocks. Performance of the optimized\nportfolios is judged using comparisons of cumulative price and the\nrisk-measures maximum drawdown, Sharpe ratio, and Rachev ratio. The results\nsuggest a positive role in price and risk-measure performance for the\nimposition of constraints on AA and SE, with SE constraints producing the\nlarger performance enhancement.\n"
    },
    {
        "paper_id": 2103.04464,
        "authors": "Anne de Bortoli",
        "title": "Environmental performance of shared micromobility and personal\n  alternatives using integrated modal LCA",
        "comments": null,
        "journal-ref": "Transportation Research Part D: Transport and Environment Volume\n  93, April 2021, 102743",
        "doi": "10.1016/j.trd.2021.102743",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The environmental performance of shared micromobility services compared to\nprivate alternatives has never been assessed using an integrated modal Life\nCycle Assessment (LCA) relying on field data. Such an LCA is conducted on three\nshared micromobility services in Paris - bikes, second-generation e-scooters,\nand e-mopeds - and their private alternatives. Global warming potential,\nprimary energy consumption, and the three endpoint damages are calculated.\nSensitivity analyses on vehicle lifespan, shipping, servicing distance, and\nelectricity mix are conducted. Electric micromobility ranks between active\nmodes and personal ICE modes. Its impacts are globally driven by vehicle\nmanufacturing. Ownership does not affect directly the environmental\nperformance: the vehicle lifetime mileage does. Assessing the sole carbon\nfootprint leads to biased environmental decision-making, as it is not\ncorrelated to the three damages: multicriteria LCA is mandatory to preserve the\nplanet. Finally, a major change of paradigm is needed to eco-design modern\ntransportation policies.\n"
    },
    {
        "paper_id": 2103.04481,
        "authors": "Charles-Albert Lehalle, Eyal Neuman and Segev Shlomov",
        "title": "Phase Transitions in Kyle's Model with Market Maker Profit Incentives",
        "comments": "32 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic game between three types of players: an inside\ntrader, noise traders and a market maker. In a similar fashion to Kyle's model,\nwe assume that the insider first chooses the size of her market-order and then\nthe market maker determines the price by observing the total order-flow\nresulting from the insider and the noise traders transactions. In addition to\nthe classical framework, a revenue term is added to the market maker's\nperformance function, which is proportional to the order flow and to the size\nof the bid-ask spread. We derive the maximizer for the insider's revenue\nfunction and prove sufficient conditions for an equilibrium in the game. Then,\nwe use neural networks methods to verify that this equilibrium holds. We show\nthat the equilibrium state in this model experience interesting phase\ntransitions, as the weight of the revenue term in the market maker's\nperformance function changes. Specifically, the asset price in equilibrium\nexperience three different phases: a linear pricing rule without a spread, a\npricing rule that includes a linear mid-price and a bid-ask spread, and a\nmetastable state with a zero mid-price and a large spread.\n"
    },
    {
        "paper_id": 2103.04898,
        "authors": "Chung-Han Hsieh",
        "title": "On Asymptotic Log-Optimal Buy-and-Hold Strategy",
        "comments": null,
        "journal-ref": "Automatica, vol. 151, pp. 110901:1-110901:11, 2023",
        "doi": "10.1016/j.automatica.2023.110901",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we consider a frequency-based portfolio optimization problem\nwith $m \\geq 2$ assets when the expected logarithmic growth (ELG) rate of\nwealth is used as the performance metric. With the aid of the notion called\ndominant asset, it is known that the optimal ELG level is achieved by investing\nall available funds on that asset. However, such an \"all-in\" strategy is\narguably too risky to implement in practice. Motivated by this issue, we study\nthe case where the portfolio weights are chosen in a rather ad-hoc manner and a\nbuy-and-hold strategy is subsequently used. Then we show that, if the\nunderlying portfolio contains a dominant asset, buy and hold on that specific\nasset is asymptotically log-optimal with a sublinear rate of convergence. This\nresult also extends to the scenario where a trader either does not have a\nprobabilistic model for the returns or does not trust a model obtained from\nhistorical data. To be more specific, we show that if a market contains a\ndominant asset, buy and hold a market portfolio involving nonzero weights for\neach asset is asymptotically log-optimal. Additionally, this paper also\nincludes a conjecture regarding the property called high-frequency maximality.\nThat is, in the absence of transaction costs, high-frequency rebalancing is\nunbeatable in the ELG sense. Support for the conjecture, involving a lemma for\na weak version of the conjecture, is provided. This conjecture, if true,\nenables us to improve the log-optimality result obtained previously. Finally, a\nresult that indicates a way regarding an issue about when should one to\nrebalance their portfolio if needed, is also provided. Examples, some involving\nsimulations with historical data, are also provided along the way to illustrate\nthe~theory.\n"
    },
    {
        "paper_id": 2103.04981,
        "authors": "Dragan Tevdovski, Petar Jolakoski and Viktor Stojkoski",
        "title": "The impact of state capacity on the cross-country variations in COVID-19\n  vaccination rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The initial period of vaccination shows strong heterogeneity between\ncountries' vaccinations rollout, both in the terms of the start of the\nvaccination process and in the dynamics of the number of people that are\nvaccinated. A predominant thesis in the ongoing debate on the drivers of this\nobserved heterogeneity is that a key determinant of the swift and extensive\nvaccine rollout is state capacity. Here, we utilize two measures that quantify\ndifferent aspects of the state capacity: i) the external capacity (measured\nthrough the soft power and the economic power of the country) and ii) the\ninternal capacity (measured via the country's government effectiveness) and\ninvestigate their relationship with the coronavirus vaccination outcome in the\ninitial period (up to 30th January 2021). By using data on 189 countries and a\ntwo-step Heckman approach, we find that the economic power of the country and\nits soft power are robust determinants of whether a country has started with\nthe vaccination process. In addition, the government effectiveness is a key\nfactor that determines vaccine roll-out. Altogether, our findings are in line\nwith the hypothesis that state capacity determines the observed heterogeneity\nbetween countries in the initial period of COVID-19 vaccines rollout.\n"
    },
    {
        "paper_id": 2103.05189,
        "authors": "Tianyi Li, Jiawen Luo, Cunrui Huang",
        "title": "Urban Epidemic Hazard Index for Chinese Cities: Why Did Small Cities\n  Become Epidemic Hotspots?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Multiple small- to middle-scale cities, mostly located in northern China,\nbecame epidemic hotspots during the second wave of the spread of COVID-19 in\nearly 2021. Despite qualitative discussions of potential social-economic\ncauses, it remains unclear how this pattern could be accounted for from a\nquantitative approach. Through the development of an urban epidemic hazard\nindex (EpiRank), we came up with a mathematical explanation for this\nphenomenon. The index is constructed from epidemic simulations on a multi-layer\ntransportation network model on top of local SEIR transmission dynamics, which\ncharacterizes intra- and inter-city compartment population flow with a detailed\nmathematical description. Essentially, we argue that these highlighted cities\npossess greater epidemic hazards due to the combined effect of large regional\npopulation and small inter-city transportation. The proposed index, dynamic and\napplicable to different epidemic settings, could be a useful indicator for the\nrisk assessment and response planning of urban epidemic hazards in China; the\nmodel framework is modularized and can be adapted for other nations without\nmuch difficulty.\n"
    },
    {
        "paper_id": 2103.05201,
        "authors": "Baishuai Zuo, Chuancun Yin",
        "title": "Multivariate tail covariance for generalized skew-elliptical\n  distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the multivariate tail covariance (MTCov) for generalized\nskew-elliptical distributions is considered. Some special cases for this\ndistribution, such as generalized skew-normal, generalized skew student-t,\ngeneralized skew-logistic and generalized skew-Laplace distributions, are also\nconsidered. In order to test the theoretical feasibility of our results, the\nMTCov for skewed and non skewed normal distributions are computed and compared.\nFinally, we give a special formula of the MTCov for generalized skew-elliptical\ndistributions.\n"
    },
    {
        "paper_id": 2103.0544,
        "authors": "Michael Krisper",
        "title": "Problems with Risk Matrices Using Ordinal Scales",
        "comments": "11 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we discuss various problems in the usage and definition of\nrisk matrices. We give an overview of the general process of risk assessment\nwith risk matrices and ordinal scales. Furthermore, we explain the fallacies in\neach phase of this process and give hints on which decisions may lead to more\nproblems than others and how to avoid them. Among those 24 discussed problems\nare ordinal scales, semi-quantitative arithmetics, range compression, risk\ninversion, ambiguity, and neglection of uncertainty. Finally, we make a case\nfor avoiding risk matrices altogether and instead propose using fully\nquantitative risk assessment methods.\n"
    },
    {
        "paper_id": 2103.05453,
        "authors": "Patrick S. Hagan, Andrew Lesniewski, Georgios E. Skoufis, and Diana E.\n  Woodward",
        "title": "Portfolio risk allocation through Shapley value",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.34681.80484",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We argue that using the Shapley value of cooperative game theory as the\nscheme for risk allocation among non-orthogonal risk factors is a natural way\nof interpreting the contribution made by each of such factors to overall\nportfolio risk. We discuss a Shapley value scheme for allocating risk to\nnon-orthogonal greeks in a portfolio of derivatives. Such a situation arises,\nfor example, when using a stochastic volatility model to capture option\nvolatility smile. We also show that Shapley value allows for a natural method\nof interpreting components of enterprise risk measures such as VaR and ES. For\nall applications discussed, we derive explicit formulas and / or numerical\nalgorithms to calculate the allocations.\n"
    },
    {
        "paper_id": 2103.05455,
        "authors": "Nicholas Moehle, Jack Gindi, Stephen Boyd, Mykel Kochenderfer",
        "title": "Portfolio Construction as Linearly Constrained Separable Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Mean-variance portfolio optimization problems often involve separable\nnonconvex terms, including penalties on capital gains, integer share\nconstraints, and minimum position and trade sizes. We propose a heuristic\nalgorithm for such problems based on the alternating direction method of\nmultipliers (ADMM). This method allows for solve times in tens to hundreds of\nmilliseconds with around 1000 securities and 100 risk factors. We also obtain a\nbound on the achievable performance. Our heuristic and bound are both derived\nfrom similar results for other optimization problems with a separable objective\nand affine equality constraints. We discuss a concrete implementation in the\ncase where the separable terms in the objective are piecewise quadratic, and we\nempirically demonstrate its effectiveness for tax-aware portfolio construction.\n"
    },
    {
        "paper_id": 2103.05556,
        "authors": "Michael Reiss",
        "title": "On the marginal utility of fiat money: insurmountable circularity or\n  not?",
        "comments": "Comments are welcome!",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The question of how a pure fiat currency is enforced and comes to have a\nnon-zero value has been much debated \\cite{10.2307/2077948}. What is less often\naddressed is, in the case where the enforcement is taken for granted and we ask\nwhat value (in terms of goods and services) the currency will end up taking.\nEstablishing a decentralised mechanism for price formation has proven a\nchallenge for economists: \"Since no decentralized out-of-equilibrium adjustment\nmechanism has been discovered, we currently have no acceptable dynamical model\nof the Walrasian system\" (Gintis 2006). In his paper, Gintis put forward a\nmodel for price discovery based on the evolution of the model's agents, i.e.\n\"poorly performing agents dying and being replaced by copies of the well\nperforming agents.\" It seems improbable that this mechanism is the driving\nforce behind price discovery in the real world. This paper proposes a more\nrealistic mechanism and presents results from a corresponding agent based\nmodel.\n"
    },
    {
        "paper_id": 2103.05623,
        "authors": "Marco Bardoscia, Paolo Barucca, Stefano Battiston, Fabio Caccioli,\n  Giulio Cimini, Diego Garlaschelli, Fabio Saracco, Tiziano Squartini, Guido\n  Caldarelli",
        "title": "The Physics of Financial Networks",
        "comments": "version submitted to Nature Reviews Physics",
        "journal-ref": "Nat. Rev. Phys. 3 (7), 490-507 (2021)",
        "doi": "10.1038/s42254-021-00322-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The field of Financial Networks is a paramount example of the novel\napplications of Statistical Physics that have made possible by the present data\nrevolution. As the total value of the global financial market has vastly\noutgrown the value of the real economy, financial institutions on this planet\nhave created a web of interactions whose size and topology calls for a\nquantitative analysis by means of Complex Networks. Financial Networks are not\nonly a playground for the use of basic tools of statistical physics as ensemble\nrepresentation and entropy maximization; rather, their particular dynamics and\nevolution triggered theoretical advancements as the definition of DebtRank to\nmeasure the impact and diffusion of shocks in the whole systems. In this review\nwe present the state of the art in this field, starting from the different\ndefinitions of financial networks (based either on loans, on assets ownership,\non contracts involving several parties -- such as credit default swaps, to\nmultiplex representation when firms are introduced in the game and a link with\nreal economy is drawn) and then discussing the various dynamics of financial\ncontagion as well as applications in financial network inference and\nvalidation. We believe that this analysis is particularly timely since\nfinancial stability as well as recent innovations in climate finance, once\nproperly analysed and understood in terms of complex network theory, can play a\npivotal role in the transformation of our society towards a more sustainable\nworld.\n"
    },
    {
        "paper_id": 2103.05777,
        "authors": "Nicole B\\\"auerle and Gregor Leimcke",
        "title": "Bayesian optimal investment and reinsurance with dependent financial and\n  insurance risks",
        "comments": "arXiv admin note: text overlap with arXiv:2001.11301",
        "journal-ref": "Statistics & Risk Modeling 39(1-2), 2022",
        "doi": "10.1515/strm-2021-0029",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Major events like natural catastrophes or the COVID-19 crisis have impact\nboth on the financial market and on claim arrival intensities and claim sizes\nof insurers. Thus, when optimal investment and reinsurance strategies have to\nbe determined it is important to consider models which reflect this dependence.\nIn this paper we make a proposal how to generate dependence between the\nfinancial market and claim sizes in times of crisis and determine via a\nstochastic control approach an optimal investment and reinsurance strategy\nwhich maximizes the expected exponential utility of terminal wealth. Moreover,\nwe also allow that the claim size distribution may be learned in the model. We\ngive comparisons and bounds on the optimal strategy using simple models. What\nturns out to be very surprising is that numerical results indicate that even a\nminimal dependence which is created in this model has a huge impact on the\ncontrol in the sense that the insurer is much more prudent then.\n"
    },
    {
        "paper_id": 2103.0588,
        "authors": "Sakae Oya",
        "title": "A Bayesian Graphical Approach for Large-Scale Portfolio Management with\n  Fewer Historical Data",
        "comments": "As the paper has been published, we have added a DOI link to this\n  preprint",
        "journal-ref": null,
        "doi": "10.1007/s10690-022-09358-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing a large-scale portfolio with many assets is one of the most\nchallenging tasks in the field of finance. It is partly because estimation of\neither covariance or precision matrix of asset returns tends to be unstable or\neven infeasible when the number of assets $p$ exceeds the number of\nobservations $n$. For this reason, most of the previous studies on portfolio\nmanagement have focused on the case of$ p < n$. To deal with the case of $p >\nn$, we propose to use a new Bayesian framework based on adaptive graphical\nLASSO for estimating the precision matrix of asset returns in a large-scale\nportfolio. Unlike the previous studies on graphical LASSO in the literature,\nour approach utilizes a Bayesian estimation method for the precision matrix\nproposed by Oya and Nakatsuma (2022) so that the positive definiteness of the\nprecision matrix should be always guaranteed. As an empirical application, we\nconstruct the global minimum variance portfolio of $p = 100$ for various values\nof n with the proposed approach as well as the non-Bayesian graphical LASSO\napproach, and compare their out-of-sample performance with the equal weight\nportfolio as the benchmark. In this comparison, the proposed approach produces\nmore stable results than the non-Bayesian approach in terms of Sharpe ratio,\nportfolio composition and turnover. Furthermore, the proposed approach succeeds\nin estimating the precision matrix even if $n$ is much smaller than $p$ and the\nnon-Bayesian approach fails to do so.\n"
    },
    {
        "paper_id": 2103.05921,
        "authors": "Damien Challet, Christian Bongiorno, Guillaume Pelletier",
        "title": "Financial factors selection with knockoffs: fund replication,\n  explanatory and prediction networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126105",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply the knockoff procedure to factor selection in finance. By building\nfake but realistic factors, this procedure makes it possible to control the\nfraction of false discovery in a given set of factors. To show its versatility,\nwe apply it to fund replication and to the inference of explanatory and\nprediction networks.\n"
    },
    {
        "paper_id": 2103.05957,
        "authors": "Ulrich Horst, Evgueni Kivman",
        "title": "Optimal trade execution under small market impact and portfolio\n  liquidation with semimartingale strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider an optimal liquidation problem with instantaneous price impact\nand stochastic resilience for small instantaneous impact factors. Within our\nmodelling framework, the optimal portfolio process converges to the solution of\nan optimal liquidation problem with general semimartingale controls when the\ninstantaneous impact factor converges to zero. Our results provide a unified\nframework within which to embed the two most commonly used modelling frameworks\nin the liquidation literature and provide a microscopic foundation for the use\nof semimartingale liquidation strategies and the use of portfolio processes of\nunbounded variation. Our convergence results are based on novel convergence\nresults for BSDEs with singular terminal conditions and novel representation\nresults of BSDEs in terms of uniformly continuous functions of forward\nprocesses.\n"
    },
    {
        "paper_id": 2103.05973,
        "authors": "A.R. Baghirzade, B. Kushbakov",
        "title": "Crowdfunding for Independent Parties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays there are a lot of creative and innovative ideas of business\nstart-ups or various projects starting from a novel or music album and\nfinishing with some innovative goods or website that makes our life better and\neasier. Unfortunately, young people often do not have enough financial support\nto bring their ideas to life. The best way to solve particular problem is to\nuse crowdfunding platforms. Crowdfunding itself is a way of financing a project\nby raising money from a crowd or simply large number of people. It is believed\nthat crowdfunding term appeared at the same time as crowdsourcing in 2006. Its\nauthor is Jeff Howe. However, the phenomenon of the national funding, of\ncourse, much older. For instance, the construction of the Statue of Liberty in\nNew York, for which funds were collected by the people. Currently, the national\nproject is financed with the use of the Internet. Author of the project in need\nof funding, can post information about the project on a special website and\nrequest sponsorship of the audience. Firstly, author selects the best\ncrowdfunding platform for project requirements and sign in. then he or she\ncreates and draws up the project. The project that is created must correspond\nto one of the categories available for selection (music, film, publishing,\netc.). If you create brand new product, it is necessary to submit the\ndraft-working prototype or sample product. A full list of design rules for a\nproject can be viewed directly on the site of crowdfunding platform. While\ncalculating the cost of project it is necessary to take into account the cost\nof realization the project, reward for your sponsors, moreover commission of\npayment systems and taxes. The project is considered successfully launched\nafter it gets through moderation on website.\n"
    },
    {
        "paper_id": 2103.06017,
        "authors": "Andrea Tacchella, Andrea Zaccaria, Marco Miccheli, Luciano Pietronero",
        "title": "Relatedness in the Era of Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Relatedness is a quantification of how much two human activities are similar\nin terms of the inputs and contexts needed for their development. Under the\nidea that it is easier to move between related activities than towards\nunrelated ones, empirical approaches to quantify relatedness are currently used\nas predictive tools to inform policies and development strategies in\ngovernments, international organizations, and firms. Here we focus on\ncountries' industries and we show that the standard, widespread approach of\nestimating Relatedness through the co-location of activities (e.g. Product\nSpace) generates a measure of relatedness that performs worse than trivial\nauto-correlation prediction strategies. We argue that this is a consequence of\nthe poor signal-to-noise ratio present in international trade data. In this\npaper we show two main findings. First, we find that a shift from two-products\ncorrelations (network-density based) to many-products correlations (decision\ntrees) can dramatically improve the quality of forecasts with a corresponding\nreduction of the risk of wrong policy choices. Then, we propose a new\nmethodology to empirically estimate Relatedness that we call Continuous\nProjection Space (CPS). CPS, which can be seen as a general network embedding\ntechnique, vastly outperforms all the co-location, network-based approaches,\nwhile retaining a similar interpretability in terms of pairwise distances.\n"
    },
    {
        "paper_id": 2103.0602,
        "authors": "Rozane Bezerra de Siqueira and Jose Ricardo Bezerra Nogueira",
        "title": "A Universal Basic Income For Brazil: Fiscal and Distributional Effects\n  of Alternative Schemes",
        "comments": null,
        "journal-ref": "Revista de Economia Contemporanea\n  (https://revistas.ufrj.br/index.php/rec), v. 27, p. 1-16, 2023",
        "doi": "10.1590/198055272701",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The outbreak of the Covid-19 pandemic has led to an increasing interest in\nUniversal Basic Income (UBI) proposals as it exposed the inadequacy of\ntraditional welfare systems to provide basic financial security to a large\nshare of the population. In this paper, we use a static tax-benefit\nmicrosimulation model to analyse the fiscal and distributional effects of the\nhypothetical implementation in Brazil of alternative UBI schemes which\npartially replace the existing tax-transfer system. The results indicate that\nthe introduction of a UBI/Flat Tax system in the country could be both\nextremely effective in reducing poverty and inequality and economically viable.\n"
    },
    {
        "paper_id": 2103.06051,
        "authors": "Andry Alamsyah, Yahya Peranginangin, Gabriel Nurhadi",
        "title": "Learning Organization using Conversational Social Network for Social\n  Customer Relationship Management Effort",
        "comments": "8 pages, 2 tables, 4 figures",
        "journal-ref": "The 2nd International Conference and Seminar on Learning\n  Organizations, 2014",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The challenge of each organization is how they adapt to the shift of more\ncomplex technology such as mobile, big data, interconnected world, and the\nInternet of things. In order to achieve their objective, they must understand\nhow to take advantage of the interconnected individuals inside and outside the\norganization. Learning organization continues to transform by listening and\nmaintain the connection with their counterparts. Customer relationship\nmanagement is an important source for business organizations to grow and to\nassure their future. The complex social network, where interconnected peoples\nget information and get influenced very quickly, certainly a big challenge for\nbusiness organizations. The combination of these complex technologies provides\nintriguing insight such as the capabilities to listen to what the markets want,\nto understand their market competition, and to understand their market\nsegmentation. In this paper, as a part of organization transformation, we show\nhow a business organization mine online conversational in Twitter related to\ntheir brand issue and analyze them in the context of customer relationship\nmanagement to extract several insights regarding their market.\n"
    },
    {
        "paper_id": 2103.06107,
        "authors": "Felix L. Wolf and Lech A. Grzelak and Griselda Deelstra",
        "title": "Cheapest-to-Deliver Collateral: A Common Factor Approach",
        "comments": "25 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The collateral choice option gives the collateral posting party the\nopportunity to switch between different collateral currencies which is\nwell-known to impact the asset price. Quantification of the option's value is\nof practical importance but remains challenging under the assumption of\nstochastic rates, as it is determined by an intractable distribution which\nrequires involved approximations. Indeed, many practitioners still rely on\ndeterministic spreads between the rates for valuation. We develop a scalable\nand stable stochastic model of the collateral spreads under the assumption of\nconditional independence. This allows for a common factor approximation which\nadmits analytical results from which further estimators are obtained. We show\nthat in modelling the spreads between collateral rates, a second order model\nyields accurate results for the value of the collateral choice option. The\nmodel remains precise for a wide range of model parameters and is numerically\nefficient even for a large number of collateral currencies.\n"
    },
    {
        "paper_id": 2103.06121,
        "authors": "Gael Poux-Medard, Sergio Cobo-Lopez, Jordi Duch, Roger Guimera, Marta\n  Sales-Pardo",
        "title": "Complex decision-making strategies in a stock market experiment\n  explained as the combination of few simple strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many studies have shown that there are regularities in the way human beings\nmake decisions. However, our ability to obtain models that capture such\nregularities and can accurately predict unobserved decisions is still limited.\nWe tackle this problem in the context of individuals who are given information\nrelative to the evolution of market prices and asked to guess the direction of\nthe market. We use a networks inference approach with stochastic block models\n(SBM) to find the model and network representation that is most predictive of\nunobserved decisions. Our results suggest that users mostly use recent\ninformation (about the market and about their previous decisions) to guess.\nFurthermore, the analysis of SBM groups reveals a set of strategies used by\nplayers to process information and make decisions that is analogous to\nbehaviors observed in other contexts. Our study provides and example on how to\nquantitatively explore human behavior strategies by representing decisions as\nnetworks and using rigorous inference and model-selection approaches.\n"
    },
    {
        "paper_id": 2103.06227,
        "authors": "Benjamin M. Bolker, Matheus R. Grasselli and Emma Holmes",
        "title": "Sensitivity analysis of an integrated climate-economic model",
        "comments": "12 pages, 4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We conduct a sensitivity analysis of a new type of integrated\nclimate-economic model recently proposed in the literature, where the core\neconomic component is based on the Goodwin-Keen dynamics instead of a\nneoclassical growth model. Because these models can exhibit much richer\nbehaviour, including multiple equilibria, runaway trajectories and unbounded\noscillations, it is crucial to determine how sensitive they are to changes in\nunderlying parameters. We focus on four economic parameters (markup rate, speed\nof price adjustments, coefficient of money illusion, growth rate of\nproductivity) and two climate parameters (size of upper ocean reservoir,\nequilibrium climate sensitivity) and show how their relative effects on the\noutcomes of the model can be quantified by methods that can be applied to an\narbitrary number of parameters.\n"
    },
    {
        "paper_id": 2103.06482,
        "authors": "Hideaki Aoyama, Corrado Di Guilmi, Yoshi Fujiwara and Hiroshi\n  Yoshikawa",
        "title": "Dual Labor Market and the \"Phillips Curve Puzzle\"",
        "comments": "22 pages, 8 figures and 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Low inflation was once a welcome to both policy makers and the public.\nHowever, Japan's experience during the 1990's changed the consensus view on\nprice of economists and central banks around the world. Facing deflation and\nzero interest bound at the same time, Bank of Japan had difficulty in\nconducting effective monetary policy. It made Japan's stagnation unusually\nprolonged. Too low inflation which annoys central banks today is translated\ninto the \"Phillips curve puzzle\". In the US and Japan, in the course of\nrecovery from the Great Recession after the 2008 global financial crisis, the\nunemployment rate had steadily declined to the level which was commonly\nregarded as lower than the natural rate or NAIRU. And yet, inflation stayed\nlow. In this paper, we consider a minimal model of dual labor market to explore\nwhat kind of change in the economy makes the Phillips curve flat. The level of\nbargaining power of workers, the elasticity of the supply of labor to wage in\nthe secondary market, and the composition of the workforce are the main factors\nin explaining the flattening of the Phillips curve. We argue that the changes\nwe consider in the model, in fact, has plausibly made the Phillips curve flat\nin recent years.\n"
    },
    {
        "paper_id": 2103.0653,
        "authors": "A.R. Baghirzade, B. Kushbakov",
        "title": "Assessment of the Effectiveness of State Participation in Economic\n  Clusters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the article we made an attempt to reveal the contents and development of\nthe concept of economic clusters, to characterize the specificity of the\nregional cluster as a project. We have identified features of an estimation of\nefficiency of state participation in the cluster, where the state is an\ninstitution representing the interests of society.\n"
    },
    {
        "paper_id": 2103.06991,
        "authors": "Anna Naszodi and Francisco Mendonca",
        "title": "A new method for identifying what Cupid's invisible hand is doing. Is it\n  spreading color blindness while turning us more \"picky'' about spousal\n  education?",
        "comments": "Prelim results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a method suitable for detecting whether racial homophily is on the\nrise and also whether the economic divide (i.e., the gap between individuals\nwith different education levels and thereby with different abilities to\ngenerate income) is growing in a society. We identify these changes with the\nchanging aggregate marital preferences over the partners' race and education\nlevel through their effects on the share of inter-racial couples and the share\nof educationally homogamous couples. These shares are shaped not only by\npreferences, but also by the distributions of marriageable men and women by\ntraits. The method proposed is designed to control for changes in the trait\ndistributions from one generation to another. By applying the method, we find\nthe economic divide in the US to display a U-curve pattern between 1960 and\n2010 followed by its slightly negative trend between 2010 and 2015. The\nidentified trend of racial homophily suggests that the American society has\nbecome more and more permissive towards racial intermarriages since 1970.\nFinally, we refute the aggregate version of the status-cast exchange hypothesis\nbased on the joint dynamics of the economic divide and the racial homophily.\n"
    },
    {
        "paper_id": 2103.07213,
        "authors": "A.R Baghirzade",
        "title": "Modern risks of small businesses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important area of anti-crisis public administration is the development of\nsmall businesses. They are an important part of the economy of developed and\ndeveloping countries, provide employment for a significant part of the\npopulation and tax revenues to budgets, and contribute to increased competition\nand the development of entrepreneurial abilities of citizens. Therefore, the\nprimary task of the state Federal and regional policy is to reduce\nadministrative barriers and risks, time and resources spent on opening and\ndeveloping small businesses, problems with small businesses ' access to Bank\ncapital [8], etc. Despite the loud statements of officials, administrative\nbarriers to the development of small businesses in trade and public catering\nare constantly increasing, including during the 2014-2016 crisis.\n"
    },
    {
        "paper_id": 2103.07407,
        "authors": "Thomas Deschatre and Pierre Gruet",
        "title": "Electricity intraday price modeling with marked Hawkes processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a 2-dimensional marked Hawkes process with increasing baseline\nintensity in order to model prices on electricity intraday markets. This model\nallows to represent different empirical facts such as increasing market\nactivity, random jump sizes but above all microstructure noise through the\nsignature plot. This last feature is of particular importance for practitioners\nand has not yet been modeled on those particular markets. We provide analytic\nformulas for first and second moments and for the signature plot, extending the\nclassic results of Bacry et al. (2013) in the context of Hawkes processes with\nrandom jump sizes and time dependent baseline intensity. The tractable model we\npropose is estimated on German data and seems to fit the data well. We also\nprovide a result about the convergence of the price process to a Brownian\nmotion with increasing volatility at macroscopic scales, highlighting the\nSamuelson effect.\n"
    },
    {
        "paper_id": 2103.0744,
        "authors": "Mohamed Amine Kacef and Kamal Boukhetala",
        "title": "A closed-form approximation for pricing geometric Istanbul options",
        "comments": "This version of the paper was submitted to the journal Statistics and\n  Probability Letters on 3 March 2020",
        "journal-ref": "Int. J. Revenue Management, Vol. 11, No. 4, pp.297-315 (2020)",
        "doi": "10.1504/IJRM.2020.110631",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Istanbul options were first introduced by Michel Jacques in 1997. These\nderivatives are considered as an extension of the Asian options. In this paper,\nwe propose an analytical approximation formula for a geometric Istanbul call\noption (GIC) under the Black-Scholes model. Our approximate pricing formula is\nobtained in closed-form using a second-order Taylor expansion. We compare our\ntheoretical results with those of Monte-Carlo simulations using the control\nvariates method. Finally, we study the effects of changes in the price of the\nunderlying asset on the value of GIC.\n"
    },
    {
        "paper_id": 2103.07651,
        "authors": "Emmanuel Coffie",
        "title": "Delay stochastic interest rate model with jump and strong convergence in\n  Monte Carlo simulations",
        "comments": "arXiv admin note: text overlap with arXiv:2107.03712",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study analytical properties of the solutions to the\ngeneralised delay Ait-Sahalia-type interest rate model with Poisson-driven\njump. Since this model does not have explicit solution, we employ several new\ntruncated Euler-Maruyama (EM) techniques to investigate finite time strong\nconvergence theory of the numerical solutions under the local Lipschitz\ncondition plus the Khasminskii-type condition. We justify the strong\nconvergence result for Monte Carlo calibration and valuation of some debt and\nderivative instruments.\n"
    },
    {
        "paper_id": 2103.07707,
        "authors": "Semra Gunduc",
        "title": "Diffusion of Innovation In Competitive Markets-A Study on the Global\n  Smartphone Diffusion",
        "comments": null,
        "journal-ref": "ACTA POLONICA A Volume: 135 Issue: 3 Pages :485-494 Published :\n  Mar 2019",
        "doi": "10.12693/APhysPolA.135.485",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this work, the aim is to study the diffusion of innovation of two\ncompeting products. The main focus has been to understand the effects of the\ncompetitive dynamic market on the diffusion of innovation. The global\nsmartphone operating system sales are chosen as an example. The availability of\nthe sales and the number of users data, as well as the predictions for the\nfuture number of users, make the smartphone diffusion a new laboratory to test\nthe innovation of diffusion models for the competitive markets. In this work,\nthe Bass model and its extensions which incorporate the competition between the\nbrands are used. The diffusion of smartphones can be considered on two levels:\nthe product level and the brand level. The diffusion of the smartphone as a\ncategory is studied by using the Bass equation (category-level diffusion). The\ndiffusion of each competing operating system (iOS and Android) are considered\nas the competition of the brands, and it is studied in the context of\ncompetitive market models (product-level diffusion). It is shown that the\neffects of personal interactions play the dominant role in the diffusion\nprocess. Moreover, the volume of near future sales can be predicted by\nintroducing appropriate dynamic market potential which helps to extrapolate the\nmodel results for the future.\n"
    },
    {
        "paper_id": 2103.08048,
        "authors": "P. Alison Paprica",
        "title": "Risks for Academic Research Projects, An Empirical Study of Perceived\n  Negative Risks and Possible Responses",
        "comments": "20 pages, two text boxes, one table, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Academic research projects receive hundreds of billions of dollars of\ngovernment investment each year. They complement business research projects by\nfocusing on the generation of new foundational knowledge and addressing\nsocietal challenges. Despite the importance of academic research, the\nmanagement of it is often undisciplined and ad hoc. It has been postulated that\nthe inherent uncertainty and complexity of academic research projects make them\nchallenging to manage. However, based on this study's analysis of input and\nvoting from more than 500 academic research team members in facilitated risk\nmanagement sessions, the most important perceived risks are general, as opposed\nto being research specific. Overall participants' top risks related to funding,\nteam instability, unreliable partners, study participant recruitment, and data\naccess. Many of these risks would require system- or organization-level\nresponses that are beyond the scope of individual academic research teams.\n"
    },
    {
        "paper_id": 2103.08131,
        "authors": "Xiangfei Yuan, Haijing Hao, Chenghua Guan, Alex Pentland",
        "title": "What are the key components of an entrepreneurial ecosystem in a\n  developing economy? A longitudinal empirical study on technology business\n  incubators in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Since the 1980s, technology business incubators (TBIs), which focus on\naccelerating businesses through resource sharing, knowledge agglomeration, and\ntechnology innovation, have become a booming industry. As such, research on\nTBIs has gained international attention, most notably in the United States,\nEurope, Japan, and China. The present study proposes an entrepreneurial\necosystem framework with four key components, i.e., people, technology,\ncapital, and infrastructure, to investigate which factors have an impact on the\nperformance of TBIs. We also empirically examine this framework based on\nunique, three-year panel survey data from 857 national TBIs across China. We\nimplemented factor analysis and panel regression models on dozens of variables\nfrom 857 national TBIs between 2015 and 2017 in all major cities in China and\nfound that a number of factors associated with people, technology, capital, and\ninfrastructure components have various statistically significant impacts on the\nperformance of TBIs at either national model or regional models.\n"
    },
    {
        "paper_id": 2103.08148,
        "authors": "Mohamed Abdelghani, Alexander Melnikov and Andrey Pak",
        "title": "On statistical estimation and inferences in optional regression models",
        "comments": "to be published in Statistics: A Journal of Theoretical and Applied\n  Statistics",
        "journal-ref": null,
        "doi": "10.1080/02331888.2021.1900186",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The main object of investigation in this paper is a very general regression\nmodel in optional setting - when an observed process is an optional\nsemimartingale depending on an unknown parameter. It is well-known that\nstatistical data may present an information flow/filtration without usual\nconditions. The estimation problem is achieved by means of structural least\nsquares (LS) estimates and their sequential versions. The main results of the\npaper are devoted to the strong consistency of such LS-estimates. For\nsequential LS-estimates the property of fixed accuracy is proved.\n"
    },
    {
        "paper_id": 2103.08258,
        "authors": "Felix Dammann and Giorgio Ferrari",
        "title": "On an Irreversible Investment Problem with Two-Factor Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a real options model for the optimal irreversible investment\nproblem of a profit maximizing company. The company has the opportunity to\ninvest into a production plant capable of producing two products, of which the\nprices follow two independent geometric Brownian motions. After paying a\nconstant sunk investment cost, the company sells the products on the market and\nthus receives a continuous stochastic revenue-flow. This investment problem is\nset as a two-dimensional optimal stopping problem. We find that the optimal\ninvestment decision is triggered by a convex curve, which we characterize as\nthe unique continuous solution to a nonlinear integral equation. Furthermore,\nwe provide analytical and numerical comparative statics results of the\ndependency of the project's value and investment decision with respect to the\nmodel's parameters.\n"
    },
    {
        "paper_id": 2103.08321,
        "authors": "Fabrizio Natale, Stefano Maria Iacus, Alessandra Conte, Spyridon\n  Spyratos, Francesco Sermi",
        "title": "Territorial differences in the spread of COVID-19 in European regions\n  and US counties",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0280780",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article explores the territorial differences in the onset and spread of\nCOVID-19 and the excess mortality associated with the pandemic, across the\nEuropean NUTS3 regions and US counties. Both in Europe and in the US, the\npandemic arrived earlier and recorded higher Rt values in urban regions than in\nintermediate and rural ones. A similar gap is also found in the data on excess\nmortality. In the weeks during the first phase of the pandemic, urban regions\nin EU countries experienced excess mortality of up to 68pp more than rural\nones. We show that, during the initial days of the pandemic, territorial\ndifferences in Rt by the degree of urbanisation can be largely explained by the\nlevel of internal, inbound and outbound mobility. The differences in the spread\nof COVID-19 by rural-urban typology and the role of mobility are less clear\nduring the second wave. This could be linked to the fact that the infection is\nwidespread across territories, to changes in mobility patterns during the\nsummer period as well as to the different containment measures which reverse\nthe causality between mobility and Rt.\n"
    },
    {
        "paper_id": 2103.0838,
        "authors": "Dongming Wei, Yogi Ahmad Erlangga, Andrey Pak, and Laila Zhexembay",
        "title": "Finite element solutions of the nonlinear RAPM Black-Scholes model",
        "comments": "11 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:2010.13541",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  his paper presents finite element methods for solving numerically the\nRisk-Adjusted Pricing Methodology (RAPM) Black-Scholes model for option pricing\nwith transaction costs. Spatial finite element models based on P1 and/or P2\nelements are formulated using some group finite elements and numerical\nquadrature to handle the nonlinear term, in combination with a\nCrank-Nicolson-type temporal scheme. The temporal scheme is implemented using\nthe Rannacher approach. Spatial-temporal mesh-size ratios are observed for\ncontrolling the stability of our method. Our results compare favorably with the\nfinite difference results in the literature for the model.\n"
    },
    {
        "paper_id": 2103.08398,
        "authors": "Cathal O'Donoghue, Denisa M. Sologon, Iryna Kyzyma, John McHale",
        "title": "A Microsimulation Analysis of the Distributional Impact over the Three\n  Waves of the COVID-19 Crisis in Ireland",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper relies on a microsimulation framework to undertake an analysis of\nthe distributional implications of the COVID-19 crisis over three waves. Given\nthe lack of real-time survey data during the fast moving crisis, it applies a\nnowcasting methodology and real-time aggregate administrative data to calibrate\nan income survey and to simulate changes in the tax benefit system that\nattempted to mitigate the impacts of the crisis. Our analysis shows how\ncrisis-induced income-support policy innovations combined with existing\nprogressive elements of the tax-benefit system were effective in avoiding an\nincrease in income inequality at all stages of waves 1-3 of the COVID-19\nemergency in Ireland. There was, however, a decline in generosity over time as\nbenefits became more targeted. On a methodological level, our paper makes a\nspecific contribution in relation to the choice of welfare measure in assessing\nthe impact of the COVID-19 crisis on inequality.\n"
    },
    {
        "paper_id": 2103.08414,
        "authors": "Gabriel Borrageiro, Nick Firoozye and Paolo Barucca",
        "title": "Online Learning with Radial Basis Function Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial time series are characterised by their nonstationarity and\nautocorrelation. Even if these time series are differenced, technically\nensuring their stationarity, they experience regular covariate shifts and\nconcept drifts. Against this backdrop, we combine feature representation\ntransfer with sequential optimisation to provide multi-horizon returns\nforecasts. Our online learning rbfnet outperforms a random-walk baseline and\nseveral powerful batch learners. The rbfnets we formulate are naturally\ndesigned to measure the similarity between test samples and continuously\nupdated prototypes that capture the characteristics of the feature space.\n"
    },
    {
        "paper_id": 2103.08447,
        "authors": "Elizaveta Zinovyeva, Raphael C. G. Reule, Wolfgang Karl H\\\"ardle",
        "title": "Understanding Smart Contracts: Hype or Hope?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Smart Contracts are commonly considered to be an important component or even\na key to many business solutions in an immense variety of sectors and promises\nto securely increase their individual efficiency in an ever more digitized\nenvironment. Introduced in the early 1990s, the technology has gained a lot of\nattention with its application to blockchain technology to an extent, that can\nbe considered a veritable hype. Reflecting the growing institutional interest,\nthis intertwined exploratory study between statistics, information technology,\nand law contrasts these idealistic stories with the data reality and provides a\nmandatory step of understanding the matter, before any further relevant\napplications are discussed as being \"factually\" able to replace traditional\nconstructions. Besides fundamental flaws and applica-tion difficulties of\ncurrently employed Smart Contracts, the technological drive and enthusiasm\nbacking it may however serve as a jump-off board for future developments\nthrusting well in the presently unshakeable traditional structures.\n"
    },
    {
        "paper_id": 2103.08627,
        "authors": "Natalia Zdanowska and Robin Morphet",
        "title": "Decentralising the United Kingdom: the Northern Powerhouse strategy and\n  urban ownership links between firms since 2010",
        "comments": "23 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores a decentralisation initiative in the United Kingdom - the\nNorthern Powerhouse strategy (NPS) - in terms of its main goal: strengthening\nconnectivity between Northern cities of England. It focuses on economic\ninteractions of these cities, defined by ownership linkages between firms,\nsince the NPS's launch in 2010. The analysis reveals a relatively weak increase\nin the intensity of economic regional patterns in the North, in spite of a\nshift away from NPS cities' traditional manufacturing base. These results\nsuggest potential directions for policy-makers in terms of the future\nimplementation of the NPS.\n"
    },
    {
        "paper_id": 2103.08842,
        "authors": "Agostino Capponi and Ruizhe Jia",
        "title": "The Adoption of Blockchain-based Decentralized Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the market microstructure of Automated Market Makers (AMMs),\nthe most prominent type of blockchain-based decentralized exchanges. We show\nthat the order execution mechanism yields token value loss for liquidity\nproviders if token exchange rates are volatile. AMMs are adopted only if their\ntoken pairs are of high personal use for investors, or the token price\nmovements of the pair are highly correlated. A pricing curve with higher\ncurvature reduces the arbitrage problem but also investors' surplus. Pooling\nmultiple tokens exacerbates the arbitrage problem. We provide statistical\nsupport for our main model implications using transaction-level data of AMMs.\n"
    },
    {
        "paper_id": 2103.09059,
        "authors": "Michel Alexandre and Kau\\^e Lopes de Moraes and Francisco Aparecido\n  Rodrigues",
        "title": "Risk-dependent centrality in the Brazilian stock market",
        "comments": "12 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to calculate the risk-dependent centrality (RDC)\nof the Brazilian stock market. We computed the RDC for assets traded on the\nBrazilian stock market between January 2008 to June 2020 at different levels of\nexternal risk. We observed that the ranking of assets based on the RDC depends\non the external risk. Rankings' volatility is related to crisis events,\ncapturing the recent Brazilian economic-political crisis. Moreover, we have\nfound a negative correlation between the average volatility of assets' ranking\nbased on the RDC and the average daily returns on the stock market. It goes in\nhand with the hypothesis that the rankings' volatility is higher in periods of\ncrisis.\n"
    },
    {
        "paper_id": 2103.09098,
        "authors": "Yusen Lin, Jinming Xue, Louiqa Raschid",
        "title": "Predicting the Behavior of Dealers in Over-The-Counter Corporate Bond\n  Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Trading in Over-The-Counter (OTC) markets is facilitated by broker-dealers,\nin comparison to public exchanges, e.g., the New York Stock Exchange (NYSE).\nDealers play an important role in stabilizing prices and providing liquidity in\nOTC markets. We apply machine learning methods to model and predict the trading\nbehavior of OTC dealers for US corporate bonds. We create sequences of daily\nhistorical transaction reports for each dealer over a vocabulary of US\ncorporate bonds. Using this history of dealer activity, we predict the future\ntrading decisions of the dealer. We consider a range of neural network-based\nprediction models. We propose an extension, the Pointwise-Product ReZero (PPRZ)\nTransformer model, and demonstrate the improved performance of our model. We\nshow that individual history provides the best predictive model for the most\nactive dealers. For less active dealers, a collective model provides improved\nperformance. Further, clustering dealers based on their similarity can improve\nperformance. Finally, prediction accuracy varies based on the activity level of\nboth the bond and the dealer.\n"
    },
    {
        "paper_id": 2103.09106,
        "authors": "Jaideep Singh and Matloob Khushi",
        "title": "Feature Learning for Stock Price Prediction Shows a Significant Role of\n  Analyst Rating",
        "comments": null,
        "journal-ref": "Appl. Syst. Innov. 2021, 4, 17",
        "doi": "10.3390/asi4010017",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To reject the Efficient Market Hypothesis a set of 5 technical indicators and\n23 fundamental indicators was identified to establish the possibility of\ngenerating excess returns on the stock market. Leveraging these data points and\nvarious classification machine learning models, trading data of the 505\nequities on the US S&P500 over the past 20 years was analysed to develop a\nclassifier effective for our cause. From any given day, we were able to predict\nthe direction of change in price by 1% up to 10 days in the future. The\npredictions had an overall accuracy of 83.62% with a precision of 85% for buy\nsignals and a recall of 100% for sell signals. Moreover, we grouped equities by\ntheir sector and repeated the experiment to see if grouping similar assets\ntogether positively effected the results but concluded that it showed no\nsignificant improvements in the performance rejecting the idea of sector-based\nanalysis. Also, using feature ranking we could identify an even smaller set of\n6 indicators while maintaining similar accuracies as that from the original 28\nfeatures and also uncovered the importance of buy, hold and sell analyst\nratings as they came out to be the top contributors in the model. Finally, to\nevaluate the effectiveness of the classifier in real-life situations, it was\nbacktested on FAANG equities using a modest trading strategy where it generated\nhigh returns of above 60% over the term of the testing dataset. In conclusion,\nour proposed methodology with the combination of purposefully picked features\nshows an improvement over the previous studies, and our model predicts the\ndirection of 1% price changes on the 10th day with high confidence and with\nenough buffer to even build a robotic trading system.\n"
    },
    {
        "paper_id": 2103.09107,
        "authors": "Guglielmo D'Amico, Stefania Scocchera, Loriano Storchi",
        "title": "Randentropy: a software to measure inequality in random systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The software Randentropy is designed to estimate inequality in a random\nsystem where several individuals interact moving among many communities and\nproducing dependent random quantities of an attribute. The overall inequality\nis assessed by computing the Random Theil's Entropy. Firstly, the software\nestimates a piecewise homogeneous Markov chain by identifying the\nchanging-points and the relative transition probability matrices. Secondly, it\nestimates the multivariate distribution function of the attribute using a\ncopula function approach and finally, through a Monte Carlo algorithm,\nevaluates the expected value of the Random Theil's Entropy. Possible\napplications are discussed as related to the fields of finance and human\nmobility\n"
    },
    {
        "paper_id": 2103.09121,
        "authors": "Guohui Guan, Jiaqi Hu, Zongxia Liang",
        "title": "Robust equilibrium strategies in a defined benefit pension plan game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the robust {non-zero-sum} games in an aggregated\n{overfunded} defined benefit (abbr. DB) pension plan. The sponsoring firm is\nconcerned with the investment performance of the fund surplus while the\nparticipants act as a union to claim a share of the fund surplus. The financial\nmarket consists of one risk-free asset and $n$ risky assets. The firm and the\nunion both are ambiguous about the financial market and care about the robust\nstrategies under the worst case scenario. {The union's objective is to maximize\nthe expected discounted utility of the additional benefits, the firm's two\ndifferent objectives are to maximizing the expected discounted utility of the\nfund surplus and the probability of the fund surplus reaching an upper level\nbefore hitting a lower level in the worst case scenario.} We formulate the\nrelated two robust non-zero-sum games for the firm and the union. Explicit\nforms and optimality of the solutions are shown by stochastic dynamic\nprogramming method. In the end of this paper, numerical results are illustrated\nto depict the economic behaviours of the robust equilibrium strategies in these\ntwo different games.\n"
    },
    {
        "paper_id": 2103.09614,
        "authors": "David Baltimore, Robert Conn, William H Press, Thomas Rosenbaum, David\n  N Spergel, Shirley M Tilghman, and Harold Varmus",
        "title": "Should the Endless Frontier of Federal Science be Expanded?",
        "comments": "Appeared as an AAAS Policy Alert On-line",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Scientific research in the United States could receive a large increase in\nfederal funding--up to 100 billion dollars over five years -- if proposed\nlegislation entitled the Endless Frontiers Act becomes law. This bipartisan and\nbicameral bill, introduced in May 2020 by Senators Chuck Schumer (D-NY) and\nTodd Young (R-IN) and Congressmen Ro Khanna (D-CA) and Mike Gallagher (R-WI),\nis intended to expand the funding of the physical sciences, engineering, and\ntechnology at the National Science Foundation (NSF) and create a new Technology\nDirectorate focused on use-inspired research. In addition to provisions to\nprotect the NSF's current missions, a minimum of 15\\% of the newly appropriated\nfunds would be used to enhance NSF's basic science portfolio. The Endless\nFrontier Act offers a rare opportunity to enhance the breadth and financial\nsupport of the American research enterprise. In this essay, we consider the\nbenefits and the liabilities of the proposed legislation and recommend changes\nthat would further strengthen it.\n"
    },
    {
        "paper_id": 2103.0969,
        "authors": "Cristina Bicchieri, Upasak Das, Samuel Gant, Rachel Sander",
        "title": "Examining norms and social expectations surrounding exclusive\n  breastfeeding: Evidence from Mali",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Why do people engage in certain behavior. What are the effects of social\nexpectations and perceptions of community behavior and beliefs on own behavior.\nGiven that proper infant feeding practices are observable and have significant\nhealth impacts, we explore the relevance of these questions in the context of\nexclusive infant breastfeeding behavior using social norms theory. We make use\nof a primary survey of mothers of children below the age of two years in the\nKayes and Sikasso region of Mali, which have a historically lower prevalence of\nexclusive breastfeeding. The findings from regression estimations, controlling\nfor a host of potential confounding factors, indicate that expectations about\nthe behavior of other community members can strongly predict individual\nexclusive breastfeeding. Beliefs about approval of the infant feeding behavior\nof the community though are found to be only modestly associated with it. In\naddition, mothers who hold false but positive beliefs about the community are\nfound to exclusively breastfeed their kids. Further, using responses from\nrandomly assigned vignettes where we experimentally manipulated the levels of\nsocial expectations, our data reveal a strong relationship between perceived\nprevalence of community level exclusive breastfeeding and individual behavior.\nThis result indicates the existence of a potential causal relationship. We\nargue that our findings represent an important foundation for the design of\npolicy interventions aimed at altering social expectations, and thus effecting\na measurable change in individual behaviors. This type of intervention, by\nusing social norm messaging to end negative behavior, avoids the use of\ncoercive measures to effect behavior change in a cost-effective and efficient\nway.\n"
    },
    {
        "paper_id": 2103.09692,
        "authors": "Jean-Philippe Bouchaud",
        "title": "Radical Complexity",
        "comments": "10 pages, edited columns published in Risk.net",
        "journal-ref": null,
        "doi": "10.3390/e23121676",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is an informal and sketchy review of six topical, somewhat unrelated\nsubjects in quantitative finance: rough volatility models; random covariance\nmatrix theory; copulas; crowded trades; high-frequency trading & market\nstability; and \"radical complexity\" & scenario based (macro)economics. Some\nopen questions and research directions are briefly discussed.\n"
    },
    {
        "paper_id": 2103.0975,
        "authors": "Zexin Hu, Yiqi Zhao and Matloob Khushi",
        "title": "A Survey of Forex and Stock Price Prediction Using Deep Learning",
        "comments": null,
        "journal-ref": "Appl. Syst. Innov. 2021, 4, 9",
        "doi": "10.3390/asi4010009",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The prediction of stock and foreign exchange (Forex) had always been a hot\nand profitable area of study. Deep learning application had proven to yields\nbetter accuracy and return in the field of financial prediction and\nforecasting. In this survey we selected papers from the DBLP database for\ncomparison and analysis. We classified papers according to different deep\nlearning methods, which included: Convolutional neural network (CNN), Long\nShort-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network\n(RNN), Reinforcement Learning, and other deep learning methods such as HAN,\nNLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable,\nmodel, and results of each article. The survey presented the results through\nthe most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe\nratio, and return rate. We identified that recent models that combined LSTM\nwith other methods, for example, DNN, are widely researched. Reinforcement\nlearning and other deep learning method yielded great returns and performances.\nWe conclude that in recent years the trend of using deep-learning based method\nfor financial modeling is exponentially rising.\n"
    },
    {
        "paper_id": 2103.09781,
        "authors": "Russell McKenna, Stefan Pfenninger, Heidi Heinrichs, Johannes Schmidt,\n  Iain Staffell, Katharina Gruber, Andrea N. Hahmann, Malte Jansen, Michael\n  Klingler, Natascha Landwehr, Xiaoli Guo Lars\\'en, Johan Lilliestam, Bryn\n  Pickering, Martin Robinius, Tim Tr\\\"ondle, Olga Turkovska, Sebastian Wehrle,\n  Jann Michael Weinand, Jan Wohland",
        "title": "Reviewing methods and assumptions for high-resolution large-scale\n  onshore wind energy potential assessments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid uptake of renewable energy technologies in recent decades has\nincreased the demand of energy researchers, policymakers and energy planners\nfor reliable data on the spatial distribution of their costs and potentials.\nFor onshore wind energy this has resulted in an active research field devoted\nto analysing these resources for regions, countries or globally. A particular\nthread of this research attempts to go beyond purely technical or spatial\nrestrictions and determine the realistic, feasible or actual potential for wind\nenergy. Motivated by these developments, this paper reviews methods and\nassumptions for analysing geographical, technical, economic and, finally,\nfeasible onshore wind potentials. We address each of these potentials in turn,\nincluding aspects related to land eligibility criteria, energy meteorology, and\ntechnical developments relating to wind turbine characteristics such as power\ndensity, specific rotor power and spacing aspects. Economic aspects of\npotential assessments are central to future deployment and are discussed on a\nturbine and system level covering levelized costs depending on locations, and\nthe system integration costs which are often overlooked in such analyses.\nNon-technical approaches include scenicness assessments of the landscape,\nexpert and stakeholder workshops, willingness to pay / accept elicitations and\nsocioeconomic cost-benefit studies. For each of these different potential\nestimations, the state of the art is critically discussed, with an attempt to\nderive best practice recommendations and highlight avenues for future research.\n"
    },
    {
        "paper_id": 2103.09813,
        "authors": "Mengda Li and Charles-Albert Lehalle",
        "title": "Do Word Embeddings Really Understand Loughran-McDonald's Polarities?",
        "comments": "31 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we perform a rigorous mathematical analysis of the word2vec\nmodel, especially when it is equipped with the Skip-gram learning scheme. Our\ngoal is to explain how embeddings, that are now widely used in NLP (Natural\nLanguage Processing), are influenced by the distribution of terms in the\ndocuments of the considered corpus. We use a mathematical formulation to shed\nlight on how the decision to use such a model makes implicit assumptions on the\nstructure of the language. We show how Markovian assumptions, that we discuss,\nlead to a very clear theoretical understanding of the formation of embeddings,\nand in particular the way it captures what we call frequentist synonyms. These\nassumptions allow to produce generative models and to conduct an explicit\nanalysis of the loss function commonly used by these NLP techniques. Moreover,\nwe produce synthetic corpora with different levels of structure and show\nempirically how the word2vec algorithm succeed, or not, to learn them. It leads\nus to empirically assess the capability of such models to capture structures on\na corpus of around 42 millions of financial News covering 12 years. That for,\nwe rely on the Loughran-McDonald Sentiment Word Lists largely used on financial\ntexts and we show that embeddings are exposed to mixing terms with opposite\npolarity, because of the way they can treat antonyms as frequentist synonyms.\nBeside we study the non-stationarity of such a financial corpus, that has\nsurprisingly not be documented in the literature. We do it via time series of\ncosine similarity between groups of polarized words or company names, and show\nthat embedding are indeed capturing a mix of English semantics and joined\ndistribution of words that is difficult to disentangle.\n"
    },
    {
        "paper_id": 2103.09987,
        "authors": "Raymond C. W. Leung and Yu-Man Tam",
        "title": "Statistical Arbitrage Risk Premium by Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How to hedge factor risks without knowing the identities of the factors? We\nfirst prove a general theoretical result: even if the exact set of factors\ncannot be identified, any risky asset can use some portfolio of similar peer\nassets to hedge against its own factor exposures. A long position of a risky\nasset and a short position of a \"replicate portfolio\" of its peers represent\nthat asset's factor residual risk. We coin the expected return of an asset's\nfactor residual risk as its Statistical Arbitrage Risk Premium (SARP). The\nchallenge in empirically estimating SARP is finding the peers for each asset\nand constructing the replicate portfolios. We use the elastic-net, a machine\nlearning method, to project each stock's past returns onto that of every other\nstock. The resulting high-dimensional but sparse projection vector serves as\ninvestment weights in constructing the stocks' replicate portfolios. We say a\nstock has high (low) Statistical Arbitrage Risk (SAR) if it has low (high)\nR-squared with its peers. The key finding is that \"unique\" stocks have both a\nhigher SARP and higher excess returns than \"ubiquitous\" stocks: in the\ncross-section, high SAR stocks have a monthly SARP (monthly excess returns)\nthat is 1.101% (0.710%) greater than low SAR stocks. The average SAR across all\nstocks is countercyclical. Our results are robust to controlling for various\nknown priced factors and characteristics.\n"
    },
    {
        "paper_id": 2103.10157,
        "authors": "Tal Miller",
        "title": "Leveraged ETF Investing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It is common knowledge that leverage can increase the potential returns of an\ninvestment, at the expense of increased risk. For a passive investor in the\nstock market, leverage can be achieved using margin debt or leveraged-ETFs. We\nperform bootstrapped Monte-Carlo simulations of leveraged (and unleveraged)\nmixed portfolios of stocks and bonds, based on past stock market data, and show\nthat leverage can amplify the potential returns, without significantly\nincreasing the risk for long-term investors.\n"
    },
    {
        "paper_id": 2103.10813,
        "authors": "Xiaoyue Li, A. Sinem Uysal, John M. Mulvey",
        "title": "Multi-Period Portfolio Optimization using Model Predictive Control with\n  Mean-Variance and Risk Parity Frameworks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We employ model predictive control for a multi-period portfolio optimization\nproblem. In addition to the mean-variance objective, we construct a portfolio\nwhose allocation is given by model predictive control with a risk-parity\nobjective, and provide a successive convex program algorithm that provides 30\ntimes faster and robust solutions in the experiments. Computational results on\nthe multi-asset universe show that multi-period models perform better than\ntheir single period counterparts in out-of-sample period, 2006-2020. The\nout-of-sample risk-adjusted performance of both mean-variance and risk-parity\nformulations beat the fix-mix benchmark, and achieve Sharpe ratio of 0.64 and\n0.97, respectively.\n"
    },
    {
        "paper_id": 2103.1086,
        "authors": "Yuchen Fang, Kan Ren, Weiqing Liu, Dong Zhou, Weinan Zhang, Jiang\n  Bian, Yong Yu, Tie-Yan Liu",
        "title": "Universal Trading for Order Execution with Oracle Policy Distillation",
        "comments": "Accepted in AAAI 2021, the code and the supplementary materials are\n  in https://seqml.github.io/opd/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a fundamental problem in algorithmic trading, order execution aims at\nfulfilling a specific trading order, either liquidation or acquirement, for a\ngiven instrument. Towards effective execution strategy, recent years have\nwitnessed the shift from the analytical view with model-based market\nassumptions to model-free perspective, i.e., reinforcement learning, due to its\nnature of sequential decision optimization. However, the noisy and yet\nimperfect market information that can be leveraged by the policy has made it\nquite challenging to build up sample efficient reinforcement learning methods\nto achieve effective order execution. In this paper, we propose a novel\nuniversal trading policy optimization framework to bridge the gap between the\nnoisy yet imperfect market states and the optimal action sequences for order\nexecution. Particularly, this framework leverages a policy distillation method\nthat can better guide the learning of the common policy towards practically\noptimal execution by an oracle teacher with perfect information to approximate\nthe optimal trading strategy. The extensive experiments have shown significant\nimprovements of our method over various strong baselines, with reasonable\ntrading actions.\n"
    },
    {
        "paper_id": 2103.10872,
        "authors": "Giuseppe Calafiore, Giulia Fracastoro, and Anton V. Proskurnikov",
        "title": "Optimal Clearing Payments in a Financial Contagion Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial networks are characterized by complex structures of mutual\nobligations. These obligations are fulfilled entirely or in part (when defaults\noccur) via a mechanism called clearing, which determines a set of payments that\nsettle the claims by respecting rules such as limited liability, absolute\npriority, and proportionality (pro-rated payments). In the presence of shocks\non the financial system, however, the clearing mechanism may lead to cascaded\ndefaults and eventually to financial disaster. In this paper, we first study\nthe clearing model under pro-rated payments of Eisenberg and Noe, and we derive\nnovel necessary and sufficient conditions for the uniqueness of the clearing\npayments, valid for an arbitrary topology of the financial network. Then, we\nargue that the proportionality rule is one of the factors responsible for\ncascaded defaults, and that the overall system loss can be reduced if this rule\nis lifted. The proposed approach thus shifts the focus from the individual\ninterest to the overall system's interest to control and contain adverse\neffects of cascaded failures, and we show that clearing payments in this\nsetting can be computed by solving suitable convex optimization problems.\n"
    },
    {
        "paper_id": 2103.10925,
        "authors": "Steven Campbell, Ting-Kam Leonard Wong",
        "title": "Functional portfolio optimization in stochastic portfolio theory",
        "comments": "41 pages, 7 figures, 1 table. Revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a concrete and fully implementable approach to the\noptimization of functionally generated portfolios in stochastic portfolio\ntheory. The main idea is to optimize over a family of rank-based portfolios\nparameterized by an exponentially concave function on the unit interval. This\nchoice can be motivated by the long term stability of the capital distribution\nobserved in large equity markets, and allows us to circumvent the curse of\ndimensionality. The resulting optimization problem, which is convex, allows for\nvarious regularizations and constraints to be imposed on the generating\nfunction. We prove an existence and uniqueness result for our optimization\nproblem and provide a stability estimate in terms of a Wasserstein metric of\nthe input measure. Then, we formulate a discretization which can be implemented\nnumerically using available software packages and analyze its approximation\nerror. Finally, we present empirical examples using CRSP data from the US stock\nmarket, including the performance of the portfolios allowing for dividends,\ndefaults, and transaction costs.\n"
    },
    {
        "paper_id": 2103.10937,
        "authors": "Martin Higgins",
        "title": "Locational Marginal Pricing: Towards a Free Market in Power",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Nothing has done more to empower the free market, enterprise, and meritocracy\nthan the spread of electricity and power to everyone. The power system has been\nthe precursor to the greatest period of innovation in our history and has meant\nthat visionaries with revolutionary ideas can compete with those with capital,\npolitical power, and means. Electricity, therefore, has been the great\nequalising force of the last 150 years, enhancing the productivity of the\nmasses and granting prosperity to whole swathes of our nation. Whilst\nelectricity has been one of the single largest innovations in enhancing the\npower of free markets, it is somewhat ironic that the way power is sold to\nconsumers is largely unfree. The market is highly regulated, centralised, and\nis often used for political football by cynical politicians on both sides of\nthe political spectrum. Introducing Locational Marginal Pricing into the UK\ngrid system will increase economic freedom in the consumer markets for power,\nreduce prices for the poorest in the UK, decrease transmission losses, increase\nthe permeation of low carbon generation in the grid, and incentivise investment\nin the UK's Northern Powerhouse initiative.\n"
    },
    {
        "paper_id": 2103.10938,
        "authors": "David Orrell, Monireh Houshmand",
        "title": "Quantum propensity in economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper describes an approach to economics that is inspired by quantum\ncomputing, and is motivated by the need to develop a consistent quantum\nmathematical framework for economics. The traditional neoclassical approach\nassumes that rational utility-optimisers drive market prices to a stable\nequilibrium, subject to external perturbations. While this approach has been\nhighly influential, it has come under increasing criticism following the\nfinancial crisis of 2007/8. The quantum approach, in contrast, is inherently\nprobabilistic and dynamic. Decision-makers are described, not by a utility\nfunction, but by a propensity function which specifies the probability of\ntransacting. We show how a number of cognitive phenomena such as preference\nreversal and the disjunction effect can be modelled by using a simple quantum\ncircuit to generate an appropriate propensity function. Conversely, a general\npropensity function can be quantized to incorporate effects such as\ninterference and entanglement that characterise human decision-making.\nApplications to some common problems in economics and finance are discussed.\n"
    },
    {
        "paper_id": 2103.10958,
        "authors": "Kerstin D\\\"achert, Ria Grindel, Elisabeth Leoff, Jonas Mahnkopp,\n  Florian Schirra and J\\\"org Wenzel",
        "title": "Multicriteria asset allocation in practice",
        "comments": "24 pages, 4 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we consider the strategic asset allocation of an insurance\ncompany. This task can be seen as a special case of portfolio optimization. In\nthe 1950s, Markowitz proposed to formulate portfolio optimization as a\nbicriteria optimization problem considering risk and return as objectives.\nHowever, recent developments in the field of insurance require four and more\nobjectives to be considered, among them the so-called solvency ratio that stems\nfrom the Solvency II directive of the European Union issued in 2009. Moreover,\nthe distance to the current portfolio plays an important role. While literature\non portfolio optimization with three objectives is already scarce, applications\nwith four and more objectives have not yet been solved so far by\nmulti-objective approaches based on scalarizations. However, recent algorithmic\nimprovements in the field of exact multi-objective methods allow the\nincorporation of many objectives and the generation of well-spread\nrepresentations within few iterations. We describe the implementation of such\nan algorithm for a strategic asset allocation with four objective functions and\ndemonstrate its usefulness for the practitioner. Our approach is in operative\nuse in a German insurance company. Our partners report a significant\nimprovement in their decision making process since, due to the proper\nintegration of the new objectives, the software proposes portfolios of much\nbetter quality than before within short running time.\n"
    },
    {
        "paper_id": 2103.10986,
        "authors": "Ivan Kitov",
        "title": "Real GDP per capita: global redistribution of economic power",
        "comments": "55 pages, 64 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Growth rate of real GDP per capita, GDPpc, is represented as a sum of two\ncomponents, a monotonically decreasing economic trend and fluctuations related\nto population change. The economic trend is modelled by an inverse function of\nGDPpc with a constant numerator which varies for the largest developed\neconomies. In 2006, a statistical analysis conducted for 19 selected OECD\ncountries for the period between 1950 and 2003 showed a very weak linear trend\nin the annual GDPpc increment for the largest economies: the USA, Japan,\nFrance, Italy, and Spain. The UK, Australia, and Canada showed a slightly\nsteeper positive linear trend. The 2012 revision showed that the positive\ntrends became much lower and some of them fell below zero due to the Great\nRecession. The fluctuations around the trend values are characterized by a\nquasi-normal distribution with heavy and asymmetric tails. This research\nrevises the previous estimates and extends the set of studied countries by\neconomies in East Europe, Latin America, BRICS, Africa, and Asia including\nseveral positive outliers with extremely fast growth. The change in GDP\ndefinitions and measuring procedures with time and economic source is discussed\nin relation to the statistical significance of the trend estimates and data\nquality requirements for a consistent economic model. The relative performance\nof all counties since 1960 is compared according to the predicted total GDPpc\ngrowth as a function of the initial value. The performance in the 21st century\nis analyzed separately as revealing potential and actual shifts in the global\neconomic powers.\n"
    },
    {
        "paper_id": 2103.10989,
        "authors": "Fouad Marri and Khouzeima Moutanabbir",
        "title": "Risk aggregation and capital allocation using a new generalized\n  Archimedean copula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we address risk aggregation and capital allocation problems in\nthe presence of dependence between risks. The dependence structure is defined\nby a mixed Bernstein copula which represents a generalization of the well-known\nArchimedean copulas. Using this new copula, the probability density function\nand the cumulative distribution function of the aggregate risk are obtained.\nThen, closed-form expressions for basic risk measures, such as tail\nvalue-at-risk(TVaR) and TVaR-based allocations, are derived.\n"
    },
    {
        "paper_id": 2103.11042,
        "authors": "Saurabh Mishra, Robert Koopman, Giuditta De-Prato, Anand Rao, Israel\n  Osorio-Rodarte, Julie Kim, Nikola Spatafora, Keith Strier, and Andrea\n  Zaccaria",
        "title": "AI Specialization for Pathways of Economic Diversification",
        "comments": "27 pages, 20 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growth in AI is rapidly transforming the structure of economic\nproduction. However, very little is known about how within-AI specialization\nmay relate to broad-based economic diversification. This paper provides a\ndata-driven framework to integrate the interconnection between AI-based\nspecialization with goods and services export specialization to help design\nfuture comparative advantage based on the inherent capabilities of nations.\nUsing detailed data on private investment in AI and export specialization for\nmore than 80 countries, we propose a systematic framework to help identify the\nconnection from AI to goods and service sector specialization. The results are\ninstructive for nations that aim to harness AI specialization to help guide\nsources of future competitive advantage. The operational framework could help\ninform the public and private sector to uncover connections with nearby areas\nof specialization.\n"
    },
    {
        "paper_id": 2103.11047,
        "authors": "Jeffrey D. Michler, Frederi G. Viens, Gerald E. Shively",
        "title": "Risk, Agricultural Production, and Weather Index Insurance in Village\n  India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the sources of variability in agricultural production and\ntheir relative importance in the context of weather index insurance for\nsmallholder farmers in India. Using parcel-level panel data, multilevel\nmodeling, and Bayesian methods we measure how large a role seasonal variation\nin weather plays in explaining yield variance. Seasonal variation in weather\naccounts for 19-20 percent of total variance in crop yields. Motivated by this\nresult, we derive pricing and payout schedules for actuarially fair index\ninsurance. These calculations shed light on the low uptake rates of index\ninsurance and provide direction for designing more suitable index insurance.\n"
    },
    {
        "paper_id": 2103.11051,
        "authors": "Jeffrey D. Michler and Benjamin M. Gramig",
        "title": "Differentiation in a Two-Dimensional Market with Endogenous Sequential\n  Entry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Previous research on two-dimensional extensions of Hotelling's location game\nhas argued that spatial competition leads to maximum differentiation in one\ndimensions and minimum differentiation in the other dimension. We expand on\nexisting models to allow for endogenous entry into the market. We find that\ncompetition may lead to the min/max finding of previous work but also may lead\nto maximum differentiation in both dimensions. The critical issue in\ndetermining the degree of differentiation is if existing firms are seeking to\ndeter entry of a new firm or to maximizing profits within an existing, stable\nmarket.\n"
    },
    {
        "paper_id": 2103.1118,
        "authors": "Jacob Bjerre Skov and David Skovmand",
        "title": "Dynamic Term Structure Models for SOFR Futures",
        "comments": "30 pages, 12 figures, and 7 tables",
        "journal-ref": null,
        "doi": "10.2139/ssrn.3692283",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The LIBOR rate is currently scheduled for discontinuation, and the\nreplacement advocated by regulators in the US is the Secured Overnight\nFinancing Rate (SOFR). The change has the potential to disrupt the $200\ntrillion market of derivatives and debt tied to the LIBOR. The only SOFR linked\nderivative with any significant liquidity and trading history is the SOFR\nfutures contract, traded at the CME since 2018. We use the historical record of\nfutures prices to construct dynamic arbitrage-free models for the SOFR term\nstructure. The models allow you to construct forward-looking SOFR term rates,\nimply a SOFR discounting curve and price and risk and risk manage SOFR\nderivatives, not yet liquidly traded in the market. We find that a standard\nthree-factor Gaussian arbitrage-free Nelson-Siegel model describes term rates\nvery well but a shadow-rate extension is needed to describe the behaviour near\nthe zero-boundary. We also find that the jumps and seasonal effects observed in\nSOFR, do not need to be specifically accounted for in a model for the futures\nprices. Finally we study the so-called convexity correction and find that it\nbecomes significant beyond the 2 year maturity. For validation purposes we\ndemonstrate that our model aligns closely with the methodology used by the\nFederal Reserve to publish indicative SOFR term rates.\n"
    },
    {
        "paper_id": 2103.11341,
        "authors": "Dave Cliff",
        "title": "Parameterised-Response Zero-Intelligence Traders",
        "comments": "56 pages, 24 figures, 90 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I introduce PRZI (Parameterised-Response Zero Intelligence), a new form of\nzero-intelligence trader intended for use in simulation studies of the dynamics\nof continuous double auction markets. Like Gode & Sunder's classic ZIC trader,\nPRZI generates quote-prices from a random distribution over some specified\ndomain of allowable quote-prices. Unlike ZIC, which uses a uniform distribution\nto generate prices, the probability distribution in a PRZI trader is\nparameterised in such a way that its probability mass function (PMF) is\ndetermined by a real-valued control variable s in the range [-1.0, +1.0] that\ndetermines the _strategy_ for that trader. When s=0, a PRZI trader is identical\nto ZIC, with a uniform PMF; but when |s|=~1 the PRZI trader's PMF becomes\nmaximally skewed to one extreme or the other of the price-range, thereby making\nits quote-prices more or less urgent, biasing the quote-price distribution\ntoward or away from the trader's limit-price. To explore the co-evolutionary\ndynamics of populations of PRZI traders that dynamically adapt their\nstrategies, I show results from long-term market experiments in which each\ntrader uses a simple stochastic hill-climber algorithm to repeatedly evaluate\nalternative s-values and choose the most profitable at any given time. In these\nexperiments the profitability of any particular s-value may be non-stationary\nbecause the profitability of one trader's strategy at any one time can depend\non the mix of strategies being played by the other traders at that time, which\nare each themselves continuously adapting. Results from these market\nexperiments demonstrate that the population of traders' strategies can exhibit\nrich dynamics, with periods of stability lasting over hundreds of thousands of\ntrader interactions interspersed by occasional periods of change. Python\nsource-code for the work reported here has been made publicly available on\nGitHub.\n"
    },
    {
        "paper_id": 2103.11435,
        "authors": "Ariel Neufeld, Julian Sester",
        "title": "A deep learning approach to data-driven model-free pricing and to\n  martingale optimal transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel and highly tractable supervised learning approach based\non neural networks that can be applied for the computation of model-free price\nbounds of, potentially high-dimensional, financial derivatives and for the\ndetermination of optimal hedging strategies attaining these bounds. In\nparticular, our methodology allows to train a single neural network offline and\nthen to use it online for the fast determination of model-free price bounds of\na whole class of financial derivatives with current market data. We show the\napplicability of this approach and highlight its accuracy in several examples\ninvolving real market data. Further, we show how a neural network can be\ntrained to solve martingale optimal transport problems involving fixed marginal\ndistributions instead of financial market data.\n"
    },
    {
        "paper_id": 2103.11455,
        "authors": "Huanming Zhang, Zhengyong Jiang, Jionglong Su",
        "title": "A Deep Deterministic Policy Gradient-based Strategy for Stocks Portfolio\n  Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the improvement of computer performance and the development of\nGPU-accelerated technology, trading with machine learning algorithms has\nattracted the attention of many researchers and practitioners. In this\nresearch, we propose a novel portfolio management strategy based on the\nframework of Deep Deterministic Policy Gradient, a policy-based reinforcement\nlearning framework, and compare its performance to that of other trading\nstrategies. In our framework, two Long Short-Term Memory neural networks and\ntwo fully connected neural networks are constructed. We also investigate the\nperformance of our strategy with and without transaction costs. Experimentally,\nwe choose eight US stocks consisting of four low-volatility stocks and four\nhigh-volatility stocks. We compare the compound annual return rate of our\nstrategy against seven other strategies, e.g., Uniform Buy and Hold,\nExponential Gradient and Universal Portfolios. In our case, the compound annual\nreturn rate is 14.12%, outperforming all other strategies. Furthermore, in\nterms of Sharpe Ratio (0.5988), our strategy is nearly 33% higher than that of\nthe second-best performing strategy.\n"
    },
    {
        "paper_id": 2103.11459,
        "authors": "Mohammadreza Ghanbari, Mahdi Goldani",
        "title": "Support Vector Regression Parameters Optimization using Golden Sine\n  Algorithm and its application in stock market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Support vector machine modeling is a new approach in machine learning for\nclassification showing good performance on forecasting problems of small\nsamples and high dimensions. Later, it promoted to Support Vector Regression\n(SVR) for regression problems. A big challenge for achieving reliable is the\nchoice of appropriate parameters. Here, a novel Golden sine algorithm (GSA)\nbased SVR is proposed for proper selection of the parameters. For comparison,\nthe performance of the proposed algorithm is compared with eleven other\nmeta-heuristic algorithms on some historical stock prices of technological\ncompanies from Yahoo Finance website based on Mean Squared Error and Mean\nAbsolute Percent Error. The results demonstrate that the given algorithm is\nefficient for tuning the parameters and is indeed competitive in terms of\naccuracy and computing time.\n"
    },
    {
        "paper_id": 2103.11504,
        "authors": "Laura Doval and Vasiliki Skreta",
        "title": "Purchase history and product personalization",
        "comments": "Replaced to correct figure placement",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Product personalization opens the door to price discrimination. A rich\nproduct line allows firms to better tailor products to consumers' tastes, but\nthe mere choice of a product carries valuable information about consumers that\ncan be leveraged for price discrimination. We study this trade-off in an\nupstream-downstream model, where a consumer buys a good of variable quality\nupstream, followed by an indivisible good downstream. The downstream firm's use\nof the consumer's purchase history for price discrimination introduces a novel\ndistortion: The upstream firm offers a subset of the products that it would\noffer if, instead, it could jointly design its product line and downstream\npricing. By controlling the degree of product personalization the upstream firm\ncurbs ratcheting forces that result from the consumer facing downstream price\ndiscrimination.\n"
    },
    {
        "paper_id": 2103.11547,
        "authors": "Alexander M. Petersen and Mohammed E. Ahmed and Ioannis Pavlidis",
        "title": "Grand challenges and emergent modes of convergence science",
        "comments": "15 pages, 5 figures; Supplementary Information: 25 pages, 12 Figures\n  and 5 Tables",
        "journal-ref": "Humanities and Social Sciences Communications, 8, 194 (2021)",
        "doi": "10.1057/s41599-021-00869-9",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To address complex problems, scholars are increasingly faced with challenges\nof integrating diverse knowledge domains. We analyzed the evolution of this\nconvergence paradigm in the broad ecosystem of brain science, which provides a\nreal-time testbed for evaluating two modes of cross-domain integration -\nsubject area exploration via expansive learning and cross-disciplinary\ncollaboration among domain experts. We show that research involving both modes\nfeatures a 16% citation premium relative to a mono-disciplinary baseline.\nFurther comparison of research integrating neighboring versus distant research\ndomains shows that the cross-disciplinary mode is essential for integrating\nacross relatively large disciplinary distances. Yet we find research utilizing\ncross-domain subject area exploration alone - a convergence shortcut - to be\ngrowing in prevalence at roughly 3% per year, significantly faster than the\nalternative cross-disciplinary mode, despite being less effective at\nintegrating domains and markedly less impactful. By measuring shifts in the\nprevalence and impact of different convergence modes in the 5-year intervals\nbefore and after 2013, our results indicate that these counterproductive\npatterns may relate to competitive pressures associated with global Human Brain\nflagship funding initiatives. Without additional policy guidance, such Grand\nChallenge flagships may unintentionally incentivize such convergence shortcuts,\nthereby undercutting the advantages of cross-disciplinary teams in tackling\nchallenges calling on convergence.\n"
    },
    {
        "paper_id": 2103.11557,
        "authors": "Yanzhao Li, Ju'e Guo, Yongwu Li, Xu Zhang",
        "title": "Optimal exit decision of venture capital under time-inconsistent\n  preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes two kinds of time-inconsistent preferences (i.e. time\nflow inconsistency and critical time point inconsistency) to further advance\nthe research on the exit decision of venture capital. Time-inconsistent\npreference, different from time-consistent preference, assumes that decision\nmakers prefer recent returns rather than future returns. Based on venture\ncapitalists' understanding of future preferences, we consider four types of\nventure capitalists, namely time-consistent venture capitalists, venture\ncapitalists who only realize critical time point inconsistency, naive venture\ncapitalists and sophisticated venture capitalists, of which the latter three\nare time-inconsistent. All types of time-inconsistent venture capitalists are\naware of critical time point inconsistency. Naive venture capitalists\nmisunderstand time flow inconsistency while sophisticated ones understand it\ncorrectly. We propose an optimal exit timing of venture capital model. Then we\nderive and compare the above four types of venture capitalists' exit\nthresholds. The main results are as follows: (1) all types of time-inconsistent\nventure capitalists tend to exit earlier than time-consistent venture\ncapitalists. (2) The longer the expire date are, the more likely venture\ncapitalists are to delay the exit, but the delay degree decreases successively\n(venture capitalists who only realize critical time point inconsistency > naive\nventure capitalists > sophisticated venture capitalists).\n"
    },
    {
        "paper_id": 2103.11706,
        "authors": "M. Merz, R. Richman, T. Tsanakas, M.V. W\\\"uthrich",
        "title": "Interpreting Deep Learning Models with Marginal Attribution by\n  Conditioning on Quantiles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A vastly growing literature on explaining deep learning models has emerged.\nThis paper contributes to that literature by introducing a global\ngradient-based model-agnostic method, which we call Marginal Attribution by\nConditioning on Quantiles (MACQ). Our approach is based on analyzing the\nmarginal attribution of predictions (outputs) to individual features (inputs).\nSpecificalllly, we consider variable importance by mixing (global) output\nlevels and, thus, explain how features marginally contribute across different\nregions of the prediction space. Hence, MACQ can be seen as a marginal\nattribution counterpart to approaches such as accumulated local effects (ALE),\nwhich study the sensitivities of outputs by perturbing inputs. Furthermore,\nMACQ allows us to separate marginal attribution of individual features from\ninteraction effect, and visually illustrate the 3-way relationship between\nmarginal attribution, output level, and feature value.\n"
    },
    {
        "paper_id": 2103.11734,
        "authors": "Etienne Chevalier, Sergio Pulido, Elizabeth Z\\'u\\~niga",
        "title": "American options in the Volterra Heston model",
        "comments": "38 pages, 1 table, 8 figures. Forthcoming in SIAM Journal on\n  Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We price American options using kernel-based approximations of the Volterra\nHeston model. We choose these approximations because they allow\nsimulation-based techniques for pricing. We prove the convergence of American\noption prices in the approximating sequence of models towards the prices in the\nVolterra Heston model. A crucial step in the proof is to exploit the affine\nstructure of the model in order to establish explicit formulas and convergence\nresults for the conditional Fourier-Laplace transform of the log price and an\nadjusted version of the forward variance. We illustrate with numerical examples\nour convergence result and the behavior of American option prices with respect\nto certain parameters of the model.\n"
    },
    {
        "paper_id": 2103.11753,
        "authors": "Davide Cipullo and Marco Le Moglie",
        "title": "To vote, or not to vote: on the epidemiological impact of electoral\n  campaigns at the time of COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Elections are crucial for legitimating modern democracies, and giving all\ncandidates the possibility to run a proper electoral campaign is necessary for\nelections' success in providing such legitimization. Yet, during a pandemic,\nthe risk that electoral campaigns would enhance the spread of the disease\nexists and is substantive. In this work, we estimate the causal impact of\nelectoral campaigns on the spread of COVID-19. Exploiting plausibly exogenous\nvariation in the schedule of local elections across Italy, we show that the\nelectoral campaign preceding this latter led to a significant worsening of the\nepidemiological situation related to the disease. Our results strongly\nhighlight the importance of undertaking stringent measures along the entire\nelectoral process to minimize its epidemiological consequences.\n"
    },
    {
        "paper_id": 2103.11772,
        "authors": "Boliang Lin and Ruixi Lin",
        "title": "The black hole of logistics costs of digitizing commodity money",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we reveal the depreciation mechanism of representative money\n(banknotes) from the perspective of logistics warehousing costs. Although it\nhas long been the dream of economists to stabilize the buying power of the\nmonetary units, the goal we have honest money always broken since the central\nbank depreciate the currency without limit. From the point of view of modern\nlogistics, the key functions of money are the store of value and low logistics\n(circulation and warehouse) cost. Although commodity money (such as gold and\nsilver) has the advantages of a wealth store, its disadvantage is the high\nlogistics cost. In comparison to commodity money, credit currency and digital\ncurrency cannot protect wealth from loss over a long period while their\nlogistics costs are negligible. We proved that there is not such honest money\nfrom the perspective of logistics costs, which is both the store of value like\nprecious metal and without logistics costs in circulation like digital\ncurrency. The reason hidden in the back of the depreciation of banknotes is the\nblack hole of storage charge of the anchor overtime after digitizing commodity\nmoney. Accordingly, it is not difficult to infer the inevitable collapse of the\nBretton woods system. Therefore, we introduce a brand-new currency named honest\ndevalued stable-coin and built a attenuation model of intrinsic value of the\nhonest money based on the change mechanism of storage cost of anchor assets,\nlike gold, which will lay the theoretical foundation for a stable monetary\nsystem.\n"
    },
    {
        "paper_id": 2103.11869,
        "authors": "Yiyan Huang, Cheuk Hang Leung, Qi Wu, Xing Yan",
        "title": "Robust Orthogonal Machine Learning of Treatment Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Causal learning is the key to obtaining stable predictions and answering\n\\textit{what if} problems in decision-makings. In causal learning, it is\ncentral to seek methods to estimate the average treatment effect (ATE) from\nobservational data. The Double/Debiased Machine Learning (DML) is one of the\nprevalent methods to estimate ATE. However, the DML estimators can suffer from\nan \\textit{error-compounding issue} and even give extreme estimates when the\npropensity scores are close to 0 or 1. Previous studies have overcome this\nissue through some empirical tricks such as propensity score trimming, yet none\nof the existing works solves it from a theoretical standpoint. In this paper,\nwe propose a \\textit{Robust Causal Learning (RCL)} method to offset the\ndeficiencies of DML estimators. Theoretically, the RCL estimators i) satisfy\nthe (higher-order) orthogonal condition and are as \\textit{consistent and\ndoubly robust} as the DML estimators, and ii) get rid of the error-compounding\nissue. Empirically, the comprehensive experiments show that: i) the RCL\nestimators give more stable estimations of the causal parameters than DML; ii)\nthe RCL estimators outperform traditional estimators and their variants when\napplying different machine learning models on both simulation and benchmark\ndatasets, and a mimic consumer credit dataset generated by WGAN.\n"
    },
    {
        "paper_id": 2103.11948,
        "authors": "Hans Buehler, Phillip Murray, Mikko S. Pakkanen, Ben Wood",
        "title": "Deep Hedging: Learning Risk-Neutral Implied Volatility Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a numerically efficient approach for learning a risk-neutral\nmeasure for paths of simulated spot and option prices up to a finite horizon\nunder convex transaction costs and convex trading constraints. This approach\ncan then be used to implement a stochastic implied volatility model in the\nfollowing two steps: 1. Train a market simulator for option prices, as\ndiscussed for example in our recent; 2. Find a risk-neutral density,\nspecifically the minimal entropy martingale measure. The resulting model can be\nused for risk-neutral pricing, or for Deep Hedging in the case of transaction\ncosts or trading constraints. To motivate the proposed approach, we also show\nthat market dynamics are free from \"statistical arbitrage\" in the absence of\ntransaction costs if and only if they follow a risk-neutral measure. We\nadditionally provide a more general characterization in the presence of convex\ntransaction costs and trading constraints. These results can be seen as an\nanalogue of the fundamental theorem of asset pricing for statistical arbitrage\nunder trading frictions and are of independent interest.\n"
    },
    {
        "paper_id": 2103.12138,
        "authors": "Michele Fioretti, Victor Saint-Jean, Simon C. Smith",
        "title": "The Shared Cost of Pursuing Shareholder Value",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We propose a portable framework to infer shareholders' preferences and\ninfluences on firms' prosocial decisions and the costs these decisions impose\non firms and shareholders. Using quasi-experimental variations from the media\ncoverage of firms' annual general meetings, we find that shareholders support\ncostly prosocial decisions, such as covid-related donations and private\nsanctions on Russia, if they can earn image gains from them. In contrast,\nshareholders that the public cannot readily associate with specific firms, like\nfinancial corporations with large portfolios, oppose them. These prosocial\nexpenditures crowd out investments at exposed firms, reducing productivity and\nearnings by between 1 and 3\\%: pursuing the values of some shareholders comes\nat the cost of others, which the shareholders' monitoring motivated by\nheterogeneous preferences could prevent.\n"
    },
    {
        "paper_id": 2103.12159,
        "authors": "Lena Janys and Bettina Siflinger",
        "title": "Mental Health and Abortions among Young Women: Time-varying Unobserved\n  Heterogeneity, Health Behaviors, and Risky Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we provide causal evidence on abortions and risky health\nbehaviors as determinants of mental health development among young women. Using\nadministrative in- and outpatient records from Sweden, we apply a novel grouped\nfixed-effects estimator proposed by Bonhomme and Manresa (2015) to allow for\ntime-varying unobserved heterogeneity. We show that the positive association\nobtained from standard estimators shrinks to zero once we control for grouped\ntime-varying unobserved heterogeneity. We estimate the group-specific profiles\nof unobserved heterogeneity, which reflect differences in unobserved risk to be\ndiagnosed with a mental health condition. We then analyze mental health\ndevelopment and risky health behaviors other than unwanted pregnancies across\ngroups. Our results suggest that these are determined by the same type of\nunobserved heterogeneity, which we attribute to the same unobserved process of\ndecision-making. We develop and estimate a theoretical model of risky choices\nand mental health, in which mental health disparity across groups is generated\nby different degrees of self-control problems. Our findings imply that mental\nhealth concerns cannot be used to justify restrictive abortion policies.\nMoreover, potential self-control problems should be targeted as early as\npossible to combat future mental health consequences.\n"
    },
    {
        "paper_id": 2103.12193,
        "authors": "Eduardo Laguna-Muggenburg, Monica Bhole, Michael Meaney",
        "title": "Understanding Factors that Influence Upskilling",
        "comments": "11 pages, 4 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the motivation and means through which individuals expand\ntheir skill-set by analyzing a survey of applicants from the Facebook Jobs\nproduct. Individuals who report being influenced by their networks or local\neconomy are over 29% more likely to have a postsecondary degree, but peer\neffects still exist among those who do not acknowledge such influences. Users\nwith postsecondary degrees are more likely to upskill in general, by continuing\ncoursework or applying to higher-skill jobs, though the latter is more common\namong users across all education backgrounds. These findings indicate that\npolicies aimed at connecting individuals with different educational backgrounds\ncan encourage upskilling. Policies that encourage users to enroll in coursework\nmay not be as effective among individuals with a high school degree or less.\nInstead, connecting such individuals to opportunities that value skills\nacquired outside of a formal education, and allow for on-the-job training, may\nbe more effective.\n"
    },
    {
        "paper_id": 2103.12345,
        "authors": "Yijian Chuan, Chaoyi Zhao, Zhenrui He, and Lan Wu",
        "title": "The Success of AdaBoost and Its Application in Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel approach to explain why AdaBoost is a successful\nclassifier. By introducing a measure of the influence of the noise points (ION)\nin the training data for the binary classification problem, we prove that there\nis a strong connection between the ION and the test error. We further identify\nthat the ION of AdaBoost decreases as the iteration number or the complexity of\nthe base learners increases. We confirm that it is impossible to obtain a\nconsistent classifier without deep trees as the base learners of AdaBoost in\nsome complicated situations. We apply AdaBoost in portfolio management via\nempirical studies in the Chinese market, which corroborates our theoretical\npropositions.\n"
    },
    {
        "paper_id": 2103.12419,
        "authors": "Artur Sokolovsky, Luca Arnaboldi, Jaume Bacardit, Thomas Gross",
        "title": "Volume-Centred Range Bars: Novel Interpretable Representation of\n  Financial Markets Designed for Machine Learning Applications",
        "comments": "The reproducibility package available at:\n  https://doi.org/10.5281/zenodo.4629567",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial markets are a source of non-stationary multidimensional time series\nwhich has been drawing attention for decades. Each financial instrument has its\nspecific changing-over-time properties, making its analysis a complex task.\nHence, improvement of understanding and development of more informative,\ngeneralisable market representations are essential for the successful operation\nin financial markets, including risk assessment, diversification, trading, and\norder execution. In this study, we propose a volume-price-based market\nrepresentation for making financial time series more suitable for machine\nlearning pipelines. We use a statistical approach for evaluating the\nrepresentation. Through the research questions, we investigate, i) whether the\nproposed representation allows the more efficient design of machine learning\nmodels; ii) whether the proposed representation leads to increased performance\nover the price levels market pattern; iii) whether the proposed representation\nperforms better on the liquid markets, and iv) whether SHAP feature\ninteractions are reliable to be used in the considered setting. Our analysis\nshows that the proposed volume-based method allows successful classification of\nthe financial time series patterns, and also leads to better classification\nperformance than the price levels-based method, excelling specifically on more\nliquid financial instruments. Finally, we propose an approach for obtaining\nfeature interactions directly from tree-based models and compare the outcomes\nto those of the SHAP method. This results in the significant similarity between\nthe two methods, hence we claim that SHAP feature interactions are reliable to\nbe used in the setting of financial markets.\n"
    },
    {
        "paper_id": 2103.12461,
        "authors": "Konstantin H\\\"ausler, Wolfgang Karl H\\\"ardle",
        "title": "Cryptocurrency Dynamics: Rodeo or Ascot?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We model the dynamics of the cryptocurrency (CC) asset class via a stochastic\nvolatility with correlated jumps (SVCJ) model with rolling-window parameter\nestimates. By analyzing the time-series of parameters, stylized patterns are\nobservable which are robust to changes of the window size and supported by\ncluster analysis. During bullish periods, volatility stabilizes at low levels\nand the size and volatility of jumps in mean decreases. In bearish periods\nthough, volatility increases and takes longer to return to its long-run trend.\nFurthermore, jumps in mean and jumps in volatility are independent. With the\nrise of the CC market in 2017, a level shift of the volatility of volatility\noccurred. All codes are available on Quantlet.com.\n"
    },
    {
        "paper_id": 2103.12503,
        "authors": "Daisuke Ida and Hirokuni Iiboshi",
        "title": "The international forward guidance transmission under a global liquidity\n  trap",
        "comments": "44 pages, 4 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper quantitatively explores the interaction effect of forward guidance\n(FG) on international monetary policy transmission using a standard two-country\nnew Keynesian model with a global liquidity trap. First, we show that the\nmagnitude of the constant risk aversion coefficient (CRRA) is important in\ndetermining the beggar-thy-neighbor and prosper-thy-neighbor effects in foreign\neconomies when the home country only faces the zero lower bound (ZLB)\nconstraints. Second,we demonstrate that both countries may benefit from\nadopting only the home country's FG policies if the home central bank only\nfaces the ZLB. Third, we find the potential benefit of the FG interaction\neffect between two countries. Thus, we document the possibility that home and\nforeign central banks can benefit from monetary policy coordination by adopting\nthe same duration of FG quarters.\n"
    },
    {
        "paper_id": 2103.12551,
        "authors": "Jay Cao, Jacky Chen, John Hull, Zissis Poulos",
        "title": "Deep Learning for Exotic Option Valuation",
        "comments": "19 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A common approach to valuing exotic options involves choosing a model and\nthen determining its parameters to fit the volatility surface as closely as\npossible. We refer to this as the model calibration approach (MCA). A\ndisadvantage of MCA is that some information in the volatility surface is lost\nduring the calibration process and the prices of exotic options will not in\ngeneral be consistent with those of plain vanilla options. We consider an\nalternative approach where the structure of the user's preferred model is\npreserved but points on the volatility are features input to a neural network.\nWe refer to this as the volatility feature approach (VFA) model. We conduct\nexperiments showing that VFA can be expected to outperform MCA for the\nvolatility surfaces encountered in practice. Once the upfront computational\ntime has been invested in developing the neural network, the valuation of\nexotic options using VFA is very fast.\n"
    },
    {
        "paper_id": 2103.12648,
        "authors": "Otto K\\\"assi, Vili Lehdonvirta, Fabian Stephany",
        "title": "How Many Online Workers are there in the World? A Data-Driven Assessment",
        "comments": "16 pages, four figures, two tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  An unknown number of people around the world are earning income by working\nthrough online labour platforms such as Upwork and Amazon Mechanical Turk. We\ncombine data collected from various sources to build a data-driven assessment\nof the number of such online workers (also known as online freelancers)\nglobally. Our headline estimate is that there are 163 million freelancer\nprofiles registered on online labour platforms globally. Approximately 19\nmillion of them have obtained work through the platform at least once, and 5\nmillion have completed at least 10 projects or earned at least $1000. These\nnumbers suggest a substantial growth from 2015 in registered worker accounts,\nbut much less growth in amount of work completed by workers. Our results\nindicate that online freelancing represents a non-trivial segment of labour\ntoday, but one that is spread thinly across countries and sectors.\n"
    },
    {
        "paper_id": 2103.12732,
        "authors": "Jiahua Xu, Krzysztof Paruch, Simon Cousaert, Yebo Feng",
        "title": "SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM)\n  Protocols",
        "comments": null,
        "journal-ref": "ACM Computing Surveys, 2023",
        "doi": "10.1145/3570639",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As an integral part of the decentralized finance (DeFi) ecosystem,\ndecentralized exchanges (DEXs) with automated market maker (AMM) protocols have\ngained massive traction with the recently revived interest in blockchain and\ndistributed ledger technology (DLT) in general. Instead of matching the buy and\nsell sides, automated market makers (AMMs) employ a peer-to-pool method and\ndetermine asset price algorithmically through a so-called conservation\nfunction. To facilitate the improvement and development of automated market\nmaker (AMM)-based decentralized exchanges (DEXs), we create the first\nsystematization of knowledge in this area. We first establish a general\nautomated market maker (AMM) framework describing the economics and formalizing\nthe system's state-space representation. We then employ our framework to\nsystematically compare the top automated market maker (AMM) protocols'\nmechanics, illustrating their conservation functions, as well as slippage and\ndivergence loss functions. We further discuss security and privacy concerns,\nhow they are enabled by automated market maker (AMM)-based decentralized\nexchanges (DEXs)' inherent properties, and explore mitigating solutions.\nFinally, we conduct a comprehensive literature review on related work covering\nboth decentralized finance (DeFi) and conventional market microstructure.\n"
    },
    {
        "paper_id": 2103.12779,
        "authors": "Sophocles Mavroeidis",
        "title": "Identification at the Zero Lower Bound",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3982/ECTA17388",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I show that the Zero Lower Bound (ZLB) on interest rates can be used to\nidentify the causal effects of monetary policy. Identification depends on the\nextent to which the ZLB limits the efficacy of monetary policy. I propose a\nsimple way to test the efficacy of unconventional policies, modelled via a\n`shadow rate'. I apply this method to U.S. monetary policy using a\nthree-equation SVAR model of inflation, unemployment and the federal funds\nrate. I reject the null hypothesis that unconventional monetary policy has no\neffect at the ZLB, but find some evidence that it is not as effective as\nconventional monetary policy.\n"
    },
    {
        "paper_id": 2103.12812,
        "authors": "George Rapciewicz Jr. and Dr. Donald Buresh",
        "title": "The Current Chinese Global Supply Chain Monopoly and the Covid-19\n  Pandemic",
        "comments": null,
        "journal-ref": "International Journal of Coronaviruses - 2(3):38-52 (2021)",
        "doi": "10.14302/issn.2692-1537.ijcv-21-3720",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Because of the ongoing Covid-19 crisis, supply chain management performance\nseems to be struggling. The purpose of this paper is to examine a variety of\ncritical factors related to the application of contingency theory to determine\nits feasibility in preventing future supply chain bottlenecks. The study\nreviewed current online news reports, previous research on contingency theory,\nas well as strategic and structural contingency theories. This paper also\nsystematically reviewed several global supply chain management and strategic\ndecision-making studies in an effort to promote a new strategy. The findings\nindicated that the need for mass production of products within the United\nStates, as well as within trading partners, is necessary to prevent additional\nCovid-19 related supply chain gaps. The paper noted that in many instances, the\nUnited States has become dependent on foreign products, where the prevention of\nfuture supply chain gaps requires the United States restore its manufacturing\nprowess.\n"
    },
    {
        "paper_id": 2103.12832,
        "authors": "Nicholas Dacre, Panos Constantinides, Joe Nandhakumar",
        "title": "How to Motivate and Engage Generation Clash of Clans at Work? Emergent\n  Properties of Business Gamification Elements in the Digital Economy",
        "comments": "International Gamification for Business Conference",
        "journal-ref": null,
        "doi": "10.2139/ssrn.3809398",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Organisations are currently lacking in developing and implementing business\nsystems in meaningful ways to motivate and engage their staff. This is\nparticularly salient as the average employee spends eleven cumulative years of\ntheir life at work, however less than one third of the workforce are actually\nengaged in their duties throughout their career. Such low levels of engagement\nare particularly prominent with younger employees, referred to as Generation Y\n(GenY), who are the least engaged of all groups at work. However, they will\ndedicate around five cumulative years of their life immersed playing video\ngames such as Clash of Clans, whether for social, competitive, extrinsic, or\nintrinsic motivational factors. Using behavioural concepts derived from video\ngames, and applying game design elements in business systems to motivate\nemployees in the digital economy, is a concept which has come to be recognised\nas Business Gamification. Thus, the purpose of this research paper is to\nfurther our understanding of game design elements for business, and investigate\ntheir properties from design to implementation in gamified systems. Following a\ntwo-year ethnographic style study with both a system development, and a\ncommunication agency largely staffed with GenY employees, findings suggest\nproperties in game design elements are emergent and temporal in their\ninstantiations.\n"
    },
    {
        "paper_id": 2103.13229,
        "authors": "Pamela Jakiela",
        "title": "Simple Diagnostics for Two-Way Fixed Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Difference-in-differences estimation is a widely used method of program\nevaluation. When treatment is implemented in different places at different\ntimes, researchers often use two-way fixed effects to control for\nlocation-specific and period-specific shocks. Such estimates can be severely\nbiased when treatment effects change over time within treated units. I review\nthe sources of this bias and propose several simple diagnostics for assessing\nits likely severity. I illustrate these tools through a case study of free\nprimary education in Sub-Saharan Africa.\n"
    },
    {
        "paper_id": 2103.13252,
        "authors": "Piergiacomo Sabino",
        "title": "Pricing Energy Derivatives in Markets Driven by Tempered Stable and CGMY\n  Processes of Ornstein-Uhlenbeck Type",
        "comments": "24 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study we consider the pricing of energy derivatives when the\nevolution of spot prices follows a tempered stable or a CGMY driven Ornstein-\nUhlenbeck process. To this end, we first calculate the characteristic function\nof the transition law of such processes in closed form. This result is\ninstrumental for the derivation of non-arbitrage conditions such that the spot\ndynamics is consistent with the forward curve. Moreover, based on the results\nof Cufaro Petroni and Sabino (2020), we also conceive efficient algorithms for\nthe exact simulation of the skeleton of such processes and propose a novel\nprocedure when they coincide with compound Poisson processes of\nOrnstein-Uhlenbeck type. We illustrate the applicability of the theoretical\nfindings and the simulation algorithms in the context of the pricing different\ncontracts namely, strips of daily call options, Asian options with European\nstyle and swing options. Finally, we present an extension to future markets.\n"
    },
    {
        "paper_id": 2103.13259,
        "authors": "Vinamra Chaturvedi",
        "title": "Mostly electric assisted airplanes (MEAP) for regional aviation: A South\n  Asian perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Aircraft manufacturing relies on pre-order bookings. The configuration of the\nto be assembled aircraft is fixed by the design assisted market surveys. The\nsensitivity of the supply chain to the market conditions, makes, the\nrelationship between the product (aircraft) and the associated service\n(aviation), precarious. Traditional model to mitigate this risk to\nprofitability rely on increasing the scales of operations. However, the\nemergence of new standards of air quality monitoring and insistence on the\nimplementation, demands additional corrective measures. In the quest for a\nsolution, this research commentary establishes a link, between the airport\ntaxes and the nature of the transporting unit. It warns, that merely,\nincreasing the number of mid haulage range aircrafts (MHA) in the fleet, may\nnot be enough, to overcome this challenge. In a two-pronged approach, the\ncommunication proposes, the use of mostly electric assisted air planes, and\nsmall sized airports as the key to solving this complex problem. As a side-note\nthe appropriateness of South Asian region, as a test-bed for MEAP based\naircrafts is also investigated. The success of this the idea can be potentially\nextended, to any other aviation friendly region of the world.\n"
    },
    {
        "paper_id": 2103.13273,
        "authors": "Nicholas Dacre, Vasilis Gkogkidis, Peter Jenkins",
        "title": "Co-Creation of Innovative Gamification Based Learning: A Case of\n  Synchronous Partnership",
        "comments": "Society for Research into Higher Education (SRHE)",
        "journal-ref": null,
        "doi": "10.2139/ssrn.3486496",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In higher education, gamification offers the prospect of providing a pivotal\nshift from traditional asynchronous forms of engagement, to developing methods\nto foster greater levels of synchronous interactivity and partnership between\nand amongst teaching and learning stakeholders. The small vein of research that\nfocuses on gamification in teaching and learning contexts, has mainly focused\non the implementation of pre-determined game elements. This approach reflects a\nlargely asynchronous approach to the development of learning practices in\neducational settings, thereby limiting stakeholder engagement in their design\nand adoption. Therefore, we draw on the theory of co-creation to examine the\ndevelopment process of gamification-based learning as a synchronous partnership\nbetween and amongst teaching and learning stakeholders. Empirical insights\nsuggest that students gain a greater sense of partnership and inclusivity as\npart of a synchronous co-creation gamification-based learning development and\nimplementation process.\n"
    },
    {
        "paper_id": 2103.13294,
        "authors": "Apostolos Chalkis, Emmanouil Christoforou, Theodore Dalamagkas,\n  Ioannis Z. Emiris",
        "title": "Modeling of crisis periods in stock markets",
        "comments": "11 pages, 10 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We exploit a recent computational framework to model and detect financial\ncrises in stock markets, as well as shock events in cryptocurrency markets,\nwhich are characterized by a sudden or severe drop in prices. Our method\nmanages to detect all past crises in the French industrial stock market\nstarting with the crash of 1929, including financial crises after 1990 (e.g.\ndot-com bubble burst of 2000, stock market downturn of 2002), and all past\ncrashes in the cryptocurrency market, namely in 2018, and also in 2020 due to\ncovid-19. We leverage copulae clustering, based on the distance between\nprobability distributions, in order to validate the reliability of the\nframework; we show that clusters contain copulae from similar market states\nsuch as normal states, or crises. Moreover, we propose a novel regression model\nthat can detect successfully all past events using less than 10% of the\ninformation that the previous framework requires. We train our model by\nhistorical data on the industry assets, and we are able to detect all past\nshock events in the cryptocurrency market. Our tools provide the essential\ncomponents of our software framework that offers fast and reliable detection,\nor even prediction, of shock events in stock and cryptocurrency markets of\nhundreds of assets.\n"
    },
    {
        "paper_id": 2103.13297,
        "authors": "Prashant Mahajan and Vaishali Patil",
        "title": "Making it normal for new enrollments: Effect of institutional and\n  pandemic influence on selecting an engineering institution under the COVID-19\n  pandemic situation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.21203/rs.3.rs-354086/v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The COVID19 pandemic has forced Indian engineering institutions (EIs) to\nbring their previous half shut shades completely down. Fetching new admissions\nto EI campuses during the pandemic has become a now or never situation for EIs.\nDuring crisis situations, institutions have struggled to return to the normal\ntrack. The pandemic has drastically changed students behavior and family\npreferences due to mental stress and the emotional life attached to it.\nConsequently, it becomes a prerequisite, and emergencies need to examine the\nchoice characteristics influencing the selection of EI during the COVID19\npandemic situation.\n  The purpose of this study is to critically examine institutional influence\nand pandemic influence due to COVID19 that affects students choice about an\nengineering institution (EI) and consequently to explore relationships between\ninstitutional and pandemic influence. The findings of this quantitative\nresearch, conducted through a self-reported survey, have revealed that\ninstitutional and pandemic influence have governed EI choice under the COVID19\npandemic. Second, pandemic influence is positively affected by institutional\ninfluence. The study demonstrated that EIs will have to reposition themselves\nto normalize pandemic influence by tuning institutional characteristics that\nregulate situational influence and new enrollments. It can be yardstick for\npolicy makers to attract new enrollments under pandemic situations.\n"
    },
    {
        "paper_id": 2103.13507,
        "authors": "Q. Wang, Y. Zhou, J. Shen",
        "title": "Intraday trading strategy based on time series and machine learning for\n  Chinese stock market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article comes up with an intraday trading strategy under T+1 using\nMarkowitz optimization and Multilayer Perceptron (MLP) with published stock\ndata obtained from the Shenzhen Stock Exchange and Shanghai Stock Exchange. The\nempirical results reveal the profitability of Markowitz portfolio optimization\nand validate the intraday stock price prediction using MLP. The findings\nfurther combine the Markowitz optimization, an MLP with the trading strategy,\nto clarify this strategy's feasibility.\n"
    },
    {
        "paper_id": 2103.13737,
        "authors": "Pramod Kumar Sur",
        "title": "Understanding the Paradox of Primary Health Care Use: Empirical Evidence\n  from India",
        "comments": "63 pages, arXiv admin note: substantial text overlap with\n  arXiv:2103.02909",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  I study households' primary health care usage in India, which presents a\nparadox. I examine why most households use fee-charging private health care\nservices even though (1) most providers have no formal medical qualifications\nand (2) in markets where qualified doctors offer free care through public\nhospitals. I present evidence that this puzzling practice has deep historical\nroutes. I examine India's coercive forced sterilization policy implemented\nbetween 1976 and 1977. Utilizing the unexpected timing of the policy, multiple\nmeasures of forced sterilization, including at a granular level, and an\ninstrumental variable approach, I document that places heavily affected by the\npolicy have lower public health care usage today. I also show that the\ninstrument I use is unrelated to a battery of demographic, economic, or\npolitical aspects before the forced sterilization period. Finally, I explore\nthe mechanism and document that supply-side factors do not explain these\ndifferences. Instead, I demonstrate that places with greater exposure to forced\nsterilization have higher confidence in private hospitals and doctors to\nprovide good treatment.\n"
    },
    {
        "paper_id": 2103.13773,
        "authors": "Philippe Bergault, Fay\\c{c}al Drissi, Olivier Gu\\'eant",
        "title": "Multi-asset optimal execution and statistical arbitrage strategies under\n  Ornstein-Uhlenbeck dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, academics, regulators, and market practitioners have\nincreasingly addressed liquidity issues. Amongst the numerous problems\naddressed, the optimal execution of large orders is probably the one that has\nattracted the most research works, mainly in the case of single-asset\nportfolios. In practice, however, optimal execution problems often involve\nlarge portfolios comprising numerous assets, and models should consequently\naccount for risks at the portfolio level. In this paper, we address multi-asset\noptimal execution in a model where prices have multivariate Ornstein-Uhlenbeck\ndynamics and where the agent maximizes the expected (exponential) utility of\nher PnL. We use the tools of stochastic optimal control and simplify the\ninitial multidimensional Hamilton-Jacobi-Bellman equation into a system of\nordinary differential equations (ODEs) involving a Matrix Riccati ODE for which\nclassical existence theorems do not apply. By using \\textit{a priori} estimates\nobtained thanks to optimal control tools, we nevertheless prove an existence\nand uniqueness result for the latter ODE, and then deduce a verification\ntheorem that provides a rigorous solution to the execution problem. Using\nexamples based on data from the foreign exchange and stock markets, we\neventually illustrate our results and discuss their implications for both\noptimal execution and statistical arbitrage.\n"
    },
    {
        "paper_id": 2103.13789,
        "authors": "Alberto Bisin and Andrea Moro",
        "title": "Spatial-SIR with Network Structure and Behavior: Lockdown Rules and the\n  Lucas Critique",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a model of the diffusion of an epidemic with demographically\nheterogeneous agents interacting socially on a spatially structured network.\nContagion-risk averse agents respond behaviorally to the diffusion of the\ninfections by limiting their social interactions. Schools and workplaces also\nrespond by allowing students and employees to attend and work remotely. The\nspatial structure induces local herd immunities along socio-demographic\ndimensions, which significantly affect the dynamics of infections. We study\nseveral non-pharmaceutical interventions; e.g., i) lockdown rules, which set\nthresholds on the spread of the infection for the closing and reopening of\neconomic activities; ii) neighborhood lockdowns, leveraging granular\n(neighborhood-level) information to improve the effectiveness public health\npolicies; iii) selective lockdowns, which restrict social interactions by\nlocation (in the network) and by the demographic characteristics of the agents.\nSubstantiating a \"Lucas critique\" argument, we assess the cost of naive\ndiscretionary policies ignoring agents and firms' behavioral responses.\n"
    },
    {
        "paper_id": 2103.13801,
        "authors": "Jens F. Peters, Mercedes Burguillo, Jose M. Arranz",
        "title": "Low emission zones: Effects on alternative-fuel vehicle uptake and fleet\n  CO2 emissions",
        "comments": "Postprint accepted for publication in Transportation Research part D.\n  7000 words on 32 pages, seven tables, four figures excl. Underlying raw data\n  available on Zenodo: https://doi.org/10.5281/zenodo.3948364",
        "journal-ref": null,
        "doi": "10.1016/j.trd.2021.102882",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study analyses the actual effect of a representative low-emission zone\n(LEZ) in terms of shifting vehicle registrations towards alternative fuel\ntechnologies and its effectiveness for reducing vehicle fleet CO2 emissions.\nVehicle registration data is combined with real life fuel consumption values on\nindividual vehicle model level, and the impact of the LEZ is then determined\nvia an econometric approach. The increase in alternative fuel vehicles (AFV)\nregistration shares due to the LEZ is found to be significant but fosters\nrather fossil fuel powered AFV and plug-in hybrid electric vehicles than zero\nemission vehicles. This is reflected in the average CO2 emissions of newly\nregistered vehicles, which do not decrease significantly. In consequence, while\nthe LEZ is an effective measure for stimulating the shift towards low emission\nvehicles, the support of non-electric AFV as low emission vehicles jeopardizes\nits effectiveness for decarbonizing the vehicle fleet.\n"
    },
    {
        "paper_id": 2103.13806,
        "authors": "Alireza Ghahtarani, Ahmed Saif, Alireza Ghasemi",
        "title": "Robust Portfolio Selection Problems: A Comprehensive Review",
        "comments": "48 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we provide a comprehensive review of recent advances in robust\nportfolio selection problems and their extensions, from both operational\nresearch and financial perspectives. A multi-dimensional classification of the\nmodels and methods proposed in the literature is presented, based on the types\nof financial problems, uncertainty sets, robust optimization approaches, and\nmathematical formulations. Several open questions and potential future research\ndirections are identified.\n"
    },
    {
        "paper_id": 2103.13965,
        "authors": "Thiago Marzagao",
        "title": "Putting a price on tenure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Government employees in Brazil are granted tenure after three years on the\njob. Firing a tenured government employee is all but impossible, so tenure is a\nbig employee benefit. But exactly how big is it? In other words: how much money\nis tenure worth to a government employee in Brazil? No one has ever attempted\nto answer that question. I do that in this paper. I use a modified version of\nthe Sharpe ratio to estimate what the risk-adjusted salaries of government\nworkers should be. The difference between actual salary and risk-adjusted\nsalary gives us an estimate of how much tenure is worth to each employee. I\nfind that in the 2005-2019 period the monthly value of tenure was 3980 reais to\nthe median federal government employee, 1971 reais to the median state\ngovernment employee, and 500 reais to the median municipal government employee.\n"
    },
    {
        "paper_id": 2103.14079,
        "authors": "Filippo Neri",
        "title": "Domain Specific Concept Drift Detectors for Predicting Financial Time\n  Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Concept drift detectors allow learning systems to maintain good accuracy on\nnon-stationary data streams. Financial time series are an instance of\nnon-stationary data streams whose concept drifts (market phases) are so\nimportant to affect investment decisions worldwide. This paper studies how\nconcept drift detectors behave when applied to financial time series. General\nresults are: a) concept drift detectors usually improve the runtime over\ncontinuous learning, b) their computational cost is usually a fraction of the\nlearning and prediction steps of even basic learners, c) it is important to\nstudy concept drift detectors in combination with the learning systems they\nwill operate with, and d) concept drift detectors can be directly applied to\nthe time series of raw financial data and not only to the model's accuracy one.\nMoreover, the study introduces three simple concept drift detectors, tailored\nto financial time series, and shows that two of them can be at least as\neffective as the most sophisticated ones from the state of the art when applied\nto financial time series. Currently submitted to Pattern Recognition\n"
    },
    {
        "paper_id": 2103.1408,
        "authors": "Firuz Kamalov, Linda Smail, Ikhlaas Gurrib",
        "title": "Forecasting with Deep Learning: S&P 500 index",
        "comments": "Published in: 2020 13th International Symposium on Computational\n  Intelligence and Design (ISCID)",
        "journal-ref": null,
        "doi": "10.1109/ISCID51228.2020.00102",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction has been the focus of a large amount of research but\nan acceptable solution has so far escaped academics. Recent advances in deep\nlearning have motivated researchers to apply neural networks to stock\nprediction. In this paper, we propose a convolution-based neural network model\nfor predicting the future value of the S&P 500 index. The proposed model is\ncapable of predicting the next-day direction of the index based on the previous\nvalues of the index. Experiments show that our model outperforms a number of\nbenchmarks achieving an accuracy rate of over 55%.\n"
    },
    {
        "paper_id": 2103.14081,
        "authors": "Firuz Kamalov, Linda Smail, Ikhlaas Gurrib",
        "title": "Stock price forecast with deep learning",
        "comments": "Published in: 2020 International Conference on Decision Aid Sciences\n  and Application (DASA)",
        "journal-ref": null,
        "doi": "10.1109/DASA51403.2020.9317260",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we compare various approaches to stock price prediction using\nneural networks. We analyze the performance fully connected, convolutional, and\nrecurrent architectures in predicting the next day value of S&P 500 index based\non its previous values. We further expand our analysis by including three\ndifferent optimization techniques: Stochastic Gradient Descent, Root Mean\nSquare Propagation, and Adaptive Moment Estimation. The numerical experiments\nreveal that a single layer recurrent neural network with RMSprop optimizer\nproduces optimal results with validation and test Mean Absolute Error of 0.0150\nand 0.0148 respectively.\n"
    },
    {
        "paper_id": 2103.1422,
        "authors": "Jason Poulos",
        "title": "Amnesty Policy and Elite Persistence in the Postbellum South: Evidence\n  from a Regression Discontinuity Design",
        "comments": null,
        "journal-ref": "Journal of Historical Political Economy, 1(3), 353-375 (2021)",
        "doi": "10.1561/115.00000013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the impact of Reconstruction-era amnesty policy on\nthe officeholding and wealth of elites in the postbellum South. Amnesty policy\nrestricted the political and economic rights of Southern elites for nearly\nthree years during Reconstruction. I estimate the effect of being excluded from\namnesty on elites' future wealth and political power using a regression\ndiscontinuity design that compares individuals just above and below a wealth\nthreshold that determined exclusion from amnesty. Results on a sample of\nReconstruction convention delegates show that exclusion from amnesty\nsignificantly decreased the likelihood of ex-post officeholding. I find no\nevidence that exclusion impacted later census wealth for Reconstruction\ndelegates or for a larger sample of known slaveholders who lived in the South\nin 1860. These findings are in line with previous studies evidencing both\nchanges to the identity of the political elite, and the continuity of economic\nmobility among the planter elite across the Civil War and Reconstruction.\n"
    },
    {
        "paper_id": 2103.14298,
        "authors": "Makoto Niwa, Yasushi Hara, Yusuke Matsuo, Hodaka Narita, Lim Yeongjoo,\n  Shintaro Sengoku, Kota Kodama",
        "title": "Superiority of mild interventions against COVID-19 on public health and\n  economic measures",
        "comments": "44 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the global spread of COVID-19, Japan has been among the top countries\nto maintain a relatively low number of infections, despite implementing limited\ninstitutional interventions. Using a Tokyo Metropolitan dataset, this study\ninvestigated how these limited intervention policies have affected public\nhealth and economic conditions in the COVID-19 context. A causal loop analysis\nsuggested that there were risks to prematurely terminating such interventions.\nOn the basis of this result and subsequent quantitative modelling, we found\nthat the short-term effectiveness of a short-term pre-emptive stay-at-home\nrequest caused a resurgence in the number of positive cases, whereas an\nadditional request provided a limited negative add-on effect for economic\nmeasures (e.g. the number of electronic word-of-mouth (eWOM) communications and\nrestaurant visits). These findings suggest the superiority of a mild and\ncontinuous intervention as a long-term countermeasure under epidemic pressures\nwhen compared to strong intermittent interventions.\n"
    },
    {
        "paper_id": 2103.14372,
        "authors": "Aymeric Vie",
        "title": "A Genetic Algorithm approach to Asymmetrical Blotto Games with\n  Heterogeneous Valuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blotto Games are a popular model of multi-dimensional strategic resource\nallocation. Two players allocate resources in different battlefields in an\nauction setting. While competition with equal budgets is well understood,\nlittle is known about strategic behavior under asymmetry of resources. We\nintroduce a genetic algorithm, a search heuristic inspired from biological\nevolution, interpreted as social learning, to solve this problem. Most\nperformant strategies are combined to create more performant strategies.\nMutations allow the algorithm to efficiently scan the space of possible\nstrategies, and consider a wide diversity of deviations. We show that our\ngenetic algorithm converges to the analytical Nash equilibrium of the symmetric\nBlotto game. We present the solution concept it provides for asymmetrical\nBlotto games. It notably sees the emergence of \"guerilla warfare\" strategies,\nconsistent with empirical and experimental findings. The player with less\nresources learns to concentrate its resources to compensate for the asymmetry\nof competition. When players value battlefields heterogeneously, counter\nstrategies and bidding focus is obtained in equilibrium. These features are\nconsistent with empirical and experimental findings, and provide a learning\nfoundation for their existence.\n"
    },
    {
        "paper_id": 2103.14379,
        "authors": "Aymeric Vie",
        "title": "Evolutionary Strategies with Analogy Partitions in p-guessing Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Keynesian Beauty Contests notably modeled by p-guessing games, players try\nto guess the average of guesses multiplied by p. Convergence of plays to Nash\nequilibrium has often been justified by agents' learning. However,\ninterrogations remain on the origin of reasoning types and equilibrium behavior\nwhen learning takes place in unstable environments. When successive values of p\ncan take values above and below 1, bounded rational agents may learn about\ntheir environment through simplified representations of the game, reasoning\nwith analogies and constructing expectations about the behavior of other\nplayers. We introduce an evolutionary process of learning to investigate the\ndynamics of learning and the resulting optimal strategies in unstable\np-guessing games environments with analogy partitions. As a validation of the\napproach, we first show that our genetic algorithm behaves consistently with\nprevious results in persistent environments, converging to the Nash\nequilibrium. We characterize strategic behavior in mixed regimes with unstable\nvalues of p. Varying the number of iterations given to the genetic algorithm to\nlearn about the game replicates the behavior of agents with different levels of\nreasoning of the level k approach. This evolutionary process hence proposes a\nlearning foundation for endogenizing existence and transitions between levels\nof reasoning in cognitive hierarchy models.\n"
    },
    {
        "paper_id": 2103.14506,
        "authors": "Wenpin Tang and Xiao Xu and Xun Yu Zhou",
        "title": "Asset Selection via Correlation Blockmodel Clustering",
        "comments": "46 pages, 9 figures and 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We aim to cluster financial assets in order to identify a small set of stocks\nto approximate the level of diversification of the whole universe of stocks. We\ndevelop a data-driven approach to clustering based on a correlation blockmodel\nin which assets in the same cluster are highly correlated with each other and,\nat the same time, have the same correlations with all other assets. We devise\nan algorithm to detect the clusters, with theoretical analysis and practical\nguidance. Finally, we conduct an empirical analysis to verify the performance\nof the algorithm.\n"
    },
    {
        "paper_id": 2103.14592,
        "authors": "Benjamin Sch\\\"afer, Marc Timme, Dirk Witthaut",
        "title": "Isolating the impact of trading on grid frequency fluctuations",
        "comments": null,
        "journal-ref": "Published in: 2018 IEEE PES Innovative Smart Grid Technologies\n  Conference Europe (ISGT-Europe)",
        "doi": "10.1109/ISGTEurope.2018.8571793",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To ensure reliable operation of power grids, their frequency shall stay\nwithin strict bounds. Multiple sources of disturbances cause fluctuations of\nthe grid frequency, ranging from changing demand over volatile feed-in to\nenergy trading. Here, we analyze frequency time series from the continental\nEuropean grid in 2011 and 2017 as a case study to isolate the impact of\ntrading. We find that trading at typical trading intervals such as full hours\nmodifies the frequency fluctuation statistics. While particularly large\nfrequency deviations in 2017 are not as frequent as in 2011, large deviations\nare more likely to occur shortly after the trading instances. A comparison\nbetween the two years indicates that trading at shorter intervals might be\nbeneficial for frequency quality and grid stability, because particularly large\nfluctuations are substantially diminished. Furthermore, we observe that the\nstatistics of the frequency fluctuations do not follow Gaussian distributions\nbut are better described using heavy-tailed and asymmetric distributions, for\nexample L\\'evy-stable distributions. Comparing intervals without trading to\nthose with trading instances indicates that frequency deviations near the\ntrading times are distributed more widely and thus extreme deviations are\norders of magnitude more likely. Finally, we briefly review a stochastic\nanalysis that allows a quantitative description of power grid frequency\nfluctuations.\n"
    },
    {
        "paper_id": 2103.14593,
        "authors": "V.A. Kalyagin, A.P. Koldanov, P.A. Koldanov",
        "title": "Reliability of MST identification in correlation-based market networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.127482",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Maximum spanning tree (MST) is a popular tool in market network analysis.\nLarge number of publications are devoted to the MST calculation and it's\ninterpretation for particular stock markets. However, much less attention is\npayed in the literature to the analysis of uncertainty of obtained results. In\nthe present paper we suggest a general framework to measure uncertainty of MST\nidentification. We study uncertainty in the framework of the concept of random\nvariable network (RVN). We consider different correlation based networks in the\nlarge class of elliptical distributions. We show that true MST is the same in\nthree networks: Pearson correlation network, Fechner correlation network, and\nKendall correlation network. We argue that among different measures of\nuncertainty the FDR (False Discovery Rate) is the most appropriated for MST\nidentification. We investigate FDR of Kruskal algorithm for MST identification\nand show that reliability of MST identification is different in these three\nnetworks. In particular, for Pearson correlation network the FDR essentially\ndepends on distribution of stock returns. We prove that for market network with\nFechner correlation the FDR is non sensitive to the assumption on stock's\nreturn distribution. Some interesting phenomena are discovered for Kendall\ncorrelation network. Our experiments show that FDR of Kruskal algorithm for MST\nidentification in Kendall correlation network weakly depend on distribution and\nat the same time the value of FDR is almost the best in comparison with MST\nidentification in other networks. These facts are important in practical\napplications.\n"
    },
    {
        "paper_id": 2103.14619,
        "authors": "Alexander J. Stewart, Joshua B. Plotkin and Nolan McCarty",
        "title": "Inequality, Identity, and Partisanship: How redistribution can stem the\n  tide of mass polarization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.2102140118",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The form of political polarization where citizens develop strongly negative\nattitudes towards out-party policies and members has become increasingly\nprominent across many democracies. Economic hardship and social inequality, as\nwell as inter-group and racial conflict, have been identified as important\ncontributing factors to this phenomenon known as \"affective polarization.\" Such\npartisan animosities are exacerbated when these interests and identities become\naligned with existing party cleavages. In this paper we use a model of cultural\nevolution to study how these forces combine to generate and maintain affective\npolitical polarization. We show that economic events can drive both affective\npolarization and sorting of group identities along party lines, which in turn\ncan magnify the effects of underlying inequality between those groups. But on a\nmore optimistic note, we show that sufficiently high levels of wealth\nredistribution through the provision of public goods can counteract this\nfeedback and limit the rise of polarization. We test some of our key\ntheoretical predictions using survey data on inter-group polarization, sorting\nof racial groups and affective polarization in the United States over the past\n50 years.\n"
    },
    {
        "paper_id": 2103.14762,
        "authors": "Juan de O\\~na, Esperanza Est\\'evez and Rocio de O\\~na",
        "title": "Public transport users versus private vehicle users: differences about\n  quality of service, satisfaction and attitudes toward public transport in\n  Madrid (Spain)",
        "comments": "23 pages, 1 figure, 6 tables",
        "journal-ref": "Travel Behaviour and Society (2021), 23, 76-85",
        "doi": "10.1016/j.tbs.2020.11.003",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper aims to further understand the main factors influencing the\nbehavioural intentions (BI) of private vehicle users towards public transport\nto provide policymakers and public transport operators with the tools they need\nto attract more private vehicle users. As service quality, satisfaction and\nattitudes towards public transport are considered the main motivational forces\nbehind the BI of public transport users, this research analyses 26 indicators\nfrequently associated with these constructs for both public transport users and\nprivate vehicle users. Non-parametric tests and ordinal logit models have been\napplied to an online survey asked in Madrid's metropolitan area with a sample\nsize of 1,025 respondents (525 regular public transport users and 500 regular\nprivate vehicle users). In order to achieve a comprehensive analysis and to\ndeal with heterogeneity in perceptions, 338 models have been developed for the\nentire sample and for 12 users' segments. The results led to the identification\nof indicators with no significant differences between public transport and\nprivate vehicle users in any of the segments being considered (punctuality,\ninformation and low-income), as well as those that did show significant\ndifferences in all the segments (proximity, intermodality, save time and money,\nand lifestyle). The main differences between public transport and private\nvehicle users were found in the attitudes towards public transport and for\ncertain user segments (residents in the city centre, males, young, with\nuniversity qualification and with incomes above 2,700EUR/month). Findings from\nthis study can be used to develop policies and recommendations for persuading\nmore private vehicle users to use the public transport services.\n"
    },
    {
        "paper_id": 2103.14769,
        "authors": "Guillermo Angeris, Alex Evans, Tarun Chitra",
        "title": "Replicating Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method for constructing Constant Function Market Makers (CFMMs)\nwhose portfolio value functions match a desired payoff. More specifically, we\nshow that the space of concave, nonnegative, nondecreasing, 1-homogeneous\npayoff functions and the space of convex CFMMs are equivalent; in other words,\nevery CFMM has a concave, nonnegative, nondecreasing, 1-homogeneous payoff\nfunction, and every payoff function with these properties has a corresponding\nconvex CFMM. We demonstrate a simple method for recovering a CFMM trading\nfunction that produces this desired payoff. This method uses only basic tools\nfrom convex analysis and is intimately related to Fenchel conjugacy. We\ndemonstrate our result by constructing trading functions corresponding to basic\npayoffs, as well as standard financial derivatives such as options and swaps.\n"
    },
    {
        "paper_id": 2103.14857,
        "authors": "Giovanni Abramo, Francesca Apponi, Ciriaco Andrea D'Angelo",
        "title": "Public-private research collaborations: longitudinal field-level\n  analysis of determinants, frequency and impact",
        "comments": "arXiv admin note: text overlap with arXiv:2102.05364",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study on public-private research collaboration measures the variation\nover time of the propensity of academics to collaborate with colleagues from\nprivate companies. It also investigates the change in weights of the main\ndrivers underlying the academics' propensity to collaborate, and whether the\ntype profile of the collaborating academics changes. To do this, the study\napplies an inferential model on a dataset of professors working in Italian\nuniversities in consecutive periods, 2010-2013 and 2014-2017. The results,\nobtained at overall and field levels, support the formulation of policies aimed\nat fostering public-private research collaborations, and should be taken into\naccount in post-assessment of their effectiveness.\n"
    },
    {
        "paper_id": 2103.14859,
        "authors": "Giovanni Abramo, Ciriaco Andrea D'Angelo",
        "title": "Drivers of academic engagement in public-private research collaboration:\n  an empirical study",
        "comments": "arXiv admin note: text overlap with arXiv:2102.05364",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  University-industry research collaboration is one of the major research\npolicy priorities of advanced economies. In this study, we try to identify the\nmain drivers that could influence the propensity of academics to engage in\nresearch collaborations with the private sector, in order to better inform\npolicies and initiatives to foster such collaborations. At this purpose, we\napply an inferential model to a dataset of 32,792 Italian professors in order\nto analyze the relative impact of individual and contextual factors affecting\nthe propensity of academics to engage in collaboration with industry, at\noverall level and across disciplines. The outcomes reveal that the typical\nprofile of the professor collaborating with industry is a male under age 40,\nfull professor, very high performer, with highly diversified research, and who\nhas a certain tradition in collaborating with industry. This professor is\nlikely to be part of a staff used to collaborating with industry, in a small\nuniversity, typically a polytechnic, located in the north of the country.\n"
    },
    {
        "paper_id": 2103.15096,
        "authors": "Jaydip Sen and Sidra Mehtab",
        "title": "Accurate Stock Price Forecasting Using Robust and Optimized Deep\n  Learning Models",
        "comments": "This paper is an accepted version of our paper in the IEEE\n  International Conference on Intelligent Technologies (IEEE CONIT), which will\n  be organized in Hubli, Karnataka, INDIA, from June 25 to June 27, 2021. The\n  paper is 8 pages long and it contains eleven tables and seventeen figures",
        "journal-ref": null,
        "doi": "10.1109/CONIT51480.2021.9498565",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing robust frameworks for precise prediction of future prices of stocks\nhas always been considered a very challenging research problem. The advocates\nof the classical efficient market hypothesis affirm that it is impossible to\naccurately predict the future prices in an efficiently operating market due to\nthe stochastic nature of the stock price variables. However, numerous\npropositions exist in the literature with varying degrees of sophistication and\ncomplexity that illustrate how algorithms and models can be designed for making\nefficient, accurate, and robust predictions of stock prices. We present a gamut\nof ten deep learning models of regression for precise and robust prediction of\nthe future prices of the stock of a critical company in the auto sector of\nIndia. Using a very granular stock price collected at 5 minutes intervals, we\ntrain the models based on the records from 31st Dec, 2012 to 27th Dec, 2013.\nThe testing of the models is done using records from 30th Dec, 2013 to 9th Jan\n2015. We explain the design principles of the models and analyze the results of\ntheir performance based on accuracy in forecasting and speed of execution.\n"
    },
    {
        "paper_id": 2103.15183,
        "authors": "Jorge Davalos and Ekkehard Ernst",
        "title": "How has labour market power evolved? Comparing labour market monopsony\n  in Peru and the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We document the evolution of labour market power by employers on the US and\nPeruvian labour markets during the 2010s. Making use of a structural estimation\nmodel of labour market dynamics, we estimate differences in market power that\nworkers face depending on their sector of activity, their age, sex, location\nand educational level. In particular, we show that differences in\ncross-sectional market power are significant and higher than variations over\nthe ten-year time span of our data. In contrast to findings of labour market\npower in developed countries such as the US, we document significant market\npower of employers in Peru vis-\\`a-vis the tertiary educated workforce,\nregardless of age and sector. In contrast, for the primary educated workforce,\nmarket power seems to be high in (private) services and manufacturing. For\nsecondary educated workers, only the mining sector stands out as moderately\nmore monopsonistic than the rest of the labour market. We also show that at\nleast for the 2010s, labour market power declined in Peru. We contrast these\nfindings with similar estimates obtained for the United States where we are\nable to show that increases in labour market power are particularly acute in\ncertain sectors, such as agriculture and entertainment and recreational\nservices, as well as in specific geographic areas where these sectors are\ndominant. Moreover, we show that for the US, the labour market power has\ngradually increased over the past ten years, in line with the general rise in\ninequality. Importantly, we show that the pervasive gender pay gap cannot be\nlinked to differential market power as men face higher labour market power than\nwomen. We also discuss possible reasons for these findings, including for the\ndifferences of labour market power across skill levels. Especially, we discuss\nthe reasons for polarization of market power that are specific to sectors and\nlocations.\n"
    },
    {
        "paper_id": 2103.15232,
        "authors": "Pier Francesco Procacci and Tomaso Aste",
        "title": "Portfolio Optimization with Sparse Multivariate Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio optimization approaches inevitably rely on multivariate modeling of\nmarkets and the economy. In this paper, we address three sources of error\nrelated to the modeling of these complex systems: 1. oversimplifying\nhypothesis; 2. uncertainties resulting from parameters' sampling error; 3.\nintrinsic non-stationarity of these systems. For what concerns point 1. we\npropose a L0-norm sparse elliptical modeling and show that sparsification is\neffective. The effects of points 2. and 3. are quantifified by studying the\nmodels' likelihood in- and out-of-sample for parameters estimated over train\nsets of different lengths. We show that models with larger off-sample\nlikelihoods lead to better performing portfolios up to when two to three years\nof daily observations are included in the train set. For larger train sets, we\nfound that portfolio performances deteriorate and detach from the models'\nlikelihood, highlighting the role of non-stationarity. We further investigate\nthis phenomenon by studying the out-of-sample likelihood of individual\nobservations showing that the system changes significantly through time. Larger\nestimation windows lead to stable likelihood in the long run, but at the cost\nof lower likelihood in the short-term: the `optimal' fit in finance needs to be\ndefined in terms of the holding period. Lastly, we show that sparse models\noutperform full-models in that they deliver higher out of sample likelihood,\nlower realized portfolio volatility and improved portfolios' stability,\navoiding typical pitfalls of the Mean-Variance optimization.\n"
    },
    {
        "paper_id": 2103.15302,
        "authors": "Kyungsub Lee and Byoung Ki Seo",
        "title": "Analytic formula for option margin with liquidity costs under dynamic\n  delta hedging",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/00036846.2021.1881430",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study derives the expected liquidity cost when performing the delta\nhedging process of a European option. This cost is represented by an\nintegration formula that includes European option prices and a certain function\ndepending on the delta process. We first define a unit liquidity cost and then\nshow that the liquidity cost is a multiplication of the unit liquidity cost,\nstock price, supply curve parameter, and the square of the number of options.\nUsing this formula, the expected liquidity cost before hedging can be\ncalculated much faster than when using a Monte Carlo simulation. Numerically\ncomputed distributions of liquidity costs in special cases are also provided.\n"
    },
    {
        "paper_id": 2103.15304,
        "authors": "Haohan Zhang",
        "title": "A Comparative Evaluation of Predominant Deep Learning Quantified Stock\n  Trading Strategies",
        "comments": "14 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study first reconstructs three deep learning powered stock trading\nmodels and their associated strategies that are representative of distinct\napproaches to the problem and established upon different aspects of the many\ntheories evolved around deep learning. It then seeks to compare the performance\nof these strategies from different perspectives through trading simulations ran\non three scenarios when the benchmarks are kept at historical low points for\nextended periods of time. The results show that in extremely adverse market\nclimates, investment portfolios managed by deep learning powered algorithms are\nable to avert accumulated losses by generating return sequences that shift the\nconstantly negative CSI 300 benchmark return upward. Among the three, the LSTM\nmodel's strategy yields the best performance when the benchmark sustains\ncontinued loss.\n"
    },
    {
        "paper_id": 2103.1531,
        "authors": "Jorge Ignacio Gonz\\'alez C\\'azares and Aleksandar Mijatovi\\'c",
        "title": "Monte Carlo algorithm for the extrema of tempered stable processes",
        "comments": "31 pages, 10 figures, video available on https://youtu.be/FJG6A3zk2lI",
        "journal-ref": "Advances in Applied Probability, p. 1-28 (2023)",
        "doi": "10.1017/apr.2023.1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel Monte Carlo algorithm for the vector consisting of the\nsupremum, the time at which the supremum is attained and the position at a\ngiven (constant) time of an exponentially tempered L\\'evy process. The\nalgorithm, based on the increments of the process without tempering, converges\ngeometrically fast (as a function of the computational cost) for discontinuous\nand locally Lipschitz functions of the vector. We prove that the corresponding\nmultilevel Monte Carlo estimator has optimal computational complexity (i.e. of\norder $\\varepsilon^{-2}$ if the mean squared error is at most $\\varepsilon^2$)\nand provide its central limit theorem (CLT). Using the CLT we construct\nconfidence intervals for barrier option prices and various risk measures based\non drawdown under the tempered stable (CGMY) model calibrated/estimated on\nreal-world data. We provide non-asymptotic and asymptotic comparisons of our\nalgorithm with existing approximations, leading to rule-of-thumb guidelines for\nusers to the best method for a given set of parameters. We illustrate the\nperformance of the algorithm with numerical examples.\n"
    },
    {
        "paper_id": 2103.154,
        "authors": "Qixuan Luo, Yu Shi, Handong Li",
        "title": "Research on Portfolio Liquidation Strategy under Discrete Times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an optimal strategy for portfolio liquidation under\ndiscrete time conditions. We assume that N risky assets held will be liquidated\naccording to the same time interval and order quantity, and the basic price\nprocesses of assets are generated by an N-dimensional independent standard\nBrownian motion. The permanent impact generated by an asset in the portfolio\nduring the liquidation will affect all assets, and the temporary impact\ngenerated by one asset will only affect itself. On this basis, we establish a\nliquidation cost model based on the VaR measurement and obtain an optimal\nliquidation time under discrete-time conditions. The optimal solution shows\nthat the liquidation time is only related to the temporary impact rather than\nthe permanent impact. In the simulation analysis, we give the relationship\nbetween volatility parameters, temporary price impact and the optimal\nliquidation strategy.\n"
    },
    {
        "paper_id": 2103.15777,
        "authors": "Carolina Mattsson, Frank W. Takes, Eelke M. Heemskerk, Cees Diks, Gert\n  Buiten, Albert Faber, Peter M.A. Sloot",
        "title": "Functional structure in production networks",
        "comments": "20 pages, 5 figures",
        "journal-ref": "Frontiers in Big Data 4 (2021)",
        "doi": "10.3389/fdata.2021.666712",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Production networks are integral to economic dynamics, yet dis-aggregated\nnetwork data on inter-firm trade is rarely collected and often proprietary.\nHere we situate company-level production networks among networks from other\ndomains according to their local connectivity structure. Through this lens, we\nstudy a regional and a national network of inferred trade relationships\nreconstructed from Dutch national economic statistics and re-interpret prior\nempirical findings. We find that company-level production networks have\nso-called functional structure, as previously identified in protein-protein\ninteraction (PPI) networks. Functional networks are distinctive in their\nover-representation of closed squares, which we quantify using an existing\nmeasure called spectral bipartivity. Shared local connectivity structure lets\nus ferry insights between domains. PPI networks are shaped by complementarity,\nrather than homophily, and we use multi-layer directed configuration models to\nshow that this principle explains the emergence of functional structure in\nproduction networks. Companies are especially similar to their close\ncompetitors, not to their trading partners. Our findings have practical\nimplications for the analysis of production networks and a thorough\nunderstanding of their local connectivity structure will help us better reason\nabout the micro-economic mechanisms behind their routine function, failure, and\ngrowth.\n"
    },
    {
        "paper_id": 2103.1579,
        "authors": "Erio Castagnoli, Giacomo Cattelan, Fabio Maccheroni, Claudio Tebaldi,\n  Ruodu Wang",
        "title": "Star-shaped Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper monetary risk measures that are positively superhomogeneous,\ncalled star-shaped risk measures, are characterized and their properties\nstudied. The measures in this class, which arise when the controversial\nsubadditivity property of coherent risk measures is dispensed with and positive\nhomogeneity is weakened, include all practically used risk measures, in\nparticular, both convex risk measures and Value-at-Risk. From a financial\nviewpoint, our relaxation of convexity is necessary to quantify the capital\nrequirements for risk exposure in the presence of liquidity risk, competitive\ndelegation, or robust aggregation mechanisms. From a decision theoretical\nperspective, star-shaped risk measures emerge from variational preferences when\nrisk mitigation strategies can be adopted by a rational decision maker.\n"
    },
    {
        "paper_id": 2103.16049,
        "authors": "Segun Michael Ojo, Edward Oladipo Ogunleye",
        "title": "Technological Leapfrogging and Manufacturing Value-added in sub-Saharan\n  African (1990-2018)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the impact of technological leapfrogging on manufacturing\nvalue-added in SSA. The study utilizes secondary data spanning 1990 to 2018.\nThe data is analyzed using cross-sectional autoregressive distributed lags\n(CS-ARDL) and cross-sectional distributed lags (CS-DL) techniques. The study\nfound that technological leapfrogging is a positive driver of manufacturing\nvalue-added in SSA. This implies that SSA can copy the foreign technologies and\nadapt them for domestic uses, rather than going through the evolutionary\nprocess of the old technologies that are relatively less efficient. If the\ngovernments of SSA could reinforce their absorptive capacity and beef up\nproductivity through proper utilization of the existing technology. The\nproductive activities of the domestic firms will stir new innovations and\ndiscoveries that will eventually translate into indigenous technology\n"
    },
    {
        "paper_id": 2103.16264,
        "authors": "Guusje Delsing, Michel Mandjes, Peter Spreij, Erik Winands",
        "title": "On Capital Allocation for a Risk Measure Derived from Ruin Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses allocation methodologies for a risk measure inherited\nfrom ruin theory. Specifically, we consider a dynamic value-at-risk (VaR)\nmeasure defined as the smallest initial capital needed to ensure that the\nultimate ruin probability is less than a given threshold. We introduce an\nintuitively appealing, novel allocation method, with a focus on its application\nto capital reserves which are determined through the dynamic value-at-risk\n(VaR) measure. Various desirable properties of the presented approach are\nderived including a limit result when considering a large time horizon and the\ncomparison with the frequently used gradient allocation method. In passing, we\nintroduce a second allocation method and discuss its relation to the other\nallocation approaches. A number of examples illustrate the applicability and\nperformance of the allocation approaches.\n"
    },
    {
        "paper_id": 2103.16388,
        "authors": "Mukul Jaggi, Priyanka Mandal, Shreya Narang, Usman Naseem and Matloob\n  Khushi",
        "title": "Text Mining of Stocktwits Data for Predicting Stock Prices",
        "comments": null,
        "journal-ref": "Appl. Syst. Innov. 2021, 4, 13",
        "doi": "10.3390/asi4010013",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock price prediction can be made more efficient by considering the price\nfluctuations and understanding the sentiments of people. A limited number of\nmodels understand financial jargon or have labelled datasets concerning stock\nprice change. To overcome this challenge, we introduced FinALBERT, an ALBERT\nbased model trained to handle financial domain text classification tasks by\nlabelling Stocktwits text data based on stock price change. We collected\nStocktwits data for over ten years for 25 different companies, including the\nmajor five FAANG (Facebook, Amazon, Apple, Netflix, Google). These datasets\nwere labelled with three labelling techniques based on stock price changes. Our\nproposed model FinALBERT is fine-tuned with these labels to achieve optimal\nresults. We experimented with the labelled dataset by training it on\ntraditional machine learning, BERT, and FinBERT models, which helped us\nunderstand how these labels behaved with different model architectures. Our\nlabelling method competitive advantage is that it can help analyse the\nhistorical data effectively, and the mathematical function can be easily\ncustomised to predict stock movement.\n"
    },
    {
        "paper_id": 2103.16409,
        "authors": "Jay Cao, Jacky Chen, John Hull, Zissis Poulos",
        "title": "Deep Hedging of Derivatives Using Reinforcement Learning",
        "comments": "21 pages, 3 figures, 4 tables",
        "journal-ref": "The Journal of Financial Data Science Winter 2021, 3 (1) 10-27",
        "doi": "10.3905/jfds.2020.1.052",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper shows how reinforcement learning can be used to derive optimal\nhedging strategies for derivatives when there are transaction costs. The paper\nillustrates the approach by showing the difference between using delta hedging\nand optimal hedging for a short position in a call option when the objective is\nto minimize a function equal to the mean hedging cost plus a constant times the\nstandard deviation of the hedging cost. Two situations are considered. In the\nfirst, the asset price follows a geometric Brownian motion. In the second, the\nasset price follows a stochastic volatility process. The paper extends the\nbasic reinforcement learning approach in a number of ways. First, it uses two\ndifferent Q-functions so that both the expected value of the cost and the\nexpected value of the square of the cost are tracked for different state/action\ncombinations. This approach increases the range of objective functions that can\nbe used. Second, it uses a learning algorithm that allows for continuous state\nand action space. Third, it compares the accounting P&L approach (where the\nhedged position is valued at each step) and the cash flow approach (where cash\ninflows and outflows are used). We find that a hybrid approach involving the\nuse of an accounting P&L approach that incorporates a relatively simple\nvaluation model works well. The valuation model does not have to correspond to\nthe process assumed for the underlying asset price.\n"
    },
    {
        "paper_id": 2103.16451,
        "authors": "Viet Anh Nguyen, Fan Zhang, Shanshan Wang, Jose Blanchet, Erick\n  Delage, Yinyu Ye",
        "title": "Robustifying Conditional Portfolio Decisions via Optimal Transport",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a data-driven portfolio selection model that integrates side\ninformation, conditional estimation and robustness using the framework of\ndistributionally robust optimization. Conditioning on the observed side\ninformation, the portfolio manager solves an allocation problem that minimizes\nthe worst-case conditional risk-return trade-off, subject to all possible\nperturbations of the covariate-return probability distribution in an optimal\ntransport ambiguity set. Despite the non-linearity of the objective function in\nthe probability measure, we show that the distributionally robust portfolio\nallocation with side information problem can be reformulated as a\nfinite-dimensional optimization problem. If portfolio decisions are made based\non either the mean-variance or the mean-Conditional Value-at-Risk criterion,\nthe resulting reformulation can be further simplified to second-order or\nsemi-definite cone programs. Empirical studies in the US equity market\ndemonstrate the advantage of our integrative framework against other\nbenchmarks.\n"
    },
    {
        "paper_id": 2103.168,
        "authors": "Lin He, Zongxia Liang, Yilun Song, Qi Ye",
        "title": "Optimal Retirement Time and Consumption with the Variation in Habitual\n  Persistence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper,we study the individual's optimal retirement time and optimal\nconsumption under habitual persistence. Because the individual feels equally\nsatisfied with a lower habitual level and is more reluctant to change the\nhabitual level after retirement, we assume that both the level and the\nsensitivity of the habitual consumption decline at the time of retirement. We\nestablish the concise form of the habitual evolutions, and obtain the optimal\nretirement time and consumption policy based on martingale and duality methods.\nThe optimal consumption experiences a sharp decline at retirement, but the\nexcess consumption raises because of the reduced sensitivity of the habitual\nlevel. This result contributes to explain the \"retirement consumption puzzle\".\nParticularly, the optimal retirement and consumption policies are balanced\nbetween the wealth effect and the habitual effect. Larger wealth increases\nconsumption, and larger growth inertia (sensitivity) of the habitual level\ndecreases consumption and brings forward the retirement time.\n"
    },
    {
        "paper_id": 2103.16918,
        "authors": "Thomas Deschatre and Olivier F\\'eron and Pierre Gruet",
        "title": "A survey of electricity spot and futures price models for risk\n  management applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This review presents the set of electricity price models proposed in the\nliterature since the opening of power markets. We focus on price models applied\nto financial pricing and risk management. We classify these models according to\ntheir ability to represent the random behavior of prices and some of their\ncharacteristics. In particular, this classification helps users to choose among\nthe most suitable models for their risk management problems.\n"
    },
    {
        "paper_id": 2103.16977,
        "authors": "Edward Hill, Marco Bardoscia and Arthur Turrell",
        "title": "Solving Heterogeneous General Equilibrium Economic Models with Deep\n  Reinforcement Learning",
        "comments": "11 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  General equilibrium macroeconomic models are a core tool used by policymakers\nto understand a nation's economy. They represent the economy as a collection of\nforward-looking actors whose behaviours combine, possibly with stochastic\neffects, to determine global variables (such as prices) in a dynamic\nequilibrium. However, standard semi-analytical techniques for solving these\nmodels make it difficult to include the important effects of heterogeneous\neconomic actors. The COVID-19 pandemic has further highlighted the importance\nof heterogeneity, for example in age and sector of employment, in macroeconomic\noutcomes and the need for models that can more easily incorporate it. We use\ntechniques from reinforcement learning to solve such models incorporating\nheterogeneous agents in a way that is simple, extensible, and computationally\nefficient. We demonstrate the method's accuracy and stability on a toy problem\nfor which there is a known analytical solution, its versatility by solving a\ngeneral equilibrium problem that includes global stochasticity, and its\nflexibility by solving a combined macroeconomic and epidemiological model to\nexplore the economic and health implications of a pandemic. The latter\nsuccessfully captures plausible economic behaviours induced by differential\nhealth risks by age.\n"
    },
    {
        "paper_id": 2103.17255,
        "authors": "Jos\\'e Miguel Flores-Contr\\'o, Kira Henshaw, Sooie-Hoe Loke,\n  S\\'everine Arnold and Corina Constantinescu",
        "title": "Subsidising Inclusive Insurance to Reduce Poverty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we assess the benefits of coordination and partnerships\nbetween governments and private insurers, and provide further evidence for\nmicroinsurance products as powerful and cost-effective tools for achieving\npoverty reduction. To explore these ideas, we model the capital of a household\nfrom a ruin-theoretic perspective to measure the impact of microinsurance on\npoverty dynamics and the governmental cost of social protection. We analyse the\nmodel under four frameworks: uninsured, insured (without subsidies), insured\nwith subsidised constant premiums and insured with subsidised flexible\npremiums. Although insurance alone (without subsidies) may not be sufficient to\nreduce the likelihood of falling into the area of poverty for specific groups\nof households, since premium payments constrain their capital growth, our\nanalysis suggests that subsidised schemes can provide maximum social benefits\nwhile reducing governmental costs.\n"
    },
    {
        "paper_id": 2104.00029,
        "authors": "Matthias Kaiser, Tatjana Buklijas, Peter Gluckman",
        "title": "Models and numbers: Representing the world or imposing order?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue for a foundational epistemic claim and a hypothesis about the\nproduction and uses of mathematical epidemiological models, exploring the\nconsequences for our political and socio-economic lives. First, in order to\nmake the best use of scientific models, we need to understand why models are\nnot truly representational of our world, but are already pitched towards\nvarious uses. Second, we need to understand the implicit power relations in\nnumbers and models in public policy, and, thus, the implications for good\ngovernance if numbers and models are used as the exclusive drivers of decision\nmaking.\n"
    },
    {
        "paper_id": 2104.00129,
        "authors": "Matthias Bahr and Leif Laszig",
        "title": "Productivity development in the construction industry and human capital:\n  a literature review",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5121/civej.2021.8101",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Tis paper is a literature review focusing on human capital, skills of\nemployees, demographic change, management, training and their impact on\nproductivity growth. Intrafirm behaviour has been recognized as a potentially\nimportant driver for productivity. Results from surveys show that management\npractices have become more structured, in the sense of involving more data\ncollection and analysis. Furthermore, a strong positive correlation between the\nmeasured management quality and firm performance can be observed. Studies\nsuggest that there is a positive association between management score and\nproductivity growth. The lack or low level of employees' skills and\nqualifications might be in different ways a possible explanation for the\nobserved slowdown of productivity growth. The main reason for the decline in\nskilled labor is the demographic change. Construction sectors are increasingly\naffected by demographic developments. Labour reserves in construction are\nlargely exhausted. Shortage of qualified workforce is impacting project cost,\nschedules and quality.\n"
    },
    {
        "paper_id": 2104.00248,
        "authors": "Hamed Amini, Zhongyuan Cao and Agnes Sulem",
        "title": "Limit Theorems for Default Contagion and Systemic Risk",
        "comments": "54 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general tractable model for default contagion and systemic risk\nin a heterogeneous financial network, subject to an exogenous macroeconomic\nshock. We show that, under some regularity assumptions, the default cascade\nmodel could be transferred to a death process problem represented by\nballs-and-bins model. We also reduce the dimension of the problem by\nclassifying banks according to different types, in an appropriate type space.\nThese types may be calibrated to real-world data by using machine learning\ntechniques. We then state various limit theorems regarding the final size of\ndefault cascade over different types. In particular, under suitable assumptions\non the degree and threshold distributions, we show that the final size of\ndefault cascade has asymptotically Gaussian fluctuations. We next state limit\ntheorems for different system-wide wealth aggregation functions and show how\nthe systemic risk measure, in a given stress test scenario, could be related to\nthe structure and heterogeneity of financial networks. We finally show how\nthese results could be used by a social planner to optimally target\ninterventions during a financial crisis, with a budget constraint and under\npartial information of the financial network.\n"
    },
    {
        "paper_id": 2104.00262,
        "authors": "Maike Torm\\\"ahlen, Galiya Klinkova and Michael Grabinski",
        "title": "Statistical significance revisited",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.20944/preprints202103.0398.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Statistical significance measures the reliability of a result obtained from a\nrandom experiment. We investigate the number of repetitions needed for a\nstatistical result to have a certain significance. In the first step, we\nconsider binomially distributed variables in the example of medication testing\nwith fixed placebo efficacy, asking how many experiments are needed in order to\nachieve a significance of 95 %. In the next step, we take the probability\ndistribution of the placebo efficacy into account, which to the best of our\nknowledge has not been done so far. Depending on the specifics, we show that in\norder to obtain identical significance, it may be necessary to perform twice as\nmany experiments than in a setting where the placebo distribution is neglected.\nWe proceed by considering more general probability distributions and close with\ncomments on some erroneous assumptions on probability distributions which lead,\nfor instance, to a trivial explanation of the fat tail.\n"
    },
    {
        "paper_id": 2104.00446,
        "authors": "Alex Evans, Guillermo Angeris and Tarun Chitra",
        "title": "Optimal Fees for Geometric Mean Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Constant Function Market Makers (CFMMs) are a family of automated market\nmakers that enable censorship-resistant decentralized exchange on public\nblockchains. Arbitrage trades have been shown to align the prices reported by\nCFMMs with those of external markets. These trades impose costs on Liquidity\nProviders (LPs) who supply reserves to CFMMs. Trading fees have been proposed\nas a mechanism for compensating LPs for arbitrage losses. However, large fees\nreduce the accuracy of the prices reported by CFMMs and can cause reserves to\ndeviate from desirable asset compositions. CFMM designers are therefore faced\nwith the problem of how to optimally select fees to attract liquidity. We\ndevelop a framework for determining the value to LPs of supplying liquidity to\na CFMM with fees when the underlying process follows a general diffusion.\nFocusing on a popular class of CFMMs which we call Geometric Mean Market Makers\n(G3Ms), our approach also allows one to select optimal fees for maximizing LP\nvalue. We illustrate our methodology by showing that an LP with mean-variance\nutility will prefer a G3M over all alternative trading strategies as fees\napproach zero.\n"
    },
    {
        "paper_id": 2104.00473,
        "authors": "Joachim Freyberger",
        "title": "Normalizations and misspecification in skill formation models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An important class of structural models studies the determinants of skill\nformation and the optimal timing of interventions. In this paper, I provide new\nidentification results for these models and investigate the effects of\nseemingly innocuous scale and location restrictions on parameters of interest.\nTo do so, I first characterize the identified set of all parameters without\nthese additional restrictions and show that important policy-relevant\nparameters are point identified under weaker assumptions than commonly used in\nthe literature. The implications of imposing standard scale and location\nrestrictions depend on how the model is specified, but they generally impact\nthe interpretation of parameters and can affect counterfactuals. Importantly,\nwith the popular CES production function, commonly used scale restrictions are\noveridentifying and lead to biased estimators. Consequently, simply changing\nthe units of measurements of observed variables might yield ineffective\ninvestment strategies and misleading policy recommendations. I show how\nexisting estimators can easily be adapted to solve these issues. As a\nbyproduct, this paper also presents a general and formal definition of when\nrestrictions are truly normalizations.\n"
    },
    {
        "paper_id": 2104.00574,
        "authors": "Aaron Gregory and Joshua Leeman",
        "title": "On the Perception of Plagiarism in Academia: Context and Intent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Plagiarism is the representation of another author's language, thoughts,\nideas, or expressions as one's own original work. In educational contexts,\nthere are differing definitions of plagiarism depending on the institution.\nProminent scholars of plagiarism include Rebecca Moore Howard, Susan Blum,\nTracey Bretag, and Sarah Elaine Eaton, among others. Plagiarism is considered a\nviolation of academic integrity and a breach of journalistic ethics. It is\nsubject to sanctions such as penalties, suspension, expulsion from school or\nwork, substantial fines and even incarceration. Recently, cases of \"extreme\nplagiarism\" have been identified in academia. The modern concept of plagiarism\nas immoral and originality as an ideal emerged in Europe in the 18th century,\nparticularly with the Romantic movement. Generally, plagiarism is not in itself\na crime, but like counterfeiting fraud can be punished in a court for\nprejudices caused by copyright infringement, violation of moral rights, or\ntorts. In academia and industry, it is a serious ethical offense. Plagiarism\nand copyright infringement overlap to a considerable extent, but they are not\nequivalent concepts, and many types of plagiarism do not constitute copyright\ninfringement, which is defined by copyright law and may be adjudicated by\ncourts. Plagiarism might not be the same in all countries. Some countries, such\nas India and Poland, consider plagiarism to be a crime, and there have been\ncases of people being imprisoned for plagiarizing. In other instances\nplagiarism might be the complete opposite of \"academic dishonesty,\" in fact\nsome countries find the act of plagiarizing a professional's work flattering.\nStudents who move to the United States and other Western countries from\ncountries where plagiarism is not frowned upon often find the transition\ndifficult.\n"
    },
    {
        "paper_id": 2104.0062,
        "authors": "Karush Suri, Xiao Qi Shi, Konstantinos Plataniotis, Yuri Lawryshyn",
        "title": "TradeR: Practical Deep Hierarchical Reinforcement Learning for Trade\n  Execution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Advances in Reinforcement Learning (RL) span a wide variety of applications\nwhich motivate development in this area. While application tasks serve as\nsuitable benchmarks for real world problems, RL is seldomly used in practical\nscenarios consisting of abrupt dynamics. This allows one to rethink the problem\nsetup in light of practical challenges. We present Trade Execution using\nReinforcement Learning (TradeR) which aims to address two such practical\nchallenges of catastrophy and surprise minimization by formulating trading as a\nreal-world hierarchical RL problem. Through this lens, TradeR makes use of\nhierarchical RL to execute trade bids on high frequency real market experiences\ncomprising of abrupt price variations during the 2019 fiscal year COVID19 stock\nmarket crash. The framework utilizes an energy-based scheme in conjunction with\nsurprise value function for estimating and minimizing surprise. In a\nlarge-scale study of 35 stock symbols from the S&P500 index, TradeR\ndemonstrates robustness to abrupt price changes and catastrophic losses while\nmaintaining profitable outcomes. We hope that our work serves as a motivating\nexample for application of RL to practical problems.\n"
    },
    {
        "paper_id": 2104.00668,
        "authors": "Jerome Garnier-Brun, Michael Benzaquen, Stefano Ciliberti,\n  Jean-Philippe Bouchaud",
        "title": "A new spin on optimal portfolios and ecological equilibria",
        "comments": "37 pages, 7 figures",
        "journal-ref": "J. Stat. Mech. (2021) 093408",
        "doi": "10.1088/1742-5468/ac21d9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the classical problem of optimal portfolio construction with the\nconstraint that no short position is allowed, or equivalently the valid\nequilibria of multispecies Lotka-Volterra equations with self-regulation in the\nspecial case where the interaction matrix is of unit rank, corresponding to\nspecies competing for a common resource. We compute the average number of\nsolutions and show that its logarithm grows as $N^\\alpha$, where $N$ is the\nnumber of assets or species and $\\alpha \\leq 2/3$ depends on the interaction\nmatrix distribution. We conjecture that the most likely number of solutions is\nmuch smaller and related to the typical sparsity $m(N)$ of the solutions, which\nwe compute explicitly. We also find that the solution landscape is similar to\nthat of spin-glasses, i.e. very different configurations are quasi-degenerate.\nCorrespondingly, \"disorder chaos\" is also present in our problem. We discuss\nthe consequence of such a property for portfolio construction and ecologies,\nand question the meaning of rational decisions when there is a very large\nnumber \"satisficing\" solutions.\n"
    },
    {
        "paper_id": 2104.00763,
        "authors": "\\\"Uzeyir Aydin, B\\\"u\\c{s}ra A\\u{g}an, \\\"Omer Aydin",
        "title": "Herd Behavior in Crypto Asset Market and Effect of Financial Information\n  on Herd Behavior",
        "comments": null,
        "journal-ref": "International Journal of Economics and Finance Studies, 2020,\n  12(2):581-604",
        "doi": "10.34109/ijefs.202012221",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The initial purpose of the study is to search whether the market exhibits\nherd behaviour or not by examining the crypto-asset market in the context of\nbehavioural finance. And the second purpose of the study is to measure whether\nthe financial information stimulates the herd behaviour or not. Within this\nframe, the announcements of the Federal Open Market Committee (FOMC), Governing\nCouncil of European Central Bank (ECB) and Policy Board of Bank of Japan (BOJ)\nfor interest change, and S&P 500, Nikkei 225, FTSE 100 and GOLD SPOT indices\ndata were used. In the study, the analyses were made over 100 cryptocurrencies\nwith the highest trading volume by the use of the 2014:5 - 2019:12 period. For\nthe analysis, the Markov Switching approach, as well as loads of empiric models\ndeveloped by Chang et al. (2000), were used. According to the results obtained,\nthe presence of herd behaviour in the crypto-asset market was determined in the\nrelevant period. But it was found that interest rate announcements and stock\nexchange performances had no effect on herd behaviour.\n"
    },
    {
        "paper_id": 2104.00834,
        "authors": "Apostolos Filippas and John Horton",
        "title": "The Production and Consumption of Social Media",
        "comments": "36 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model social media as collections of users producing and consuming\ncontent. Users value consuming content, but doing so uses up their scarce\nattention, and hence they prefer content produced by more able users. Users\nalso value receiving attention, creating the incentive to attract an audience\nby producing valuable content, but also through attention bartering -- users\nagree to become each others' audience. Attention bartering can profoundly\naffect the patterns of production and consumption on social media, explains key\nfeatures of social media behavior and platform decision-making, and yields\nsharp predictions that are consistent with data we collect from EconTwitter.\n"
    },
    {
        "paper_id": 2104.00911,
        "authors": "Hyungbin Park",
        "title": "Influence of risk tolerance on long-term investments: A Malliavin\n  calculus approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the influence of risk tolerance on the expected\nutility in the long run. We estimate the extent to which the expected utility\nof optimal portfolios is affected by small changes in the risk tolerance. For\nthis purpose, we adopt the Malliavin calculus method and the Hansen--Scheinkman\ndecomposition, through which the expected utility is expressed in terms of the\neigenvalues and eigenfunctions of an operator. We conclude that the influence\nof risk aversion on the expected utility is determined by these eigenvalues and\neigenfunctions in the long run.\n"
    },
    {
        "paper_id": 2104.00935,
        "authors": "Pramod Kumar Sur and Masaru Sasaki",
        "title": "The Persistent Effect of Famine on Present-Day China: Evidence from the\n  Billionaires",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  More than half a century has passed since the Great Chinese Famine\n(1959-1961), and China has transformed from a poor, underdeveloped country to\nthe world's leading emerging economy. Does the effect of the famine persist\ntoday? To explore this question, we combine historical data on province-level\nfamine exposure with contemporary data on individual wealth. To better\nunderstand if the relationship is causal, we simultaneously account for the\nwell-known historical evidence on the selection effect arising for those who\nsurvive the famine and those born during this period, as well as the issue of\nendogeneity on the exposure of a province to the famine. We find robust\nevidence showing that famine exposure has had a considerable negative effect on\nthe contemporary wealth of individuals born during this period. Together, the\nevidence suggests that the famine had an adverse effect on wealth, and it is\neven present among the wealthiest cohort of individuals in present-day China.\n"
    },
    {
        "paper_id": 2104.00938,
        "authors": "Qida Su, David Z.W. Wang",
        "title": "Bottleneck Congestion And Work Starting Time Distribution Considering\n  Household Travels",
        "comments": "45 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Flextime is one of the efficient approaches in travel demand management to\nreduce peak hour congestion and encourage social distancing in epidemic\nprevention. Previous literature has developed bi-level models of the work\nstarting time choice considering both labor output and urban mobility. Yet,\nmost analytical studies assume the single trip purpose in peak hours (to work)\nonly and do not consider the household travels (daycare drop-off/pick-up). In\nfact, as one of the main reasons to adopt flextime, household travel plays an\ninfluential role in travelers' decision making on work schedule selection. On\nthis account, we incorporate household travels into the work starting time\nchoice model in this study. Both short-run travel behaviours and long-run work\nstart time selection of heterogenous commuters are examined under agglomeration\neconomies. If flextime is not flexible enough, commuters tend to agglomerate in\nwork schedule choice at long-run equilibrium. Further, we analyze optimal\nschedule choices with two system performance indicators. For total commuting\ncost, it is found that the rigid school schedule for households may impede the\nbenefits of flextime in commuting cost saving. In terms of total net benefit,\nwhile work schedule agglomeration of all commuters leads to the maximum in some\ncases, the polarized agglomeration of the two heterogenous groups can never\nachieve the optimum.\n"
    },
    {
        "paper_id": 2104.0097,
        "authors": "Jiahua Xu, Nikhil Vadgama",
        "title": "From banks to DeFi: the evolution of the lending market",
        "comments": null,
        "journal-ref": "Enabling the Internet of Value (2022) 53-66",
        "doi": "10.1007/978-3-030-78184-2_6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Internet of Value (IOV) with its distributed ledger technology (DLT)\nunderpinning has created new forms of lending markets. As an integral part of\nthe decentralised finance (DeFi) ecosystem, lending protocols are gaining\ntremendous traction, holding an aggregate liquidity supply of over $40 billion\nat the time of writing. In this paper, we enumerate the challenges of\ntraditional money markets led by banks and lending platforms, and present\nadvantageous characteristics of DeFi lending protocols that might help resolve\ndeep-rooted issues in the conventional lending environment. With the examples\nof Maker, Compound and Aave, we describe in detail the mechanism of DeFi\nlending protocols. We discuss the persisting reliance of DeFi lending on the\ntraditional financial system, and conclude with the outlook of the lending\nmarket in the IOV era.\n"
    },
    {
        "paper_id": 2104.0104,
        "authors": "Igor Halperin",
        "title": "Distributional Offline Continuous-Time Reinforcement Learning with\n  Neural Physics-Informed PDEs (SciPhy RL for DOCTR-L)",
        "comments": "24 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper addresses distributional offline continuous-time reinforcement\nlearning (DOCTR-L) with stochastic policies for high-dimensional optimal\ncontrol. A soft distributional version of the classical Hamilton-Jacobi-Bellman\n(HJB) equation is given by a semilinear partial differential equation (PDE).\nThis `soft HJB equation' can be learned from offline data without assuming that\nthe latter correspond to a previous optimal or near-optimal policy. A\ndata-driven solution of the soft HJB equation uses methods of Neural PDEs and\nPhysics-Informed Neural Networks developed in the field of Scientific Machine\nLearning (SciML). The suggested approach, dubbed `SciPhy RL', thus reduces\nDOCTR-L to solving neural PDEs from data. Our algorithm called Deep DOCTR-L\nconverts offline high-dimensional data into an optimal policy in one step by\nreducing it to supervised learning, instead of relying on value iteration or\npolicy iteration methods. The method enables a computable approach to the\nquality control of obtained policies in terms of both their expected returns\nand uncertainties about their values.\n"
    },
    {
        "paper_id": 2104.01097,
        "authors": "Davide Fiaschi, Cristina Tealdi",
        "title": "A general methodology to measure labour market dynamics",
        "comments": "28 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a general methodology to measure labour market dynamics, inspired\nby the search and matching framework, based on the estimate of the transition\nrates between labour market states. We show how to estimate instantaneous\ntransition rates starting from discrete time observations provided in\nlongitudinal datasets, allowing for any number of states. We illustrate the\npotential of such methodology using Italian labour market data. First, we\ndecompose the unemployment rate fluctuations into inflow and outflow driven\ncomponents; then, we evaluate the impact of the implementation of a labour\nmarket reform, which substantially changed the regulations of temporary\ncontracts.\n"
    },
    {
        "paper_id": 2104.01127,
        "authors": "Hsuan-Ku Liu",
        "title": "Perpetual callable American volatility options in a mean-reverting\n  volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates problems associated with the valuation of callable\nAmerican volatility put options. Our approach involves modeling volatility\ndynamics as a mean-reverting 3/2 volatility process. We first propose a pricing\nformula for the perpetual American knock-out put. Under the given conditions,\nthe value of perpetual callable American volatility put options is discussed.\n"
    },
    {
        "paper_id": 2104.01176,
        "authors": "Antonio S\\'anchez-Bay\\'on, Miguel \\'Angel Garc\\'ia-Ramos Lucero, Annie\n  Ng Cheng San, Choy Johnn Yee, Krishna Moorthy, Alex Foo Tun Lee, Angelita\n  Kithatu-Kiwekete, Shikha Vyas-Doorgapersad, Anthony Kiryagana Isabirye,\n  Nobukhosi Dlodlo, Lydia Mbati, Edmore Tarambiwa, Chengedzai Mafini, Anastas\n  Djurovski, Ephrem Habtemichael Redda, Jhalukpreya Surujlal",
        "title": "Trends in eBusiness and eGovernment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The first chapter is a critical review and a case study in eBusiness, with\nspecial attention to the digital currencies resource and its possibilities. 2.\nchapter attempts to incorporate the UTAUT model with perceived risk theory to\nexplore its impact on the intention to use m-government services. 3. chapter\naims to assess the level of gender inclusivity in the municipal e-procurement\nprocesses in the City of Johannesburg as a case study. It uses a GAD approach.\n4. chapter examines the impediments that derail the intensive uptake of\neLearning programmes in a particular higher education institution. The study\nadopted an inductive research paradigm that followed a qualitative research\nstrategy. Data were collected by means of one-on-one in-depth interviews from\nselected faculty members at a nominated institution of higher learning. 5.\nchapter investigated the role of KMS in enhancing the export performance of\nfirms operating within the manufacturing sector in Zimbabwe. The study used a\nquantitative approach in which a survey questionnaire was distributed to 555\nmanagers drawn from 185 manufacturing firms based in Harare. Data analyses\ninvolved the use of descriptive statistics, Spearman correlations and\nregression analysis. In the sixth chapter, a survey was undertaken on 131 SMEs\nfrom the Pelagonija region in order to determine the current level of SME\ndigitalization within the region. It is aimed to compare with the EU average\nand to make conclusions on the impact of the SME digitalization on region GDP\ngrowth as well as revenues collection. The last chapter s purpose was to\ndevelop a measuring and modelling framework, an instrument of IBSQ for the\nSouth African banking sector. Snowball and convenience sampling, both\nnon-probability techniques were used to recruit participants for the study. A\ntotal of 310 Internet banking customer responses were utilised in the analysis.\n"
    },
    {
        "paper_id": 2104.01285,
        "authors": "Irene Brunetti, Davide Fiaschi",
        "title": "Occupational Mobility: Theory and Estimation for Italy",
        "comments": "31 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1007/s10888-023-09568-8",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a model where intergenerational occupational mobility is\nthe joint outcome of three main determinants: income incentives, equality of\nopportunity and changes in the composition of occupations. The model\nrationalizes the use of transition matrices to measure mobility, which allows\nfor the identification of asymmetric mobility patterns and for the formulation\nof a specific mobility index for each determinant. Italian children born in\n1940-1951 had a lower mobility with respect to those born after 1965. The\nsteady mobility for children born after 1965, however, covers a lower\nstructural mobility in favour of upper-middle classes and a higher downward\nmobility from upper-middle classes. Equality of opportunity was far from the\nperfection but steady for those born after 1965. Changes in income incentives\ninstead played a major role, leading to a higher downward mobility from\nupper-middle classes and lower upward mobility from the lower class.\n"
    },
    {
        "paper_id": 2104.01295,
        "authors": "Judith A. Chevalier, Jason L. Schwartz, Yihua Su, Kevin R. Williams",
        "title": "Equity Impacts of Dollar Store Vaccine Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We use geospatial data to examine the unprecedented national program\ncurrentlyunderway in the United States to distribute and administer vaccines\nagainst COVID-19. We quantify the impact of the proposed federal partnership\nwith the companyDollar General to serve as vaccination sites and compare\nvaccine access with DollarGeneral to the current Federal Retail Pharmacy\nPartnership Program. Although dollarstores have been viewed with skepticism and\ncontroversy in the policy sector, we showthat, relative to the locations of the\ncurrent federal program, Dollar General stores aredisproportionately likely to\nbe located in Census tracts with high social vulnerability;using these stores\nas vaccination sites would greatly decrease the distance to vaccinesfor both\nlow-income and minority households. We consider a hypothetical\nalternativepartnership with Dollar Tree and show that adding these stores to\nthe vaccinationprogram would be similarly valuable, but impact different\ngeographic areas than theDollar General partnership. Adding Dollar General to\nthe current pharmacy partnersgreatly surpasses the goal set by the Biden\nadministration of having 90% of the popu-lation within 5 miles of a vaccine\nsite. We discuss the potential benefits of leveragingthese partnerships for\nother vaccinations, including against influenza.\n"
    },
    {
        "paper_id": 2104.01365,
        "authors": "Johan Auster, Ludovic Mathys, Fabio Maeder",
        "title": "JDOI Variance Reduction Method and the Pricing of American-Style Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present article revisits the Diffusion Operator Integral (DOI) variance\nreduction technique originally proposed in Heath and Platen (2002) and extends\nits theoretical concept to the pricing of American-style options under\n(time-homogeneous) L\\'evy stochastic differential equations. The resulting Jump\nDiffusion Operator Integral (JDOI) method can be combined with numerous Monte\nCarlo based stopping-time algorithms, including the ubiquitous least-squares\nMonte Carlo (LSMC) algorithm of Longstaff and Schwartz (cf. Carriere (1996),\nLongstaff and Schwartz (2001)). We exemplify the usefulness of our theoretical\nderivations under a concrete, though very general jump-diffusion stochastic\nvolatility dynamics and test the resulting LSMC based version of the JDOI\nmethod. The results provide evidence of a strong variance reduction when\ncompared with a simple application of the LSMC algorithm and proves that\napplying our technique on top of Monte Carlo based pricing schemes provides a\npowerful way to speed-up these methods.\n"
    },
    {
        "paper_id": 2104.01437,
        "authors": "Jorino van Rhijn, Cornelis W. Oosterlee, Lech A. Grzelak, Shuaiqiang\n  Liu",
        "title": "Monte Carlo Simulation of SDEs using GANs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Generative adversarial networks (GANs) have shown promising results when\napplied on partial differential equations and financial time series generation.\nWe investigate if GANs can also be used to approximate one-dimensional Ito\nstochastic differential equations (SDEs). We propose a scheme that approximates\nthe path-wise conditional distribution of SDEs for large time steps. Standard\nGANs are only able to approximate processes in distribution, yielding a weak\napproximation to the SDE. A conditional GAN architecture is proposed that\nenables strong approximation. We inform the discriminator of this GAN with the\nmap between the prior input to the generator and the corresponding output\nsamples, i.e. we introduce a `supervised GAN'. We compare the input-output map\nobtained with the standard GAN and supervised GAN and show experimentally that\nthe standard GAN may fail to provide a path-wise approximation. The GAN is\ntrained on a dataset obtained with exact simulation. The architecture was\ntested on geometric Brownian motion (GBM) and the Cox-Ingersoll-Ross (CIR)\nprocess. The supervised GAN outperformed the Euler and Milstein schemes in\nstrong error on a discretisation with large time steps. It also outperformed\nthe standard conditional GAN when approximating the conditional distribution.\nWe also demonstrate how standard GANs may give rise to non-parsimonious\ninput-output maps that are sensitive to perturbations, which motivates the need\nfor constraints and regularisation on GAN generators.\n"
    },
    {
        "paper_id": 2104.01571,
        "authors": "Viktor Stojkoski, Trifce Sandev, Ljupco Kocarev and Arnab Pal",
        "title": "Geometric Brownian Motion under Stochastic Resetting: A Stationary yet\n  Non-ergodic Process",
        "comments": null,
        "journal-ref": "Phys. Rev. E 104, 014121 (2021)",
        "doi": "10.1103/PhysRevE.104.014121",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the effects of stochastic resetting on geometric Brownian motion\n(GBM), a canonical stochastic multiplicative process for non-stationary and\nnon-ergodic dynamics. Resetting is a sudden interruption of a process, which\nconsecutively renews its dynamics. We show that, although resetting renders GBM\nstationary, the resulting process remains non-ergodic. Quite surprisingly, the\neffect of resetting is pivotal in manifesting the non-ergodic behavior. In\nparticular, we observe three different long-time regimes: a quenched state, an\nunstable and a stable annealed state depending on the resetting strength.\nNotably, in the last regime, the system is self-averaging and thus the sample\naverage will always mimic ergodic behavior establishing a stand alone feature\nfor GBM under resetting. Crucially, the above-mentioned regimes are well\nseparated by a self-averaging time period which can be minimized by an optimal\nresetting rate. Our results can be useful to interpret data emanating from\nstock market collapse or reconstitution of investment portfolios.\n"
    },
    {
        "paper_id": 2104.01761,
        "authors": "Qida Su",
        "title": "Optimal parking provision in multi-modal morning commute problem\n  considering ride-sourcing service",
        "comments": "43 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing morning commute traffic through parking provision management has\nbeen well studied in the literature. However, most previous studies made the\nassumption that all road users require parking spaces at CBD area. However, in\nrecent years, due to technological advancements and low market entry barrier,\nmore and more e-dispatch FHVs (eFHVs) are provided in service. The rapidly\ngrowing eFHVs, on one hand, supply substantial trip services and complete the\ntrips requiring no parking demand; on the other hand, imposes congestion\neffects to all road users. In this study, we investigate the multi-modal\nmorning commute problem with bottleneck congestion and parking space\nconstraints in the presence of ride-sourcing and transit service. Meanwhile, we\nderive the optimal number of parking spaces to best manage the commute traffic.\nOne interesting finding is that, in the presence of ride-sourcing, excessive\nsupply of parking spaces could incur higher system commute costs in the\nmulti-modal case.\n"
    },
    {
        "paper_id": 2104.01764,
        "authors": "Dorsa Mohammadi Arezooji",
        "title": "A Big Data Analysis of the Ethereum Network: from Blockchain to Google\n  Trends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  First, a big data analysis of the transactions and smart contracts made on\nthe Ethereum blockchain is performed, revealing interesting trends in motion.\nNext, these trends are compared with the public's interest in Ether and\nBitcoin, measured by the volume of online searches. An analysis of the crypto\nprices and search trends suggests the existence of big players (and not the\nregular users), manipulating the market after a drop in prices. Lastly, a\ncross-correlation study of crypto prices and search trends reveals the pairs\nproviding more accurate and timely predictions of Ether prices.\n"
    },
    {
        "paper_id": 2104.01773,
        "authors": "Qida Su, David Z.W. Wang",
        "title": "Spatial parking planning design with mixed conventional and autonomous\n  vehicles",
        "comments": "51 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Travellers in autonomous vehicles (AVs) need not to walk to the destination\nany more after parking like those in conventional human-driven vehicles (HVs).\nInstead, they can drop off directly at the destination and AVs can cruise for\nparking autonomously. It is a revolutionary change that such parking autonomy\nof AVs may increase the potential parking span substantially and affect the\nspatial parking equilibrium. Given this, from urban planners' perspective, it\nis of great necessity to reconsider the planning of parking supply along the\ncity. To this end, this paper is the first to examine the spatial parking\nequilibrium considering the mix of AVs and HVs with parking cruising effect. It\nis found that the equilibrium solution of travellers' parking location choices\ncan be biased due to the ignorance of cruising effects. On top of that, the\noptimal parking span of AVs at given parking supply should be no less than that\nat equilibrium. Besides, the optimal parking planning to minimize the total\nparking cost is also explored in a bi-level parking planning design problem\n(PPDP). While the optimal differentiated pricing allows the system to achieve\noptimal parking distribution, this study suggests that it is beneficial to\nencourage AVs to cruise further to park by reserving less than enough parking\nareas for AVs.\n"
    },
    {
        "paper_id": 2104.01847,
        "authors": "Valentina Semenova and Julian Winkler",
        "title": "Social contagion and asset prices: Reddit's self-organised bull runs",
        "comments": "This paper was originally put online as a departmental pre-print on\n  the departmental website at\n  http://www.inet.ox.ac.uk/publications/no-2021-04-reddits-self-organised-bull-runs/\n  and on MPRA on February 2, 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Can unstructured text data from social media help explain the drivers of\nlarge asset price fluctuations? This paper investigates how social forces\naffect asset prices, by using machine learning tools to extract beliefs and\npositions of `hype' traders active on Reddit's WallStreetBets (WSB) forum. Our\nstylized model shows that peer effects help explain return predictability and\nreversals, as well as bubble dynamics. We empirically document that sentiments\nexpressed by WSB users about assets' future performances (bullish or bearish)\nare in part due to sentiments of their peers and past asset returns. The paper\ndirectly estimates the effect of WSB activity on asset prices. We document:\nthat retail trader demand follows WSB discussions through using Trade and Quote\n(TAQ) data, the predictability of prices from retail trader discourse, the\namplified market impact of idiosyncratic investor sentiment from viral content\nonline, and the greater exposure of hype investors to bubbles in the markets.\n"
    },
    {
        "paper_id": 2104.01868,
        "authors": "W. Brian Arthur",
        "title": "Economics in Nouns and Verbs",
        "comments": "12 pages; corrected spellings; added preprint information on front\n  page",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Standard economic theory uses mathematics as its main means of understanding,\nand this brings clarity of reasoning and logical power. But there is a\ndrawback: algebraic mathematics restricts economic modeling to what can be\nexpressed only in quantitative nouns, and this forces theory to leave out\nmatters to do with process, formation, adjustment, creation and nonequilibrium.\nFor these we need a different means of understanding, one that allows verbs as\nwell as nouns. Algorithmic expression is such a means. It allows verbs\n(processes) as well as nouns (objects and quantities). It allows fuller\ndescription in economics, and can include heterogeneity of agents, actions as\nwell as objects, and realistic models of behavior in ill-defined situations.\nThe world that algorithms reveal is action-based as well as object-based,\norganic, possibly ever-changing, and not fully knowable. But it is strangely\nand wonderfully alive.\n"
    },
    {
        "paper_id": 2104.02009,
        "authors": "YingHua He, Shruti Sinha, Xiaoting Sun",
        "title": "Identification and Estimation in Many-to-one Two-sided Matching without\n  Transfers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In a setting of many-to-one two-sided matching with non-transferable\nutilities, e.g., college admissions, we study conditions under which\npreferences of both sides are identified with data on one single market.\nRegardless of whether the market is centralized or decentralized, assuming that\nthe observed matching is stable, we show nonparametric identification of\npreferences of both sides under certain exclusion restrictions. To take our\nresults to the data, we use Monte Carlo simulations to evaluate different\nestimators, including the ones that are directly constructed from the\nidentification. We find that a parametric Bayesian approach with a Gibbs\nsampler works well in realistically sized problems. Finally, we illustrate our\nmethodology in decentralized admissions to public and private schools in Chile\nand conduct a counterfactual analysis of an affirmative action policy.\n"
    },
    {
        "paper_id": 2104.02318,
        "authors": "Nick James and Max Menzies",
        "title": "Efficiency of communities and financial markets during the 2020 pandemic",
        "comments": "Substantial edits and new experiments since v1. Equal contribution",
        "journal-ref": "Chaos 31, 083116 (2021)",
        "doi": "10.1063/5.0054493",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the relationship between the spread of the COVID-19\npandemic, the state of community activity, and the financial index performance\nacross 20 countries. First, we analyze which countries behaved similarly in\n2020 with respect to one of three multivariate time series: daily COVID-19\ncases, Apple mobility data and national equity index price. Next, we study the\ntrajectories of all three of these attributes in conjunction to determine which\nexhibited greater similarity. Finally, we investigate whether country financial\nindices or mobility data responded quicker to surges in COVID-19 cases. Our\nresults indicate that mobility data and national financial indices exhibited\nthe most similarity in their trajectories, with financial indices responding\nquicker. This suggests that financial market participants may have interpreted\nand responded to COVID-19 data more efficiently than governments. Further,\nresults imply that efforts to study community mobility data as a leading\nindicator for financial market performance during the pandemic were misguided.\n"
    },
    {
        "paper_id": 2104.02544,
        "authors": "Sepideh Baghaee, Saeed Nosratabadi, Farshid Aram, and Amir Mosavi",
        "title": "Driving Factors Behind the Social Role of Retail Centers on Recreational\n  Activities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Retail centers can be considered as places for interactional and recreational\nactivities and such social roles of retail centers contribute to the popularity\nof the retail centers. Therefore, the main objective of this study was to\nidentify effective factors encouraging customers to engage with interactional\nactivities and measure how these factors affect customer behavior. Accordingly,\ntwo hypotheses were raised illustrating that the travel time (i.e., the time it\ntakes for a customer to reach the retail center) and the variety of shops (in a\nretail center) increase the percentage of people who spend their leisure time\nand recreational activities retail centers. Two case studies were conducted in\ntwo analogous retail centers, one in Tehran, Iran, and the other in Madrid,\nSpain. According to the results, there is an interaction between the travel\ntime and the motivation for the presence of people in the retail center.\nFurthermore, the results revealed that half of both retail center goers who\nspend more than 10 minutes to reach the retail centers prefer to do leisure\nactivities and browsing than shopping. In other words, the longer it takes a\nperson to get to the center, the more likely he/she is to spend more time in\nthe mall and do more leisure activities. It is also found that there is a\nsignificant relationship between the variety of shops in a retail center and\nthe motivation of customers attending a retail center that encourages people to\nspend their leisure time in retail centers.\n"
    },
    {
        "paper_id": 2104.0258,
        "authors": "David Pastor-Escuredo",
        "title": "Future of work: ethics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Work must be reshaped in the upcoming new era characterized by new challenges\nand the presence of new technologies and computational tools. Over-automation\nseems to be the driver of the digitalization process. Substitution is the\nparadigm leading Artificial Intelligence and robotics development against human\ncognition. Digital technology should be designed to enhance human skills and\nmake more productive use of human cognition and capacities. Digital technology\nis characterized also by scalability because of its easy and inexpensive\ndeployment. Thus, automation can lead to the absence of jobs and scalable\nnegative impact in human development and the performance of business. A look at\ndigitalization from the lens of Sustainable Development Goals can tell us how\ndigitalization impact in different sectors and areas considering society as a\ncomplex interconnected system. Here, reflections on how AI and Data impact\nfuture of work and sustainable development are provided grounded on an ethical\ncore that comprises human-level principles and also systemic principles.\n"
    },
    {
        "paper_id": 2104.02688,
        "authors": "Laurence Carassus and Emmanuel L\\'epinette",
        "title": "Pricing without no-arbitrage condition in discrete time",
        "comments": "arXiv admin note: text overlap with arXiv:1807.04612",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a discrete time setting, we study the central problem of giving a fair\nprice to some financial product. For several decades, the no-arbitrage\nconditions and the martingale measures have played a major role for solving\nthis problem. We propose a new approach for estimating the super-replication\ncost based on convex duality instead of martingale measures duality: The prices\nare expressed using Fenchel conjugate and bi-conjugate without using any\nno-arbitrage condition.The super-hedging problem resolution leads endogenously\nto a weak no-arbitrage condition called Absence of Instantaneous Profit (AIP)\nunder which prices are finite. We study this condition in details, propose\nseveral characterizations and compare it to the no-arbitrage condition.\n"
    },
    {
        "paper_id": 2104.02694,
        "authors": "Anatoliy Swishchuk",
        "title": "Merton Investment Problems in Finance and Insurance for the Hawkes-based\n  Models",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show how to solve Merton optimal investment stochastic control problem for\nHawkes-based models in finance and insurance, i.e., for a wealth portfolio X(t)\nconsisting of a bond and a stock price described by general compound Hawkes\nprocess (GCHP), and for a capital R(t) of an insurance company with the amount\nof claims described by the risk model based on GCHP. The novelty of the results\nconsists of the new Hawkes-based models and in the new optimal investment\nresults in finance and insurance for those models.\n"
    },
    {
        "paper_id": 2104.02752,
        "authors": "David Pastor-Escuredo and Philip Treleaven",
        "title": "Multiscale Governance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Future societal systems will be characterized by heterogeneous human\nbehaviors and also collective action. The interaction between local systems and\nglobal systems will be complex. Humandemics will propagate because of the\npathways that connect the different systems and several invariant behaviors and\npatterns that have emerged globally. On the contrary, infodemics of\nmisinformation can be a risk as it has occurred in the COVID-19 pandemic. The\nemerging fragility or robustness of the system will depend on how this complex\nnetwork of systems is governed. Future societal systems will not be only\nmultiscale in terms of the social dimension, but also in the temporality.\nNecessary and proper prevention and response systems based on complexity, ethic\nand multi-scale governance will be required. Real-time response systems are the\nbasis for resilience to be the foundation of robust societies. A top-down\napproach led by Governmental organs for managing humandemics is not sufficient\nand may be only effective if policies are very restrictive and their efficacy\ndepends not only in the measures implemented but also on the dynamics of the\npolicies and the population perception and compliance. This top-down approach\nis even weaker if there is not national and international coordination.\nCoordinating top-down agencies with bottom-up constructs will be the design\nprinciple. Multi-scale governance integrates decision-making processes with\nsignaling, sensing and leadership mechanisms to drive thriving societal systems\nwith real-time sensitivity.\n"
    },
    {
        "paper_id": 2104.03053,
        "authors": "Maksim Malyy (1), Zeljko Tekic (1 and 2), Tatiana Podladchikova (1)\n  ((1) Skolkovo Institute of Science and Technology, (2) HSE University,\n  Graduate School of Business)",
        "title": "The value of big data for analyzing growth dynamics of technology based\n  new ventures",
        "comments": "38 pages, 10 figures, 9 tables, to be published in Technological\n  Forecasting & Social Change journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study demonstrates that web-search traffic information, in particular,\nGoogle Trends data, is a credible novel source of high-quality and\neasy-to-access data for analyzing technology-based new ventures (TBNVs) growth\ntrajectories. Utilizing the diverse sample of 241 US-based TBNVs, we\ncomparatively analyze the relationship between companies' evolution curves\nrepresented by search activity on the one hand and by valuations achieved\nthrough rounds of venture investments on another. The results suggest that\nTBNV's growth dynamics are positively and strongly correlated with its web\nsearch traffic across the sample. This correlation is more robust when a\ncompany is a) more successful (in terms of valuation achieved) - especially if\nit is a \"unicorn\"; b) consumer-oriented (i.e., b2c); and 3) develops products\nin the form of a digital platform. Further analysis based on fuzzy-set\nQualitative Comparative Analysis (fsQCA) shows that for the most successful\ncompanies (\"unicorns\") and consumer-oriented digital platforms (i.e., b2c\ndigital platform companies) proposed approach may be extremely reliable, while\nfor other high-growth TBNVs it is useful for analyzing their growth dynamics,\nalbeit to a more limited degree. The proposed methodological approach opens a\nwide range of possibilities for analyzing, researching and predicting the\ngrowth of recently formed growth-oriented companies, in practice and academia.\n"
    },
    {
        "paper_id": 2104.03545,
        "authors": "Kevin Kuo and Ronald Richman",
        "title": "Embeddings and Attention in Predictive Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore in depth how categorical data can be processed with embeddings in\nthe context of claim severity modeling. We develop several models that range in\ncomplexity from simple neural networks to state-of-the-art attention based\narchitectures that utilize embeddings. We illustrate the utility of learned\nembeddings from neural networks as pretrained features in generalized linear\nmodels, and discuss methods for visualizing and interpreting embeddings.\nFinally, we explore how attention based models can contextually augment\nembeddings, leading to enhanced predictive performance.\n"
    },
    {
        "paper_id": 2104.03667,
        "authors": "Andrea Bucci and Vito Ciciretti",
        "title": "Market Regime Detection via Realized Covariances: A Comparison between\n  Unsupervised Learning and Nonlinear Models",
        "comments": "21 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is broad empirical evidence of regime switching in financial markets.\nThe transition between different market regimes is mirrored in correlation\nmatrices, whose time-varying coefficients usually jump higher in highly\nvolatile regimes, leading to the failure of common diversification methods. In\nthis article, we aim to identify market regimes from covariance matrices and\ndetect transitions towards highly volatile regimes, hence improving tail-risk\nhedging. Starting from the time series of fractionally differentiated\nsentiment-like future values, two models are applied on monthly realized\ncovariance matrices to detect market regimes. Specifically, the regime\ndetection is implemented via vector logistic smooth transition autoregressive\nmodel (VLSTAR) and through an unsupervised learning methodology, the\nagglomerative hierarchical clustering. Since market regime switches are\nunobservable processes that describe the latent change of market behaviour, the\nability of correctly detecting market regimes is validated in two ways:\nfirstly, randomly generated data are used to assess a correct classification\nwhen regimes are known; secondly, a na\\\"{i}ve trading strategy filtered with\nthe detected regime switches is used to understand whether an improvement is\nshowed when accounting for regime switches. The results point to the VLSTAR as\nthe best performing model for labelling market regimes.\n"
    },
    {
        "paper_id": 2104.03794,
        "authors": "Claudia Tomasini Montenegro, Jens F. Peters, Manuel Baumann, Zhirong\n  Zhao-Karger, Christopher Wolter and Marcel Weil",
        "title": "Environmental assessment of a new generation battery: The\n  magnesium-sulfur system",
        "comments": "pre-print updated by revised version as accepted; 21 pages, 5\n  Figures, 1 table. Funded by the German Research Foundation (DFG) under\n  Project ID 390874152, the Initiative and Networking Fund of the Helmholtz\n  Association (ExNet-003) and the European Union's Horizon 2020 Research and\n  Innovation Programme under Grant Agreement No. 754382",
        "journal-ref": "Journal of Energy Storage Volume 35, March 2021, 102053. ISSN\n  2352-152X\n  (https://www.sciencedirect.com/science/article/pii/S2352152X20318879)",
        "doi": "10.1016/j.est.2020.102053",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  As environmental concerns mostly drive the electrification of our economy and\nthe corresponding increase in demand for battery storage systems, information\nabout the potential environmental impacts of the different battery systems is\nrequired. However, this kind of information is scarce for emerging post-lithium\nsystems such as the magnesium-sulfur (MgS) battery. Therefore, we use life\ncycle assessment following a cradle-to-gate perspective to quantify the\ncumulative energy demand and potential environmental impacts per Wh of the\nstorage capacity of a hypothetical MgS battery (46 Wh/kg). Furthermore, we also\nestimate global warming potential (0.33 kg CO2 eq/Wh) , fossil depletion\npotential (0.09 kg oil eq / Wh), ozone depletion potential (2.5E-08 kg\nCFC-11/Wh) and metal depletion potential (0.044 kg Fe eq/Wh), associated with\nthe MgS battery production. The battery is modelled based on an existing\nprototype MgS pouch cell and hypothetically optimised according to the current\nstate of the art in lithium-ion batteries (LIB), exploring future improvement\npotentials. It turns out that the initial (non-optimised) prototype cell cannot\ncompete with current LIB in terms of energy density or environmental\nperformance, mainly due to the high share of non-active components, decreasing\nits performance substantially. Therefore, if the assumed evolutions of the MgS\ncell composition are achieved to overcome current design hurdles and reach a\ncomparable lifespan, efficiency, cost and safety levels to that of existing\nLIB; then the MgS battery has significant potential to outperform both existing\nLIB, and lithium-sulfur batteries.\n"
    },
    {
        "paper_id": 2104.03911,
        "authors": "Ahmet Kaya, \\\"Omer Aydin",
        "title": "E-Commerce in Turkey and SAP Integrated E-Commerce System",
        "comments": null,
        "journal-ref": "INTERNATIONAL JOURNAL OF eBUSINESS and eGOVERNMENT STUDIES, Vol\n  11, No 2, Year: 2019, ISSN: 2146-0744, pp. 207-225",
        "doi": "10.34111/ijebeg.20191128",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  E-commerce is a kind of shopping by use of the internet. E-commerce, very\ndifferent from the usual shopping concept, is compatible with today's economic\ndynamics. E-commerce is becoming an indispensable method with the increase of\ninternet usage. With the use of E-commerce, there are also a number of\nadvantages for companies. On the other hand, SAP is a pioneer and leader in the\ncompany resource planning software sector. SAP is very important for\nlarge-scale companies. They manage all their processes on SAP and its\nintegration is very important with other related software. In this article, we\ngive brief information on some important aspects of e-commerce and propose a\nsolution for ERP integration of an e-commerce system.\n"
    },
    {
        "paper_id": 2104.04036,
        "authors": "Matias Selser, Javier Kreiner, Manuel Maurette",
        "title": "Optimal Market Making by Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We apply Reinforcement Learning algorithms to solve the classic quantitative\nfinance Market Making problem, in which an agent provides liquidity to the\nmarket by placing buy and sell orders while maximizing a utility function. The\noptimal agent has to find a delicate balance between the price risk of her\ninventory and the profits obtained by capturing the bid-ask spread. We design\nan environment with a reward function that determines an order relation between\npolicies equivalent to the original utility function. When comparing our agents\nwith the optimal solution and a benchmark symmetric agent, we find that the\nDeep Q-Learning algorithm manages to recover the optimal agent.\n"
    },
    {
        "paper_id": 2104.04041,
        "authors": "Jia Wang, Tong Sun, Benyuan Liu, Yu Cao, Hongwei Zhu",
        "title": "CLVSA: A Convolutional LSTM Based Variational Sequence-to-Sequence Model\n  with Attention for Predicting Trends of Financial Markets",
        "comments": "7 pages, Proceedings of the Twenty-Eighth International Joint\n  Conference on Artificial Intelligence (IJCAI-19)",
        "journal-ref": null,
        "doi": "10.24963/ijcai.2019/514",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial markets are a complex dynamical system. The complexity comes from\nthe interaction between a market and its participants, in other words, the\nintegrated outcome of activities of the entire participants determines the\nmarkets trend, while the markets trend affects activities of participants.\nThese interwoven interactions make financial markets keep evolving. Inspired by\nstochastic recurrent models that successfully capture variability observed in\nnatural sequential data such as speech and video, we propose CLVSA, a hybrid\nmodel that consists of stochastic recurrent networks, the sequence-to-sequence\narchitecture, the self- and inter-attention mechanism, and convolutional LSTM\nunits to capture variationally underlying features in raw financial trading\ndata. Our model outperforms basic models, such as convolutional neural network,\nvanilla LSTM network, and sequence-to-sequence model with attention, based on\nbacktesting results of six futures from January 2010 to December 2017. Our\nexperimental results show that, by introducing an approximate posterior, CLVSA\ntakes advantage of an extra regularizer based on the Kullback-Leibler\ndivergence to prevent itself from overfitting traps.\n"
    },
    {
        "paper_id": 2104.04134,
        "authors": "Istvan Gere, Szabolcs Kelemen, Geza Toth, Tamas Biro and Zoltan Neda",
        "title": "Wealth distribution in modern societies: collected data and a master\n  equation approach",
        "comments": "Latex, 10 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126194",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A mean-field like stochastic evolution equation with growth and reset terms\n(LGGR model) is used to model wealth distribution in modern societies. The\nstationary solution of the model leads to an analytical form for the density\nfunction that is successful in describing the observed data for all wealth\ncategories. In the limit of high wealth values the proposed density function\nhas the accepted Tsallis-Pareto shape. Our results are in agreement with the\npredictions of an earlier approach based on a mean-field like wealth exchange\nprocess.\n"
    },
    {
        "paper_id": 2104.04233,
        "authors": "Ofelia Bonesini, Giorgia Callegaro, Antoine Jacquier",
        "title": "Functional quantization of rough volatility and applications to\n  volatility derivatives",
        "comments": null,
        "journal-ref": "Quantitative Finance, 23:12, 1769-1792 (2023)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a product functional quantization of rough volatility. Since the\nquantizers can be computed offline, this new technique, built on the insightful\nworks by Luschgy and Pages, becomes a strong competitor in the new arena of\nnumerical tools for rough volatility. We concentrate our numerical analysis to\npricing VIX Futures in the rough Bergomi model and compare our results to other\nrecently suggested benchmarks.\n"
    },
    {
        "paper_id": 2104.04264,
        "authors": "Jozef Barunik and Josef Kurka",
        "title": "Risks of heterogeneously persistent higher moments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using intraday data for the cross-section of individual stocks, we show that\nboth transitory and persistent fluctuations in realized market and average\nidiosyncratic volatility, skewness and kurtosis are differentially priced in\nthe cross-section of asset returns, implying a heterogeneous persistence\nstructure of different sources of higher moment risks. Specifically, we find\nthat idiosyncratic transitory shocks to volatility as well as idiosyncratic\npersistent shocks to skewness contain strong commonalities that are relevant to\ninvestors.\n"
    },
    {
        "paper_id": 2104.04396,
        "authors": "David Itkin, Martin Larsson",
        "title": "On A Class Of Rank-Based Continuous Semimartingales",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the theory of Dirichlet forms we construct a large class of continuous\nsemimartingales on an open domain $E \\subset \\mathbb{R}^d$, which are governed\nby rank-based, in addition to name-based, characteristics. Using the results of\nBaur et al. [Potential Analysis, 38(4):1233-1258,2013] we obtain a strong\nFeller property for this class of diffusions. As a consequence we are able to\nestablish the nonexistence of triple collisions and obtain a simplified formula\nfor the dynamics of its rank process. We also establish conditions under which\nthe process is ergodic. Our main motivation is Stochastic Portfolio Theory\n(SPT), where rank-based diffusions of this type are used to model financial\nmarkets. We show that three main classes of models studied in SPT -- Atlas\nmodels, generalized volatility-stabilized models and polynomial models -- are\nspecial cases of our framework.\n"
    },
    {
        "paper_id": 2104.04545,
        "authors": "Daniel Straulino, Juan C. Saldarriaga, Jairo A. G\\'omez, Juan C.\n  Duque, Neave O'Clery",
        "title": "Uncovering commercial activity in informal cities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Knowledge of the spatial organisation of economic activity within a city is\nkey to policy concerns. However, in developing cities with high levels of\ninformality, this information is often unavailable. Recent progress in machine\nlearning together with the availability of street imagery offers an affordable\nand easily automated solution. Here we propose an algorithm that can detect\nwhat we call 'visible firms' using street view imagery. Using Medell\\'in,\nColombia as a case study, we illustrate how this approach can be used to\nuncover previously unseen economic activity. Applying spatial analysis to our\ndataset we detect a polycentric structure with five distinct clusters located\nin both the established centre and peripheral areas. Comparing the density of\nvisible and registered firms, we find that informal activity concentrates in\npoor but densely populated areas. Our findings highlight the large gap between\nwhat is captured in official data and the reality on the ground.\n"
    },
    {
        "paper_id": 2104.0457,
        "authors": "Marco Due\\~nas and V\\'ictor Ortiz and Massimo Riccaboni and Francesco\n  Serti",
        "title": "Assessing the Impact of COVID-19 on Trade: a Machine Learning\n  Counterfactual Analysis",
        "comments": "31 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By interpreting exporters' dynamics as a complex learning process, this paper\nconstitutes the first attempt to investigate the effectiveness of different\nMachine Learning (ML) techniques in predicting firms' trade status. We focus on\nthe probability of Colombian firms surviving in the export market under two\ndifferent scenarios: a COVID-19 setting and a non-COVID-19 counterfactual\nsituation. By comparing the resulting predictions, we estimate the individual\ntreatment effect of the COVID-19 shock on firms' outcomes. Finally, we use\nrecursive partitioning methods to identify subgroups with differential\ntreatment effects. We find that, besides the temporal dimension, the main\nfactors predicting treatment heterogeneity are interactions between firm size\nand industry.\n"
    },
    {
        "paper_id": 2104.04595,
        "authors": "Ivan Kitov",
        "title": "The link between unemployment and real economic growth in developed\n  countries",
        "comments": "39 pages, 30 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ten years ago we presented a modified version of Okun law for the biggest\ndeveloped economies and reported its excellent predictive power. In this study,\nwe revisit the original models using the estimates of real GDP per capita and\nunemployment rate between 2010 and 2019. The initial results show that the\nchange in unemployment rate can be accurately predicted by variations in the\nrate of real economic growth. There is a discrete version of the model which is\nrepresented by a piece wise linear dependence of the annual increment in\nunemployment rate on the annual rate of change in real GDP per capita. The\nlengths of the country-dependent time segments are defined by breaks in the GDP\nmeasurement units associated with definitional revisions to the nominal GDP and\nGDP deflator (dGDP). The difference between the CPI and dGDP indices since the\nbeginning of measurements reveals the years of such breaks. Statistically, the\nlink between the studied variables in the revised models is characterized by\nthe coefficient of determination in the range from R2=0.866 (Australia) to\nR2=0.977 (France). The residual errors can be likely associated with the\nmeasurement errors, e.g. the estimates of real GDP per capita from various\nsources differ by tens of percent. The obtained results confirm the original\nfinding on the absence of structural unemployment in the studied developed\ncountries.\n"
    },
    {
        "paper_id": 2104.04601,
        "authors": "Daniel Boller, Michael Lechner and Gabriel Okasa",
        "title": "The Effect of Sport in Online Dating: Evidence from Causal Machine\n  Learning",
        "comments": "97 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online dating emerged as a key platform for human mating. Previous research\nfocused on socio-demographic characteristics to explain human mating in online\ndating environments, neglecting the commonly recognized relevance of sport.\nThis research investigates the effect of sport activity on human mating by\nexploiting a unique data set from an online dating platform. Thereby, we\nleverage recent advances in the causal machine learning literature to estimate\nthe causal effect of sport frequency on the contact chances. We find that for\nmale users, doing sport on a weekly basis increases the probability to receive\na first message from a woman by 50%, relatively to not doing sport at all. For\nfemale users, we do not find evidence for such an effect. In addition, for male\nusers the effect increases with higher income.\n"
    },
    {
        "paper_id": 2104.04639,
        "authors": "L\\'aszl\\'o Czaller, Gerg\\H{o} T\\'oth, Bal\\'azs Lengyel",
        "title": "Vaccine allocation to blue-collar workers",
        "comments": "11 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Vaccination may be the solution to the pandemic-induced health crisis, but\nthe allocation of vaccines is a complex task in which economic and social\nconsiderations can be important. The central problem is to use the limited\nnumber of vaccines in a country to reduce the risk of infection and mitigate\neconomic uncertainty at the same time. In this paper, we propose a simple\neconomic model for vaccine allocation across two types of workers:\nwhite-collars can work from home; while blue-collars must work on site. These\nworker types are complementary to each other, thus a negative shock to the\nsupply of either one decreases the demand for the other that leads to\nunemployment. Using parameters of blue and white-collar labor supply, their\ninfection risks, productivity losses at home office during lock-down, and\navailable vaccines, we express the optimal share of vaccines allocated to\nblue-collars. The model points to the dominance of blue-collar vaccination,\nespecially during waves when their relative infection risks increase and when\nthe number of available vaccines is limited. Taking labor supply data from 28\nEuropean countries, we quantify blue-collar vaccine allocation that minimizes\nunemployment across levels of blue- and white-collar infection risks. The model\nfavours blue-collar vaccination identically across European countries in case\nof vaccine scarcity. As more vaccines become available, economies that host\nlarge-shares of employees in home-office shall increasingly immunize them in\ncase blue-collar infection risks can be kept down. Our results highlight that\nvaccination plans should include workers and rank them by type of occupation.\nWe propose that prioritizing blue-collar workers during infection waves and\nearly vaccination can also favour economy besides helping the most vulnerable\nwho can transmit more infection.\n"
    },
    {
        "paper_id": 2104.04729,
        "authors": "Yue Liu, Lixin Tian, Zhuyun Xie, Zaili Zhen, Huaping Sun",
        "title": "Option to survive or surrender: carbon asset management and optimization\n  in thermal power enterprises from China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Carbon emission right allowance is a double-edged sword, one edge is to\nreduce emission as its original design intention, another edge has in practice\nslain many less developed coal-consuming enterprises, especially for those in\nthermal power industry. Partially governed on the hilt in hands of the\nauthority, body of this sword is the prices of carbon emission right. How\nshould the thermal power plants dance on the blade motivates this research.\nConsidering the impact of price fluctuations of carbon emission right\nallowance, we investigate the operation of Chinese thermal power plant by\nmodeling the decision-making with optimal stopping problem, which is\nestablished on the stochastic environment with carbon emission allowance price\nprocess simulated by geometric Brownian motion. Under the overall goal of\nmaximizing the ultimate profitability, the optimal stopping indicates the\ntiming of suspend or halt of production, hence the optimal stopping boundary\ncurve implies the edge of life and death with regard to this enterprise.\nApplying this methodology, real cases of failure and survival of several\nChinese representative thermal power plants were analyzed to explore the\nindustry ecotope, which leads to the findings that: 1) The survival environment\nof existed thermal power plants becomes severer when facing more pressure from\nthe newborn carbon-finance market. 2) Boundaries of survival environment is\nmainly drawn by the technical improvements for rising the utilization rate of\ncarbon emission. Based on the same optimal stopping model, outlook of this\nindustry is drawn with a demarcation surface defining the vivosphere of thermal\npower plants with different levels of profitability. This finding provides\nbenchmarks for those enterprises struggling for survival and policy makers\nscheming better supervision and necessary intervene.\n"
    },
    {
        "paper_id": 2104.04744,
        "authors": "Sareesh Rawat",
        "title": "WTO GPA and Sustainable Procurement as Tools for Transitioning to a\n  Circular Economy",
        "comments": "30 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We live in an age of consumption with an ever-increasing demand of already\nscarce resources and equally fast growing problems of waste generation and\nclimate change. To tackle these difficult issues, we must learn from mother\nnature. Just like waste does not exist in nature, we must strive to create\ncircular ecosystems where waste is minimized and energy is conserved. This\npaper focuses on how public procurement can help us transition to a more\ncircular economy, while navigating international trade laws that govern it.\n"
    },
    {
        "paper_id": 2104.04813,
        "authors": "Kerstin H\\\"otte",
        "title": "Demand-pull, technology-push, and the direction of technological change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the impact of Demand-pull (DP) and Technology-push (TP) on\ngrowth, innovation, and the factor bias of technological change in a two-layer\nnetwork of input-output (market) and patent citation (innovation) links among\n307 6-digit US manufacturing industries in 1977-2012. Two types of TP and DP\nare distinguished: (1) DP and TP are between-layer spillovers when market\ndemand shocks pull innovation and innovation pushes market growth. (2)\nWithin-layer DP arises if downstream users trigger upstream innovation and\ngrowth, while TP effects spill over from up- to downstream industries. The\nresults support between- and within-layer TP: Innovation spillovers from\nupstream industries drive market growth and innovation. Within the market,\nupstream supply shocks stimulate growth, but this effect differs across\nindustries. DP is not supported but shows a factor bias favoring labor, while\nTP comes with a shift towards non-production work. The results are strongest\nafter the 2000s and shed light on the drivers of recent technological change\nand its factor bias.\n"
    },
    {
        "paper_id": 2104.04918,
        "authors": "Giuseppe Storti, Chao Wang",
        "title": "Modelling uncertainty in financial tail risk: a forecast combination and\n  weighted quantile approach",
        "comments": "32 pages, 3 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2005.04868",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A novel forecast combination and weighted quantile based tail-risk\nforecasting framework is proposed, aiming to reduce the impact of modelling\nuncertainty in tail-risk forecasting. The proposed approach is based on a\ntwo-step estimation procedure. The first step involves the combination of\nValue-at-Risk (VaR) forecasts at a grid of quantile levels. A range of\nparametric and semi-parametric models is selected as the model universe in the\nforecast combination procedure. The quantile forecast combination weights are\nestimated by optimizing the quantile loss. In the second step, the Expected\nShortfall (ES) is computed as a weighted average of combined quantiles. The\nquantiles weighting structure for ES forecasting is determined by minimizing a\nstrictly consistent joint VaR and ES loss function of the Fissler-Ziegel class.\nThe proposed framework is applied to six stock market indices and its\nforecasting performance is compared to each individual model in the universe, a\nsimple average approach and a weighted quantile approach. The forecasting\nresults support the proposed framework.\n"
    },
    {
        "paper_id": 2104.0496,
        "authors": "Fabrizio Lillo, Giulia Livieri, Stefano Marmi, Anton Solomko, Sandro\n  Vaienti",
        "title": "Analysis of bank leverage via dynamical systems and deep neural networks",
        "comments": "51 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a model of a simple financial system consisting of a leveraged\ninvestor that invests in a risky asset and manages risk by using Value-at-Risk\n(VaR). The VaR is estimated by using past data via an adaptive expectation\nscheme. We show that the leverage dynamics can be described by a dynamical\nsystem of slow-fast type associated with a unimodal map on [0,1] with an\nadditive heteroscedastic noise whose variance is related to the portfolio\nrebalancing frequency to target leverage. In absence of noise the model is\npurely deterministic and the parameter space splits in two regions: (i) a\nregion with a globally attracting fixed point or a 2-cycle; (ii) a dynamical\ncore region, where the map could exhibit chaotic behavior. Whenever the model\nis randomly perturbed, we prove the existence of a unique stationary density\nwith bounded variation, the stochastic stability of the process and the almost\ncertain existence and continuity of the Lyapunov exponent for the stationary\nmeasure. We then use deep neural networks to estimate map parameters from a\nshort time series. Using this method, we estimate the model in a large dataset\nof US commercial banks over the period 2001-2014. We find that the parameters\nof a substantial fraction of banks lie in the dynamical core, and their\nleverage time series are consistent with a chaotic behavior. We also present\nevidence that the time series of the leverage of large banks tend to exhibit\nchaoticity more frequently than those of small banks.\n"
    },
    {
        "paper_id": 2104.05204,
        "authors": "Tianxiang Zhan, Fuyuan Xiao",
        "title": "A Fast Evidential Approach for Stock Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/int.22598",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the framework of evidence theory, the confidence functions of\ndifferent information can be combined into a combined confidence function to\nsolve uncertain problems. The Dempster combination rule is a classic method of\nfusing different information. This paper proposes a similar confidence function\nfor the time point in the time series. The Dempster combination rule can be\nused to fuse the growth rate of the last time point, and finally a relatively\naccurate forecast data can be obtained. Stock price forecasting is a concern of\neconomics. The stock price data is large in volume, and more accurate forecasts\nare required at the same time. The classic methods of time series, such as\nARIMA, cannot balance forecasting efficiency and forecasting accuracy at the\nsame time. In this paper, the fusion method of evidence theory is applied to\nstock price prediction. Evidence theory deals with the uncertainty of stock\nprice prediction and improves the accuracy of prediction. At the same time, the\nfusion method of evidence theory has low time complexity and fast prediction\nprocessing speed.\n"
    },
    {
        "paper_id": 2104.05229,
        "authors": "A Jayakrishnan, Anil Lal S",
        "title": "Assessing the practicability of the condition used for dynamic\n  equilibrium in Pasinetti theory of distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this note an assessment of the condition \\(K_w/K=S_w/S\\) is made to\ninterpret its meaning to the Passineti's theory of\ndistribution\\cite{pasinetti1962rate}. This condition leads the theory to\nenforce the result \\(s_w\\rightarrow0\\) as \\(P_w\\rightarrow 0\\), which is the\nPasinetti's description about behavior of the workers. We find that the\nPasinetti's claim, of long run worker's propensity to save as not influencing\nthe distribution of income between profits and the wage can not be generalized.\nThis claim is found to be valid only when \\(W>>P_w\\) or \\(P_w=0\\) with\n\\(W\\ne0\\). In practice, the Pasinetti's condition imposes a restriction on the\nactual savings by one of the agents to a lower level compared to its full\nsaving capacity. An implied relationship between the propensities to save by\nworkers and capitalists shows that the Passineti's condition can be practiced\nonly through a contract for a constant value of \\(R=s_w/s_c\\), to be agreed\nupon between the workers and the capitalists. It is showed that the Passineti's\ncondition can not be described as a dynamic equilibrium of economic growth.\nImplementation of this condition (a) may lead to accumulation of unsaved\nincome, (b) reduces growth of capital, (c)is not practicable and (d) is not\nwarranted. We have also presented simple mathematical steps for the derivation\nof the Pasinetti's final equation compared to those presented in\n\\cite{pasinetti1962rate}\n"
    },
    {
        "paper_id": 2104.05273,
        "authors": "Claudiu Albulescu (CRIEF), Michel Mina, Cornel Oros",
        "title": "Oil-US Stock Market Nexus: Some insights about the New Coronavirus\n  Crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a new investigation of the relationship between oil and stock\nprices in the context of the outbreak of the new coronavirus crisis.\nSpecifically, we assess to what extent the uncertainty induced by COVID-19\naffects the interaction between oil and the United States (US) stock markets.\nTo this end, we use a wavelet approach and daily data from February 18, 2020 to\nAugust 15, 2020. We identify the lead-lag relationship between oil and stock\nprices, and the intensity of this relationship at different frequency cycles\nand moments in time. Our unique findings show that co-movements between oil and\nstock prices manifest at 3-5-day cycle and are stronger in the first part of\nMarch and the second part of April 2020, when oil prices are leading stock\nprices. The partial wavelet coherence analysis, controlling for the effect of\nCOVID-19 and US economic policy-induced uncertainty, reveals that the\ncoronavirus crisis amplifies the shock propagation between oil and stock\nprices.\n"
    },
    {
        "paper_id": 2104.0528,
        "authors": "Zheng Gong, Carmine Ventre, John O'Hara",
        "title": "The Efficient Hedging Frontier with Deep Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The trade off between risks and returns gives rise to multi-criteria\noptimisation problems that are well understood in finance, efficient frontiers\nbeing the tool to navigate their set of optimal solutions. Motivated by the\nrecent advances in the use of deep neural networks in the context of hedging\nvanilla options when markets have frictions, we introduce the Efficient Hedging\nFrontier (EHF) by enriching the pipeline with a filtering step that allows to\ntrade off costs and risks. This way, a trader's risk preference is matched with\nan expected hedging cost on the frontier, and the corresponding hedging\nstrategy can be computed with a deep neural network.\n  We further develop our framework to improve the EHF and find better hedging\nstrategies. By adding a random forest classifier to the pipeline to forecast\nmarket movements, we show how the frontier shifts towards lower costs and\nreduced risks, which indicates that the overall hedging performances have\nimproved. In addition, by designing a new recurrent neural network, we also\nfind strategies on the frontier where hedging costs are even lower.\n"
    },
    {
        "paper_id": 2104.05413,
        "authors": "Jia Wang, Tong Sun, Benyuan Liu, Yu Cao, Degang Wang",
        "title": "Financial Markets Prediction with Deep Learning",
        "comments": "8 pages, 2018 17th IEEE International Conference on Machine Learning\n  and Applications (ICMLA). IEEE, 2018",
        "journal-ref": null,
        "doi": "10.1109/ICMLA.2018.00022",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial markets are difficult to predict due to its complex systems\ndynamics. Although there have been some recent studies that use machine\nlearning techniques for financial markets prediction, they do not offer\nsatisfactory performance on financial returns. We propose a novel\none-dimensional convolutional neural networks (CNN) model to predict financial\nmarket movement. The customized one-dimensional convolutional layers scan\nfinancial trading data through time, while different types of data, such as\nprices and volume, share parameters (kernels) with each other. Our model\nautomatically extracts features instead of using traditional technical\nindicators and thus can avoid biases caused by selection of technical\nindicators and pre-defined coefficients in technical indicators. We evaluate\nthe performance of our prediction model with strictly backtesting on historical\ntrading data of six futures from January 2010 to October 2017. The experiment\nresults show that our CNN model can effectively extract more generalized and\ninformative features than traditional technical indicators, and achieves more\nrobust and profitable financial performance than previous machine learning\napproaches.\n"
    },
    {
        "paper_id": 2104.05754,
        "authors": "Mattie Landman, Sanna Ojanper\\\"a, Stephen Kinsella, Neave O'Clery",
        "title": "The role of relatedness and strategic linkages between domestic and MNE\n  sectors in regional branching and resilience",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the key role of multinational enterprises (MNEs) in both\ninternational markets and domestic economies, there is no consensus on their\nimpact on their host economy. In particular, do MNEs stimulate new domestic\nfirms through knowledge spillovers? Here, we look at the impact of MNEs on the\nentry and exit of domestic industries in Irish regions before, during, and\nafter the 2008 Financial Crisis. Specifically, we are interested in whether the\npresence of MNEs in a region results in knowledge spillovers and the creation\nof new domestic industries in related sectors. To quantify how related an\nindustry is to a region's industry basket we propose two cohesion measures,\nweighted closeness and strategic closeness, which capture direct linkages and\nthe complex connectivity structure between industries in a region respectively.\nWe use a dataset of government-supported firms in Ireland (covering 90% of\nmanufacturing and exporting) between 2006-2019. We find that domestic\nindustries are both more likely to enter and less likely to leave a region if\nthey are related to so-called 'overlapping' industries containing both domestic\nand MNE firms. In contrast, we find a negative impact on domestic entry and\nsurvival from cohesion to 'exclusive MNE' industries, suggesting that domestic\nfirms are unable to 'leap' and thrive in MNE-proximate industries likely due to\na technology or know-how gap. This dynamic was broken, with domestic firms\nentering MNE exclusive sectors, by a large injection of Brexit diversification\nfunds in 2017-18. Finally, the type of cohesion matters. For example, strategic\nrather than weighted closeness to exclusive domestic sectors matters for both\nentries and exits.\n"
    },
    {
        "paper_id": 2104.05831,
        "authors": "Gabriel Suarez, Juan Raful, Maria A. Luque, Carlos F. Valencia,\n  Alejandro Correa-Bahnsen",
        "title": "Enhancing User' s Income Estimation with Super-App Alternative Data",
        "comments": "5 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper presents the advantages of alternative data from Super-Apps to\nenhance user' s income estimation models. It compares the performance of these\nalternative data sources with the performance of industry-accepted bureau\nincome estimators that takes into account only financial system information;\nsuccessfully showing that the alternative data manage to capture information\nthat bureau income estimators do not. By implementing the TreeSHAP method for\nStochastic Gradient Boosting Interpretation, this paper highlights which of the\ncustomer' s behavioral and transactional patterns within a Super-App have a\nstronger predictive power when estimating user' s income. Ultimately, this\npaper shows the incentive for financial institutions to seek to incorporate\nalternative data into constructing their risk profiles.\n"
    },
    {
        "paper_id": 2104.05835,
        "authors": "Cheng Cai and Tiziano De Angelis",
        "title": "A change of variable formula with applications to multi-dimensional\n  optimal stopping problems",
        "comments": "26 pages; final version accepted for publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a change of variable formula for $C^1$ functions\n$U:\\R_+\\times\\R^m\\to\\R$ whose second order spatial derivatives may explode and\nnot be integrable in the neighbourhood of a surface $b:\\R_+\\times\\R^{m-1}\\to\n\\R$ that splits the state space into two sets $\\cC$ and $\\cD$. The formula is\ntailored for applications in problems of optimal stopping where it is generally\nvery hard to control the second order derivatives of the value function near\nthe optimal stopping boundary. Differently to other existing papers on similar\ntopics we only require that the surface $b$ be monotonic in each variable and\nwe formally obtain the same expression as the classical It\\^o's formula.\n"
    },
    {
        "paper_id": 2104.05844,
        "authors": "Kevin Patrick Darby",
        "title": "Time is Money: The Equilibrium Trading Horizon and Optimal Arrival Price",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Executing even moderately large derivatives orders can be expensive and\nrisky; it's hard to balance the uncertainty of working an order over time\nversus paying a liquidity premium for immediate execution. Here, we introduce\nthe Time Is Money model, which calculates the Equilibrium Trading Horizon over\nwhich to execute an order within the adversarial forces of variance risk and\nliquidity premium. We construct a hypothetical at-the-money option within\nArithmetic Brownian Motion and invert the Bachelier model to compute an\ninflection point between implied variance and liquidity cost as governed by a\ncentral limit order book, each in real time as they evolve. As a result, we\ndemonstrate a novel, continuous-time Arrival Price framework. Further, we argue\nthat traders should be indifferent to choosing between variance risk and\nliquidity cost, unless they have a predetermined bias or an exogenous position\nwith a convex payoff. We, therefore, introduce half-life factor asymptotics to\nthe model based on a convexity factor and compare results to existing models.\nWe also describe a specialization of the model for trading a basket of\ncorrelated instruments, as exemplified by a futures calendar spread. Finally,\nwe establish groundwork for microstructure optimizations as well as explore\nshort term drift and conditional expected slippage within the Equilibrium\nHorizon framework.\n"
    },
    {
        "paper_id": 2104.05993,
        "authors": "Ravshanbek Khodzhimatov, Stephan Leitner, Friederike Wall",
        "title": "On the effect of social norms on performance in teams with distributed\n  decision makers",
        "comments": "arXiv admin note: text overlap with arXiv:2102.12309",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social norms are rules and standards of expected behavior that emerge in\nsocieties as a result of information exchange between agents. This paper\nstudies the effects of emergent social norms on the performance of teams. We\nuse the NK-framework to build an agent-based model, in which agents work on a\nset of interdependent tasks and exchange information regarding their past\nbehavior with their peers. Social norms emerge from these interactions. We find\nthat social norms come at a cost for the overall performance, unless tasks\nassigned to the team members are highly correlated, and the effect is stronger\nwhen agents share information regarding more tasks, but is unchanged when\nagents communicate with more peers. Finally, we find that the established\nfinding that the team-based incentive schemes improve performance for highly\ncomplex tasks still holds in presence of social norms.\n"
    },
    {
        "paper_id": 2104.06044,
        "authors": "Andrea Giuseppe Di Iura, Giulia Terenzi",
        "title": "A Bayesian analysis of gain-loss asymmetry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a quantitative analysis of the gain/loss asymmetry for financial\ntime series by using a Bayesian approach. In particular, we focus on some\nselected indices and analyze the statistical significance of the asymmetry\namount through a Bayesian generalization of the t-Test, which relaxes the\nnormality assumption on the underlying distribution. We propose two different\nmodels for data distribution, we study the convergence of our method and we\nprovide several graphical representations of our numerical results. Finally, we\nperform a sensitivity analysis with respect to model parameters in order to\nstudy the reliability and robustness of our results.\n"
    },
    {
        "paper_id": 2104.06115,
        "authors": "Daniel Sevcovic, Cyril Izuchukwu Udeani",
        "title": "Application of maximal monotone operator method for solving\n  Hamilton-Jacobi-Bellman equation arising from optimal portfolio selection\n  problem",
        "comments": "7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate a fully nonlinear evolutionary\nHamilton-Jacobi-Bellman (HJB) parabolic equation utilizing the monotone\noperator technique. We consider the HJB equation arising from portfolio\noptimization selection, where the goal is to maximize the conditional expected\nvalue of the terminal utility of the portfolio. The fully nonlinear HJB\nequation is transformed into a quasilinear parabolic equation using the\nso-called Riccati transformation method. The transformed parabolic equation can\nbe viewed as the porous media type of equation with source term. Under some\nassumptions, we obtain that the diffusion function to the quasilinear parabolic\nequation is globally Lipschitz continuous, which is a crucial requirement for\nsolving the Cauchy problem. We employ Banach's fixed point theorem to obtain\nthe existence and uniqueness of a solution to the general form of the\ntransformed parabolic equation in a suitable Sobolev space in an abstract\nsetting. Some financial applications of the proposed result are presented in\none-dimensional space.\n"
    },
    {
        "paper_id": 2104.06152,
        "authors": "Paulwin Graewe, Ulrich Horst, Ronnie Sircar",
        "title": "A Maximum Principle approach to deterministic Mean Field Games of\n  Control with Absorption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a class of deterministic mean field games on finite and infinite\ntime horizons arising in models of optimal exploitation of exhaustible\nresources. The main characteristic of our game is an absorption constraint on\nthe players' state process. As a result of the state constraint the optimal\ntime of absorption becomes part of the equilibrium. This requires a novel\napproach when applying Pontyagin's maximum principle. We prove the existence\nand uniqueness of equilibria and solve the infinite horizon models in closed\nform. As players may drop out of the game over time, equilibrium production\nrates need not be monotone nor smooth.\n"
    },
    {
        "paper_id": 2104.06169,
        "authors": "Samson Lasaulce, Chao Zhang, Vineeth Varma, and Irinel Constantin\n  Morarescu",
        "title": "Analysis of the tradeoff between health and economic impacts of the\n  Covid-19 epidemic",
        "comments": null,
        "journal-ref": "Frontiers in Public Health, 2021",
        "doi": "10.3389/fpubh.2021.620770",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Various measures have been taken in different countries to mitigate the\nCovid-19 epidemic. But, throughout the world, many citizens don't understand\nwell how these measures are taken and even question the decisions taken by\ntheir government. Should the measures be more (or less) restrictive? Are they\ntaken for a too long (or too short) period of time? To provide some\nquantitative elements of response to these questions, we consider the\nwell-known SEIR model for the Covid-19 epidemic propagation and propose a\npragmatic model of the government decision-making operation. Although simple\nand obviously improvable, the proposed model allows us to study the tradeoff\nbetween health and economic aspects in a pragmatic and insightful way. Assuming\na given number of phases for the epidemic and a desired tradeoff between health\nand economic aspects, it is then possible to determine the optimal duration of\neach phase and the optimal severity level for each of them. The numerical\nanalysis is performed for the case of France but the adopted approach can be\napplied to any country. One of the takeaway messages of this analysis is that\nbeing able to implement the optimal 4-phase epidemic management strategy in\nFrance would have led to 1.05 million infected people and a GDP loss of 231\nbillion euro instead of 6.88 million of infected and a loss of 241 billion\neuro. This indicates that, seen from the proposed model perspective, the\neffectively implemented epidemic management strategy is good economically,\nwhereas substantial improvements might have been obtained in terms of health\nimpact. Our analysis indicates that the lockdown/severe phase should have been\nmore severe but shorter, and the adjustment phase occurred earlier. Due to the\nnatural tendency of people to deviate from the official rules, updating\nmeasures every month over the whole epidemic episode seems to be more\nappropriate.\n"
    },
    {
        "paper_id": 2104.0617,
        "authors": "John Horton",
        "title": "The Ruble Collapse in an Online Marketplace: Some Lessons for Market\n  Designers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The sharp devaluation of the ruble in 2014 increased the real returns to\nRussians from working in a global online labor marketplace, as con- tracts in\nthis market are dollar-denominated. Russians clearly noticed the opportunity,\nwith Russian hours-worked increasing substantially, primarily on the extensive\nmargin -- incumbent Russians already active were fairly inelastic. Contrary to\nthe predictions of bargaining models, there was little to no pass-through of\nthe ruble price changes in to wages. There was also no evidence of a\ndemand-side response, with buyers not posting more \"Russian friendly\" jobs,\nsuggesting limited cross-side externalities. The key findings -- a high\nextensive margin elasticity but low intensive margin elasticity; little\npass-through into wages; and little evidence of a cross-side externality --\nhave implications for market designers with respect to pricing and supply\nacquisition.\n"
    },
    {
        "paper_id": 2104.06179,
        "authors": "Geoff Boeing",
        "title": "We Live in a Motorized Civilization: Robert Moses Replies to Robert Caro",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 1974, Robert Caro published The Power Broker, a critical biography of\nRobert Moses's dictatorial tenure as the \"master builder\" of mid-century New\nYork. Moses profoundly transformed New York's urban fabric and transportation\nsystem, producing the Brooklyn Battery Tunnel, the Verrazano Narrows Bridge,\nthe Westside Highway, the Cross-Bronx Expressway, the Lincoln Center, the UN\nheadquarters, Shea Stadium, Jones Beach State Park and many other projects.\nHowever, The Power Broker did lasting damage to his public image and today he\nremains one of the most controversial figures in city planning history. On\nAugust 26, 1974, Moses issued a turgid 23-page statement denouncing Caro's work\nas \"full of mistakes, unsupported charges, nasty baseless personalities, and\nrandom haymakers.\" Moses's original typewritten statement survives today as a\ngrainy photocopy in the New York City Parks Department archive. To better\npreserve and disseminate it, I have extracted and transcribed its text using\noptical character recognition and edited the result to correct errors. Here I\ncompile my transcription of Moses's statement, alongside Caro's reply to it.\n"
    },
    {
        "paper_id": 2104.06229,
        "authors": "Bartosz Bartkowski",
        "title": "Don't throw efficiency out with the bathwater: A reply to Jeffery and\n  Verheijen (2020)",
        "comments": null,
        "journal-ref": "Environmental Science and Policy 122 (2021) 72-74",
        "doi": "10.1016/j.envsci.2021.04.011",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, I reply to the recent article by Jeffery and Verheijen (2020)\n'A new soil health policy paradigm: Pay for practice not performance!'. While\nexpressing support for their call for a more pronounced role of soil protection\nin agri-environmental policy, I critically discuss the two main elements of\ntheir specific proposal: its emphasis of the concept of soil health and the\nrecommendation to use action-based payments as the main policy instrument. I\nargue for using soil functions as a more established concept (and thus more\nadequate for policy purposes), which is also informationally richer than soil\nhealth. Furthermore, I provide a more differentiated discussion of the relative\nadvantages and disadvantages of result-based and action-based payments, while\naddressing the specific criticisms towards the former that Jeffery and\nVerheijen voice. Also, I suggest an alternative approach (a hybrid model-based\nscheme) that addresses the limitations of both Jeffery and Verheijen's own\nproposal and the valid criticisms they direct at result-based payments.\n"
    },
    {
        "paper_id": 2104.06254,
        "authors": "E. Ferreira (1), S.Orbe (1), J. Ascorbebeitia (2), B. \\'Alvarez\n  Pereira (3), E. Estrada (4) ((1) Department of Quantitative Methods,\n  University of the Basque Country UPV/EHU, (2) Department of Economic\n  Analysis, University of the Basque Country UPV/EHU, (3) Nova School of\n  Business and Economics (Nova SBE), NOVAFRICA, and BELAB, (4) Institute of\n  Mathematics and Applications, University of Zaragoza, ARAID Foundation.\n  Institute for Cross-Disciplinary Physics and Complex Systems (IFISC,\n  UIB-CSIC), Campus Universitat de les Illes Balears)",
        "title": "Loss of structural balance in stock markets",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use rank correlations as distance functions to establish the\ninterconnectivity between stock returns, building weighted signed networks for\nthe stocks of seven European countries, the US and Japan. We establish the\ntheoretical relationship between the level of balance in a network and stock\npredictability, studying its evolution from 2005 to the third quarter of 2020.\nWe find a clear balance-unbalance transition for six of the nine countries,\nfollowing the August 2011 Black Monday in the US, when the Economic Policy\nUncertainty index for this country reached its highest monthly level before the\nCOVID-19 crisis. This sudden loss of balance is mainly caused by a\nreorganization of the market networks triggered by a group of low\ncapitalization stocks belonging to the non-financial sector. After the\ntransition, the stocks of companies in these groups become all negatively\ncorrelated between them and with most of the rest of the stocks in the market.\nThe implied change in the network topology is directly related to a decrease in\nstocks predictability, a finding with novel important implications for asset\nallocation and portfolio hedging strategies.\n"
    },
    {
        "paper_id": 2104.06259,
        "authors": "Jaydip Sen, Abhishek Dutta, Sidra Mehtab",
        "title": "Profitability Analysis in Stock Investment Using an LSTM-Based Deep\n  Learning Model",
        "comments": "This is the accepted version of our paper in the Second IEEE\n  International Conference on Emerging Technologies (IEEE INCET 2021) which\n  will be organized in Belgaum, Karnataka, INDIA from May 21 to May 23, 2021.\n  The paper is eight pages long, and has fifteen tables and fourteen figures.\n  This is not the final version of the paper",
        "journal-ref": null,
        "doi": "10.1109/INCET51464.2021.9456385",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing robust systems for precise prediction of future prices of stocks\nhas always been considered a very challenging research problem. Even more\nchallenging is to build a system for constructing an optimum portfolio of\nstocks based on the forecasted future stock prices. We present a deep\nlearning-based regression model built on a long-and-short-term memory network\n(LSTM) network that automatically scraps the web and extracts historical stock\nprices based on a stock's ticker name for a specified pair of start and end\ndates, and forecasts the future stock prices. We deploy the model on 75\nsignificant stocks chosen from 15 critical sectors of the Indian stock market.\nFor each of the stocks, the model is evaluated for its forecast accuracy.\nMoreover, the predicted values of the stock prices are used as the basis for\ninvestment decisions, and the returns on the investments are computed.\nExtensive results are presented on the performance of the model. The analysis\nof the results demonstrates the efficacy and effectiveness of the system and\nenables us to compare the profitability of the sectors from the point of view\nof the investors in the stock market.\n"
    },
    {
        "paper_id": 2104.06293,
        "authors": "Minglian Lin and Indranil SenGupta",
        "title": "Analysis of optimal portfolio on finite and small time horizons for a\n  stochastic volatility market model",
        "comments": null,
        "journal-ref": "SIAM Journal on Financial Mathematics, 2021",
        "doi": "10.1137/21M1412281",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the portfolio optimization problem in a financial\nmarket under a general utility function. Empirical results suggest that if a\nsignificant market fluctuation occurs, invested wealth tends to have a notable\nchange from its current value. We consider an incomplete stochastic volatility\nmarket model, that is driven by both a Brownian motion and a jump process. At\nfirst, we obtain a closed-form formula for an approximation to the optimal\nportfolio in a small-time horizon. This is obtained by finding the associated\nHamilton-Jacobi-Bellman integro-differential equation and then approximating\nthe value function by constructing appropriate super-solution and sub-solution.\nIt is shown that the true value function can be obtained by sandwiching the\nconstructed super-solution and sub-solution. We also prove the accuracy of the\napproximation formulas. Finally, we provide a procedure for generating a\nclose-to-optimal portfolio for a finite time horizon.\n"
    },
    {
        "paper_id": 2104.06735,
        "authors": "Przemys{\\l}aw Biecek, Marcin Chlebus, Janusz Gajda, Alicja Gosiewska,\n  Anna Kozak, Dominik Ogonowski, Jakub Sztachelski, Piotr Wojewnik",
        "title": "Enabling Machine Learning Algorithms for Credit Scoring -- Explainable\n  Artificial Intelligence (XAI) methods for clear understanding complex\n  predictive models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rapid development of advanced modelling techniques gives an opportunity to\ndevelop tools that are more and more accurate. However as usually, everything\ncomes with a price and in this case, the price to pay is to loose\ninterpretability of a model while gaining on its accuracy and precision. For\nmanagers to control and effectively manage credit risk and for regulators to be\nconvinced with model quality the price to pay is too high. In this paper, we\nshow how to take credit scoring analytics in to the next level, namely we\npresent comparison of various predictive models (logistic regression, logistic\nregression with weight of evidence transformations and modern artificial\nintelligence algorithms) and show that advanced tree based models give best\nresults in prediction of client default. What is even more important and\nvaluable we also show how to boost advanced models using techniques which allow\nto interpret them and made them more accessible for credit risk practitioners,\nresolving the crucial obstacle in widespread deployment of more complex, 'black\nbox' models like random forests, gradient boosted or extreme gradient boosted\ntrees. All this will be shown on the large dataset obtained from the Polish\nCredit Bureau to which all the banks and most of the lending companies in the\ncountry do report the credit files. In this paper the data from lending\ncompanies were used. The paper then compares state of the art best practices in\ncredit risk modelling with new advanced modern statistical tools boosted by the\nlatest developments in the field of interpretability and explainability of\nartificial intelligence algorithms. We believe that this is a valuable\ncontribution when it comes to presentation of different modelling tools but\nwhat is even more important it is showing which methods might be used to get\ninsight and understanding of AI methods in credit risk context.\n"
    },
    {
        "paper_id": 2104.06776,
        "authors": "Zachary Feinstein and Andreas Sojmark",
        "title": "Contagious McKean-Vlasov systems with heterogeneous impact and exposure",
        "comments": "40 pages. A lacuna in the proof of the old Lemma 3.4 has been\n  rectified through a new Proposition 3.4. The overall presentation has been\n  improved, shortening non-essential parts and giving more details in the\n  proofs in Section 3",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a heterogeneous formulation of a contagious McKean-Vlasov\nsystem, whose inherent heterogeneity comes from asymmetric interactions with a\nnatural and highly tractable structure. It is shown that this formulation\ncharacterises the limit points of a finite particle system, deriving from a\nbalance sheet based model of solvency contagion in interbank markets, where\nbanks have heterogeneous exposure to and impact on the distress within the\nsystem. We also provide a simple result on global uniqueness for the full\nproblem with common noise under a smallness condition on the strength of\ninteractions, and we show that, in the problem without common noise, there is a\nunique differentiable solution up to an explosion time. Finally, we discuss an\nintuitive and consistent way of specifying how the system should jump to\nresolve an instability when the contagious pressures become too large. This is\nknown to happen even in the homogeneous version of the problem, where jumps are\nspecified by a 'physical' notion of solution, but no such notion currently\nexists for a heterogeneous formulation of the system.\n"
    },
    {
        "paper_id": 2104.06904,
        "authors": "Biqing Zhu, Rui Guo, Zhu Deng, Wenli Zhao, Piyu Ke, Xinyu Dou, Steven\n  J. Davis, Philippe Ciais, Pierre Gentine, Zhu Liu",
        "title": "Unprecedented decarbonization of China's power system in the post-COVID\n  era",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In October of 2020, China announced that it aims to start reducing its carbon\ndioxide (CO2) emissions before 2030 and achieve carbon neutrality before 20601.\nThe surprise announcement came in the midst of the COVID-19 pandemic which\ncaused a transient drop in China's emissions in the first half of 2020. Here,\nwe show an unprecedented de-carbonization of China's power system in late 2020:\nalthough China's power related carbon emissions were 0.5% higher in 2020 than\n2019, the majority (92.9%) of the increased power demand was met by increases\nin low-carbon (renewables and nuclear) generation (increased by 9.3%), as\ncompared to only 0.4% increase for fossil fuels. China's low-carbon generation\nin the country grew in the second half of 2020, supplying a record high of\n36.7% (increased by 1.9% compared to 2019) of total electricity in 2020, when\nthe fossil production dropped to a historical low of 63.3%. Combined, the\ncarbon intensity of China's power sector decreased to an historical low of\n519.9 tCO2/GWh in 2020. If the fast decarbonization and slowed down power\ndemand growth from 2019 to 2020 were to continue, by 2030, over half (50.8%) of\nChina's power demand could be provided by low carbon sources. Our results thus\nreveal that China made progress towards its carbon neutrality target during the\npandemic, and suggest the potential for substantial further decarbonization in\nthe next few years if the latest trends persist.\n"
    },
    {
        "paper_id": 2104.07049,
        "authors": "Mohammad Abbas Rezaei",
        "title": "Optimal Design of Limited Partnership Agreements",
        "comments": "49 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  General partners (GP) are sometimes paid on a deal-by-deal basis and other\ntimes on a whole-portfolio basis. When is one method of payment better than the\nother? I show that when assets (projects or firms) are highly correlated or\nwhen GPs have low reputation, whole-portfolio contracting is superior to\ndeal-by-deal contracting. In this case, by bundling payouts together,\nwhole-portfolio contracting enhances incentives for GPs to exert effort.\nTherefore, it is better suited to alleviate the moral hazard problem which is\nstronger than the adverse selection problem in the case of high correlation of\nassets or low reputation of GPs. In contrast, for low correlation of assets or\nhigh reputation of GPs, information asymmetry concerns dominate and\ndeal-by-deal contracts become optimal, as they can efficiently weed out bad\nprojects one by one. These results shed light on recent empirical findings on\nthe relationship between investors and venture capitalists.\n"
    },
    {
        "paper_id": 2104.0726,
        "authors": "Christian Diem and Andr\\'as Borsos and Tobias Reisch and J\\'anos\n  Kert\\'esz and Stefan Thurner",
        "title": "Quantifying firm-level economic systemic risk from nation-wide supply\n  networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crises like COVID-19 or the Japanese earthquake in 2011 exposed the fragility\nof corporate supply networks. The production of goods and services is a highly\ninterdependent process and can be severely impacted by the default of critical\nsuppliers or customers. While knowing the impact of individual companies on\nnational economies is a prerequisite for efficient risk management, the\nquantitative assessment of the involved economic systemic risks (ESR) is\nhitherto practically non-existent, mainly because of a lack of fine-grained\ndata in combination with coherent methods. Based on a unique value added tax\ndataset we derive the detailed production network of an entire country and\npresent a novel approach for computing the ESR of all individual firms. We\ndemonstrate that a tiny fraction (0.035%) of companies has extraordinarily high\nsystemic risk impacting about 23% of the national economic production should\nany of them default. Firm size alone cannot explain the ESR of individual\ncompanies; their position in the production networks does matter substantially.\nIf companies are ranked according to their economic systemic risk index (ESRI),\nfirms with a rank above a characteristic value have very similar ESRI values,\nwhile for the rest the rank distribution of ESRI decays slowly as a power-law;\n99.8% of all companies have an impact on less than 1% of the economy. We show\nthat the assessment of ESR is impossible with aggregate data as used in\ntraditional Input-Output Economics. We discuss how simple policies of\nintroducing supply chain redundancies can reduce ESR of some extremely risky\ncompanies.\n"
    },
    {
        "paper_id": 2104.07319,
        "authors": "Asier Minondo",
        "title": "Exporters' reaction to positive foreign demand shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I use the quasi-natural experiment of the 2018 African swine fever (ASF)\noutbreak in China to analyze swine exporters' reaction to a foreign market's\npositive demand shock. I use the universe of Spanish firms' export transactions\nto China and other countries, and compare the performance of swine and other\nexporters before and after the ASF. The ASF almost tripled Spanish swine\nexporters' sales to China. Swine exporters did not increase exported product\nportfolio or export revenue concentration in their best-performing products in\nChina after the ASF. The increase in exports to China positively impacted\nexport revenue and survival in third markets. This positive impact was\nespecially intense for small swine exporters. Domestic sales also increased for\nswine exporters with liquidity constraints before the ASF.\n"
    },
    {
        "paper_id": 2104.07379,
        "authors": "Shin-Ichiro Inaba",
        "title": "The Struggle with Inequality",
        "comments": "This manuscript is an English translation of Shin-ichiro Inaba,\n  \"Fubyoudou tono Tatakai\" (Tokyo, Bungei Shunjuu, 2016), by the author himself",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is an introductory textbook of the history of economics of inequality\nfor undergraduates and genreral readers. It begins with Adam Smith's critique\nof Rousseau. The first and second chapters focus on Smith and Karl Marx, in the\nbroad classical tradition of economics, where it is believed that there is an\ninseparable relationship between production and distribution, economic growth\nand inequality. Chapters 3 and 4 argue that despite the fact that the founders\nof the neoclassical school had shown an active interest in social issues,\nnamely worker poverty, the issues of production and distribution became\ndiscussed separately among neoclassicals. Toward the end of the 20th century,\nhowever, there was a renewed awareness within economics of the problem of the\nrelationship between production and distribution. The young Piketty's\nbeginnings as an economist are set against this backdrop. Chapters 5 to 8\nexplain the circumstances of the restoration of classical concerns within the\nneoclassical framework. Then, in chapters 9 and 10, I discuss the fact that\nThomas Piketty's seminal work is a new development in this \"inequality\nrenaissance,\" and try to gain a perspective on future trends in the debate.\nMathematical appendix presents simple models of growth and distribution.\n"
    },
    {
        "paper_id": 2104.07438,
        "authors": "Julian D. Cortes, Liliana Rivera, Katerina Bohle Carbonell",
        "title": "Mission Statements in Universities: Readability and performance",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.iedeen.2021.100183",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The mission statement(s) (MS) is one of the most-used tools for planning and\nmanagement. Universities worldwide have implemented MS in their knowledge\nplanning and management processes since the 1980s. Research studies have\nextensively explored the content and readability of MS and its effect on\nperformance in firms, but their effect on public or nonprofit institutions such\nas universities has not been scrutinized with the same intensity. This study\nused Gunning's Fog Index score to determine the readability of a sample of\nworldwide universities' MS and two rankings, i.e., Quacquarelli Symonds World\nUniversity Ranking and SCImago Institutions Rankings, to determine their effect\non performance. No significant readability differences were identified in\nregions, size, focus, research type, age band, or status. Logistic regression\n(cumulative link model) results showed that variables, such as universities'\nage, focus, and size, have more-significant explanatory power on performance\nthan MS readability.\n"
    },
    {
        "paper_id": 2104.07469,
        "authors": "Nazish Ashfaq, Zubair Nawaz, Muhammad Ilyas",
        "title": "A comparative study of Different Machine Learning Regressors For Stock\n  Market Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the development of successful share trading strategies, forecasting the\ncourse of action of the stock market index is important. Effective prediction\nof closing stock prices could guarantee investors attractive benefits. Machine\nlearning algorithms have the ability to process and forecast almost reliable\nclosing prices for historical stock patterns. In this article, we intensively\nstudied NASDAQ stock market and targeted to choose the portfolio of ten\ndifferent companies belongs to different sectors. The objective is to compute\nopening price of next day stock using historical data. To fulfill this task\nnine different Machine Learning regressor applied on this data and evaluated\nusing MSE and R2 as performance metric.\n"
    },
    {
        "paper_id": 2104.07476,
        "authors": "Julian D. Cortes, Diego Tellez, Jesus Godoy",
        "title": "Mission Statement Effect on Research and Innovation Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The mission statement (MS) is the most used organizational strategic planning\ntool worldwide. The relationship between an MS and an organizations financial\nperformance has been shown to be significantly positive, albeit small. However,\nan MSs relationship to the macroeconomic environment and to organizational\ninnovation has not been investigated. We implemented a Structural Equation\nModeling using the SCImago Institutional Ranking (SIR) as a global baseline\nsample and assessment of organizational research and innovation (RandI), an\nautomated MS content analysis, and the Economic Complexity Index (ECI) as a\ncomprehensive macroeconomic environment measure. We found that the median\nperformance of organizations that do not report an MS is significantly higher\nthan that of reporting organizations, and that a path-dependence driven by the\nState's long-term view and investment is a better explanatory variable for\norganizational RandI performance than the MS construct or the intermediate-term\nmacroeconomic environment.\n"
    },
    {
        "paper_id": 2104.07536,
        "authors": "Taimyra Batz Li\\~neiro and Felix M\\\"usgens",
        "title": "Lessons Learned from Photovoltaic Auctions in Germany",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Auctions have become the primary instrument for promoting renewable energy\naround the world. However, the data published on such auctions are typically\nlimited to aggregated information (e.g., total awarded capacity, average\npayments). These data constraints hinder the evaluation of realisation rates\nand other relevant auction dynamics. In this study, we present an algorithm to\novercome these data limitations in German renewable energy auction programme by\ncombining publicly available information from four different databases. We\napply it to the German solar auction programme and evaluate auctions using\nquantitative methods. We calculate realisation rates and - using correlation\nand regression analysis - explore the impact of PV module prices, competition,\nand project and developer characteristics on project realisation and bid\nvalues. Our results confirm that the German auctions were effective. We also\nfound that project realisation took, on average, 1.5 years (with 28% of\nprojects finished late and incurring a financial penalty), nearly half of\nprojects changed location before completion (again, incurring a financial\npenalty) and small and inexperienced developers could successfully participate\nin auctions.\n"
    },
    {
        "paper_id": 2104.07617,
        "authors": "Yusuke Narita and Ayumi Sudo",
        "title": "Curse of Democracy: Evidence from the 21st Century",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Democracy is widely believed to contribute to economic growth and public\nhealth in the 20th and earlier centuries. We find that this conventional wisdom\nis reversed in this century, i.e., democracy has persistent negative impacts on\nGDP growth during 2001-2020. This finding emerges from five different\ninstrumental variable strategies. Our analysis suggests that democracies cause\nslower growth through less investment and trade. For 2020, democracy is also\nfound to cause more deaths from Covid-19.\n"
    },
    {
        "paper_id": 2104.07718,
        "authors": "Yuyu Chen, Liyuan Lin, Ruodu Wang",
        "title": "Risk Aggregation under Dependence Uncertainty and an Order Constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the aggregation of two risks when the marginal distributions are\nknown and the dependence structure is unknown, under the additional constraint\nthat one risk is smaller than or equal to the other. Risk aggregation problems\nwith the order constraint are closely related to the recently introduced notion\nof the directional lower (DL) coupling. The largest aggregate risk in concave\norder (thus, the smallest aggregate risk in convex order) is attained by the DL\ncoupling. These results are further generalized to calculate the best-case and\nworst-case values of tail risk measures. In particular, we obtain analytical\nformulas for bounds on Value-at-Risk. Our numerical results suggest that the\nnew bounds on risk measures with the extra order constraint can greatly improve\nthose with full dependence uncertainty.\n"
    },
    {
        "paper_id": 2104.07761,
        "authors": "Guanghua Chi, Han Fang, Sourav Chatterjee, Joshua E. Blumenstock",
        "title": "Micro-Estimates of Wealth for all Low- and Middle-Income Countries",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1073/pnas.2113658119",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many critical policy decisions, from strategic investments to the allocation\nof humanitarian aid, rely on data about the geographic distribution of wealth\nand poverty. Yet many poverty maps are out of date or exist only at very coarse\nlevels of granularity. Here we develop the first micro-estimates of wealth and\npoverty that cover the populated surface of all 135 low and middle-income\ncountries (LMICs) at 2.4km resolution. The estimates are built by applying\nmachine learning algorithms to vast and heterogeneous data from satellites,\nmobile phone networks, topographic maps, as well as aggregated and\nde-identified connectivity data from Facebook. We train and calibrate the\nestimates using nationally-representative household survey data from 56 LMICs,\nthen validate their accuracy using four independent sources of household survey\ndata from 18 countries. We also provide confidence intervals for each\nmicro-estimate to facilitate responsible downstream use. These estimates are\nprovided free for public use in the hope that they enable targeted policy\nresponse to the COVID-19 pandemic, provide the foundation for new insights into\nthe causes and consequences of economic development and growth, and promote\nresponsible policymaking in support of the Sustainable Development Goals.\n"
    },
    {
        "paper_id": 2104.07839,
        "authors": "Endah R.M. Putri, Lutfi Mardianto, Amirul Hakam, Chairul Imron, Hadi\n  Susanto",
        "title": "Removing non-smoothness in solving Black-Scholes equation using a\n  perturbation method",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physleta.2021.127367",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Black-Scholes equation as one of the most celebrated mathematical models has\nan explicit analytical solution known as the Black-Scholes formula. Later\nvariations of the equation, such as fractional or nonlinear Black-Scholes\nequations, do not have a closed form expression for the corresponding formula.\nIn that case, one will need asymptotic expansions, including homotopy\nperturbation method, to give an approximate analytical solution. However, the\nsolution is non-smooth at a special point. We modify the method by {first}\nperforming variable transformations that push the point to infinity. As a test\nbed, we apply the method to the solvable Black-Scholes equation, where\nexcellent agreement with the exact solution is obtained. We also extend our\nstudy to multi-asset basket and quanto options by reducing the cases to\nsingle-asset ones. Additionally we provide a novel analytical solution of the\nsingle-asset quanto option that is simple and different from the existing\nexpression.\n"
    },
    {
        "paper_id": 2104.07888,
        "authors": "Luyao Zhang and Yulin Liu",
        "title": "Optimal Algorithmic Monetary Policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Centralized monetary policy, leading to persistent inflation, is often\ninconsistent, untrustworthy, and unpredictable. Algorithmic stablecoins enabled\nby blockchain technology are promising in solving this problem. Algorithmic\nstablecoins utilize a monetary policy that is entirely rule-based. However,\nthere is little understanding of how to optimize the rule. We propose a model\nthat trade-off the price for supply stability. We further study the comparative\nstatics by varying several design features. Finally, we discuss the empirical\nimplications for designing stablecoins by the private sector and Central Bank\nDigital Currency (CBDC) by the public sector.\n"
    },
    {
        "paper_id": 2104.07962,
        "authors": "Marcel Ausloos, Valerio Ficcadenti, Gurjeet Dhesi, Muhammad Shakeel",
        "title": "Benford's laws tests on S&P500 daily closing values and the\n  corresponding daily log-returns both point to huge non-conformity",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.125969",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The so-called Benford's laws are of frequent use in order to observe\nanomalies and regularities in data sets, in particular, in election results and\nfinancial statements. Yet, basic financial market indices have not been much\nstudied, if studied at all, within such a perspective. This paper presents\nfeatures in the distributions of S\\&P500 daily closing values and the\ncorresponding daily log returns over a long time interval, [03/01/1950 -\n22/08/2014], amounting to 16265 data points. We address the frequencies of the\nfirst, second, and first two significant digits counts and explore the\nconformance to Benford's laws of these distributions at five different (equal\nsize) levels of disaggregation. The log returns are studied for either positive\nor negative cases. The results for the S&P500 daily closing values are showing\na huge lack of non-conformity, whatever the different levels of disaggregation.\nSome \"first digits\" and \"first two digits\" values are even missing. The causes\nof this non-conformity are discussed, pointing to the danger in taking\nBenford's laws for granted in huge databases, whence drawing \"definite\nconclusions\". The agreements with Benford's laws are much better for the log\nreturns. Such a disparity in agreements finds an explanation in the data set\nitself: the inherent trend in the index. To further validate this, daily\nreturns have been simulated calibrating the simulations with the observed data\naverages and tested against Benford's laws. One finds that not only the trend\nbut also the standard deviation of the distributions are relevant parameters in\nconcluding about conformity with Benford's laws.\n"
    },
    {
        "paper_id": 2104.07976,
        "authors": "Jan Rosenzweig",
        "title": "Power-law Portfolios",
        "comments": null,
        "journal-ref": "Wilmott 2022",
        "doi": "10.54946/wilm.10987",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio optimization methods suffer from a catalogue of known problems,\nmainly due to the facts that pair correlations of asset returns are unstable,\nand that extremal risk measures such as maximum drawdown are difficult to\npredict due to the non-Gaussianity of portfolio returns. \\\\ In order to look at\noptimal portfolios for arbitrary risk penalty functions, we construct portfolio\nshapes where the penalty is proportional to a moment of the returns of\narbitrary order $p>2$. \\\\ The resulting component weight in the portfolio\nscales sub-linearly with its return, with the power-law $w \\propto\n\\mu^{1/(p-1)}$. This leads to significantly improved diversification when\ncompared to Kelly portfolios, due to the dilution of the winner-takes-all\neffect.\\\\ In the limit of penalty order $p\\rightarrow\\infty$, we recover the\nsimple trading heuristic whereby assets are allocated a fixed positive weight\nwhen their return exceeds the hurdle rate, and zero otherwise. Infinite order\npower-law portfolios thus fall into the class of perfectly diversified\nportfolios.\n"
    },
    {
        "paper_id": 2104.08144,
        "authors": "Manoel Benedito Nirdo da Silva Campos",
        "title": "Contribui\\c{c}\\~ao do ecoturismo para o uso sustent\\'avel dos recursos\n  h\\'idricos do munic\\'ipio de Rondon\\'opolis-MT",
        "comments": "113 p., in Portuguese: il. Baseado na disserta\\c{c}\\~ao de Mestrado\n  em Geografia do Campus Universit\\'ario de Rondon\\'opolis da Universidade\n  Federal de Mato Grosso, intitulado: Contribui\\c{c}\\~ao do ecoturismo para o\n  uso sustent\\'avel dos recursos h\\'idricos do munic\\'ipio de Rondon\\'opolis -\n  MT, defendida em 2015. Sob orienta\\c{c}\\~ao da Professora Dra. Ant\\^onia\n  Mar\\'ilia Medeiros Nardes",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Municipality of Rondon\\'opolis possesses several touristic attractions\nsuch as a great diversity of waterfalls and little beaches located in the\nsurroundings of the urban area, which attract tourists from various locations.\nAiming to understand how ecotourism can contribute to the conservation of water\nresources in the leisure areas, as well as their potential development of\ntouristic activities in those places. The procedures included the use of\nvarious techniques subsidized in remote sensing and geoprocessing tools that\nallowed the analysis and spatial distribution of tourism activities of the main\nleisure areas. The spatial distribution of the waterfalls and its surroundings,\nwe observe the biophysical characters such as: the endemic vegetation, the\ncachoeiras, the waterfalls, the rocky outcrops, rivers, little beaches and\nespraiados. The results showed a correct perception of respondents on existing\ninter-relationships between ecotourism practices and the sustainable use of\nwater resources. In conclusion though, a long way must be performed in order to\nprevent the economic benefits of ecotourism generate an inappropriate\nexploitation of natural resources, causing environmental problems, particularly\nto water resources in the surroundings.\n"
    },
    {
        "paper_id": 2104.08182,
        "authors": "Geoffrey L. Cueto, Francis Aldrine A. Uy, Keith Anshilo Diaz",
        "title": "Exploratory Data Analysis of Electric Tricycle as Sustainable Public\n  Transport Mode in General Santos City Using Logistic Regression",
        "comments": "Submitted to Engineering Journal, 13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  General Santos City, as the tuna capital of the Philippines, relies with the\npresence of tricycles in moving people and goods. Considered as a\nhighly-urbanized city, General Santos City serves as vital link of the entire\nSOCKSARGEN region's economic activities. With the current thrust of the city in\nproviding a sustainable transport service, several options were identified to\nadopt in the entire city, that includes cleaner and better transport mode.\nElectric tricycle is an after sought alternative that offers better choice in\nterms of identified factors of sustainable transport: reliability, safety,\ncomfort, environment, affordability, and facility. A literature review was\nconducted to provide a comparison of cost and emission between a motorized\ntricycle and an e-tricycle. The study identified the existing tricycle industry\nof the city and reviewed the modal share with the city's travel pattern. The\nsurvey revealed a number of hazards were with the current motorized tricycle\nthat needs to address for the welfare of the passengers and drivers. The study\nfavors the shift to adopting E-tricycle. The model derived from binary\nlogistics regression provided a 72.72% model accuracy. Based from the results\nand findings, electric tricycle can be an alternative mode of public transport\nin the city that highly support sustainable option that provides local populace\nto improve their quality of life through mobility and economic activity.\nFurther recommendation to local policy makers in the transport sector of the\ncity include the clustering of barangays for better traffic management and\nfranchise regulation, the inclusion of transport-related infrastructure related\nto tricycle service with their investment planning and programming, the roll\nout and implementation of tricycle code of the city, and the piloting activity\nof introducing e-tricycle in the city.\n"
    },
    {
        "paper_id": 2104.08213,
        "authors": "Yafei Zhang, Lin Wang, Jonathan J. H. Zhu, Xiaofan Wang",
        "title": "The spatial dissemination of COVID-19 and associated socio-economic\n  consequences",
        "comments": "9 pages, 4 figures",
        "journal-ref": "J. R. Soc. Interface. 19 (2022) 20210662",
        "doi": "10.1098/rsif.2021.0662",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ongoing coronavirus disease 2019 (COVID-19) pandemic has wreaked havoc\nworldwide with millions of lives claimed, human travel restricted and economic\ndevelopment halted. Leveraging city-level mobility and case data, our analysis\nshows that the spatial dissemination of COVID-19 can be well explained by a\nlocal diffusion process in the mobility network rather than a global diffusion\nprocess, indicating the effectiveness of the implemented disease prevention and\ncontrol measures. Based on the constructed case prediction model, it is\nestimated that there could be distinct social consequences if the COVID-19\noutbreak happened in different areas. During the epidemic control period, human\nmobility experienced substantial reductions and the mobility network underwent\nremarkable local and global structural changes toward containing the spread of\nCOVID-19. Our work has important implications for the mitigation of disease and\nthe evaluation of the socio-economic consequences of COVID-19 on society.\n"
    },
    {
        "paper_id": 2104.08276,
        "authors": "Sareesh Rawat",
        "title": "Improving Transparency in IDIQ Contracts: A Comparison of Common\n  Procurement Issues Affecting Economies Across the Atlantic and Suggested\n  Solutions",
        "comments": "20 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Expansions in the size and scope of public procurement across the Atlantic\nhave increased calls for accountability of democratic governments. Indefinite\nDelivery, Indefinite Quantity contracts by their very nature are less\ntransparent but serve as major tools of public procurement in both the European\nand American economies. This paper utilizes a cross-Atlantic perspective to\ndiscuss common challenges faced by governments and contracting entities while\nhighlighting the need for balancing transparency with efficiency to avoid\nnegative economic outcomes. It concludes by discussing and providing potential\nsolutions to certain common challenges.\n"
    },
    {
        "paper_id": 2104.08342,
        "authors": "Katsumasa Tanaka, Christian Azar, Olivier Boucher, Philippe Ciais,\n  Yann Gaucher, Daniel J. A. Johansson",
        "title": "Paris Agreement requires substantial, broad, and sustained engagements\n  beyond COVID-19 public stimulus packages",
        "comments": null,
        "journal-ref": "Climatic Change 172, 1 (2022)",
        "doi": "10.1007/s10584-022-03355-6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It has been claimed that COVID-19 public stimulus packages could be\nsufficient to meet the short-term energy investment needs to leverage a shift\ntoward a pathway consistent with the 1.5 degrees C target of the Paris\nAgreement. Here we provide complementary perspectives to reiterate that\nsubstantial, broad, and sustained engagements beyond stimulus packages will be\nneeded for achieving the Paris Agreement long-term targets. Low-carbon\ninvestments will need to scale up and persist over the next several decades\nfollowing short-term stimulus packages. The required total energy investments\nin the real world can be larger than the currently available estimates from\nIntegrated Assessment Models (IAMs). Existing databases from IAMs are not\nsufficient for analyzing the effect of public spending on emission reduction.\nTo inform what role COVID-19 stimulus packages and public investments may play\nfor reaching the Paris Agreement targets, explicit modelling of such policies\nis required.\n"
    },
    {
        "paper_id": 2104.08502,
        "authors": "Cheng Cai, Tiziano De Angelis, Jan Palczewski",
        "title": "The American put with finite-time maturity and stochastic interest rate",
        "comments": "Corrections in proofs of Propositions 3.3 and 3.11",
        "journal-ref": null,
        "doi": "10.1111/mafi.12361",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study pricing of American put options on the Black and\nScholes market with a stochastic interest rate and finite-time maturity. We\nprove that the option value is a $C^1$ function of the initial time, interest\nrate and stock price. By means of Ito calculus we rigorously derive the option\nvalue's early exercise premium formula and the associated hedging portfolio. We\nprove the existence of an optimal exercise boundary splitting the state space\ninto continuation and stopping region. The boundary has a parametrisation as a\njointly continuous function of time and stock price, and it is the unique\nsolution to an integral equation which we compute numerically. Our results hold\nfor a large class of interest rate models including CIR and Vasicek models. We\nshow a numerical study of the option price and the optimal exercise boundary\nfor Vasicek model.\n"
    },
    {
        "paper_id": 2104.08605,
        "authors": "Sangita Das and Suchandan Kayal",
        "title": "Ordering results between the largest claims arising from two general\n  heterogeneous portfolios",
        "comments": "22 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is entirely devoted to compare the largest claims from two\nheterogeneous portfolios. It is assumed that the claim amounts in an insurance\nportfolio are nonnegative absolutely continuous random variables and belong to\na general family of distributions. The largest claims have been compared based\non various stochastic orderings. The established sufficient conditions are\nassociated with the matrices and vectors of model parameters. Applications of\nthe results are provided for the purpose of illustration.\n"
    },
    {
        "paper_id": 2104.08686,
        "authors": "Jaehyuk Choi, Minsuk Kwak, Chyng Wen Tee, Yumeng Wang",
        "title": "A Black-Scholes user's guide to the Bachelier model",
        "comments": null,
        "journal-ref": "Journal of Futures Markets, 42(5):959-980, 2022",
        "doi": "10.1002/fut.22315",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To cope with the negative oil futures price caused by the COVID-19 recession,\nglobal commodity futures exchanges temporarily switched the option model from\nBlack--Scholes to Bachelier in 2020. This study reviews the literature on\nBachelier's pioneering option pricing model and summarizes the practical\nresults on volatility conversion, risk management, stochastic volatility, and\nbarrier options pricing to facilitate the model transition. In particular,\nusing the displaced Black-Scholes model as a model family with the\nBlack-Scholes and Bachelier models as special cases, we not only connect the\ntwo models but also present a continuous spectrum of model choices.\n"
    },
    {
        "paper_id": 2104.08956,
        "authors": "Frank Bosserhoff, An Chen, Nils Sorensen, Mitja Stadje",
        "title": "On the Investment Strategies in Occupational Pension Plans",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Demographic changes increase the necessity to base the pension system more\nand more on the second and the third pillar, namely the occupational and\nprivate pension plans; this paper deals with Target Date Funds (TDFs), which\nare a typical investment opportunity for occupational pension planners. TDFs\nare usually identified with a decreasing fraction of wealth invested in equity\n(a so-called glide path) as retirement comes closer, i.e., wealth is invested\nmore risky the younger the saver is. We investigate whether this is actually\noptimal in the presence of non-tradable income risk in a stochastic volatility\nenvironment. The retirement planning procedure is formulated as a stochastic\noptimization problem. We find it is the (random) contributions that induce the\noptimal path exhibiting a glide path structure, both in the constant and\nstochastic volatility environment. Moreover, the initial wealth and the initial\ncontribution made to a retirement account strongly influence the fractional\namount of wealth to be invested in risky assets. The risk aversion of an\nindividual mainly determines the steepness of the glide path.\n"
    },
    {
        "paper_id": 2104.08975,
        "authors": "Fabio Vanni and David Lambert",
        "title": "On the regularity of human mobility patterns at times of a pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study of human mobility patterns is a crucially important research field\nfor its impact on several socio-economic aspects and, in particular, the\nmeasure of regularity patters of human mobility can provide a across-the-board\nview of many social distancing variables in epidemics such as: human movement\ntrends, physical interpersonal distances and population density. We will show\nthat the notion of information entropy is also strongly related to demographic\nand economic trends by the use and analysis of real-time data. In the present\nresearch paper we address three different problems. First, we provide an\nevidence-based analytical approach which relates the human mobility patterns,\nsocial distancing attitudes and population density, with entropic measures\nwhich depict for erraticity of human contact behaviors. Second, we investigate\nthe correlations between the aggregated mobility and entropic measures versus\nfive external economic indicators. Finally,we show how entropic measures\nrepresents a useful tool for testing the limitations of typical assumptions in\nepidemiological and mobility models.\n"
    },
    {
        "paper_id": 2104.09141,
        "authors": "Anna Naszodi",
        "title": "Decomposition scheme matters more than you may think",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper promotes the application of a path-independent decomposition\nscheme. Besides presenting some theoretical arguments supporting this\ndecomposition scheme, this study also illustrates the difference between the\npath-independent decomposition scheme and a popular sequential decomposition\nwith an empirical application of the two schemes. The empirical application is\nabout identifying a directly unobservable phenomenon, i.e. the changing social\ngap between people from different educational strata, through its effect on\nmarriages and cohabitations. It exploits census data from four waves between\n1977 and 2011 about the American, French, Hungarian, Portuguese, and Romanian\nsocieties. For some societies and periods, the outcome of the decomposition is\nfound to be highly sensitive to the choice of the decomposition scheme. These\nexamples illustrate the point that a careful selection of the decomposition\nscheme is crucial for adequately documenting the dynamics of unobservable\nfactors.\n"
    },
    {
        "paper_id": 2104.0921,
        "authors": "Renata Gomes Alcoforado and Alfredo D. Eg\\'idio dos Reis",
        "title": "A public micro pension programme in Brazil: Heterogeneity among states\n  and setting up of benefit age adjustment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Brazil is the 5th largest country in the world, despite of having a ``High\nHuman Development'' it is the 9th most unequal country. The existing Brazilian\nmicro pension programme is one of the safety nets for poor people. To become\neligible for this benefit, each person must have an income that is less than a\nquarter of the Brazilian minimum monthly wage and be either over 65 or\nconsidered disabled. That minimum income corresponds to approximately $2$\ndollars per day. This paper analyses quantitatively some aspects of this\nprogramme in the Public Pension System of Brazil. We look for the impact of\nsome particular economic variables on the number of people receiving the\nbenefit, and seek if that impact significantly differs among the 27 Brazilian\nFederal Units. We search for heterogeneity. We perform regression and spatial\ncluster analysis for detection of geographical grouping. We use a database that\nincludes the entire population that receives the benefit. Afterwards, we\ncalculate the amount that the system spends with the beneficiaries, estimate\nvalues \\textit{per capita} and the weight of each UF, searching for\nheterogeneity reflected on the amount spent \\textit{per capita}. In this latter\ncalculation we use a more comprehensive database, by individual, that includes\nall people that started receiving a benefit under the programme in the period\nfrom 2nd of January 2018 to 6th of April 2018. We compute the expected\ndiscounted benefit and confirm a high heterogeneity among UF's as well as\ngender. We propose achieving a more equitable system by introducing `age\nadjusting factors' to change the benefit age.\n"
    },
    {
        "paper_id": 2104.09309,
        "authors": "Juan Camilo Henao Londono and Thomas Guhr",
        "title": "Foreign exchange markets: price response and spread impact",
        "comments": "12 pages, 3 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:2010.15105",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2021.126587",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We carry out a detailed large-scale data analysis of price response functions\nin the spot foreign exchange market for different years and different time\nscales. Such response functions provide quantitative information on the\ndeviation from Markovian behavior. The price response functions show an\nincrease to a maximum followed by a slow decrease as the time lag grows, in\ntrade time scale and in physical time scale, for all analyzed years.\nFurthermore, we use a price increment point (pip) bid-ask spread definition to\ngroup different foreign exchange pairs and analyze the impact of the spread in\nthe price response functions. We find that large pip spreads have a stronger\nimpact on the response. This is similar to what has been found in stock\nmarkets.\n"
    },
    {
        "paper_id": 2104.09341,
        "authors": "Ekaterina Zolotareva",
        "title": "Aiding Long-Term Investment Decisions with XGBoost Machine Learning\n  Model",
        "comments": "29 pages, 10 figures,16 tables. This paper is the full text of the\n  research, presented at the 20th International Conference on Artificial\n  Intelligence and Soft Computing Web System (ICAISC 2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The ability to identify stock market trends has obvious advantages for\ninvestors. Buying stock on an upward trend (as well as selling it in case of\ndownward movement) results in profit. Accordingly, the start and end-points of\nthe trend are the optimal points for entering and leaving the market. The\nresearch concentrates on recognizing stock market long-term upward and downward\ntrends. The key results are obtained with the use of gradient boosting\nalgorithms, XGBoost in particular. The raw data is represented by time series\nwith basic stock market quotes with periods labelled by experts as Trend or\nFlat. The features are then obtained via various data transformations, aiming\nto catch implicit factors resulting in a change of stock direction. Modelling\nis done in two stages: stage one aims to detect endpoints of tendencies (i.e.\nsliding windows), stage two recognizes the tendency itself inside the window.\nThe research addresses such issues as imbalanced datasets and contradicting\nlabels, as well as the need for specific quality metrics to keep up with\npractical applicability. The model can be used to design an investment strategy\nthough further research in feature engineering and fine calibration is\nrequired.This paper is the full text of the research, presented at the 20th\nInternational Conference on Artificial Intelligence and Soft Computing Web\nSystem (ICAISC 2021)\n"
    },
    {
        "paper_id": 2104.09368,
        "authors": "Mingli Chen, Andreas Joseph, Michael Kumhof, Xinlei Pan, Xuan Zhou",
        "title": "Deep Reinforcement Learning in a Monetary Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose using deep reinforcement learning to solve dynamic stochastic\ngeneral equilibrium models. Agents are represented by deep artificial neural\nnetworks and learn to solve their dynamic optimisation problem by interacting\nwith the model environment, of which they have no a priori knowledge. Deep\nreinforcement learning offers a flexible yet principled way to model bounded\nrationality within this general class of models. We apply our proposed approach\nto a classical model from the adaptive learning literature in macroeconomics\nwhich looks at the interaction of monetary and fiscal policy. We find that,\ncontrary to adaptive learning, the artificially intelligent household can solve\nthe model in all policy regimes.\n"
    },
    {
        "paper_id": 2104.09471,
        "authors": "Julian D. Cortes",
        "title": "Dissension or consensus? Management and Business Research in Latin\n  America and the Caribbean",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study presents longitudinal evidence on the dissension of Management and\nBusiness Research (MBR) in Latin America and the Caribbean (LAC). It looks\nafter intellectual bridges linking clusters among such dissension. It was\nimplemented a coword network analysis to a sample of 12,000+ articles published\nby authors from LAC during 1998-2017. Structural network scores showed an\nincreasing number of keywords and mean degree but decreasing modularity and\ndensity. The intellectual bridges were those of the cluster formed by\ndisciplines/fields that tend toward consensus (e.g., mathematical models) and\nnot by core MBR subjects (e.g., strategic planning).\n"
    },
    {
        "paper_id": 2104.09476,
        "authors": "Damiano Brigo, Xiaoshan Huang, Andrea Pallavicini, Haitz Saez de\n  Ocariz Borde",
        "title": "Interpretability in deep learning for finance: a case study for the\n  Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep learning is a powerful tool whose applications in quantitative finance\nare growing every day. Yet, artificial neural networks behave as black boxes\nand this hinders validation and accountability processes. Being able to\ninterpret the inner functioning and the input-output relationship of these\nnetworks has become key for the acceptance of such tools. In this paper we\nfocus on the calibration process of a stochastic volatility model, a subject\nrecently tackled by deep learning algorithms. We analyze the Heston model in\nparticular, as this model's properties are well known, resulting in an ideal\nbenchmark case. We investigate the capability of local strategies and global\nstrategies coming from cooperative game theory to explain the trained neural\nnetworks, and we find that global strategies such as Shapley values can be\neffectively used in practice. Our analysis also highlights that Shapley values\nmay help choose the network architecture, as we find that fully-connected\nneural networks perform better than convolutional neural networks in predicting\nand interpreting the Heston model prices to parameters relationship.\n"
    },
    {
        "paper_id": 2104.097,
        "authors": "Mingwen Liu, Junbang Huo, Yulin Wu, Jinge Wu",
        "title": "Stock Market Trend Analysis Using Hidden Markov Model and Long Short\n  Term Memory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper intends to apply the Hidden Markov Model into stock market and and\nmake predictions. Moreover, four different methods of improvement, which are\nGMM-HMM, XGB-HMM, GMM-HMM+LSTM and XGB-HMM+LSTM, will be discussed later with\nthe results of experiment respectively. After that we will analyze the pros and\ncons of different models. And finally, one of the best will be used into stock\nmarket for timing strategy.\n"
    },
    {
        "paper_id": 2104.09863,
        "authors": "Ivan Jericevich and Murray McKechnie and Tim Gebbie",
        "title": "Calibrating an adaptive Farmer-Joshi agent-based model for financial\n  markets",
        "comments": "9 pages, 13 figures",
        "journal-ref": null,
        "doi": "10.25375/uct.11636922.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We replicate the contested calibration of the Farmer and Joshi agent based\nmodel of financial markets using a genetic algorithm and a Nelder-Mead with\nthreshold accepting algorithm following Fabretti. The novelty of the\nFarmer-Joshi model is that the dynamics are driven by trade entry and exit\nthresholds alone. We recover the known claim that some important stylized facts\nobserved in financial markets cannot be easily found under calibration -- in\nparticular those relating to the auto-correlations in the absolute values of\nthe price fluctuations, and sufficient kurtosis. However, rather than concerns\nrelating to the calibration method, what is novel here is that we extended the\nFarmer-Joshi model to include agent adaptation using an Brock and Hommes\napproach to strategy fitness based on trading strategy profitability. We call\nthis an adaptive Farmer-Joshi model: the model allows trading agents to switch\nbetween strategies by favouring strategies that have been more profitable over\nsome period of time determined by a free-parameter fixing the profit monitoring\ntime-horizon. In the adaptive model we are able to calibrate and recover\nadditional stylized facts, despite apparent degeneracy's. This is achieved by\ncombining the interactions of trade entry levels with trade strategy switching.\nWe use this to argue that for low-frequency trading across days, as calibrated\nto daily sampled data, feed-backs can be accounted for by strategy die-out\nbased on intermediate term profitability; we find that the average trade\nmonitoring horizon is approximately two to three months (or 40 to 60 days) of\ntrading.\n"
    },
    {
        "paper_id": 2104.09879,
        "authors": "Hibiki Kaibuchi, Yoshinori Kawasaki and Gilles Stupfler",
        "title": "GARCH-UGH: A bias-reduced approach for dynamic extreme Value-at-Risk\n  estimation in financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Value-at-Risk (VaR) is a widely used instrument in financial risk\nmanagement. The question of estimating the VaR of loss return distributions at\nextreme levels is an important question in financial applications, both from\noperational and regulatory perspectives; in particular, the dynamic estimation\nof extreme VaR given the recent past has received substantial attention. We\npropose here a two-step bias-reduced estimation methodology called GARCH-UGH\n(Unbiased Gomes-de Haan), whereby financial returns are first filtered using an\nAR-GARCH model, and then a bias-reduced estimator of extreme quantiles is\napplied to the standardized residuals to estimate one-step ahead dynamic\nextreme VaR. Our results indicate that the GARCH-UGH estimates are more\naccurate than those obtained by combining conventional AR-GARCH filtering and\nextreme value estimates from the perspective of in-sample and out-of-sample\nbacktestings of historical daily returns on several financial time series.\n"
    },
    {
        "paper_id": 2104.09898,
        "authors": "Saurab Chhachhi, Fei Teng",
        "title": "Market Value of Differentially-Private Smart Meter Data",
        "comments": "5 pages, 4 figures, submitted to the 2021 IEEE Power & Energy Society\n  Innovative Smart Grid Technologies Conference (ISGT NA)",
        "journal-ref": null,
        "doi": "10.1109/ISGT49243.2021.9372228",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes a framework to investigate the value of sharing\nprivacy-protected smart meter data between domestic consumers and load serving\nentities. The framework consists of a discounted differential privacy model to\nensure individuals cannot be identified from aggregated data, a ANN-based\nshort-term load forecasting to quantify the impact of data availability and\nprivacy protection on the forecasting error and an optimal procurement problem\nin day-ahead and balancing markets to assess the market value of the\nprivacy-utility trade-off. The framework demonstrates that when the load\nprofile of a consumer group differs from the system average, which is\nquantified using the Kullback-Leibler divergence, there is significant value in\nsharing smart meter data while retaining individual consumer privacy.\n"
    },
    {
        "paper_id": 2104.09988,
        "authors": "P. Murialdo, L. Ponta and A. Carbone",
        "title": "Inferring Multi-Period Optimal Portfolios via Detrending Moving Average\n  Cluster Entropy",
        "comments": "7 pages, 5 figures",
        "journal-ref": "EPL 133 60004 (2021)",
        "doi": "10.1209/0295-5075/133/60004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite half a century of research, there is still no general agreement about\nthe optimal approach to build a robust multi-period portfolio. We address this\nquestion by proposing the detrended cluster entropy approach to estimate the\nportfolio weights of high-frequency market indices. The information measure\nproduces reliable estimates of the portfolio weights gathered from the\nreal-world market data at varying temporal horizons. The portfolio exhibits a\nhigh level of diversity, robustness and stability as it is not affected by the\ndrawbacks of traditional mean-variance approaches.\n"
    },
    {
        "paper_id": 2104.10163,
        "authors": "Jean-Christophe Breton, Youssef El-Khatib, Jun Fan, Nicolas Privault",
        "title": "A q-binomial extension of the CRR asset pricing model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an extension of the Cox-Ross-Rubinstein (CRR) model based on\n$q$-binomial (or Kemp) random walks, with application to default with logistic\nfailure rates. This model allows us to consider time-dependent switching\nprobabilities varying according to a trend parameter on a non-self-similar\nbinomial tree. In particular, it includes tilt and stretch parameters that\ncontrol increment sizes. Option pricing formulas are written using $q$-binomial\ncoefficients, and we study the convergence of this model to a Black-Scholes\ntype formula in continuous time. A convergence rate of order $O(N^{-1/2})$ is\nobtained.\n"
    },
    {
        "paper_id": 2104.10187,
        "authors": "Mauricio Contreras G. and Roberto Ortiz H",
        "title": "Three little arbitrage theorems",
        "comments": "13pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We prove three theorems about the exact solutions of a generalized or\ninteracting Black-Scholes equation that explicitly includes arbitrage bubbles.\nThese arbitrage bubbles can be characterized by an arbitrage number $A_N(T)$.\nThe first theorem states that if $A_N(T) = 0$, then the solution at the\nmaturity of the interacting equation is identical to the solution of the free\nBlack-Scholes equation with the same initial interest rate $r$. The second\ntheorem states that if $A_N(T) \\ne 0$, the solution can be expressed in terms\nof all higher derivatives of solutions to the free Black-Scholes equation with\nthe initial interest rate $r$. The third theorem states that whatever the\narbitrage number is, the solution is a solution to the free Black-Scholes\nequation with a variable interest rate $r(\\tau) = r + (1/\\tau) A_N(\\tau)$.\nAlso, we show, by using the Feynman-Kac theorem, that for the special case of a\nCall contract, the exact solution for a Call with strike price $K$ is\nequivalent to the usual Call solution to the Black-Scholes equation with strike\nprice $\\tilde{K} = K e^{-A_N(T)}$.\n"
    },
    {
        "paper_id": 2104.10279,
        "authors": "Felber Arroyave, Oscar Yandy Romero Goyeneche, Meredith Gore, Gaston\n  Heimeriks, Jeffrey Jenkins, Alexander Petersen",
        "title": "On the social and cognitive dimensions of wicked environmental problems\n  characterized by conceptual and solution uncertainty",
        "comments": null,
        "journal-ref": "Advances in Complex Systems 24, 215005 (2021)",
        "doi": "10.1142/S0219525921500053",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a quantitative framework for understanding the class of wicked\nproblems that emerge at the intersections of natural, social, and technological\ncomplex systems. Wicked problems reflect our incomplete understanding of\ninterdependent global systems and the systemic risk they pose; such problems\nescape solutions because they are often ill-defined, and thus mis-identified\nand under-appreciated by communities of problem-solvers. While there are\nwell-documented benefits to tackling boundary-crossing problems from various\nviewpoints, the integration of diverse approaches can nevertheless contribute\nconfusion around the collective understanding of the core concepts and feasible\nsolutions. We explore this paradox by analyzing the development of both\nscholarly (social) and topical (cognitive) communities -- two facets of\nknowledge production studies here that contribute towards the evolution of\nknowledge in and around a problem, termed a knowledge trajectory -- associated\nwith three wicked problems: deforestation, invasive species, and wildlife\ntrade. We posit that saturation in the dynamics of social and cognitive\ndiversity growth is an indicator of reduced uncertainty in the evolution of the\ncomprehensive knowledge trajectory emerging around each wicked problem.\nInformed by comprehensive bibliometric data capturing both social and cognitive\ndimensions of each problem domain, we thereby develop a framework that assesses\nthe stability of knowledge trajectory dynamics as an indicator of wickedness\nassociated with conceptual and solution uncertainty. As such, our results\nidentify wildlife trade as a wicked problem that may be difficult to address\ngiven recent instability in its knowledge trajectory.\n"
    },
    {
        "paper_id": 2104.10281,
        "authors": "Diego Alejandro Murillo Taborda",
        "title": "Nonlinear Pricing with Misspecified and Arbitrary Perception of the\n  Marginal Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the context of nonlinear prices, the empirical evidence suggests that the\nconsumers have cognitive biases represented in a limited understanding of\nnonlinear price structures, and they respond to some alternative perceptions of\nthe marginal prices. In particular, consumers usually make choices based more\non the average than the marginal prices, which can result in a suboptimal\nbehavior. Taking the misspecification in the marginal price as exogenous, this\ndocument analyzes how is the optimal quadratic price scheme for a monopolist\nand the optimal quadratic price scheme that maximizes welfare when there is a\ncontinuum of representative consumers with an arbitrary perception of the\nmarginal price. Under simple preferences and costs functional forms and very\nstraightforward hypotheses, the results suggest that the bias in the marginal\nprice doesn't affect the maximum welfare attainable with quadratic price\nschemes, and it has a negligible effect over the efficiency cost caused by the\nmonopolist, so the misspecification in the marginal price is not relevant for\nwelfare increasing policies. However, almost always the misspecification in the\nmarginal price is beneficial for the monopolist. An interesting result is that\nunder these functional forms, more misspecification in the marginal price is\nbeneficial for both the consumers and the monopolist if the level of bias is\nlow, so not always is socially optima to have educated consumers about the real\nmarginal price. Finally, the document shows that the bias in the marginal price\nhas a negative effect on reduction of aggregate consumption using two-tier\nincreasing tariffs, which are commonly used for reduction of aggregate\nconsumption.\n"
    },
    {
        "paper_id": 2104.10365,
        "authors": "Christiern Rose and Lizi Yu",
        "title": "Identification of Peer Effects with Miss-specified Peer Groups: Missing\n  Data and Group Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider identification of peer effects under peer group\nmiss-specification. Two leading cases are missing data and peer group\nuncertainty. Missing data can take the form of some individuals being entirely\nabsent from the data. The researcher need not have any information on missing\nindividuals and need not even know that they are missing. We show that peer\neffects are nevertheless identifiable under mild restrictions on the\nprobabilities of observing individuals, and propose a GMM estimator to estimate\nthe peer effects. In practice this means that the researcher need only have\naccess to an individual level sample with group identifiers. Group uncertainty\narises when the relevant peer group for the outcome under study is unknown. We\nshow that peer effects are nevertheless identifiable if the candidate groups\nare nested within one another and propose a non-linear least squares estimator.\nWe conduct a Monte-Carlo experiment to demonstrate our identification results\nand the performance of the proposed estimators, and apply our method to study\npeer effects in the career decisions of junior lawyers.\n"
    },
    {
        "paper_id": 2104.10457,
        "authors": "Sonja Tilly, Giacomo Livan",
        "title": "Macroeconomic forecasting with statistically validated knowledge graphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study leverages narrative from global newspapers to construct\ntheme-based knowledge graphs about world events, demonstrating that features\nextracted from such graphs improve forecasts of industrial production in three\nlarge economies compared to a number of benchmarks. Our analysis relies on a\nfiltering methodology that extracts \"backbones\" of statistically significant\nedges from large graph data sets. We find that changes in the eigenvector\ncentrality of nodes in such backbones capture shifts in relative importance\nbetween different themes significantly better than graph similarity measures.\nWe supplement our results with an interpretability analysis, showing that the\ntheme categories \"disease\" and \"economic\" have the strongest predictive power\nduring the time period that we consider. Our work serves as a blueprint for the\nconstruction of parsimonious - yet informative - theme-based knowledge graphs\nto monitor in real time the evolution of relevant phenomena in socio-economic\nsystems.\n"
    },
    {
        "paper_id": 2104.10483,
        "authors": "Eric Benhamou and David Saltiel and Serge Tabachnik and Sui Kai Wong\n  and Fran\\c{c}ois Chareyron",
        "title": "Adaptive learning for financial markets mixing model-based and\n  model-free RL for volatility targeting",
        "comments": "8 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Model-Free Reinforcement Learning has achieved meaningful results in stable\nenvironments but, to this day, it remains problematic in regime changing\nenvironments like financial markets. In contrast, model-based RL is able to\ncapture some fundamental and dynamical concepts of the environment but suffer\nfrom cognitive bias. In this work, we propose to combine the best of the two\ntechniques by selecting various model-based approaches thanks to Model-Free\nDeep Reinforcement Learning. Using not only past performance and volatility, we\ninclude additional contextual information such as macro and risk appetite\nsignals to account for implicit regime changes. We also adapt traditional RL\nmethods to real-life situations by considering only past data for the training\nsets. Hence, we cannot use future information in our training data set as\nimplied by K-fold cross validation. Building on traditional statistical\nmethods, we use the traditional \"walk-forward analysis\", which is defined by\nsuccessive training and testing based on expanding periods, to assert the\nrobustness of the resulting agent.\n  Finally, we present the concept of statistical difference's significance\nbased on a two-tailed T-test, to highlight the ways in which our models differ\nfrom more traditional ones. Our experimental results show that our approach\noutperforms traditional financial baseline portfolio models such as the\nMarkowitz model in almost all evaluation metrics commonly used in financial\nmathematics, namely net performance, Sharpe and Sortino ratios, maximum\ndrawdown, maximum drawdown over volatility.\n"
    },
    {
        "paper_id": 2104.10657,
        "authors": "Lin Hu, Anqi Li, and Xu Tan",
        "title": "A Rational Inattention Theory of Echo Chamber",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a rational inattention theory of echo chambers, where players\nallocate limited attention across biased primary sources and other players to\ngather information about an uncertain state. The resulting Poisson attention\nnetwork transmits information from the primary source to a player either\ndirectly or indirectly through other players. Rational inattention creates\nheterogeneous information demands among players who are biased toward different\ndecisions. In an echo-chamber equilibrium, each player focuses on his\nown-biased source and like-minded friends, who attend to the same primary\nsource as his and can serve as secondary sources if the attention channel from\nthe primary source to him is disrupted. We establish conditions for the\nemergence of echo-chamber equilibria, characterize the attention networks\nwithin echo chambers, and offer insights for designing and regulating\ninformation platforms.\n"
    },
    {
        "paper_id": 2104.10673,
        "authors": "Tobias Fissler and Yannick Hoga",
        "title": "Backtesting Systemic Risk Forecasts using Multi-Objective Elicitability",
        "comments": "28 pages + 25 Appendix, 9 figures Structure improved; minor additions\n  and corrections",
        "journal-ref": "Journal of Business & Economic Statistics (2023)",
        "doi": "10.1080/07350015.2023.2200514",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Systemic risk measures such as CoVaR, CoES and MES are widely-used in\nfinance, macroeconomics and by regulatory bodies. Despite their importance, we\nshow that they fail to be elicitable and identifiable. This renders forecast\ncomparison and validation, commonly summarised as `backtesting', impossible.\nThe novel notion of \\emph{multi-objective elicitability} solves this problem.\nSpecifically, we propose Diebold--Mariano type tests utilising two-dimensional\nscores equipped with the lexicographic order. We illustrate the test decisions\nby an easy-to-apply traffic-light approach. We apply our traffic-light approach\nto DAX~30 and S\\&P~500 returns, and infer some recommendations for regulators.\n"
    },
    {
        "paper_id": 2104.10827,
        "authors": "Paul Bechly",
        "title": "An Examination of Demographic Differences in Obtaining Investment and\n  Financial Planning Information",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3369267",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial literacy and financial education are important components of modern\nlife. The importance of financial literacy is increasing for financial\nconsumers because of the weakening of both government and employer-based\nretirement systems. Unfortunately, empirical research shows that financial\nconsumers are not fully informed and are not able to make proper choices even\nwhen appropriate information is available. More research is needed as to how\nfinancial consumers obtain investment and financial planning information. A\nprimary data study was conducted to understand the differences between the\ndemographic categories of gender, age, education-level, and income-level with\nthe means of obtaining investment and financial planning information. In this\nresearch study, which selected a population from the LinkedIn platform,\nstatistical differences between gender, age, education-level, and income-level\nwere confirmed. These differences helped to confirm prior research in this\nfield of study. Practical opportunities for commercial outreach to specific\npopulations became evident through this type of research. Providers of\ninvestment and financial planning information can access their targeted\naudience more effectively by understanding the demographic profile of the\naudience, as well as the propensity of the demographic profile of the audience\nto respond. As this type of research is relatively easy to construct and\nadminister, commercial outreach for providers of investment and financial\nplanning information can be conducted in a cost-efficient and effective manner.\n"
    },
    {
        "paper_id": 2104.10877,
        "authors": "Takuji Arai",
        "title": "Approximate option pricing formula for Barndorff-Nielsen and Shephard\n  model",
        "comments": "arXiv admin note: text overlap with arXiv:2005.07393",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For the Barndorff-Nielsen and Shephard model, we present approximate\nexpressions of call option prices based on the decomposition formula developed\nby Arai (2021). Besides, some numerical experiments are also implemented to\nmake sure how effective our approximations are.\n"
    },
    {
        "paper_id": 2104.10973,
        "authors": "Sanmay Shelat, Oded Cats, Sander van Cranenburgh",
        "title": "Traveller behaviour in public transport in the early stages of the\n  COVID-19 pandemic in the Netherlands",
        "comments": null,
        "journal-ref": "Transp. Res. A: Policy Pract. 159 (2022) 357-371",
        "doi": "10.1016/j.tra.2022.03.027",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Public transport ridership around the world has been hit hard by the COVID-19\npandemic. Travellers are likely to adapt their behaviour to avoid the risk of\ntransmission and these changes may even be sustained after the pandemic. To\nevaluate travellers' behaviour in public transport networks during these times\nand assess how they will respond to future changes in the pandemic, we conduct\na stated choice experiment with train travellers in the Netherlands. We\nspecifically assess behaviour related to three criteria affecting the risk of\nCOVID-19 transmission: (i) crowding, (ii) exposure duration, and (iii)\nprevalent infection rate.\n  Observed choices are analysed using a latent class choice model which reveals\ntwo, nearly equally sized traveller segments: 'COVID Conscious' and 'Infection\nIndifferent'. The former has a significantly higher valuation of crowding,\naccepting, on average 8.75 minutes extra waiting time to reduce one person\non-board. Moreover, they demonstrate a strong desire to sit without anybody in\ntheir neighbouring seat and are quite sensitive to changes in the prevalent\ninfection rate. By contrast, Infection Indifferent travellers' value of\ncrowding (1.04 waiting time minutes/person) is only slightly higher than\npre-pandemic estimates and they are relatively unaffected by infection rates.\nWe find that older and female travellers are more likely to be COVD Conscious\nwhile those reporting to use the trains more frequently during the pandemic\ntend to be Infection Indifferent. Further analysis also reveals differences\nbetween the two segments in attitudes towards the pandemic and self-reported\nrule-following behaviour. The behavioural insights from this study will not\nonly contribute to better demand forecasting for service planning but will also\ninform public transport policy decisions aimed at curbing the shift to private\nmodes.\n"
    },
    {
        "paper_id": 2104.113,
        "authors": "Joshua Becker, Douglas Guilbeault, Ned Smith",
        "title": "The Crowd Classification Problem: Social Dynamics of Binary Choice\n  Accuracy",
        "comments": "41 pages incl. title and Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decades of research suggest that information exchange in groups and\norganizations can reliably improve judgment accuracy in tasks such as financial\nforecasting, market research, and medical decision-making. However, we show\nthat improving the accuracy of numeric estimates does not necessarily improve\nthe accuracy of decisions. For binary choice judgments, also known as\nclassification tasks--e.g. yes/no or build/buy decisions--social influence is\nmost likely to grow the majority vote share, regardless of the accuracy of that\nopinion. As a result, initially inaccurate groups become increasingly\ninaccurate after information exchange even as they signal stronger support. We\nterm this dynamic the \"crowd classification problem.\" Using both a novel\ndataset as well as a reanalysis of three previous datasets, we study this\nprocess in two types of information exchange: (1) when people share votes only,\nand (2) when people form and exchange numeric estimates prior to voting.\nSurprisingly, when people exchange numeric estimates prior to voting, the\nbinary choice vote can become less accurate even as the average numeric\nestimate becomes more accurate. Our findings recommend against voting as a form\nof decision-making when groups are optimizing for accuracy. For those cases\nwhere voting is required, we discuss strategies for managing communication to\navoid the crowd classification problem. We close with a discussion of how our\nresults contribute to a broader contingency theory of collective intelligence.\n"
    },
    {
        "paper_id": 2104.11461,
        "authors": "Darren Shannon, Grigorios Fountas",
        "title": "Extending the Heston Model to Forecast Motor Vehicle Collision Rates",
        "comments": "25 pages (excl. Ref, Appendices). 11 figures, 7 tables, 3 appendices",
        "journal-ref": "Shannon, D. and Fountas, G., 2021. Extending the Heston model to\n  forecast motor vehicle collision rates. Accident Analysis & Prevention, 159,\n  p.106250",
        "doi": "10.1016/j.aap.2021.106250",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We present an alternative approach to the forecasting of motor vehicle\ncollision rates. We adopt an oft-used tool in mathematical finance, the Heston\nStochastic Volatility model, to forecast the short-term and long-term evolution\nof motor vehicle collision rates. We incorporate a number of extensions to the\nHeston model to make it fit for modelling motor vehicle collision rates. We\nincorporate the temporally-unstable and non-deterministic nature of collision\nrate fluctuations, and introduce a parameter to account for periods of\naccelerated safety. We also adjust estimates to account for the seasonality of\ncollision patterns. Using these parameters, we perform a short-term forecast of\ncollision rates and explore a number of plausible scenarios using long-term\nforecasts. The short-term forecast shows a close affinity with realised rates\n(over 95% accuracy), and outperforms forecasting models currently used in road\nsafety research (Vasicek, SARIMA, SARIMA-GARCH). The long-term scenarios\nsuggest that modest targets to reduce collision rates (1.83% annually) and\ntargets to reduce the fluctuations of month-to-month collision rates (by half)\ncould have significant benefits for road safety. The median forecast in this\nscenario suggests a 50% fall in collision rates, with 75% of simulations\nsuggesting that an effective change in collision rates is observed before 2044.\nThe main benefit the model provides is eschewing the necessity for setting\nunreasonable safety targets that are often missed. Instead, the model presents\nthe effects that modest and achievable targets can have on road safety over the\nlong run, while incorporating random variability. Examining the parameters that\nunderlie expected collision rates will aid policymakers in determining the\neffectiveness of implemented policies.\n"
    },
    {
        "paper_id": 2104.11594,
        "authors": "Bahareh Afhami, Mohsen Rezapour, Mohsen Madadi, Vahed Maroufy",
        "title": "Dynamic investment portfolio optimization using a Multivariate Merton\n  Model with Correlated Jump Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we are concerned with the optimization of a dynamic investment\nportfolio when the securities which follow a multivariate Merton model with\ndependent jumps are periodically invested and proceed by approximating the\nCondition-Value-at-Risk (CVaR) by comonotonic bounds and maximize the expected\nterminal wealth. Numerical studies as well as applications of our results to\nreal datasets are also provided.\n"
    },
    {
        "paper_id": 2104.11595,
        "authors": "D\\'avid Zolt\\'an Szab\\'o and Diego Andr\\'es P\\'erez",
        "title": "Does home advantage without crowd exist in American football?",
        "comments": "16 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It is well-known that home team has an inherent advantage against visiting\nteams when playing team sports. One of the most obvious underlying reasons, the\npresence of supporting fans has mostly disappeared in major leagues with the\nemergence of COVID-19 pandemic. This paper investigates with the help of\nhistorical National Football League (NFL) data, how much effect spectators have\non the game outcome. Our findings reveal that under no allowance of spectators\nthe home teams' performance is substantially lower than under normal\ncircumstances, even performing slightly worse than the visiting teams. On the\nother hand, when a limited amount of spectators are allowed to the game, the\nhome teams' performance is no longer significantly different than what we\nobserve with full stadiums. This suggests that from a psychological point of\nview the effect of crowd support is already induced by a fraction of regular\nfans.\n"
    },
    {
        "paper_id": 2104.11652,
        "authors": "Mario A. Maggioni and Domenico Rossignoli",
        "title": "If it Looks like a Human and Speaks like a Human ... Dialogue and\n  cooperation in human-robot interactions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper presents the results of a behavioral experiment conducted between\nFebruary 2020 and March 2021 at Universit\\`a Cattolica del Sacro Cuore, Milan\nCampus in which students were matched to either a human or a humanoid robotic\npartner to play an iterated Prisoner's Dilemma. The results of a Logit\nestimation procedure show that subjects are more likely to cooperate with human\nrather robotic partners; that are more likely to cooperate after receiving a\ndialogic verbal reaction following the realization of a sub-obtimal social\noutcome; that the effect of the verbal reaction is independent on the nature of\nthe partner. Our findings provide new evidence on the effect of verbal\ncommunication in strategic frameworks. Results are robust to the exclusion of\nstudents of Economics related subjects, to the inclusion of a set of\npsychological and behavioral controls, to the way subjects perceive robots'\nbehavior and to potential gender biases in human-human interactions.\n"
    },
    {
        "paper_id": 2104.11684,
        "authors": "Silvia Lavagnini",
        "title": "Pricing Asian Options with Correlators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a series expansion by Hermite polynomials for the price of an\narithmetic Asian option. This series requires the computation of moments and\ncorrelators of the underlying price process, but for a polynomial\njump-diffusion, these are given in closed form, hence no numerical simulation\nis required to evaluate the series. This allows, for example, for the explicit\ncomputation of Greeks. The weight function defining the Hermite polynomials is\na Gaussian density with scale $b$. We find that the rate of convergence for the\nseries depends on $b$, for which we prove a lower bound to guarantee\nconvergence. Numerical examples show that the series expansion is accurate but\nunstable for initial values of the underlying process far from zero, mainly due\nto rounding errors.\n"
    },
    {
        "paper_id": 2104.11726,
        "authors": "Maria Tresita Paul V., N. Uma Devi",
        "title": "Managing mental & psychological wellbeing amidst COVID-19 pandemic:\n  Positive psychology interventions",
        "comments": "10 Pages, 3 Figures, 3 Tables",
        "journal-ref": "The American Journal of Humanities and Social Sciences Research,\n  2021, Vol. 4, Issue 3, Pages 121-131",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  COVID-19 pandemic has shaken the roots of healthcare facilities worldwide,\nwith the US being one of the most affected countries irrespective of being a\nsuperpower. Along with the current pandemic, COVID-19 can cause a secondary\ncrisis of mental health pandemic if left unignored. Various studies from past\nepidemics, financial turmoil and pandemic, especially SARS and MERS, have shown\na steep increase in mental and psychological issues like depression, low\nquality of life, self-harm and suicidal tendencies among general populations.\nThe most venerable being the individuals infected and cured due to social\ndiscrimination. The government is taking steps to contain and prevent further\ninfections of COVID-19. However, the mental and psychological wellbeing of\npeople is still left ignored in developing countries like India. There is a\nsignificant gap in India concerning mental and psychological health still being\nstigmatized and considered 'non-existent'. This study's effort is to highlight\nthe importance of mental and psychological health and to suggest interventions\nbased on positive psychology literature. These interventions can support the\nwellbeing of people acting as a psychological first aid. Keywords: COVID-19,\nCoronavirus, Pandemic, Mental wellbeing, Psychological Wellbeing, Positive\nPsychology Interventions.\n  KEYWORDS - COVID-19, Coronavirus, Pandemic, Wellbeing, Positive Psychology,\nInterventions, PPI.\n"
    },
    {
        "paper_id": 2104.11768,
        "authors": "Narayan Ganesan, Bernhard Hientzsch",
        "title": "Estimating Future VaR from Value Samples and Applications to Future\n  Initial Margin",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting future values at risk (fVaR) is an important problem in finance.\nThey arise in the modelling of future initial margin requirements for\ncounterparty credit risk and future market risk VaR. One is also interested in\nderived quantities such as: i) Dynamic Initial Margin (DIM) and Margin Value\nAdjustment (MVA) in the counterparty risk context; and ii) risk weighted assets\n(RWA) and Capital Value Adjustment (KVA) for market risk. This paper describes\nseveral methods that can be used to predict fVaRs. We begin with the Nested\nMC-empirical quantile method as benchmark, but it is too computationally\nintensive for routine use. We review several known methods and discuss their\nnovel applications to the problem at hand.\n  The techniques considered include computing percentiles from distributions\n(Normal and Johnson) that were matched to parametric moments or percentile\nestimates, quantile regressions methods, and others with more specific\nassumptions or requirements.\n  We also consider how limited inner simulations can be used to improve the\nperformance of these techniques. The paper also provides illustrations,\nresults, and visualizations of intermediate and final results for the various\napproaches and methods.\n"
    },
    {
        "paper_id": 2104.11772,
        "authors": "Luna Yue Huang, Solomon Hsiang, Marco Gonzalez-Navarro",
        "title": "Using Satellite Imagery and Deep Learning to Evaluate the Impact of\n  Anti-Poverty Programs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rigorous evaluation of anti-poverty programs is key to the fight against\nglobal poverty. Traditional evaluation approaches rely heavily on repeated\nin-person field surveys to measure changes in economic well-being and thus\nprogram effects. However, this is known to be costly, time-consuming, and often\nlogistically challenging. Here we provide the first evidence that we can\nconduct such program evaluations based solely on high-resolution satellite\nimagery and deep learning methods. Our application estimates changes in\nhousehold welfare in the context of a recent anti-poverty program in rural\nKenya. The approach we use is based on a large literature documenting a\nreliable relationship between housing quality and household wealth. We infer\nchanges in household wealth based on satellite-derived changes in housing\nquality and obtain consistent results with the traditional field-survey based\napproach. Our approach can be used to obtain inexpensive and timely insights on\nprogram effectiveness in international development programs.\n"
    },
    {
        "paper_id": 2104.11783,
        "authors": "Yanci Zhang, Tianming Du, Yujie Sun, Lawrence Donohue, Rui Dai",
        "title": "Form 10-Q Itemization",
        "comments": "6 pages, 3 figures, 3 tables, http://review10q.ddns.net/",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.\n"
    },
    {
        "paper_id": 2104.11863,
        "authors": "Zhibin Niu, Junqi Wu, Dawei Cheng, and Jiawan Zhang",
        "title": "Regshock: Interactive Visual Analytics of Systemic Risk in Financial\n  Networks",
        "comments": "9 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial regulatory agencies are struggling to manage the systemic risks\nattributed to negative economic shocks. Preventive interventions are prominent\nto eliminate the risks and help to build a more resilient financial system.\nAlthough tremendous efforts have been made to measure multi-risk severity\nlevels, understand the contagion behaviors and other risk management problems,\nthere still lacks a theoretical framework revealing what and how regulatory\nintervention measurements can mitigate systemic risk. Here we demonstrate\nregshock, a practical visual analytical approach to support the exploration and\nevaluation of financial regulation measurements. We propose risk-island, an\nunprecedented risk-centered visualization algorithm to help uncover the risk\npatterns while preserving the topology of financial networks. We further\npropose regshock, a novel visual exploration and assessment approach based on\nthe simulation-intervention-evaluation analysis loop, to provide a heuristic\nsurgical intervention capability for systemic risk mitigation. We evaluate our\napproach through extensive case studies and expert reviews. To our knowledge,\nthis is the first practical systemic method for the financial network\nintervention and risk mitigation problem; our validated approach potentially\nimproves the risk management and control capabilities of financial experts.\n"
    },
    {
        "paper_id": 2104.1187,
        "authors": "Li Chen and Guang Zhang",
        "title": "Hermite Polynomial-based Valuation of American Options with General\n  Jump-Diffusion Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new approximation scheme for the price and exercise policy of\nAmerican options. The scheme is based on Hermite polynomial expansions of the\ntransition density of the underlying asset dynamics and the early exercise\npremium representation of the American option price. The advantages of the\nproposed approach are threefold. First, our approach does not require the\ntransition density and characteristic functions of the underlying asset\ndynamics to be attainable in closed form. Second, our approach is fast and\naccurate, while the prices and exercise policy can be jointly produced. Third,\nour approach has a wide range of applications. We show that the proposed\napproximations of the price and optimal exercise boundary converge to the true\nones. We also provide a numerical method based on a step function to implement\nour proposed approach. Applications to nonlinear mean-reverting models, double\nmean-reverting models, Merton's and Kou's jump-diffusion models are presented\nand discussed.\n"
    },
    {
        "paper_id": 2104.11891,
        "authors": "Loretta Mastroeni, Alessandro Mazzoccoli, Greta Quaresima, Pierluigi\n  Vellucci",
        "title": "Wavelet analysis and energy-based measures for oil-food price\n  relationship as a footprint of financialisation effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we exploit the wavelet analysis approach to investigate\noil-food price correlation and its determinants in the domains of time and\nfrequency. Wavelet analysis is able to differentiate high frequency from low\nfrequency movements which correspond, respectively, to short and long run\ndynamics. We show that the significant local correlation between food and oil\nis only apparent and this is mainly due both to the activity of commodity index\ninvestments and, to a lesser extent, to a growing demand from emerging\neconomies. Moreover, the activity of commodity index investments gives evidence\nof the overall financialisation process. In addition, we employ wavelet entropy\nto assess the predictability of the time series under consideration at\ndifferent frequencies. We find that some variables share a similar\npredictability structure with food and oil. These variables are the ones that\nmove the most along with oil and food. We also introduce a novel measure, the\nCross Wavelet Energy Entropy Measure (CWEEM), based on wavelet transformation\nand information entropy, with the aim of quantifying the intrinsic\npredictability of food and oil given demand from emerging economies, commodity\nindex investments, financial stress, and global economic activity. The results\nshow that these dynamics are best predicted by global economic activity at all\nfrequencies and by demand from emerging economies and commodity index\ninvestments at high frequencies only.\n"
    },
    {
        "paper_id": 2104.12004,
        "authors": "Christopher Boudreaux, George Clarke, and Anand Jha",
        "title": "Social capital and small business productivity: The mediating roles of\n  financing and customer relationships",
        "comments": "45 pages, 4 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How does an entrepreneur's social capital improve small informal business\nproductivity? Although studies have investigated this relationship, we still\nknow little about the underlying theoretical mechanisms driving these findings.\nUsing a unique Zambian Business Survey of 1,971 entrepreneurs administered by\nthe World Bank, we find an entrepreneur's social capital facilitates small\nbusiness productivity through the mediating channels of firm financing and\ncustomer relationships. Our findings identify specific mechanisms that channel\nsocial capital toward an informal business' productivity, which prior studies\nhave overlooked.\n"
    },
    {
        "paper_id": 2104.12008,
        "authors": "Christopher Boudreaux, Anand Jha, and Monica Escaleras",
        "title": "Weathering the Storm: How Foreign Aid and Institutions Affect\n  Entrepreneurship Following Natural Disasters",
        "comments": "38 pages, 3 figures, 4 tables",
        "journal-ref": "Entrepreneurship Theory and Practice 2021",
        "doi": "10.1177/10422587211002185",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines how foreign aid and institutions affect entrepreneurship\nactivity following natural disasters. We use insights from the\nentrepreneurship, development, and institutions literature to develop a model\nof entrepreneurship activity in the aftermath of natural disasters. First, we\nhypothesize the effect of natural disasters on entrepreneurship activity\ndepends on the amount of foreign aid received. Second, we hypothesize that\nnatural disasters and foreign aid either encourages or discourages\nentrepreneurship activity depending on two important institutional conditions:\nthe quality of government and economic freedom. The findings from our panel of\n85 countries from 2006 to 2016 indicate that natural disasters are negatively\nassociated with entrepreneurship activity, but both foreign aid and economic\nfreedom attenuate this effect. In addition, we observe that foreign aid is\npositively associated with entrepreneurship activity but only in countries with\nhigh quality government. Hence, we conclude that the effect of natural\ndisasters on entrepreneurship depends crucially on the quality of government,\neconomic freedom, and foreign aid. Our findings provide new insights into how\nnatural disasters and foreign aid affect entrepreneurship and highlight the\nimportant role of the institutional context.\n"
    },
    {
        "paper_id": 2104.1221,
        "authors": "Haoyang Cao and Xin Guo",
        "title": "Generative Adversarial Network: Some Analytical Perspectives",
        "comments": "Contributed to the book, \"Machine Learning in Financial Markets: A\n  Guide to Contemporary Practice\", edited by Agostino Capponi and\n  Charles-Albert Lehalle",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ever since its debut, generative adversarial networks (GANs) have attracted\ntremendous amount of attention. Over the past years, different variations of\nGANs models have been developed and tailored to different applications in\npractice. Meanwhile, some issues regarding the performance and training of GANs\nhave been noticed and investigated from various theoretical perspectives. This\nsubchapter will start from an introduction of GANs from an analytical\nperspective, then move on to the training of GANs via SDE approximations and\nfinally discuss some applications of GANs in computing high dimensional MFGs as\nwell as tackling mathematical finance problems.\n"
    },
    {
        "paper_id": 2104.12215,
        "authors": "Shreya Biswas, Upasak Das",
        "title": "Whats the worth of a promise? Evaluating the indirect effects of a\n  program to reduce early marriage in India",
        "comments": "Education; labor participation; early marriage; Haryana",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  One important dimension of Conditional Cash Transfer Programs apart from\nconditionality is the provision of continuous frequency of payouts. On the\ncontrary, the Apni Beti Apna Dhan program, implemented in the state of Haryana\nin India from 1994 to 1998 offers a promised amount to female beneficiaries\nredeemable only after attaining 18 years of age if she remains unmarried. This\npaper assesses the impact of this long-term financial incentivization on\noutcomes, not directly associated with the conditionality. Using multiple\ndatasets in a triple difference framework, the findings reveal a significant\npositive impact on years of education though it does not translate into gains\nin labor participation. While gauging the potential channels, we did not\nobserve higher educational effects beyond secondary education. Additionally,\nimpact on time allocation for leisure, socialization or self-care, age of\nmarriage beyond 18 years, age at first birth, and post-marital empowerment\nindicators are found to be limited. These evidence indicate failure of the\nprogram in altering the prevailing gender norms despite improvements in\neducational outcomes. The paper recommends a set of complementary potential\npolicy instruments that include altering gender norms through behavioral\ninterventions skill development and incentives to encourage female work\nparticipation.\n"
    },
    {
        "paper_id": 2104.12387,
        "authors": "Aiwei Huang",
        "title": "To What Extent do Labor Market Outcomes respond to UI Extensions?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Unemployment benefits in the US were extended by up to 73 weeks during the\nGreat Recession. Equilibrium labor market theory indicates that extensions of\nbenefit duration impact not only search decisions by job seekers but also job\nvacancy creations by employers. Most of the literature focused on the former to\nshow partial equilibrium effect that increment of unemployment benefits\ndiscourage job search and lead to a rise in unemployment. To study the total\neffect of UI benefit extensions on unemployment, I follow border county\nidentification strategy, take advantage of quasi-differenced specification to\ncontrol for changes in future benefit policies, apply interactive fixed effects\nmodel to deal with unobserved shocks so as to obtain unbiased and consistent\nestimation. I find that benefit extensions have a statistically significant\npositive effect on unemployment, which is consistent with the results of\nprevailing literature.\n"
    },
    {
        "paper_id": 2104.12484,
        "authors": "Xin Zhang, Lan Wu, Zhixue Chen",
        "title": "Constructing long-short stock portfolio with a new listwise\n  learn-to-rank algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Factor strategies have gained growing popularity in industry with the fast\ndevelopment of machine learning. Usually, multi-factors are fed to an algorithm\nfor some cross-sectional return predictions, which are further used to\nconstruct a long-short portfolio. Instead of predicting the value of the stock\nreturn, emerging studies predict a ranked stock list using the mature\nlearn-to-rank technology. In this study, we propose a new listwise\nlearn-to-rank loss function which aims to emphasize both the top and the bottom\nof a rank list. Our loss function, motivated by the long-short strategy, is\nendogenously shift-invariant and can be viewed as a direct generalization of\nListMLE. Under different transformation functions, our loss can lead to\nconsistency with binary classification loss or permutation level 0-1 loss. A\nprobabilistic explanation for our model is also given as a generalized\nPlackett-Luce model. Based on a dataset of 68 factors in China A-share market\nfrom 2006 to 2019, our empirical study has demonstrated the strength of our\nmethod which achieves an out-of-sample annual return of 38% with the Sharpe\nratio being 2.\n"
    },
    {
        "paper_id": 2104.1264,
        "authors": "Jerome Faure, Lauriane Mouysset, Sabrina Gaba",
        "title": "Combining incentives for pollination with collective action to provide a\n  bundle of ecosystem services in farmland",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A polycentric approach to ecosystem service (ES) governance that combines\nindividual incentives for interdependent ES providers with collective action is\na promising lever to overcome the decline in ES and generate win-win solutions\nin agricultural landscapes. In this study, we explored the effectiveness of\nsuch an approach by focusing on incentives for managed pollination targeting\neither beekeepers or farmers who were either in communication with each other\nor not. We used a stylized bioeconomic model to simulate (i) the mutual\ninterdependency through pollination in intensive agricultural landscapes and\n(ii) the economic and ecological impacts of introducing two beekeeping\nsubsidies and one pesticide tax. The findings showed that incentives generated\na spillover effect, affecting not only targeted stakeholders but non-targeted\nstakeholders as well as the landscape, and that this effect was amplified by\ncommunication. However, none of the simulated types of polycentric ES\ngovernance proved sustainable overall: subsidies showed excellent economic but\nlow environmental performance, while the tax led to economic losses but was\nbeneficial for the landscape. Based on these results, we identified three\nconditions for sustainable ES governance based on communication between\nstakeholders and incentives: (i) strong mutual interdependency (i.e. few\nalternatives exist for stakeholders), (ii) the benefits of communication\noutweigh the costs, and (iii) the incentivized ES drivers are not detrimental\nto other ES. Further research is needed to systematize which combination of\nindividual payments and collaboration are sustainable in which conditions.\n"
    },
    {
        "paper_id": 2104.12706,
        "authors": "Felipe Avileis and Mindy Mallory",
        "title": "The Impact of Brazil on Global Grain Dynamics: A Study on Cross-Market\n  Volatility Spillovers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Brazil rose as a global powerhouse producer of soybeans and corn over the\npast 15 years has fundamentally changed global markets in these commodities.\nThis is arguably due to the development of varieties of soybean and corn\nadapted to climates within Brazil, allowing farmers to double-crop corn after\nsoybeans in the same year. Corn and soybean market participants increasingly\nlook to Brazil for fundamental price information, and studies have shown that\nthe two markets have become cointegrated. However little is known about how\nmuch volatility from each market spills over to the other. In this article we\nmeasure volatility spillover ratios between U.S. and Brazilian first crop corn,\nsecond crop corn, and soybeans. We find that linkages between the two countries\nincreased after double cropping corn after soybeans expanded, volatility\nspillover magnitudes expanded, and the direction of volatility spillovers\nflipped from U.S. volatility spilling over to Brazil before double cropping, to\nBrazil spilling over to U.S. after double cropping.\n"
    },
    {
        "paper_id": 2104.12707,
        "authors": "Anthony N. Rezitis and Gregor Kastner",
        "title": "On the joint volatility dynamics in dairy markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present study investigates the price (co)volatility of four dairy\ncommodities -- skim milk powder, whole milk powder, butter and cheddar cheese\n-- in three major dairy markets. It uses a multivariate factor stochastic\nvolatility model for estimating the time-varying covariance and correlation\nmatrices by imposing a low-dimensional latent dynamic factor structure. The\nempirical results support four factors representing the European Union and\nOceania dairy sectors as well as the milk powder markets. Factor volatilities\nand marginal posterior volatilities of each dairy commodity increase after the\n2006/07 global (food) crisis, which also coincides with the free trade\nagreements enacted from 2007 onwards and EU and US liberalization policy\nchanges. The model-implied correlation matrices show increasing dependence\nduring the second half of 2006, throughout the first half of 2007, as well as\nduring 2008 and 2014, which can be attributed to various regional agricultural\ndairy policies. Furthermore, in-sample value at risk measures (VaRs and CoVaRs)\nare provided for each dairy commodity under consideration.\n"
    },
    {
        "paper_id": 2104.1274,
        "authors": "Martin Herdegen, D\\\"orte Kreher",
        "title": "Bubbles in discrete time models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new definition of speculative bubbles in discrete-time models\nbased on the discounted stock price losing mass at some finite drop-down under\nan equivalent martingale measure. We provide equivalent probabilistic\ncharacterisations of this definition and give examples of discrete-time\nmartingales that are speculative bubbles and those that are not. In the\nMarkovian case, we provide sufficient analytic conditions for the presence of\nspeculative bubbles. We also show that the existence of speculative bubbles is\ndirectly linked to the existence of a non-trivial solution to a linear Volterra\nintegral equation of the second kind involving the Markov kernel. Finally, we\nshow that our definition of speculative bubbles in discrete time is consistent\nwith the strict local martingale definition of speculative bubbles in\ncontinuous time in the sense that a properly discretised strict local\nmartingale in continuous time is a speculative bubble in discrete time.\n"
    },
    {
        "paper_id": 2104.12902,
        "authors": "Guy Tchuente",
        "title": "Early Human Capital Accumulation and Decentralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decentralization is a centerpiece in Cameroonian's government institutions'\ndesign. This chapter elaborates a simple hierarchy model for the analysis of\nthe effects of power devolution. The model predicts overall positive effects of\ndecentralization with larger effects when the local authority processes useful\ninformation on how to better allocate the resources. The estimation of the\neffects of the 2010's power devolution to municipalities in Cameroon suggests a\npositive impact of decentralization on early human capital accumulation. The\nvalue added by decentralization is the same for Anglophone and Francophone\nmunicipalities; the effects of decentralization are larger for advanced levels\nof primary school.\n"
    },
    {
        "paper_id": 2104.12975,
        "authors": "Christopher G. Lamoureux and Huacheng Zhang",
        "title": "An Empirical Assessment of Characteristics and Optimal Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze characteristics' joint predictive information through the lens of\nout-of-sample power utility functions. Linking weights to characteristics to\nform optimal portfolios suffers from estimation error which we mitigate by\nmaximizing an in-sample loss function that is more concave than the utility\nfunction. While no single characteristic can be used to enhance utility by all\ninvestors, conditioning on momentum, size, and residual volatility produces\nportfolios with significantly higher certainty equivalents than benchmarks for\nall investors. Characteristic complementarities produce the benefits, for\nexample momentum mitigates overfitting inherent in other characteristics.\nOptimal portfolios' returns lie largely outside the span of traditional\nfactors.\n"
    },
    {
        "paper_id": 2104.13159,
        "authors": "Vasudha Jain and Mark Whitmeyer",
        "title": "Search and Competition with Flexible Investigations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We modify the standard model of price competition with horizontally\ndifferentiated products, imperfect information, and search frictions by\nallowing consumers to flexibly acquire information about a product's match\nvalue during their visits. We characterize a consumer's optimal search and\ninformation acquisition protocol and analyze the pricing game between firms.\nNotably, we establish that in search markets there are fundamental differences\nbetween search frictions and information frictions, which affect market prices,\nprofits, and consumer welfare in markedly different ways. Although higher\nsearch costs beget higher prices (and profits for firms), higher information\nacquisition costs lead to lower prices and may benefit consumers. We discuss\nimplications of our findings for policies concerning disclosure rules and\nhidden fees.\n"
    },
    {
        "paper_id": 2104.1333,
        "authors": "Lee Cartier and Svan Lembke",
        "title": "Climate Change Adaptation in the British Columbia Wine Industry Can\n  carbon sequestration technology lower the B.C. Wine Industry's greenhouse gas\n  emissions?",
        "comments": null,
        "journal-ref": "Applied Economics and Finance, Vol 8 No. 4, July 2021",
        "doi": "10.11114/aef.v8i4.5259",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The purpose of this study is to measure the benefits and costs of using\nbiochar, a carbon sequestration technology, to reduce the B.C Wine Industry's\ncarbon emissions. An economic model was developed to calculate the value-added\nfor each of the three sectors that comprise the BC Wine industry. Results\nindicate that each sector of the wine value chain is potentially profitable,\nwith 9,000 tonnes of CO2 sequestered each year. The study is unique in that it\ndemonstrates that using biochar, produced from wine industry waste, to\nsequester atmospheric CO2 can be both profitable and environmentally\nsustainable.\n"
    },
    {
        "paper_id": 2104.13367,
        "authors": "Davide Viviano, Kaspar Wuthrich and Paul Niehaus",
        "title": "A model of multiple hypothesis testing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multiple hypothesis testing practices vary widely, without consensus on which\nare appropriate when. This paper provides an economic foundation for these\npractices designed to capture processes of scientific communication, such as\nregulatory approval on the basis of clinical trials. In studies of multiple\ntreatments or sub-populations, adjustments may be appropriate depending on\nscale economies in the research production function, with control of classical\nnotions of compound errors emerging in some but not all cases. In studies with\nmultiple outcomes, indexing is appropriate and adjustments to test levels may\nbe appropriate if the intended audience is heterogeneous. Data on actual costs\nin the drug approval process suggest both that some adjustment is warranted in\nthat setting and that standard procedures are overly conservative.\n"
    },
    {
        "paper_id": 2104.13425,
        "authors": "Richard S.J. Tol",
        "title": "State capacity and vulnerability to natural disasters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many empirical studies have shown that government quality is a key\ndeterminant of vulnerability to natural disasters. Protection against natural\ndisasters can be a public good -- flood protection, for example -- or a natural\nmonopoly -- early warning systems, for instance. Recovery from natural\ndisasters is easier when the financial system is well-developed, particularly\ninsurance services. This requires a strong legal and regulatory environment.\nThis paper reviews the empirical literature to find that government quality and\ndemocracy reduce vulnerability to natural disasters while corruption of public\nofficials increases vulnerability. The paper complements the literature by\nincluding tax revenue as an explanatory variable for vulnerability to natural\ndisasters, and by modelling both the probability of natural disaster and the\ndamage done. Countries with a larger public sector are better at preventing\nextreme events from doing harm. Countries that take more of their revenue in\nincome taxes are better that reducing harm from natural disasters.\n"
    },
    {
        "paper_id": 2104.13475,
        "authors": "Ruiwu Liu",
        "title": "A Review of Disease and Development",
        "comments": "This paper was from my advanced case study report (2019) in the\n  Australian National University",
        "journal-ref": "Acemoglu, D and Johnson, S 2007, 'Disease and Development: The\n  Effect of Life Expectancy on Economic Growth.', Journal of Political Economy,\n  vol. 115, no. 6, pp. 925-85",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Acemoglu and Johnson (2007) put forward the unprecedented view that health\nimprovement has no significant effect on income growth. To arrive at this\nconclusion, they constructed predicted mortality as an instrumental variable\nbased on the WHO international disease interventions to analyse this problem. I\nreplicate the process of their research and eliminate some biases in their\nestimate. In addition, and more importantly, we argue that the construction of\ntheir instrumental variable contains a violation of the exclusion restriction\nof their instrumental variable. This negative correlation between health\nimprovement and income growth still lacks an accurate causal explanation,\naccording to which the instrumental variable they constructed increases reverse\ncausality bias instead of eliminating it.\n"
    },
    {
        "paper_id": 2104.13652,
        "authors": "Caroline Graf, Eva-Maria Merz, Bianca Suanet and Pamala Wiepking",
        "title": "Social Norms Offer Explanation for Inconsistent Effects of Incentives on\n  Prosocial Behavior",
        "comments": "34 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Incentives have surprisingly inconsistent effects when it comes to\nencouraging people to behave prosocially. Classical economic theory, according\nto which a specific behavior becomes more prevalent when it is rewarded,\nstruggles to explain why incentives sometimes backfire. More recent theories\ntherefore posit a reputational cost offsetting the benefits of receiving an\nincentive -- yet unexplained effects of incentives remain, for instance across\nincentive types and countries. We propose that social norms can offer an\nexplanation for these inconsistencies. Ultimately, social norms determine the\nreputational costs or benefits resulting from a given behavior, and thus\nvariation in the effect of incentives may reflect variation in norms. We\nimplemented a formal model of prosocial behavior integrating social norms,\nwhich we empirically tested on the real-world prosocial behavior of blood\ndonation. Blood donation is essential for many life-saving medical procedures,\nbut also presents an ideal testing ground for our theory: Various incentive\npolicies for blood donors exist across countries, enabling a comparative\napproach. Our preregistered analyses reveal that social norms can indeed\naccount for the varying effects of financial and time incentives on\nindividual-level blood donation behavior across 28 European countries.\nIncentives are associated with higher levels of prosociality when norms\nregarding the incentive are more positive. The results indicate that social\nnorms play an important role in explaining the relationship between incentives\nand prosocial behavior. More generally, our approach highlights the potential\nof integrating theory from across the economic and behavioral sciences to\ngenerate novel insights, with tangible consequences for policy-making.\n"
    },
    {
        "paper_id": 2104.13669,
        "authors": "Calypso Herrera, Florian Krach, Pierre Ruyssen, Josef Teichmann",
        "title": "Optimal Stopping via Randomized Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper presents the benefits of using randomized neural networks instead\nof standard basis functions or deep neural networks to approximate the\nsolutions of optimal stopping problems. The key idea is to use neural networks,\nwhere the parameters of the hidden layers are generated randomly and only the\nlast layer is trained, in order to approximate the continuation value. Our\napproaches are applicable to high dimensional problems where the existing\napproaches become increasingly impractical. In addition, since our approaches\ncan be optimized using simple linear regression, they are easy to implement and\ntheoretical guarantees can be provided. We test our approaches for American\noption pricing on Black--Scholes, Heston and rough Heston models and for\noptimally stopping a fractional Brownian motion. In all cases, our algorithms\noutperform the state-of-the-art and other relevant machine learning approaches\nin terms of computation time while achieving comparable results. Moreover, we\nshow that they can also be used to efficiently compute Greeks of American\noptions.\n"
    },
    {
        "paper_id": 2104.13747,
        "authors": "Fabian Stephany, Hanno Lorenz",
        "title": "The Future of Employment Revisited: How Model Selection Determines\n  Automation Forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The uniqueness of human labour is at question in times of smart technologies.\nThe 250 years-old discussion on technological unemployment reawakens.\nProminently, Frey and Osborne (2017) estimated that half of US employment will\nbe automated by algorithms within the next 20 years. Other follow-up studies\nconclude that only a small fraction of workers will be replaced by digital\ntechnologies. The main contribution of our work is to show that the diversity\nof previous findings regarding the degree of job automation is, to a large\nextent, driven by model selection and not by controlling for personal\ncharacteristics or tasks. For our case study, we consult experts in machine\nlearning and industry professionals on the susceptibility to digital\ntechnologies in the Austrian labour market. Our results indicate that, while\nclerical computer-based routine jobs are likely to change in the next decade,\nprofessional activities, such as the processing of complex information, are\nless prone to digital change.\n"
    },
    {
        "paper_id": 2104.13947,
        "authors": "Nathan Thomas Provost",
        "title": "Modelling Net Loan Loss with Bayesian and Frequentist Regression\n  Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We create two distinct nonlinear regression models relating net loan loss (as\nan outcome) to several other financial and sociological quantities. We consider\nthese data for the time interval between April 1st 2011 and April 1st 2020. We\nalso include temporal quantities (month and year) in our model to improve\naccuracy. One model follows the frequentist paradigm for nonlinear regression,\nwhile the other follows the Bayesian paradigm. By using the two methods, we\nobtain a rounded understanding of the relationship between net loan losses and\nour given financial, sociological, and temporal variables, improving our\nability to make financial predictions regarding the profitability of loan\nallocation.\n"
    },
    {
        "paper_id": 2104.13948,
        "authors": "Ekaterina Zolotareva",
        "title": "Applying Convolutional Neural Networks for Stock Market Trends\n  Identification",
        "comments": "22 pages, 8 figures, 6 tables. This paper is the full text of the\n  research, presented at the 20th International Conference on Artificial\n  Intelligence and Soft Computing Web System (ICAISC 2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we apply a specific type ANNs - convolutional neural networks\n(CNNs) - to the problem of finding start and endpoints of trends, which are the\noptimal points for entering and leaving the market. We aim to explore long-term\ntrends, which last several months, not days. The key distinction of our model\nis that its labels are fully based on expert opinion data. Despite the various\nmodels based solely on stock price data, some market experts still argue that\ntraders are able to see hidden opportunities. The labelling was done via the\nGUI interface, which means that the experts worked directly with images, not\nnumerical data. This fact makes CNN the natural choice of algorithm. The\nproposed framework requires the sequential interaction of three CNN submodels,\nwhich identify the presence of a changepoint in a window, locate it and finally\nrecognize the type of new tendency - upward, downward or flat. These submodels\nhave certain pitfalls, therefore the calibration of their hyperparameters is\nthe main direction of further research. The research addresses such issues as\nimbalanced datasets and contradicting labels, as well as the need for specific\nquality metrics to keep up with practical applicability. This paper is the full\ntext of the research, presented at the 20th International Conference on\nArtificial Intelligence and Soft Computing Web System (ICAISC 2021)\n"
    },
    {
        "paper_id": 2104.14002,
        "authors": "Friederike Wall",
        "title": "Modeling Managerial Search Behavior based on Simon's Concept of\n  Satisficing",
        "comments": "32 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Computational models of managerial search often build on backward-looking\nsearch based on hill-climbing algorithms. Regardless of its prevalence, there\nis some evidence that this family of algorithms does not universally represent\nmanagers' search behavior. Against this background, the paper proposes an\nalternative algorithm that captures key elements of Simon's concept of\nsatisficing which received considerable support in behavioral experiments. The\npaper contrasts the satisficing-based algorithm to two variants of\nhill-climbing search in an agent-based model of a simple decision-making\norganization. The model builds on the framework of NK fitness landscapes which\nallows controlling for the complexity of the decision problem to be solved. The\nresults suggest that the model's behavior may remarkably differ depending on\nwhether satisficing or hill-climbing serves as an algorithmic representation\nfor decision-makers' search. Moreover, with the satisficing algorithm, results\nindicate oscillating aspiration levels, even to the negative, and intense - and\npotentially destabilizing - search activities when intra-organizational\ncomplexity increases. Findings may shed some new light on prior computational\nmodels of decision-making in organizations and point to avenues for future\nresearch.\n"
    },
    {
        "paper_id": 2104.14043,
        "authors": "Ari Pramono and Harmen Oppewal",
        "title": "Where to Refuel: Modeling On-the-way Choice of Convenience Outlet",
        "comments": null,
        "journal-ref": "Journal of Retailing and Consumer Services (2021)",
        "doi": "10.1016/j.jretconser.2021.102572",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper introduces on-the-way choice of retail outlet as a form of\nconvenience shopping. It presents a model of on-the-way choice of retail outlet\nand applies the model in the context of fuel retailing to explore its\nimplications for segmentation and spatial competition. The model is a latent\nclass random utility choice model. An application to gas station choices\nobserved in a medium-sized Asian city show the model to fit substantially\nbetter than existing models. The empirical results indicate consumers may adopt\none of two decision strategies. When adopting an immediacy-oriented strategy\nthey behave in accordance with the traditional gravity-based retail models and\ntend to choose the most spatially convenient outlet. When following a\ndestination-oriented strategy they focus more on maintaining their overall trip\nefficiency and so will tend to visit outlets located closer to their main\ndestination and are more susceptible to retail agglomeration effects. The paper\ndemonstrates how the model can be used to inform segmentation and local\ncompetition analyses that account for variations in these strategies as well as\nvariations in consumer type, origin and time of travel. Simulations of a\nduopoly setting further demonstrate the implications.\n"
    },
    {
        "paper_id": 2104.14188,
        "authors": "Luigi Biagini, Simone Severini (Tutor)",
        "title": "The role of Common Agricultural Policy (CAP) in enhancing and\n  stabilising farm income: an analysis of income transfer efficiency and the\n  Income Stabilisation Tool",
        "comments": "127 page",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Since its inception, the E.U.'s Common Agricultural Policy (CAP) aimed at\nensuring an adequate and stable farm income. While recognizing that the CAP\npursues a larger set of objectives, this thesis focuses on the impact of the\nCAP on the level and the stability of farm income in Italian farms. It uses\nmicrodata from a high standardized dataset, the Farm Accountancy Data Network\n(FADN), that is available in all E.U. countries. This allows if perceived as\nuseful, to replicate the analyses to other countries. The thesis first assesses\nthe Income Transfer Efficiency (i.e., how much of the support translate to farm\nincome) of several CAP measures. Secondly, it analyses the role of a specific\nand relatively new CAP measure (i.e., the Income Stabilisation Tool - IST) that\nis specifically aimed at stabilising farm income. The assessment of the\npotential use of Machine Learning procedures to develop an adequate ratemaking\nin IST. These are used to predict indemnity levels because this is an essential\npoint for a similar insurance scheme. The assessment of ratemaking is\nchallenging: indemnity distribution is zero-inflated, not-continuous,\nright-skewed, and several factors can potentially explain it. We address these\nproblems by using Tweedie distributions and three Machine Learning procedures.\nThe objective is to assess whether this improves the ratemaking by using the\nprospective application of the Income Stabilization Tool in Italy as a case\nstudy. We look at the econometric performance of the models and the impact of\nusing their predictions in practice. Some of these procedures efficiently\npredict indemnities, using a limited number of regressors, and ensuring the\nscheme's financial stability.\n"
    },
    {
        "paper_id": 2104.1419,
        "authors": "Anton Koshelev",
        "title": "FX Market Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper aims at solving FX market volatility modeling problem and finding\nthe most becoming approach to this task. Validity of two competing approaches,\nclassical econometric generalized conditional heteroscedasticity and\nmathematical (singular spectrum analysis and dynamical systems stability\nanalysis) are tested on major currency pairs (EUR/USD, USD/JPY, GBP/USD) and\nunique high-frequency USD/RUB data. The study shows that both mathematical\ntools, understudied in econometric discourse, have a great potential in scope\nof discussed problematic, as for all experiments covered in this research, both\nof them show promising results.\n"
    },
    {
        "paper_id": 2104.14199,
        "authors": "Michal Brzezinski",
        "title": "The impact of past pandemics on CO$_2$ emissions and transition to\n  renewable energy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We estimate the short- to medium term impact of six major past pandemic\ncrises on the CO2 emissions and energy transition to renewable electricity. The\nresults show that the previous pandemics led on average to a 3.4-3.7% fall in\nthe CO2 emissions in the short-run (1-2 years since the start of the pandemic).\nThe effect is present only in the rich countries, as well as in countries with\nthe highest pandemic death toll (where it disappears only after 8 years) and in\ncountries that were hit by the pandemic during economic recessions. We found\nthat the past pandemics increased the share of electricity generated from\nrenewable sources within the fiveyear horizon by 1.9-2.3 percentage points in\nthe OECD countries and by 3.2-3.9 percentage points in countries experiencing\neconomic recessions. We discuss the implications of our findings in the context\nof CO2 emissions and the transition to renewable energy in the post-COVID-19\nera.\n"
    },
    {
        "paper_id": 2104.14204,
        "authors": "Micha{\\l} Narajewski, Florian Ziel",
        "title": "Optimal bidding in hourly and quarter-hourly electricity price auctions:\n  trading large volumes of power with market impact and transaction costs",
        "comments": null,
        "journal-ref": "Enrgy Economics, 110 (2022) 105974",
        "doi": "10.1016/j.eneco.2022.105974",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the question of how much to bid to maximize the profit\nwhen trading in two electricity markets: the hourly Day-Ahead Auction and the\nquarter-hourly Intraday Auction. For optimal coordinated bidding many price\nscenarios are examined, the own non-linear market impact is estimated by\nconsidering empirical supply and demand curves, and a number of trading\nstrategies is used. Additionally, we provide theoretical results for risk\nneutral agents. The application study is conducted using the German market\ndata, but the presented methods can be easily utilized with other two\nconsecutive auctions. This paper contributes to the existing literature by\nevaluating the costs of electricity trading, i.e. the price impact and the\ntransaction costs. The empirical results for the German EPEX market show that\nit is far more profitable to minimize the price impact rather than maximize the\narbitrage.\n"
    },
    {
        "paper_id": 2104.14214,
        "authors": "Xi-Ning Zhuang, Zhao-Yun Chen, Yu-Chun Wu, Guo-Ping Guo",
        "title": "Quantum Quantitative Trading: High-Frequency Statistical Arbitrage\n  Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1367-2630/ac7f26",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantitative trading is an integral part of financial markets with high\ncalculation speed requirements, while no quantum algorithms have been\nintroduced into this field yet. We propose quantum algorithms for\nhigh-frequency statistical arbitrage trading in this work by utilizing variable\ntime condition number estimation and quantum linear regression.The algorithm\ncomplexity has been reduced from the classical benchmark O(N^2d) to\nO(sqrt(d)(kappa)^2(log(1/epsilon))^2 )). It shows quantum advantage, where N is\nthe length of trading data, and d is the number of stocks, kappa is the\ncondition number and epsilon is the desired precision. Moreover, two tool\nalgorithms for condition number estimation and cointegration test are\ndeveloped.\n"
    },
    {
        "paper_id": 2104.1424,
        "authors": "Fabian Scheller, Isabel Doser, Emily Schulte, Simon Johanning, Russell\n  McKenna, Thomas Bruckner",
        "title": "Stakeholder dynamics in residential solar energy adoption: findings from\n  focus group discussions in Germany",
        "comments": null,
        "journal-ref": "Energy Research & Social Science, Volume 76, 2021, 102065",
        "doi": "10.1016/j.erss.2021.102065",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Although there is a clear indication that stages of residential decision\nmaking are characterized by their own stakeholders, activities, and outcomes,\nmany studies on residential low-carbon technology adoption only implicitly\naddress stage-specific dynamics. This paper explores stakeholder influences on\nresidential photovoltaic adoption from a procedural perspective, so-called\nstakeholder dynamics. The major objective is the understanding of underlying\nmechanisms to better exploit the potential for residential photovoltaic uptake.\nFour focus groups have been conducted in close collaboration with the\nindependent institute for social science research SINUS Markt- und\nSozialforschung in East Germany. By applying a qualitative content analysis,\nmajor influence dynamics within three decision stages are synthesized with the\nhelp of egocentric network maps from the perspective of residential\ndecision-makers. Results indicate that actors closest in terms of emotional and\nspatial proximity such as members of the social network represent the major\ninfluence on residential PV decision-making throughout the stages. Furthermore,\ndecision-makers with a higher level of knowledge are more likely to move on to\nthe subsequent stage. A shift from passive exposure to proactive search takes\nplace through the process, but this shift is less pronounced among risk-averse\ndecision-makers who continuously request proactive influences. The discussions\nrevealed largely unexploited potential regarding the stakeholders local\nutilities and local governments who are perceived as independent, trustworthy\nand credible stakeholders. Public stakeholders must fulfill their\nresponsibility in achieving climate goals by advising, assisting, and financing\nservices for low-carbon technology adoption at the local level. Supporting\ncommunity initiatives through political frameworks appears to be another\npromising step.\n"
    },
    {
        "paper_id": 2104.14286,
        "authors": "Saeed Nosratabadi, Sina Ardabili, Zoltan Lakner, Csaba Mako, Amir\n  Mosavi",
        "title": "Prediction of Food Production Using Machine Learning Algorithms of\n  Multilayer Perceptron and ANFIS",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Advancing models for accurate estimation of food production is essential for\npolicymaking and managing national plans of action for food security. This\nresearch proposes two machine learning models for the prediction of food\nproduction. The adaptive network-based fuzzy inference system (ANFIS) and\nmultilayer perceptron (MLP) methods are used to advance the prediction models.\nIn the present study, two variables of livestock production and agricultural\nproduction were considered as the source of food production. Three variables\nwere used to evaluate livestock production, namely livestock yield, live\nanimals, and animal slaughtered, and two variables were used to assess\nagricultural production, namely agricultural production yields and losses. Iran\nwas selected as the case study of the current study. Therefore, time-series\ndata related to livestock and agricultural productions in Iran from 1961 to\n2017 have been collected from the FAOSTAT database. First, 70% of this data was\nused to train ANFIS and MLP, and the remaining 30% of the data was used to test\nthe models. The results disclosed that the ANFIS model with Generalized\nbell-shaped (Gbell) built-in membership functions has the lowest error level in\npredicting food production. The findings of this study provide a suitable tool\nfor policymakers who can use this model and predict the future of food\nproduction to provide a proper plan for the future of food security and food\nsupply for the next generations.\n"
    },
    {
        "paper_id": 2104.14301,
        "authors": "Musaab Mousa, Saeed Nosratabadi, Judit Sagi and Amir Mosavi",
        "title": "The Effect of Marketing Investment on Firm Value and Systematic Risk",
        "comments": null,
        "journal-ref": "Journal of Open Innovation: Technology, Market, and Complexity,\n  2021, 7(1), 64",
        "doi": "10.3390/joitmc7010064",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Analyzing the financial benefit of marketing is still a critical topic for\nboth practitioners and researchers. Companies consider marketing costs as a\ntype of investment and expect this investment to be returned to the company in\nthe form of profit. On the other hand, companies adopt different innovative\nstrategies to increase their value. Therefore, this study aims to test the\nimpact of marketing investment on firm value and systematic risk. To do so,\ndata related to four Arabic emerging markets during the period 2010-2019 are\nconsidered, and firm share price and beta share are considered to measure firm\nvalue and systematic risk, respectively. Since a firm's ownership concentration\nis a determinant factor in firm value and systematic risk, this variable is\nconsidered a moderated variable in the relationship between marketing\ninvestment and firm value and systematic risk. The findings of the study, using\npanel data regression, indicate that increasing investment in marketing has a\npositive effect on the firm value valuation model. It is also found that the\nownership concentration variable has a reinforcing role in the relationship\nbetween marketing investment and firm value. It is also disclosed that it\nmoderates the systematic risk aligned with the monitoring impact of controlling\nshareholders. This study provides a logical combination of governance-marketing\ndimensions to interpret performance indicators in the capital market.\n"
    },
    {
        "paper_id": 2104.14319,
        "authors": "Lech A. Grzelak",
        "title": "Sparse Grid Method for Highly Efficient Computation of Exposures for xVA",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Every \"x\"-adjustment in the so-called xVA financial risk management framework\nrelies on the computation of exposures. Considering thousands of Monte Carlo\npaths and tens of simulation steps, a financial portfolio needs to be evaluated\nnumerous times during the lifetime of the underlying assets. This is the\nbottleneck of every simulation of xVA. In this article, we explore numerical\ntechniques for improving the simulation of exposures. We aim to decimate the\nnumber of portfolio evaluations, particularly for large portfolios involving\nmultiple, correlated risk factors. The usage of the Stochastic Collocation (SC)\nmethod, together with Smolyak's sparse grid extension, allows for a significant\nreduction in the number of portfolio evaluations, even when dealing with many\nrisk factors. The proposed model can be easily applied to any portfolio and\nsize. We report that for a realistic portfolio comprising linear and non-linear\nderivatives, the expected reduction in the portfolio evaluations may exceed\n6000 times, depending on the dimensionality and the required accuracy. We give\nillustrative examples and examine the method with realistic multi-currency\nportfolios consisting of interest rate swaps and swaptions.\n"
    },
    {
        "paper_id": 2104.14412,
        "authors": "Erniel B. Barrios, Paolo Victor T. Redondo",
        "title": "Nonparametric Test for Volatility in Clustered Multiple Time Series",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10614-023-10362-x",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Contagion arising from clustering of multiple time series like those in the\nstock market indicators can further complicate the nature of volatility,\nrendering a parametric test (relying on asymptotic distribution) to suffer from\nissues on size and power. We propose a test on volatility based on the\nbootstrap method for multiple time series, intended to account for possible\npresence of contagion effect. While the test is fairly robust to distributional\nassumptions, it depends on the nature of volatility. The test is correctly\nsized even in cases where the time series are almost nonstationary. The test is\nalso powerful specially when the time series are stationary in mean and that\nvolatility are contained only in fewer clusters. We illustrate the method in\nglobal stock prices data.\n"
    },
    {
        "paper_id": 2104.14414,
        "authors": "Iva Raycheva",
        "title": "Regional poverty in Bulgaria in the period 2008-2019",
        "comments": null,
        "journal-ref": null,
        "doi": "10.9790/0837-2603064147",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Background: Poverty among the population of a country is one of the most\ndisputable topics in social studies. Many researchers devote their work to\nidentifying the factors that influence it most. Bulgaria is one of the EU\nmember states with the highest poverty levels. Regional facets of social\nexclusion and risks of poverty among the population are a key priority of the\nNational Development Strategy for the third decade of 21st century. In order to\nmitigate the regional poverty levels it is necessary for the social policy\nmakers to pay more attention to the various factors expected to influence these\nlevels. Results: Poverty reduction is observed in most areas of the country.\nThe regions with obviously favorable developments are Sofia district, Pernik,\nPleven, Lovech, Gabrovo, Veliko Tarnovo, Silistra, Shumen, Stara Zagora,\nSmolyan, Kyustendil and others. Increased levels of poverty are found for\nRazgrad and Montana districts. It was fond that the reduction in the risk of\npoverty is associated to the increase in employment, investment, and housing.\nConclusion: The social policy making needs to be aware of the fact that the\ndegree of exposition to risk of poverty and social exclusion significantly\nrelates to the levels of regional employment, investment and housing.\n"
    },
    {
        "paper_id": 2104.14615,
        "authors": "Rene Carmona and Laura Leal",
        "title": "Optimal Execution with Quadratic Variation Inventories",
        "comments": "26 pages, 20 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The first half of the paper is devoted to description and implementation of\nstatistical tests arguing for the presence of a Brownian component in the\ninventories and wealth processes of individual traders. We use intra-day data\nfrom the Toronto Stock Exchange to provide empirical evidence of this claim. We\nwork with regularly spaced time intervals, as well as with asynchronously\nobserved data. The tests reveal with high significance the presence of a\nnon-zero Brownian motion component. The second half of the paper is concerned\nwith the analysis of trader behaviors throughout the day. We extend the\ntheoretical analysis of an existing optimal execution model to accommodate the\npresence of It\\^o inventory processes, and we compare empirically the optimal\nbehavior of traders in such fitted models, to their actual behavior as inferred\nfrom the data.\n"
    },
    {
        "paper_id": 2104.14683,
        "authors": "Alessio Brini and Daniele Tantari",
        "title": "Deep Reinforcement Trading with Predictable Returns",
        "comments": "37 pages, 15 figures. Added a more detailed appendix explaining the\n  approached followed. Revised version to be published in Physyca A",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical portfolio optimization often requires forecasting asset returns and\ntheir corresponding variances in spite of the low signal-to-noise ratio\nprovided in the financial markets. Modern deep reinforcement learning (DRL)\noffers a framework for optimizing sequential trader decisions but lacks\ntheoretical guarantees of convergence. On the other hand, the performances on\nreal financial trading problems are strongly affected by the goodness of the\nsignal used to predict returns. To disentangle the effects coming from return\nunpredictability from those coming from algorithm un-trainability, we\ninvestigate the performance of model-free DRL traders in a market environment\nwith different known mean-reverting factors driving the dynamics. When the\nframework admits an exact dynamic programming solution, we can assess the\nlimits and capabilities of different value-based algorithms to retrieve\nmeaningful trading signals in a data-driven manner. We consider DRL agents that\nleverage classical strategies to increase their performances and we show that\nthis approach guarantees flexibility, outperforming the benchmark strategy when\nthe price dynamics is misspecified and some original assumptions on the market\nenvironment are violated with the presence of extreme events and volatility\nclustering.\n"
    },
    {
        "paper_id": 2104.1474,
        "authors": "Hao Yi Ong, Daniel Freund, Davide Crapis",
        "title": "Driver Positioning and Incentive Budgeting with an Escrow Mechanism for\n  Ridesharing Platforms",
        "comments": "Forthcoming in INFORMS Journal on Applied Analytics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Drivers on the Lyft rideshare platform do not always know where the areas of\nsupply shortage are in real time. This lack of information hurts both riders\ntrying to find a ride and drivers trying to determine how to maximize their\nearnings opportunity. Lyft's Personal Power Zone (PPZ) product helps the\ncompany to maintain high levels of service on the platform by influencing the\nspatial distribution of drivers in real time via monetary incentives that\nencourage them to reposition their vehicles. The underlying system that powers\nthe product has two main components: (1) a novel 'escrow mechanism' that tracks\navailable incentive budgets tied to locations within a city in real time, and\n(2) an algorithm that solves the stochastic driver positioning problem to\nmaximize short-run revenue from riders' fares. The optimization problem is a\nmultiagent dynamic program that is too complicated to solve optimally for our\nlarge-scale application. Our approach is to decompose it into two subproblems.\nThe first determines the set of drivers to incentivize and where to incentivize\nthem to position themselves. The second determines how to fund each incentive\nusing the escrow budget. By formulating it as two convex programs, we are able\nto use commercial solvers that find the optimal solution in a matter of\nseconds. Rolled out to all 320 cities in which Lyft's operates in a little over\na year, the system now generates millions of bonuses that incentivize hundreds\nof thousands of active drivers to optimally position themselves in anticipation\nof ride requests every week. Together, the PPZ product and its underlying\nalgorithms represent a paradigm shift in how Lyft drivers drive and generate\nearnings on the platform. Its direct business impact has been a 0.5% increase\nin incremental bookings, amounting to tens of millions of dollars per year.\n"
    },
    {
        "paper_id": 2105.00051,
        "authors": "Falko Baustian and Martin Fencl and Jan Posp\\'i\\v{s}il and Vladim\\'ir\n  \\v{S}v\\'igler",
        "title": "A note on a PDE approach to option pricing under xVA",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study partial differential equations (PDEs) that can be used\nto model value adjustments. Different value adjustments denoted generally as\nxVA are nowadays added to the risk-free financial derivative values and the PDE\napproach allows their easy incorporation. The aim of this paper is to show how\nto solve the PDE analytically in the Black-Scholes setting to get new\nsemi-closed formulas that we compare to the widely used Monte-Carlo simulations\nand to the numerical solutions of the PDE. Particular example of collateral\ntaken as the values from the past will be of interest.\n"
    },
    {
        "paper_id": 2105.00054,
        "authors": "Louis R. Eeckhoudt and Roger J. A. Laeven",
        "title": "Probability Premium and Attitude Towards Probability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Employing a generalized definition of Pratt (1964) and Arrow's (1965, 1971)\nprobability premium, we introduce a new concept of attitude towards\nprobability. We illustrate in a problem of risk sharing that whether attitude\ntowards probability is a first-order or second-order phenomenon has important\neconomic applications. By developing a local approximation to the probability\npremium, we show that the canonical rank-dependent utility model usually\nexhibits attitude towards probability of first order, whereas under the dual\ntheory with smooth probability weighting functions attitude towards probability\nis a second-order trait.\n"
    },
    {
        "paper_id": 2105.0013,
        "authors": "Frederik vom Scheidt, Jingyi Qu, Philipp Staudt, Dharik S.\n  Mallapragada, Christof Weinhardt",
        "title": "Integrating Hydrogen in Single-Price Electricity Systems: The Effects of\n  Spatial Economic Signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Hydrogen can contribute substantially to the reduction of carbon emissions in\nindustry and transportation. However, the production of hydrogen through\nelectrolysis creates interdependencies between hydrogen supply chains and\nelectricity systems. Therefore, as governments worldwide are planning\nconsiderable financial subsidies and new regulation to promote hydrogen\ninfrastructure investments in the next years, energy policy research is needed\nto guide such policies with holistic analyses. In this study, we link a\nelectrolytic hydrogen supply chain model with an electricity system dispatch\nmodel, for a cross-sectoral case study of Germany in 2030. We find that\nhydrogen infrastructure investments and their effects on the electricity system\nare strongly influenced by electricity prices. Given current uniform prices,\nhydrogen production increases congestion costs in the electricity grid by 17%.\nIn contrast, passing spatially resolved electricity price signals leads to\nelectrolyzers being placed at low-cost grid nodes and further away from\nconsumption centers. This causes lower end-use costs for hydrogen. Moreover,\ncongestion management costs decrease substantially, by up to 20% compared to\nthe benchmark case without hydrogen. These savings could be transferred into\naccording subsidies for hydrogen production. Thus, our study demonstrates the\nbenefits of differentiating economic signals for hydrogen production based on\nspatial criteria.\n"
    },
    {
        "paper_id": 2105.00204,
        "authors": "Ahrash Dianat and Mikhail Freer",
        "title": "Credibility in Second-Price Auctions: An Experimental Test",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide the first direct test of how the credibility of an auction format\naffects bidding behavior and final outcomes. To do so, we conduct a series of\nlaboratory experiments where the role of the seller is played by a human\nsubject who receives the revenue from the auction and who (depending on the\ntreatment) has agency to determine the outcome of the auction. Contrary to\ntheoretical predictions, we find that the non-credible second-price auction\nfails to converge to the first-price auction. We provide a behavioral\nexplanation for our results based on sellers' aversion to rule-breaking, which\nis confirmed by an additional experiment.\n"
    },
    {
        "paper_id": 2105.00337,
        "authors": "David Imhof and Hannes Wallimann",
        "title": "Detecting bid-rigging coalitions in different countries and auction\n  formats",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an original application of screening methods using machine\nlearning to detect collusive groups of firms in procurement auctions. As a\nmethodical innovation, we calculate coalition-based screens by forming\ncoalitions of bidders in tenders to flag bid-rigging cartels. Using Swiss,\nJapanese and Italian procurement data, we investigate the effectiveness of our\nmethod in different countries and auction settings, in our cases first-price\nsealed-bid and mean-price sealed-bid auctions. We correctly classify 90\\% of\nthe collusive and competitive coalitions when applying four machine learning\nalgorithms: lasso, support vector machine, random forest, and super learner\nensemble method. Finally, we find that coalition-based screens for the variance\nand the uniformity of bids are in all the cases the most important predictors\naccording the random forest.\n"
    },
    {
        "paper_id": 2105.00517,
        "authors": "{\\O}ystein Daljord, Guillaume Pouliot, Junji Xiao, Mandy Hu",
        "title": "The Black Market for Beijing License Plates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Black markets can reduce the effects of distortionary regulations by\nreallocating scarce resources toward consumers who value them most. The illegal\nnature of black markets, however, creates transaction costs that reduce the\ngains from trade. We take a partial identification approach to infer gains from\ntrade and transaction costs in the black market for Beijing car license plates,\nwhich emerged following their recent rationing. We find that at least 11% of\nemitted license plates are illegally traded. The estimated transaction costs\nsuggest severe market frictions: between 61% and 82% of the realized gains from\ntrade are lost to transaction costs.\n"
    },
    {
        "paper_id": 2105.00521,
        "authors": "Fabrizio Lillo",
        "title": "Order flow and price formation",
        "comments": "24 pages. To appear in \"Machine Learning in Financial Markets\" (A.\n  Capponi and C.A Lehalle, editors), Cambridge University Press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I present an overview of some recent advancements on the empirical analysis\nand theoretical modeling of the process of price formation in financial markets\nas the result of the arrival of orders in a limit order book exchange. After\ndiscussing critically the possible modeling approaches and the observed\nstylized facts of order flow, I consider in detail market impact and\ntransaction cost of trades executed incrementally over an extended period of\ntime, by comparing model predictions and recent extensive empirical results. I\nalso discuss how the simultaneous presence of many algorithmic trading\nexecutions affects the quality and cost of trading.\n"
    },
    {
        "paper_id": 2105.00556,
        "authors": "Francesco Sarracino (STATEC Research, GLO) and Kelsey J. O'Connor\n  (STATEC Research, IZA, GLO)",
        "title": "Neo-humanism and COVID-19: Opportunities for a socially and\n  environmentally sustainable world",
        "comments": "34 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A series of crises, culminating with COVID-19, shows that going Beyond GDP is\nurgently necessary. Social and environmental degradation are consequences of\nemphasizing GDP as a measure of progress. This degradation created the\nconditions for the COVID-19 pandemic and limited the efficacy of\ncounter-measures. Additionally, rich countries did not fare the pandemic much\nbetter than poor ones. COVID-19 thrived on inequalities and a lack of\ncooperation. In this article we leverage on defensive growth models to explain\nthe complex relationships between these factors, and we put forward the idea of\nneo-humanism, a cultural movement grounded on evidence from quality-of-life\nstudies. The movement proposes a new culture leading towards a socially and\nenvironmentally sustainable future. Specifically, neo-humanism suggests that\nprioritizing well-being by, for instance, promoting social relations, would\nbenefit the environment, enable collective action to address public issues,\nwhich in turn positively affects productivity and health, among other\nbehavioral outcomes, and thereby instills a virtuous cycle. Arguably, such a\nsociety would have been better endowed to cope with COVID-19, and possibly even\nprevented the pandemic. Neo-humanism proposes a world in which the well-being\nof people comes before the well-being of markets, in which promoting\ncooperation and social relations represents the starting point for better\nlives, and a peaceful and respectful coexistence with other species on Earth.\n"
    },
    {
        "paper_id": 2105.00617,
        "authors": "Patrick Opoku Asuming, Hyuncheol Bryant Kim, and Armand Sim",
        "title": "Selection and Behavioral Responses of Health Insurance Subsidies in the\n  Long Run: Evidence from a Field Experiment in Ghana",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1811.09004",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct a randomized experiment that varies one-time health insurance\nsubsidy amounts (partial and full) in Ghana to study the impacts of subsidies\non insurance enrollment and health care utilization. We find that both partial\nand full subsidies promote insurance enrollment in the long run, even after the\nsubsidies expired. Although the long run enrollment rate and selective\nenrollment do not differ by subsidy level, long run health care utilization\nincreased only for the partial subsidy group. We show that this can plausibly\nbe explained by stronger learning-through-experience behavior in the partial\nthan in the full subsidy group.\n"
    },
    {
        "paper_id": 2105.00655,
        "authors": "Riccardo Aiolfi, Nicola Moreni, Marco Bianchetti, Marco Scaringi,\n  Filippo Fogliani",
        "title": "Learning Bermudans",
        "comments": "24 pages, 6 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  American and Bermudan-type financial instruments are often priced with\nspecific Monte Carlo techniques whose efficiency critically depends on the\neffective dimensionality of the problem and the available computational power.\nIn our work we focus on Bermudan Swaptions, well-known interest rate\nderivatives embedded in callable debt instruments or traded in the OTC market\nfor hedging or speculation purposes, and we adopt an original pricing approach\nbased on Supervised Learning (SL) algorithms. In particular, we link the price\nof a Bermudan Swaption to its natural hedges, i.e. the underlying European\nSwaptions, and other sound financial quantities through SL non-parametric\nregressions. We test different algorithms, from linear models to decision\ntree-based models and Artificial Neural Networks (ANN), analyzing their\npredictive performances. All the SL algorithms result to be reliable and fast,\nallowing to overcome the computational bottleneck of standard Monte Carlo\nsimulations; the best performing algorithms for our problem result to be Ridge,\nANN and Gradient Boosted Regression Tree. Moreover, using feature importance\ntechniques, we are able to rank the most important driving factors of a\nBermudan Swaption price, confirming that the value of the maximum underlying\nEuropean Swaption is the prevailing feature.\n"
    },
    {
        "paper_id": 2105.00707,
        "authors": "Qiutong Guo and Shun Lei and Qing Ye and Zhiyang Fang",
        "title": "MRC-LSTM: A Hybrid Approach of Multi-scale Residual CNN and LSTM to\n  Predict Bitcoin Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bitcoin, one of the major cryptocurrencies, presents great opportunities and\nchallenges with its tremendous potential returns accompanying high risks. The\nhigh volatility of Bitcoin and the complex factors affecting them make the\nstudy of effective price forecasting methods of great practical importance to\nfinancial investors and researchers worldwide. In this paper, we propose a\nnovel approach called MRC-LSTM, which combines a Multi-scale Residual\nConvolutional neural network (MRC) and a Long Short-Term Memory (LSTM) to\nimplement Bitcoin closing price prediction. Specifically, the Multi-scale\nresidual module is based on one-dimensional convolution, which is not only\ncapable of adaptive detecting features of different time scales in multivariate\ntime series, but also enables the fusion of these features. LSTM has the\nability to learn long-term dependencies in series, which is widely used in\nfinancial time series forecasting. By mixing these two methods, the model is\nable to obtain highly expressive features and efficiently learn trends and\ninteractions of multivariate time series. In the study, the impact of external\nfactors such as macroeconomic variables and investor attention on the Bitcoin\nprice is considered in addition to the trading information of the Bitcoin\nmarket. We performed experiments to predict the daily closing price of Bitcoin\n(USD), and the experimental results show that MRC-LSTM significantly\noutperforms a variety of other network structures. Furthermore, we conduct\nadditional experiments on two other cryptocurrencies, Ethereum and Litecoin, to\nfurther confirm the effectiveness of the MRC-LSTM in short-term forecasting for\nmultivariate time series of cryptocurrencies.\n"
    },
    {
        "paper_id": 2105.00778,
        "authors": "Christian Bayer and Paul Hager and Sebastian Riedel and John\n  Schoenmakers",
        "title": "Optimal stopping with signatures",
        "comments": "39 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method for solving optimal stopping problems (such as\nAmerican option pricing in finance) under minimal assumptions on the underlying\nstochastic process $X$.\n  We consider classic and randomized stopping times represented by linear and\nnon-linear functionals of the rough path signature $\\mathbb{X}^{<\\infty}$\nassociated to $X$, and prove that maximizing over these classes of signature\nstopping times, in fact, solves the original optimal stopping problem. Using\nthe algebraic properties of the signature, we can then recast the problem as a\n(deterministic) optimization problem depending only on the (truncated) expected\nsignature $\\mathbb{E}\\left[ \\mathbb{X}^{\\le N}_{0,T} \\right]$. By applying a\ndeep neural network approach to approximate the non-linear signature\nfunctionals, we can efficiently solve the optimal stopping problem numerically.\n  The only assumption on the process $X$ is that it is a continuous (geometric)\nrandom rough path. Hence, the theory encompasses processes such as fractional\nBrownian motion, which fail to be either semi-martingales or Markov processes,\nand can be used, in particular, for American-type option pricing in fractional\nmodels, e.g. on financial or electricity markets.\n"
    },
    {
        "paper_id": 2105.00796,
        "authors": "Fabian Scheller, S\\\"oren Graupner, James Edwards, Jann Weinand, Thomas\n  Bruckner",
        "title": "Active peer effects in residential photovoltaic adoption: evidence on\n  impact drivers among potential and current adopters in Germany",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  While the importance of peer influences has been demonstrated in several\nstudies, little is known about the underlying mechanisms of active peer effects\nin residential photovoltaic (PV) diffusion. Empirical evidence indicates that\nthe impacts of inter-subjective exchanges are dependent on the subjective\nmutual evaluation of the interlocutors. This paper aims to quantify, how\nsubjective evaluations of peers affect peer effects across different stages of\nPV adoption decision-making. The findings of a survey among potential and\ncurrent adopters in Germany(N=1,165)confirm two hypotheses. First, peer effects\nplay a role in residential PV adoption: the number of peer adopters in the\ndecision-maker's social circle has a positive effect on the decision-maker's\nbelief that their social network supports PV adoption; their ascription of\ncredibility on PV-related topics to their peers; and their interest in actively\nseeking information from their peers in all decision-making stages. Second,\nthere is a correlation between the perceived positive attributes of a given\npeer and the reported influence of said peer within the decision-making\nprocess, suggesting that decision-makers' subjective evaluations of peers play\nan important role in active peer effects. Decision-makers are significantly\nmore likely to engage in and be influenced by interactions with peers who they\nperceive as competent, trustworthy, and likeable. In contrast, attributes such\nas physical closeness and availability have a less significant effect. From a\npolicymaking perspective, this study suggests that the density and quality of\npeer connections empower potential adopters. Accordingly, peer consultation and\ncommunity-led outreach initiatives should be promoted to accelerate residential\nPV adoption.\n"
    },
    {
        "paper_id": 2105.00844,
        "authors": "Patrizia Semeraro",
        "title": "Multivariate tempered stable additive subordination for financial models",
        "comments": "31 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a class of multivariate tempered stable distributions and introduce\nthe associated class of tempered stable Sato subordinators. These Sato\nsubordinators are used to build additive inhomogeneous processes by\nsubordination of a multiparameter Brownian motion. The resulting process is\nadditive and time inhomogeneous. Furthermore, these processes are associated\nwith the distribution at unit time of a class of L\\'evy process with good fit\nproperties on fifinancial data. The main feature of the Sato subordinated\nBrownian motion is that it has time dependent correlation, whereas the L\\'evy\ncounterpart does not. We provide a numerical illustration of the correlation\ndynamics.\n"
    },
    {
        "paper_id": 2105.00935,
        "authors": "Jan Obloj, Johannes Wiesel",
        "title": "Distributionally robust portfolio maximisation and marginal utility\n  pricing in one period financial markets",
        "comments": "Final version as published in \"Mathematical Finance\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal investment and marginal utility pricing problem of a\nrisk averse agent and quantify their exposure to a small amount of model\nuncertainty. Specifically, we compute explicitly the first-order sensitivity of\ntheir value function, optimal investment policy and marginal option prices to\nmodel uncertainty. The latter is understood as replacing a baseline model\n$\\mathbb{P}$ with an adverse choice from a small Wasserstein ball around\n$\\mathbb{P}$ in the space of probability measures. Our sensitivities are thus\nfully non-parametric. We show that the results entangle the baseline model\nspecification and the agent's risk attitudes. The sensitivities can behave in a\nnon-monotone way as a function of the baseline model's Sharpe's ratio, the\nrelative weighting of assets in an agent's portfolio can change and marginal\nprices can increase when an agent faces model uncertainty.\n"
    },
    {
        "paper_id": 2105.00939,
        "authors": "Justin Loye, Katia Jaffr\\`es-Runser and Dima Shepelyansky",
        "title": "Post-Brexit power of European Union from the world trade network\n  analysis",
        "comments": "11 pages, 6 figures + Supplementary Material",
        "journal-ref": null,
        "doi": "10.52825/bis.v1i.48",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop the Google matrix analysis of the multiproduct world trade network\nobtained from the UN COMTRADE database in recent years. The comparison is done\nbetween this new approach and the usual Import-Export description of this world\ntrade network. The Google matrix analysis takes into account the multiplicity\nof trade transactions thus highlighting in a better way the world influence of\nspecific countries and products. It shows that after Brexit, the European Union\nof 27 countries has the leading position in the world trade network ranking,\nbeing ahead of USA and China. Our approach determines also a sensitivity of\ntrade country balance to specific products showing the dominant role of\nmachinery and mineral fuels in multiproduct exchanges. It also underlines the\ngrowing influence of Asian countries.\n"
    },
    {
        "paper_id": 2105.01043,
        "authors": "Mohsen Foroughifar",
        "title": "Errors in Learning from Others' Choices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Observation of other people's choices can provide useful information in many\ncircumstances. However, individuals may not utilize this information\nefficiently, i.e., they may make decision-making errors in social interactions.\nIn this paper, I use a simple and transparent experimental setting to identify\nthese errors. In a within-subject design, I first show that subjects exhibit a\nhigher level of irrationality in the presence than in the absence of social\ninteraction, even when they receive informationally equivalent signals across\nthe two conditions. A series of treatments aimed at identifying mechanisms\nsuggests that a decision maker is often uncertain about the behavior of other\npeople so that she has difficulty in inferring the information contained in\nothers' choices. Building upon these reduced-from results, I then introduce a\ngeneral decision-making process to highlight three sources of error in\ndecision-making under social interactions. This model is non-parametrically\nestimated and sheds light on what variation in the data identifies which error.\n"
    },
    {
        "paper_id": 2105.01127,
        "authors": "Diana B\\\"ottger, Philipp H\\\"artel",
        "title": "On Wholesale Electricity Prices and Market Values in a Carbon-Neutral\n  Energy System",
        "comments": "Preprint submitted to Energy Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Climate and energy policy targets of the European Commission aim to make\nEurope the first climate-neutral continent by 2050. For low-carbon and\nnet-neutral energy systems primarily based on variable renewable power\ngeneration, issues related to the market integration, cannibalisation of\nrevenues, and cost recovery of wind and solar photovoltaics have become major\nconcerns. The traditional discussion of the merit-order effect expects\nwholesale power prices in a system with 100 % renewable energy sources to\nalternate between very high and very low values. Unlike previous work, we\npresent a structured and technology-specific analysis of the cross-sectoral\ndemand bidding effect for the price formation in low-carbon power markets.\nStarting from a stylised market arrangement and by successively augmenting it\nwith all relevant technologies, we construct and quantify the cross-sectoral\ndemand bidding effects in future European power markets with the cross-sectoral\nmarket modelling framework SCOPE SD. As the main contribution, we explain and\nsubstantiate the market clearing effects of new market participants in detail.\nHereby, we put a special focus on hybrid heat supply systems consisting of\ncombined heat and power plant, fuel boiler, thermal storage and electrical back\nup and derive the opportunity costs of these systems. Furthermore, we show the\neffects of cross-border integration for a large-scale European net-neutral\nenergy scenario. Finally, the detailed information on market clearing effects\nallows us to evaluate the resulting revenues of all major technology categories\non future electricity markets.\n"
    },
    {
        "paper_id": 2105.01142,
        "authors": "Alexandre K. Ligo, Emerson Mahoney, Jeffrey Cegan, Benjamin D. Trump,\n  Andrew S. Jin, Maksim Kitsak, Jesse Keenan, Igor Linkov",
        "title": "Relationship among state reopening policies, health outcomes and\n  economic recovery through first wave of the COVID-19 pandemic in the U.S",
        "comments": "Revised on October 4, 2021",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0260015",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  State governments in the U.S. have been facing difficult decisions involving\ntradeoffs between economic and health-related outcomes during the COVID-19\npandemic. Despite evidence of the effectiveness of government-mandated\nrestrictions mitigating the spread of contagion, these orders are stigmatized\ndue to undesirable economic consequences. This tradeoff resulted in state\ngovernments employing mandates in widely different ways. We compare the\ndifferent policies states implemented during periods of restriction (lockdown)\nand reopening with indicators of COVID-19 spread and consumer card spending at\neach state during the first wave of the pandemic in the U.S. between March and\nAugust 2020. We find that while some states enacted reopening decisions when\nthe incidence rate of COVID-19 was minimal or sustained in its relative\ndecline, other states relaxed socioeconomic restrictions near their highest\nincidence and prevalence rates experienced so far. Nevertheless, all states\nexperienced similar trends in consumer card spending recovery, which was\nstrongly correlated with reopening policies following the lockdowns and\nrelatively independent from COVID-19 incidence rates at the time. Our findings\nsuggest that consumer card spending patterns can be attributed to government\nmandates rather than COVID-19 incidence in the states. We estimate the recovery\nin states that reopened in late April was more than the recovery in states that\ndid not reopen in the same period - 15% for consumer card spending and 18% for\nspending by high income households. This result highlights the important role\nof state policies in minimizing health impacts while promoting economic\nrecovery and helps planning effective interventions in subsequent waves and\nimmunization efforts.\n"
    },
    {
        "paper_id": 2105.01154,
        "authors": "Gregor Berz, Florian Rupp, Brian Sieben",
        "title": "How the 'Auction Cube' Supports the Selection of Auction Designs in\n  Industrial Procurement",
        "comments": "32 pages, in German, 3 figures, 9 tabels",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  It is well known that rightly applied reverse auctions offer big commercial\npotential to procurement departments. However, the sheer number of auction\ntypes often overwhelms users in practice. And since the implications of a\nwrongly chosen auction type are equally well known, the overall usage of\nreverse auctions lacks its potential significantly. In this paper, a novel\nmethod is being proposed that guides the user in selecting the right\ncombination of basic auction forms for single lot events, considering both\nmarket-, as well as supplier-related, bijective criteria.\n"
    },
    {
        "paper_id": 2105.0138,
        "authors": "Antoine Falck, Adam Rej, David Thesmar",
        "title": "Why and how systematic strategies decay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose ex-ante characteristics that predict the drop in\nrisk-adjusted performance out-of-sample for a large set of stock anomalies\npublished in finance and accounting academic journals. Our set of predictors is\ngenerated by hypotheses of OOS decay put forward by McLean and Pontiff (2016):\narbitrage capital flowing into newly published strategies and in-sample\noverfitting linked to multiple hypothesis testing. The year of publication\nalone - compatible with both hypotheses - explains 30% of the variance of\nSharpe decay across factors: Every year, the Sharpe decay of newly-published\nfactors increases by 5ppt. The other important variables are directly related\nto overfitting: the number of operations required to calculate the signal and\ntwo measures of sensitivity of in-sample Sharpe to outliers together add\nanother 15% of explanatory power. Some arbitrage-related variables are\nstatistically significant, but their predictive power is marginal.\n"
    },
    {
        "paper_id": 2105.01426,
        "authors": "Martin Huber, Jonas Meier, Hannes Wallimann",
        "title": "Business analytics meets artificial intelligence: Assessing the demand\n  effects of discounts on Swiss train tickets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We assess the demand effects of discounts on train tickets issued by the\nSwiss Federal Railways, the so-called `supersaver tickets', based on machine\nlearning, a subfield of artificial intelligence. Considering a survey-based\nsample of buyers of supersaver tickets, we investigate which customer- or\ntrip-related characteristics (including the discount rate) predict buying\nbehavior, namely: booking a trip otherwise not realized by train, buying a\nfirst- rather than second-class ticket, or rescheduling a trip (e.g.\\ away from\nrush hours) when being offered a supersaver ticket. Predictive machine learning\nsuggests that customer's age, demand-related information for a specific\nconnection (like departure time and utilization), and the discount level permit\nforecasting buying behavior to a certain extent. Furthermore, we use causal\nmachine learning to assess the impact of the discount rate on rescheduling a\ntrip, which seems relevant in the light of capacity constraints at rush hours.\nAssuming that (i) the discount rate is quasi-random conditional on our rich set\nof characteristics and (ii) the buying decision increases weakly monotonically\nin the discount rate, we identify the discount rate's effect among `always\nbuyers', who would have traveled even without a discount, based on our survey\nthat asks about customer behavior in the absence of discounts. We find that on\naverage, increasing the discount rate by one percentage point increases the\nshare of rescheduled trips by 0.16 percentage points among always buyers.\nInvestigating effect heterogeneity across observables suggests that the effects\nare higher for leisure travelers and during peak hours when controlling several\nother characteristics.\n"
    },
    {
        "paper_id": 2105.01446,
        "authors": "Federico Fioravanti, Fernando Delbianco and Fernando Tohm\\'e",
        "title": "Home advantage and crowd attendance: Evidence from rugby during the\n  Covid 19 pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The COVID-19 pandemic forced almost all professional and amateur sports to be\nplayed without attending crowds. Thus, it induced a large-scale natural\nexperiment on the impact of social pressure on decision making and behavior in\nsports fields. Using a data set of 1027 rugby union matches from 11 tournaments\nin 10 countries, we find that home teams have won less matches and their points\ndifference decreased during the pandemics, shedding light on the impact of\ncrowd attendance on the {\\em home advantage} of sports teams.\n"
    },
    {
        "paper_id": 2105.01581,
        "authors": "Mehmet Ekmekci and Hanzhe Zhang",
        "title": "Reputational Bargaining with Ultimatum Opportunities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study two-sided reputational bargaining with opportunities to issue an\nultimatum -- threats to force dispute resolution. Each player is either a\njustified type, who never concedes and issues an ultimatum whenever an\nopportunity arrives, or an unjustified type, who can concede, wait, or bluff\nwith an ultimatum. In equilibrium, the presence of ultimatum opportunities can\nharm or benefit a player by decelerating or accelerating reputation building.\nWhen only one player can issue an ultimatum, equilibrium play is unique. The\nhazard rate of dispute resolution is discontinuous and piecewise monotonic in\ntime. As the probabilities of being justified vanish, agreement is immediate\nand efficient, and if the set of justifiable demands is rich, payoffs modify\nAbreu and Gul (2000), with the discount rate replaced by the ultimatum\nopportunity arrival rate if the former is smaller. When both players' ultimatum\nopportunities arrive sufficiently fast, there may exist multiple equilibria in\nwhich their reputations do not build up and negotiation lasts forever.\n"
    },
    {
        "paper_id": 2105.01644,
        "authors": "Jun Wong, Jonathan Santoso, Marjorie Went, and Daniel Sanchez",
        "title": "Market Potential for CO$_2$ Removal and Sequestration from Renewable\n  Natural Gas Production in California",
        "comments": "25 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bioenergy with Carbon Capture and Sequestration (BECCS) is critical for\nstringent climate change mitigation, but is commercially and technologically\nimmature and resource-intensive. In California, state and federal fuel and\nclimate policies can drive first-markets for BECCS. We develop a spatially\nexplicit optimization model to assess niche markets for renewable natural gas\n(RNG) production with carbon capture and sequestration (CCS) from waste biomass\nin California. Existing biomass residues produce biogas and RNG and enable\nlow-cost CCS through the upgrading process and CO$_2$ truck transport. Under\ncurrent state and federal policy incentives, we could capture and sequester 2.9\nmillion MT CO$_2$/year (0.7% of California's 2018 CO$_2$ emissions) and produce\n93 PJ RNG/year (4% of California's 2018 natural gas demand) with a profit\nmaximizing objective. Existing federal and state policies produce profits of\n\\$11/GJ. Distributed RNG production with CCS potentially catalyzes markets and\ntechnologies for CO$_2$ capture, transport, and storage in California.\n"
    },
    {
        "paper_id": 2105.01745,
        "authors": "Katerina Antonopoulou, Nicholas Dacre",
        "title": "Exploring Diffusion Characteristics that Influence Serious Games\n  Adoption Decisions",
        "comments": "Citation: Antonopoulou, K., & Dacre, N. (2015). Exploring Diffusion\n  Characteristics that Influence Serious Games Adoption Decisions. Innovation\n  in Information Infrastructures, Coventry, UK.\n  https://dx.doi.org/10.2139/ssrn.3829185",
        "journal-ref": "Innovation in Information Infrastructures 2015",
        "doi": "10.2139/ssrn.3829185",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we discuss the diffusion of serious games and present reasons\nfor why Rogers traditional approach is limited in this context. We present an\nalternative overview through the characteristics of relative advantage,\ncompatibility, complexity, trialability, and observability, that reflect on the\nadoption decision and contributes on the commercialization of serious games.\n"
    },
    {
        "paper_id": 2105.01792,
        "authors": "Ranjan Pal, Ziyuan Huang, Xinlong Yin, Sergey Lototsky, Swades De,\n  Sasu Tarkoma, Mingyan Liu, Jon Crowcroft, Nishanth Sastry",
        "title": "Aggregate Cyber-Risk Management in the IoT Age: Cautionary Statistics\n  for (Re)Insurers and Likes",
        "comments": "incrementally updated version to version in IEEE Internet of Things\n  Journal",
        "journal-ref": null,
        "doi": "10.1109/JIOT.2020.3039254",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we provide (i) a rigorous general theory to elicit conditions\non (tail-dependent) heavy-tailed cyber-risk distributions under which a risk\nmanagement firm might find it (non)sustainable to provide aggregate cyber-risk\ncoverage services for smart societies, and (ii)a real-data driven numerical\nstudy to validate claims made in theory assuming boundedly rational cyber-risk\nmanagers, alongside providing ideas to boost markets that aggregate dependent\ncyber-risks with heavy-tails.To the best of our knowledge, this is the only\ncomplete general theory till date on the feasibility of aggregate cyber-risk\nmanagement.\n"
    },
    {
        "paper_id": 2105.01829,
        "authors": "Xue Dong He and Xun Yu Zhou",
        "title": "Who Are I: Time Inconsistency and Intrapersonal Conflict and\n  Reconciliation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time inconsistency is prevalent in dynamic choice problems: a plan of actions\nto be taken in the future that is optimal for an agent today may not be optimal\nfor the same agent in the future. If the agent is aware of this intra-personal\nconflict but unable to commit herself in the future to following the optimal\nplan today, the rational strategy for her today is to reconcile with her future\nselves, namely to correctly anticipate her actions in the future and then act\ntoday accordingly. Such a strategy is named intra-personal equilibrium and has\nbeen studied since as early as in the 1950s. A rigorous treatment in\ncontinuous-time settings, however, had not been available until a decade ago.\nSince then, the study on intra-personal equilibrium for time-inconsistent\nproblems in continuous time has grown rapidly. In this chapter, we review the\nclassical results and some recent development in this literature.\n"
    },
    {
        "paper_id": 2105.02057,
        "authors": "Vygintas Gontis",
        "title": "Order flow in the financial markets from the perspective of the\n  Fractional L\\'evy stable motion",
        "comments": "13 pages, 6 figures, 2 tables",
        "journal-ref": "Commun. Nonlinear Sci. Numer. Simul., vol. 105, p. 106087, 2022",
        "doi": "10.1016/j.cnsns.2021.106087",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  It is a challenging task to identify the best possible models based on given\nempirical data of observed time series. Though the financial markets provide us\nwith a vast amount of empirical data, the best model selection is still a big\nchallenge for researchers. The widely used long-range memory and\nself-similarity estimators give varying values of the parameters as these\nestimators themselves are developed for the specific models of time series.\nHere we investigate from the general fractional L\\'evy stable motion\nperspective the order disbalance time series constructed from the limit order\nbook data of the financial markets. Our results suggest that previous findings\nof persistence in order flow could be related to the power-law distribution of\norder sizes and other deviations from the normal distribution. Still, orders\nhave stable estimates of anti-correlation for the 18 randomly selected stocks\nwhen Absolute value and Higuchi's estimators are implemented. Though the burst\nduration analysis based on the first passage problem of time series and\nimplemented in this research gives slightly higher estimates of the Hurst and\nmemory parameters, it qualitatively supports the importance of the power-law\ndistribution of order sizes.\n"
    },
    {
        "paper_id": 2105.02094,
        "authors": "Jacopo De Tullio, Giuseppe Puleio",
        "title": "Sustainability of Collusion and Market Transparency in a Sequential\n  Search Market: a Generalization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The present work generalizes the analytical results of Petrikaite (2016) to a\nmarket where more than two firms interact. As a consequence, for a generic\nnumber of firms in the oligopoly model described by Janssen et al (2005), the\nrelationship between the critical discount factor which sustains the monopoly\ncollusive allocation and the share of perfectly informed buyers is\nnon-monotonic, reaching a unique internal point of minimum. The first section\nlocates the work within the proper economic framework. The second section hosts\nthe analytical computations and the mathematical reasoning needed to derive the\ndesired generalization, which mainly relies on the Leibniz rule for the\ndifferentiation under the integral sign and the Bounded Convergence Theorem.\n"
    },
    {
        "paper_id": 2105.02211,
        "authors": "Ivan Jericevich and Patrick Chang and Tim Gebbie",
        "title": "Simulation and estimation of a point-process market-model with a\n  matching engine",
        "comments": "19 pages, 33 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The extent to which a matching engine can cloud the modelling of underlying\norder submission and management processes in a financial market remains an\nunanswered concern with regards to market models. Here we consider a 10-variate\nHawkes process with simple rules to simulate common order types which are\nsubmitted to a matching engine. Hawkes processes can be used to model the time\nand order of events, and how these events relate to each other. However, they\nprovide a freedom with regards to implementation mechanics relating to the\nprices and volumes of injected orders. This allows us to consider a reference\nHawkes model and two additional models which have rules that change the\nbehaviour of limit orders. The resulting trade and quote data from the\nsimulations are then calibrated and compared with the original order generating\nprocess to determine the extent with which implementation rules can distort\nmodel parameters. Evidence from validation and hypothesis tests suggest that\nthe true model specification can be significantly distorted by market\nmechanics, and that practical considerations not directly due to model\nspecification can be important with regards to model identification within an\ninherently asynchronous trading environment.\n"
    },
    {
        "paper_id": 2105.02325,
        "authors": "Nicholas Salmon and Indranil SenGupta",
        "title": "Fractional Barndorff-Nielsen and Shephard model: applications in\n  variance and volatility swaps, and hedging",
        "comments": "28 pages, 4 figures, 4 tables",
        "journal-ref": "Annals of Finance, 2021",
        "doi": "10.1007/s10436-021-00394-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce and analyze the fractional Barndorff-Nielsen and\nShephard (BN-S) stochastic volatility model. The proposed model is based upon\ntwo desirable properties of the long-term variance process suggested by the\nempirical data: long-term memory and jumps. The proposed model incorporates the\nlong-term memory and positive autocorrelation properties of fractional Brownian\nmotion with $H>1/2$, and the jump properties of the BN-S model. We find\narbitrage-free prices for variance and volatility swaps for this new model.\nBecause fractional Brownian motion is still a Gaussian process, we derive some\nnew expressions for the distributions of integrals of continuous Gaussian\nprocesses as we work towards an analytic expression for the prices of these\nswaps. The model is analyzed in connection to the quadratic hedging problem and\nsome related analytical results are developed. The amount of derivatives\nrequired to minimize a quadratic hedging error is obtained. Finally, we provide\nsome numerical analysis based on the VIX data. Numerical results show the\nefficiency of the proposed model compared to the Heston model and the classical\nBN-S model.\n"
    },
    {
        "paper_id": 2105.02387,
        "authors": "Torsten Heinrich",
        "title": "Epidemics in modern economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How are economies in a modern age impacted by epidemics? In what ways is\neconomic life disrupted? How can pandemics be modeled? What can be done to\nmitigate and manage the danger? Does the threat of pandemics increase or\ndecrease in the modern world? The Covid-19 pandemic has demonstrated the\nimportance of these questions and the potential of complex systems science to\nprovide answers. This article offers a broad overview of the history of\npandemics, of established facts, and of models of infection diffusion,\nmitigation strategies, and economic impact. The example of the Covid-19\npandemic is used to illustrate the theoretical aspects, but the article also\nincludes considerations concerning other historic epidemics and the danger of\nmore infectious and less controllable outbreaks in the future.\n"
    },
    {
        "paper_id": 2105.02728,
        "authors": "Tolga Buz, Gerard de Melo",
        "title": "Should You Take Investment Advice From WallStreetBets? A Data-Driven\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reddit's WallStreetBets (WSB) community has come to prominence in light of\nits notable role in affecting the stock prices of what are now referred to as\nmeme stocks. Yet very little is known about the reliability of the highly\nspeculative investment advice disseminated on WSB. This paper analyses WSB data\nspanning from January 2019 to April 2021 in order to assess how successful an\ninvestment strategy relying on the community's recommendations could have been.\nWe detect buy and sell advice and identify the community's most popular stocks,\nbased on which we define a WSB portfolio. Our evaluation shows that this\nportfolio has grown approx. 200% over the last three years and approx. 480%\nover the last year, significantly outperforming the S&P500. The average\nshort-term accuracy of buy and sell signals, in contrast, is not found to be\nsignificantly better than randomly or equally distributed buy decisions within\nthe same time frame. However, we present a technique for estimating whether\nposts are proactive as opposed to reactive and show that by focusing on a\nsubset of more promising buy signals, a trader could have made investments\nyielding higher returns than the broader market or the strategy of trusting all\nposted buy signals. Lastly, the analysis is also conducted specifically for the\nperiod before 2021 in order to factor out the effects of the GameStop hype of\nJanuary 2021 - the results confirm the conclusions and suggest that the 2021\nhype merely amplified pre-existing characteristics.\n"
    },
    {
        "paper_id": 2105.02781,
        "authors": "Edouard Ribes (CERNA i3)",
        "title": "Where are the opportunities for growth in the professional services\n  space?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The professional services industry (legal, accounting, consulting,\narchitectural services{\\ldots}) employs an important share of the active\npopulation in mature countries. However, after decades of undisputed growth,\nthe sector appears to be at a turning point in certain geographies. This\narticle therefore proposes a simple framework to help diagnose where growth\nopportunities (if any) may lie.When applied to the US economic context, the\nmodel indicates that at a macro-economic national level the sector should\nstall, which concurs with the trend observed over the past decade. However, it\nalso highlights that a few industrial sectors (e.g. the US beverage industry)\nstill offer pockets of growth for a variety of professional expertises.\nReplicating and fine-tuning those findings could be interesting for\npractitioners to steer their marketing and business development efforts. On the\nother hand, the quantitative framework presented in this study could pave the\nway for future research in the academic community.\n"
    },
    {
        "paper_id": 2105.02782,
        "authors": "Johannes Rude Jensen, Mohsen Pourpouneh, Kurt Nielsen, Omri Ross",
        "title": "The Homogenous Properties of Automated Market Makers",
        "comments": "Automated Market Maker, DeFi, Blockchain, Constant Function Market\n  Maker",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Automated market makers (AMM) have grown to obtain significant market share\nwithin the cryptocurrency ecosystem, resulting in a proliferation of new\nproducts pursuing exotic strategies for horizontal differentiation. Yet, their\ntheoretical properties are curiously homogeneous when a set of basic\nassumptions are met. In this paper, we start by presenting a universal approach\nto deriving a formula for liquidity provisioning for AMMs. Next, we show that\nthe constant function market maker and token swap market maker models are\ntheoretically equivalent when liquidity reserves are uniform. Proceeding with\nan examination of AMM market microstructure, we show how non-linear price\neffect translates into slippage for traders and impermanent losses for\nliquidity providers. We proceed by showing how impermanent losses are a\nfunction of both volatility and market depth and discuss the implications of\nthese findings within the context of the literature.\n"
    },
    {
        "paper_id": 2105.02784,
        "authors": "Ye Wang and Yan Chen and Haotian Wu and Liyi Zhou and Shuiguang Deng\n  and Roger Wattenhofer",
        "title": "Cyclic Arbitrage in Decentralized Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Decentralized Exchanges (DEXes) enable users to create markets for exchanging\nany pair of cryptocurrencies. The direct exchange rate of two tokens may not\nmatch the cross-exchange rate in the market, and such price discrepancies open\nup arbitrage possibilities with trading through different cryptocurrencies\ncyclically. In this paper, we conduct a systematic investigation on cyclic\narbitrages in DEXes. We propose a theoretical framework for studying cyclic\narbitrage. With our framework, we analyze the profitability conditions and\noptimal trading strategies of cyclic transactions. We further examine\nexploitable arbitrage opportunities and the market size of cyclic arbitrages\nwith transaction-level data of Uniswap V2. We find that traders have executed\n292,606 cyclic arbitrages over eleven months and exploited more than 138\nmillion USD in revenue. However, the revenue of the most profitable unexploited\nopportunity is persistently higher than 1 ETH (4,000 USD), which indicates that\nDEX markets may not be efficient enough. By analyzing how traders implement\ncyclic arbitrages, we find that traders can utilize smart contracts to issue\natomic transactions and the atomic implementations could mitigate users'\nfinancial loss in cyclic arbitrage from the price impact.\n"
    },
    {
        "paper_id": 2105.02785,
        "authors": "Navid Mottaghi and Sara Farhangdoost",
        "title": "Stock Price Forecasting in Presence of Covid-19 Pandemic and Evaluating\n  Performances of Machine Learning Models for Time-Series Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the heightened volatility in stock prices during the Covid-19 pandemic,\nthe need for price forecasting has become more critical. We investigated the\nforecast performance of four models including Long-Short Term Memory, XGBoost,\nAutoregression, and Last Value on stock prices of Facebook, Amazon, Tesla,\nGoogle, and Apple in COVID-19 pandemic time to understand the accuracy and\npredictability of the models in this highly volatile time region. To train the\nmodels, the data of all stocks are split into train and test datasets. The test\ndataset starts from January 2020 to April 2021 which covers the COVID-19\npandemic period. The results show that the Autoregression and Last value models\nhave higher accuracy in predicting the stock prices because of the strong\ncorrelation between the previous day and the next day's price value.\nAdditionally, the results suggest that the machine learning models (Long-Short\nTerm Memory and XGBoost) are not performing as well as Autoregression models\nwhen the market experiences high volatility.\n"
    },
    {
        "paper_id": 2105.02849,
        "authors": "Dr. Cesar R Salas-Guerra",
        "title": "Impact of digital economic activity on regional economic growth: A Case\n  study from northern Minas Gerais between 2009 To 2018",
        "comments": "in Spanish",
        "journal-ref": "UAGM Doctoral Thesis 2020",
        "doi": "10.13140/RG.2.2.15633.35686",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  At present, the economic measurement of the national statistical offices has\nnot defined or captured the benefits of the digital economy activity due to the\nlow quality or inexistence of methodologies. Currently, there is a relevant\ndebate on the capacity of the digital economy activity to generate\nproductivity, economic growth, and well-being through innovation and knowledge.\nFor this reason, this research identified and studied specialized knowledge,\nhuman settlement, and digital economic activity as the factors that influence\nregional economic growth. As a result, the impact generated by a new business\noperating models based on information technology was measured. Furthermore,\nthis research used an empirical measurement model that made it possible to\nidentify certain phenomena such as regional poles of regional economic\ndevelopment (PRDE) that surround economically flourishing regions. In addition,\nit showed that municipalities with high degrees of economic growth were\nimpacted by digital economic activity and specialized knowledge. This finding\nis consistent with economic growth theories that point to technological\nevolution as the main factor of modern economic growth. Consequently, this\nstudy contributed beneficial results to the local government to develop\nstrategies framed in solving industrial cooperation of economically flourishing\nregions with their neighbors, facing the problem of agglomeration of resources\nand capital reflected in human settlement promote an imbalance in economic\ngrowth and social development.\n"
    },
    {
        "paper_id": 2105.03071,
        "authors": "Piergiacomo Sabino",
        "title": "Normal Tempered Stable Processes and the Pricing of Energy Derivatives",
        "comments": "27 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2103.13252",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study we consider the pricing of energy derivatives when the\nevolution of spot prices is modeled with a normal tempered stable driven\nOrnstein-Uhlenbeck process. Such processes are the generalization of normal\ninverse Gaussian processes that are widely used in energy finance applications.\nWe first specify their statistical properties calculating their characteristic\nfunction in closed form. This result is instrumental for the derivation of\nnon-arbitrage conditions such that the spot dynamics is consistent with the\nforward curve without relying on numerical approximations or on numerical\nintegration. Moreover, we conceive an efficient algorithm for the exact\ngeneration of the trajectories which gives the possibility to implement Monte\nCarlo simulations without approximations or bias. We illustrate the\napplicability of the theoretical findings and the simulation algorithms in the\ncontext of the pricing of different contracts, namely, strips of daily call\noptions, Asian options with European style and swing options. Finally, we\npresent an extension to future markets.\n"
    },
    {
        "paper_id": 2105.03405,
        "authors": "Arega Getaneh Abate, Rosana Riccardi, Carlos Ruiz",
        "title": "Dynamic tariffs-based demand response in retail electricity market under\n  uncertainty",
        "comments": "32",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Demand response (DR) programs play a crucial role in improving system\nreliability and mitigating price volatility by altering the core profile of\nelectricity consumption. This paper proposes a game-theoretical model that\ncaptures the dynamic interplay between retailers (leaders) and consumers\n(followers) in a tariffs-based electricity market under uncertainty. The\nproposed procedure offers theoretical and economic insights by analyzing demand\nflexibility within a hierarchical decision-making framework. In particular, two\nmain market configurations are examined under uncertainty: i) there exists a\nretailer that exercises market power over consumers, and ii) the retailer and\nthe consumers participate in a perfect competitive game. The former case is\nformulated as a mathematical program with equilibrium constraints (MPEC),\nwhereas the latter case is recast as a mixed-integer linear program (MILP).\nThese problems are solved by deriving equivalent tractable reformulations based\non the Karush-Kuhn-Tucker (KKT) optimality conditions of each agent's problem.\n  Numerical simulations based on real data from the European Energy Exchange\nplatform are used to illustrate the performance of the proposed methodology.\nThe results indicate that the proposed model effectively characterizes the\ninteractions between retailers and flexible consumers in both perfect and\nimperfect market structures. Under perfect competition, the economic benefits\nextend not only to consumers but also to overall social welfare. Conversely, in\nan imperfect market, retailers leverage consumer flexibility to enhance their\nexpected profits, transferring the risk of uncertainty to end-users.\nAdditionally, the degree of consumer flexibility and their valuation of\nelectricity consumption play significant roles in shaping market outcomes.\n"
    },
    {
        "paper_id": 2105.03512,
        "authors": "Jason Soria, Amanda Stathopoulos",
        "title": "Investigating Socio-spatial Differences between Solo Ridehailing and\n  Pooled Rides in Diverse Communities",
        "comments": "Submitted to journal of Transport Geography",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Transformative mobility services present both considerable opportunities and\nchallenges for urban mobility systems. Increasing attention is being paid to\nridehailing platforms and connections between demand and continuous innovation\nin service features; one of these features is dynamic ride-pooling. To\ndisentangle how ridehailing impacts existing transportation networks and its\nability to support economic vitality and community livability it is essential\nto consider the distribution of demand across diverse communities. In this\npaper we expand the literature on ridehailing demand by exploring community\nvariation and spatial dependence in ridehailing use. Specifically, we\ninvestigate the diffusion and role of solo requests versus ride-pooling to shed\nlight on how different mobility services, with different environmental and\naccessibility implications, are used by diverse communities. This paper employs\na Social Disadvantage Index, Transit Access Analysis, and a Spatial Durbin\nModel to investigate the influence of both local and spatial spillover effects\non the demand for shared and solo ridehailing. The analysis of 127 million\nridehailing rides, of which 15% are pooled, confirms the presence of spatial\neffects. Results indicate that density and vibrancy variables have analogue\neffects, both direct and indirect, on demand for solo vs pooled rides. Instead,\nour analysis reveals significant contrasting effects for socio-economic\ndisadvantage, which is positively correlated with ride-pooling and negatively\nwith solo rides. Additionally, we find that higher rail transit access is\nassociated with higher demand for both solo and pooled ridehailing along with\nsubstantial spatial spillovers. We discuss implications for policy, operations\nand research related to the novel insight on how pooled ridesourcing relate to\ngeography, living conditions, and transit interactions.\n"
    },
    {
        "paper_id": 2105.03514,
        "authors": "Thilini Mahanama, Abootaleb Shirvani, and Svetlozar Rachev",
        "title": "Global Index on Financial Losses due to Crime in the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crime can have a volatile impact on investments. Despite the potential\nimportance of crime rates in investments, there are no indices dedicated to\nevaluating the financial impact of crime in the United States. As such, this\npaper presents an index-based insurance portfolio for crime in the United\nStates by utilizing the financial losses reported by the Federal Bureau of\nInvestigation for property crimes and cybercrimes. Our research intends to help\ninvestors envision risk exposure in our portfolio, gauge investment risk based\non their desired risk level, and hedge strategies for potential losses due to\neconomic crashes. Underlying the index, we hedge the investments by issuing\nmarketable European call and put options and providing risk budgets\n(diversifying risk to each type of crime). We find that real estate,\nransomware, and government impersonation are the main risk contributors. We\nthen evaluate the performance of our index to determine its resilience to\neconomic crisis. The unemployment rate potentially demonstrates a high systemic\nrisk on the portfolio compared to the economic factors used in this study. In\nconclusion, we provide a basis for the securitization of insurance risk from\ncertain crimes that could forewarn investors to transfer their risk to capital\nmarket investors.\n"
    },
    {
        "paper_id": 2105.03562,
        "authors": "Takuro Kobashi, Younghun Choi, Yujiro Hirano, Yoshiki Yamagata, Kelvin\n  Say",
        "title": "Deeply decarbonizing residential and urban central districts through\n  photovoltaics plus electric vehicle applications",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": "10.1016/j.apenergy.2021.118142",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the costs of renewable energy technologies declining, new forms of urban\nenergy systems are emerging that can be established in a cost-effective way.\nThe SolarEV City concept has been proposed that uses rooftop Photovoltaics (PV)\nto its maximum extent, combined with Electric Vehicle (EV) with bi-directional\ncharging for energy storage. Urban environments consist of various areas, such\nas residential and commercial districts, with different energy consumption\npatterns, building structures, and car parks. The cost effectiveness and\ndecarbonization potentials of PV + EV and PV (+ battery) systems vary across\nthese different urban environments and change over time as cost structures\ngradually shift. To evaluate these characteristics, we performed\ntechno-economic analyses of PV, battery, and EV technologies for a residential\narea in Shinchi, Fukushima and the central commercial district of Kyoto, Japan\nbetween 2020 and 2040. We found that PV + EV and PV only systems in 2020 are\nalready cost competitive relative to existing energy systems (grid electricity\nand gasoline car). In particular, the PV + EV system rapidly increases its\neconomic advantage over time, particularly in the residential district which\nhas larger PV capacity and EV battery storage relative to the size of energy\ndemand. Electricity exchanges between neighbors (e.g., peer-to-peer or\nmicrogrid) further enhanced the economic value (net present value) and\ndecarbonization potential of PV + EV systems up to 23 percent and 7 percent in\n2030, respectively. These outcomes have important strategic implications for\nurban decarbonization over the coming decades.\n"
    },
    {
        "paper_id": 2105.03625,
        "authors": "Zhishun Wang, Wei Lu, Kaixin Zhang, Tianhao Li, Zixi Zhao",
        "title": "A parallel-network continuous quantitative trading model with GARCH and\n  PPO",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is a difficult task for both professional investors and individual traders\ncontinuously making profit in stock market. With the development of computer\nscience and deep reinforcement learning, Buy\\&Hold (B\\&H) has been oversteped\nby many artificial intelligence trading algorithms. However, the information\nand process are not enough, which limit the performance of reinforcement\nlearning algorithms. Thus, we propose a parallel-network continuous\nquantitative trading model with GARCH and PPO to enrich the basical deep\nreinforcement learning model, where the deep learning parallel network layers\ndeal with 3 different frequencies data (including GARCH information) and\nproximal policy optimization (PPO) algorithm interacts actions and rewards with\nstock trading environment. Experiments in 5 stocks from Chinese stock market\nshow our method achieves more extra profit comparing with basical reinforcement\nlearning methods and bench models.\n"
    },
    {
        "paper_id": 2105.03656,
        "authors": "Richard S.J. Tol",
        "title": "Estimates of the social cost of carbon have increased over time",
        "comments": "arXiv admin note: text overlap with arXiv:2003.09276",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A meta-analysis of published estimates shows that the social cost of carbon\nhas increased as knowledge about climate change accumulates. Correcting for\ninflation and emission year and controlling for the discount rate, kernel\ndensity decomposition reveals a non-stationary distribution. In the last 10\nyears, estimates of the social cost of carbon have increased from $33/tC to\n$146/tC for a high discount rate and from $446/tC to $1925/tC for a low\ndiscount rate. Actual carbon prices are almost everywhere below its estimated\nvalue and should therefore go up.\n"
    },
    {
        "paper_id": 2105.0367,
        "authors": "Hongyan Cai, Danhong Chen, Yunfei Peng and Wei Wei",
        "title": "On the Time-Inconsistent Deterministic Linear-Quadratic Control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A fundamental theory of deterministic linear-quadratic (LQ) control is the\nequivalent relationship between control problems, two-point boundary value\nproblems and Riccati equations. In this paper, we extend the equivalence to a\ngeneral time-inconsistent deterministic LQ problem, where the inconsistency\narises from non-exponential discount functions. By studying the solvability of\nthe Riccati equation, we show the existence and uniqueness of the linear\nequilibrium for the time-inconsistent LQ problem.\n"
    },
    {
        "paper_id": 2105.03707,
        "authors": "James H. Merrick, John E.T. Bistline, Geoffrey J. Blanford",
        "title": "On representation of energy storage in electricity planning models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper considers the representation of energy storage in electricity\nsector capacity planning models. The incorporation of storage in long-term\nsystems models of this type is increasingly relevant as the cost of storage\ntechnologies, particularly batteries, and of complementary variable renewable\ntechnologies, decline. To value storage technologies appropriately, a\nrepresentation of linkages between time periods is required, breaking classical\ntemporal aggregation strategies that greatly improve computation time. We\nappraise approaches to address this problem, highlighting a common underlying\nstructure, conditions for lossless aggregation, and challenges of aggregation\nat relevant geographical scales. We then investigate solutions to the modeling\nproblem including a decomposition scheme to avoid temporal aggregation at a\nparallelizable computational cost. These examples frame aspects of the problem\nripe for contributions from the operational research community.\n"
    },
    {
        "paper_id": 2105.03844,
        "authors": "Sihang Chen, Weiqi Luo and Chao Yu",
        "title": "Reinforcement Learning with Expert Trajectory For Quantitative Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, quantitative investment methods combined with artificial\nintelligence have attracted more and more attention from investors and\nresearchers. Existing related methods based on the supervised learning are not\nvery suitable for learning problems with long-term goals and delayed rewards in\nreal futures trading. In this paper, therefore, we model the price prediction\nproblem as a Markov decision process (MDP), and optimize it by reinforcement\nlearning with expert trajectory. In the proposed method, we employ more than\n100 short-term alpha factors instead of price, volume and several technical\nfactors in used existing methods to describe the states of MDP. Furthermore,\nunlike DQN (deep Q-learning) and BC (behavior cloning) in related methods, we\nintroduce expert experience in training stage, and consider both the\nexpert-environment interaction and the agent-environment interaction to design\nthe temporal difference error so that the agents are more adaptable for\ninevitable noise in financial data. Experimental results evaluated on share\nprice index futures in China, including IF (CSI 300) and IC (CSI 500), show\nthat the advantages of the proposed method compared with three typical\ntechnical analysis and two deep leaning based methods.\n"
    },
    {
        "paper_id": 2105.04073,
        "authors": "Masaaki Fukasawa, Blanka Horvath and Peter Tankov",
        "title": "Hedging under rough volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this chapter we first briefly review the existing approaches to hedging in\nrough volatility models. Next, we present a simple but general result which\nshows that in a one-factor rough stochastic volatility model, any option may be\nperfectly hedged with a dynamic portfolio containing the underlying and one\nother asset such as a variance swap. In the final section we report the results\nof a back-test experiment using real data, where VIX options are hedged with a\nforward variance swap. In this experiment, using a rough volatility model\nallows to almost completely remove the bias and reduce the overall hedging\nerror by a factor of 27% compared to traditional diffusion-based models.\n"
    },
    {
        "paper_id": 2105.04131,
        "authors": "Geoffrey Ducournau",
        "title": "Symbol Dynamics, Information theory and Complexity of Economic time\n  series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to examine the predictability and the complexity characteristics\nof the Standard&Poor500 dynamics behaviors in a coarse-grained way using the\nsymbolic dynamics method and under the prism of the Information theory through\nthe concept of entropy and uncertainty. We believe that experimental\nmeasurement of entropy as a way of examining the complexity of a system is more\nrelevant than more common tests of universality in the transition to chaos\nbecause it does not make any prior prejudices on the underlying causes\nassociated with the system dynamics, whether deterministic or stochastic. We\nregard the studied economic time series as being complex and propose to express\nit in terms of the amount of information this last is producing on different\ntime scales and according to various scaling parameters.\n"
    },
    {
        "paper_id": 2105.04171,
        "authors": "Geoffrey Ducournau",
        "title": "Bayesian inference and superstatistics to describe long memory processes\n  of financial time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the standardized features of financial data is that log-returns are\nuncorrelated, but absolute log-returns or their squares namely the fluctuating\nvolatility are correlated and is characterized by heavy tailed in the sense\nthat some moment of the absolute log-returns is infinite and typically\nnon-Gaussian [20]. And this last characteristic change accordantly to different\ntimescales. We propose to model this long-memory phenomenon by superstatistical\ndynamics and provide a Bayesian Inference methodology drawing on\nMetropolis-Hasting random walk sampling to determine which superstatistics\namong inverse-Gamma and log-Normal describe the best log-returns complexity on\ndifferent timescales, from high to low frequency. We show that on smaller\ntimescales (minutes) even though the Inverse-Gamma superstatistics works the\nbest, the log-Normal model remains very reliable and suitable to fit the\nabsolute log-returns probability density distribution with strong capacity of\ndescribing heavy tails and power law decays. On larger timescales (daily), we\nshow in terms of Bayes factor that the inverse-Gamma superstatistics is\npreferred to the log-Normal model. We also show evidence of a transition of\nstatistics from power law decay on small timescales to exponential decay on\nlarge scale with less heavy tails meaning that on larger time scales the\nfluctuating volatility tend to be memoryless, consequently superstatistics\nbecomes less relevant.\n"
    },
    {
        "paper_id": 2105.04395,
        "authors": "Axel Pr\\\"user, Imre Kondor and Andreas Engel",
        "title": "Aspects of a phase transition in high-dimensional random geometry",
        "comments": null,
        "journal-ref": "Entropy 2021, 23(7), 805",
        "doi": "10.3390/e23070805",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A phase transition in high-dimensional random geometry is analyzed as it\narises in a variety of problems. A prominent example is the feasibility of a\nminimax problem that represents the extremal case of a class of financial risk\nmeasures, among them the current regulatory market risk measure Expected\nShortfall. Others include portfolio optimization with a ban on short selling,\nthe storage capacity of the perceptron, the solvability of a set of linear\nequations with random coefficients, and competition for resources in an\necological system. These examples shed light on various aspects of the\nunderlying geometric phase transition, create links between problems belonging\nto seemingly distant fields and offer the possibility for further\nramifications.\n"
    },
    {
        "paper_id": 2105.04511,
        "authors": "Henrique Guerreiro and Jo\\~ao Guerra",
        "title": "Least squares Monte Carlo methods in stochastic Volterra rough\n  volatility models",
        "comments": "30 pages, 11 figures",
        "journal-ref": "Journal of Computational Finance, 26(3):73-101, 2022",
        "doi": "10.21314/JCF.2022.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In stochastic Volterra rough volatility models, the volatility follows a\ntruncated Brownian semi-stationary process with stochastic vol-of-vol.\nRecently, efficient VIX pricing Monte Carlo methods have been proposed for the\ncase where the vol-of-vol is Markovian and independent of the volatility.\nFollowing recent empirical data, we discuss the VIX option pricing problem for\na generalized framework of these models, where the vol-of-vol may depend on the\nvolatility and/or not be Markovian. In such a setting, the aforementioned Monte\nCarlo methods are not valid. Moreover, the classical least squares Monte Carlo\nfaces exponentially increasing complexity with the number of grid time steps,\nwhilst the nested Monte Carlo method requires a prohibitive number of\nsimulations. By exploring the infinite dimensional Markovian representation of\nthese models, we device a scalable least squares Monte Carlo for VIX option\npricing. We apply our method firstly under the independence assumption for\nbenchmarks, and then to the generalized framework. We also discuss the rough\nvol-of-vol setting, where Markovianity of the vol-of-vol is not present. We\npresent simulations and benchmarks to establish the efficiency of our method.\n"
    },
    {
        "paper_id": 2105.04514,
        "authors": "Stephan Leitner",
        "title": "On the Role of Incentives in Evolutionary Approaches to Organizational\n  Design",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a model of a stylized organization that is comprised of\nseveral departments that autonomously allocate tasks. To do so, the departments\neither take short-sighted decisions that immediately maximize their utility or\ntake long-sighted decisions that aim at minimizing the interdependencies\nbetween tasks. The organization guides the departments' behavior by either an\nindividualistic, a balanced, or an altruistic linear incentive scheme. Even if\ntasks are perfectly decomposable, altruistic incentive schemes are preferred\nover individualistic incentive schemes since they substantially increase the\norganization's performance. Interestingly, if altruistic incentive schemes are\neffective, short-sighted decisions appear favorable since they do not only\nincrease performance in the short run but also result in significantly higher\nperformances in the long run.\n"
    },
    {
        "paper_id": 2105.04667,
        "authors": "Yossi Haimberg",
        "title": "Can an Agency Role-Reversal Lead to an Organizational Collapse?; A Study\n  Proposal",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Principal-Agent Theory model is widely used to explain governance role\nwhere there is a separation of ownership and control, as it defines clear\nboundaries between governance and executives. However, examination of recent\ncorporate failure reveals the concerning contribution of the Board of Directors\nto such failures and calls into question governance effectiveness in the\npresence of a powerful and charismatic CEO. This study proposes a framework for\nanalyzing the relationship between the Board of Directors and the CEO, and how\ncertain relationships affect the power structure and behavior of the Board,\nwhich leads to a role reversal in the Principal-Agent Theory, as the Board\nassumes the role of the CEO's agent. This study's results may help create a red\nflag for a board and leader's behavior that may result in governance failure.\n"
    },
    {
        "paper_id": 2105.04718,
        "authors": "Zoe Goss, Daniel Coles, Matthew Piggott",
        "title": "Economic analysis of tidal stream turbine arrays: a review",
        "comments": "29 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This tidal stream energy industry has to date been comprised of small\ndemonstrator projects made up of one to a four turbines. However, there are\ncurrently plans to expand to commercially sized projects with tens of turbines\nor more. As the industry moves to large-scale arrays for the first time, there\nhas been a push to develop tools to optimise the array design and help bring\ndown the costs. This review investigates different methods of modelling the\neconomic performance of tidal-stream arrays, for use within these optimisation\ntools. The different cost reduction pathways are discussed from costs falling\nas the global installed capacity increases, due to greater experience, improved\npower curves through larger-diameter higher-rated turbines, to economic\nefficiencies that can be found by moving to large-scale arrays. A literature\nreview is conducted to establish the most appropriate input values for use in\neconomic models. This includes finding a best case, worst case and typical\nvalues for costs and other related parameters. The information collated in this\nreview can provide a useful steering for the many optimisation tools that have\nbeen developed, especially when cost information is commercially sensitive and\na realistic parameter range is difficult to obtain.\n"
    },
    {
        "paper_id": 2105.049,
        "authors": "A. Fronzetti Colladon, F. Grippa, B. Guardabascio, G. Costante, F.\n  Ravazzolo",
        "title": "Forecasting consumer confidence through semantic network analysis of\n  online news",
        "comments": null,
        "journal-ref": "Scientific Reports 13, 11785 (2023)",
        "doi": "10.1038/s41598-023-38400-6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research studies the impact of online news on social and economic\nconsumer perceptions through semantic network analysis. Using over 1.8 million\nonline articles on Italian media covering four years, we calculate the semantic\nimportance of specific economic-related keywords to see if words appearing in\nthe articles could anticipate consumers' judgments about the economic situation\nand the Consumer Confidence Index. We use an innovative approach to analyze big\ntextual data, combining methods and tools of text mining and social network\nanalysis. Results show a strong predictive power for the judgments about the\ncurrent households and national situation. Our indicator offers a complementary\napproach to estimating consumer confidence, lessening the limitations of\ntraditional survey-based methods.\n"
    },
    {
        "paper_id": 2105.05148,
        "authors": "Katharina Gruber, Tobias Gauster, Peter Regner, Gregor Laaha, Johannes\n  Schmidt",
        "title": "Winterization of Texan power system infrastructure is profitable but\n  risky",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We deliver the first analysis of the 2021 cold spell in Texas which combines\ntemperature dependent load estimates with temperature dependent estimates of\npower plant outages to understand the frequency of loss of load events, using a\n71 year long time series of climate data. The expected revenue from full\nwinterization is 11.74bn$ over a 30 years investment period. We find that\nlarge-scale winterization, in particular of gas infrastructure and gas power\nplants, would be profitable, as related costs for winterization are\nsubstantially lower. At the same moment, the associated investment risks are\nhigh due to the low-frequency of events - the 2021 event was the largest and we\nobserve only 8 other similar ones for the simulated 71 years. As risks to\ninvestors are considerable, regulatory measures may be necessary to enforce\nwinterization.\n"
    },
    {
        "paper_id": 2105.05297,
        "authors": "Oliver Pf\\\"auti",
        "title": "Inflation -- who cares? Monetary Policy in Times of Low Attention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I propose an approach to quantify attention to inflation in the data and show\nthat the decrease in the volatility and persistence of U.S. inflation after the\nGreat Inflation period was accompanied by a decline in the public's attention\nto inflation. This decline in attention has important implications (positive\nand normative) for monetary policy as it renders managing inflation\nexpectations more difficult and can lead to inflation-attention traps:\nprolonged periods of a binding lower bound and low inflation due to\nslowly-adjusting inflation expectations. As attention declines the optimal\npolicy response is to increase the inflation target. Accounting for the lower\nbound fundamentally changes the normative implications of declining attention.\nWhile lower attention raises welfare absent the lower-bound constraint, it\ndecreases welfare when accounting for the lower bound.\n"
    },
    {
        "paper_id": 2105.05356,
        "authors": "Florian Bourgey and Stefano De Marco",
        "title": "Multilevel Monte Carlo simulation for VIX options in the rough Bergomi\n  model",
        "comments": "22 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the pricing of VIX options in the rough Bergomi model. In this\nsetting, the VIX random variable is defined by the one-dimensional integral of\nthe exponential of a Gaussian process with correlated increments, hence\napproximate samples of the VIX can be constructed via discretization of the\nintegral and simulation of a correlated Gaussian vector. A Monte-Carlo\nestimator of VIX options based on a rectangle discretization scheme and exact\nGaussian sampling via the Cholesky method has a computational complexity of\norder $\\mathcal{O}(\\varepsilon^{-4})$ when the mean-squared error is set to\n$\\varepsilon^2$. We demonstrate that this cost can be reduced to\n$\\mathcal{O}(\\varepsilon^{-2} \\log^2(\\varepsilon))$ combining the scheme above\nwith the multilevel method, and further reduced to the asymptotically optimal\ncost $\\mathcal{O}(\\varepsilon^{-2})$ when using a trapezoidal discretization.\nWe provide numerical experiments highlighting the efficiency of the multilevel\napproach in the pricing of VIX options in such a rough forward variance\nsetting.\n"
    },
    {
        "paper_id": 2105.05359,
        "authors": "Masaaki Fukasawa and Jim Gatheral",
        "title": "A rough SABR formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following an approach originally suggested by Balland in the context of the\nSABR model, we derive an ODE that is satisfied by normalized volatility smiles\nfor short maturities under a rough volatility extension of the SABR model that\nextends also the rough Bergomi model. We solve this ODE numerically and further\npresent a very accurate approximation to the numerical solution that we dub the\nrough SABR formula.\n"
    },
    {
        "paper_id": 2105.05651,
        "authors": "Csaba Mako, Miklos Illessy, Jozsef Pap, Saeed Nosratabadi",
        "title": "Emerging Platform Work in the Context of the Regulatory Loophole (The\n  Uber Fiasco in Hungary)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study examines the essential features of the so-called platform-based\nwork, which is rapidly evolving into a major, potentially game-changing force\nin the labor market. From low-skilled, low-paid services (such as passenger\ntransport) to highly skilled and high-paying project-based work (such as the\ndevelopment of artificial intelligence algorithms), a broad range of tasks can\nbe carried out through a variety of digital platforms. Our paper discusses the\nplatform-based content, working conditions, employment status, and advocacy\nproblems. Terminological and methodological problems are dealt with in-depth in\nthe course of the literature review, together with the 'gray areas' of work and\nemployment regulation. To examine some of the complex dynamics of this\nfast-evolving arena, we focus on the unsuccessful market entry of the digital\nplatform company Uber in Hungary 2016 and the relationship to\ninstitutional-regulatory platform-based work standards. Dilemmas relevant to\nthe enforcement of labor law regarding platform-based work are also paid\nspecial attention to the study. Employing a digital workforce is a significant\nchallenge not only for labor law regulation but also for stakeholder advocacy.\n"
    },
    {
        "paper_id": 2105.05669,
        "authors": "Markus Schlott, Omar El Sayed, Mariia Bilousova, Fabian Hofmann,\n  Alexander Kies, Horst St\\\"ocker",
        "title": "Carbon Leakage in a European Power System with Inhomogeneous Carbon\n  Prices",
        "comments": "19 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global warming is one of the main threats to the future of humanity and\nextensive emissions of greenhouse gases are found to be the main cause of\nglobal temperature rise as well as climate change. During the last decades\ninternational attention has focused on this issue, as well as on searching for\nviable solutions to mitigate global warming. In this context, the pricing of\ngreenhouse gas emissions turned out to be the most prominent mechanism: First,\nto lower the emissions, and second, to capture their external costs. By now,\nvarious carbon dioxide taxes have been adopted by several countries in Europe\nand around the world; moreover, the list of these countries is growing.\nHowever, there is no standardized approach and the price for carbon varies\nsignificantly from one country to another. Regionally diversified carbon prices\nin turn lead to carbon leakage, which will offset the climate protection goals.\nIn this paper, a simplified European power system with flexible carbon prices\nregarding the Gross Domestic Product (GDP) is investigated. A distribution\nparameter that quantifies carbon leakage is defined and varied together with\nthe base carbon price, where the combination of both parameters describes the\nspatially resolved price distribution, i.e. the effective carbon pricing among\nthe European regions. It is shown that inhomogeneous carbon prices will indeed\nlead to significant carbon leakage across the continent, and that coal-fired\nelectricity generation will remain a cheap and therefore major source of power\nin Eastern and South-Eastern Europe - representing a potential risk for the\nlong term decarbonization targets within the European Union.\n"
    },
    {
        "paper_id": 2105.05793,
        "authors": "A. Fronzetti Colladon and E. Remondi",
        "title": "Using social network analysis to prevent money laundering",
        "comments": null,
        "journal-ref": "Expert Systems with Applications 67, 49-58 (2017)",
        "doi": "10.1016/j.eswa.2016.09.029",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This research explores the opportunities for the application of network\nanalytic techniques to prevent money laundering. We worked on real world data\nby analyzing the central database of a factoring company, mainly operating in\nItaly, over a period of 19 months. This database contained the financial\noperations linked to the factoring business, together with other useful\ninformation about the company clients. We propose a new approach to sort and\nmap relational data and present predictive models, based on network metrics, to\nassess risk profiles of clients involved in the factoring business. We find\nthat risk profiles can be predicted by using social network metrics. In our\ndataset, the most dangerous social actors deal with bigger or more frequent\nfinancial operations; they are more peripheral in the transactions network;\nthey mediate transactions across different economic sectors and operate in\nriskier countries or Italian regions. Finally, to spot potential clusters of\ncriminals, we propose a visual analysis of the tacit links existing among\ndifferent companies who share the same owner or representative. Our findings\nshow the importance of using a network-based approach when looking for\nsuspicious financial operations and potential criminals.\n"
    },
    {
        "paper_id": 2105.06017,
        "authors": "Scott W. Hegerty",
        "title": "Do City Borders Constrain Ethnic Diversity?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  U.S. metropolitan areas, particularly in the industrial Midwest and\nNortheast, are well-known for high levels of racial segregation. This is\nespecially true where core cities end and suburbs begin; often crossing the\nstreet can lead to physically similar, but much less ethnically diverse,\nsuburban neighborhood. While these differences are often visually or\n\"intuitively\" apparent, this study seeks to quantify them using Geographic\nInformation Systems and a variety of statistical methods. 2016 Census block\ngroup data are used to calculate an ethnic Herfindahl index for a set of two\ndozen large U.S. cities and their contiguous suburbs. Then, a mathematical\nmethod is developed to calculate a block-group-level \"Border Disparity Index\"\n(BDI), which is shown to vary by MSA and by specific suburbs. Its values can be\ncompared across the sample to examine which cities are more likely to have\nborders that separate more-diverse block groups from less-diverse ones. The\nindex can also be used to see which core cities are relatively more or less\ndiverse than their suburbs, and which individual suburbs have the largest\ndisparities vis-\\`a-vis their core city. Atlanta and Detroit have particularly\ndiverse suburbs, while Milwaukee's are not. Regression analysis shows that\nincome differences and suburban shares of Black residents play significant\nroles in explaining variation across suburbs.\n"
    },
    {
        "paper_id": 2105.06021,
        "authors": "Scott W. Hegerty",
        "title": "How Unique is Milwaukee's 53206? An Examination of Disaggregated\n  Socioeconomic Characteristics Across the City and Beyond",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Milwaukee's 53206 ZIP code, located on the city's near North Side, has drawn\nconsiderable attention for its poverty and incarceration rates, as well as for\nits large proportion of vacant properties. As a result, it has benefited from\ntargeted policies at the city level. Keeping in mind that ZIP codes are often\nnot the most effective unit of geographic analysis, this study investigates\nMilwaukee's socioeconomic conditions at the block group level. These smaller\nareas' statistics are then compared with those of their corresponding ZIP\ncodes. The 53206 ZIP code is compared against others in Milwaukee for eight\nsocioeconomic variables and is found to be near the extreme end of most\nrankings. This ZIP code would also be among Chicago's most extreme areas, but\nwould lie near the middle of the rankings if located in Detroit. Parts of other\nZIP codes, which are often adjacent, are statistically similar to 53206,\nhowever--suggesting that a focus solely on ZIP codes, while a convenient\nshorthand, might overlook neighborhoods that have similar need for investment.\nA multivariate index created for this study performs similarly to a standard\nmultivariate index of economic deprivation if spatial correlation is taken into\naccount, confirming that poverty and other socioeconomic stresses are\nclustered, both in the 53206 ZIP code and across Milwaukee.\n"
    },
    {
        "paper_id": 2105.0639,
        "authors": "Claude Martini, Iacopo Raffaelli",
        "title": "Revisiting the Implied Remaining Variance framework of Carr and Sun\n  (2014): Locally consistent dynamics and sandwiched martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Implied volatility is at the very core of modern finance, notwithstanding\nstandard option pricing models continue to derive option prices starting from\nthe joint dynamics of the underlying asset price and the spot volatility. These\nmodels often cause difficulties: no closed formulas for prices, demanding\ncalibration techniques, unclear maps between spot and implied volatility.\nInspired by the practice of using implied volatility as quoting system for\noption prices, models for the joint dynamics of the underlying asset price and\nthe implied volatility have been proposed to replace standard option pricing\nmodels. Starting from Carr and Sun (2014), we develop a framework based on the\nImplied Remaining Variance where minimal conditions for absence of arbitrage\nare identified, and smile bubbles are dealt with. The key concepts arising from\nthe new IRV framework are those of locally consistent dynamics and sandwiched\nmartingale. Within the new IRV framework, the results of Schweizer and Wissel\n(2008b) are reformulated, while those of El Amrani, Jacquier and Martini (2021)\nare independently derived.\n"
    },
    {
        "paper_id": 2105.06584,
        "authors": "Bruno P. C. Levy, Hedibert F. Lopes",
        "title": "Dynamic Portfolio Allocation in High Dimensions using Sparse Risk\n  Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a fast and flexible method to scale multivariate return volatility\npredictions up to high-dimensions using a dynamic risk factor model. Our\napproach increases parsimony via time-varying sparsity on factor loadings and\nis able to sequentially learn the use of constant or time-varying parameters\nand volatilities. We show in a dynamic portfolio allocation problem with 452\nstocks from the S&P 500 index that our dynamic risk factor model is able to\nproduce more stable and sparse predictions, achieving not just considerable\nportfolio performance improvements but also higher utility gains for the\nmean-variance investor compared to the traditional Wishart benchmark and the\npassive investment on the market index.\n"
    },
    {
        "paper_id": 2105.06607,
        "authors": "Zongxia Liang and Fengyi Yuan",
        "title": "Weak equilibria for time-inconsistent control: with applications to\n  investment-withdrawal decisions",
        "comments": null,
        "journal-ref": "Mathematical Finance, 2023, volume 33, issue 3, pages 891-945",
        "doi": "10.1111/mafi.12391",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers time-inconsistent problems when control and stopping\nstrategies are required to be made simultaneously (called stopping control\nproblems by us). We first formulate the timeinconsistent stopping control\nproblems under general multi-dimensional controlled diffusion model and propose\na formal definition of their equilibria. We show that an admissible pair\n$(\\hat{u},C)$ of controlstopping policy is equilibrium if and only if the\nauxiliary function associated with it solves the extended HJB system, providing\na methodology to verify or exclude equilibrium solutions. We provide several\nexamples to illustrate applications to mathematical finance and control theory.\nFor a problem whose reward function endogenously depends on the current wealth,\nthe equilibrium is explicitly obtained. For another model with a\nnon-exponential discount, we prove that any constant proportion strategy can\nnot be equilibrium. We further show that general non-constant equilibrium\nexists and is described by singular boundary value problems. This example shows\nthat considering our combined problems is essentially different from\ninvestigating them separately. In the end, we also provide a two-dimensional\nexample with a hyperbolic discount.\n"
    },
    {
        "paper_id": 2105.06654,
        "authors": "Anna Aksamit, Libo Li, Marek Rutkowski",
        "title": "Generalized BSDEs with random time horizon in a progressively enlarged\n  filtration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study generalized backward stochastic differential equations (BSDEs) up to\na random time horizon $\\vartheta$, which is not a stopping time, under minimal\nassumptions regarding the properties of $\\vartheta$. In contrast to existing\nworks in this area, we do not impose specific assumptions on the random time\n$\\vartheta$ and we study the existence of solutions to BSDEs and reflected\nBSDEs with a random time horizon through the method of reduction. In addition,\nwe also examine BSDEs and reflected BSDEs with a l\\`adl\\`ag driver where the\ndriver is allowed to have a finite number of common jumps with the martingale\npart.\n"
    },
    {
        "paper_id": 2105.06827,
        "authors": "Mohsen Asgari, Hossein Khasteh",
        "title": "Profitable Strategy Design for Trades on Cryptocurrency Markets with\n  Machine Learning Techniques",
        "comments": "Manuscript Text Revised, Some Results Added",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  AI and data driven solutions have been applied to different fields and\nachieved outperforming and promising results. In this research work we apply\nk-Nearest Neighbours, eXtreme Gradient Boosting and Random Forest classifiers\nfor detecting the trend problem of three cryptocurrency markets. We use these\nclassifiers to design a strategy to trade in those markets. Our input data in\nthe experiments include price data with and without technical indicators in\nseparate tests to see the effect of using them. Our test results on unseen data\nare very promising and show a great potential for this approach in helping\ninvestors with an expert system to exploit the market and gain profit. Our\nhighest profit factor for an unseen 66 day span is 1.60. We also discuss\nlimitations of these approaches and their potential impact on Efficient Market\nHypothesis.\n"
    },
    {
        "paper_id": 2105.06833,
        "authors": "Andr\\'e Barreira da Silva Rocha, Matheus Oliveira Meirim, Lara\n  Corr\\^ea Nogueira",
        "title": "Trends in the E-commerce and in the Traditional Retail Sectors During\n  the Covid-19 Pandemic: an Evolutionary Game Approach",
        "comments": "14 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An evolutionary game model is developed to study the interplay between\nconsumers and producers when trade takes place on an e-commerce marketplace.\nThe type of delivery service available and consumers' taste are particularly\nimportant regarding both game payoffs and players' strategies. The game payoff\nmatrix is then adapted to analyse the different trading patterns that were\ndeveloped during the COVID-19 pandemic in both the traditional retail and\ne-commerce sectors. In contrast to the former, investment in logistics and\nwarehouses in the e-commerce sector allowed for the emergence of a trend in\nwhich fast delivery and eager consumers are becoming the norm.\n"
    },
    {
        "paper_id": 2105.06999,
        "authors": "Foad Shokrollahi, Davood Ahmadian, Luca Vincenzo Ballestra",
        "title": "Actuarial strategy for pricing Asian options under a mixed fractional\n  Brownian motion with jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The mixed fractional Brownian motion ($mfBm$) has become quite popular in\nfinance, since it allows one to model long-range dependence and self-similarity\nwhile remaining, for certain values of the Hurst parameter, arbitrage-free. In\nthe present paper, we propose approximate closed-form solutions for pricing\narithmetic Asian options on an underlying described by the $mfBm$.\nSpecifically, we consider both arithmetic Asian options and arithmetic Asian\npower options, and we obtain analytical formulas for pricing them based on a\nconvenient approximation of the strike price. Both the standard $mfBm$ and the\n$mfBm$ with Poisson log-normally distributed jumps are taken into account.\n"
    },
    {
        "paper_id": 2105.07061,
        "authors": "Yuriy Krepkiy, Asif Lakhany and Amber Zhang",
        "title": "Efficient Least Squares Monte-Carlo Technique for PFE/EE Calculations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a regression-based method, generally referred to as the Least\nSquares Monte Carlo (LSMC) method, to speed up exposure calculations of a\nportfolio. We assume that the portfolio contains several exotic derivatives\nthat are priced using Monte-Carlo on each real world scenario and time step.\nSuch a setting is often referred to as a Monte Carlo over a Monte Carlo or a\nNested Monte Carlo method.\n"
    },
    {
        "paper_id": 2105.07213,
        "authors": "Haoyang Cao, Jodi Dianetti and Giorgio Ferrari",
        "title": "Stationary Discounted and Ergodic Mean Field Games of Singular Control",
        "comments": "28 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study stationary mean field games with singular controls in which the\nrepresentative player interacts with a long-time weighted average of the\npopulation through a discounted and an ergodic performance criterion. This\nclass of games finds natural applications in the context of optimal\nproductivity expansion in dynamic oligopolies. We prove existence and\nuniqueness of the mean field equilibria, which are completely characterized\nthrough nonlinear equations. Furthermore, we relate the mean field equilibria\nfor the discounted and the ergodic games by showing the validity of an Abelian\nlimit. The latter allows also to approximate Nash equilibria of - so far\nunexplored - symmetric N-player ergodic singular control games through the mean\nfield equilibrium of the discounted game. Numerical examples finally illustrate\nin a case study the dependency of the mean field equilibria with respect to the\nparameters of the games.\n"
    },
    {
        "paper_id": 2105.07248,
        "authors": "Karoline Bax, \\\"Ozge Sahin, Claudia Czado, Sandra Paterlini",
        "title": "ESG, Risk, and (Tail) Dependence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While environmental, social, and governance (ESG) trading activity has been a\ndistinctive feature of financial markets, the debate if ESG scores can also\nconvey information regarding a company's riskiness remains open. Regulatory\nauthorities, such as the European Banking Authority (EBA), have acknowledged\nthat ESG factors can contribute to risk. Therefore, it is important to model\nsuch risks and quantify what part of a company's riskiness can be attributed to\nthe ESG scores. This paper aims to question whether ESG scores can be used to\nprovide information on (tail) riskiness. By analyzing the (tail) dependence\nstructure of companies with a range of ESG scores, that is within an ESG rating\nclass, using high-dimensional vine copula modelling, we are able to show that\nrisk can also depend on and be directly associated with a specific ESG rating\nclass. Empirical findings on real-world data show positive not negligible ESG\nrisks determined by ESG scores, especially during the 2008 crisis.\n"
    },
    {
        "paper_id": 2105.07341,
        "authors": "Fei Cao and Sebastien Motsch",
        "title": "Derivation of wealth distributions from biased exchange of money",
        "comments": "47 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the manuscript, we are interested in using kinetic theory to better\nunderstand the time evolution of wealth distribution and their large scale\nbehavior such as the evolution of inequality (e.g. Gini index). We investigate\nthree type of dynamics denoted unbiased, poor-biased and rich-biased dynamics.\nAt the particle level, one agent is picked randomly based on its wealth and one\nof its dollar is redistributed among the population. Proving the so-called\npropagation of chaos, we identify the limit of each dynamics as the number of\nindividual approaches infinity using both coupling techniques [48] and\nmartingale-based approach [36]. Equipped with the limit equation, we identify\nand prove the convergence to specific equilibrium for both the unbiased and\npoor-biased dynamics. In the rich-biased dynamics however, we observe a more\ncomplex behavior where a dispersive wave emerges. Although the dispersive wave\nis vanishing in time, its also accumulates all the wealth leading to a Gini\napproaching 1 (its maximum value). We characterize numerically the behavior of\ndispersive wave but further analytic investigation is needed to derive such\ndispersive wave directly from the dynamics.\n"
    },
    {
        "paper_id": 2105.07524,
        "authors": "Claudia Ceci, Katia Colaneri, Alessandra Cretarola",
        "title": "Optimal Reinsurance and Investment under Common Shock Dependence Between\n  Financial and Actuarial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal proportional reinsurance and investment strategies for an\ninsurance company which experiences both ordinary and catastrophic claims and\nwishes to maximize the expected exponential utility of its terminal wealth. We\npropose a model where the insurance framework is affected by environmental\nfactors, and aggregate claims and stock prices are subject to common shocks,\ni.e. drastic events such as earthquakes, extreme weather conditions, or even\npandemics, that have an immediate impact on the financial market and\nsimultaneously induce insurance claims. Using the classical stochastic control\napproach based on the Hamilton-Jacobi-Bellman equation, we provide a\nverification result for the value function via classical solutions to two\nbackward partial differential equations and characterize the optimal\nreinsurance and investment strategies. Finally, we make a comparison analysis\nto discuss the effect of common shock dependence.\n"
    },
    {
        "paper_id": 2105.07554,
        "authors": "Laura Blattner and Scott Nelson",
        "title": "How Costly is Noise? Data and Disparities in Consumer Credit",
        "comments": "86 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that lenders face more uncertainty when assessing default risk of\nhistorically under-served groups in US credit markets and that this information\ndisparity is a quantitatively important driver of inefficient and unequal\ncredit market outcomes. We first document that widely used credit scores are\nstatistically noisier indicators of default risk for historically under-served\ngroups. This noise emerges primarily through the explanatory power of the\nunderlying credit report data (e.g., thin credit files), not through issues\nwith model fit (e.g., the inability to include protected class in the scoring\nmodel). Estimating a structural model of lending with heterogeneity in\ninformation, we quantify the gains from addressing these information\ndisparities for the US mortgage market. We find that equalizing the precision\nof credit scores can reduce disparities in approval rates and in credit\nmisallocation for disadvantaged groups by approximately half.\n"
    },
    {
        "paper_id": 2105.07565,
        "authors": "D\\'aniel Csaba",
        "title": "Attention elasticities and invariant information costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a generalization of rational inattention problems by measuring\ncosts of information through the information radius (Sibson, 1969; Verd\\'u,\n2015) of statistical experiments. We introduce a notion of attention elasticity\nmeasuring the sensitivity of attention strategies with respect to changes in\nincentives. We show how the introduced class of cost functions controls\nattention elasticities while the Shannon model restricts attention elasticity\nto be unity. We explore further differences and similarities relative to the\nShannon model in relation to invariance, posterior separability, consideration\nsets, and the ability to learn events with certainty. Lastly, we provide an\nefficient alternating minimization method -- analogous to the Blahut-Arimoto\nalgorithm -- to obtain optimal attention strategies.\n"
    },
    {
        "paper_id": 2105.07749,
        "authors": "A. Fronzetti Colladon, F. Grippa, R. Innarella",
        "title": "Studying the association of online brand importance with museum\n  visitors: An application of the semantic brand score",
        "comments": null,
        "journal-ref": "Tourism Management Perspectives 33, 100588 (2020)",
        "doi": "10.1016/j.tmp.2019.100588",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper explores the association between brand importance and growth in\nmuseum visitors. We analyzed 10 years of online forum discussions and applied\nthe Semantic Brand Score (SBS) to assess the brand importance of five European\nMuseums. Our Naive Bayes and regression models indicate that variations in the\ncombined dimensions of the SBS (prevalence, diversity and connectivity) are\naligned with changes in museum visitors. Results suggest that, in order to\nattract more visitors, museum brand managers should focus on increasing the\nvolume of online posting and the richness of information generated by users\naround the brand, rather than controlling for the posts' overall positivity or\nnegativity.\n"
    },
    {
        "paper_id": 2105.07821,
        "authors": "Scott W. Hegerty",
        "title": "Spatial Measures of Socioeconomic Deprivation: An Application to Four\n  Midwestern Industrial Cities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decades of economic decline have led to areas of increased deprivation in a\nnumber of U.S. inner cities, which can be linked to adverse health and other\noutcomes. Yet the calculation of a single \"deprivation\" index, which has\nreceived wide application in Britain and elsewhere in the world, involves a\nchoice of variables and methods that have not been directly compared in the\nAmerican context. This study creates four related measures--using two sets of\nvariables and two weighting schemes--to create such indices for block groups in\nBuffalo, Cleveland, Detroit, and Milwaukee. After examining the indices'\nsimilarities, we then map concentrations of high deprivation in each city and\nanalyze their relationships to income, racial makeup, and transportation usage.\nOverall, we find certain measures to have higher correlations than others, but\nthat all show deprivation to be linked with lower incomes and a higher nonwhite\npopulation.\n"
    },
    {
        "paper_id": 2105.07822,
        "authors": "Scott W. Hegerty",
        "title": "Acquisitive Crimes, Time of Day, and Multiunit Housing in the City of\n  Milwaukee",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  According to \"Social Disorganization\" theory, criminal activity increases if\nthe societal institutions that might be responsible for maintaining order are\nweakened. Do large apartment buildings, which often have fairly transient\npopulations and low levels of community involvement, have disproportionately\nhigh rates of crime? Do these rates differ during the daytime or nighttime,\ndepending when residents are present, or away from their property? This study\nexamines four types of \"acquisitive\" crime in Milwaukee during 2014. Overall,\nnighttime crimes are shown to be more dispersed than daytime crimes. A spatial\nregression estimation finds that the density of multiunit housing is positively\nrelated to all types of crime except burglaries, but not for all times of day.\nDaytime robberies, in particular, increase as the density of multiunit housing\nincreases.\n"
    },
    {
        "paper_id": 2105.07823,
        "authors": "Scott W. Hegerty",
        "title": "Bank Density, Population Density, and Economic Deprivation Across the\n  United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent research on the geographic locations of bank branches in the United\nStates has identified thresholds below which a given area can be considered to\nbe a \"banking desert.\" Thus far, most analyses of the country as a whole have\ntended to focus on minimum distances from geographic areas to the nearest bank,\nwhile a recent density-based analysis focused only on the city of Chicago. As\nsuch, there is not yet a nationwide study of bank densities for the entire\nUnited States. This study calculates banks per square mile for U.S. Census\ntracts over ten different ranges of population density. One main finding is\nthat bank density is sensitive to the measurement radius used (for example,\ndensity in urban areas can be calculated as the number of banks within two\nmiles, while some rural areas require a 20-mile radius). This study then\ncompiles a set of lower 5- and 10-percent thresholds that might be used to\nidentify \"banking deserts\" in various urban, suburban, and rural areas; these\nlargely conform to the findings of previous analyses. Finally, adjusting for\npopulation density using regression residuals, this paper examines whether an\nindex of economic deprivation is significantly higher in the five percent of\n\"desert\" tracts than in the remaining 95 percent. The differences are largest\n-- and highly significant -- in the densest tracts in large urban areas.\n"
    },
    {
        "paper_id": 2105.07824,
        "authors": "Scott W. Hegerty",
        "title": "Are the Spatial Concentrations of Core-City and Suburban Poverty\n  Converging in the Rust Belt?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decades of deindustrialization have led to economic decline and population\nloss throughout the U.S. Midwest, with the highest national poverty rates found\nin Detroit, Cleveland, and Buffalo. This poverty is often confined to core\ncities themselves, however, as many of their surrounding suburbs continue to\nprosper. Poverty can therefore be highly concentrated at the MSA level, but\nmore evenly distributed within the borders of the city proper. One result of\nthis disparity is that if suburbanites consider poverty to be confined to the\ncentral city, they might be less willing to devote resources to alleviate it.\nBut due to recent increases in suburban poverty, particularly since the 2008\nrecession, such urban-suburban gaps might be shrinking. Using Census\ntract-level data, this study quantifies poverty concentrations for four \"Rust\nBelt\" MSAs, comparing core-city and suburban concentrations in 2000, 2010, and\n2015. There is evidence of a large gap between core cities and outlying areas,\nwhich is closing in the three highest-poverty cities, but not in Milwaukee. A\nset of four comparison cities show a smaller, more stable city-suburban divide\nin the U.S. \"Sunbelt,\" while Chicago resembles a \"Rust Belt\" metro.\n"
    },
    {
        "paper_id": 2105.07867,
        "authors": "Juan B Gonzalez, Alfonso Sanchez",
        "title": "What shapes climate change perceptions in Africa? A random forest\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Climate change perceptions are fundamental for adaptation and environmental\npolicy support. Although Africa is one of the most vulnerable regions to\nclimate change, little research has focused on how climate change is perceived\nin the continent. Using random forest methodology, we analyse Afrobarometer\ndata (N = 45,732), joint with climatic data, to explore what shapes climate\nchange perceptions in Africa. We include 5 different dimensions of climate\nchange perceptions: awareness, belief in its human cause, risk perception, need\nto stop it and self-efficacy. Results indicate that perceived agriculture\nconditions are crucial for perceiving climate change. Country-level factors and\nlong-term changes in local weather conditions are among the most important\npredictors. Moreover, education level, access to information, poverty,\nauthoritarian values, and trust in institutions shape individual climate change\nperceptions. Demographic effects -- including religion -- seem negligible.\nThese findings suggest policymakers and environmental communicators how to\nframe climate change in Africa to raise awareness, gather public support and\ninduce adaptation.\n"
    },
    {
        "paper_id": 2105.07915,
        "authors": "Thomas Krabichler, Marcus Wunsch",
        "title": "Hedging Goals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Goal-based investing is concerned with reaching a monetary investment goal by\na given finite deadline, which differs from mean-variance optimization in\nmodern portfolio theory. In this article, we expand the close connection\nbetween goal-based investing and option hedging that was originally discovered\nin [Bro99b] by allowing for varying degrees of investor risk aversion using\nlower partial moments of different orders. Moreover, we show that maximizing\nthe probability of reaching the goal (quantile hedging, cf. [FL99]) and\nminimizing the expected shortfall (efficient hedging, cf. [FL00]) yield, in\nfact, the same optimal investment policy. We furthermore present an innovative\nand model-free approach to goal-based investing using methods of reinforcement\nlearning. To the best of our knowledge, we offer the first algorithmic approach\nto goal-based investing that can find optimal solutions in the presence of\ntransaction costs.\n"
    },
    {
        "paper_id": 2105.08133,
        "authors": "Tim Leung and Theodore Zhao",
        "title": "Adaptive Complementary Ensemble EMD and Energy-Frequency Spectra of\n  Cryptocurrency Prices",
        "comments": "20 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the price dynamics of cryptocurrencies using adaptive complementary\nensemble empirical mode decomposition (ACE-EMD) and Hilbert spectral analysis.\nThis is a multiscale noise-assisted approach that decomposes any time series\ninto a number of intrinsic mode functions, along with the corresponding\ninstantaneous amplitudes and instantaneous frequencies. The decomposition is\nadaptive to the time-varying volatility of each cryptocurrency price evolution.\nDifferent combinations of modes allow us to reconstruct the time series using\ncomponents of different timescales. We then apply Hilbert spectral analysis to\ndefine and compute the instantaneous energy-frequency spectrum of each\ncryptocurrency to illustrate the properties of various timescales embedded in\nthe original time series.\n"
    },
    {
        "paper_id": 2105.08139,
        "authors": "Andrey Sarantsev",
        "title": "Optimal Portfolio with Power Utility of Absolute and Relative Wealth",
        "comments": "Merton's problem; stochastic optimization; portfolio theory; wealth\n  process",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio managers often evaluate performance relative to benchmark, usually\ntaken to be the Standard & Poor 500 stock index fund. This relative portfolio\nwealth is defined as the absolute portfolio wealth divided by wealth from\ninvesting in the benchmark (including reinvested dividends). The classic Merton\nproblem for portfolio optimization considers absolute portfolio wealth. We\ncombine absolute and relative wealth in our new utility function. We also\nconsider the case of multiple benchmarks. To both absolute and relative wealth,\nwe apply power utility functions, possibly with different exponents. We obtain\nan explicit solution and compare it to the classic Merton solution. We apply\nour results to the Capital Asset Pricing Model setting.\n"
    },
    {
        "paper_id": 2105.08208,
        "authors": "Tjeerd de Vries",
        "title": "A Tale of Two Tails: A Model-free Approach to Estimating Disaster Risk\n  Premia and Testing Asset Pricing Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  I introduce a model-free methodology to assess the impact of disaster risk on\nthe market return. Using S&P500 returns and the risk-neutral quantile function\nderived from option prices, I employ quantile regression to estimate local\ndifferences between the conditional physical and risk-neutral distributions.\nThe results indicate substantial disparities primarily in the left-tail,\nreflecting the influence of disaster risk on the equity premium. These\ndifferences vary over time and persist beyond crisis periods. On average, the\nbottom 5% of returns contribute to 17% of the equity premium, shedding light on\nthe Peso problem. I also find that disaster risk increases the stochastic\ndiscount factor's volatility. Using a lower bound observed from option prices\non the left-tail difference between the physical and risk-neutral quantile\nfunctions, I obtain similar results, reinforcing the robustness of my findings.\n"
    },
    {
        "paper_id": 2105.0831,
        "authors": "Dave Cliff",
        "title": "BBE: Simulating the Microstructural Dynamics of an In-Play Betting\n  Exchange via Agent-Based Modelling",
        "comments": "47 pages, 9 figures, 120 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I describe the rationale for, and design of, an agent-based simulation model\nof a contemporary online sports-betting exchange: such exchanges, closely\nrelated to the exchange mechanisms at the heart of major financial markets,\nhave revolutionized the gambling industry in the past 20 years, but gathering\nsufficiently large quantities of rich and temporally high-resolution data from\nreal exchanges - i.e., the sort of data that is needed in large quantities for\nDeep Learning - is often very expensive, and sometimes simply impossible; this\ncreates a need for a plausibly realistic synthetic data generator, which is\nwhat this simulation now provides. The simulator, named the \"Bristol Betting\nExchange\" (BBE), is intended as a common platform, a data-source and\nexperimental test-bed, for researchers studying the application of AI and\nmachine learning (ML) techniques to issues arising in betting exchanges; and,\nas far as I have been able to determine, BBE is the first of its kind: a free\nopen-source agent-based simulation model consisting not only of a\nsports-betting exchange, but also a minimal simulation model of racetrack\nsporting events (e.g., horse-races or car-races) about which bets may be made,\nand a population of simulated bettors who each form their own private\nevaluation of odds and place bets on the exchange before and - crucially -\nduring the race itself (i.e., so-called \"in-play\" betting) and whose betting\nopinions change second-by-second as each race event unfolds. BBE is offered as\na proof-of-concept system that enables the generation of large high-resolution\ndata-sets for automated discovery or improvement of profitable strategies for\nbetting on sporting events via the application of AI/ML and advanced data\nanalytics techniques. This paper offers an extensive survey of relevant\nliterature and explains the motivation and design of BBE, and presents brief\nillustrative results.\n"
    },
    {
        "paper_id": 2105.08349,
        "authors": "Elena Gubar, Laura Policardo, Edgar J. Sanchez Carrera, Vladislav\n  Taynitskiy",
        "title": "Optimal Lockdown Policies driven by Socioeconomic Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this research paper we modify a classical SIR model to better adapt to the\ndynamics of COVID-19, that is we propose the heterogeneous SQAIRD model where\nCOVID-19 spreads over a population of economic agents, namely: the elderly,\nadults and young people. We then compute and simulate an optimal control\nproblem faced by a Government, where its objective is to minimize the costs\ngenerated by the pandemics using as control a compulsory quarantine measure\n(that is, a lockdown). We first analyze the problem from a theoretical\nperspective, claiming that different lockdown policies (total lockdown, no\nlockdown or partial lockdown) may justified by different cost (concave or\nconvex) structures of the economies. We then focus on a particular cost\nstructure (convex costs) and we simulate a targeted optimal policy vs. a\nuniform optimal policy, by dividing the whole population in three demographic\ngroups (young, adults and old). We also simulate the dynamic of the pandemic\nwith no policy implemented. Simulations highlighted the fact that: a) a policy\nof lockdown is always better than the \\emph{laissez faire} policy, because it\nlimits the costs that the pandemic generates in an uncontrolled situation; b) a\ntargeted policy based on age of the individuals outperforms a uniform policy in\nterms of costs that it generates, being a targeted policy less costly and\nequally effective in the control of the pandemic.\n"
    },
    {
        "paper_id": 2105.08377,
        "authors": "Thierry Roncalli, Amina Cherief, Fatma Karray-Meziou, Margaux Regnault",
        "title": "Liquidity Stress Testing in Asset Management -- Part 2. Modeling the\n  Asset Liquidity Risk",
        "comments": "86 pages, 36 figures, 44 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article is part of a comprehensive research project on liquidity risk in\nasset management, which can be divided into three dimensions. The first\ndimension covers liability liquidity risk (or funding liquidity) modeling, the\nsecond dimension focuses on asset liquidity risk (or market liquidity)\nmodeling, and the third dimension considers the asset-liability management of\nthe liquidity gap risk (or asset-liability matching). The purpose of this\nresearch is to propose a methodological and practical framework in order to\nperform liquidity stress testing programs, which comply with regulatory\nguidelines (ESMA, 2019, 2020) and are useful for fund managers. The review of\nthe academic literature and professional research studies shows that there is a\nlack of standardized and analytical models. The aim of this research project is\nthen to fill the gap with the goal of developing mathematical and statistical\napproaches, and providing appropriate answers.\n  In this second article focused on asset liquidity risk modeling, we propose a\nmarket impact model to estimate transaction costs. After presenting a toy model\nthat helps to understand the main concepts of asset liquidity, we consider a\ntwo-regime model, which is based on the power-law property of price impact.\nThen, we define several asset liquidity measures such as liquidity cost,\nliquidation ratio and shortfall or time to liquidation in order to assess the\ndifferent dimensions of asset liquidity. Finally, we apply this asset liquidity\nframework to stocks and bonds and discuss the issues of calibrating the\ntransaction cost model.\n"
    },
    {
        "paper_id": 2105.08475,
        "authors": "Katya Klinova and Anton Korinek",
        "title": "AI and Shared Prosperity",
        "comments": null,
        "journal-ref": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES '21)",
        "doi": "10.1145/3461702.3462619",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Future advances in AI that automate away human labor may have stark\nimplications for labor markets and inequality. This paper proposes a framework\nto analyze the effects of specific types of AI systems on the labor market,\nbased on how much labor demand they will create versus displace, while taking\ninto account that productivity gains also make society wealthier and thereby\ncontribute to additional labor demand. This analysis enables ethically-minded\ncompanies creating or deploying AI systems as well as researchers and\npolicymakers to take into account the effects of their actions on labor markets\nand inequality, and therefore to steer progress in AI in a direction that\nadvances shared prosperity and an inclusive economic future for all of\nhumanity.\n"
    },
    {
        "paper_id": 2105.08664,
        "authors": "Farzan Soleymani, Eric Paquet",
        "title": "Deep Graph Convolutional Reinforcement Learning for Financial Portfolio\n  Management -- DeepPocket",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eswa.2021.115127",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management aims at maximizing the return on investment while\nminimizing risk by continuously reallocating the assets forming the portfolio.\nThese assets are not independent but correlated during a short time period. A\ngraph convolutional reinforcement learning framework called DeepPocket is\nproposed whose objective is to exploit the time-varying interrelations between\nfinancial instruments. These interrelations are represented by a graph whose\nnodes correspond to the financial instruments while the edges correspond to a\npair-wise correlation function in between assets. DeepPocket consists of a\nrestricted, stacked autoencoder for feature extraction, a convolutional network\nto collect underlying local information shared among financial instruments, and\nan actor-critic reinforcement learning agent. The actor-critic structure\ncontains two convolutional networks in which the actor learns and enforces an\ninvestment policy which is, in turn, evaluated by the critic in order to\ndetermine the best course of action by constantly reallocating the various\nportfolio assets to optimize the expected return on investment. The agent is\ninitially trained offline with online stochastic batching on historical data.\nAs new data become available, it is trained online with a passive concept drift\napproach to handle unexpected changes in their distributions. DeepPocket is\nevaluated against five real-life datasets over three distinct investment\nperiods, including during the Covid-19 crisis, and clearly outperformed market\nindexes.\n"
    },
    {
        "paper_id": 2105.08804,
        "authors": "Laurence Carassus and Massinissa Ferhoune",
        "title": "Efficient approximations for utility-based pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a context of illiquidity, the reservation price is a well-accepted\nalternative to the usual martingale approach which does not apply. However,\nthis price is not available in closed form and requires numerical methods such\nas Monte Carlo or polynomial approximations to evaluate it. We show that these\nmethods can be inaccurate and propose a deterministic decomposition of the\nreservation price using the Lambert function. This decomposition allows us to\nperform an improved Monte Carlo method, which we name Lambert Monte Carlo (LMC)\nand to give deterministic approximations of the reservation price and of the\noptimal strategies based on the Lambert function. We also give an answer to the\nproblem of selecting a hedging asset that minimizes the reservation price and\nalso the cash invested. Our theoretical results are illustrated by numerical\nsimulations.\n"
    },
    {
        "paper_id": 2105.08814,
        "authors": "Shiqin Liu, Carl Higgs, Jonathan Arundel, Geoff Boeing, Nicholas\n  Cerdera, David Moctezuma, Ester Cerin, Deepti Adlakha, Melanie Lowe, and\n  Billie Giles-Corti",
        "title": "A Generalized Framework for Measuring Pedestrian Accessibility around\n  the World Using Open Data",
        "comments": null,
        "journal-ref": "Geographical Analysis, 2021",
        "doi": "10.1111/gean.12290",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pedestrian accessibility is an important factor in urban transport and land\nuse policy and critical for creating healthy, sustainable cities. Developing\nand evaluating indicators measuring inequalities in pedestrian accessibility\ncan help planners and policymakers benchmark and monitor the progress of city\nplanning interventions. However, measuring and assessing indicators of urban\ndesign and transport features at high resolution worldwide to enable city\ncomparisons is challenging due to limited availability of official, high\nquality, and comparable spatial data, as well as spatial analysis tools\noffering customizable frameworks for indicator construction and analysis. To\naddress these challenges, this study develops an open source software framework\nto construct pedestrian accessibility indicators for cities using open and\nconsistent data. It presents a generalized method to consistently measure\npedestrian accessibility at high resolution and spatially aggregated scale, to\nallow for both within- and between-city analyses. The open source and open data\nmethods developed in this study can be extended to other cities worldwide to\nsupport local planning and policymaking. The software is made publicly\navailable for reuse in an open repository.\n"
    },
    {
        "paper_id": 2105.0914,
        "authors": "Matthieu Garcin",
        "title": "Forecasting with fractional Brownian motion: a financial perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fractional Brownian motion (fBm) extends the standard Brownian motion by\nintroducing some dependence between non-overlapping increments. Consequently,\nif one considers for example that log-prices follow an fBm, one can exploit the\nnon-Markovian nature of the fBm to forecast future states of the process and\nmake statistical arbitrages. We provide new insights into forecasting an fBm,\nby proposing theoretical formulas for accuracy metrics relevant to a systematic\ntrader, from the hit ratio to the expected gain and risk of a simple strategy.\nIn addition, we answer some key questions about optimizing trading strategies\nin the fBm framework: Which lagged increments of the fBm, observed in discrete\ntime, are to be considered? If the predicted increment is close to zero, up to\nwhich threshold is it more profitable not to invest? We also propose empirical\napplications on high-frequency FX rates, as well as on realized volatility\nseries, exploring the rough volatility concept in a forecasting perspective.\n"
    },
    {
        "paper_id": 2105.09148,
        "authors": "Fabian Stephany, Otto K\\\"assi, Uma Rani, Vili Lehdonvirta",
        "title": "Online Labour Index 2020: New ways to measure the world's remote\n  freelancing market",
        "comments": "10 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Online Labour Index (OLI) was launched in 2016 to measure the global\nutilisation of online freelance work at scale. Five years after its creation,\nthe OLI has become a point of reference for scholars and policy experts\ninvestigating the online gig economy. As the market for online freelancing work\nmatures, a high volume of data and new analytical tools allow us to revisit\nhalf a decade of online freelance monitoring and extend the index's scope to\nmore dimensions of the global online freelancing market. In addition to\nmeasuring the utilisation of online labour across countries and occupations by\ntracking the number of projects and tasks posted on major English-language\nplatforms, the new Online Labour Index 2020 (OLI 2020) also tracks Spanish- and\nRussian-language platforms, reveals changes over time in the geography of\nlabour supply, and estimates female participation in the online gig economy.\nThe rising popularity of software and tech work and the concentration of\nfreelancers on the Indian subcontinent are examples of the insights that the\nOLI 2020 provides. The OLI 2020 delivers a more detailed picture of the world\nof online freelancing via an interactive online visualisation updated daily. It\nprovides easy access to downloadable open data for policymakers, labour market\nresearchers, and the general public (www.onlinelabourobservatory.org).\n"
    },
    {
        "paper_id": 2105.09154,
        "authors": "M. Elshendy, A. Fronzetti Colladon, E. Battistoni, P. A. Gloor",
        "title": "Using four different online media sources to forecast the crude oil\n  price",
        "comments": null,
        "journal-ref": "Journal of Information Science 44(3), 408-421 (2018)",
        "doi": "10.1177/0165551517698298",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study looks for signals of economic awareness on online social media and\ntests their significance in economic predictions. The study analyses, over a\nperiod of two years, the relationship between the West Texas Intermediate daily\ncrude oil price and multiple predictors extracted from Twitter, Google Trends,\nWikipedia, and the Global Data on Events, Language, and Tone database (GDELT).\nSemantic analysis is applied to study the sentiment, emotionality and\ncomplexity of the language used. Autoregressive Integrated Moving Average with\nExplanatory Variable (ARIMAX) models are used to make predictions and to\nconfirm the value of the study variables. Results show that the combined\nanalysis of the four media platforms carries valuable information in making\nfinancial forecasting. Twitter language complexity, GDELT number of articles\nand Wikipedia page reads have the highest predictive power. This study also\nallows a comparison of the different fore-sighting abilities of each platform,\nin terms of how many days ahead a platform can predict a price movement before\nit happens. In comparison with previous work, more media sources and more\ndimensions of the interaction and of the language used are combined in a joint\nanalysis.\n"
    },
    {
        "paper_id": 2105.09264,
        "authors": "Haoran Wang, Shi Yu",
        "title": "Robo-Advising: Enhancing Investment with Inverse Optimization and Deep\n  Reinforcement Learning",
        "comments": "13 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine Learning (ML) has been embraced as a powerful tool by the financial\nindustry, with notable applications spreading in various domains including\ninvestment management. In this work, we propose a full-cycle data-driven\ninvestment robo-advising framework, consisting of two ML agents. The first\nagent, an inverse portfolio optimization agent, infers an investor's risk\npreference and expected return directly from historical allocation data using\nonline inverse optimization. The second agent, a deep reinforcement learning\n(RL) agent, aggregates the inferred sequence of expected returns to formulate a\nnew multi-period mean-variance portfolio optimization problem that can be\nsolved using deep RL approaches. The proposed investment pipeline is applied on\nreal market data from April 1, 2016 to February 1, 2021 and has shown to\nconsistently outperform the S&P 500 benchmark portfolio that represents the\naggregate market optimal allocation. The outperformance may be attributed to\nthe the multi-period planning (versus single-period planning) and the\ndata-driven RL approach (versus classical estimation approach).\n"
    },
    {
        "paper_id": 2105.09276,
        "authors": "Giorgia Callegaro, Alessandro Gnoatto, Martino Grasselli",
        "title": "A Fully Quantization-based Scheme for FBSDEs",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a quantization-based numerical scheme for a family of decoupled\nFBSDEs. We simplify the scheme for the control in Pag\\`es and Sagna (2018) so\nthat our approach is fully based on recursive marginal quantization and does\nnot involve any Monte Carlo simulation for the computation of conditional\nexpectations. We analyse in detail the numerical error of our scheme and we\nshow through some examples the performance of the whole procedure, which proves\nto be very effective in view of financial applications.\n"
    },
    {
        "paper_id": 2105.09473,
        "authors": "Dodo Natatou Moutari, Hassane Abba Mallam, Diakarya Barro, Bisso Saley",
        "title": "Dependence Modeling and Risk Assessment of a Financial Portfolio with\n  ARMA-APARCH-EVT models based on HACs",
        "comments": "16 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study aims to widen the sphere of pratical applicability of the HAC\nmodel combined with the ARMA-APARCH volatility forecast model and the extreme\nvalues theory. A sequential process of modeling of the VaR of a portfolio based\non the ARMA-APARCH-EVT-HAC model was discussed. The empirical analysis\nconducted with data from international stock market indices clearly illustrates\nthe performance and accuracy of modeling based on HACs.\n"
    },
    {
        "paper_id": 2105.09581,
        "authors": "Bartosz Jaroszkowski, Max Jensen",
        "title": "Valuation of European Options under an Uncertain Market Price of\n  Volatility Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a model to quantify the effect of parameter uncertainty on the\noption price in the Heston model. More precisely, we present a\nHamilton-Jacobi-Bellman framework which allows us to evaluate best and worst\ncase scenarios under an uncertain market price of volatility risk. For the\nnumerical approximation the Hamilton--Jacobi--Bellman equation is reformulated\nto enable the solution with a finite element method. A case study with\nbutterfly options exhibits how the dependence of Delta on the magnitude of the\nuncertainty is nonlinear and highly varied across the parameter regime.\n  Keywords: Uncertain market price, Volatility risk, Hamilton-Jacobi-Bellman\nequation, Finite element method, Uncertainty quantification\n"
    },
    {
        "paper_id": 2105.09736,
        "authors": "R. McKenna, I. Mulalic, I. Soutar, J. M. Weinand, J. Price, S.\n  Petrovic, K. Mainzer",
        "title": "Exploring trade-offs between landscape impact, land use and resource\n  quality for onshore variable renewable energy: an application to Great\n  Britain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The ambitious Net Zero aspirations of Great Britain (GB) require massive and\nrapid developments of Variable Renewable Energy (VRE) technologies. GB\npossesses substantial resources for these technologies, but questions remain\nabout which VRE should be exploited where. This study explores the trade-offs\nbetween landscape impact, land use competition and resource quality for onshore\nwind as well as ground- and roof-mounted photovoltaic (PV) systems for GB.\nThese trade-offs constrain the technical and economic potentials for these\ntechnologies at the Local Authority level. Our approach combines\ntechno-economic and geospatial analyses with crowd-sourced scenicness data to\nquantify landscape aesthetics. Despite strong correlations between scenicness\nand planning application outcomes for onshore wind, no such relationship exists\nfor ground-mounted PV. The innovative method for rooftop-PV assessment combines\nbottom-up analysis of four cities with a top-down approach at the national\nlevel. The results show large technical potentials that are strongly\nconstrained by both landscape and land use aspects. This equates to about 1324\nTWh of onshore wind, 153 TWh of rooftop PV and 1200-7093 TWh ground-mounted PV,\ndepending on scenario. We conclude with five recommendations that focus around\naligning energy and planning policies for VRE technologies across multiple\nscales and governance arenas.\n"
    },
    {
        "paper_id": 2105.09782,
        "authors": "A. G. A. Cariappa, B. S. Chandel, G. Sankhala, V. Mani, R. Sendhil, A.\n  K. Dixit and B. S. Meena",
        "title": "Estimation of economic losses due to milk fever and efficiency gains if\n  prevented: evidence from Haryana, India",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3851567",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Calcium (Ca) requirement increases tenfold upon parturition in dairy cows &\nbuffaloes and its deficiency leads to a condition called milk fever (MF).\nEstimation of losses is necessary to understand the depth of the problem and\ndesign preventive measures. How much is the economic loss due to MF? What will\nbe the efficiency gain if MF is prevented at the advent of a technology? We\nanswer these questions using survey data and official statistics employing\neconomic surplus model. MF incidence in sample buffaloes and cows was 19% and\n28%, respectively. Total economic losses were calculated as a sum total of\nlosses from milk production, mortality of animals and treatment costs. Yearly\neconomic loss due to MF was estimated to be INR 1000 crores (US$ 137 million)\nin Haryana. Value of milk lost had the highest share in total economic losses\n(58%), followed by losses due to mortality (29%) and treatment costs (13%).\nDespite lower MF incidence, losses were higher in buffaloes due to higher milk\nprices and market value of animals. The efficiency gain accruing to producers\nif MF is prevented, resulting from increased milk production at decreased costs\nwas estimated at INR 10990 crores (US$ 1.5 billion). As the potential gain if\nprevented is around 10 times the economic losses, this study calls for the use\nof preventive technology against MF.\n"
    },
    {
        "paper_id": 2105.10007,
        "authors": "Ji Hyung Lee, Yuya Sasaki, Alexis Akira Toda, Yulong Wang",
        "title": "Fixed-k Tail Regression: New Evidence on Tax and Wealth Inequality from\n  Forbes 400",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel fixed-k tail regression method that accommodates the\nunique feature in the Forbes 400 data that observations are truncated from\nbelow at the 400th largest order statistic. Applying this method, we find that\nhigher maximum marginal income tax rates induce higher wealth Pareto exponents.\nSetting the maximum tax rate to 30-40% (as in U.S. currently) leads to a Pareto\nexponent of 1.5-1.8, while counterfactually setting it to 80% (as suggested by\nPiketty, 2014) would lead to a Pareto exponent of 2.6. We present a simple\neconomic model that explains these findings and discuss the welfare\nimplications of taxation.\n"
    }
]